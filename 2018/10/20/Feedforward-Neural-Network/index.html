<!DOCTYPE HTML>
<html class="no-js" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
    <!--[if lte IE 9]>
<meta http-equiv="refresh" content="0;url=http://yoursite.com/warn.html">
<![endif]-->
<meta charset="utf-8">
<meta http-equiv="X-DNS-Prefetch-Control" content="on">
<link rel="dns-prefetch" href="http://yoursite.com">
<link rel="dns-prefetch" href="//www.google-analytics.com">
<link rel="prefetch" href="http://yoursite.com">
<link rel="prefetch" href="//www.google-analytics.com">


<link rel="prerender" href="http://yoursite.com">

<meta http-equiv="X-UA-Compatible" content="IE=Edge">
<meta name="renderer" content="webkit">
<meta name="viewport" content="width=device-width, initial-scale=1.0,user-scalable=no">
<meta http-equiv="mobile-agent" content="format=html5; url=http://yoursite.com">
<meta name="author" content="Louis Hsu">
<link rel="stylesheet" href="/css/JSimple.css">

<link rel="shortcut icon" href="/images/favicon.png">


<title>feedforward neural network - LOUIS&#39; BLOG</title>

<meta name="keywords" content="">

<meta name="description " content="Inside! Insane!">

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                processEscapes: true
            }
        });
    </script>


    

    

</head>
<body>
<div id="nav">
    <nav class="nav-menu">
        <a class="site-name current" href="/" title="彬">彬</a>
        <a class="site-index current" href="/"><i class="fa fa-home"></i><span>Home</span></a>
        <a href="/archives" title="Archives"><i class="fa fa-archives"></i><span>Archives</span></a>
        <a href="/tags" title="Tags"><i class="fa fa-tags"></i><span>Tags</span></a>
        <!-- custom single page of menus -->
        
        
        <a href="/help" title="帮助">
            <i class="fa fa-question-circle"></i>
            <span>帮助</span>
        </a>
        
    </nav>
</div>

<div class="nav-user">
    <a class="btn-search" href="#"><i class="fa fa-search"></i></a>
    <a class="btn-read-mode" href="#"><i class="fa fa-sun-o"></i></a>
    <a class="btn-sns-qr" href="javascript:"><i class="fa fa-telegram"></i></a>
</div>

<div id="wrapper" class="clearfix">
    <div id="body">
        <div class="main" id="main">
            <div id="cover">
    <div class="cover-img"></div>
    <div class="cover-info">
        
        <h1 class="cover-siteName">LOUIS&#39; BLOG</h1>
        <h3 class="cover-siteTitle">人生苦短，不如不管，继续任性</h3>
        <p class="cover-siteDesc">技术博客？</p>
        <div class="cover-sns">
            

        </div>
    </div>
</div>

            <div class="page-title">
    <ul>
        <li><a href="/">Recent Posts</a></li>
        
        
        
        <li class="page-search">
    <form id="search" class="search-form">
        <input type="text" readonly="readonly" id="local-search-input-tip" placeholder="click to search...">
        <button type="button" disabled="disabled" class="search-form-submit"><i class="fa fa-search"></i></button>
    </form>
</li>

    </ul>
</div>
<div class="main-inner">
    <article class="post" itemscope="" itemtype="http://schema.org/BlogPosting">
        <div class="post-header">
            <div class="post-author clearfix">
                <a class="avatar fleft" href="https://louishsu.xyz/" target="_blank">
                    <img width="48" src="/images/favicon.png" alt="avatar">
                </a>
                <p><span class="label">Author</span>
                    <a href="https://louishsu.xyz/" target="_blank">Louis Hsu</a>
                    <span title="Last edited at&nbsp;2018-10-20">2018-10-20</span>
                </p>
                <p>有味道的程序员</p>
            </div>
            <h2 class="post-title">Feedforward Neural Network</h2>
            <div class="post-meta">
                emm... 14368 words in the article |
                you are the&nbsp;<span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span>th friend who reading now
            </div>
        </div>
        <div class="post-content markdown-body">
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>前馈神经网络是一种最简单的神经网络，各神经元分层排列。每个神经元只与前一层的神经元相连。接收前一层的输出，并输出给下一层．各层间没有反馈。是目前应用最广泛、发展最迅速的人工神经网络之一，既可以用于解决分类问题，也可以用于解决回归问题。</p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>前馈神经网络也叫作多层感知机，包含输入层，隐含层和输出层三个部分。它的目的是为了实现输入到输出的映射。</p>
<script type="math/tex; mode=display">
y = f(x;W)</script><p>由于各层采用了非线性激活函数，神经网络具有良好的非线性特性，如下图所示。</p>
<ul>
<li>激活函数为线性单元<br><img src="/2018/10/20/Feedforward-Neural-Network/Linear.png" alt="Linear"></li>
<li>激活函数为非线性单元<br><img src="/2018/10/20/Feedforward-Neural-Network/nonLinear.png" alt="nonLinear"></li>
</ul>
<p>前馈神经网络可用于解决非线性的分类或回归问题，参数通过反向传播算法<code>(Back Propagation)</code>学习。</p>
<h1 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h1><h2 id="神经元与网络结构图"><a href="#神经元与网络结构图" class="headerlink" title="神经元与网络结构图"></a>神经元与网络结构图</h2><p>单个神经元的示意图如下，输入为前一层的输出参数$X^{(l-1)}$</p>
<script type="math/tex; mode=display">
h_{w, b}(x) = \sigma (WX + b)</script><p>$\sigma(·)$表示激活函数。</p>
<p><img src="/2018/10/20/Feedforward-Neural-Network/单个神经元示意图.png" alt="单个神经元示意图"></p>
<p>以下为典型的神经网络结构图<br><img src="/2018/10/20/Feedforward-Neural-Network/前馈神经网络结构图.png" alt="前馈神经网络结构图"></p>
<ul>
<li>第一层为输入层<code>input layer</code>，一般不设置权值，预处理在输入网络前完成；</li>
<li>最后一层为输出层<code>output layer</code>；</li>
<li>其余层称为隐藏层<code>hidden layer</code>，隐藏层用于提取数据特征，隐藏层层数与各层神经元个数为超参数。</li>
</ul>
<blockquote>
<p>神经元权值取值不同，可实现不同的逻辑运算，单个超平面只能进行二元划分，利用逻辑运算可将多个超平面划分的区域拼接起来，如图<br><img src="/2018/10/20/Feedforward-Neural-Network/超平面划分区域的拼接.jpg" alt="超平面划分区域的拼接"></p>
<p>以下说明逻辑运算的实现方法<br><img src="/2018/10/20/Feedforward-Neural-Network/二元逻辑运算.png" alt="二元逻辑运算"><br>其中</p>
<script type="math/tex; mode=display">
f(z) = \begin{cases}
    1 & z \geq 0 \\
    0 & otherwise
\end{cases}</script><ul>
<li><p>与运算 $a ∧ b$</p>
<script type="math/tex; mode=display">w_1 = 20, w_2 = 20, b = -30</script></li>
<li><p>或运算 $a ∧ b$</p>
<script type="math/tex; mode=display">w_1 = 20, w_2 = 20, b = -10</script></li>
<li><p>非运算 $a = \overline{b}$</p>
<script type="math/tex; mode=display">w_1 = -20, w_2 = 0, b = 0</script></li>
<li><p>异或运算 $a \bigoplus b$，可通过组合运算实现</p>
<script type="math/tex; mode=display">a \bigoplus b = (\overline{a} ∧ b) ∨ (a ∧ \overline{b})</script></li>
</ul>
</blockquote>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><ul>
<li><p>隐藏层的激活函数，详情可查看<a href="https://louishsu.xyz/2018/10/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" target="_blank" rel="noopener">另一篇博文：神经网络的激活函数</a>；</p>
</li>
<li><p>输出层的激活函数</p>
<ul>
<li><p>回归问题时，采用线性单元即可</p>
<script type="math/tex; mode=display">
  f(x) = x</script></li>
<li><p>分类问题时，一般有以下几种选择</p>
<ul>
<li><p>单类别概率输出<br>  即每个神经元的输出对应该类别的$0-1$分布输出，这就需要将输出值限制在$[0, 1]$内，例如</p>
<script type="math/tex; mode=display">P(y=1|x )= max\{0, min\{1, z\}\}</script><p>  <img src="/2018/10/20/Feedforward-Neural-Network/clf_linearout.png" alt="线性输出单元"></p>
<p>  但是可以看到，当$(w^Tx+b)$处于单位区间外时，模型的输出对它的参数的梯度都将为$0$ ，不利于网络的训练，故采用$S$形函数<code>Sigmoid</code>(<a href="https://louishsu.xyz/2018/10/18/Logistic%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">详情</a>)</p>
<script type="math/tex; mode=display">
  P(y=1|x ) = \frac{1}{1+e^{-(w^Tx+b)}}</script><blockquote>
<p>$(1)$ <code>Sigmoid</code>函数定义域为$(-\infty, \infty)$，值域为$(0, 1)$，且在整个定义域上单调递增，即为单值函数，故可将线性输出单元的结果映射到$(0, 1)$范围内；<br>$(2)$ 在定义域上处处可导。</p>
</blockquote>
</li>
<li><p>多类别的概率输出<br>  即每个神经元的输出对应判别为该类别的概率，且有</p>
<script type="math/tex; mode=display">
  \sum_{i=1}^C y_i = 1</script><p>  例如</p>
<script type="math/tex; mode=display">
  y_i = \frac{z_i}{\sum_j z_j}</script><p>  但是分式求导异常麻烦，故采用<code>Softmax</code>函数(<a href="https://louishsu.xyz/2018/10/18/softmax%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">详情</a>)作为输出结点的激活函数，该函数求导结果比较简洁，且可利用输出计算导数，计算量减少。</p>
<script type="math/tex; mode=display">
  Softmax(x) = \frac
              {1}
              {\sum_{k=1}^K exp(x_k)}
              \left[
                  \begin{matrix}
                      exp(x_1)\\
                      exp(x_2)\\
                      ...\\
                      exp(x_K)
                  \end{matrix}
              \right]</script></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><ul>
<li><p>回归问题<br>  常见的用于回归问题的损失函数为<code>MSE</code>，即</p>
<script type="math/tex; mode=display">
  L(y, \hat{y}) = \frac{1}{2M} \sum_{i=1}^M (\hat{y}^{(i)} - y^{(i)})^2</script></li>
<li><p>分类问题<br>  一般采用交叉熵作为损失函数，如下</p>
<script type="math/tex; mode=display">
  L(\hat{y}, y) = - \frac{1}{M} \sum_{i=1}^M 1\{y^{(i)}_j=k\} 
  \log (\hat{y}^{(i)}_j)</script><script type="math/tex; mode=display">
  1\{y^{(i)}_j=k\} = 
      \begin{cases}
          1 & y^{(i)}_j = k \\
          0 & y^{(i)}_j \neq k 
      \end{cases}　j = 1, ..., N</script><p>  或者</p>
<script type="math/tex; mode=display">
  L(\hat{y}, y) = - \frac{1}{M} \sum_{i=1}^M 
  y^{(i)T} \log (\hat{y}^{(i)})</script><p>  其中$y^{(i)}, \hat{y}^{(i)}$均表示向量，采用<code>one-hot</code>编码。</p>
</li>
</ul>
<h1 id="梯度推导"><a href="#梯度推导" class="headerlink" title="梯度推导"></a>梯度推导</h1><p>以上内容网上资料一大堆，进入重点，反向传播时的梯度推导，给出网络结构如下。</p>
<ul>
<li>回归与分类在输出层有所区别；</li>
<li>各层激活函数的输入变量以$z^{(l)}$表示，输出变量均以$x^{(l)}$表示；</li>
<li>$W^{(l)}$表示从第$l$层到第$(l+1)$层的权值矩阵，则$w^{(l)}_{ij}$表示第$l$层第$j$个神经元到$(l+1)$层第$i$个神经元的连接权值；</li>
<li>$b^{(l)}$表示第$l$层到第$(l+1)$层的偏置，则$b^{(l)}_i$表示到第$(l+1)$层第$i$个神经元的偏置值；</li>
<li>各层变量维度推广为输入$d_{i}$，中间层$d_{h}$，输出层$d_{o}$；</li>
<li>全连接，部分线条已省略，激活函数已省略；</li>
</ul>
<p><img src="/2018/10/20/Feedforward-Neural-Network/fnn.jpg" alt="FNN"></p>
<p>则各层参数矩阵为</p>
<script type="math/tex; mode=display">
W^{(1)} = \left[
        \begin{matrix}
            w^{(1)}_{11} & ... & w^{(1)}_{1d_i} \\
            ... & ... & ... \\
            w^{(1)}_{d_h1} & ... & w^{(1)}_{d_hd_i}
        \end{matrix}
\right]　
b^{(1)} = \left[
        \begin{matrix}
            b^{(1)}_{1} \\
            ... \\
            b^{(1)}_{d_h}
        \end{matrix}
\right]</script><script type="math/tex; mode=display">
W^{(2)} = \left[
        \begin{matrix}
            w^{(2)}_{11} & ... & w^{(2)}_{1d_h} \\
            ... & ... & ... \\
            w^{(2)}_{d_o1} & ... & w^{(2)}_{d_od_h}
        \end{matrix}
\right]　
b^{(2)} = \left[
        \begin{matrix}
            b^{(2)}_{1} \\
            ... \\
            b^{(2)}_{d_o}
        \end{matrix}
\right]</script><p>有</p>
<script type="math/tex; mode=display">
Z^{(2)} = W^{(1)} X^{(1)} + b^{(1)}</script><script type="math/tex; mode=display">
X^{(2)} = \sigma_1 (Z^{(2)})</script><script type="math/tex; mode=display">
Z^{(3)} = W^{(2)} X^{(2)} + b^{(2)}</script><script type="math/tex; mode=display">
X^{(3)} = \sigma_2 (Z^{(3)})</script><script type="math/tex; mode=display">
X^{(1)} = X　\hat{Y} = X^{(3)}</script><h2 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h2><p>损失函数采用<code>MSE</code>，即</p>
<script type="math/tex; mode=display">
L(Y, \hat{Y}) = \frac{1}{M} \sum_{i=1}^M L(Y^{(i)}, \hat{Y}^{(i)})</script><script type="math/tex; mode=display">
L(Y^{(i)}, \hat{Y}^{(i)}) 
= \frac{1}{2} || \hat{Y}^{(i)} - Y^{(i)} ||_2^2
= \frac{1}{2} 
\sum_{d_2=1}^{d_o}
(\hat{y}^{(i)}_{d_2} - y^{(i)}_{d_2})^2</script><p>下面推导单个样本的损失函数的梯度，该批数据的梯度为均值。</p>
<blockquote>
<p>省略样本标记<code>$^{(i)}$</code></p>
</blockquote>
<ul>
<li><p>隐含层到输出层</p>
<ul>
<li><p>对权值矩阵的梯度</p>
<script type="math/tex; mode=display">
  \frac{∂L}{∂w^{(2)}_{ij}}
  = \frac{∂}{∂w^{(2)}_{ij}} \frac{1}{2} \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2})^2</script><script type="math/tex; mode=display">
  = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \frac{∂}{∂w^{(2)}_{ij}} \hat{y}_{d_2} \tag{1}</script><p>  其中</p>
<script type="math/tex; mode=display">
  \begin{cases}
      \hat{y}_{d_2} = \sigma_2 (z_{d_2}^{(3)}) \\
      z_{d_2}^{(3)} = \sum_{d_1=1}^{d_h} w^{(2)}_{d_2 d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2}
  \end{cases}</script><p>  且</p>
<script type="math/tex; mode=display">
  \frac{∂}{∂w^{(2)}_{ij}} \hat{y}_{d_2}
  = \sigma_2' (z_{d_2}^{(3)}) \frac{∂z_{d_2}^{(3)}}{∂w^{(2)}_{ij}} \tag{2}</script><script type="math/tex; mode=display">
  \frac{∂}{∂w^{(2)}_{ij}} z_{d_2}^{(3)} = 
      \begin{cases}
          x^{(2)}_{d_1} & d_1 = j, d_2 = i \\
          0 & otherwise
      \end{cases} \tag{3}</script><p>  $(3)$代入$(2)$，再代入$(1)$可得到</p>
<script type="math/tex; mode=display">
  \frac{∂L}{∂w^{(2)}_{ij}}
  = (\hat{y}_{d_2} - y_{d_2}) \sigma_2' (z_{d_2}^{(3)}) x^{(2)}_{d_1} | _{d_1=j, d_2=i}
  = (\hat{y}_{i} - y_{i}) \sigma_2' (z_{i}^{(3)}) x^{(2)}_{j} \tag{*1}</script></li>
<li><p>对偏置矩阵的梯度</p>
<script type="math/tex; mode=display">
  \frac{∂L}{∂b^{(2)}_i}
  = \frac{∂}{∂b^{(2)}_i} \frac{1}{2} \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2})^2</script><script type="math/tex; mode=display">
  = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \frac{∂}{∂b^{(2)}_i} \hat{y}_{d_2} \tag{4}</script><p>  其中</p>
<script type="math/tex; mode=display">
  \begin{cases}
      \hat{y}_{d_2} = \sigma_2 (z_{d_2}^{(3)}) \\
      z_{d_2}^{(3)} = \sum_{d_1=1}^{d_h} w^{(2)}_{d_2 d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2}
  \end{cases}</script><p>  有</p>
<script type="math/tex; mode=display">
  \frac{∂}{∂b^{(2)}_i} z_{d_2}^{(3)} = 
      \begin{cases}
          1 &  d_2 = i \\
          0 & otherwise
      \end{cases} \tag{5}</script><p>  所以</p>
<script type="math/tex; mode=display">
  \frac{∂L}{∂b^{(2)}_i} = (\hat{y}_{d_2} - y_{d_2}) \sigma_2' (z_{d_2}^{(3)}) | _{d_2=i}
  = (\hat{y}_{i} - y_{i}) \sigma_2' (z_i^{(3)}) \tag{*2}</script></li>
</ul>
</li>
<li><p>输入层到隐含层</p>
<ul>
<li><p>对权值矩阵的梯度</p>
<script type="math/tex; mode=display">
  \frac{∂L}{∂w^{(1)}_{ij}}
  = \frac{∂}{∂w^{(1)}_{ij}} \frac{1}{2} \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2})^2</script><script type="math/tex; mode=display">
  = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \frac{∂}{∂w^{(1)}_{ij}} \hat{y}_{d_2} \tag{6}</script><p>  其中</p>
<script type="math/tex; mode=display">
  \begin{cases}
      \hat{y}_{d_2} = \sigma_2 (z_{d_2}^{(3)}) \\
      z_{d_2}^{(3)} = \sum_{d_1=1}^{d_h} w^{(2)}_{d_2 d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2} \\
      x^{(2)}_{d_1} = \sigma_1 (z_{d_1}^{(2)}) \\
      z_{d_1}^{(2)} = \sum_{d_0=1}^{d_i} w^{(1)}_{d_1 d_0} x^{(1)}_{d_0} + b^{(1)}_{d_1}
  \end{cases}</script><p>  故</p>
<script type="math/tex; mode=display">
  \frac{∂}{∂w^{(1)}_{ij}} \hat{y}_{d_2}
  = \frac{∂\hat{y}_{d_2}}{∂z_{d_2}^{(3)}} 
      \frac{∂z_{d_2}^{(3)}}{∂w^{(1)}_{ij}} \tag{7}</script><p>  其中</p>
<script type="math/tex; mode=display">
  \frac{∂\hat{y}_{d_2}}{∂z_{d_2}^{(3)}} 
  = \sigma_2' (z_{d_2}^{(3)})  \tag{8}</script><script type="math/tex; mode=display">
  \frac{∂z_{d_2}^{(3)}}{∂w^{(1)}_{ij}} 
  = \sum_{d1=1}^{d_h} w^{(2)}_{d_2 d_1} \frac{∂x^{(2)}_{d_1}}{∂w^{(1)}_{ij}} \tag{9}</script><script type="math/tex; mode=display">
  \frac{∂x^{(2)}_{d_1}}{∂w^{(1)}_{ij}}
  = \frac{∂x^{(2)}_{d_1}}{∂z_{d_1}^{(2)}}
      \frac{∂z_{d_1}^{(2)}}{∂w^{(1)}_{ij}}  \tag{10}</script><p>  而其中</p>
<script type="math/tex; mode=display">
  \frac{∂x^{(2)}_{d_1}}{∂z_{d_1}^{(2)}} = x^{(2)}_{d_1} = \sigma_1' (z_{d_1}^{(2)}) \tag{11}</script><script type="math/tex; mode=display">
  \frac{∂z_{d_1}^{(2)}}{∂w^{(1)}_{ij}} = 
  \begin{cases}
      x^{(1)}_{d_0} & d_1 = i, d_0 = j\\
      0 & otherwise
  \end{cases} \tag{12}</script><p>  $(11),(12)$代入$(10)$得到</p>
<script type="math/tex; mode=display">
  \frac{∂x^{(2)}_{d_1}}{∂w^{(1)}_{ij}} = 
  \sigma_1' (z_{d_1}^{(2)})
      x^{(1)}_{d_0} | _{d_1 = i, d_0 = j} \tag{13}</script><p>  $(13)$代回$(9)$，有</p>
<script type="math/tex; mode=display">
  \frac{∂z_{d_2}^{(3)}}{∂w^{(1)}_{ij}} 
  = \sum_{d1=1}^{d_h} 
  \left[
      w^{(2)}_{d_2 d_1} 
      \sigma_1' (z_{d_1}^{(2)})
      x^{(1)}_{d_0}
  \right] | _{d_1 = i, d_0 = j}</script><script type="math/tex; mode=display">
  = w^{(2)}_{d_2 i} 
      \sigma_1' (z_i^{(2)})
      x^{(1)}_j \tag{14}</script><p>  将$(8),(14)$代入$(7)$得到</p>
<script type="math/tex; mode=display">
  \frac{∂}{∂w^{(1)}_{ij}} \hat{y}_{d_2}
  = \sigma_2' (z_{d_2}^{(3)}) 
       w^{(2)}_{d_2 i} 
      \sigma_1' (z_i^{(2)})
      x^{(1)}_j \tag{15}</script><p>  $(15)$代入$(6)$有</p>
<script type="math/tex; mode=display">
  \frac{∂L}{∂w^{(1)}_{ij}}
  = \sum_{d_2=1}^{d_o} 
      (\hat{y}_{d_2} - y_{d_2})      
      \sigma_2' (z_{d_2}^{(3)}) 
       w^{(2)}_{d_2 i} 
      \sigma_1' (z_i^{(2)})
      x^{(1)}_j \tag{*3}</script></li>
<li><p>对偏置矩阵的梯度</p>
<script type="math/tex; mode=display">
  \frac{∂L}{∂b^{(1)}_i}
  = \frac{∂}{∂b^{(1)}_i} \frac{1}{2} \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2})^2</script><script type="math/tex; mode=display">
  = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \frac{∂}{∂b^{(1)}_i} \hat{y}_{d_2} \tag{16}</script><p>  同理可得</p>
<script type="math/tex; mode=display">
  \frac{∂}{∂b^{(1)}_i} \hat{y}_{d_2}
  = \sigma_2' (z_{d_2}^{(3)}) 
       w^{(2)}_{d_2 i} 
      \sigma_1' (z_i^{(2)})  \tag{17}</script><p>  所以</p>
<script type="math/tex; mode=display">
  \frac{∂L}{∂b^{(1)}_i} =
  \sum_{d_2=1}^{d_o} 
      (\hat{y}_{d_2} - y_{d_2})      
      \sigma_2' (z_{d_2}^{(3)}) 
       w^{(2)}_{d_2 i} 
      \sigma_1' (z_i^{(2)}) \tag{*4}</script></li>
</ul>
</li>
</ul>
<p>综上所述</p>
<script type="math/tex; mode=display">
\frac{∂L}{∂w^{(2)}_{ij}}
= (\hat{y}_{i} - y_{i}) 
    \sigma_2' (z_{i}^{(3)}) x^{(2)}_{j}</script><script type="math/tex; mode=display">
\frac{∂L}{∂b^{(2)}_i} 
= (\hat{y}_{i} - y_{i}) 
    \sigma_2' (z_i^{(3)})</script><script type="math/tex; mode=display">
\frac{∂L}{∂w^{(1)}_{ij}}
= \sum_{d_2=1}^{d_o} 
    (\hat{y}_{d_2} - y_{d_2})      
    \sigma_2' (z_{d_2}^{(3)}) 
    w^{(2)}_{d_2 i} 
    \sigma_1' (z_i^{(2)})
    x^{(1)}_j</script><script type="math/tex; mode=display">
\frac{∂L}{∂b^{(1)}_i} 
= \sum_{d_2=1}^{d_o} 
    (\hat{y}_{d_2} - y_{d_2})      
    \sigma_2' (z_{d_2}^{(3)}) 
    w^{(2)}_{d_2 i} 
    \sigma_1' (z_i^{(2)})</script><p>令</p>
<script type="math/tex; mode=display">
\begin{cases}
    \delta^{(2)}_i 
    = (\hat{y}_{i} - y_{i}) 
        \sigma_2' (z_i^{(3)}) \\
    \delta^{(1)}_i 
    = \sum_{d_2=1}^{d_o} 
        \delta^{(2)}_{d_2} 
        w^{(2)}_{d_2 i} 
        \sigma_1' (z_i^{(2)})
\end{cases}</script><p>有</p>
<script type="math/tex; mode=display">
\begin{cases}
    \frac{∂L}{∂w^{(2)}_{ij}} = \delta^{(2)}_i x^{(2)}_{j}\\
    \frac{∂L}{∂b^{(2)}_i}    = \delta^{(2)}_i\\
    \frac{∂L}{∂w^{(1)}_{ij}} = \delta^{(1)}_i x^{(1)}_j\\
    \frac{∂L}{∂b^{(1)}_i}    = \delta^{(1)}_i
\end{cases}</script><p>至此推导完毕。</p>
<blockquote>
<p>当隐藏层采用<code>Sigmoid</code>函数，输出层采用线性单元，可得到</p>
<script type="math/tex; mode=display">
\sigma_1' (z_i^{(2)}) 
= \sigma_1 (z_i^{(2)}) 
    \left[1 - \sigma_1 (z_i^{(2)}) \right]
= x_i^{(2)} (1 - x_i^{(2)})</script><script type="math/tex; mode=display">
\sigma_2' (z_i^{(3)}) = z_i^{(3)}</script><p>此时</p>
<script type="math/tex; mode=display">
\begin{cases}
    \frac{∂L}{∂w^{(2)}_{ij}} = (\hat{y}_{i} - y_{i}) z_i^{(3)} x^{(2)}_{j} \\
    \frac{∂L}{∂b^{(2)}_i}    = (\hat{y}_{i} - y_{i}) z_i^{(3)} \\
    \frac{∂L}{∂w^{(1)}_{ij}} = \sum_{d_2=1}^{d_o} \delta^{(2)}_{d_2} w^{(2)}_{d_2 i} z_i^{(2)} x^{(1)}_j \\
    \frac{∂L}{∂b^{(1)}_i}    = \sum_{d_2=1}^{d_o} \delta^{(2)}_{d_2} w^{(2)}_{d_2 i} z_i^{(2)}
\end{cases}</script><p>可以看到，计算梯度时使用的数据在上一次前向传播时已计算得，故可减少计算量。</p>
</blockquote>
<h2 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h2><p>损失函数采用<code>Cross Entropy</code>，即</p>
<script type="math/tex; mode=display">
L(\hat{y}, y) = 
\frac{1}{M} \sum_{i=1}^M L(\hat{y}^{(i)}, y^{(i)})</script><script type="math/tex; mode=display">
L(\hat{y}^{(i)}, y^{(i)}) 
= - y^{(i)T} \log (\hat{y}^{(i)})</script><p>上式中，$y^{(i)}, \hat{y}^{(i)}$均为列向量，且$y^{(i)}$表示<code>one-hot</code>编码后的标签向量，也可写作</p>
<script type="math/tex; mode=display">
L(\hat{y}^{(i)}, y^{(i)})
= - \log \hat{y}^{(i)}_{y^{(i)}}</script><ul>
<li>由该式可以看出，若输出层激活函数采用<code>Sigmoid</code>作为激活函数，则隐藏层——输出层之间权值矩阵$W^{(2)}$只会更新$w^{(i)}_{y^{(i)} d_1}, d_1 = 1, …, d_h$；</li>
<li>一般采用<code>SoftMax</code>作为输出层激活函数，<code>Sigmoid</code>下面不作推导。</li>
</ul>
<blockquote>
<p>关于<code>SoftMax</code>的梯度，移步<a href="https://louishsu.xyz/2018/10/18/softmax%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">SoftMax Regression</a>中查看详细推导过程，这里直接给出结论。<br>对于</p>
<script type="math/tex; mode=display">
S(x) = \frac {1} {\sum_{k=1}^K exp(x_k)} \left[ \begin{matrix} exp(x_1)\\ exp(x_2)\\ ...\\ exp(x_K) \end{matrix} \right]</script><p>其梯度为</p>
<script type="math/tex; mode=display">
\frac{∂S(x)}{∂x_i}_{K×1} =  \left[ \begin{matrix} 0\\ ...\\ p_i\\ ...\\ 0 \end{matrix} \right] -  \left[ \begin{matrix} p_i p_1\\ ...\\ p_i^2\\ ...\\ p_i p_K \end{matrix} \right]
= \left( \left[ \begin{matrix}  0 \\ ...\\ 1\\ ...\\ 0 \end{matrix} \right] - p \right)p_i</script><p>省略样本标记<code>$^{(i)}$</code></p>
</blockquote>
<ul>
<li><p>隐含层到输出层</p>
<ul>
<li><p>对权值矩阵的梯度</p>
<script type="math/tex; mode=display">
  \frac{∂L}{∂w^{(2)}_{ij}}
  = - \frac{∂}{∂w^{(2)}_{ij}} \log \hat{y}_{y}
  = - \frac{1}{\hat{y}_y} 
      \frac{∂\hat{y}_{y}}{∂w^{(2)}_{ij}} \tag{18}</script><p>  其中$\hat{y}_{y}$与$z^{(3)}_{d_2}(d_2 = 1, …, d_o) $均有联系，故</p>
<script type="math/tex; mode=display">
  \frac{∂\hat{y}_{y}}{∂w^{(2)}_{ij}}
  = \sum_{d2=1}^{d_o} 
      \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}} 
      \frac{∂z^{(3)}_{d_2}}{∂w^{(2)}_{ij}} \tag{19}</script><p>  而</p>
<script type="math/tex; mode=display">
  \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}}
  = \begin{cases}
      \hat{y}_{y} (1 - \hat{y}_{d_2}) & d_2 = y \\
      - \hat{y}_{y} \hat{y}_{d_2} & otherwise
  \end{cases}</script><script type="math/tex; mode=display">
  \frac{∂z^{(3)}_{d_2}}{∂w^{(2)}_{ij}}
  = \begin{cases}
      x^{(2)}_{d_1} & i = d_2, j = d_1 \\
      0 & otherwise
  \end{cases}</script><blockquote>
<p>$z^{(3)}_{d_2} = \sum_{d_1=0}^{d_h} w^{(2)}_{d_2d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2}$</p>
</blockquote>
<p>  代回$(19)$，再带回$(18)$，有</p>
<script type="math/tex; mode=display">
  \frac{∂L}{∂w^{(2)}_{ij}}
  = - \frac{1}{\hat{y}_{y}} 
      \sum_{d_2=1}^{d_o} 
      \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}} 
      x^{(2)}_{d_1} | _{d_2=i, d_1=j}</script><script type="math/tex; mode=display">
  = \begin{cases}
      - \frac{1}{\hat{y}_{y}} \hat{y}_{y} (1 - \hat{y}_i) x^{(2)}_j & i = y \\
      - \frac{1}{\hat{y}_{y}} (- \hat{y}_{y} \hat{y}_i) x^{(2)}_j & otherwise
  \end{cases}</script><script type="math/tex; mode=display">
  = \begin{cases}
      (\hat{y}_i - 1) x^{(2)}_j & i = y \\
      \hat{y}_i x^{(2)}_j & otherwise
  \end{cases}</script><p>  即</p>
<script type="math/tex; mode=display">
  \frac{∂L}{∂w^{(2)}_{ij}}
  = (\hat{y}_i - y_i) x^{(2)}_j \tag{*5}</script></li>
<li><p>对偏置矩阵的梯度</p>
<script type="math/tex; mode=display">
  \frac{∂L}{∂b^{(2)}_i}
  = \hat{y}_i - y_i \tag{*6}</script></li>
</ul>
</li>
<li><p>输入层到隐含层</p>
<ul>
<li><p>对权值矩阵的梯度</p>
<script type="math/tex; mode=display">
  \frac{∂L}{∂w^{(1)}_{ij}}
  = - \frac{∂}{∂w^{(1)}_{ij}} \log \hat{y}_{y}
  = - \frac{1}{\hat{y}_{y}} 
      \frac{∂\hat{y}_{y}}{∂w^{(1)}_{ij}} \tag{20}</script><p>  其中</p>
<script type="math/tex; mode=display">
  \frac{∂\hat{y}_{y}}{∂w^{(1)}_{ij}}
  = \sum_{d_2=1}^{d_o} 
      \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}} 
      \frac{∂z^{(3)}_{d_2}}{∂w^{(1)}_{ij}} \tag{21}</script><p>  $\frac{∂z^{(3)}_{d_2}}{∂w^{(1)}_{ij}}$部分与回归相同，有</p>
<script type="math/tex; mode=display">
  \frac{∂z_{d_2}^{(3)}}{∂w^{(1)}_{ij}}
  = w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)}) x^{(1)}_j</script><p>  由上面分析可得</p>
<script type="math/tex; mode=display">
  \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}}
  = \begin{cases}
      \hat{y}_{y} (1 - \hat{y}_{d_2}) & d_2 = y \\
      - \hat{y}_{y} \hat{y}_{d_2} & otherwise
  \end{cases}</script><p>  故代回$(20)$可得到</p>
<script type="math/tex; mode=display">
  \frac{∂L}{∂w^{(1)}_{ij}}
  = - \frac{1}{\hat{y}_{y}}
      \sum_{d_2=1}^{d_o} 
      \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}} 
      \frac{∂z^{(3)}_{d_2}}{∂w^{(1)}_{ij}}</script><script type="math/tex; mode=display">
  = - \frac{1}{\hat{y}_{y}}
      \sum_{d_2=1}^{d_o} 
      \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}} 
      w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)}) x^{(1)}_j</script><script type="math/tex; mode=display">
  = \left[ 
      \sum_{d_2=1, d_2 \neq y}^{d_o} \hat{y}_{d_2} w^{(2)}_{d_2 i} + 
      (\hat{y}_y - 1) w^{(2)}_{y i} 
  \right] \sigma_1' (z_i^{(2)}) x^{(1)}_j</script><script type="math/tex; mode=display">
  = \left[ 
      \sum_{d_2=1}^{d_o} \hat{y}_{d_2} w^{(2)}_{d_2 i} - 
      w^{(2)}_{y i}
  \right] \sigma_1' (z_i^{(2)}) x^{(1)}_j \tag{*7}</script></li>
<li><p>对偏置矩阵的梯度</p>
<script type="math/tex; mode=display">
  \frac{∂L}{∂b^{(1)}_i}
  = \left[ 
      \sum_{d_2=1}^{d_o} \hat{y}_{d_2} w^{(2)}_{d_2 i} - 
      w^{(2)}_{y i}
  \right] \sigma_1' (z_i^{(2)}) \tag{*8}</script></li>
</ul>
</li>
</ul>
<p>至此推导完毕。</p>
<blockquote>
<p>这个推导，仅供参考</p>
</blockquote>
<h1 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h1><p>和其他算法一样，前馈神经网络也存在过拟合的问题，解决方法有以下几种</p>
<ul>
<li><p>正则化<br>  与线性回归类似，神经网络也可以加入范数惩罚项，以下$C$表示普通的损失函数，$\lambda$为惩罚系数，$n$为样本数目，$w$表示权值参数。</p>
<ul>
<li><code>L1</code>正则化<br>  惩罚项为网络所有权值的绝对值之和。<script type="math/tex; mode=display">
  C = C_0 + \frac{\lambda}{n} \sum_w |w|</script></li>
<li><code>L2</code>正则化<br>  又称权值衰减<code>weights decay</code>，惩罚项为网络所有权值的平方和。<script type="math/tex; mode=display">
  C = C_0 + \frac{\lambda}{2n} \sum_w w^2</script></li>
</ul>
</li>
<li><p>Dropout<br>  以概率大小为<code>p</code>使部分神经元输出值直接为0，如此可以使反向传播时相关权值系数不做更新，只有被保留下来的权值和偏置值会被更新。<br>  <img src="/2018/10/20/Feedforward-Neural-Network/dropout_1.png" alt="dropout_1"><br>  <img src="/2018/10/20/Feedforward-Neural-Network/dropout_2.png" alt="dropout_2"></p>
</li>
<li><p>增加训练数据大小<br>  可在原数据上加以变换或噪声，图像的扩增方法可查看<a href="https://louishsu.xyz/2018/11/02/%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E9%9B%86%E6%89%A9%E5%A2%9E-Augment-%E6%96%B9%E6%B3%95/" target="_blank" rel="noopener">图像数据集扩增</a>。</p>
</li>
</ul>
<h1 id="程序"><a href="#程序" class="headerlink" title="程序"></a>程序</h1><p><a href="https://github.com/isLouisHsu/Python-Examples-for-PyTorch-Tutorial/blob/master/NeuralNetwork_ANN_MNIST.py" target="_blank" rel="noopener">@Github: Code of Neural Network</a></p>
<p>使用<code>PyTorch</code>实现神经网络，以下为模型定义<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class AnnNet(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(AnnNet, self).__init__()</span><br><span class="line">        self.input_size = 28 * 28</span><br><span class="line">        self.hidden_size = 100</span><br><span class="line">        self.output_size = 10</span><br><span class="line">        self.fc1 = nn.Linear(self.input_size,  self.hidden_size)    # input   - hidden</span><br><span class="line">        self.fc2 = nn.Linear(self.hidden_size, self.output_size )   # hidden  - output</span><br><span class="line">        # self.activate = nn.Sigmoid()  # 参数更新非常慢，特别是层数多时</span><br><span class="line">        self.activate = nn.ReLU()       # 事实证明ReLU作为激活函数更加合适</span><br><span class="line">        self.softmax = nn.Softmax()</span><br><span class="line">    def forward(self, X):</span><br><span class="line">        h = self.activate(self.fc1(X))</span><br><span class="line">        y_pred = self.softmax(self.fc2(h))</span><br><span class="line">        return y_pred</span><br></pre></td></tr></table></figure></p>

        </div>
        <div class="post-tool">
            <a class="btn-thumbs-up" href="javascript:void(0);" data-cid="52" title="95">
                <i class="fa fa-thumbs-up" aria-hidden="true"></i> Donate
            </a>
        </div>
        
        <div class="post-tags">Tags：
            
        </div>
        
    </article>
    
    <p style="text-align: center">This article just represents my own viewpoint. If there is something wrong, please correct me.</p>
    
    

    

</div>
<script src="/js/busuanzi.pure.mini.js"></script>


        </div><!-- end #main-->
    </div><!-- end #body -->
    <footer class="footer">
    <div class="footer-inner" style="text-align: center">
        <p>
            <a href="/about" title="About">About</a>&nbsp;&nbsp<em>·</em>&nbsp;&nbsp
            <!-- 自定义链接 -->
            <a href="/help" title="Help">Help</a>&nbsp;&nbsp<em>·</em>&nbsp;&nbsp
            <a href="/links" title="Links">Links</a>&nbsp;&nbsp<em>·</em>&nbsp;&nbsp
            <a href="/sitemap.xml" title="SiteMap">SiteMap</a>
        </p>
        <p>
            Has been established&nbsp<a href="/timeline" id="siteBuildingTime"></a>&nbspDays，<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="licence">Based on Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)</a><br>
            ©2017-<span id="cpYear"></span> Based on&nbsp<a href="http://hexo.io" target="_blank" rel="nofollow">Hexo</a>
            ，Theme by&nbsp&nbsp<a href="https://github.com/tangkunyin/hexo-theme-jsimple" target="_blank" rel="bookmark">JSimple</a>
            ，Author&nbsp<a href="https://louishsu.xyz/" target="_blank" rel="friend">Louis Hsu</a>
            ，Hosted by <a href="https://pages.github.com/" target="_blank" rel="nofollow">GitHub Pages</a>
        </p>
    </div>
</footer>
<script src="/js/SimpleCore.js"></script>

</div>
<!-- search pop -->
<div class="popup search-popup local-search-popup">
    <div class="local-search-header clearfix">
        <span class="search-icon">
            <i class="fa fa-search"></i>
        </span>
        <span class="popup-btn-close">
            <i class="fa fa-times-circle"></i>
        </span>
        <div class="local-search-input-wrapper">
            <input id="local-search-input" spellcheck="false" type="text" autocomplete="off" placeholder="Input query keywords here...">
        </div>
    </div>
    <div id="local-search-result"></div>
</div>
<div class="fixed-btn">
    <a class="btn-gotop" href="javascript:"> <i class="fa fa-angle-up"></i></a>
</div>
<script>
    $(function () {
        var jsi_config = {
            buildingTime: '10/20/2018',
            current: $('.post-tags').length > 0 ? 'post' : 'archive',
            snsQRCode: '/images/sns-qrcode.png',
            donateImg: '/images/donate-qr.png',
            localSearch: { dbPath: '' },
            readMode: 'day'
        };
        
            jsi_config.localSearch = {
                dbPath: '/search.json',
                trigger: 'auto',
                topN: '1',
                unescape: 'false'
            }
        
        SimpleCore.init(jsi_config);
        
    });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
