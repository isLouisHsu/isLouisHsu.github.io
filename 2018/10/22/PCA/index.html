<!DOCTYPE HTML>
<html class="no-js" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
    <!--[if lte IE 9]>
<meta http-equiv="refresh" content="0;url=http://yoursite.com/warn.html">
<![endif]-->
<meta charset="utf-8">
<meta http-equiv="X-DNS-Prefetch-Control" content="on">
<link rel="dns-prefetch" href="http://yoursite.com">
<link rel="dns-prefetch" href="//www.google-analytics.com">
<link rel="prefetch" href="http://yoursite.com">
<link rel="prefetch" href="//www.google-analytics.com">


<link rel="prerender" href="http://yoursite.com">

<meta http-equiv="X-UA-Compatible" content="IE=Edge">
<meta name="renderer" content="webkit">
<meta name="viewport" content="width=device-width, initial-scale=1.0,user-scalable=no">
<meta http-equiv="mobile-agent" content="format=html5; url=http://yoursite.com">
<meta name="author" content="Louis Hsu">
<link rel="stylesheet" href="/css/JSimple.css">

<link rel="shortcut icon" href="/images/favicon.png">


<title>pca - LOUIS&#39; BLOG</title>

<meta name="keywords" content="">

<meta name="description " content="Inside! Insane!">

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                processEscapes: true
            }
        });
    </script>


    

    

</head>
<body>
<div id="nav">
    <nav class="nav-menu">
        <a class="site-name current" href="/" title="彬">彬</a>
        <a class="site-index current" href="/"><i class="fa fa-home"></i><span>首页</span></a>
        <a href="/archives" title="归档"><i class="fa fa-archives"></i><span>归档</span></a>
        <a href="/tags" title="标签"><i class="fa fa-tags"></i><span>标签</span></a>
        <!-- custom single page of menus -->
        
        
        <a href="/help" title="帮助">
            <i class="fa fa-question-circle"></i>
            <span>帮助</span>
        </a>
        
    </nav>
</div>

<div class="nav-user">
    <a class="btn-search" href="#"><i class="fa fa-search"></i></a>
    <a class="btn-read-mode" href="#"><i class="fa fa-sun-o"></i></a>
    <a class="btn-sns-qr" href="javascript:"><i class="fa fa-telegram"></i></a>
</div>

<div id="wrapper" class="clearfix">
    <div id="body">
        <div class="main" id="main">
            <div id="cover">
    <div class="cover-img"></div>
    <div class="cover-info">
        
        <h1 class="cover-siteName">LOUIS&#39; BLOG</h1>
        <h3 class="cover-siteTitle">人生苦短，不如不管，继续任性</h3>
        <p class="cover-siteDesc">技术博客？</p>
        <div class="cover-sns">
            

        </div>
    </div>
</div>

            <div class="page-title">
    <ul>
        <li><a href="/">最近</a></li>
        
            
                <li class="">
                    <a href="/categories//" data-name="主页">主页</a>
                </li>
            
                <li class="">
                    <a href="/categories/Python" data-name="Python">Python</a>
                </li>
            
                <li class="">
                    <a href="/categories/Machine-Learning" data-name="机器学习">机器学习</a>
                </li>
            
                <li class="">
                    <a href="/categories/Deep-Learning" data-name="深度学习">深度学习</a>
                </li>
            
                <li class="">
                    <a href="/categories/冯唐" data-name="冯唐">冯唐</a>
                </li>
            
                <li class="">
                    <a href="/categories/Others" data-name="其他">其他</a>
                </li>
            
        
        <li class="page-search">
    <form id="search" class="search-form">
        <input type="text" readonly="readonly" id="local-search-input-tip" placeholder="读物检索~">
        <button type="button" disabled="disabled" class="search-form-submit"><i class="fa fa-search"></i></button>
    </form>
</li>

    </ul>
</div>
<div class="main-inner">
    <article class="post" itemscope="" itemtype="http://schema.org/BlogPosting">
        <div class="post-header">
            <div class="post-author clearfix">
                <a class="avatar fleft" href="https://louishsu.xyz/" target="_blank">
                    <img width="48" src="/images/favicon.png" alt="avatar">
                </a>
                <p><span class="label">作者</span>
                    <a href="https://louishsu.xyz/" target="_blank">徐耀彬</a>
                    <span title="最后编辑于&nbsp;2018-10-22">2018-10-22</span>
                </p>
                <p>有味道的程序员</p>
            </div>
            <h2 class="post-title">PCA</h2>
            <div class="post-meta">
                本文共计10409个字 |
                您是第&nbsp;<span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span>位看到它们的小伙伴
            </div>
        </div>
        <div class="post-content markdown-body">
            <h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p><code>PCA</code>全称<code>Principal Component Analysis</code>，即主成分分析，是一种常用的数据降维方法。它可以通过线性变换将原始数据变换为一组各维度线性无关的表示，以此来提取数据的主要线性分量。</p>
<h1 id="向量的投影"><a href="#向量的投影" class="headerlink" title="向量的投影"></a>向量的投影</h1><p>现有两个任意不共线向量$\vec{u}, \vec{v}$，将$\vec{u}$投射到$\vec{v}$上<br><img src="/2018/10/22/PCA/向量投影.jpg" alt="向量投影"></p>
<p>投影后，可以得到两个正交向量</p>
<script type="math/tex; mode=display">
\vec{u}' · (\vec{u} - \vec{u}') = 0</script><p>我们设</p>
<script type="math/tex; mode=display">
\vec{u}' = \mu \vec{v} \tag{1}</script><p>代入后有</p>
<script type="math/tex; mode=display">
\mu \vec{v} · (\vec{u} - \mu \vec{v}) = 0</script><p>引入矩阵运算，即</p>
<script type="math/tex; mode=display">
(\mu v)^T (u - \mu v) = 0</script><p>有</p>
<script type="math/tex; mode=display">
v^T u = \mu v^T v</script><p>则得到$u’$以$v$为基向量的坐标</p>
<script type="math/tex; mode=display">
\mu  = (v^T v)^{-1} v^T u \tag{2}</script><p>所以得到</p>
<script type="math/tex; mode=display">
u' = v (v^T v)^{-1} v^T u \tag{*}</script><blockquote>
<ul>
<li><p>坐标变换求解投影向量：$u’$可视作$u$经坐标变换$u’ = P u$得到，所以</p>
<script type="math/tex; mode=display">
P = v (v^T v)^{-1} v^T</script></li>
<li><p>推广至多个向量的投影，即得到</p>
<script type="math/tex; mode=display">
P = X (X^T X)^{-1} X^T</script><p>这与<a href="https://louishsu.xyz/2018/10/18/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">线性回归</a>中得到的结论一致。</p>
</li>
</ul>
</blockquote>
<p>实际上</p>
<script type="math/tex; mode=display">
u' = v (v^T v)^{-1} v^T u = \frac{v}{||v||} (\frac{v}{||v||})^T u</script><p>记单位向量$\frac{v}{||v||}$为$v_0$，得到</p>
<script type="math/tex; mode=display">
u' = v_0 v_0^T u</script><p>由几何关系，可以计算得投影后的长度为</p>
<script type="math/tex; mode=display">
d 
= ||u|| \cos \theta 
= ||u|| \frac{v^T u}{||u||||v||}
= v_0^T u</script><p>所以在向量投影中，$u^T v_0$表示以$v_0$为基向量的坐标。</p>
<h1 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h1><p>现在有$N$维数据集$D=\{x^{(1)}, x^{(2)}, …, x^{(M)}\}$，其中$x^{(i)} = \left[x^{(i)}_1, x^{(i)}_2, …, x^{(i)}_N\right]^T$，各维特征$D_{j}$间存在线性相关性，利用主成分分析可使</p>
<ul>
<li>数据维度降低；</li>
<li>提取主成分，且各成分间不相关。</li>
</ul>
<blockquote>
<p>说明</p>
<ul>
<li>由于选取的特征轴是正交的，所以计算结果线性无关；</li>
<li>提取了方差较大的几个特征，为主要线性分量。</li>
</ul>
</blockquote>
<p>以二维空间中的数据$x^{(i)} = \left[\begin{matrix}<br>    x^{(i)}_1 \\ x^{(i)}_2<br>\end{matrix}\right]$为例，维度可降至一维，如下图所示。<br><img src="/2018/10/22/PCA/PCA动态图.gif" alt="PCA动态图"></p>
<p>主轴可有无穷多种选择，那么问题就是<strong>如何选取最优的主轴</strong>。先给出<code>PCA</code>的计算步骤。</p>
<h2 id="计算步骤"><a href="#计算步骤" class="headerlink" title="计算步骤"></a>计算步骤</h2><p>输入的$M$个$N$维样本，有样本矩阵</p>
<script type="math/tex; mode=display">
X_{N×M} = \left[x^{(1)}, x^{(2)}, ..., x^{(M)} \right]
= \left[
    \begin{matrix}
        x^{(1)}_1 & x^{(2)}_1 & ... & x^{(M)}_1 \\
        x^{(1)}_2 & x^{(2)}_2 & ... & x^{(M)}_2 \\
        ... \\
        x^{(1)}_N & x^{(2)}_N & ... & x^{(M)}_N \\
    \end{matrix}
\right]</script><h3 id="投影"><a href="#投影" class="headerlink" title="投影"></a>投影</h3><ol>
<li><p>对每个维度(行)进行去均值化</p>
<script type="math/tex; mode=display">
X_j := X_j - \mu_j</script><p> 其中$\mu_j = \overline{X_j}$，$j = 1, 2, …, N$</p>
</li>
<li><p>求各维度间的协方差矩阵$\Sigma_{N×N}$</p>
<script type="math/tex; mode=display">\Sigma_{ij} = Cov(x_i, x_j)</script><p> 或</p>
<script type="math/tex; mode=display">
 \Sigma = \frac{1}{M} X X^T</script></li>
</ol>
<blockquote>
<p>注：</p>
<ol>
<li><script type="math/tex; mode=display">
X X^T = \left[
           \begin{matrix}
     \sum_{i=1}^M x^{(i)}_1 x^{(i)}_1 & 
     \sum_{i=1}^M x^{(i)}_1 x^{(i)}_2 &
     ... &
     \sum_{i=1}^M x^{(i)}_1 x^{(i)}_N \\
     \sum_{i=1}^M x^{(i)}_2 x^{(i)}_1 & 
     \sum_{i=1}^M x^{(i)}_2 x^{(i)}_2 &
     ... &
     \sum_{i=1}^M x^{(i)}_2 x^{(i)}_N \\
     ... &
     ... &
     ... &
     ... \\
     \sum_{i=1}^M x^{(i)}_N x^{(i)}_1 & 
     \sum_{i=1}^M x^{(i)}_N x^{(i)}_2 &
     ... &
     \sum_{i=1}^M x^{(i)}_N x^{(i)}_N
 \end{matrix}
\right]</script><script type="math/tex; mode=display">
= \sum_{i=1}^M \left[
           \begin{matrix}
         x^{(i)}_1 x^{(i)}_1 & 
         x^{(i)}_1 x^{(i)}_2 &
         ... &
         x^{(i)}_1 x^{(i)}_N \\
         x^{(i)}_2 x^{(i)}_1 & 
         x^{(i)}_2 x^{(i)}_2 &
         ... &
         x^{(i)}_2 x^{(i)}_N \\
         ... &
         ... &
         ... &
         ... \\
         x^{(i)}_N x^{(i)}_1 & 
         x^{(i)}_N x^{(i)}_2 &
         ... &
         x^{(i)}_N x^{(i)}_N
 \end{matrix}
\right]</script><script type="math/tex; mode=display">
= \sum_{i=1}^M x^{(i)} x^{(i)T}</script></li>
<li><p>协方差定义式</p>
<script type="math/tex; mode=display">
   Cov(x,y)≝\frac{1}{n-1} ∑_{i=1}^n (x_i−\overline{x})^T(y_i−\overline{y})</script><p>其中$x=[x_1, x_2, …, x_n]^T, y=[y_1, y_2, …, y_n]^T$</p>
</li>
</ol>
</blockquote>
<ol>
<li>求协方差矩阵$\Sigma$的特征值$λ_i$及其对应特征向量$α_i$，$i=1, …, N$；</li>
<li><p>按照特征值从大到小排列特征对$(λ_i,α_i)$，选取$K$个最大特征值对应的特征向量作为降维后的主轴$ \beta_1, \beta_2, …, \beta_K $，其中$\beta_k$为单位向量</p>
<script type="math/tex; mode=display">
\beta_k = \left[ \beta_{k1}, \beta_{k2}, ..., \beta_{kN} \right]^T</script><p>记</p>
<script type="math/tex; mode=display">
B_{N×K} = \left[ \beta_1, \beta_2, ..., \beta_K \right]</script><p>$K$的选取方法有如下两种：</p>
<ul>
<li>指定选取$K$个主轴</li>
<li>保留$99\%$的方差<script type="math/tex; mode=display">\frac{\sum_{i=1}^K \lambda_i}{\sum_{j=1}^N \lambda_j} > 0.99</script></li>
</ul>
</li>
</ol>
<ol>
<li><p>将样本点投射到$K$维坐标系上<br> 样本$X^{(i)}$投射到主成分轴$\beta_k$上，其坐标表示为向量，为</p>
<script type="math/tex; mode=display">
 S^{(i)}_k = X^{(i)T}\beta_k</script><blockquote>
<p>注意此时的基座标为$\beta_k$，或者说$X’^{(i)} = S^{(i)} \frac{\beta_k}{||\beta_k||}$</p>
</blockquote>
<p> 所有样本在主轴$\beta_k$上的投影坐标即</p>
<script type="math/tex; mode=display">
 S = B^T X</script><p> 其中$S_{K×M}$，$B_{N×K}$，$X_{N×M}$</p>
</li>
</ol>
<blockquote>
<p>注：若取$K=N$，可重建数据，如下<br><img src="/2018/10/22/PCA/pca_restructure1.png" alt="pca_restructure1"><br><img src="/2018/10/22/PCA/pca_restructure2.png" alt="pca_restructure2"></p>
</blockquote>
<h3 id="复原"><a href="#复原" class="headerlink" title="复原"></a>复原</h3><p>第$5$步中，样本点向量$X^{(i)}$的主要分量投射到$K$个$N$维向量上，投影坐标为$S^{(i)}_k$，即</p>
<script type="math/tex; mode=display">
X^{(i)} \approx \sum_{k=1}^K S^{(i)}_k \beta_k</script><p>以上就是样本点的复原公式，矩阵形式即</p>
<script type="math/tex; mode=display">
\hat{X} = BS</script><p>其中$\hat{X}_{N×M}$，$B_{N×K}$，$S_{K×M}$</p>
<p>考虑到已去均值化，故</p>
<script type="math/tex; mode=display">
\hat{X}_j \approx \hat{X}_j + \mu_j</script><h2 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h2><blockquote>
<p>投影向量的$2$范数最大，或者说，投影后的坐标平方和最大</p>
</blockquote>
<p>当所有样本$X$投射到第一主轴$\beta_1$上，其坐标为</p>
<script type="math/tex; mode=display">
S_1 = X^T \beta_1</script><p>所有元素的平方和，或向量$S_1$的$2$范数为</p>
<script type="math/tex; mode=display">
||S_1||_2^2 
= S_1^T S_1 = \beta_1^T X X^T \beta_1 \tag{1}</script><p>即优化目标为</p>
<script type="math/tex; mode=display">
\max ||S_1||_2^2</script><script type="math/tex; mode=display">
s.t.　||\beta_1||_2^2 = 1</script><p>矩阵$C=XX^T$为对称矩阵，故可单位正交化</p>
<script type="math/tex; mode=display">
C = W \Lambda W^T</script><script type="math/tex; mode=display">
W = \left[\begin{matrix}
    | & & |\\
    w_1 & ... & w_M\\
    | & & |\\
\end{matrix}\right]　
\Lambda = \left[\begin{matrix}
    \lambda_1 &  & \\
     & ... & \\
     &  & \lambda_M\\
\end{matrix}\right]</script><p>其中$\lambda_1 &gt; …&gt; \lambda_M$，$w_i(i=1,…,M)$为矩阵$C$的特征向量(单位向量，互相正交)</p>
<blockquote>
<p>实际上$R(C) \leq (n-1)$，即最多有$(n-1)$个特征值大于$0$。</p>
</blockquote>
<script type="math/tex; mode=display">
||S_1||_2^2
= \beta_1^T W \Lambda W^T \beta_1 \tag{2}</script><p>令$\alpha_1 = W^T \beta_1, \beta_1 = W \alpha_1$，可得</p>
<script type="math/tex; mode=display">
||S_1||_2^2
= \alpha_1^T \Lambda \alpha_1 \tag{3}</script><p>即</p>
<script type="math/tex; mode=display">
||S_1||_2^2 = \sum_{i=1}^M \lambda_i \alpha_{1i}^2 \tag{4}</script><p>进一步</p>
<script type="math/tex; mode=display">
\sum_{i=1}^M \lambda_i \alpha_{1i}^2
\leq \lambda_1 \sum_{i=1}^M \alpha_{1i}^2 \tag{5}</script><p>且由于$\beta_1^T\beta_1 = 1$，故</p>
<script type="math/tex; mode=display">
1 = \beta_1^T\beta_1 = \alpha_1^T W^T W \alpha = \alpha^T \alpha = 
\sum_{i=1}^M \alpha_{1i}^2</script><p>可得</p>
<script type="math/tex; mode=display">
||S_1||_2^2
= \sum_{i=1}^M \lambda_i \alpha_{1i}^2
\leq \lambda_1  \tag{6}</script><p>为使$(6)$取等号，即达最大值，可使</p>
<script type="math/tex; mode=display">
\begin{cases}
    \alpha_{11} = 1 \\
    \alpha_{12} = ... = \alpha_{1M} = 0
\end{cases}</script><p>即令</p>
<script type="math/tex; mode=display">
\beta_1 = W \alpha_1 = w_1</script><blockquote>
<p>$\alpha_1 = [1, 0, …, 0]^T$</p>
</blockquote>
<p>所以$\beta_1$对应矩阵$C=XX^T$的特征向量$w_1$，且有</p>
<script type="math/tex; mode=display">
||S_1||_2^2 = \lambda_1</script><blockquote>
<p>或者第一主成分的证明也可以这样，建立优化目标</p>
<script type="math/tex; mode=display">
\beta_1 = \arg \max　||S_1||_2^2</script><script type="math/tex; mode=display">s.t.　||\beta_1||_2^2 = 1</script><p>构造拉格朗日函数</p>
<script type="math/tex; mode=display">
L(\beta_1, \lambda_1) = ||S_1||_2^2 + \lambda_1 (1 - ||\beta_1||_2^2)</script><p>也即</p>
<script type="math/tex; mode=display">
L(\beta_1, \lambda_1) = \beta_1^T X X^T \beta_1 + \lambda_1 (1 - \beta_1^T \beta_1)</script><p>求其极值点</p>
<script type="math/tex; mode=display">
▽_{\beta_1}L(\beta_1) = 2 X X^T \beta_1 - 2 \lambda_1 \beta_1 = 0</script><p>有</p>
<script type="math/tex; mode=display">
X X^T \beta_1 = \lambda_1 \beta_1</script><p>可见$\beta_1$即方阵$X X^T$的特征向量</p>
</blockquote>
<p>当我们希望用更多的主成分刻画数据，如已经求得主成分$\beta_1, …, \beta_{r-1}$，先需求解$\beta_r$，引入正交约束$\beta_r^T \beta_i = 0$，即目标函数为</p>
<script type="math/tex; mode=display">
||S_r||_2^2 = \beta_r^T C \beta_r</script><script type="math/tex; mode=display">
s.t.　\beta_r^T \beta_i = 0, i = 1, ..., r-1</script><script type="math/tex; mode=display">
||\beta_r||_2^2 = 1</script><p>令$\beta_r = W \alpha_r$，则</p>
<script type="math/tex; mode=display">
||S_r||_2^2
= \alpha_r^T \Lambda \alpha_r
= \sum_i \lambda_i \alpha_{ri}^2</script><p>而根据正交约束</p>
<script type="math/tex; mode=display">
0 = \beta_r^T \beta_i = \alpha_r^T W^T w_i = \alpha_{ri},　
i = 1, ..., r-1</script><blockquote>
<p>$ W^T w_i = \left[0, …, 1_i, …, 0\right]^T$</p>
</blockquote>
<p>所以</p>
<script type="math/tex; mode=display">
||S_r||_2^2 = \sum_i \lambda_i \alpha_{ri}^2 = \lambda_r \alpha_{rr}^2 \tag{5}</script><p>又因为$\beta_r^T \beta_r = 1$(单位向量)，故</p>
<script type="math/tex; mode=display">
\beta_r^T \beta_r = \alpha_r^T W^T W \alpha_r = \alpha_r^T \alpha_r = \sum_i \alpha_{ri}^2 = 1</script><p>于是类似的，为使$(5)$取最大，取</p>
<script type="math/tex; mode=display">
\begin{cases}
    \alpha_{rr} = 1\\
    \alpha_{ri} = 0,　i = 1, ..., M, i \neq r
\end{cases}</script><blockquote>
<p>$\alpha_r = [0, …, 1_r, …, 0]$</p>
</blockquote>
<p>则此时</p>
<script type="math/tex; mode=display">
\beta_r = W \alpha_r = w_r</script><p>且有</p>
<script type="math/tex; mode=display">
||S_r||_2^2 = \lambda_r</script><p>证毕。</p>
<h2 id="白化-whitening"><a href="#白化-whitening" class="headerlink" title="白化(whitening)"></a>白化(whitening)</h2><p><code>whitening</code>的目的是去掉数据之间的相关联度，是很多算法进行预处理的步骤。比如说当训练图片数据时，由于图片中相邻像素值有一定的关联，所以很多信息是冗余的。这时候去相关的操作就可以采用白化操作。</p>
<p>数据的<code>whitening</code>必须满足两个条件：</p>
<ol>
<li>不同特征间相关性最小，接近$0$；</li>
<li>所有特征的方差相等（不一定为$1$）。</li>
</ol>
<p>常见的白化操作有<code>PCA whitening</code>和<code>ZCA whitening</code>。</p>
<blockquote>
<p><a href="http://deeplearning.stanford.edu/wiki/index.php/Whitening" target="_blank" rel="noopener">Whitening - Ufldl</a></p>
</blockquote>
<ul>
<li>PCA whitening<br>  <code>PCA whitening</code>指将数据$X$经过<code>PCA</code>降维为$S$后，可以看出$S$中每一维是独立的，满足<code>whitening</code>的第一个条件，这是只需要将$S$中的每一维都除以标准差就得到了每一维的方差为$1$，也就是说方差相等。<script type="math/tex; mode=display">
  X_{PCAwhite, j} = \frac{X_{rot, j}}{\sqrt{\lambda_j}}</script></li>
</ul>
<ul>
<li>ZCA whitening<br>  <code>ZCA whitening</code>是指数据$X$先经过<code>PCA</code>变换为$S$，但是并不降维，因为这里是把所有的成分都选进去了。这是也同样满足<code>whtienning</code>的第一个条件，特征间相互独立。然后同样进行方差为$1$的操作，最后将得到的矩阵左乘一个特征向量矩阵$U$即可。<script type="math/tex; mode=display">
  X_{ZCAwhite} = U · X_{PCAwhite}</script></li>
</ul>
<h1 id="Kernel-PCA"><a href="#Kernel-PCA" class="headerlink" title="Kernel PCA"></a>Kernel PCA</h1><p><code>Kernel PCA</code>的思想是在高维的特征空间中求解协方差矩阵</p>
<script type="math/tex; mode=display">
\Sigma = \frac{1}{M} \sum_{i=1}^M \Phi(X^{(i)}) \Phi(X^{(i)})^T</script><p>其中$\Phi(X^{(i)})$表示将样本$i$映射到高维空间后中的向量，即</p>
<script type="math/tex; mode=display">
\Phi(X^{(i)}) = \left[ \phi^{(i)}_1, \phi^{(i)}_2, ..., \phi^{(i)}_{N'} \right]^T</script><p>其中$N’ &gt; N$，由于$\Phi(X^{(i)})$为隐式的，故设置核函数求解，记</p>
<script type="math/tex; mode=display">
\kappa(i, j) = \Phi(X^{(i)}) \Phi(X^{(i)})^T</script><blockquote>
<p>关于核技巧，移步<a href="">非线性支持向量机</a></p>
</blockquote>
<p><img src="/2018/10/22/PCA/kernel_pca.jpg" alt="kernel_pca"></p>
<h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><p>可利用<code>PCA</code>与线性回归求解$3$维空间中平面的法向量</p>
<ol>
<li>利用<code>PCA</code>重建数据(不降维，此时为$3$维)，此时第$1, 2$主成分轴可张成所求平面，即该平面可表示为<script type="math/tex; mode=display">
 \Pi = span \{ \beta_1, \beta_2 \}</script></li>
</ol>
<blockquote>
<p>就是说，第一、二主成分是这些点“拉伸”最大的方向 :-)，好懂不？</p>
</blockquote>
<ol>
<li><p>由<a href="https://louishsu.xyz/2018/10/18/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">正交投影</a>可知，平面外一点$y$可通过最小二乘(线性回归)的方法投射到平面上，向量运算，不考虑偏置项，即</p>
<script type="math/tex; mode=display">
 \hat{y} 
 = \theta_1 x_1 + \theta_2 x_2 \tag{*}</script><p> 其中$x_1, x_2$表示第一、第二主成分$\beta_1, \beta_2$，为$3$维向量</p>
<script type="math/tex; mode=display">
 \hat{y} = \left[
     \begin{matrix}
         \hat{y_1} \\
         \hat{y_2} \\
         \hat{y_3} \\
     \end{matrix}
 \right]　
 x_i = \left[
     \begin{matrix}
         x_{i1} \\
         x_{i2} \\
         x_{i3} \\
     \end{matrix}
 \right]</script><p> 可利用公式求解回归参数$\theta$</p>
<script type="math/tex; mode=display">
 \theta = (X^TX+\lambda I)^{-1} X^T y</script><blockquote>
<p>注意：$X(n_samples, n_features)$，这里把$(x_{1j}, x_{2j}, y_{j})作为一组样本$</p>
</blockquote>
<p> 此时该参数表示在主轴上的坐标$(\theta_1, \theta_2)$，带回$(*))$即可解得$\hat{y}$</p>
<script type="math/tex; mode=display">
 \hat{y} 
 = \theta_1 \beta_1 + \theta_2 \beta_2 \tag{*}</script><p> 通俗理解，一掌把$y$拍平在了平面$\Pi$上，变成了$\hat{y}$，但是哪有这么好拍。。。这个时候刺在掌心里一定有一个垂直的向量分量，即为该平面的法向量</p>
<script type="math/tex; mode=display">
 \vec{n} = y - \hat{y}</script><p> <strong>也可使用粗暴一点的方法，直接将第三主成分作为法向量。</strong></p>
<blockquote>
<p>或者直接上投影公式：</p>
<script type="math/tex; mode=display">
\hat{y} = Py</script><script type="math/tex; mode=display">　
P = X (X^TX+\lambda I)^{-1} X^T</script></blockquote>
</li>
</ol>
<pre><code>![projection](/PCA/projection.jpg)

总体的运算流程如下
- 利用所有样本点(近似平面)计算主成分，第一、二主成分张成平面$\Pi$；
- 选出其中一个样本点，将平行于平面$\Pi$的成分投射到$\Pi$上；
- 该样本点剩余分量即法向量；
- 一般来说，取所有点法向量的均值。
</code></pre><h1 id="程序"><a href="#程序" class="headerlink" title="程序"></a>程序</h1><p><a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/tree/master/Andrew%20Ng%2C%20Stanford%20University/mlclass-ex7-3-pca" target="_blank" rel="noopener">@Github: PCA</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">class PrincipalComponentAnalysis():</span><br><span class="line">    def __init__(self, n_component=-1):</span><br><span class="line">        self.n_component = n_component</span><br><span class="line">        self.meanVal = None</span><br><span class="line">        self.axis = None</span><br><span class="line">    def fit(self, X, prop=0.99):</span><br><span class="line">        &apos;&apos;&apos;</span><br><span class="line">        the parameter &apos;prop&apos; is only for &apos;n_component = -1&apos;</span><br><span class="line">        &apos;&apos;&apos;</span><br><span class="line">        # 第一步: 归一化</span><br><span class="line">        self.meanVal = np.mean(X, axis=0)                   # 训练样本每个特征上的的均值</span><br><span class="line">        X_normalized = (X - self.meanVal)                   # 归一化训练样本</span><br><span class="line">        # 第二步：计算协方差矩阵</span><br><span class="line">        # cov = X_normalized.T.dot(X_normalized)</span><br><span class="line">        cov = np.cov(X_normalized.T)                        # 协方差矩阵</span><br><span class="line">        eigVal, eigVec = np.linalg.eig(cov)                 # EVD</span><br><span class="line">        order = np.argsort(eigVal)[::-1]                    # 从大到小排序</span><br><span class="line">        eigVal = eigVal[order]</span><br><span class="line">        eigVec = eigVec.T[order].T</span><br><span class="line">        # 选择主成分的数量</span><br><span class="line">        if self.n_component == -1:</span><br><span class="line">            sumOfEigVal = np.sum(eigVal)</span><br><span class="line">            sum_tmp = 0</span><br><span class="line">            for k in range(eigVal.shape[0]):</span><br><span class="line">                sum_tmp += eigVal[k]</span><br><span class="line">                if sum_tmp &gt; prop * sumOfEigVal:            # 平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的 K 值</span><br><span class="line">                    self.n_component = k + 1</span><br><span class="line">                    break</span><br><span class="line">        # 选择投影坐标轴</span><br><span class="line">        self.axis = eigVec[:, :self.n_component]            # 选择前n_component个特征向量作为投影坐标轴</span><br><span class="line">    def transform(self, X):</span><br><span class="line">        # 第一步：归一化</span><br><span class="line">        X_normalized = (X - self.meanVal)                   # 归一化测试样本</span><br><span class="line">        # 第二步：投影 X_nxk · V_kxk&apos; = X&apos;_nxk&apos;</span><br><span class="line">        X_transformed = X_normalized.dot(self.axis)</span><br><span class="line">        return X_transformed</span><br><span class="line">    def fit_transform(self, X, prop=0.99):</span><br><span class="line">        self.fit(X, prop=prop)</span><br><span class="line">        return self.transform(X)</span><br><span class="line">    def transform_inv(self, X_transformed):</span><br><span class="line">        # 视投影向量长度为一个单位长度，投影结果为投影向量上的坐标</span><br><span class="line">        # X&apos;_nxk&apos; · V_kxk&apos;.T = X&apos;&apos;_nxk</span><br><span class="line">        X_restructed = X_transformed.dot(self.axis.T)</span><br><span class="line">        # 还原数据</span><br><span class="line">        X_restructed = X_restructed + self.meanVal</span><br><span class="line">        return X_restructed</span><br></pre></td></tr></table></figure>
<p>实验结果</p>
<ul>
<li><p>Demo1: PCA applied on 2-d datasets<br>  <img src="/2018/10/22/PCA/2d_restructed.png" alt="2d_restructed"></p>
</li>
<li><p>Demo2: PCA applied on wild face</p>
<ul>
<li>origin<br><img src="/2018/10/22/PCA/face_origin.png" alt="origin"></li>
<li>reduced<br><img src="/2018/10/22/PCA/face_reduced.png" alt="reduced"></li>
<li>restructured<br><img src="/2018/10/22/PCA/face_restructed.png" alt="restructured"></li>
</ul>
</li>
</ul>

        </div>
        <div class="post-tool">
            <a class="btn-thumbs-up" href="javascript:void(0);" data-cid="52" title="95">
                <i class="fa fa-thumbs-up" aria-hidden="true"></i> 打赏
            </a>
        </div>
        
        <div class="post-tags">标签：
            
        </div>
        
    </article>
    
    <p style="text-align: center">本文代表个人观点，内容仅供参考。若有不恰当之处，望不吝赐教！</p>
    
    

    

</div>
<script src="/js/busuanzi.pure.mini.js"></script>


        </div><!-- end #main-->
    </div><!-- end #body -->
    <footer class="footer">
    <div class="footer-inner" style="text-align: center">
        <p>
            <a href="/about" title="关于">关于</a>&nbsp;&nbsp<em>·</em>&nbsp;&nbsp
            <!-- 自定义链接 -->
            <a href="/help" title="帮助">帮助</a>&nbsp;&nbsp<em>·</em>&nbsp;&nbsp
            <a href="/links" title="友链">友链</a>&nbsp;&nbsp<em>·</em>&nbsp;&nbsp
            <a href="/sitemap.xml" title="地图">地图</a>
        </p>
        <p>
            本站已建立&nbsp<a href="/timeline" id="siteBuildingTime"></a>&nbsp天，<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="licence">采用知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议创作</a><br>
            ©2017-<span id="cpYear"></span> 基于&nbsp<a href="http://hexo.io" target="_blank" rel="nofollow">Hexo</a>
            ，主题采用&nbsp&nbsp<a href="https://github.com/tangkunyin/hexo-theme-jsimple" target="_blank" rel="bookmark">JSimple</a>
            ，作者&nbsp<a href="https://louishsu.xyz/" target="_blank" rel="friend">徐耀彬</a>
            ，Hosted by <a href="https://pages.github.com/" target="_blank" rel="nofollow">GitHub Pages</a>
        </p>
    </div>
</footer>
<script src="/js/SimpleCore.js"></script>

</div>
<!-- search pop -->
<div class="popup search-popup local-search-popup">
    <div class="local-search-header clearfix">
        <span class="search-icon">
            <i class="fa fa-search"></i>
        </span>
        <span class="popup-btn-close">
            <i class="fa fa-times-circle"></i>
        </span>
        <div class="local-search-input-wrapper">
            <input id="local-search-input" spellcheck="false" type="text" autocomplete="off" placeholder="请输入查询关键词">
        </div>
    </div>
    <div id="local-search-result"></div>
</div>
<div class="fixed-btn">
    <a class="btn-gotop" href="javascript:"> <i class="fa fa-angle-up"></i></a>
</div>
<script>
    $(function () {
        var jsi_config = {
            buildingTime: '10/20/2018',
            current: $('.post-tags').length > 0 ? 'post' : 'archive',
            snsQRCode: '/images/sns-qrcode.png',
            donateImg: '/images/donate-qr.png',
            localSearch: { dbPath: '' },
            readMode: 'day'
        };
        
            jsi_config.localSearch = {
                dbPath: '/search.xml',
                trigger: 'auto',
                topN: '1',
                unescape: 'false'
            }
        
        SimpleCore.init(jsi_config);
        
    });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
