<!DOCTYPE HTML>
<html class="no-js" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!--[if lte IE 9]>
<meta http-equiv="refresh" content="0;url=http://yoursite.com/warn.html">
<![endif]-->
<meta charset="utf-8">
<meta http-equiv="X-DNS-Prefetch-Control" content="on">
<link rel="dns-prefetch" href="http://yoursite.com">
<link rel="dns-prefetch" href="//www.google-analytics.com">
<link rel="prefetch" href="http://yoursite.com">
<link rel="prefetch" href="//www.google-analytics.com">


<link rel="prerender" href="http://yoursite.com">

<meta http-equiv="X-UA-Compatible" content="IE=Edge">
<meta name="renderer" content="webkit">
<meta name="viewport" content="width=device-width, initial-scale=1.0,user-scalable=no">
<meta http-equiv="mobile-agent" content="format=html5; url=http://yoursite.com">
<meta name="author" content="Louis Hsu">
<link rel="stylesheet" href="/css/JSimple.css">

<link rel="shortcut icon" href="/images/favicon.png">


<title>spam classification - LOUIS&#39; BLOG</title>

<meta name="keywords" content="">

<meta name="description " content="Inside! Insane!">

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                processEscapes: true
            }
        });
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


    

    

</head>
<body>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="nav">
    <nav class="nav-menu">
        <a class="site-name current" href="/" title="彬">彬</a>
        <a class="site-index current" href="/"><i class="fa fa-home"></i><span>Home</span></a>
        <a href="/archives" title="Archives"><i class="fa fa-archives"></i><span>Archives</span></a>
        <a href="/tags" title="Tags"><i class="fa fa-tags"></i><span>Tags</span></a>
        <!-- custom single page of menus -->
        
        
        <a href="/help" title="帮助">
            <i class="fa fa-question-circle"></i>
            <span>帮助</span>
        </a>
        
    </nav>
</div>

<div class="nav-user">
    <a class="btn-search" href="#"><i class="fa fa-search"></i></a>
    <a class="btn-read-mode" href="#"><i class="fa fa-sun-o"></i></a>
    <a class="btn-sns-qr" href="javascript:"><i class="fa fa-telegram"></i></a>
</div>

<div id="wrapper" class="clearfix">
    <div id="body">
        <div class="main" id="main">
            <div id="cover">
    <div class="cover-img"></div>
    <div class="cover-info">
        
        <img class="avatar" width="72" src="/images/favicon.png" alt="avatar">
        
        <h1 class="cover-siteName">LOUIS&#39; BLOG</h1>
        <h3 class="cover-siteTitle">人生苦短，不如不管，继续任性</h3>
        <p class="cover-siteDesc">技术博客？</p>
        <div class="cover-sns">
            

        </div>
    </div>
</div>

            <div class="page-title">
    <ul>
        <li><a href="/">Recent Posts</a></li>
        
            
                <li class="">
                    <a href="/categories//" data-name="主页">主页</a>
                </li>
            
                <li class="">
                    <a href="/categories/Python" data-name="Python">Python</a>
                </li>
            
                <li class="">
                    <a href="/categories/Machine-Learning" data-name="机器学习">机器学习</a>
                </li>
            
                <li class="">
                    <a href="/categories/Deep-Learning" data-name="深度学习">深度学习</a>
                </li>
            
                <li class="">
                    <a href="/categories/冯唐" data-name="冯唐">冯唐</a>
                </li>
            
                <li class="">
                    <a href="/categories/Others" data-name="其他">其他</a>
                </li>
            
        
        <li class="page-search">
    <form id="search" class="search-form">
        <input type="text" readonly="readonly" id="local-search-input-tip" placeholder="click to search...">
        <button type="button" disabled="disabled" class="search-form-submit"><i class="fa fa-search"></i></button>
    </form>
</li>

    </ul>
</div>
<div class="main-inner">
    <article class="post" itemscope="" itemtype="http://schema.org/BlogPosting">
        <div class="post-header">
            <div class="post-author clearfix">
                <a class="avatar fleft" href="https://louishsu.xyz/" target="_blank">
                    <img width="48" src="/images/favicon.png" alt="avatar">
                </a>
                <p><span class="label">Author</span>
                    <a href="https://louishsu.xyz/" target="_blank">徐耀彬</a>
                    <span title="Last edited at&nbsp;2018-10-26">2018-10-26</span>
                </p>
                <p>有味道的程序员</p>
            </div>
            <h2 class="post-title">Spam Classification</h2>
            <div class="post-meta">
                emm... 8747 words in the article |
                you are the&nbsp;<span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span>th friend who reading now
            </div>
        </div>
        <div class="post-content markdown-body">
            <blockquote>
<p>踩坑？？？全部给我踩平！！！</p>
</blockquote>
<p>来自<a href="https://www.lintcode.com/ai/spam-message-classification/overview" target="_blank" rel="noopener">LintCode垃圾短信分类</a><br><a href="https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/tree/master/spam%20or%20ham" target="_blank" rel="noopener">@Github: spam or ham</a></p>
<h1 id="垒代码"><a href="#垒代码" class="headerlink" title="垒代码"></a>垒代码</h1><h2 id="预处理及向量化"><a href="#预处理及向量化" class="headerlink" title="预处理及向量化"></a>预处理及向量化</h2><p>观察各文本后，发现各文本中包含的单词多种多样，包含标点、数字等，例如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</span><br><span class="line">- XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here&gt;&gt; http://wap. </span><br><span class="line">- 07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder.</span><br></pre></td></tr></table></figure></p>
<p>且按空格分词后，部分单词中仍包含<code>whitespace</code>，故选择的预处理方案是，<strong>去除分词后文本中的标点、数字、空格等，并将单词中字母全部转为小写</strong>。</p>
<blockquote>
<p>中文分词可采用<code>jieba</code>(街霸？)</p>
</blockquote>
<p>预处理后，按当前的文本内容建立字典，并统计各样本的词数向量，详细代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">class Words2Vector():</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    建立字典，将输入的词列表转换为向量，表示各词出现的次数</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.dict = None</span><br><span class="line">        self.n_word = None</span><br><span class="line">    def fit_transform(self, words):</span><br><span class="line">        self.fit(words)</span><br><span class="line">        return self.transform(words)</span><br><span class="line">    def fit(self, words):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        @param &#123;list[list[str]]&#125; words</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        words = _flatten(words)                                                 # 展开为1维列表</span><br><span class="line">        words = self.filt(words)                                                # 滤除空格、数字、标点</span><br><span class="line"></span><br><span class="line">        self.word = list(set(words))                                            # 去重</span><br><span class="line">        self.n_word = len(set(words))                                           # 统计词的个数</span><br><span class="line">        self.dict = dict(zip(self.word, [_ for _ in range(self.n_word)]))       # 各词在字典中的位置</span><br><span class="line">    def transform(self, words):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        @param &#123;list[list[str]]&#125; words</span><br><span class="line">        @return &#123;ndarray&#125; retarray: vector</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        retarray = np.zeros(shape=(len(words), self.n_word))                    # 返回的词数向量</span><br><span class="line">        for i in range(len(words)):</span><br><span class="line">            words[i] = self.filt(words[i])                                      # 滤除空格、数字、标点</span><br><span class="line">        for i in range(len(words)):</span><br><span class="line">            for w in words[i]:</span><br><span class="line">                if w in self.word:                                              # 是否在训练集生成的字典中</span><br><span class="line">                    retarray[i, self.dict[w]] += 1                              # 查询字典，找到对应特征的下标</span><br><span class="line">        return retarray</span><br><span class="line">    def filt(self, flattenWords):</span><br><span class="line">        retWords = []</span><br><span class="line">        en_stops = set(stopwords.words(&apos;english&apos;))                              # 停用词列表</span><br><span class="line">        for word in flattenWords:</span><br><span class="line">            word = word.translate(str.maketrans(&apos;&apos;, &apos;&apos;, string.whitespace))     # 去除空白</span><br><span class="line">            word = word.translate(str.maketrans(&apos;&apos;, &apos;&apos;, string.punctuation))    # 去除标点</span><br><span class="line">            word = word.translate(str.maketrans(&apos;&apos;, &apos;&apos;, string.digits))         # 去除数字</span><br><span class="line">            if word not in en_stops and (len(word) &gt; 1):                        # 删除停用词，并除去长度小于等于2的词</span><br><span class="line">                retWords.append(word.lower())</span><br><span class="line">        return retWords</span><br></pre></td></tr></table></figure></p>
<h2 id="TF-IDF方法"><a href="#TF-IDF方法" class="headerlink" title="TF-IDF方法"></a>TF-IDF方法</h2><p>由词数向量可计算词频，但只用词频忽略了各文本在不同文档中的重要程度，关于<code>TF-IDF</code>，在<a href="https://louishsu.xyz/2018/10/25/TF-IDF/" target="_blank" rel="noopener">另一篇博文</a>中详细说明。</p>
<p>由于剔除了停用词等，部分向量不包含任何内容，即词数向量为$\vec{0}$，这时计算词频和单位化时，会出现<code>nan</code>的运算结果，故只对非空向量进行计算。</p>
<p>训练后需要保存的是<code>IDF</code>向量，<code>TF</code>向量在新样本输入后重新计算，故无需保存。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">class TfidfVectorizer():</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.idf = None</span><br><span class="line">    def fit_transform(self, num_vec):</span><br><span class="line">        self.fit(num_vec)</span><br><span class="line">        return self.transform(num_vec)</span><br><span class="line">    def fit(self, num_vec):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        @param &#123;ndarray&#125;: num_vec, shape(N_sample, N_feature)</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        num_vec[num_vec&gt;0] = 1</span><br><span class="line">        n_doc = num_vec.shape[0]</span><br><span class="line">        n_term = np.sum(num_vec, axis=0)    # 各词出现过的文档次数</span><br><span class="line">        self.idf = np.log((n_doc + 1) / (n_term + 1)) + 1</span><br><span class="line">        return self.idf</span><br><span class="line">    def transform(self, num_vec):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        @param &#123;ndarray&#125;: num_vec, shape(N_sample, N_feature)</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # 求解词频向量，由于部分向量为空，故下句会出现问题</span><br><span class="line">        # tf = num_vec / np.sum(num_vec, axis=1).reshape(-1, 1) =&gt; nan</span><br><span class="line">        # 解决方法：只对非空向量进行词频计算</span><br><span class="line">        tf = np.zeros(shape=num_vec.shape)</span><br><span class="line">        n_terms = np.sum(num_vec, axis=1); idx = (n_terms!=0)</span><br><span class="line"></span><br><span class="line">        tf[idx] = num_vec[idx] / n_terms[idx].reshape(-1, 1)            # 计算词频，只对非空向量进行</span><br><span class="line">        </span><br><span class="line">        tfidf = tf * self.idf</span><br><span class="line">        tfidf[idx] /= np.linalg.norm(tfidf, axis=1)[idx].reshape(-1, 1) # 单位化，只对非空向量进行</span><br><span class="line">        </span><br><span class="line">        return tfidf</span><br></pre></td></tr></table></figure>
<h2 id="贝叶斯决策"><a href="#贝叶斯决策" class="headerlink" title="贝叶斯决策"></a>贝叶斯决策</h2><p>各文本向量化后，就可通过机器学习算法进行模型的训练和预测，这里采用的是贝叶斯决策的方法，需要注意的有以下几点</p>
<ul>
<li>似然函数$p(x|c_k)$与<a href="https://louishsu.xyz/2018/10/18/Bayes-Decision/" target="_blank" rel="noopener">贝叶斯决策</a>文中例不同，这里宜采用高斯分布作为分布模型；</li>
<li><p>按朴素贝叶斯计算$p(x|c_k)$，但注意此处不能将各维特征单独训练$1$维高斯分布模型，然后计算预测样本似然函数值时进行累乘，如下</p>
<script type="math/tex; mode=display">
p(x|c_k) = \prod_{j=1}^{N_feature} p(x_j|c_k)</script><p>因为特征维度特别高，各个特征单独用$1$维高斯分布描述，累乘计算会下溢，故这里采用多元高斯分布</p>
<script type="math/tex; mode=display">
p(x|c_k) = \frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma_k|^{\frac{1}{2}}} · 
e^{-\frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k)}</script><ul>
<li>且经主成分分析后，各维度间线性相关性降低，故假定<script type="math/tex; mode=display">
\Sigma_k = diag\{\sigma_{k1}, ..., \sigma_{kn}\}</script></li>
<li><p>但分母$(2\pi)^{\frac{n}{2}}|\Sigma_k|^{\frac{1}{2}}$在计算时不稳定，且各特征标准差大小相差无几，故这里假定</p>
<script type="math/tex; mode=display">
\Sigma_k = I</script></li>
<li><p>最终简化后的似然函数计算方法为</p>
<script type="math/tex; mode=display">
p(x|c_k) =  
e^{-\frac{1}{2} (x - \mu_k)^T (x - \mu_k)}</script></li>
</ul>
</li>
</ul>
<h3 id="贝叶斯决策模型训练"><a href="#贝叶斯决策模型训练" class="headerlink" title="贝叶斯决策模型训练"></a>贝叶斯决策模型训练</h3><p>基于上述假设，只需训练多元高斯分布的各维均值$\mu_j$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">def fit(self, labels, text):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    @param &#123;ndarray&#125; labels: shape(N_samples, ), labels[i] \in &#123;0, 1&#125;</span><br><span class="line">    @param &#123;list[list[str]]&#125; words</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">    labels = self.encodeLabel(labels); words = self.text2words(self.clean(text))</span><br><span class="line"></span><br><span class="line">    vecwords = self.numvectorizer.fit_transform(words)              # 向量化</span><br><span class="line">    vecwords = self.tfidfvectorizer.fit_transform(vecwords)         # tfidf, shape(N_samples, N_features)</span><br><span class="line"></span><br><span class="line">    isnotEmpty = (np.sum(vecwords, axis=1)!=0)                      # 去掉空的样本</span><br><span class="line">    vecwords = vecwords[isnotEmpty]; labels = labels[isnotEmpty]</span><br><span class="line"></span><br><span class="line">    # vecwords = self.reduce_dim.fit_transform(vecwords)              # 降维，计算量太大</span><br><span class="line">    self.n_features = vecwords.shape[1]</span><br><span class="line"></span><br><span class="line">    labels = OneHotEncoder().fit_transform(labels.reshape((-1, 1))).toarray()</span><br><span class="line">        self.priori = np.mean(labels, axis=0)                           # 先验概率</span><br><span class="line"></span><br><span class="line">    self.likelihood_mu = np.zeros(shape=(2, vecwords.shape[1]))	    # 设似然函数p(x|c)为高斯分布</span><br><span class="line">    for i in range(2):</span><br><span class="line">        vec = vecwords[labels[:, i]==1]</span><br><span class="line">        self.likelihood_mu[i] = np.mean(vec, axis=0)</span><br></pre></td></tr></table></figure>
<h3 id="贝叶斯决策模型预测"><a href="#贝叶斯决策模型预测" class="headerlink" title="贝叶斯决策模型预测"></a>贝叶斯决策模型预测</h3><p>决策函数为</p>
<script type="math/tex; mode=display">
if　p(x|c_i)P(c_i) > p(x|c_j)P(c_j),　then　x \in c_i</script><p>但实际效果显示，等先验概率$P(c_j)$结果更好$(???)$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def multigaussian(self, x, mu):</span><br><span class="line">    &quot;&quot;&quot; 简化</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x = x - mu</span><br><span class="line">    a = np.exp(-0.5 * x.T.dot(x))</span><br><span class="line">    return a</span><br><span class="line">def predict(self, text):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    @param &#123;list[list[str]]&#125; words</span><br><span class="line">    @note:</span><br><span class="line">                      p(x|c)P(c)</span><br><span class="line">            P(c|x) = ------------</span><br><span class="line">                         p(x)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    pred_porba = np.ones(shape=(len(self.clean(text)), 2))      </span><br><span class="line">        </span><br><span class="line">    words = self.text2words(text)</span><br><span class="line">    vecwords = self.tfidfvectorizer.transform(</span><br><span class="line">                                self.numvectorizer.transform(words))    # 向量化</span><br><span class="line"></span><br><span class="line">    for i in range(vecwords.shape[0]):</span><br><span class="line">        for c in range(2):</span><br><span class="line">            # pred_porba[i, c] = self.priori[c] * self.multigaussian(vecwords[i], self.likelihood_mu[c])</span><br><span class="line">            pred_porba[i, c] = self.multigaussian(vecwords[i], self.likelihood_mu[c])</span><br><span class="line"></span><br><span class="line">    pred = np.argmax(pred_porba, axis=1)</span><br><span class="line">    return self.decodeLabel(pred)</span><br></pre></td></tr></table></figure>
<h1 id="调包"><a href="#调包" class="headerlink" title="调包"></a>调包</h1><p>主要用到了<code>scikit-learn</code>机器学习包以下几个功能</p>
<ul>
<li><code>sklearn.feature_extraction.text.TfidfVectorizer()</code></li>
<li><code>sklearn.decomposition.PCA()</code></li>
<li><code>sklearn.naive_bayes.BernoulliNB()</code></li>
</ul>
<p>最终准确率在$97\%$左右，代码比较简单，不进行说明。</p>
<blockquote>
<p>采用<code>sklearn.linear_model import.LogisticRegressionCV()</code>效果更佳</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">def main():</span><br><span class="line">    trainfile = &quot;./data/train.csv&quot;</span><br><span class="line">    testfile = &quot;./data/test.csv&quot;</span><br><span class="line">    </span><br><span class="line">    # 读取原始数据</span><br><span class="line">    data_train = pd.read_csv(trainfile, names=[&apos;Label&apos;, &apos;Text&apos;])</span><br><span class="line">    txt_train  = list(data_train[&apos;Text&apos;])[1: ]; label_train = list(data_train[&apos;Label&apos;])[1: ]</span><br><span class="line">    drop(txt_train)                                             # 删除数字和标点</span><br><span class="line">    txt_test   = list(pd.read_csv(testfile, names=[&apos;Text&apos;])[&apos;Text&apos;])[1: ]</span><br><span class="line">    drop(txt_test)                                              # 删除数字和标点</span><br><span class="line"></span><br><span class="line">    # 训练</span><br><span class="line">    vectorizer = TfidfVectorizer(stop_words=&apos;english&apos;)          # 删除英文停用词</span><br><span class="line">    vec_train = vectorizer.fit_transform(txt_train).toarray()   # 提取文本特征向量</span><br><span class="line">    # reduce_dim = PCA(n_components = 4096)</span><br><span class="line">    # vec_train = reduce_dim.fit_transform(vec_train)</span><br><span class="line">    estimator = BernoulliNB()</span><br><span class="line">    estimator.fit(vec_train, label_train)                       # 训练朴素贝叶斯模型</span><br><span class="line"></span><br><span class="line">    # 测试</span><br><span class="line">    label_train_pred = estimator.predict(vec_train)</span><br><span class="line">    acc = np.mean((label_train_pred==label_train).astype(&apos;float&apos;))</span><br><span class="line">    </span><br><span class="line">    # 预测</span><br><span class="line">    vec_test = vectorizer.transform(txt_test).toarray()</span><br><span class="line">    # vec_test = reduce_dim.transform(vec_test)</span><br><span class="line">    label_test_pred = estimator.predict(vec_test)</span><br><span class="line">    with open(&apos;./data/sampleSubmission.txt&apos;, &apos;w&apos;) as f:</span><br><span class="line">        for i in range(label_test_pred.shape[0]):</span><br><span class="line">            f.write(label_test_pred[i] + &apos;\n&apos;)</span><br></pre></td></tr></table></figure>
        </div>
        <div class="post-tool">
            <a class="btn-thumbs-up" href="javascript:void(0);" data-cid="52" title="95">
                <i class="fa fa-thumbs-up" aria-hidden="true"></i> Donate
            </a>
        </div>
        
        <div class="post-tags">Tags：
            
        </div>
        
    </article>
    
    <p style="text-align: center">This article just represents my own viewpoint. If there is something wrong, please correct me.</p>
    
    

    

</div>
<script src="/js/busuanzi.pure.mini.js"></script>


        </div><!-- end #main-->
    </div><!-- end #body -->
    <footer class="footer">
    <div class="footer-inner" style="text-align: center">
        <p>
            <a href="/about" title="About">About</a>&nbsp;&nbsp<em>·</em>&nbsp;&nbsp
            <!-- 自定义链接 -->
            <a href="/help" title="Help">Help</a>&nbsp;&nbsp<em>·</em>&nbsp;&nbsp
            <a href="/links" title="Links">Links</a>&nbsp;&nbsp<em>·</em>&nbsp;&nbsp
            <a href="/sitemap.xml" title="SiteMap">SiteMap</a>
        </p>
        <p>
            Has been established&nbsp<a href="/timeline" id="siteBuildingTime"></a>&nbspDays，<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="licence">Based on Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)</a><br>
            ©2017-<span id="cpYear"></span> Based on&nbsp<a href="http://hexo.io" target="_blank" rel="nofollow">Hexo</a>
            ，Theme by&nbsp&nbsp<a href="https://github.com/tangkunyin/hexo-theme-jsimple" target="_blank" rel="bookmark">JSimple</a>
            ，Author&nbsp<a href="https://louishsu.xyz/" target="_blank" rel="friend">徐耀彬</a>
            ，Hosted by <a href="https://pages.github.com/" target="_blank" rel="nofollow">GitHub Pages</a>
        </p>
    </div>
</footer>
<script src="/js/SimpleCore.js"></script>

</div>
<!-- search pop -->
<div class="popup search-popup local-search-popup">
    <div class="local-search-header clearfix">
        <span class="search-icon">
            <i class="fa fa-search"></i>
        </span>
        <span class="popup-btn-close">
            <i class="fa fa-times-circle"></i>
        </span>
        <div class="local-search-input-wrapper">
            <input id="local-search-input" spellcheck="false" type="text" autocomplete="off" placeholder="Input query keywords here...">
        </div>
    </div>
    <div id="local-search-result"></div>
</div>
<div class="fixed-btn">
    <a class="btn-gotop" href="javascript:"> <i class="fa fa-angle-up"></i></a>
</div>
<script>
    $(function () {
        var jsi_config = {
            buildingTime: '10/20/2018',
            current: $('.post-tags').length > 0 ? 'post' : 'archive',
            snsQRCode: '/images/sns-qrcode.png',
            donateImg: '/images/donate-qr.png',
            localSearch: { dbPath: '' },
            readMode: 'day'
        };
        
            jsi_config.localSearch = {
                dbPath: '/search.json',
                trigger: 'auto',
                topN: '1',
                unescape: 'false'
            }
        
        SimpleCore.init(jsi_config);
        
    });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
