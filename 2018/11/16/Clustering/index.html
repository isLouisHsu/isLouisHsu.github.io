<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.4.2" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.4.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.4.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.4.2">


  <link rel="mask-icon" href="/images/logo.svg?v=6.4.2" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '6.4.2',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="前言这是第一篇关于无监督学习的博文，无监督的学习则不是尝试预测任何东西，而是寻找数据中的特征，在无监督学习中，有一个重要的方法称为聚类，是把具有相同特征的数据聚集在一组。 基础知识距离度量方法机器学习中距离度量方法有很多，以下简单介绍几种。  机器学习常用的距离度量方法 - taotiezhengfeng的博客 - CSDN博客算法中的各种距离（欧式距离，马氏距离，闵可夫斯基距离……） - 啊哦1">
<meta property="og:type" content="article">
<meta property="og:title" content="Clustering">
<meta property="og:url" content="http://yoursite.com/2018/11/16/Clustering/index.html">
<meta property="og:site_name" content="LOUIS&#39; BLOG">
<meta property="og:description" content="前言这是第一篇关于无监督学习的博文，无监督的学习则不是尝试预测任何东西，而是寻找数据中的特征，在无监督学习中，有一个重要的方法称为聚类，是把具有相同特征的数据聚集在一组。 基础知识距离度量方法机器学习中距离度量方法有很多，以下简单介绍几种。  机器学习常用的距离度量方法 - taotiezhengfeng的博客 - CSDN博客算法中的各种距离（欧式距离，马氏距离，闵可夫斯基距离……） - 啊哦1">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://yoursite.com/2018/11/16/Clustering/cosine_distance.png">
<meta property="og:image" content="http://yoursite.com/2018/11/16/Clustering/kmeans_example.gif">
<meta property="og:image" content="http://yoursite.com/2018/11/16/Clustering/k_means_init.png">
<meta property="og:image" content="http://yoursite.com/2018/11/16/Clustering/k_means_choose_K.png">
<meta property="og:image" content="http://yoursite.com/2018/11/16/Clustering/DBSCAN4.jpg">
<meta property="og:image" content="http://yoursite.com/2018/11/16/Clustering/DBSCAN3.jpg">
<meta property="og:image" content="http://yoursite.com/2018/11/16/Clustering/meanshift_example1.gif">
<meta property="og:image" content="http://yoursite.com/2018/11/16/Clustering/meanshift_example2.gif">
<meta property="og:image" content="http://yoursite.com/2018/11/16/Clustering/DBSCAN5.jpg">
<meta property="og:image" content="http://yoursite.com/2018/11/16/Clustering/cut.jpg">
<meta property="og:image" content="http://yoursite.com/2018/11/16/Clustering/spectral_clustering.png">
<meta property="og:image" content="http://yoursite.com/2018/11/16/Clustering/DBSCAN1.jpg">
<meta property="og:image" content="http://yoursite.com/2018/11/16/Clustering/DBSCAN2.jpg">
<meta property="og:image" content="http://yoursite.com/2018/11/16/Clustering/层次聚类.png">
<meta property="og:updated_time" content="2019-08-07T09:14:19.472Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Clustering">
<meta name="twitter:description" content="前言这是第一篇关于无监督学习的博文，无监督的学习则不是尝试预测任何东西，而是寻找数据中的特征，在无监督学习中，有一个重要的方法称为聚类，是把具有相同特征的数据聚集在一组。 基础知识距离度量方法机器学习中距离度量方法有很多，以下简单介绍几种。  机器学习常用的距离度量方法 - taotiezhengfeng的博客 - CSDN博客算法中的各种距离（欧式距离，马氏距离，闵可夫斯基距离……） - 啊哦1">
<meta name="twitter:image" content="http://yoursite.com/2018/11/16/Clustering/cosine_distance.png">






  <link rel="canonical" href="http://yoursite.com/2018/11/16/Clustering/">



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Clustering | LOUIS' BLOG</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">LOUIS' BLOG</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">To be better</p>
      
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
    <a href="/categories/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-guestbook">
    <a href="/guestbook" rel="section">
      <i class="menu-item-icon fa fa-fw fa-guest"></i> <br>留言</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>
  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    
  
  
  
    
      
    
    <a href="https://github.com/isLouisHsu" class="github-corner" target="_blank" title="Follow me on GitHub" aria-label="Follow me on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#222; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg>
    
      </a>
    



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  
    <div class="reading-progress-bar"></div>
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/11/16/Clustering/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Louis Hsu">
      <meta itemprop="description" content="技术博客？">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LOUIS' BLOG">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Clustering
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-11-16 17:32:44" itemprop="dateCreated datePublished" datetime="2018-11-16T17:32:44+08:00">2018-11-16</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-07 17:14:19" itemprop="dateModified" datetime="2019-08-07T17:14:19+08:00">2019-08-07</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="post-meta-item-icon">
            <i class="fa fa-eye"></i>
             阅读次数： 
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这是第一篇关于无监督学习的博文，无监督的学习则不是尝试预测任何东西，而是寻找数据中的特征，在无监督学习中，有一个重要的方法称为聚类，是把具有相同特征的数据聚集在一组。</p>
<h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><h2 id="距离度量方法"><a href="#距离度量方法" class="headerlink" title="距离度量方法"></a>距离度量方法</h2><p>机器学习中距离度量方法有很多，以下简单介绍几种。</p>
<blockquote>
<p><a href="https://blog.csdn.net/taotiezhengfeng/article/details/80492128" target="_blank" rel="noopener">机器学习常用的距离度量方法 - taotiezhengfeng的博客 - CSDN博客</a><br><a href="https://blog.csdn.net/u014782458/article/details/58180885" target="_blank" rel="noopener">算法中的各种距离（欧式距离，马氏距离，闵可夫斯基距离……） - 啊哦123的博客 - CSDN博客 </a></p>
</blockquote>
<p>定义两个$n$维向量</p>
<script type="math/tex; mode=display">
x = [x_1, x_2, ..., x_n]^T</script><script type="math/tex; mode=display">
y = [y_1, y_2, ..., y_n]^T</script><ul>
<li><p>曼哈顿距离<code>(Manhattan Distance)</code></p>
<script type="math/tex; mode=display">
  d = || x - y ||_1 = \sum_i |x_i - y_i|</script></li>
<li><p>欧氏距离<code>(Euclidean Distance)</code></p>
<script type="math/tex; mode=display">
  d = || x - y ||_2 = \sqrt{\sum_i (x_i - y_i)^2}</script></li>
<li><p>闽可夫斯基距离<code>(Minkowski Distance)</code></p>
<script type="math/tex; mode=display">
  d = || x - y ||_p = \left(\sum_i | x_i - y_i |^{p} \right)^{\frac{1}{p}}</script><p>  当$p$取$1$时为曼哈顿距离，取$2$时为欧式距离。</p>
</li>
<li><p>余弦距离<code>(Cosine)</code></p>
<script type="math/tex; mode=display">
  d = \frac{x^T y}{||x||_2 ||y||_2} = \frac{\sum_i x_i y_i}{\sqrt{\sum_i x_i^2} \sqrt{\sum_i y_i^2}}</script><blockquote>
<p>突然想到为什么向量的夹角余弦是怎么来的，高中学习一直背的公式，现在给一下证明。<br>证明：向量的夹角公式<br><img src="/2018/11/16/Clustering/cosine_distance.png" alt="cosine_distance"></p>
<p>从余弦定理(余弦定理用几何即可)出发，有</p>
<script type="math/tex; mode=display">
\cos \theta = \frac{a^2+b^2-c^2}{2ab}</script><p>其中</p>
<script type="math/tex; mode=display">
||\vec{a}|| = \sqrt{x_1^2 + y_1^2}</script><script type="math/tex; mode=display">
||\vec{b}|| = \sqrt{x_2^2 + y_2^2}</script><script type="math/tex; mode=display">
||\vec{c}|| = \sqrt{(x_1 - x_2)^2 + (x_2 - y_2)^2}</script><p>故</p>
<script type="math/tex; mode=display">
\cos \theta = \frac
  {(\sqrt{x_1^2 + y_1^2})^2 + (\sqrt{x_2^2 + y_2^2})^2 - (\sqrt{(x_1 - x_2)^2 + (x_2 - y_2))^2}}
  {2 \sqrt{x_1^2 + y_1^2} \sqrt{x_2^2 + y_2^2}}</script><script type="math/tex; mode=display">
= \frac
  {x_1 x_2 + y_1 y_2}
  {\sqrt{x_1^2 + y_1^2} \sqrt{x_2^2 + y_2^2}}
  = \frac{a^T b}{||a||·||b||}</script></blockquote>
</li>
</ul>
<h2 id="hard-vs-soft-clustering"><a href="#hard-vs-soft-clustering" class="headerlink" title="hard vs. soft clustering"></a>hard vs. soft clustering</h2><ul>
<li>硬聚类<code>(hard clustering)</code><br>  计算的是一个硬分配<code>(hard ssignment)</code>过程,即每个样本仅仅属于一个簇。</li>
<li>软聚类<code>(soft clustering)</code><br>  分配过程是软的，即一个样本的分配结果是在所有簇上的一个分布，在软分配结果中，一个样本可能对多个簇都具有隶属度。</li>
</ul>
<h2 id="聚类方法的分类"><a href="#聚类方法的分类" class="headerlink" title="聚类方法的分类"></a>聚类方法的分类</h2><ul>
<li>划分方法<br>  <code>K-means</code>，<code>K-medoids</code>，<code>GMM</code>等。</li>
<li>层次方法<br>  <code>AGNES</code>，<code>DIANA</code>，<code>BIRCH</code>，<code>CURE</code>和<code>CURE-NS</code>等。</li>
<li>基于密度的方法<br>  <code>DBSCAN</code>，<code>OPTICS</code>，<code>DENCLUE</code>等。</li>
<li>其他<br>  如<code>STING</code>等。</li>
</ul>
<h1 id="常用聚类方法"><a href="#常用聚类方法" class="headerlink" title="常用聚类方法"></a>常用聚类方法</h1><h2 id="K均值-K-means"><a href="#K均值-K-means" class="headerlink" title="K均值(K-means)"></a>K均值(K-means)</h2><p>是最为经典的基于划分的聚类方法，是十大经典数据挖掘算法之一，通常用于寻找次优解，再通过其他算法(如<code>GMM</code>)寻找更优的聚类结果。</p>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>给定$N$维数据集</p>
<script type="math/tex; mode=display">
X = [x^{(1)}, x^{(2)}, ..., x^{(M)}]</script><p>指定类别数$K$与初始中心点$\mu^{(0)}$，将样本划分到中心点距离其最近的簇中，再根据本次划分更新各簇的中心$\mu^{(t)}$，如此迭代直至得到最好的聚类结果。预测测试样本时，将其划分到中心点距其最近的簇，也可通过<code>KNN</code>等方法。</p>
<p>一般使用欧式距离度量样本到各中心点的距离，也可选择余弦距离等，这也是<code>K-means</code>算法的关键</p>
<script type="math/tex; mode=display">
D(x^{(i)}, \mu_k) = || x^{(i)} - \mu_k ||_2^2</script><p>定义损失函数为</p>
<script type="math/tex; mode=display">
J(\Omega) = \sum_i \sum_k r^{(i)}_k D(x^{(i)}, \mu_k)</script><p>其中</p>
<script type="math/tex; mode=display">
r^{(i)}_k = \begin{cases}
    1 & x^{(i)} \in C_k \\
    0 & otherwise
\end{cases}</script><p>或表示为</p>
<script type="math/tex; mode=display">
r^{(i)} = [0, ..., 1_k, ..., 0]^T</script><p>在迭代过程中，损失函数的值不断下降，优化目标为</p>
<script type="math/tex; mode=display">
\min J(\Omega)</script><h3 id="计算步骤"><a href="#计算步骤" class="headerlink" title="计算步骤"></a>计算步骤</h3><ol>
<li>随机选取$K$个中心点；</li>
<li>遍历所有数据，计算每个点到各中心点的距离；</li>
<li>将每个数据划分到最近的中心点中；</li>
<li>计算每个聚类的平均值，作为新的中心点；</li>
<li>重复步骤2-步骤4，直到这k个中线点不再变化(收敛)，或执行了足够多的迭代；</li>
</ol>
<p><code>K-means</code>更新迭代过程如下图<br><img src="/2018/11/16/Clustering/kmeans_example.gif" alt="kmeans_example"></p>
<h3 id="缺点与部分解决方法"><a href="#缺点与部分解决方法" class="headerlink" title="缺点与部分解决方法"></a>缺点与部分解决方法</h3><ul>
<li>局部最优</li>
<li>初值敏感<br>  初始点的选择会影响<code>K-means</code>聚类的结果，即可能会陷入局部最优解，如下图<br>  <img src="/2018/11/16/Clustering/k_means_init.png" alt="k_means_init"><br>  可通过如下方法解决<ul>
<li>多次选择初始点运行<code>K-means</code>算法，选择最优的作为输出结果；</li>
<li><code>K-means++</code></li>
</ul>
</li>
<li>需要定义<code>mean</code>，对于标称型<code>(categorical)</code>数据不适用</li>
<li>需要给定聚类簇数目$K$<br>  这里给出一种选择簇数目的方法，选择多个$K$值进行聚类，计算代价函数，做成折线图后如下，可以看到在$K=3$处损失值的变化率出现较大变化，则可选择簇的数目为$3$。<br>  <img src="/2018/11/16/Clustering/k_means_choose_K.png" alt="k_means_choose_K"></li>
<li>噪声数据干扰大</li>
<li>对于非凸集<code>(non-convex)</code>数据无能为力<br>  谱聚类可解决非凸集数据的聚类问题。</li>
</ul>
<h3 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h3><ul>
<li><code>K-means++</code><br>  改进初始点选择方法，第$1$个中心点随机选择；之后的初始中心点根据前面选择的中心点决定，若已选取$n$个初始聚类中心$(0&lt;n&lt;K)$，选取第$n+1$个聚类中心时，距离当前$n$个聚类中心越远的点会有更高的概率被选为第$n+1$个聚类中心。</li>
<li><code>ISODATA</code><br>  思想：当属于某个类别的样本数过少时把这个类别去除，当属于某个类别的样本数过多、分散程度较大时把这个类别分为两个子类别.</li>
<li><code>Kernel K-means</code><br>  参照支持向量机中核函数的思想，将所有样本映射到另外一个特征空间中再进行聚类。</li>
</ul>
<h3 id="类似的算法"><a href="#类似的算法" class="headerlink" title="类似的算法"></a>类似的算法</h3><p>与<code>K-means</code>类似的算法有很多，例如</p>
<ul>
<li><code>K-medoids</code><br>  <code>K-means</code>的取值范围可以是连续空间中的任意值，要求所有数据样本处在一个欧式空间中，对于有很多噪声的数据就会造成极大的误差。<code>K-medoids</code>的取值是数据样本范围中的样本，且可应用在非数值型数据样本上。</li>
<li><code>k-medians</code><br>  $K$中值，选择中位数更新各簇的中心点。</li>
<li><code>K-centers</code><br>  <a href="https://www.bjdxs.com/xueshu/28151.html" target="_blank" rel="noopener">混合类型数据的K-Centers聚类算法/The K-Centers Clustering Algorithm for Categorical and Mixe</a></li>
</ul>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p><a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/blob/master/Andrew%20Ng%2C%20Stanford%20University/mlclass-ex7-1-kmeans/KMeans.py" target="_blank" rel="noopener">@Github: K-Means</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line">class KMeans():</span><br><span class="line">    def __init__(self, n_cluster, mode):</span><br><span class="line">        self.n_cluster = n_cluster  # 簇的个数</span><br><span class="line">        self.mode = mode            # 距离度量方式</span><br><span class="line">        self.centroids = None       # 簇的中心</span><br><span class="line">        self.loss = float(&apos;inf&apos;)    # 优化目标值</span><br><span class="line">        plt.ion()</span><br><span class="line">    def fit(self, X, max_iter=5, min_move=0.1, display=False):</span><br><span class="line">        def initializeCentroids():</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            选择初始点</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            centroid = np.zeros(shape=(self.n_cluster, X.shape[1])) # 保存选出的点</span><br><span class="line">            pointIdx = []                                           # 保存已选出的点的索引</span><br><span class="line">            for n in range(self.n_cluster):</span><br><span class="line">                idx = np.random.randint(0, X.shape[0])              # 随机选择一个点</span><br><span class="line">                while idx in pointIdx:                              # 若该点已选出，则丢弃重新选择</span><br><span class="line">                    idx = np.random.randint(0, X.shape[0])</span><br><span class="line">                pointIdx.append(idx)</span><br><span class="line">                centroid[n] = X[idx]</span><br><span class="line">            return centroid</span><br><span class="line">        def dist2Centroids(x, centroids, mode):</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            返回向量x到k个中心点的距离值</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            d = np.zeros(shape=(self.n_cluster,))</span><br><span class="line">            for n in range(self.n_cluster):</span><br><span class="line">                d[n] = mathFunc.distance(x, centroids[n], mode)</span><br><span class="line">            return d</span><br><span class="line">        def nearestInfo(centroids, mode):</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            每个点最近的簇中心索引、距离</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            ctIdx = -np.ones(shape=(X.shape[0],), dtype=np.int8)    # 每个点最近的簇中心索引，初始化为-1，可作为异常条件</span><br><span class="line">            ctDist = np.ones(shape=(X.shape[0],), dtype=np.float)   # 每个点到最近簇中心的距离</span><br><span class="line">            for i in range(X.shape[0]):</span><br><span class="line">                dists = dist2Centroids(X[i], centroids, mode)</span><br><span class="line">                if mode == &apos;Euclidean&apos;: ctIdx[i] = np.argmin(dists)</span><br><span class="line">                elif mode == &apos;Cosine&apos;:  ctIdx[i] = np.argmax(dists)</span><br><span class="line">                ctDist[i] = dists[ctIdx[i]]             # 保存最相似的距离度量，用于计算loss</span><br><span class="line">            return ctIdx, ctDist</span><br><span class="line">        def updateCentroids(ctIdx):</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            更新簇中心</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            centroids = np.zeros(shape=(self.n_cluster, X.shape[1]))</span><br><span class="line">            for n in range(self.n_cluster):</span><br><span class="line">                X_ = X[ctIdx == n]                      # 筛选出离簇中心Cn最近的样本点</span><br><span class="line">                centroids[n] = np.mean(X_, axis=0)      # 根据筛选出的样本点更新中心值</span><br><span class="line">            return centroids</span><br><span class="line">        def loss(dist):</span><br><span class="line">            return np.mean(dist**2)</span><br><span class="line">        # -----------------------------------------</span><br><span class="line">        loss_min = float(&apos;inf&apos;)                         # 最优分类时的损失值，最小</span><br><span class="line">        n_iter = 0     </span><br><span class="line">        while n_iter &lt; max_iter:                        # 每次迭代选择不同的初始点</span><br><span class="line">            n_iter += 1; isDone = False                 # 表示本次迭代是否已收敛</span><br><span class="line">            centroids_tmp = initializeCentroids()       # 选择本次迭代的初始点</span><br><span class="line">            loss_last = float(&apos;inf&apos;)                    # 本次迭代中，中心点更新前的损失值</span><br><span class="line">            n_update = 0                                # 本次迭代的更新次数计数</span><br><span class="line">            while not isDone:</span><br><span class="line">                n_update += 1</span><br><span class="line">                ctIdx, ctDist = nearestInfo(centroids_tmp, mode=self.mode)</span><br><span class="line">                centroids_tmp = updateCentroids(ctIdx)  # 更新簇中心</span><br><span class="line">                # --- 可视化 ---</span><br><span class="line">                if (display==True) and (X.shape[1] == 2):</span><br><span class="line">                    plt.ion()</span><br><span class="line">                    plt.figure(n_iter); plt.cla()</span><br><span class="line">                    plt.scatter(X[:, 0], X[:, 1], c=ctIdx)</span><br><span class="line">                    plt.scatter(centroids_tmp[:, 0], centroids_tmp[:, 1], c=&apos;r&apos;)</span><br><span class="line">                    plt.pause(0.5)</span><br><span class="line">                # -------------</span><br><span class="line">                loss_now = loss(ctDist); moved = np.abs(loss_last - loss_now)</span><br><span class="line">                if moved &lt; min_move:                    # 若移动过小，则本次迭代收敛</span><br><span class="line">                    isDone = True</span><br><span class="line">                    print(&apos;第%d次迭代结束，中心点更新%d次&apos; % (n_iter, n_update))</span><br><span class="line">                else: loss_last = loss_now</span><br><span class="line">            if loss_now &lt; loss_min:</span><br><span class="line">                self.centroids = centroids_tmp          # 保存损失最小的模型(最优)</span><br><span class="line">                loss_min = loss_now</span><br><span class="line">                # print(&apos;聚类结果已更新&apos;)</span><br><span class="line">        self.loss = loss_min</span><br><span class="line">        print(&apos;=========== 迭代结束 ===========&apos;)</span><br><span class="line">    def predict(self, X):</span><br><span class="line">        &apos;&apos;&apos;</span><br><span class="line">        各个样本的最近簇中心索引</span><br><span class="line">        &apos;&apos;&apos;</span><br><span class="line">        labels = -np.ones(shape=(X.shape[0],), dtype=np.int)    # 初始化为-1，可用作异常条件</span><br><span class="line">        for i in range(X.shape[0]):</span><br><span class="line">            dists_i = np.zeros(shape=(self.n_cluster,))         # 保存X[i]到中心点Cn的距离</span><br><span class="line">            for n in range(self.n_cluster):</span><br><span class="line">                dists_i[n] = mathFunc.distance(X[i], self.centroids[n], mode=self.mode)</span><br><span class="line">            if self.mode == &apos;Euclidean&apos;:</span><br><span class="line">                labels[i] = np.argmin(dists_i)</span><br><span class="line">            elif self.mode == &apos;Cosine&apos;:</span><br><span class="line">                labels[i] = np.argmax(dists_i)</span><br><span class="line">        return labels</span><br></pre></td></tr></table></figure></p>
<p>簇数的选择代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def chooseBestK(X, start, stop, step=1, mode=&apos;Euclidean&apos;):</span><br><span class="line">    Ks = np.arange(start, stop + 1, step, dtype=np.int) # 待选择的K</span><br><span class="line">    Losses = np.zeros(shape=Ks.shape)                   # 保存不同K值时的最小损失值</span><br><span class="line">    for k in range(1, Ks.shape[0] + 1):                 # 对于不同的K，训练模型，计算损失</span><br><span class="line">        print(&apos;K = %d&apos;, k)</span><br><span class="line">        estimator = KMeans(n_cluster=k, mode=mode)</span><br><span class="line">        estimator.fit(X, max_iter=10, min_move=0.01, display=False)</span><br><span class="line">        Losses[k - 1] = estimator.loss</span><br><span class="line">    plt.ioff()</span><br><span class="line">    plt.figure(); plt.xlabel(&apos;n_clusters&apos;); plt.ylabel(&apos;loss&apos;)</span><br><span class="line">    plt.plot(Ks, Losses)                                # 做出loss-K曲线</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p>
<h2 id="均值漂移-Meanshift"><a href="#均值漂移-Meanshift" class="headerlink" title="均值漂移(Meanshift)"></a>均值漂移(Meanshift)</h2><p>本质是一个迭代的过程，能够在一组数据的密度分布中寻找到局部极值，比较稳定，而且是无参密度估计(不需要事先知道样本数据的概率密度分布函数，完全依靠对样本点的计算)，而且在采样充分的情况下，一定会收敛，即可以对服从任意分布的数据进行密度估计。</p>
<h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p>有一个滑动窗口的思想，即利用当前中心点一定范围内(通常为球域)的点迭代更新中心点，重复移动窗口，直到满足收敛条件。简单的说，<code>Meanshift</code>就是沿着密度上升的方向寻找同属一个簇的数据点。</p>
<p>定义点$x_0$的$\epsilon$球域如下</p>
<script type="math/tex; mode=display">
S_h(x_0) = \{ x | (x - x_0)^T (x - x_0) \leq \epsilon \}</script><p>若有$n$个点$(x_1, …, x_n)$落在中心点$ptCentroid$的邻域内，其分布如图<br><img src="/2018/11/16/Clustering/DBSCAN4.jpg" alt="DBSCAN4"></p>
<p>则偏移向量计算方式为</p>
<script type="math/tex; mode=display">
vecShift = \frac{1}{n} \sum_{i=1}^n (x_i - ptCentroid)</script><p>中心点更新公式为</p>
<script type="math/tex; mode=display">
ptCentroid := ptCentroid + vecShift</script><blockquote>
<p>展开后可发现，其更新公式即</p>
<script type="math/tex; mode=display">
vecShift
= \frac{1}{n} \sum_{i=1}^n x_i - ptCentroid</script><script type="math/tex; mode=display">
ptCentroid :=  \frac{1}{n} \sum_{i=1}^n x_i</script></blockquote>
<p><img src="/2018/11/16/Clustering/DBSCAN3.jpg" alt="DBSCAN3"></p>
<p>一个滑动窗口的动态更新过程如下图<br><img src="/2018/11/16/Clustering/meanshift_example1.gif" alt="meanshift_example1"><br>初始化多个滑动窗口进行<code>MeanShift</code>算法，其更新过程如下，其中每个黑点代表滑动窗口的质心，每个灰点代表一个数据点<br><img src="/2018/11/16/Clustering/meanshift_example2.gif" alt="meanshift_example2"></p>
<h3 id="高斯权重"><a href="#高斯权重" class="headerlink" title="高斯权重"></a>高斯权重</h3><p>基本思想是，距离当前中心点近的向量对更新结果权重大，而远的权重小，可减小远点的干扰，如下图，$vecShift_2$为高斯权重下的偏移向量<br><img src="/2018/11/16/Clustering/DBSCAN5.jpg" alt="DBSCAN5"></p>
<p>其偏移向量计算方式为</p>
<script type="math/tex; mode=display">
vecShift = \frac{1}{n} \sum_{i=1}^n w_i · (x_i - ptCentroid)</script><script type="math/tex; mode=display">
w_i = \frac{\kappa(x_i - ptCentroid)}{\sum_j \kappa(x_j - ptCentroid)}</script><p>其中</p>
<script type="math/tex; mode=display">
\kappa(z) = \frac{1}{\sqrt{2\pi}} \exp \left( - \frac{||z||^2}{2\sigma^2} \right)</script><p>中心点更新公式仍然为</p>
<script type="math/tex; mode=display">
ptCentroid := ptCentroid + vecShift</script><blockquote>
<p>展开也可得到</p>
<script type="math/tex; mode=display">
ptCentroid := \frac{\sum_{i=1}^n w_i x_i}{\sum_j w_j}</script></blockquote>
<h3 id="计算步骤-1"><a href="#计算步骤-1" class="headerlink" title="计算步骤"></a>计算步骤</h3><p>对于给定的$N$维数据集$X = (x^{(1)}, x^{(2)}, …, x^{(M)})$，指定邻域参数$\epsilon_0$，终止条件参数$\epsilon_1$，簇合并参数$\epsilon_2$，并指定样本距离度量方式，目标为将其划分为$K$个簇。</p>
<ol>
<li>初始化：<ul>
<li>在样本集中随机选择$K_0(K_0 \gg K)$个样本作为初始中心点，以邻域大小为$\epsilon_0$建立滑动窗口；</li>
<li>各个样本初始化一个标记向量，用于记录被各类别访问的次数；</li>
</ul>
</li>
<li>以单个滑动窗口分析，记其中心点为$ptCentroid$，找到滑动窗口内的所有点，记作集合$M$，认为这些点属于该滑动窗口所属的簇类别，同时，这些点被该簇访问的次数$+1$；</li>
<li>以$ptCentroid$为中心，计算其到集合$M$中各个元素的向量，以这些向量计算得到偏移向量$vecShift$；</li>
<li>更新中心点：$ptCentroid = ptCentroid + vecShift$，即滑动窗口沿着$vecShift$方向移动，距离为$||vecShift||$；</li>
<li>重复步骤$2-4$，直到$||vecShift||&lt;\epsilon_1$，保存当前中心点；</li>
<li>如果收敛时当前簇$ptCentroid$与其它已经存在的簇的中心的距离小于阈值$\epsilon_2$，那么这两个簇合并。否则，把当前簇作为新的簇类，增加$1$类；</li>
<li>重复迭代直到所有的点都被标记访问；</li>
<li>根据每个样本被各簇的访问频率，取访问频率最大的那个簇类别作为当前点集的所属类。</li>
</ol>
<blockquote>
<p>即不同类型的滑窗沿着密度上升的方向进行移动，对各样本点进行标记，最后将样本划分为标记最多的类别；当两类非常接近时，合并为一类。</p>
</blockquote>
<h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><p><a href="https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/blob/master/examples/p71_meanshift.py" target="_blank" rel="noopener">@Github: MeanShift</a></p>
<p>先定义了窗格对象<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SlidingWindow</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        centroid: &#123;ndarray(n_features,)&#125;</span></span><br><span class="line"><span class="string">        epsilon: &#123;float&#125; 滑动窗格大小，为半径的平方</span></span><br><span class="line"><span class="string">        sigma: &#123;float&#125; 高斯核函数的参数</span></span><br><span class="line"><span class="string">        label: &#123;int&#125; 该窗格的标记</span></span><br><span class="line"><span class="string">        X: &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        containIdx: &#123;ndarray(n_contain,)&#125; 窗格内包含点的索引</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, centroid, epsilon, sigma, label, X)</span>:</span></span><br><span class="line">        self.centroid = centroid</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        self.sigma = sigma</span><br><span class="line">        self.label = label</span><br><span class="line">        self.containIdx = self.updateContain(X)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">k</span><span class="params">(self, z)</span>:</span></span><br><span class="line">        <span class="string">""" 高斯核函数</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            z: &#123;ndarray(n_features,)&#125;</span></span><br><span class="line"><span class="string">        Notes:</span></span><br><span class="line"><span class="string">            - \kappa(z) = \frac&#123;1&#125;&#123;\sqrt&#123;2\pi&#125;&#125; \exp \left( - \frac&#123;||z||^2&#125;&#123;2\sigma^2&#125; \right)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        norm = np.linalg.norm(z)</span><br><span class="line">        <span class="keyword">return</span> np.exp(- <span class="number">0.5</span> * (norm / self.sigma)**<span class="number">2</span>) / np.sqrt(<span class="number">2</span>*np.pi)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" 更新滑动窗格的中心点和所包含点</span></span><br><span class="line"><span class="string">        Returns: &#123;float&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        dshift = self.shift(X)</span><br><span class="line">        self.containIdx = self.updateContain(X)</span><br><span class="line">        <span class="keyword">return</span> dshift</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">shift</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" 移动窗格</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            vecShift: &#123;ndarray(n_features,)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            dshift: &#123;float&#125; 移动的距离</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        (n_samples, n_features) = X.shape</span><br><span class="line">        n_contain = self.containIdx.shape[<span class="number">0</span>]</span><br><span class="line">        contain_weighted_sum = np.zeros(shape=(n_features, ))</span><br><span class="line">        weight_sum = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 按包含的点进行移动</span></span><br><span class="line">        <span class="keyword">for</span> i_contain <span class="keyword">in</span> range(n_contain):</span><br><span class="line">            vector = X[self.containIdx[i_contain]] - self.centroid</span><br><span class="line">            weight = self.k(vector)</span><br><span class="line">            contain_weighted_sum += weight*X[self.containIdx[i_contain]]</span><br><span class="line">            weight_sum += weight</span><br><span class="line">        centroid = contain_weighted_sum / weight_sum	 </span><br><span class="line">        <span class="comment"># 计算移动的距离   </span></span><br><span class="line">        dshift = np.linalg.norm(self.centroid - centroid)</span><br><span class="line">        self.centroid = centroid</span><br><span class="line">        <span class="keyword">return</span> dshift</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">updateContain</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" 更新窗格内的点索引</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            X: &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Notes:</span></span><br><span class="line"><span class="string">            - 用欧式距离作为度量</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        d = <span class="keyword">lambda</span> x_i, x_j: np.linalg.norm(x_i - x_j)</span><br><span class="line">        n_samples = X.shape[<span class="number">0</span>]</span><br><span class="line">        containIdx = np.array([], dtype=<span class="string">'int'</span>)</span><br><span class="line">        <span class="keyword">for</span> i_samples <span class="keyword">in</span> range(n_samples):</span><br><span class="line">            <span class="keyword">if</span> d(X[i_samples], self.centroid) &lt; self.epsilon:</span><br><span class="line">                containIdx = np.r_[containIdx, i_samples]</span><br><span class="line">        <span class="keyword">return</span> containIdx</span><br></pre></td></tr></table></figure></p>
<p>聚类算法如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MeanShift</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        n_clusters: &#123;int&#125; 划分簇的个数</span></span><br><span class="line"><span class="string">        n_windows: &#123;int&#125; 滑动窗格的个数</span></span><br><span class="line"><span class="string">        epsilon: &#123;float&#125; 滑动窗格的大小</span></span><br><span class="line"><span class="string">        sigma: &#123;float&#125; &#123;float&#125; 高斯核参数</span></span><br><span class="line"><span class="string">        thresh: &#123;float&#125; 若两个窗格中心距离小于thresh，则合并两类簇</span></span><br><span class="line"><span class="string">        min_move: &#123;float&#125; 终止条件</span></span><br><span class="line"><span class="string">        windows: &#123;list[class SlidingWindow()]&#125;</span></span><br><span class="line"><span class="string">    Note:</span></span><br><span class="line"><span class="string">        - 假设所有点均被窗格划过</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_clusters, n_windows=<span class="number">-1</span>, epsilon=<span class="number">0.5</span>, sigma=<span class="number">2</span>, thresh=<span class="number">1e-2</span>, min_move=<span class="number">1e-3</span>)</span>:</span></span><br><span class="line">        self.n_clusters = n_clusters</span><br><span class="line">        self.n_windows = <span class="number">5</span>*n_clusters <span class="keyword">if</span> (n_windows == <span class="number">-1</span>) <span class="keyword">else</span> n_windows</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        self.sigma = sigma</span><br><span class="line">        self.thresh = thresh</span><br><span class="line">        self.min_move = min_move</span><br><span class="line">        self.windows = []</span><br><span class="line">        self.centroids = <span class="keyword">None</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        (n_samples, n_features) = X.shape</span><br><span class="line">        <span class="comment"># 创建窗格</span></span><br><span class="line">        <span class="keyword">for</span> i_windows <span class="keyword">in</span> range(self.n_windows):</span><br><span class="line">            idx = np.random.randint(n_samples)</span><br><span class="line">            window = SlidingWindow(X[idx], self.epsilon,</span><br><span class="line">                            self.sigma, i_windows, X)</span><br><span class="line">            <span class="comment"># 将各窗格包含的点标记</span></span><br><span class="line">            n_contain = window.containIdx.shape[<span class="number">0</span>]</span><br><span class="line">            self.windows.append(window)</span><br><span class="line"></span><br><span class="line">        dshift = float(<span class="string">'inf'</span>)   <span class="comment"># 初始化为无穷大</span></span><br><span class="line">        plt.figure(); plt.ion()</span><br><span class="line">        <span class="keyword">while</span> dshift &gt; self.min_move:</span><br><span class="line">            <span class="comment"># ------ 做图显示 ------</span></span><br><span class="line">            plt.cla()</span><br><span class="line">            plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=<span class="string">'b'</span>)</span><br><span class="line">            <span class="keyword">for</span> i_windows <span class="keyword">in</span> range(self.n_windows):</span><br><span class="line">                centroid = self.windows[i_windows].centroid</span><br><span class="line">                plt.scatter(centroid[<span class="number">0</span>], centroid[<span class="number">1</span>], c=<span class="string">'r'</span>)</span><br><span class="line">            plt.pause(<span class="number">0.5</span>)</span><br><span class="line">            <span class="comment"># ---------------------</span></span><br><span class="line">            dshift = self.step(X)</span><br><span class="line">        plt.ioff()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 合并窗格</span></span><br><span class="line">        dists = np.zeros(shape=(self.n_windows, self.n_windows))</span><br><span class="line">        <span class="keyword">for</span> i_windows <span class="keyword">in</span> range(self.n_windows):</span><br><span class="line">            <span class="keyword">for</span> j_windows <span class="keyword">in</span> range(i_windows):</span><br><span class="line">                centroid_i = self.windows[i_windows].centroid</span><br><span class="line">                centroid_j = self.windows[j_windows].centroid</span><br><span class="line">                dists[i_windows, j_windows] = np.linalg.norm(centroid_i-centroid_j)</span><br><span class="line">                dists[j_windows, i_windows] = dists[i_windows, j_windows]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 获得距离相近索引</span></span><br><span class="line">        index = np.where(dists&lt;self.thresh)</span><br><span class="line">        <span class="comment"># 用于标记类别</span></span><br><span class="line">        winlabel = np.zeros(shape=(self.n_windows,), dtype=<span class="string">'int'</span>)</span><br><span class="line">        label = <span class="number">1</span>; winlabel[<span class="number">0</span>] = label</span><br><span class="line">        <span class="keyword">for</span> i_windows <span class="keyword">in</span> range(self.n_windows):</span><br><span class="line">            idx_row = index[<span class="number">0</span>][i_windows]</span><br><span class="line">            idx_col = index[<span class="number">1</span>][i_windows]</span><br><span class="line">            <span class="comment"># 若其中一个点被标记，则将令一个点并入该类</span></span><br><span class="line">            <span class="keyword">if</span> winlabel[idx_row]!=<span class="number">0</span>:</span><br><span class="line">                winlabel[idx_col] = winlabel[idx_row]</span><br><span class="line">            <span class="keyword">elif</span> winlabel[idx_col]!=<span class="number">0</span>:</span><br><span class="line">                winlabel[idx_row] = winlabel[idx_col]</span><br><span class="line">            <span class="comment"># 否则新创建类别</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                label += <span class="number">1</span></span><br><span class="line">                winlabel[idx_row] = label</span><br><span class="line">                winlabel[idx_col] = label</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将标签一样的窗格合并</span></span><br><span class="line">        labels = list(set(winlabel))                            <span class="comment"># 去重后的标签</span></span><br><span class="line">        n_labels = len(labels)                                  <span class="comment"># 标签种类数</span></span><br><span class="line">        self.centroids = np.zeros(shape=(n_labels, n_features)) <span class="comment"># 记录最终聚类中心</span></span><br><span class="line">        <span class="keyword">for</span> i_labels <span class="keyword">in</span> range(n_labels):</span><br><span class="line">            cnt = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i_windows <span class="keyword">in</span> range(self.n_windows):</span><br><span class="line">                <span class="keyword">if</span> winlabel[i_windows] == labels[i_labels]:</span><br><span class="line">                    self.centroids[i_labels] += self.windows[i_windows].centroid</span><br><span class="line">                    cnt += <span class="number">1</span></span><br><span class="line">            self.centroids[i_labels] /= cnt	                    <span class="comment"># 取同类窗格中心点的均值</span></span><br><span class="line">        <span class="keyword">return</span> self.centroids</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" update all sliding windows</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            dshift: \sum_i^&#123;n_windows&#125; dshift_&#123;i&#125;</span></span><br><span class="line"><span class="string">        """</span> </span><br><span class="line">        dshift = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i_windows <span class="keyword">in</span> range(self.n_windows):</span><br><span class="line">            dshift += self.windows[i_windows].step(X)</span><br><span class="line">            <span class="comment"># label the points</span></span><br><span class="line">            n_contain = self.windows[i_windows].containIdx.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> dshift</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" 简单的用近邻的方法求</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        (n_samples, n_features) = X.shape</span><br><span class="line">        dists = np.zeros(shape=(n_samples, self.n_clusters))</span><br><span class="line">        <span class="keyword">for</span> i_samples <span class="keyword">in</span> range(n_samples):</span><br><span class="line">            <span class="keyword">for</span> i_clusters <span class="keyword">in</span> range(self.n_clusters):</span><br><span class="line">                dists[i_samples, i_clusters] = np.linalg.norm(X[i_samples]-self.centroids[i_clusters])</span><br><span class="line">        <span class="keyword">return</span> np.argmin(dists, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="谱聚类-Spectral-Clustering"><a href="#谱聚类-Spectral-Clustering" class="headerlink" title="谱聚类(Spectral Clustering)"></a>谱聚类(Spectral Clustering)</h2><p>谱聚类是从图论中演化出来的算法，后来在聚类中得到了广泛的应用，比起传统的<code>K-Means</code>算法，谱聚类对数据分布的适应性更强，聚类效果也很优秀，同时聚类的计算量也小很多。</p>
<h3 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h3><blockquote>
<p><a href="https://www.cnblogs.com/pinard/p/6221564.html" target="_blank" rel="noopener">谱聚类（spectral clustering）原理总结 - 刘建平Pinard - 博客园 </a></p>
</blockquote>
<h4 id="无向权重图"><a href="#无向权重图" class="headerlink" title="无向权重图"></a>无向权重图</h4><p>我们用点的集合$V$和边的集合$E$描述一个图，即$G(V, E)$，其中$V$即数据集中的点</p>
<script type="math/tex; mode=display">
V = [v_1, v_2, ..., v_n]</script><p>而点$v_i, v_j$间连接权值$w_{ij}$组成邻接矩阵$W$，由于为无向图，故满足$w_{ij}=w_{ji}$</p>
<script type="math/tex; mode=display">
W = \left[
    \begin{matrix}
        w_{11} & ... & w_{1n} \\
        ... & ... & ... \\
        w_{n1} & ... & w_{nn} \\
    \end{matrix}
    \right]</script><p>对于图中的任意一个点$v_i$，定义其度$d_i$为</p>
<script type="math/tex; mode=display">
d_i = \sum_{j=1}^n w_{ij}</script><p>则我们可以得到一个度矩阵$D=diag(d_1, …, d_n)$</p>
<script type="math/tex; mode=display">
D = \left[
        \begin{matrix}
            d_1 & & \\
            & ... & \\
             & & d_n\\
        \end{matrix}
    \right]</script><p>除此之外，对于$V$中子集$V_{sub} \subset V$，定义子集$V_{sub}$点的个数为</p>
<script type="math/tex; mode=display">
|V_{sub}| := n_{sub}</script><p>另外，定义该子集中点的度之和为</p>
<script type="math/tex; mode=display">
vol(V_{sub}) = \sum_{i \in V_{sub}} d_i</script><h4 id="相似矩阵"><a href="#相似矩阵" class="headerlink" title="相似矩阵"></a>相似矩阵</h4><p>上面讲到的邻接矩阵$W$可以指定权值，但对于数据量庞大的数据集，这显然不是一个$wise$的选择。我们可以用相似矩阵$S$来获得邻接矩阵$W$，基本思想是，距离较远的两个点之间的边权重值较低，而距离较近的两个点之间的边权重值较高。</p>
<p>构建邻接矩阵$W$的方法有三类：$\epsilon$-邻近法，$K$邻近法和全连接法，定义距离</p>
<script type="math/tex; mode=display">
d_{ij} = ||x^{(i)} - x^{(j)}||_2^2</script><ul>
<li><p>$\epsilon$-邻近法<br>  设置距离阈值$\epsilon$，用欧式距离度量两点的距离$d_{ij}$，然后通过下式确定邻接权值$w_{ij}$</p>
<script type="math/tex; mode=display">
  w_{ij} = \begin{cases}
      0 & d_{ij} > \epsilon \\
      \epsilon & otherwise
  \end{cases}</script><blockquote>
<p>两点间的权重要不就是$\epsilon$，要不就是0，距离远近度量很不精确，因此在实际应用中，我们很少使用$\epsilon$-邻近法。</p>
</blockquote>
</li>
<li><p>$K$邻近法</p>
<ul>
<li><p>第一种<br>  只要一个点在另一个点的$K$近邻中，则保留$d_{ij}$</p>
<script type="math/tex; mode=display">
  w_{ij} = \begin{cases}
      \exp \left( -\frac{d_{ij}}{2\sigma^2} \right) & x^{(i)} \in KNN(x^{(j)})　or　x^{(j)} \in KNN(x^{(i)}) \\
      0 & otherwise
  \end{cases}</script></li>
<li><p>第二种<br>  互为$K$近邻时保留$d_{ij}$</p>
<script type="math/tex; mode=display">
  w_{ij} = \begin{cases}
      \exp \left( -\frac{d_{ij}}{2\sigma^2} \right) & x^{(i)} \in KNN(x^{(j)})　and　x^{(j)} \in KNN(x^{(i)}) \\
      0 & otherwise
  \end{cases}</script></li>
</ul>
</li>
<li><p>全连接法<br>  可以选择不同的核函数来定义边权重，常用的有多项式核函数，高斯核函数和<code>Sigmoid</code>核函数。最常用的是高斯核函数<code>RBF</code>，此时相似矩阵和邻接矩阵相同</p>
<script type="math/tex; mode=display">
  w_{ij} = \exp \left( -\frac{d_{ij}}{2\sigma^2} \right)</script></li>
</ul>
<h4 id="拉普拉斯矩阵-Graph-Laplacians"><a href="#拉普拉斯矩阵-Graph-Laplacians" class="headerlink" title="拉普拉斯矩阵(Graph Laplacians)"></a>拉普拉斯矩阵(Graph Laplacians)</h4><p>定义</p>
<script type="math/tex; mode=display">
L = D - W</script><p>正则化的拉普拉斯矩阵为</p>
<script type="math/tex; mode=display">
L = D^{-1} (D - W)</script><p>具有的性质如下</p>
<ol>
<li>$L^T = L$</li>
<li>其特征值均为实数，即$\lambda_i \in \mathbb{R}$</li>
<li>正定性：$\lambda_i \geq 0$</li>
<li><p>对于任意向量$x$，都有</p>
<script type="math/tex; mode=display">
x^T L x = \frac{1}{2} \sum_{i,j} w_{ij} (x_i - x_j)^2</script><blockquote>
<p>证明：</p>
<script type="math/tex; mode=display">
x^T L x = x^T D x - x^T W x = \sum_i d_i x_i^2 - \sum_{ij} w_{ij} x_i x_j</script><script type="math/tex; mode=display">
= \frac{1}{2} \left[ \sum_i d_i x_i^2 - 2\sum_{ij} w_{ij} x_i x_j + \sum_j d_j x_j^2 \right]</script><p>其中$ d_i = \sum_j w_{ij} $，所以</p>
<script type="math/tex; mode=display">
x^T L x = \frac{1}{2} \sum_{i,j} w_{ij} (x_i - x_j)^2</script></blockquote>
</li>
</ol>
<h4 id="无向图的切图"><a href="#无向图的切图" class="headerlink" title="无向图的切图"></a>无向图的切图</h4><h5 id="cut"><a href="#cut" class="headerlink" title="cut"></a>cut</h5><p>我们希望把一张无向图$G(V, E)$按一定方法切成多个子图，各个子图间无连接，每个子图的点集为$V_1, …, V_K$，满足</p>
<ul>
<li>$\bigcup_{k=1}^K V_k = V$</li>
<li>$V_i \cap V_j = \emptyset$</li>
</ul>
<p>定义两个子图点集合$A, B$之间的切图权重为</p>
<script type="math/tex; mode=display">
W(A, B) = \sum_{i \in A, j \in B} w_{ij}</script><blockquote>
<p>共有$n_A × n_B$个权值作累加</p>
</blockquote>
<p>那么对于$K$个子图点的集合$V_1, …, V_K$，定义切图为</p>
<script type="math/tex; mode=display">
cut(V_1, ..., V_K) = \frac{1}{2} \sum_{i=1}^K cut(V_i, \overline{V_i})</script><script type="math/tex; mode=display">
cut(V_i, \overline{V_i}) = W (V_i, \overline{V_i})</script><p>其中$\overline{V_i}$表示$V_i$的补集，或者</p>
<script type="math/tex; mode=display">
\overline{V_i} = \bigcup_{k \neq i} V_k</script><p>通过最小化$cut(V_1, …, V_K)$使子图内权重和大，而子图间权重和小。但是这种方法存在问题，如下图<br><img src="/2018/11/16/Clustering/cut.jpg" alt="cut"></p>
<p>选择一个权重最小的边缘的<strong>点</strong>，比如$C$和$H$之间进行$cut$，这样可以最小化$cut(V_1, …, V_K)$，但是却不是最优的切图。</p>
<p>为解决上述问题，需要对每个子图的规模做出限定，以下介绍两种切图方式。</p>
<h5 id="Ratio-Cut"><a href="#Ratio-Cut" class="headerlink" title="Ratio Cut"></a>Ratio Cut</h5><p>不仅考虑最小化$cut(V_1, …, V_K)$，而且最大化每个子图的点个数，即</p>
<script type="math/tex; mode=display">
RatioCut(V_1, ..., V_K) = \frac{1}{2} \sum_k \frac{cut(V_i, \overline{V_i})}{|V_k|}</script><script type="math/tex; mode=display">
cut(V_i, \overline{V_i}) = W (V_i, \overline{V_i})</script><blockquote>
<ul>
<li>$W(V_k, \overline{V_k}) = \sum_{i \in V_k, j \in \overline{V_k}} w_{ij}$</li>
<li>$|V_k| = n_k$</li>
</ul>
</blockquote>
<p>如果按照遍历的方法求解，由前面分析，$W(V_k, \overline{V_k})$需计算$n_{V_k} × n_{\overline{V_k}}$次累加，计算量庞大，那么如何求解呢？</p>
<p>定义指示向量$h_k$，其构成矩阵$H$</p>
<script type="math/tex; mode=display">
H = [ h_1, ..., h_k, ..., h_K]</script><p>其中</p>
<script type="math/tex; mode=display">
h_k = \left[h_{k1}, h_{k2}, , ..., h_{kM} \right]^T</script><script type="math/tex; mode=display">
h_{ki} = \begin{cases}
    \frac{1}{\sqrt{|V_k|}} & x^{(i)}\in V_k \\
    0 & otherwise
\end{cases}</script><blockquote>
<p>$h_k$为单位向量，且两两正交</p>
<script type="math/tex; mode=display">
h_i^T h_j = 
      \begin{cases}
          \sum_{|V_i|} \frac{1}{|V_i|} = |V_i| · \frac{1}{|V_i|} = 1 & i = j \\
          0 & i \neq j
      \end{cases}</script></blockquote>
<p>那么由拉式矩阵性质$4$</p>
<script type="math/tex; mode=display">
h_k^T L h_k = \frac{1}{2} \sum_{i,j} w_{ij} (h_{ki} - h_{kj})^2</script><script type="math/tex; mode=display">
= \frac{1}{2} 
    [
        \sum_{i \in V_k, j \in V_k} w_{ij} (h_{ki} - h_{kj})^2 + 
        \sum_{i \notin V_k, j \in V_k} w_{ij} (h_{ki} - h_{kj})^2 +</script><script type="math/tex; mode=display">
        \sum_{i \in V_k, j \notin V_k} w_{ij} (h_{ki} - h_{kj})^2 + 
        \sum_{i \notin V_k, j \notin V_k} w_{ij} (h_{ki} - h_{kj})^2
    ]</script><script type="math/tex; mode=display">
= \frac{1}{2}
    [
        \sum_{i \in V_k, j \in V_k} w_{ij} (\frac{1}{\sqrt{|V_k|}} - \frac{1}{\sqrt{|V_k|}})^2 + 
        \sum_{i \notin V_k, j \in V_k} w_{ij} (0 - \frac{1}{\sqrt{|V_k|}})^2 +</script><script type="math/tex; mode=display">
        \sum_{i \in V_k, j \notin V_k} w_{ij} (\frac{1}{\sqrt{|V_k|}} - 0)^2 +
        \sum_{i \notin V_k, j \notin V_k} w_{ij} (0 - 0)^2
    ]</script><script type="math/tex; mode=display">
= \frac{1}{2}
    [
        \sum_{i \notin V_k, j \in V_k} w_{ij} \frac{1}{|V_k|} +
        \sum_{i \in V_k, j \notin V_k} w_{ij} \frac{1}{|V_k|}
    ]</script><blockquote>
<script type="math/tex; mode=display">cut(V_i, \overline{V_i}) = W (V_i, \overline{V_i}) = \sum_{i \in V_k, j \in \overline{V_k}} w_{ij}</script></blockquote>
<script type="math/tex; mode=display">
h_k^T L h_k = \frac{1}{2} [\frac{1}{|V_k|} cut(V_k, \overline{V_k}) + \frac{1}{|V_k|} cut(V_k, \overline{V_k})]</script><script type="math/tex; mode=display">
= \frac{1}{|V_k|} cut(V_k, \overline{V_k})</script><p>推到这里就能理解为什么要定义$h_k$了</p>
<script type="math/tex; mode=display">
RatioCut(V_1, ..., V_K)
= \frac{1}{2} \sum_k h_k^T L h_k</script><p>并且</p>
<script type="math/tex; mode=display">
h_k^T L h_k = tr(H^T L H)</script><blockquote>
<script type="math/tex; mode=display">
H^T L H 
= \left[
      \begin{matrix}
          — & h_1^T & — \\
           & ... &  \\
          — & h_K^T & — \\
      \end{matrix}
\right]
L
\left[  
        \begin{matrix}
            | & & | \\
            h_1 & ... & h_K \\
            | & & |
        \end{matrix}
    \right]</script><script type="math/tex; mode=display">
= \left[
      \begin{matrix}
          h_1^T L h_1 & ... & h_1^T L h_K \\
          ... & ... & ... \\
          h_K^T L h_K & ... & h_K^T L h_K \\
      \end{matrix}
\right]</script></blockquote>
<p>所以最终优化目标为</p>
<script type="math/tex; mode=display">
\min_H tr(H^T L H)</script><script type="math/tex; mode=display">
s.t.　H^T H = I</script><blockquote>
<script type="math/tex; mode=display">
H^T H = \left[
      \begin{matrix}
          h_1^T h_1 & ... & h_1^T h_K \\
          ... & ... & ... \\
          h_K^T h_K & ... & h_K^T h_K \\
      \end{matrix}
\right]</script></blockquote>
<p>而矩阵的正交相似变换$A = P \Lambda P^{-1}$满足</p>
<script type="math/tex; mode=display">
tr(A) = tr(\Lambda) = \sum_i \lambda_i</script><p>故</p>
<script type="math/tex; mode=display">
tr(H^T L H) = tr(L) = \sum_{i=1}^M \lambda_i</script><p>$\lambda_i$为矩阵$L$的特征值。</p>
<p>我们再进行维度规约，将维度从$M$降到$k_1$，即找到$k_1$个最小的特征值之和。</p>
<h5 id="N-Cut"><a href="#N-Cut" class="headerlink" title="N Cut"></a>N Cut</h5><p>推导过程与<code>RatioCut</code>完全一致，只是将分母$|V_i|$换成$vol(V_i)$</p>
<script type="math/tex; mode=display">
NCut(V_1, ..., V_K) = \frac{1}{2} \sum_k \frac{cut(V_i, \overline{V_i})}{vol(V_i)}</script><script type="math/tex; mode=display">
cut(V_i, \overline{V_i}) = W (V_i, \overline{V_i})</script><blockquote>
<script type="math/tex; mode=display">vol(V_{sub}) = \sum_{i \in V_{sub}} d_i</script></blockquote>
<h3 id="计算步骤-2"><a href="#计算步骤-2" class="headerlink" title="计算步骤"></a>计算步骤</h3><p>对于给定的$N$维数据集$X = (x^{(1)}, x^{(2)}, …, x^{(M)})$，将其划分为$K$类$(C_1, …, C_K)$</p>
<ol>
<li>根据输入的相似矩阵的生成方式构建样本的相似矩阵$S_{M×M}$；</li>
<li>根据相似矩阵$S$构建邻接矩阵$W_{M×M}$；</li>
<li>构建度矩阵$D_{M×M}$；</li>
<li>计算拉普拉斯矩阵$L_{M×M}$，可进行规范化$ L := D^{-1}L $；</li>
<li>对$L$进行特征值分解<code>(EVD)</code>，得到特征对$ (\lambda_i, \alpha_i), i=1,…,M $；</li>
<li>指定超参数$K_1$，选取$K_1$个最小特征值对应的特征向量组成矩阵$F_{M×K_1}$，并将其按行标准化；</li>
<li>以$F$的行向量作为新的样本数($k_1$维，这里也有降维操作)进行聚类，划分为$K$类，可使用<code>K-means</code>；</li>
<li>聚类结果即为输出结果</li>
</ol>
<p>注意是$K_1$个最小特征值对应的特征向量，别问我为什么知道。。。</p>
<h3 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h3><p><a href="https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/blob/master/examples/p86_spectral_clustering.py" target="_blank" rel="noopener">@Github: Spectral Clustering</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpectralClustering</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        k: &#123;int&#125;, k &lt; n_samples</span></span><br><span class="line"><span class="string">        sigma: &#123;float&#125;</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        Steps:</span></span><br><span class="line"><span class="string">            - similarity matrix [W_&#123;n×n&#125;]</span></span><br><span class="line"><span class="string">            - diagonal matrix [D_&#123;n×n&#125;] is defined as</span></span><br><span class="line"><span class="string">                    D_&#123;ii&#125; = \begin&#123;cases&#125;</span></span><br><span class="line"><span class="string">                                \sum_j W_&#123;ij&#125; &amp; i \neq j \\</span></span><br><span class="line"><span class="string">                                0 &amp; i = j</span></span><br><span class="line"><span class="string">                            \end&#123;cases&#125;</span></span><br><span class="line"><span class="string">            - Laplacian matrix [L_&#123;n×n&#125;], Laplacian matrix is defined as</span></span><br><span class="line"><span class="string">                    L = D - W or L = D^&#123;-1&#125; (D - W)</span></span><br><span class="line"><span class="string">            - EVD: L \alpha_i = \lambda_i \alpha_i</span></span><br><span class="line"><span class="string">            - takes the eigenvector corresponding to the largest eigenvalue as</span></span><br><span class="line"><span class="string">                    B_&#123;n×k&#125; = [\beta_1, \beta_2, ..., \beta_k]</span></span><br><span class="line"><span class="string">            - apply K-Means to the row vectors of matrix B</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, k, n_clusters=<span class="number">2</span>, sigma=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">        self.kmeans = KMeans(n_clusters=n_clusters)</span><br><span class="line">        self.k = k</span><br><span class="line">        self.sigma = sigma</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        n_samples = X.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># step 1</span></span><br><span class="line">        kernelGaussian = <span class="keyword">lambda</span> z, sigma: np.exp(<span class="number">-0.5</span> * np.square(z/sigma))</span><br><span class="line">        W = np.zeros((n_samples, n_samples))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_samples):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i):</span><br><span class="line">                W[i, j] = kernelGaussian(np.linalg.norm(X[i] - X[j]), self.sigma)</span><br><span class="line">                W[j, i] = W[i, j]</span><br><span class="line">        <span class="comment"># step 2</span></span><br><span class="line">        D = np.diag(np.sum(W, axis=<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># step 3</span></span><br><span class="line">        L = D - W</span><br><span class="line">        L = np.linalg.inv(D).dot(L)</span><br><span class="line">        <span class="comment"># step 4</span></span><br><span class="line">        eigval, eigvec = np.linalg.eig(L)</span><br><span class="line">        <span class="comment"># step 5</span></span><br><span class="line">        order = np.argsort(eigval)</span><br><span class="line">        eigvec = eigvec[:, order]</span><br><span class="line">        beta = eigvec[:, :self.k]</span><br><span class="line">        <span class="comment"># step 6</span></span><br><span class="line">        self.kmeans.fit(beta)</span><br><span class="line">        <span class="keyword">return</span> self.kmeans.labels_</span><br></pre></td></tr></table></figure></p>
<p><img src="/2018/11/16/Clustering/spectral_clustering.png" alt="spectral_clustering"></p>
<h2 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h2><p><code>DBSCAN(Density-Based Spatial Clustering of Applications with Noise)</code>，具有噪声的基于密度的聚类方法。假定类别可以通过样本分布的紧密程度决定。同一类别的样本，他们之间的紧密相连的。</p>
<blockquote>
<p><a href="https://www.cnblogs.com/pinard/p/6208966.html" target="_blank" rel="noopener">DBSCAN密度聚类算法 - 刘建平Pinard - 博客园</a></p>
</blockquote>
<h3 id="原理-3"><a href="#原理-3" class="headerlink" title="原理"></a>原理</h3><p>先介绍几个关于密度的概念</p>
<ul>
<li><p>$\epsilon$-邻域<br>  对于样本$x^{(i)}$，其$\epsilon$-邻域包含样本集中与$x^{(i)}$距离不大于$\epsilon$的子样本集，其样本个数记作$|N_{\epsilon}(x^{(i)})|$。</p>
<script type="math/tex; mode=display">
  N_{\epsilon}(x^{(i)}) = \{ x^{(j)} | d_{ij} \leq \epsilon \}</script></li>
<li><p>核心对象<br>  对于任一样本$x^{(i)}$，若其$\epsilon$-邻域$N_{\epsilon}(x^{(i)})$至少包含$minPts$个样本，则该样本为核心对象。如图，选择若选取$\epsilon=5$，则红点均为核心对象</p>
</li>
<li>密度直达<br>  若样本$x^{(j)} \in N_{\epsilon}(x^{(i)})$，且$x^{(i)}$为核心对象，则称$x^{(j)}$由$x^{(i)}$密度直达。不满足对称性，即反之不一定成立，除非$x^{(j)}$也为核心对象。如图，$x^{(8)}$可由$x^{(6)}$密度直达，而反之$x^{(6)}$不可由$x^{(8)}$密度直达，因为$x^{(8)}$不为核心对象。</li>
<li>密度可达<br>  若存在样本序列$p_1, p_2, …, p_T$，满足$p_1 = x^{(i)}, p_T = x^{(j)}$，且$p_{t+1}$可由$p_t$密度直达，也就是说$p_1, p_2, …, p_{T-1}$均为核心对象，则称$x^{(j)}$由$x^{(i)}$密度可达。也不满足对称性。如图，$x^{(4)}$可由$x^{(1)}$密度可达，而$x^{(2)}$不可由$x^{(4)}$密度可达，因为$x^{(4)}$不为核心对象。</li>
<li>密度相连<br>  存在核心对象$x^{(k)}$，使得$x^{(i)}$与$x^{(j)}$均由$x^{(k)}$密度可达，则称$x^{(i)}$与$x^{(j)}$密度相连。注意密度相连满足对称性。如图，$x^{(8)}$与$x^{(4)}$均可由$x^{(1)}$密度可达，则$x^{(8)}$与$x^{(4)}$密度相连。</li>
</ul>
<p><img src="/2018/11/16/Clustering/DBSCAN1.jpg" alt="DBSCAN1"></p>
<h3 id="计算思想"><a href="#计算思想" class="headerlink" title="计算思想"></a>计算思想</h3><p><code>DBSCAN</code>的聚类思想是，由<strong>密度可达关系</strong>导出的最大密度相连的样本集合，即为我们最终聚类的一个簇，这个簇里可能只有一个核心对象，也可能有多个核心对象，若有多个，则簇里的任意一个核心对象的$\epsilon$-邻域中一定有一个其他的核心对象，否则这两个核心对象无法密度可达。</p>
<p>另外，考虑以下三个问题</p>
<ul>
<li>噪音点<br>  一些异常样本点或者说少量游离于簇外的样本点，这些点不在任何一个核心对象在周围，这些样本点标记为噪音点，<code>with Noise</code>就是这个意思。</li>
<li>距离的度量<br>  一般采用最近邻思想，采用某一种距离度量来衡量样本距离，比如欧式距离。这和<code>KNN</code>算法的最近邻思想完全相同。对应少量的样本，寻找最近邻可以直接去计算所有样本的距离，如果样本量较大，则一般采用<code>KDTree</code>或者球树来快速的搜索最近邻。</li>
<li>类别重复时的判别<br>  某些样本可能到两个核心对象的距离都小于$\epsilon$，但是这两个核心对象如下图所示，不是密度直达，又不属于同一个聚类簇，那么如果界定这个样本的类别呢？<br>  <img src="/2018/11/16/Clustering/DBSCAN2.jpg" alt="DBSCAN2"><br>  一般来说，此时<code>DBSCAN</code>采用<strong>先来后到</strong>，先进行聚类的类别簇会标记这个样本为它的类别。也就是说<code>BDSCAN</code>不是完全稳定的算法。</li>
</ul>
<h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><p>对于给定的$N$维数据集$X = (x^{(1)}, x^{(2)}, …, x^{(M)})$，指定邻域参数$(\epsilon, minPts)$与样本距离度量方式，将其划分为$K$类。</p>
<ol>
<li>检测数据库中尚未检查过的对象$p$，如果$p$未被处理(归为某个簇或者标记为噪声)，则检查其邻域：<ul>
<li>若包含的对象数不小于$minPts$，建立新簇$C$，将其中的所有点加入候选集$N$；</li>
</ul>
</li>
<li>对候选集$N$中所有尚未被处理的对象$q$，检查其邻域：<ul>
<li>若至少包含$minPts$个对象，则将这些对象加入$N$；</li>
<li>如果$q$未归入任何一个簇，则将$q$加入$C$；</li>
</ul>
</li>
<li>重复步骤$2$，继续检查$N$中未处理的对象，直到当前候选集$N$为空；</li>
<li>重复步骤$1$-$3$，直到所有对象都归入了某个簇或标记为噪声。</li>
</ol>
<h2 id="高斯混合模型-GMM"><a href="#高斯混合模型-GMM" class="headerlink" title="高斯混合模型(GMM)"></a>高斯混合模型(GMM)</h2><p>详情查看<a href="https://louishsu.xyz/2018/11/12/EM%E7%AE%97%E6%B3%95-GMM%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">EM算法 &amp; GMM模型</a>。</p>
<h2 id="层次聚类-Hierarchical-Clustering"><a href="#层次聚类-Hierarchical-Clustering" class="headerlink" title="层次聚类(Hierarchical Clustering)"></a>层次聚类(Hierarchical Clustering)</h2><p>层次聚类更多的是一种思想，而不是方法，通过从下往上不断合并簇，或者从上往下不断分离簇形成嵌套的簇。例如上面讲到的<code>DBSCAN</code>最后簇的合并就有这种思想。</p>
<p>层次的类通过“树状图”来表示，如下<br><img src="/2018/11/16/Clustering/层次聚类.png" alt="层次聚类"></p>
<p>主要的思想或方法有两种</p>
<ul>
<li>自底向上的凝聚方法<code>(agglomerative hierarchical clustering)</code><br>  如<code>AGNES</code>。</li>
<li>自上向下的分裂方法<code>(divisive hierarchical clustering)</code><br>  如<code>DIANA</code>。</li>
</ul>
<h2 id="图团体检测-Graph-Community-Detection"><a href="#图团体检测-Graph-Community-Detection" class="headerlink" title="图团体检测(Graph Community Detection)"></a>图团体检测(Graph Community Detection)</h2><p>略</p>

      
    </div>

    

    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div></div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="Louis Hsu 支付宝">
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/11/13/Hidden-Markov-Model/" rel="next" title="Hidden Markov Model">
                <i class="fa fa-chevron-left"></i> Hidden Markov Model
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/11/19/Parameter-Estimation/" rel="prev" title="Parameter Estimation">
                Parameter Estimation <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div id="gitalk-container">
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="Louis Hsu">
            
              <p class="site-author-name" itemprop="name">Louis Hsu</p>
              <p class="site-description motion-element" itemprop="description">技术博客？</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">92</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">9</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">12</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/isLouisHsu" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://is.louishsu@foxmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.zhihu.com/people/islouishsu" target="_blank" title="Zhihu"><i class="fa fa-fw fa-zhihu"></i>Zhihu</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="http://weibo.com/islouishsu" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i>Weibo</a>
                  
                </span>
              
            </div>
          

          
          

          
          

          <div id="music163player">
            <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="110" src="//music.163.com/outchain/player?type=0&id=2703291040&auto=1&height=90">
            </iframe>
          </div>

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#前言"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#基础知识"><span class="nav-number">2.</span> <span class="nav-text">基础知识</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#距离度量方法"><span class="nav-number">2.1.</span> <span class="nav-text">距离度量方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hard-vs-soft-clustering"><span class="nav-number">2.2.</span> <span class="nav-text">hard vs. soft clustering</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#聚类方法的分类"><span class="nav-number">2.3.</span> <span class="nav-text">聚类方法的分类</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#常用聚类方法"><span class="nav-number">3.</span> <span class="nav-text">常用聚类方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#K均值-K-means"><span class="nav-number">3.1.</span> <span class="nav-text">K均值(K-means)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#原理"><span class="nav-number">3.1.1.</span> <span class="nav-text">原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#计算步骤"><span class="nav-number">3.1.2.</span> <span class="nav-text">计算步骤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#缺点与部分解决方法"><span class="nav-number">3.1.3.</span> <span class="nav-text">缺点与部分解决方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#改进"><span class="nav-number">3.1.4.</span> <span class="nav-text">改进</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#类似的算法"><span class="nav-number">3.1.5.</span> <span class="nav-text">类似的算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代码"><span class="nav-number">3.1.6.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#均值漂移-Meanshift"><span class="nav-number">3.2.</span> <span class="nav-text">均值漂移(Meanshift)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#原理-1"><span class="nav-number">3.2.1.</span> <span class="nav-text">原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#高斯权重"><span class="nav-number">3.2.2.</span> <span class="nav-text">高斯权重</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#计算步骤-1"><span class="nav-number">3.2.3.</span> <span class="nav-text">计算步骤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代码-1"><span class="nav-number">3.2.4.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#谱聚类-Spectral-Clustering"><span class="nav-number">3.3.</span> <span class="nav-text">谱聚类(Spectral Clustering)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#原理-2"><span class="nav-number">3.3.1.</span> <span class="nav-text">原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#无向权重图"><span class="nav-number">3.3.1.1.</span> <span class="nav-text">无向权重图</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#相似矩阵"><span class="nav-number">3.3.1.2.</span> <span class="nav-text">相似矩阵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#拉普拉斯矩阵-Graph-Laplacians"><span class="nav-number">3.3.1.3.</span> <span class="nav-text">拉普拉斯矩阵(Graph Laplacians)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#无向图的切图"><span class="nav-number">3.3.1.4.</span> <span class="nav-text">无向图的切图</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#cut"><span class="nav-number">3.3.1.4.1.</span> <span class="nav-text">cut</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Ratio-Cut"><span class="nav-number">3.3.1.4.2.</span> <span class="nav-text">Ratio Cut</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#N-Cut"><span class="nav-number">3.3.1.4.3.</span> <span class="nav-text">N Cut</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#计算步骤-2"><span class="nav-number">3.3.2.</span> <span class="nav-text">计算步骤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代码-2"><span class="nav-number">3.3.3.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DBSCAN"><span class="nav-number">3.4.</span> <span class="nav-text">DBSCAN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#原理-3"><span class="nav-number">3.4.1.</span> <span class="nav-text">原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#计算思想"><span class="nav-number">3.4.2.</span> <span class="nav-text">计算思想</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#算法步骤"><span class="nav-number">3.4.3.</span> <span class="nav-text">算法步骤</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#高斯混合模型-GMM"><span class="nav-number">3.5.</span> <span class="nav-text">高斯混合模型(GMM)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#层次聚类-Hierarchical-Clustering"><span class="nav-number">3.6.</span> <span class="nav-text">层次聚类(Hierarchical Clustering)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#图团体检测-Graph-Community-Detection"><span class="nav-number">3.7.</span> <span class="nav-text">图团体检测(Graph Community Detection)</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Louis Hsu</span>

  

  
</div>











        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv" title="总访客量">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="site-pv" title="总访问量">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>
























  



  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/reading_progress/reading_progress.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.4.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.4.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.4.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.4.2"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.4.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.4.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.2"></script>



  



  










  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
  <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
   <script type="text/javascript">
        var gitalk = new Gitalk({
          clientID: 'e65d27f7cf5c62feaf97',
          clientSecret: '356386826698e8b817ca076b08d7c0e9814f52ea',
          repo: 'isLouisHsu.github.io',
          owner: 'isLouisHsu',
          admin: ['isLouisHsu'],
          id: md5(window.location.pathname),
          distractionFreeMode: 'true'
        })
        gitalk.render('gitalk-container')
       </script>

  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('3');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  

  

  

  

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #eee;
      background-image: linear-gradient(#fcfcfc, #eee);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>
  <script>
    $('.highlight').each(function (i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap')
      $(e).after($wrap)
      $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function (e) {
        var code = $(this).parent().find('.code').find('.line').map(function (i, e) {
          return $(e).text()
        }).toArray().join('\n')
        var ta = document.createElement('textarea')
        document.body.appendChild(ta)
        ta.style.position = 'absolute'
        ta.style.top = '0px'
        ta.style.left = '0px'
        ta.value = code
        ta.select()
        ta.focus()
        var result = document.execCommand('copy')
        document.body.removeChild(ta)
        
        $(this).blur()
      })).on('mouseleave', function (e) {
        var $b = $(this).find('.copy-btn')
        setTimeout(function () {
          $b.text('复制')
        }, 300)
      }).append(e)
    })
  </script>



  <script type="text/javascript" src="/js/click_show_text.js"></script>
  <script type="text/javascript" src="/js/hone_hone_clock.js"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
