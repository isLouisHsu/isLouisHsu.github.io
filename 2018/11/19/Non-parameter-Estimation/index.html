<!DOCTYPE HTML>
<html class="no-js" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
    <!--[if lte IE 9]>
<meta http-equiv="refresh" content="0;url=http://yoursite.com/warn.html">
<![endif]-->
<meta charset="utf-8">
<meta http-equiv="X-DNS-Prefetch-Control" content="on">
<link rel="dns-prefetch" href="http://yoursite.com">
<link rel="dns-prefetch" href="//www.google-analytics.com">
<link rel="prefetch" href="http://yoursite.com">
<link rel="prefetch" href="//www.google-analytics.com">


<link rel="prerender" href="http://yoursite.com">

<meta http-equiv="X-UA-Compatible" content="IE=Edge">
<meta name="renderer" content="webkit">
<meta name="viewport" content="width=device-width, initial-scale=1.0,user-scalable=no">
<meta http-equiv="mobile-agent" content="format=html5; url=http://yoursite.com">
<meta name="author" content="Louis Hsu">
<link rel="stylesheet" href="/css/JSimple.css">

<link rel="shortcut icon" href="/images/favicon.png">


<title>non-parameter estimation - LOUIS&#39; BLOG</title>

<meta name="keywords" content="">

<meta name="description " content="Inside! Insane!">

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                processEscapes: true
            }
        });
    </script>


    

    

</head>
<body>
<div id="nav">
    <nav class="nav-menu">
        <a class="site-name current" href="/" title="彬">彬</a>
        <a class="site-index current" href="/"><i class="fa fa-home"></i><span>Home</span></a>
        <a href="/archives" title="Archives"><i class="fa fa-archives"></i><span>Archives</span></a>
        <a href="/tags" title="Tags"><i class="fa fa-tags"></i><span>Tags</span></a>
        <!-- custom single page of menus -->
        
        
        <a href="/help" title="帮助">
            <i class="fa fa-question-circle"></i>
            <span>帮助</span>
        </a>
        
    </nav>
</div>

<div class="nav-user">
    <a class="btn-search" href="#"><i class="fa fa-search"></i></a>
    <a class="btn-read-mode" href="#"><i class="fa fa-sun-o"></i></a>
    <a class="btn-sns-qr" href="javascript:"><i class="fa fa-telegram"></i></a>
</div>

<div id="wrapper" class="clearfix">
    <div id="body">
        <div class="main" id="main">
            <div id="cover">
    <div class="cover-img"></div>
    <div class="cover-info">
        
        <h1 class="cover-siteName">LOUIS&#39; BLOG</h1>
        <h3 class="cover-siteTitle">人生苦短，不如不管，继续任性</h3>
        <p class="cover-siteDesc">技术博客？</p>
        <div class="cover-sns">
            

        </div>
    </div>
</div>

            <div class="page-title">
    <ul>
        <li><a href="/">Recent Posts</a></li>
        
            
                <li class="">
                    <a href="/categories//" data-name="主页">主页</a>
                </li>
            
                <li class="">
                    <a href="/categories/Python" data-name="Python">Python</a>
                </li>
            
                <li class="">
                    <a href="/categories/Machine-Learning" data-name="机器学习">机器学习</a>
                </li>
            
                <li class="">
                    <a href="/categories/Deep-Learning" data-name="深度学习">深度学习</a>
                </li>
            
                <li class="">
                    <a href="/categories/冯唐" data-name="冯唐">冯唐</a>
                </li>
            
                <li class="">
                    <a href="/categories/Others" data-name="其他">其他</a>
                </li>
            
        
        <li class="page-search">
    <form id="search" class="search-form">
        <input type="text" readonly="readonly" id="local-search-input-tip" placeholder="click to search...">
        <button type="button" disabled="disabled" class="search-form-submit"><i class="fa fa-search"></i></button>
    </form>
</li>

    </ul>
</div>
<div class="main-inner">
    <article class="post" itemscope="" itemtype="http://schema.org/BlogPosting">
        <div class="post-header">
            <div class="post-author clearfix">
                <a class="avatar fleft" href="https://louishsu.xyz/" target="_blank">
                    <img width="48" src="/images/favicon.png" alt="avatar">
                </a>
                <p><span class="label">Author</span>
                    <a href="https://louishsu.xyz/" target="_blank">Louis Hsu</a>
                    <span title="Last edited at&nbsp;2018-11-19">2018-11-19</span>
                </p>
                <p>有味道的程序员</p>
            </div>
            <h2 class="post-title">Non-parameter Estimation</h2>
            <div class="post-meta">
                emm... 6583 words in the article |
                you are the&nbsp;<span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span>th friend who reading now
            </div>
        </div>
        <div class="post-content markdown-body">
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>若参数估计时我们不知道样本的分布形式，那么就无法确定需要估计的概率密度函数，无法用<a href="https://louishsu.xyz/2018/10/22/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95/" target="_blank" rel="noopener">最大似然估计、贝叶斯估计等参数估计方法</a>，应该用非参数估计方法。</p>
<p>需要知道的是，作为非参数方法的共同问题是对样本数量需求较大，只要样本数目足够大众可以保证收敛于任何复杂的位置密度，但是计算量和存储量都比较大。当样本数很少时，如果能够对密度函数有先验认识，则参数估计能取得更好的估计效果。</p>
<h1 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h1><p>若有$M$个样本$x^{(1)}, …, x^{(M)}$，依概率密度函数$p(x)$独立同分布抽样得到。</p>
<p>一个样本$x$落在区域$R$中的概率$P$可表示为</p>
<script type="math/tex; mode=display">
P = \int_R p(x) dx \tag{1}</script><p>我们通过计算$P$来估计概率密度$p(x)$。</p>
<p>$K$个样本落入区域$R$的概率$P_K$为二项分布，即$K \sim B(M, P)$</p>
<script type="math/tex; mode=display">
P_K = \left(\begin{matrix} M\\K \end{matrix}\right) P^K (1-P)^{M-K} \tag{2}</script><p>则$K$的期望与方差分别为</p>
<script type="math/tex; mode=display">
E(K) = MP;　D(K) = MP(1-P)</script><p>样本个数$M$越多，$D(K)$越大，即$K$在期望附近的波峰越明显，因此样本足够多时，用$K/M$作为$P$的一个估计非常准确，即</p>
<script type="math/tex; mode=display">
P \approx \frac{K}{M} \tag{3}</script><p>若我们假设$p(x)$是连续的，且区域$R$足够小，记其体积为$V$，那么有</p>
<script type="math/tex; mode=display">
P = \int_R p(x)dx \approx p(x) V \tag{4}</script><p>所以根据$(3)(4)$，得到</p>
<script type="math/tex; mode=display">
p(x) \approx \frac{K/M}{V} \tag{*}</script><p>但是我们获得的其实为平滑后的概率密度函数</p>
<script type="math/tex; mode=display">
\frac{P}{V} = \frac{\int_R p(x)dx}{\int_R dx}</script><p>我们希望其尽可能地趋近$p(x)$，那么必须要求$V \rightarrow 0$，但是这样就可能不包含任何样本，那么$p(x)\approx 0$，这样估计的结果毫无意义。</p>
<p>所以在实际中，一般构造多个包含样本$x$的区域$R_1, …, R_i, …, R_n$，第$i$个区域使用$i$个样本，记$V_i$为$R_i$的体积，$M_i$为落在$R_i$中的样本个数，则对$p(x)$第$i$次估计$p_i(x)$表示为</p>
<script type="math/tex; mode=display">
p_i(x) \approx \frac{M_i / M}{V_i} \tag{5}</script><p>若要求$p_i(x)$收敛到$p(x)$，则必须满足</p>
<ul>
<li>$\lim_{i\rightarrow \infty} V_i = 0$</li>
<li>$\lim_{i\rightarrow \infty} M_i = 0$</li>
<li>$\lim_{i\rightarrow \infty} \frac{M_i}{M} = 0$</li>
</ul>
<h1 id="直方图法"><a href="#直方图法" class="headerlink" title="直方图法"></a>直方图法</h1><p>记不记得小学时的直方图统计，直方图方法的思想就是这样，以$1$维样本为例，我们将$x$的取值范围平均等分为$K$个区间，统计每个区间内样本的个数，由此计算区间的概率密度。</p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>若共有$N$维样本$M$组，在每个维度上$K$等分，就有$K^N$个小空间，每个小空间的体积$V_i$可以定义为</p>
<script type="math/tex; mode=display">
V_i = \prod_{n=1}^N d_n,　i=1,...,K^N</script><p>其中</p>
<script type="math/tex; mode=display">
d_n = \frac{\max x_n - \min x_n}{K}</script><p>假设样本落到各个小空间的概率相同，若第$i$个小空间包含$M_i$个样本，则该空间的概率密度$\hat{p_i}$为</p>
<script type="math/tex; mode=display">
\hat{p_i} = \frac{M_i / M}{V_i} \tag{6}</script><p>估计的效果与小区间的大小密切相连，如果区域选择过大，会导致最终估计出来的概率密度函数非常粗糙；如果区域的选择过小，可能会导致有些区域内根本没有样本或者样本非常少，这样会导致估计出来的概率密度函数很不连续。</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p><a href="https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/blob/master/examples/p64_non_parametric_estmation.py" target="_blank" rel="noopener">@Github: Non-parametric Estmation</a></p>
<p>我们可以用<code>matplotlib.pyplot.hist()</code>或<code>numpy.histogram()</code>实现</p>
<ul>
<li><p><code>matplotlib</code></p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">n, bins, patches = plt.hist(arr, bins=10, normed=0, facecolor=&apos;black&apos;, edgecolor=&apos;black&apos;,alpha=1，histtype=&apos;bar&apos;)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>Args</code><br>  参数很多，选几个常用的讲解<ul>
<li>arr: 需要计算直方图的一维数组</li>
<li>bins: 直方图的柱数，可选项，默认为10</li>
<li>normed: 是否将得到的直方图向量归一化。默认为0</li>
<li>facecolor: 直方图颜色</li>
<li>edgecolor: 直方图边框颜色</li>
<li>alpha: 透明度</li>
<li>histtype: 直方图类型，‘bar’, ‘barstacked’, ‘step’, ‘stepfilled’</li>
</ul>
</li>
<li><code>Returns</code><ul>
<li>n: 直方图向量，是否归一化由参数normed设定</li>
<li>bins: 返回各个bin的区间范围</li>
<li>patches: 返回每个bin里面包含的数据，是一个list</li>
</ul>
</li>
</ul>
</li>
<li><p><code>numpy</code></p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hist, bin_edges = histogram(a, bins=10, range=None, normed=None, weights=None, density=None)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def histEstimate(X, n_bins, showfig=False):</span><br><span class="line">    &quot;&quot;&quot; 直方图密度估计</span><br><span class="line">    Args:</span><br><span class="line">        n_bins: &#123;int&#125; 直方图的条数</span><br><span class="line">    Returns:</span><br><span class="line">        hist: &#123;ndarray(n_bins,)&#125;</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    n, bins, patches = plt.hist(X, bins=n_bins, normed=1, facecolor=&apos;lightblue&apos;, edgecolor=&apos;white&apos;)</span><br><span class="line">    if showfig: plt.show()</span><br><span class="line">    return n, bins, patches</span><br></pre></td></tr></table></figure>
<p><code>matplotlib</code>直方图显示如下<br><img src="/2018/11/19/Non-parameter-Estimation/hist_matplotlib.png" alt="hist_matplotlib"></p>
<p>拟合各中心点显示如下<br><img src="/2018/11/19/Non-parameter-Estimation/hist_ploy.png" alt="hist_ploy"></p>
<h1 id="K-n-近邻估计法"><a href="#K-n-近邻估计法" class="headerlink" title="$K_n$近邻估计法"></a>$K_n$近邻估计法</h1><p>随着样本数的增加，区域的体积应该尽可能小，同时又必须保证区域内有充分多的样本，但是每个区域的样本数有必须是总样本数的很小的一部分，而不是与直方图估计那样体积不变。</p>
<p>那么我们想，能否根据样本的分布调整分区大小呢，$K$近邻估计法就是一种采用可变大小区间的密度估计方法。</p>
<h2 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h2><p>根据总样本确定参数$K_n$，在求样本$x$处的密度估计$\hat{p}(x)$时，调整区域体积$V(x)$，直到区域内恰好落入$K_n$个样本，估计公式为</p>
<script type="math/tex; mode=display">
\hat{p}(x) = \frac{K_n/M}{V(x)} \tag{7}</script><p>一般指定超参数$a$，取</p>
<script type="math/tex; mode=display">
K_n = a × \sqrt{M} \tag{8}</script><blockquote>
<script type="math/tex; mode=display">
\hat{p}(x) = \frac{a × \sqrt{M} /M}{V(x)} = \frac{K_n'/M}{V'(x)}</script><p>其中$K_n’ = a,V’(x) = V(x)×\frac{1}{\sqrt{M}}$</p>
</blockquote>
<p>在样本密度比较高的区域的体积就会比较小，而在密度低的区域的体积则会自动增大，这样就能够较好的兼顾在高密度区域估计的分辨率和在低密度区域估计的连续性。</p>
<h1 id="Parzen窗法"><a href="#Parzen窗法" class="headerlink" title="Parzen窗法"></a>Parzen窗法</h1><p>又称核密度估计。</p>
<h2 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h2><p>我们暂时假设待估计点$x$的附近区间$R$为一个$N$维的<strong>超立方体</strong>，用$h$表示边的长度，那么</p>
<script type="math/tex; mode=display">
V_i = h^N</script><p>即<br><img src="/2018/11/19/Non-parameter-Estimation/Parzen_window.jpg" alt="Parzen_window"><br>定义窗函数$\varphi(·)$，表示落入以$x$为中心的超立方体的区域的点</p>
<script type="math/tex; mode=display">
\varphi \left(\frac{x_i-x}{h}\right) 
= \begin{cases}
    1 & \frac{|x_{in}-x_n|}{h} \leq \frac{1}{2},　n=1,...,N \\
    0 & otherwise
\end{cases} \tag{9}</script><blockquote>
<script type="math/tex; mode=display">\frac{|x_{in}-x_n|}{h} \leq \frac{1}{2}　即　(x_i-x)_n \leq \frac{h}{2}</script><p>这里的$h$起到单位化的作用，便于推广</p>
</blockquote>
<p>那么落入以$x$为中心的<strong>超立方体</strong>的区域的点的个数为</p>
<script type="math/tex; mode=display">
M_i = \sum_{i=1}^M \varphi \left(\frac{x_i-x}{h}\right) \tag{10}</script><p>代入$p(x) \approx \frac{M_i/M}{V_i}$，我们得到</p>
<script type="math/tex; mode=display">
p(x) 
\approx \frac{\sum_{i=1}^M \varphi \left(\frac{x_i-x}{h}\right)/M}{V_i}
= \frac{1}{M} \sum_{i=1}^M \frac{1}{V_i} \varphi \left(\frac{x_i-x}{h}\right) \tag{11}</script><p>我们定义核函数(或称“窗函数”)</p>
<script type="math/tex; mode=display">
\kappa(z) = \frac{1}{V_i} \varphi(z) \tag{12}</script><p>核函数反应了一个观测样本$x_i$对在$x$处的概率密度估计的贡献，与样本$x_i$和$x$的距离有关。而概率密度估计就是在这一点上把所有观测样本的贡献进行平均</p>
<script type="math/tex; mode=display">
p(x) 
\approx \frac{1}{M} \sum_{i=1}^M \kappa\left(\frac{x_i-x}{h}\right) \tag{13}</script><h2 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2><p>核函数应满足概率密度的要求，即</p>
<script type="math/tex; mode=display">
\kappa(z) \geq 0　\And　\int \kappa(z)dz = 1</script><p>通常有以下几种核函数</p>
<ul>
<li><p>均匀核</p>
<script type="math/tex; mode=display">
  \kappa(z)
  = \begin{cases}
      1 & |z_n| \leq \frac{1}{2},　n=1,...,N \\
      0 & otherwise
  \end{cases}</script></li>
<li><p>高斯核(正态核)<br>  高斯核是将窗放大到整个空间，各个观测样本$x_i$对待观测点$x$的加权和(越远权值越小)。</p>
<script type="math/tex; mode=display">
  \kappa(z)
  = \frac{1}{(2\pi)^{N/2}|\Sigma|^{1/2}}
  \exp \left(-\frac{1}{2} (z - \mu)^T \Sigma^{-1} (z - \mu)\right)</script></li>
<li><p>超球窗</p>
<script type="math/tex; mode=display">
  \kappa(z)
  = \begin{cases}
      V^{-1} & ||z|| \leq 1 \\
      0 & otherwise
  \end{cases}</script><blockquote>
<p>$z=\frac{x_i-x}{h}$，故$||z||\leq 1$即$||x_i-x||^2\leq h^2$<br>此时$h$表示超球体的半径</p>
</blockquote>
</li>
</ul>
<h2 id="sklearn"><a href="#sklearn" class="headerlink" title="sklearn"></a>sklearn</h2><p><a href="http://sklearn.apachecn.org/en/0.19.0/modules/generated/sklearn.neighbors.KernelDensity.html#sklearn.neighbors.KernelDensity" target="_blank" rel="noopener">sklearn.neighbors.KernelDensity — scikit-learn 0.19.0 documentation - ApacheCN</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from sklearn.neighbors import KernelDensity</span><br><span class="line">&gt;&gt;&gt; import numpy as np</span><br><span class="line">&gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])</span><br><span class="line">&gt;&gt;&gt; kde = KernelDensity(kernel=&apos;gaussian&apos;, bandwidth=0.2).fit(X)</span><br><span class="line">&gt;&gt;&gt; kde.score_samples(X)</span><br><span class="line">array([-0.41075698, -0.41075698, -0.41076071, -0.41075698, -0.41075698,</span><br><span class="line">       -0.41076071])</span><br><span class="line">&gt;&gt;&gt; kde.sample(10)</span><br><span class="line">array([[ 1.80042291,  1.1030739 ],</span><br><span class="line">       [ 0.87299669,  1.0762352 ],</span><br><span class="line">       [-2.40180586, -1.19554374],</span><br><span class="line">       [-1.97985919, -1.19361193],</span><br><span class="line">       [-2.95866231, -2.1972637 ],</span><br><span class="line">       [-1.12739556, -0.80851063],</span><br><span class="line">       [ 1.03756706,  1.24855099],</span><br><span class="line">       [ 1.21729703,  1.02345815],</span><br><span class="line">       [-2.11816867, -1.0486257 ],</span><br><span class="line">       [-1.04875537, -0.89928711]])</span><br></pre></td></tr></table></figure></p>
<h2 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h2><p>具体代码见<br><a href="https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/blob/master/examples/p64_non_parametric_estmation.py" target="_blank" rel="noopener">@Github: Non-parametric Estmation</a></p>
<p>定义核函数如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 高斯核</span><br><span class="line">gaussian = lambda z: np.exp(-0.5*(np.linalg.norm(z)**2)) / np.sqrt(2*np.pi)</span><br><span class="line"># 均匀核</span><br><span class="line">square = lambda z: 1 if (np.linalg.norm(z) &lt;= 0.5) else 0</span><br></pre></td></tr></table></figure></p>
<p>密度估计函数如下，需要对连续范围内的各个点，即$x \in [min(X), max(X)]$进行估计获得<code>p</code>，作图显示$x-p$即可<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def parzenEstimate(X, kernel, h, n_num=50):</span><br><span class="line">    &quot;&quot;&quot; 核参数估计</span><br><span class="line">    Args:</span><br><span class="line">        X: &#123;ndarray(n_samples,)&#125;</span><br><span class="line">        kernel: &#123;function&#125; 可调用的核函数</span><br><span class="line">        h: &#123;float&#125; 核函数的参数</span><br><span class="line">    Returns:</span><br><span class="line">        p: &#123;ndarray(n_num,)&#125;</span><br><span class="line">    Notes:</span><br><span class="line">        - 一维，故`V_i = h`</span><br><span class="line">        - p(x) = \frac&#123;1&#125;&#123;M&#125; \sum_&#123;i=1&#125;^M \kappa \left( \frac&#123;x_i - x&#125;&#123;h&#125; \right)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x = np.linspace(np.min(X), np.max(X), num=n_num)</span><br><span class="line">    p = np.zeros(shape=(x.shape[0],))</span><br><span class="line">    z = lambda x, x_i, h: (x - x_i) / h</span><br><span class="line">    V_i = h; n_samples = X.shape[0]</span><br><span class="line">    for idx in range(x.shape[0]):</span><br><span class="line">        for i in range(X.shape[0]):</span><br><span class="line">            p[idx] += kernel(z(x[idx], X[i], h)) / V_i</span><br><span class="line">        p[idx] /= n_samples</span><br><span class="line">    return p</span><br></pre></td></tr></table></figure></p>
<h3 id="均匀核"><a href="#均匀核" class="headerlink" title="均匀核"></a>均匀核</h3><ul>
<li><p>$h=0.5$<br>  <img src="/2018/11/19/Non-parameter-Estimation/01_h_0.5.png" alt="01_h_0.5"></p>
</li>
<li><p>$h=0.8$<br>  <img src="/2018/11/19/Non-parameter-Estimation/01_h_0.8.png" alt="01_h_0.8"></p>
</li>
<li><p>$h=1.0$<br>  <img src="/2018/11/19/Non-parameter-Estimation/01_h_1.0.png" alt="01_h_1.0"></p>
</li>
<li><p>$h=2.0$<br>  <img src="/2018/11/19/Non-parameter-Estimation/01_h_2.0.png" alt="01_h_2.0"></p>
</li>
</ul>
<h3 id="高斯核"><a href="#高斯核" class="headerlink" title="高斯核"></a>高斯核</h3><ul>
<li><p>$h=0.5$<br>  <img src="/2018/11/19/Non-parameter-Estimation/gaussian_h_0.5.png" alt="gaussian_h_0.5"></p>
</li>
<li><p>$h=0.8$<br>  <img src="/2018/11/19/Non-parameter-Estimation/gaussian_h_0.8.png" alt="gaussian_h_0.8"></p>
</li>
<li><p>$h=1.0$<br>  <img src="/2018/11/19/Non-parameter-Estimation/gaussian_h_1.0.png" alt="gaussian_h_1.0"></p>
</li>
<li><p>$h=2.0$<br>  <img src="/2018/11/19/Non-parameter-Estimation/gaussian_h_2.0.png" alt="gaussian_h_2.0"></p>
</li>
</ul>

        </div>
        <div class="post-tool">
            <a class="btn-thumbs-up" href="javascript:void(0);" data-cid="52" title="95">
                <i class="fa fa-thumbs-up" aria-hidden="true"></i> Donate
            </a>
        </div>
        
        <div class="post-tags">Tags：
            
        </div>
        
    </article>
    
    <p style="text-align: center">This article just represents my own viewpoint. If there is something wrong, please correct me.</p>
    
    

    

</div>
<script src="/js/busuanzi.pure.mini.js"></script>


        </div><!-- end #main-->
    </div><!-- end #body -->
    <footer class="footer">
    <div class="footer-inner" style="text-align: center">
        <p>
            <a href="/about" title="About">About</a>&nbsp;&nbsp<em>·</em>&nbsp;&nbsp
            <!-- 自定义链接 -->
            <a href="/help" title="Help">Help</a>&nbsp;&nbsp<em>·</em>&nbsp;&nbsp
            <a href="/links" title="Links">Links</a>&nbsp;&nbsp<em>·</em>&nbsp;&nbsp
            <a href="/sitemap.xml" title="SiteMap">SiteMap</a>
        </p>
        <p>
            Has been established&nbsp<a href="/timeline" id="siteBuildingTime"></a>&nbspDays，<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="licence">Based on Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)</a><br>
            ©2017-<span id="cpYear"></span> Based on&nbsp<a href="http://hexo.io" target="_blank" rel="nofollow">Hexo</a>
            ，Theme by&nbsp&nbsp<a href="https://github.com/tangkunyin/hexo-theme-jsimple" target="_blank" rel="bookmark">JSimple</a>
            ，Author&nbsp<a href="https://louishsu.xyz/" target="_blank" rel="friend">Louis Hsu</a>
            ，Hosted by <a href="https://pages.github.com/" target="_blank" rel="nofollow">GitHub Pages</a>
        </p>
    </div>
</footer>
<script src="/js/SimpleCore.js"></script>

</div>
<!-- search pop -->
<div class="popup search-popup local-search-popup">
    <div class="local-search-header clearfix">
        <span class="search-icon">
            <i class="fa fa-search"></i>
        </span>
        <span class="popup-btn-close">
            <i class="fa fa-times-circle"></i>
        </span>
        <div class="local-search-input-wrapper">
            <input id="local-search-input" spellcheck="false" type="text" autocomplete="off" placeholder="Input query keywords here...">
        </div>
    </div>
    <div id="local-search-result"></div>
</div>
<div class="fixed-btn">
    <a class="btn-gotop" href="javascript:"> <i class="fa fa-angle-up"></i></a>
</div>
<script>
    $(function () {
        var jsi_config = {
            buildingTime: '10/20/2018',
            current: $('.post-tags').length > 0 ? 'post' : 'archive',
            snsQRCode: '/images/sns-qrcode.png',
            donateImg: '/images/donate-qr.png',
            localSearch: { dbPath: '' },
            readMode: 'day'
        };
        
            jsi_config.localSearch = {
                dbPath: '/search.json',
                trigger: 'auto',
                topN: '1',
                unescape: 'false'
            }
        
        SimpleCore.init(jsi_config);
        
    });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
