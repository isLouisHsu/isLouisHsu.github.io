<!DOCTYPE HTML>
<html class="no-js" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
    <!--[if lte IE 9]>
<meta http-equiv="refresh" content="0;url=http://yoursite.com/warn.html">
<![endif]-->
<meta charset="utf-8">
<meta http-equiv="X-DNS-Prefetch-Control" content="on">
<link rel="dns-prefetch" href="http://yoursite.com">
<link rel="dns-prefetch" href="//www.google-analytics.com">
<link rel="prefetch" href="http://yoursite.com">
<link rel="prefetch" href="//www.google-analytics.com">


<link rel="prerender" href="http://yoursite.com">

<meta http-equiv="X-UA-Compatible" content="IE=Edge">
<meta name="renderer" content="webkit">
<meta name="viewport" content="width=device-width, initial-scale=1.0,user-scalable=no">
<meta http-equiv="mobile-agent" content="format=html5; url=http://yoursite.com">
<meta name="author" content="Louis Hsu">
<link rel="stylesheet" href="/css/JSimple.css">

<link rel="shortcut icon" href="/images/favicon.png">


<title>parameter estimation - LOUIS&#39; BLOG</title>

<meta name="keywords" content="">

<meta name="description " content="Inside! Insane!">

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                processEscapes: true
            }
        });
    </script>


    

    

</head>
<body>
<div id="nav">
    <nav class="nav-menu">
        <a class="site-name current" href="/" title="彬">彬</a>
        <a class="site-index current" href="/"><i class="fa fa-home"></i><span>首页</span></a>
        <a href="/archives" title="归档"><i class="fa fa-archives"></i><span>归档</span></a>
        <a href="/tags" title="标签"><i class="fa fa-tags"></i><span>标签</span></a>
        <!-- custom single page of menus -->
        
        
        <a href="/help" title="帮助">
            <i class="fa fa-question-circle"></i>
            <span>帮助</span>
        </a>
        
    </nav>
</div>

<div class="nav-user">
    <a class="btn-search" href="#"><i class="fa fa-search"></i></a>
    <a class="btn-read-mode" href="#"><i class="fa fa-sun-o"></i></a>
    <a class="btn-sns-qr" href="javascript:"><i class="fa fa-telegram"></i></a>
</div>

<div id="wrapper" class="clearfix">
    <div id="body">
        <div class="main" id="main">
            <div id="cover">
    <div class="cover-img"></div>
    <div class="cover-info">
        
        <img class="avatar" width="72" src="/images/favicon.png" alt="avatar">
        
        <h1 class="cover-siteName">LOUIS&#39; BLOG</h1>
        <h3 class="cover-siteTitle">人生苦短，不如不管，继续任性</h3>
        <p class="cover-siteDesc">技术博客？</p>
        <div class="cover-sns">
            

        </div>
    </div>
</div>

            <div class="page-title">
    <ul>
        <li><a href="/">最近</a></li>
        
            
                <li class="">
                    <a href="/categories//" data-name="主页">主页</a>
                </li>
            
                <li class="">
                    <a href="/categories/Python" data-name="Python">Python</a>
                </li>
            
                <li class="">
                    <a href="/categories/Machine-Learning" data-name="机器学习">机器学习</a>
                </li>
            
                <li class="">
                    <a href="/categories/Deep-Learning" data-name="深度学习">深度学习</a>
                </li>
            
                <li class="">
                    <a href="/categories/冯唐" data-name="冯唐">冯唐</a>
                </li>
            
                <li class="">
                    <a href="/categories/Others" data-name="其他">其他</a>
                </li>
            
        
        <li class="page-search">
    <form id="search" class="search-form">
        <input type="text" readonly="readonly" id="local-search-input-tip" placeholder="读物检索~">
        <button type="button" disabled="disabled" class="search-form-submit"><i class="fa fa-search"></i></button>
    </form>
</li>

    </ul>
</div>
<div class="main-inner">
    <article class="post" itemscope="" itemtype="http://schema.org/BlogPosting">
        <div class="post-header">
            <div class="post-author clearfix">
                <a class="avatar fleft" href="https://louishsu.xyz/" target="_blank">
                    <img width="48" src="/images/favicon.png" alt="avatar">
                </a>
                <p><span class="label">作者</span>
                    <a href="https://louishsu.xyz/" target="_blank">徐耀彬</a>
                    <span title="最后编辑于&nbsp;2018-11-19">2018-11-19</span>
                </p>
                <p>有味道的程序员</p>
            </div>
            <h2 class="post-title">Parameter Estimation</h2>
            <div class="post-meta">
                本文共计9678个字 |
                您是第&nbsp;<span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span>位看到它们的小伙伴
            </div>
        </div>
        <div class="post-content markdown-body">
            <p><a href="https://www.zhihu.com/question/20587681/answer/17435552" target="_blank" rel="noopener">贝叶斯学派与频率学派有何不同？ - 任坤的回答 - 知乎</a></p>
<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>参数估计<code>(parameter estimation)</code>，统计推断的一种。根据从总体中抽取的随机样书．来估计总体分布中未知参数的过程。主要介绍最大似然估计<code>(MLE: Maximum Likelihood Estimation)</code>，最大后验概率估计<code>(MAP: Maximum A Posteriori Estimation)</code>，贝叶斯估计<code>(Bayesian Estimation)</code>。</p>
<blockquote>
<p>解释一下“似然函数”和“后验概率”，在<a href="https://louishsu.xyz/2018/10/18/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/" target="_blank" rel="noopener">贝叶斯决策</a>一节，给出定义如下</p>
<script type="math/tex; mode=display">P(c_k|x)=\frac{p(x|c_k)P(c_k)}{p(x)}</script><p>上式中$ k=1,…,K  $，各部分定义如下<br>$P(c_k|x)$——<code>后验概率(posteriori probability)</code><br>$P(c_k)$——<code>先验概率(priori probability)</code><br>$p(x|c_k)$——$c_k$关于$x$的<code>似然函数(likelihood)</code><br>$p(x)$——<code>证据因子(evidence)</code></p>
</blockquote>
<h1 id="引例"><a href="#引例" class="headerlink" title="引例"></a>引例</h1><p>以最经典的掷硬币实验为例，假设有一枚硬币，投掷一次出现正面记$”1”$，投掷$10$次的实验结果如下</p>
<script type="math/tex; mode=display">
\{ 0， 1， 1， 1， 1， 0， 1， 1， 1，0 \}</script><p>记硬币投掷结果为随机变量$X$，且$ x \in \{0, 1\}$，硬币投掷一次服从二项分布，估计二项分布的参数$\theta$</p>
<h1 id="最大似然估计-MLE"><a href="#最大似然估计-MLE" class="headerlink" title="最大似然估计(MLE)"></a>最大似然估计(MLE)</h1><h2 id="似然函数"><a href="#似然函数" class="headerlink" title="似然函数"></a>似然函数</h2><blockquote>
<p><a href="https://en.wikipedia.org/wiki/Likelihood_function#Definition" target="_blank" rel="noopener">Likelihood function - Wikipedia</a></p>
</blockquote>
<ul>
<li><p>离散型</p>
<script type="math/tex; mode=display">
L(x | \theta) = p_{\theta}(x)=P_{\theta}(X = x)</script></li>
<li><p>连续型</p>
<script type="math/tex; mode=display">
L(x | \theta) = f_{\theta}(x)</script></li>
</ul>
<blockquote>
<p>很多人能讲出一大堆哲学理论来阐明这一对区别。<br>但我觉得，从工程师角度来讲，这样理解就够了:<br>频率 $vs$ 贝叶斯 = $P(X; w)$ $vs$ $P(X|w)$ 或 $P(X,w)$</p>
<p>作者：许铁-巡洋舰科技<br>链接：<a href="https://www.zhihu.com/question/20587681/answer/122348889" target="_blank" rel="noopener">https://www.zhihu.com/question/20587681/answer/122348889</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
</blockquote>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>有数据集$D = \{x_1, x_2, …, x_N\}$，按$c$个类别分成$\{D_1, D_2, …, D_C\}$，各个类别服从的概率分布密度函数模型已给出，估计参数$\hat{\Theta} = \{\hat{\theta}_{c_1}, \hat{\theta}_{c_2}, …, \hat{\theta}_{c_C}\} $</p>
<p><strong>假定</strong></p>
<ul>
<li>类别间独立，且各自服从概率分布密度函数为$p(x|c_j)$</li>
<li>各类别的概率密度$p(x|c_j)$以参数$\theta_{c_j}$确定，即$p(x|c_j; \theta_{c_j})$</li>
</ul>
<p>故似然函数为</p>
<script type="math/tex; mode=display">
L(D | \Theta) = P(x_1, x_2, ..., x_N | \Theta) = \prod_{i=1}^N p(x_i | \theta_{x_i \in c_j})</script><blockquote>
<p>理解为，在参数$\Theta$为何值的条件下，实验结果出现数据集$D$的概率最大</p>
</blockquote>
<p>求取其极大值对应的参数即可</p>
<ul>
<li>一般取对数似然函数<script type="math/tex; mode=display">\log L(D | \Theta) = \sum_{i=1}^N \log p(x_i | \theta_{x_i \in c_j})</script></li>
<li>极大值即对应梯度为$\vec{0}$的位置，即<script type="math/tex; mode=display">
∇_\Theta  \log L(D | \Theta) = \vec{0}
\Rightarrow
\hat{\Theta}</script></li>
</ul>
<blockquote>
<p>Some comments about ML</p>
<ul>
<li>ML estimation is usually simpler than alternative methods. </li>
<li>Has good convergence properties as the number of training samples increases. </li>
<li>If the model chosen for p(x|θ) is correct, and independence assumptions among variables are true, ML will give very good results.</li>
<li>If the model is wrong, ML will give poor results.<div style="text-align: right"> —— Zhao Haitao. Maximum Likelihood and Bayes Estimation </div>

</li>
</ul>
</blockquote>
<h2 id="例：正态分布的最大似然估计"><a href="#例：正态分布的最大似然估计" class="headerlink" title="例：正态分布的最大似然估计"></a>例：正态分布的最大似然估计</h2><p>数据集(单类别)服从高斯分布$N(\mu, \sigma^2)$时的的最大似然估计</p>
<script type="math/tex; mode=display">
P(x_i | \mu, \sigma^2) 
= \frac{1}{\sqrt{2\pi} \sigma } 
e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}</script><script type="math/tex; mode=display">
L(D | \mu, \sigma^2) 
= \prod_{i=1}^N 
\frac{1}{\sqrt{2\pi} \sigma } 
e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}
=\left( \frac{1}{\sqrt{2\pi} \sigma } \right)^N 
\prod_{i=1}^N 
e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}</script><p>取对数似然</p>
<script type="math/tex; mode=display">
\log L(D | \mu, \sigma^2) 
= - \frac{N}{2} \log(2\pi \sigma^2) - 
\frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2</script><h3 id="1-参数-mu-的估计"><a href="#1-参数-mu-的估计" class="headerlink" title="1. 参数$\mu$的估计"></a>1. 参数$\mu$的估计</h3><script type="math/tex; mode=display">
\frac{∂}{∂\mu} 
L(D | \mu, \sigma^2) 
= \frac{1}{\sigma^2} (\sum_{i=1}^N x_i - N\mu)
= 0</script><script type="math/tex; mode=display">
\Rightarrow
\hat{\mu}
= \frac{1}{N}
\sum_{i=1}^N x_i</script><h3 id="2-参数-sigma-2-的估计"><a href="#2-参数-sigma-2-的估计" class="headerlink" title="2. 参数$\sigma^2$的估计"></a>2. 参数$\sigma^2$的估计</h3><script type="math/tex; mode=display">
\frac{∂}{∂\sigma^2} 
\log L(D | \mu, \sigma^2) 
= - \frac{N}{2\sigma^2} +
\frac{1}{2\sigma^4} \sum_{i=1}^N (x_i - \mu)^2
= 0</script><script type="math/tex; mode=display">
\Rightarrow
\hat{\sigma^2} 
= \frac{1}{N} \sum_{i=1}^N (x_i - \mu)^2</script><blockquote>
<p>参数$\hat{\mu}, \hat{\sigma}^2$的值与样本均值和样本方差相等</p>
</blockquote>
<h1 id="最大后验概率估计-MAP"><a href="#最大后验概率估计-MAP" class="headerlink" title="最大后验概率估计(MAP)"></a>最大后验概率估计(MAP)</h1><!-- > [高斯混合模型(GMM)与最大期望算法(EM)]() -->
<h2 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h2><p>最大似然估计是求参数$\theta$, 使似然函数$P(D | \theta)$最大，最大后验概率估计则是求$\theta$使$P(\theta | D)$最大</p>
<blockquote>
<p>理解为，在已出现的实验样本$D$上，参数$\theta$取何值的概率最大</p>
</blockquote>
<p>且注意到</p>
<script type="math/tex; mode=display">
P(\theta | D) = \frac{P(D | \theta)P(\theta)}{P(D)}</script><p>故$MAP$不仅仅使似然函数$P(D | \theta)$最大，而且使$P(\theta)$最大，即</p>
<script type="math/tex; mode=display">
\theta = argmax L(\theta | D)</script><script type="math/tex; mode=display">
L(\theta | D) = P(\theta) P(D | \theta)
= P(\theta) \prod_{i=1}^N p(x_i | \theta)</script><blockquote>
<p>比$ML$多了一项$P(\theta)$</p>
</blockquote>
<ul>
<li><p>取对数后</p>
<script type="math/tex; mode=display">
\log L(\theta | D) = \sum_{i=1}^N \log p(x_i | \theta) + \log P(\theta)</script></li>
<li><p>求取极大值</p>
<script type="math/tex; mode=display">
∇_\theta L(\theta | D) = 0
\Rightarrow
\hat{\theta}</script></li>
</ul>
<blockquote>
<p>$MAP$和$MLE$的区别：<br>$MAP$允许我们把先验知识加入到估计模型中，这在<strong>样本很少</strong>的时候是很有用的，因为样本很少的时候我们的观测结果很可能出现偏差，此时先验知识会把估计的结果“拉”向先验，实际的预估结果将会在先验结果的两侧形成一个顶峰。通过调节先验分布的参数，比如<code>beta</code>分布的$\alpha, \beta$，我们还可以调节把估计的结果“拉”向先验的幅度，$\alpha, \beta$越大，这个顶峰越尖锐。这样的参数，我们叫做预估模型的“超参数”。<br><a href="https://blog.csdn.net/hustlx/article/details/51144710" target="_blank" rel="noopener">极大似然估计，最大后验概率估计(MAP)，贝叶斯估计 - 李鑫o_O - CSDN博客</a></p>
</blockquote>
<h2 id="例：正态分布的最大后验概率估计"><a href="#例：正态分布的最大后验概率估计" class="headerlink" title="例：正态分布的最大后验概率估计"></a>例：正态分布的最大后验概率估计</h2><p>数据集(单类别)服从高斯分布$N(\mu, \sigma^2)$时的最大后验概率估计</p>
<script type="math/tex; mode=display">
p(x_i | \mu, \sigma^2) 
= \frac{1}{\sqrt{2\pi} \sigma } 
e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}</script><blockquote>
<script type="math/tex; mode=display">
\log p(x_i | \mu, \sigma^2)
= - \frac{1}{2} \log(2\pi \sigma^2) - 
\frac{1}{2\sigma^2} (x_i - \mu)^2</script></blockquote>
<h3 id="1-参数-mu-的估计-1"><a href="#1-参数-mu-的估计-1" class="headerlink" title="1. 参数$\mu$的估计"></a>1. 参数$\mu$的估计</h3><p>给定先验条件：$\mu$服从正态分布$N(\mu_0, \sigma_{\mu_0}^2)$，即</p>
<script type="math/tex; mode=display">
p(\mu) = \frac{1}{\sqrt{2\pi}\sigma_{\mu_0}} 
e^ {-\frac{(\mu - \mu_0)^2}{2\sigma_{\mu_0}^2}}</script><blockquote>
<script type="math/tex; mode=display">
\log p(\mu)
= - \frac{1}{2} \log(2\pi \sigma_{\mu_0}^2) - 
\frac{1}{2\sigma_{\mu_0}^2} (\mu - \mu_0)^2</script></blockquote>
<p>则</p>
<script type="math/tex; mode=display">
\log L(\mu, \sigma^2 | D)
= - \frac{N}{2} \log(2\pi \sigma^2) - 
\frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2 - 
\frac{1}{2} \log(2\pi \sigma_{\mu_0}^2) - 
\frac{1}{2\sigma_{\mu_0}^2} (\mu - \mu_0)^2</script><p>则</p>
<script type="math/tex; mode=display">
\frac{∂}{∂\mu} \log L(\mu, \sigma^2 | D)
= \frac{1}{\sigma^2} \sum_{i=0}^N (x_i - \mu) - \frac{1}{\sigma_{\mu_0}^2} (\mu - \mu_0)
= 0</script><script type="math/tex; mode=display">
\Rightarrow
\hat{\mu}
= \frac{\mu_0 \sigma^2 + \sigma_{\mu_0}^2 \sum_{i=0}^N x_i}
{\sigma^2 + N \sigma_{\mu_0}^2}
= \frac{\mu_0 + \frac{\sigma_{\mu_0}^2}{\sigma^2} \sum_{i=0}^N x_i}
{1 + \frac{\sigma_{\mu_0}^2}{\sigma^2} N }</script><h3 id="2-参数-sigma-2-的估计-1"><a href="#2-参数-sigma-2-的估计-1" class="headerlink" title="2. 参数$\sigma^2$的估计"></a>2. 参数$\sigma^2$的估计</h3><p>给定先验条件：$\sigma^2$服从正态分布$N(\sigma_0^2, \sigma_{\sigma_0^2}^2)$，即</p>
<script type="math/tex; mode=display">
p(\sigma^2) = \frac{1}{\sqrt{2\pi} \sigma_{\sigma_0^2}} e^ {-\frac{(\sigma^2- \sigma_0^2)^2}{2 \sigma_{\sigma_0^2} ^2}}</script><blockquote>
<script type="math/tex; mode=display">
\log p(\sigma^2) 
= - \frac{1}{2} \log(2\pi \sigma_{\sigma_0}^2) - 
\frac{1}{2\sigma_{\sigma_0}^2} (\sigma - \sigma_0)^2</script></blockquote>
<p>则</p>
<script type="math/tex; mode=display">
\log L(\mu, \sigma^2 | D)
= - \frac{N}{2} \log(2\pi \sigma^2) - 
\frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2 - \frac{1}{2} \log(2\pi \sigma_{\sigma_0}^2) - 
\frac{1}{2\sigma_{\sigma_0}^2} (\sigma - \sigma_0)^2</script><p>则</p>
<script type="math/tex; mode=display">
\frac{∂}{∂\sigma^2} \log L(\mu, \sigma^2 | D)
= - \frac{N}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i=1}^N (x_i - \mu)^2 - 
\frac{1}{2\sigma_{\sigma_0}^2} 
\frac{\sigma - \sigma_0}{\sigma}
\Rightarrow
\hat{\sigma^2}(略)</script><blockquote>
<script type="math/tex; mode=display">
\frac{∂}{∂\sigma^2}(\sigma - \sigma_0)^2
= 2(\sigma - \sigma_0)
\frac{∂}{∂\sigma^2} (\sigma - \sigma_0)
= \frac{\sigma - \sigma_0}{\sigma}</script></blockquote>
<h1 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h1><h2 id="模型-2"><a href="#模型-2" class="headerlink" title="模型"></a>模型</h2><!-- 
贝叶斯公式：
$$
P(c_i|x) = \frac{p(x|c_i)p(c_i)}{\sum_j p(x|c_j)p(c_j)}
$$

在给定数据集$D=\{x_1, x_2, ..., x_N\}$的情况下，可从数据中估计先验概率和似然函数，即
$$
P(c_i|x, D) = \frac{p(x|c_i, D)p(c_i, D)}{\sum_j p(x|c_j, D)p(c_j, D)}
$$

**假定**
- 先验概率密度函数为$p(c_i)$已知
- 抽样结果几乎与真实分布一致，即$ p(c_i, D) \approx p(c_i) $

则
$$
P(c_i|x, D) = \frac{p(x|c_i, D)p(c_i)}{\sum_j p(x|c_j, D)p(c_j)}
$$

只需从样本中，估计每个类别的似然函数$p(x|c_i, D)$即可

>-----------------------------------------------

现考虑**单个类别**中抽取的数据集$D$，如何估计该类别的似然函数$p(x | \theta)$参数$\theta$呢？

若概率密度函数为$p(x | \theta)$，记从数据集中估计得到的似然函数为$p(x | D)$，有
$$
p(x | \theta) \approx p(x | D)
$$

> $p(x | D)$ would be the estimate of $p(x | \theta)$ given $D$

且
$$
p(x | D) 
= \int p(x, \theta | D) d \theta
= \int p(x | \theta, D) p(\theta | D) d \theta
$$

> Links $p(x | D)$ with $p(θ | D)$

其中
- $
p(x | \theta, D) \approx p(x | \theta) 
$

- $
p(\theta | D) 
= \frac {P(D | \theta)p(\theta)} {P(D)}
= \frac{p(\theta) \prod_{i=1}^N p(x_i | \theta)}{P(D)}
$

故
$$
p(x | D) 
= \int p(x | \theta) p(\theta | D) d \theta
$$

总的来说 
-->
<script type="math/tex; mode=display">
p(\theta | D) 
= \frac {P(D | \theta)p(\theta)} {P(D)}
= a · p(\theta) \prod_{i=1}^N p(x_i | \theta)</script><p>其中$a$是使</p>
<script type="math/tex; mode=display">
\int p(\theta | D)  = 1</script><p>利用“质心公式”求解贝叶斯的点估计</p>
<script type="math/tex; mode=display">
θ_{Bayes} = \int θ·p(θ|D) d θ</script><h2 id="例：正态分布的贝叶斯估计"><a href="#例：正态分布的贝叶斯估计" class="headerlink" title="例：正态分布的贝叶斯估计"></a>例：正态分布的贝叶斯估计</h2><p>数据集(单类别)服从高斯分布$N(\mu, \sigma^2)$时的贝叶斯估计</p>
<script type="math/tex; mode=display">
p(x_i | \mu, \sigma^2) 
= \frac{1}{\sqrt{2\pi} \sigma } 
e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}</script><h3 id="参数-mu-的估计"><a href="#参数-mu-的估计" class="headerlink" title="参数$\mu$的估计"></a>参数$\mu$的估计</h3><p>给定先验条件：$\mu$服从正态分布$N(\mu_0, \sigma_{\mu_0}^2)$，即</p>
<script type="math/tex; mode=display">
p(\mu) = \frac{1}{\sqrt{2\pi}\sigma_{\mu_0}} 
e^ {-\frac{(\mu - \mu_0)^2}{2\sigma_{\mu_0}^2}}</script><p>则</p>
<script type="math/tex; mode=display">
P(\mu | D) 
= a · p(\mu) \prod_{i=1}^N p(x_i | \mu)
= a · 
\frac{1}{\sqrt{2\pi}\sigma_{\mu_0}} 
e^ {-\frac{(\mu - \mu_0)^2}{2\sigma_{\mu_0}^2}}
\prod_{i=1}^N 
\frac{1}{\sqrt{2\pi} \sigma } 
e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}</script><script type="math/tex; mode=display">
= a · 
\left( \frac{1}{\sqrt{2\pi}} \right)^{N + 1}
\frac{1}{\sigma_{\mu_0} \sigma^N}
e^ {
-\frac{(\mu - \mu_0)^2}{2\sigma_{\mu_0}^2} 
-\sum_{i=1}^N \frac{(x_i - \mu)^2}{2\sigma^2}
}</script><p>易证</p>
<blockquote>
<p></p><p align="right">我已经想到了一个绝妙的证明,但是这台电脑的硬盘太小了,写不下。</p><br><!-- > <p align="right">诶嘿，显然成立，2333333</p> --><p></p>
</blockquote>
<script type="math/tex; mode=display">
p(\mu | D) = \frac{1}{\sqrt{2\pi}\sigma_N} 
e^ {-\frac{(\mu - \mu_N)^2}{2\sigma_N^2}}</script><p>其中</p>
<script type="math/tex; mode=display">
\mu_N = 
\frac{N \sigma_0^2}
{N \sigma_0^2 + \sigma^2}
\frac{1}{N} \sum_{i=1}^N x_i
+\frac{\sigma^2}{N \sigma_0^2 + \sigma^2}
\mu_0</script><script type="math/tex; mode=display">
\sigma_N^2 = 
\frac{\sigma_0^2 \sigma^2}
{N \sigma_0^2 + \sigma^2}</script><blockquote>
<p><strong>与$MLE$，$MAP$的区别</strong></p>
<ul>
<li>相比较$MLE$与$MAP$的点估计，贝叶斯估计得到的结果是参数$\theta$的密度函数$p(\theta | D)$</li>
<li><p>最大后验概率估计为求取对应最大后验概率的点</p>
<script type="math/tex; mode=display">\theta = argmax_\theta p(\theta | D)</script></li>
<li><p>贝叶斯估计为求取整个取值范围的概率密度$p(\theta | D)$，既然如此，必有</p>
<script type="math/tex; mode=display">\int p(\theta | D) d\theta = 1</script></li>
</ul>
<p><a href="https://www.cnblogs.com/zjh225901/p/7495505.html" target="_blank" rel="noopener">统计学习方法学习笔记（一）—极大似然估计与贝叶斯估计原理及区别 - YJ-20 - 博客园</a></p>
<script type="math/tex; mode=display">
p(\theta | D) 
= \frac
{p(\theta) \prod_{i=1}^N p(x_i | \theta)}
{\int_\theta p(\theta) \prod_{i=1}^N p(x_i | \theta) d\theta}</script><p>由于$\theta$是满足一定概率分布的变量，所以在计算的时候需要将考虑所有$\theta$取值的情况，在计算过程中不可避免地高复杂度。所以计算时并不把所有地后验概率$p(\theta | D)$都找出来，而是采用类似于极大似然估计地思想，来极大化后验概率，得到这种有效的叫做$MAP$</p>
</blockquote>
<h1 id="引例的求解"><a href="#引例的求解" class="headerlink" title="引例的求解"></a>引例的求解</h1><p>已知硬币投掷结果服从$Bernoulli$分布</p>
<table>
  <tr>
    <th>X</th>
    <th>0</th>
    <th>1</th>
  </tr>
  <tr>
    <td>P</td>
    <td>1-θ</td>
    <td>θ</td>
  </tr>
</table>

<p>或者</p>
<script type="math/tex; mode=display">
P(X_i) = \theta ^{X_i} (1 - \theta) ^{1 - X_i}</script><h2 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h2><p>实验结果中正面出现$7$次，反面出现$3$次，似然函数为</p>
<script type="math/tex; mode=display">
L(\theta) 
= \prod_{i=1}^{10} \theta ^{X_i} (1 - \theta) ^{1 - X_i} 
= \theta ^7 (1 - \theta) ^3</script><p>取对数似然函数并求极大值</p>
<script type="math/tex; mode=display">
\log L(\theta) = 7 \log \theta + 3 \log (1 - \theta)</script><p>令</p>
<script type="math/tex; mode=display">
\frac{∂}{∂ \theta} \log L(\theta)
= \frac{7}{\theta} - \frac{3}{1-\theta} = 0</script><p>解得</p>
<script type="math/tex; mode=display">
\theta = 0.7</script><p>即硬币服从$B(1, 0.7)$的概率分布</p>
<blockquote>
<p>做出$L(\theta)$图像验证，如下<br><img src="/2018/11/19/Parameter-Estimation/最大似然估计.png" alt="最大似然估计"></p>
</blockquote>
<h2 id="最大后验概率估计"><a href="#最大后验概率估计" class="headerlink" title="最大后验概率估计"></a>最大后验概率估计</h2><p>给定先验条件</p>
<script type="math/tex; mode=display">
\theta \thicksim N(\theta_0, \sigma_{\theta_0}^2)</script><p>则最大化</p>
<script type="math/tex; mode=display">
L(\theta | D) = \theta ^7 (1 - \theta) ^3 · \frac{1}{\sqrt{2\pi}\sigma_{\theta_0}} e^ {-\frac{(\theta - \theta_0)^2}{2\sigma_{\theta_0}^2}}</script><p>取对数</p>
<script type="math/tex; mode=display">
\log L(\theta | D)
= 7 \log \theta + 3 \log (1 - \theta) - 
\frac{1}{2} \log(2\pi \sigma_{\theta_0}^2) - 
\frac{1}{2\sigma_{\theta_0}^2} (\theta - \theta_0)^2</script><p>求取极大值点</p>
<script type="math/tex; mode=display">
\frac{∂}{∂\theta} \log L(\theta | D) 
= \frac {7}{\theta} - \frac{3}{1-\theta} - 
\frac{\theta - \theta_0}{\sigma_{\theta_0}^2} = 0</script><p>得到</p>
<script type="math/tex; mode=display">
\theta^3 - (\theta_0 + 1) \theta^2 + (\theta_0 - 10\sigma_{\theta_0}^2) \theta + 7\sigma_{\theta_0}^2 = 0</script><p>以下为选取不同先验条件时的$L(\theta | D)$图像，用于对比</p>
<blockquote>
<ul>
<li>第一张图为极大似然估计$L(D|\theta)$</li>
<li>第二张图为先验概率密度函数$P(\theta)$</li>
<li>第三张图为最大后验概率估计$L(\theta | D)$，$\hat{\theta}$由查表法求解<br>代码见<a href="https://github.com/isLouisHsu/isLouisHsu.github.io/tree/Hexo/source/_posts//参数估计的几种方法/temp.py" target="_blank" rel="noopener">仓库</a></li>
</ul>
</blockquote>
<ul>
<li><p>$\theta_0 = 0.3, \sigma_{\theta_0} = 0.1$ $\Rightarrow$ $\hat{\theta} = 0.42$<br><img src="/2018/11/19/Parameter-Estimation/MAP_theta0_0.3&#32;sigma0_0.1.png" alt="对比图"></p>
</li>
<li><p>$\theta_0 = 0.5, \sigma_{\theta_0} = 0.1$ $\Rightarrow$ $\hat{\theta} = 0.56$<br><img src="/2018/11/19/Parameter-Estimation/MAP_theta0_0.5&#32;sigma0_0.1.png" alt="对比图"></p>
</li>
<li><p>$\theta_0 = 0.7, \sigma_{\theta_0} = 0.1$ $\Rightarrow$ $\hat{\theta} = 0.70$<br><img src="/2018/11/19/Parameter-Estimation/MAP_theta0_0.7&#32;sigma0_0.1.png" alt="对比图"></p>
</li>
<li><p>$\theta_0 = 0.5, \sigma_{\theta_0} = 0.01$ $\Rightarrow$ $\hat{\theta} = 0.50$<br><img src="/2018/11/19/Parameter-Estimation/MAP_theta0_0.5&#32;sigma0_0.01.png" alt="对比图"></p>
</li>
<li><p>$\theta_0 = 0.5, \sigma_{\theta_0} = 1.0$ $\Rightarrow$ $\hat{\theta} = 0.70$<br><img src="/2018/11/19/Parameter-Estimation/MAP_theta0_0.5&#32;sigma0_1.0.png" alt="对比图"></p>
</li>
</ul>
<blockquote>
<p>结论</p>
<ul>
<li>由图$1, 2, 3$，可以看到当$\theta_0$偏移$0.7$时，$MAP$结果也相应偏移；</li>
<li>由图$2, 4, 5$，可以看到当$\sigma_{\theta_0}^2$越小，即越确定先验概率分布时，$MAP$结果也越趋向于先验概率分布。</li>
</ul>
</blockquote>
<h2 id="贝叶斯估计-1"><a href="#贝叶斯估计-1" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h2><p>先验条件为正态分布</p>
<script type="math/tex; mode=display">
\theta \thicksim N(\theta_0, \sigma_{\theta_0}^2)</script><script type="math/tex; mode=display">
p(\theta | D)
= a · p(\theta) \prod_{i=1}^N p(x_i | \theta)
= a 
· \frac{1}{\sqrt{2\pi}\sigma_{\theta_0}} e^ {-\frac{(\theta - \theta_0)^2}{2\sigma_{\theta_0}^2}} 
· \theta ^7 (1 - \theta) ^3</script><blockquote>
<p>参数$a$使用<code>scipy.integrate.quad</code>求解</p>
</blockquote>
<p>选取不同先验条件时的$L(\theta | D)$图像，用于对比</p>
<ul>
<li><p>$\theta_0 = 0.3, \sigma_{\theta_0} = 0.1$<br><img src="/2018/11/19/Parameter-Estimation/BE_theta0_0.3&#32;sigma0_0.1.png" alt="对比图"></p>
</li>
<li><p>$\theta_0 = 0.5, \sigma_{\theta_0} = 0.1$<br><img src="/2018/11/19/Parameter-Estimation/BE_theta0_0.5&#32;sigma0_0.1.png" alt="对比图"></p>
</li>
<li><p>$\theta_0 = 0.7, \sigma_{\theta_0} = 0.1$<br><img src="/2018/11/19/Parameter-Estimation/BE_theta0_0.7&#32;sigma0_0.1.png" alt="对比图"></p>
</li>
<li><p>$\theta_0 = 0.5, \sigma_{\theta_0} = 0.01$<br><img src="/2018/11/19/Parameter-Estimation/BE_theta0_0.5&#32;sigma0_0.01.png" alt="对比图"></p>
</li>
<li><p>$\theta_0 = 0.5, \sigma_{\theta_0} = 1.0$<br><img src="/2018/11/19/Parameter-Estimation/BE_theta0_0.5&#32;sigma0_1.0.png" alt="对比图"></p>
</li>
</ul>

        </div>
        <div class="post-tool">
            <a class="btn-thumbs-up" href="javascript:void(0);" data-cid="52" title="95">
                <i class="fa fa-thumbs-up" aria-hidden="true"></i> 打赏
            </a>
        </div>
        
        <div class="post-tags">标签：
            
        </div>
        
    </article>
    
    <p style="text-align: center">本文代表个人观点，内容仅供参考。若有不恰当之处，望不吝赐教！</p>
    
    

    

</div>
<script src="/js/busuanzi.pure.mini.js"></script>


        </div><!-- end #main-->
    </div><!-- end #body -->
    <footer class="footer">
    <div class="footer-inner" style="text-align: center">
        <p>
            <a href="/about" title="关于">关于</a>&nbsp;&nbsp<em>·</em>&nbsp;&nbsp
            <!-- 自定义链接 -->
            <a href="/help" title="帮助">帮助</a>&nbsp;&nbsp<em>·</em>&nbsp;&nbsp
            <a href="/links" title="友链">友链</a>&nbsp;&nbsp<em>·</em>&nbsp;&nbsp
            <a href="/sitemap.xml" title="地图">地图</a>
        </p>
        <p>
            本站已建立&nbsp<a href="/timeline" id="siteBuildingTime"></a>&nbsp天，<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="licence">采用知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议创作</a><br>
            ©2017-<span id="cpYear"></span> 基于&nbsp<a href="http://hexo.io" target="_blank" rel="nofollow">Hexo</a>
            ，主题采用&nbsp&nbsp<a href="https://github.com/tangkunyin/hexo-theme-jsimple" target="_blank" rel="bookmark">JSimple</a>
            ，作者&nbsp<a href="https://louishsu.xyz/" target="_blank" rel="friend">徐耀彬</a>
            ，Hosted by <a href="https://pages.github.com/" target="_blank" rel="nofollow">GitHub Pages</a>
        </p>
    </div>
</footer>
<script src="/js/SimpleCore.js"></script>

</div>
<!-- search pop -->
<div class="popup search-popup local-search-popup">
    <div class="local-search-header clearfix">
        <span class="search-icon">
            <i class="fa fa-search"></i>
        </span>
        <span class="popup-btn-close">
            <i class="fa fa-times-circle"></i>
        </span>
        <div class="local-search-input-wrapper">
            <input id="local-search-input" spellcheck="false" type="text" autocomplete="off" placeholder="请输入查询关键词">
        </div>
    </div>
    <div id="local-search-result"></div>
</div>
<div class="fixed-btn">
    <a class="btn-gotop" href="javascript:"> <i class="fa fa-angle-up"></i></a>
</div>
<script>
    $(function () {
        var jsi_config = {
            buildingTime: '10/20/2018',
            current: $('.post-tags').length > 0 ? 'post' : 'archive',
            snsQRCode: '/images/sns-qrcode.png',
            donateImg: '/images/donate-qr.png',
            localSearch: { dbPath: '' },
            readMode: 'day'
        };
        
            jsi_config.localSearch = {
                dbPath: '/search.json',
                trigger: 'auto',
                topN: '1',
                unescape: 'false'
            }
        
        SimpleCore.init(jsi_config);
        
    });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
