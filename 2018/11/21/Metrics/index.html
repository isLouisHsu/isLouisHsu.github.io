<!DOCTYPE HTML>
<html class="no-js" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
    <!--[if lte IE 9]>
<meta http-equiv="refresh" content="0;url=http://yoursite.com/warn.html">
<![endif]-->
<meta charset="utf-8">
<meta http-equiv="X-DNS-Prefetch-Control" content="on">
<link rel="dns-prefetch" href="http://yoursite.com">
<link rel="dns-prefetch" href="//www.google-analytics.com">
<link rel="prefetch" href="http://yoursite.com">
<link rel="prefetch" href="//www.google-analytics.com">


<link rel="prerender" href="http://yoursite.com">

<meta http-equiv="X-UA-Compatible" content="IE=Edge">
<meta name="renderer" content="webkit">
<meta name="viewport" content="width=device-width, initial-scale=1.0,user-scalable=no">
<meta http-equiv="mobile-agent" content="format=html5; url=http://yoursite.com">
<meta name="author" content="Louis Hsu">
<link rel="stylesheet" href="/css/JSimple.css">

<link rel="shortcut icon" href="/images/favicon.png">


<title>metrics - LOUIS&#39; BLOG</title>

<meta name="keywords" content="">

<meta name="description " content="Inside! Insane!">

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                processEscapes: true
            }
        });
    </script>


    

    

</head>
<body>
<div id="nav">
    <nav class="nav-menu">
        <a class="site-name current" href="/" title="说">说</a>
        <a class="site-index current" href="/"><i class="fa fa-home"></i><span>Home</span></a>
        <a href="/archives" title="Archives"><i class="fa fa-archives"></i><span>Archives</span></a>
        <a href="/tags" title="Tags"><i class="fa fa-tags"></i><span>Tags</span></a>
        <!-- custom single page of menus -->
        
        
        <a href="/help" title="帮助">
            <i class="fa fa-question-circle"></i>
            <span>帮助</span>
        </a>
        
    </nav>
</div>

<div class="nav-user">
    <a class="btn-search" href="#"><i class="fa fa-search"></i></a>
    <a class="btn-read-mode" href="#"><i class="fa fa-sun-o"></i></a>
    <a class="btn-sns-qr" href="javascript:"><i class="fa fa-telegram"></i></a>
</div>

<div id="wrapper" class="clearfix">
    <div id="body">
        <div class="main" id="main">
            <div id="cover">
    <div class="cover-img"></div>
    <div class="cover-info">
        
        <h1 class="cover-siteName">说IT</h1>
        <h3 class="cover-siteTitle">用代码摇滚这个世界</h3>
        <p class="cover-siteDesc">一个关注技术与人文的IT博客</p>
        <div class="cover-sns">
            
    &nbsp;&nbsp;<div class="btn btn-telegram">
        <a href="http://t.me/kunyintang" target="_blank" title="telegram" ref="friend">
            <i class="fa fa-telegram"></i>
        </a>
    </div>

    &nbsp;&nbsp;<div class="btn btn-instagram">
        <a href="https://www.instagram.com/mtangsir/" target="_blank" title="instagram" ref="friend">
            <i class="fa fa-instagram"></i>
        </a>
    </div>

    &nbsp;&nbsp;<div class="btn btn-twitter">
        <a href="https://twitter.com/tangkunyin" target="_blank" title="twitter" ref="friend">
            <i class="fa fa-twitter"></i>
        </a>
    </div>

    &nbsp;&nbsp;<div class="btn btn-github">
        <a href="https://github.com/tangkunyin" target="_blank" title="github" ref="friend">
            <i class="fa fa-github"></i>
        </a>
    </div>


        </div>
    </div>
</div>

            <div class="page-title">
    <ul>
        <li><a href="/">Recent Posts</a></li>
        
            
                <li class="">
                    <a href="/categories/tech-notes" data-name="技术">技术</a>
                </li>
            
                <li class="">
                    <a href="/categories/humanities" data-name="人文">人文</a>
                </li>
            
                <li class="">
                    <a href="/categories/others" data-name="其他">其他</a>
                </li>
            
        
        <li class="page-search">
    <form id="search" class="search-form">
        <input type="text" readonly="readonly" id="local-search-input-tip" placeholder="click to search...">
        <button type="button" disabled="disabled" class="search-form-submit"><i class="fa fa-search"></i></button>
    </form>
</li>

    </ul>
</div>
<div class="main-inner">
    <article class="post" itemscope="" itemtype="http://schema.org/BlogPosting">
        <div class="post-header">
            <div class="post-author clearfix">
                <a class="avatar fleft" href="https://shuoit.net" target="_blank">
                    <img width="48" src="/images/favicon.png" alt="avatar">
                </a>
                <p><span class="label">Author</span>
                    <a href="https://shuoit.net" target="_blank">纠结伦</a>
                    <span title="Last edited at&nbsp;2018-11-21">2018-11-21</span>
                </p>
                <p>一个搬🧱的劳斯基😁️️</p>
            </div>
            <h2 class="post-title">Metrics</h2>
            <div class="post-meta">
                emm... 13589 words in the article |
                you are the&nbsp;<span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span>th friend who reading now
            </div>
        </div>
        <div class="post-content markdown-body">
            <h1 id="回归-regression-评估指标"><a href="#回归-regression-评估指标" class="headerlink" title="回归(regression)评估指标"></a>回归(regression)评估指标</h1><h2 id="解释方差-Explained-Variance"><a href="#解释方差-Explained-Variance" class="headerlink" title="解释方差(Explained Variance)"></a>解释方差(Explained Variance)</h2><script type="math/tex; mode=display">
EV(\hat{y}, y)
= 1 - \frac{Var(y-\hat{y})}{Var(y)}</script><p>解释方差越接近$1$表示回归效果越好。</p>
<h2 id="平均绝对误差-Mean-Absolute-Error-MAE"><a href="#平均绝对误差-Mean-Absolute-Error-MAE" class="headerlink" title="平均绝对误差(Mean Absolute Error - MAE)"></a>平均绝对误差(Mean Absolute Error - MAE)</h2><script type="math/tex; mode=display">
MAE(\hat{y}, y) 
= E(||\hat{y} - y||_1)
= \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} |\hat{y}^{(i)} - y^{(i)}|</script><p>$MAE$越小表示回归效果越好。</p>
<h2 id="平均平方误差-Mean-Squared-Error-MSE"><a href="#平均平方误差-Mean-Squared-Error-MSE" class="headerlink" title="平均平方误差(Mean Squared Error - MSE)"></a>平均平方误差(Mean Squared Error - MSE)</h2><p>在<a href="https://louishsu.xyz/2018/10/18/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">线性回归</a>一节，使用的损失函数即$MSE$</p>
<script type="math/tex; mode=display">
MSE(\hat{y}, y) 
= E(||\hat{y} - y||_2^2)
= \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} (\hat{y}^{(i)} - y^{(i)})^2</script><p>其中$y$与$\hat{y}$均为$1$维向量，$MSE$越小表示回归效果越好。</p>
<p>其含义比较直观，即偏差的平方和。也可以从最小化方差的角度解释，定义误差向量</p>
<script type="math/tex; mode=display">
e = \hat{y} - y</script><p>我们假定其期望为$0$，即</p>
<script type="math/tex; mode=display">
E(e) = 0　或　\overline{e} = 0</script><p>那么误差的方差为</p>
<script type="math/tex; mode=display">
Var(e) = E[(e - \overline{e})^T (e - \overline{e})] = E(||e||_2^2)</script><p>也即$MSE$。</p>
<h2 id="均方根误差-Root-Mean-Squared-Error-RMSE"><a href="#均方根误差-Root-Mean-Squared-Error-RMSE" class="headerlink" title="均方根误差(Root Mean Squared Error - RMSE)"></a>均方根误差(Root Mean Squared Error - RMSE)</h2><script type="math/tex; mode=display">
RMSE(\hat{y}, y) 
= \sqrt{\frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} (\hat{y}^{(i)} - y^{(i)})^2}</script><p>实质与$MSE$是一样的。只不过用于数据更好的描述，使计算得损失的值较小。$RMSE$越小表示回归效果越好。</p>
<h2 id="均方对数误差-Mean-Squard-Logarithmic-Error-MSLE"><a href="#均方对数误差-Mean-Squard-Logarithmic-Error-MSLE" class="headerlink" title="均方对数误差(Mean Squard Logarithmic Error - MSLE)"></a>均方对数误差(Mean Squard Logarithmic Error - MSLE)</h2><script type="math/tex; mode=display">
MSLE(\hat{y}, y) = \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} \left[\log (1+y^{(i)}) - \log (1+\hat{y}^{(i)})\right]^2</script><p>通常用于输出指数增长的模型，如，人口统计，商品的平均销售量，以及一段时间内的平均销售量等。注意，由对数性质，这一指标对过小的预测的惩罚大于预测过大的预测的惩罚。</p>
<h2 id="中值绝对误差-Median-Absolute-Error-MedAE"><a href="#中值绝对误差-Median-Absolute-Error-MedAE" class="headerlink" title="中值绝对误差(Median Absolute Error - MedAE)"></a>中值绝对误差(Median Absolute Error - MedAE)</h2><script type="math/tex; mode=display">
MedAE(\hat{y}, y) = median(|y - \hat{y}|)</script><h2 id="R决定系数-R2"><a href="#R决定系数-R2" class="headerlink" title="R决定系数(R2)"></a>R决定系数(R2)</h2><p>又称拟合优度，提供了一个衡量未来样本有多好的预测模型。最佳可能的分数是$1.0$，它可以是负的(因为模型可以任意恶化)。一个常数模型总是预测$y$的期望值，而不考虑输入特性，则得到$R^2$分数为$0.0$。</p>
<script type="math/tex; mode=display">
R^2(\hat{y}, y) = 1 - \frac{\sum_{i=1}^{n_{samples}} (y^{(i)} - \hat{y}^{(i)})^2}{\sum_{i=1}^{n_{samples}} (y^{(i)} - \overline{y})^2}</script><p>其中</p>
<script type="math/tex; mode=display">
\overline{y} = \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} y^{(i)}</script><h1 id="分类-classification-评估指标"><a href="#分类-classification-评估指标" class="headerlink" title="分类(classification)评估指标"></a>分类(classification)评估指标</h1><p>先作如下定义<br><img src="/2018/11/21/Metrics/terminology_and_derivations_1.png" alt="terminology_and_derivations_1"><br><img src="/2018/11/21/Metrics/terminology_and_derivations_2.png" alt="terminology_and_derivations_2"><br><img src="/2018/11/21/Metrics/terminology_and_derivations_3.png" alt="terminology_and_derivations_3"></p>
<p><img src="/2018/11/21/Metrics/metrics_classification2.png" alt="metrics_classification2"></p>
<h2 id="准确率-Accuracy"><a href="#准确率-Accuracy" class="headerlink" title="准确率(Accuracy)"></a>准确率(Accuracy)</h2><script type="math/tex; mode=display">
Accuracy(y, \hat{y})
= \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} 1(y^{(i)}=\hat{y}^{(i)})</script><p>也即</p>
<script type="math/tex; mode=display">
Accuracy
= \frac{TN+TP}{TN+TP+FN+FP}</script><p>精度只是简单地计算出比例，但是没有对不同类别进行区分。因为不同类别错误代价可能不同。例如：判断这个病人是不是病危，如果不是病危错误判断为病危，那只是损失一点医务人员的时间和精力，如果是把病危的人判断为非病危状态，那损失的就是一条人命。他们之间存在重要性差异，这时候就不能用精度。对于样本不均衡的情况，也不是用精度来衡量。例如：有A类1000个，B类5个，如果我把这1005个样本都预测成A类，正确率=1000/1005=99.5%。</p>
<h2 id="精确率-Precision-与召回率-Recall"><a href="#精确率-Precision-与召回率-Recall" class="headerlink" title="精确率(Precision)与召回率(Recall)"></a>精确率(Precision)与召回率(Recall)</h2><ul>
<li><p>精确率<code>(Precision)</code><br>  即预测正样本中，实际为正样本的百分比，度量了分类器不会将真正的负样本错误地分为正样本的能力。</p>
<script type="math/tex; mode=display">
  Precision = \frac{TP}{TP+FP}</script></li>
<li><p>召回率<code>(Recall)</code><br>  又称查全率，即实际正样本中，被预测为正样本的百分比，度量了分类器找到所有正样本的能力。</p>
<script type="math/tex; mode=display">
  Recall = \frac{TP}{TP + FN}</script><p><img src="/2018/11/21/Metrics/precision_recall.png" alt="precision_recall"></p>
</li>
</ul>
<h2 id="F度量"><a href="#F度量" class="headerlink" title="F度量"></a>F度量</h2><blockquote>
<p><a href="https://en.wikipedia.org/wiki/F1_score" target="_blank" rel="noopener">F1 score - Wikipedia</a></p>
</blockquote>
<ul>
<li><p>$F_1$<br>  为精确率<code>(Precision)</code>与召回率<code>(Recall)</code>的调和均值<code>(harmonic mean)</code>。</p>
<script type="math/tex; mode=display">
  \frac{1}{F_1} 
  = \frac{1}{2} (\frac{1}{Precision} + \frac{1}{Recall})</script><p>  也即</p>
<script type="math/tex; mode=display">
  F_1 = 2 · \frac{Precision·Recall}{Precision + Recall}</script></li>
<li><p>$F_{\beta}$<br>  在$F_1$度量的基础上增加权值$\beta$，$\beta$越大，$Recall$的权重越大，否则$Precision$的权重越大。</p>
<script type="math/tex; mode=display">
  \frac{1}{F_{\beta}} = \frac{1}{1+\beta^2} \frac{1}{Precision} + \frac{\beta^2}{1+\beta^2}\frac{1}{Recall}</script><p>  也即</p>
<script type="math/tex; mode=display">
  F_{\beta} = (1+\beta^2)·\frac{Precision·Recall}{(\beta^2·Precision) + Recall}</script></li>
</ul>
<h2 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h2><p><code>Confusion matrix</code>，也被称作错误矩阵<code>(Error matrix)</code>，是一个特别的表。无监督学习中，通常称作匹配矩阵<code>(Matching matrix)</code>。每一列表达了分类器对样本的类别预测，每一行表达了样本所属的真实类别。</p>
<p>例如我们有$27$个待分类样本，将其划分为<code>Cat</code>，<code>Dog</code>，<code>Rabbit</code>，讲实际标签与预测标签数目统计后填入混淆矩阵。</p>
<p><img src="/2018/11/21/Metrics/confusion_matrix.png" alt="confusion_matrix"></p>
<p>例如实际上有$8$个样本为<code>Cat</code>，而该分类器将其中$3$个划分为<code>Dog</code>，将$2$个为<code>Dog</code>的样本划分为<code>Cat</code>。我们可以根据上述混淆矩阵得出结论，该分类器对<code>Dog</code>和<code>Cat</code>分类能力较弱，而对<code>Rabbit</code>分类能力较强。而且正确预测的样本数目都在对角线上，很容易直观地检查表中的预测错误。</p>
<p>以下为<code>scikit-learn</code>中混淆矩阵的<code>API</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from sklearn.metrics import confusion_matrix</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; y_true = [2, 0, 2, 2, 0, 1]</span><br><span class="line">&gt;&gt;&gt; y_pred = [0, 0, 2, 2, 0, 2]</span><br><span class="line">&gt;&gt;&gt; confusion_matrix(y_true, y_pred)</span><br><span class="line">array([[2, 0, 0],</span><br><span class="line">       [0, 0, 1],</span><br><span class="line">       [1, 0, 2]])</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; y_true = [&quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;bird&quot;]</span><br><span class="line">&gt;&gt;&gt; y_pred = [&quot;ant&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;]</span><br><span class="line">&gt;&gt;&gt; confusion_matrix(y_true, y_pred, labels=[&quot;ant&quot;, &quot;bird&quot;, &quot;cat&quot;])</span><br><span class="line">array([[2, 0, 0],</span><br><span class="line">       [0, 0, 1],</span><br><span class="line">       [1, 0, 2]])</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; # In the binary case, we can extract true positives, etc as follows:</span><br><span class="line">&gt;&gt;&gt; tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()</span><br><span class="line">&gt;&gt;&gt; (tn, fp, fn, tp)</span><br><span class="line">(0, 2, 1, 1)</span><br></pre></td></tr></table></figure></p>
<h2 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h2><p><code>Receiver Operating Characteristic</code>，是根据一系列不同的二分类方式(分界值或决定阈)，以召回率(真正率<code>TPR</code>、灵敏度)为纵坐标，<code>fall-out</code>(假正率<code>FPR</code>、$1$-特异度)为横坐标绘制的曲线。</p>
<ul>
<li><p><code>true positive rate - TPR</code><br>  所有阳性样本中有多少正确的阳性结果。</p>
<script type="math/tex; mode=display">
  TPR = \frac{TP}{P} = \frac{TP}{TP + FN}</script></li>
<li><p><code>false positive rate - FPR</code><br>  所有阴性样本中有多少不正确的阳性结果。</p>
<script type="math/tex; mode=display">
  FPR = \frac{FP}{N} = \frac{FP}{FP + TN}</script></li>
</ul>
<h3 id="ROC-space"><a href="#ROC-space" class="headerlink" title="ROC space"></a>ROC space</h3><p><img src="/2018/11/21/Metrics/模型的评估指标/ROC_space.png" alt="ROC_space"></p>
<ul>
<li>分别以<code>FPR</code>与<code>TPR</code>作为横纵轴(又称灵敏度-$1$特异度曲线<code>sensitivity vs (1 − specificity) plot</code>)；</li>
<li>每次预测结果或混淆矩阵的实例代表了<code>ROC</code>空间中的一个点；<br>  例如上图中$A, B, C, C’$是以下表数据计算得到的点。<br>  <img src="/2018/11/21/Metrics/ROC_space_samples.png" alt="ROC_space_samples"></li>
<li>在<code>ROC</code>空间中最左上方的点$(0, 1)$称作完美分类器<code>(perfect classification)</code>；</li>
<li>随机分类器的结果分布在<code>ROC space</code>对角线$(0, 0)-(1, 1)$上，当实验次数足够多，其分区趋向$(0.5, 0.5)$;</li>
<li>对角线以上的点代表好的分类结果(比随机的好)；线下的点代表坏的结果(比随机的差)；</li>
<li>注意，持续不良分类器的输出可以简单地反转以获得一个好的分类器，反转后的分类器与原分类器在平面上关于对角线对称，例如点$C’$。</li>
</ul>
<h3 id="ROC曲线的绘制"><a href="#ROC曲线的绘制" class="headerlink" title="ROC曲线的绘制"></a>ROC曲线的绘制</h3><p>若训练集样本中，正样本与负样本以正态分布的形式分布在样本平面上，如下图，左峰为负样本，右峰为正样本，存在部分重叠(不然就不用搞这么多分类算法了)。</p>
<p><img src="/2018/11/21/Metrics/ROC_curves.svg.png" alt="ROC_curves.svg"></p>
<p>若假设正样本概率密度为$f_1(x)$，负样本的概率密度为$f_0(x)$，给定阈值$T$，则右</p>
<script type="math/tex; mode=display">
TPR(T) = \int_T^{\infty} f_1(x) dx</script><script type="math/tex; mode=display">
FPR(T) = \int_T^{\infty} f_0(x) dx</script><p>选取不同的阈值划分分类器输出，就能得到<code>ROC</code>曲线。</p>
<p>在基于有限样本作<code>ROC</code>图时，可以看到曲线每次都是一个“爬坡”，遇到正例往上爬一格$(1/m+)$，错了往右爬一格$(1/m-)$，显然往上爬对于算法性能来说是最好的。<br><img src="/2018/11/21/Metrics/ROC_curves_up_right.png" alt="ROC_curves_up_right"></p>
<h3 id="Area-Under-the-Curve-AUC"><a href="#Area-Under-the-Curve-AUC" class="headerlink" title="Area Under the Curve - AUC"></a>Area Under the Curve - AUC</h3><p><code>ROC</code>曲线下的面积<code>AUC</code>物理意义为，任取一对正负样本，正样本的预测值大于负样本的预测值的概率。</p>
<script type="math/tex; mode=display">
A = \int_{-\infty}^{\infty} TPR(T) dFPR(T)</script><script type="math/tex; mode=display">
= \int_{-\infty}^{\infty} 
\int_{-\infty}^{\infty}
I(T'> T)
f_1(T') f_0(T)
dT' dT</script><script type="math/tex; mode=display">
= P(X_1 > X_0)</script><p>同样的，在有限个样本下，其面积用累加的方法计算(梯形面积)</p>
<p><img src="/2018/11/21/Metrics/ROC_curves_AUC.png" alt="ROC_curves_AUC"></p>
<script type="math/tex; mode=display">
AUC = \sum_{i=1}^{m-1} \frac{1}{2} (y_{i+1} + y_i)(x_{i+1} - x_i)</script><ul>
<li>$AUC = 1$，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。</li>
<li>$0.5 &lt; AUC &lt; 1$，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。</li>
<li>$AUC = 0.5$，跟随机猜测一样（例：丢铜板），模型没有预测价值。</li>
<li>$AUC &lt; 0.5$，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。</li>
</ul>
<h3 id="sklearn"><a href="#sklearn" class="headerlink" title="sklearn"></a>sklearn</h3><p>以下为<code>scikit-learn</code>中混淆矩阵的<code>ROC</code>曲线<code>API</code>。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import numpy as np</span><br><span class="line">&gt;&gt;&gt; from sklearn import metrics</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; y = np.array([1, 1, 2, 2])</span><br><span class="line">&gt;&gt;&gt; scores = np.array([0.1, 0.4, 0.35, 0.8])</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)</span><br><span class="line">&gt;&gt;&gt; fpr</span><br><span class="line">array([ 0. ,  0.5,  0.5,  1. ])</span><br><span class="line">&gt;&gt;&gt; tpr</span><br><span class="line">array([ 0.5,  0.5,  1. ,  1. ])</span><br><span class="line">&gt;&gt;&gt; thresholds</span><br><span class="line">array([ 0.8 ,  0.4 ,  0.35,  0.1 ])</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; metrics.auc(fpr, tpr)</span><br><span class="line">0.75</span><br></pre></td></tr></table></figure></p>
<h1 id="聚类-clustering-评估指标"><a href="#聚类-clustering-评估指标" class="headerlink" title="聚类(clustering)评估指标"></a>聚类(clustering)评估指标</h1><blockquote>
<ul>
<li><a href="https://blog.csdn.net/darkrabbit/article/details/80378597" target="_blank" rel="noopener">AI（005） - 笔记 - 聚类性能评估（Clustering Evaluation） - DarkRabbit的专栏 - CSDN博客 </a></li>
<li><a href="https://en.wikipedia.org/wiki/Main_Page" target="_blank" rel="noopener">Wikipedia, the free encyclopedia</a></li>
</ul>
</blockquote>
<h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>聚类性能比较好，就是聚类结果簇内相似度<code>(intra-cluster similarity)</code>高，而簇间相似度<code>(inter-cluster similarity)</code>低，即同一簇的样本尽可能的相似，不同簇的样本尽可能不同。</p>
<p>聚类性能的评估（度量）分为两大类：</p>
<ul>
<li>外部评估<code>(external evaluation)</code>：将结果与某个参考模型<code>(reference model)</code>进行比较；</li>
<li>内部评估<code>(internal evaluation)</code>：直接考虑聚类结果而不利用任何参考模型。</li>
</ul>
<p>将$n_{samples}$个样本$\{x^{(1)}, …, x^{(n_{samples})}\}$用待评估聚类算法划分为$K$个类$\{X_1, …, X_K\}$，假定参考模型将其划分为$L$类$\{Y_1, …, Y_L\}$，将样本两辆匹配</p>
<script type="math/tex; mode=display">
\begin{cases}
    a = |SS| &  SS = \{(x^{(i)}, x^{(j)}) | x^{(i)}, x^{(j)} \in X_k; x^{(i)}, x^{(j)} \in Y_l\} \\
    b = |SD| &  SD = \{(x^{(i)}, x^{(j)}) | x^{(i)}, x^{(j)} \in X_k; x^{(i)} \in Y_{l1}, x^{(j)} \in Y_{l2}\} \\
    c = |DS| &  DS = \{(x^{(i)}, x^{(j)}) | x^{(i)} \in X_{k1}, x^{(j)} \in X_{k2}; x^{(i)}, x^{(j)} \in Y_l\} \\
    d = |DD| &  DD = \{(x^{(i)}, x^{(j)}) | x^{(i)} \in X_{k1}, x^{(j)} \in X_{k2}; x^{(i)} \in Y_{l1}, x^{(j)} \in Y_{l2}\}
\end{cases}</script><p>其中$k = 1, …, K; l = 1, …, L$</p>
<script type="math/tex; mode=display">
a + b + c + d
=   \left(
        \begin{matrix}
            n \\ 2
        \end{matrix}
    \right)
= \frac{n(n-1)}{2}</script><blockquote>
<ul>
<li>$SS$包含两种划分中均属于同一类的样本对；</li>
<li>$SD$包含用待评估聚类算法划分中属于同一类，而在参考模型中属于不同类的样本对；</li>
<li>$DS$包含用待评估聚类算法划分中属于不同类，而在参考模型中属于同一类的样本对；</li>
<li>$DD$包含两种划分中均不属于同一类的样本对。</li>
</ul>
</blockquote>
<h2 id="常用外部评估-external-evaluation"><a href="#常用外部评估-external-evaluation" class="headerlink" title="常用外部评估(external evaluation)"></a>常用外部评估(external evaluation)</h2><h3 id="Rand-Index-RI"><a href="#Rand-Index-RI" class="headerlink" title="Rand Index(RI)"></a>Rand Index(RI)</h3><blockquote>
<p><a href="https://en.wikipedia.org/wiki/Rand_index" target="_blank" rel="noopener">Rand index - Wikipedia</a></p>
</blockquote>
<script type="math/tex; mode=display">
RI = \frac{a+d}{a + b + c + d} = \frac{a+d}{\left(\begin{matrix} n \\ 2 \end{matrix}\right)}</script><p>显然，结果值在$[0,1]$之间，且值越大越好。</p>
<ul>
<li>当为$0$时，两个聚类无重叠；</li>
<li>当为$1$时，两个聚类完全重叠。</li>
</ul>
<h3 id="Adjust-Rand-Index-ARI"><a href="#Adjust-Rand-Index-ARI" class="headerlink" title="Adjust Rand Index(ARI)"></a>Adjust Rand Index(ARI)</h3><p>让$RI$有了修正机会<code>(corrected-for-chance)</code>，在取值上从$[0,1]$变成$[-1, 1]$</p>
<p>对于$X$与$Y$的重叠可以用一个列联表<code>(contingency table)</code>表示，记作$[n_{ij}]$，$n_{ij} = |X_i \bigcap Y_j|$<br><img src="/2018/11/21/Metrics/聚类/ARI.svg" alt="ARI"></p>
<p>则定义$ARI$如下<br><img src="/2018/11/21/Metrics/聚类/ARI_Def.svg" alt="ARI_Def"></p>
<h3 id="互信息与调整互信息-Adjusted-Mutual-Information-AMI"><a href="#互信息与调整互信息-Adjusted-Mutual-Information-AMI" class="headerlink" title="互信息与调整互信息(Adjusted Mutual Information - AMI)"></a>互信息与调整互信息(Adjusted Mutual Information - AMI)</h3><blockquote>
<p>关于互信息可查看<a href="">熵</a>一节说明。</p>
</blockquote>
<p>$X_i$类别的概率定义为</p>
<script type="math/tex; mode=display">
P(k) = \frac{|X_k|}{N}</script><p>则划分结果的熵定义为</p>
<script type="math/tex; mode=display">
H(X) = - \sum_k P(k) \log P(k)</script><p>类似的</p>
<script type="math/tex; mode=display">
P'(l) = \frac{|Y_l|}{N}</script><script type="math/tex; mode=display">
H(Y) = - \sum_j P'(l) \log P'(l)</script><p>另外</p>
<script type="math/tex; mode=display">
P(k, l) = \frac{|X_k, Y_l|}{N}</script><p>那么两种划分的互信息定义为</p>
<script type="math/tex; mode=display">
MI(X, Y) = \sum_{k, l} P(k, l) \log \frac{P(k, l)}{P(k) P'(l)}</script><p>和$ARI$一样，我们对它进行调整。</p>
<script type="math/tex; mode=display">
E[MI(X, Y)] 
= \sum_k \sum_l \sum_{n_{kl} = \max\{1, a_k + b_l - N\}}^{\min \{a_k, b_l\}}
\frac{n_{kl}}{N}
\log \left( \frac{N·n_{kl}}{a_k b_l} \right) ×</script><script type="math/tex; mode=display">
\frac
{a_k!b_l!(N-a_k)!(N-b_l)!}
{N!n_{kl}!(a_k-n_{kl})!(b_l-n_{kl})!(N-a_k-b_l+n_{kl})!}</script><p>最终$AMI$表达式为</p>
<script type="math/tex; mode=display">
AMI(X, Y) = \frac{MI(X, Y) - E[MI(X, Y)]}{\max \{H(X), H(Y)\} - E[MI(X, Y)]}</script><h3 id="同质性-Homogeneity-与完整性-Completeness"><a href="#同质性-Homogeneity-与完整性-Completeness" class="headerlink" title="同质性(Homogeneity)与完整性(Completeness)"></a>同质性(Homogeneity)与完整性(Completeness)</h3><p>这两个类似分类种的的准确率<code>(accuracy)</code>与召回率<code>(recall)</code>。</p>
<ul>
<li><p>同质性<code>(Homogeneity)</code><br>  即一个簇仅包含一个类别的样本</p>
<script type="math/tex; mode=display">
  H = 1 - \frac{H(X|Y)}{H(X)}</script><p>  其中$H(X|Y)$为条件熵</p>
<script type="math/tex; mode=display">
  H(X|Y) = \sum_k \sum_l P(X_k, Y_l) \log \frac{P(Y_l)}{P(X_k, Y_l)}
  = \sum_k \sum_l \frac{n_{kl}}{N} \log \frac{n_{kl}}{N}</script></li>
<li><p>完整性<code>(Completeness)</code><br>  同类别样本被归类到相同簇中</p>
<script type="math/tex; mode=display">
  C = 1 - \frac{H(Y|X)}{H(Y)}</script></li>
<li><p>$V-measure$<br>  <code>Homogeneity</code>和<code>Completeness</code>的调和平均</p>
<script type="math/tex; mode=display">
  V = \frac{1}{\frac{1}{2} \left(\frac{1}{H} + \frac{1}{C}\right)} = \frac{2HC}{H + C}</script></li>
</ul>
<h3 id="Fowlkes-Mallows-index-FMI"><a href="#Fowlkes-Mallows-index-FMI" class="headerlink" title="Fowlkes-Mallows index(FMI)"></a>Fowlkes-Mallows index(FMI)</h3><p>成对精度和召回率的几何均值</p>
<blockquote>
<p><a href="https://en.wikipedia.org/wiki/Fowlkes%E2%80%93Mallows_index" target="_blank" rel="noopener">Fowlkes–Mallows index - Wikipedia</a></p>
</blockquote>
<p>定义</p>
<ul>
<li>$TP$ as the number of points that are present in the same cluster in both $A_1$ and $A_2$.</li>
<li>$FP$ as the number of points that are present in the same cluster in $A_1$ but not in $A_2$.</li>
<li>$FN$ as the number of points that are present in the same cluster in $A_2$ but not in $A_1$.</li>
<li>$TN$ as the number of points that are in different clusters in both $A_1$ and $A_2$.</li>
</ul>
<p>则</p>
<script type="math/tex; mode=display">
TP + FP + TN + FN = \frac{n(n-1)}{2}</script><p>定义</p>
<script type="math/tex; mode=display">
FMI = \sqrt{\frac{TP}{TP + FP} · \frac{TP}{TP + FN}}</script><h3 id="杰卡德系数-Jaccard-Coefficient-JC"><a href="#杰卡德系数-Jaccard-Coefficient-JC" class="headerlink" title="杰卡德系数(Jaccard Coefficient - JC)"></a>杰卡德系数(Jaccard Coefficient - JC)</h3><blockquote>
<p><a href="https://en.wikipedia.org/wiki/Jaccard_index" target="_blank" rel="noopener">Jaccard index - Wikipedia</a></p>
</blockquote>
<p>给定两个具有$n$个元素的集合$A, B$，定义</p>
<ul>
<li>$M_{11}$ represents the total number of attributes where $A$ and $B$ both have a value of $1$.</li>
<li>$M_{01}$ represents the total number of attributes where the attribute of $A$ is $0$ and the attribute of $B$ is $1$.</li>
<li>$M_{10}$ represents the total number of attributes where the attribute of $A$ is $1$ and the attribute of $B$ is $0$.</li>
<li>$M_{00}$ represents the total number of attributes where $A$ and $B$ both have a value of $0$.</li>
</ul>
<p>则有</p>
<script type="math/tex; mode=display">
M_{11} + M_{01} + M_{10} + M_{00} = n</script><ul>
<li><p><code>Jaccard</code>相似度系数</p>
<script type="math/tex; mode=display">
  J = \frac{M_{11}}{M_{01} + M_{10} + M_{11}}</script><blockquote>
<p>也即$J=\frac{A \cap B}{A \cup B}$</p>
</blockquote>
</li>
<li><p><code>Jaccard</code>距离</p>
<script type="math/tex; mode=display">
  D_J = 1 - J</script></li>
</ul>
<h2 id="常用内部评估-internal-evaluation"><a href="#常用内部评估-internal-evaluation" class="headerlink" title="常用内部评估(internal evaluation)"></a>常用内部评估(internal evaluation)</h2><h3 id="轮廓系数-Silhouette-coefficient"><a href="#轮廓系数-Silhouette-coefficient" class="headerlink" title="轮廓系数(Silhouette coefficient)"></a>轮廓系数(Silhouette coefficient)</h3><p>又称侧影法，适用于实际类别信息未知的情况，对其中一个样本点$x^{(i)}$，记</p>
<ul>
<li>$a(i)$：到本簇其他样本点的距离的平均值</li>
<li>$b(i)$：该点到其他各个簇的样本点的平均距离的最小值</li>
</ul>
<p>定义轮廓系数</p>
<script type="math/tex; mode=display">
S(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}</script><p>或者</p>
<script type="math/tex; mode=display">
S(i) = \begin{cases}
    1 - \frac{a(i)}{b(i)} & a(i) < b(i) \\
    0 & a(i) = b(i) \\
    \frac{b(i)}{a(i)} - 1 & a(i) > b(i)
\end{cases}</script><p>其含义如下</p>
<ul>
<li>当$a(i) \ll b(i)$时，无限接近于$1$，则意味着聚类合适；</li>
<li>当$a(i) \gg b(i)$时，无限接近于$-1$，则意味着把样本i聚类到相邻簇中更合适；</li>
<li>当$a(i)\approxeq b(i)$时，无限接近于$0$，则意味着样本在两个簇交集处。</li>
</ul>
<p>一般再对各个点的轮廓系数求均值</p>
<script type="math/tex; mode=display">
\overline{S} = \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} S(i)</script><ul>
<li>当$\overline{S} &gt; 0.5$，表示聚类合适；</li>
<li>当$\overline{S} &lt; 0.2$，表示表明数据不存在聚类特征</li>
</ul>
<h3 id="Calinski-Harabaz-CH"><a href="#Calinski-Harabaz-CH" class="headerlink" title="Calinski-Harabaz(CH)"></a>Calinski-Harabaz(CH)</h3><p>也适用于实际类别信息未知的情况，以$K$分类为例</p>
<ul>
<li><p>类内散度$W$</p>
<script type="math/tex; mode=display">
  W(K) = \sum_k \sum_{C(j)=k} ||x_j - \overline{x_k}||^2</script></li>
<li><p>类间散度$B$</p>
<script type="math/tex; mode=display">
  B(K) = \sum_k a_k ||\overline{x_k} - \overline{x}||^2</script></li>
<li><p>$CH$</p>
<script type="math/tex; mode=display">
  CH(K) = \frac{B(K)(N-K)}{W(K)(K-1)}</script></li>
</ul>
<h3 id="Davies-Bouldin-Index-DBI"><a href="#Davies-Bouldin-Index-DBI" class="headerlink" title="Davies-Bouldin Index(DBI)"></a>Davies-Bouldin Index(DBI)</h3><p>定义</p>
<ul>
<li>$c_k$：簇$C_k$的中心点</li>
<li>$\sigma_k$：簇$C_k$中所有元素到$c_k$的距离的均值</li>
<li>$d(c_i, c_j)$：簇中心$c_i$与$c_j$之间的距离</li>
</ul>
<p>则</p>
<script type="math/tex; mode=display">
DBI = \frac{1}{K} \sum_{i=1}^K \max_{j \neq i} \left( \frac{\sigma_i + \sigma_j}{d(c_i, c_j)} \right)</script><p>$DBI$越小越好</p>
<h3 id="Dunn-index-DI"><a href="#Dunn-index-DI" class="headerlink" title="Dunn index(DI)"></a>Dunn index(DI)</h3><p>定义</p>
<ul>
<li>$d(i,j)$：两类簇的距离，定义方法多样，例如两类簇中心的距离；</li>
<li>$d’(k)$：簇$C_k$的类内距离，同样的，可定义多种，例如簇$C_k$中任意两点距离的最大值。</li>
</ul>
<p>则</p>
<script type="math/tex; mode=display">
DI = \frac{\min_{1 \leq i < j \leq K} d(i, j)}{\max_{1 \leq k \leq K} d'(k)}</script><h1 id="sklearn中的评价指标"><a href="#sklearn中的评价指标" class="headerlink" title="sklearn中的评价指标"></a>sklearn中的评价指标</h1><blockquote>
<p><a href="http://sklearn.apachecn.org/en/0.19.0/modules/model_evaluation.html" target="_blank" rel="noopener">3.3. Model evaluation: quantifying the quality of predictions — scikit-learn 0.19.0 documentation - ApacheCN</a></p>
</blockquote>
<p><img src="/2018/11/21/Metrics/sklearn_metrics.png" alt="sklearn_metrics"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from sklearn import metrics</span><br><span class="line">&gt;&gt;&gt; dir(metrics)</span><br><span class="line">[&apos;SCORERS&apos;, &apos;__all__&apos;, &apos;__builtins__&apos;, &apos;__cached__&apos;, &apos;__doc__&apos;, </span><br><span class="line">&apos;__file__&apos;, &apos;__loader__&apos;, &apos;__name__&apos;, &apos;__package__&apos;, &apos;__path__&apos;, &apos;__spec__&apos;,</span><br><span class="line"> &apos;accuracy_score&apos;, &apos;adjusted_mutual_info_score&apos;, &apos;adjusted_rand_score&apos;, </span><br><span class="line"> &apos;auc&apos;, &apos;average_precision_score&apos;, &apos;balanced_accuracy_score&apos;, </span><br><span class="line"> &apos;base&apos;, &apos;brier_score_loss&apos;, &apos;calinski_harabaz_score&apos;, &apos;check_scoring&apos;, </span><br><span class="line"> &apos;classification&apos;, &apos;classification_report&apos;, &apos;cluster&apos;, &apos;cohen_kappa_score&apos;, </span><br><span class="line"> &apos;completeness_score&apos;, &apos;confusion_matrix&apos;, &apos;consensus_score&apos;, </span><br><span class="line"> &apos;coverage_error&apos;, &apos;davies_bouldin_score&apos;, &apos;euclidean_distances&apos;, </span><br><span class="line"> &apos;explained_variance_score&apos;, &apos;f1_score&apos;, &apos;fbeta_score&apos;, </span><br><span class="line"> &apos;fowlkes_mallows_score&apos;, &apos;get_scorer&apos;, &apos;hamming_loss&apos;, &apos;hinge_loss&apos;, </span><br><span class="line"> &apos;homogeneity_completeness_v_measure&apos;, &apos;homogeneity_score&apos;, </span><br><span class="line"> &apos;jaccard_similarity_score&apos;, &apos;label_ranking_average_precision_score&apos;, </span><br><span class="line"> &apos;label_ranking_loss&apos;, &apos;log_loss&apos;, &apos;make_scorer&apos;, &apos;matthews_corrcoef&apos;, </span><br><span class="line"> &apos;mean_absolute_error&apos;, &apos;mean_squared_error&apos;, &apos;mean_squared_log_error&apos;, </span><br><span class="line"> &apos;median_absolute_error&apos;, &apos;mutual_info_score&apos;, </span><br><span class="line"> &apos;normalized_mutual_info_score&apos;, &apos;pairwise&apos;, &apos;pairwise_distances&apos;, </span><br><span class="line"> &apos;pairwise_distances_argmin&apos;, &apos;pairwise_distances_argmin_min&apos;, </span><br><span class="line"> &apos;pairwise_distances_chunked&apos;, &apos;pairwise_fast&apos;, &apos;pairwise_kernels&apos;, </span><br><span class="line"> &apos;precision_recall_curve&apos;, &apos;precision_recall_fscore_support&apos;, </span><br><span class="line"> &apos;precision_score&apos;, &apos;r2_score&apos;, &apos;ranking&apos;, &apos;recall_score&apos;, &apos;regression&apos;, </span><br><span class="line"> &apos;roc_auc_score&apos;, &apos;roc_curve&apos;, &apos;scorer&apos;, &apos;silhouette_samples&apos;, </span><br><span class="line"> &apos;silhouette_score&apos;, </span><br><span class="line"> &apos;v_measure_score&apos;, </span><br><span class="line"> &apos;zero_one_loss&apos;]</span><br></pre></td></tr></table></figure>
        </div>
        <div class="post-tool">
            <a class="btn-thumbs-up" href="javascript:void(0);" data-cid="52" title="95">
                <i class="fa fa-thumbs-up" aria-hidden="true"></i> Donate
            </a>
        </div>
        
        <div class="post-tags">Tags：
            
        </div>
        
    </article>
    
    <p style="text-align: center">This article just represents my own viewpoint. If there is something wrong, please correct me.</p>
    
    

    

</div>
<script src="/js/busuanzi.pure.mini.js"></script>


        </div><!-- end #main-->
    </div><!-- end #body -->
    <footer class="footer">
    <div class="footer-inner" style="text-align: center">
        <p>
            <a href="/about" title="About">About</a>&nbsp;&nbsp<em>·</em>&nbsp;&nbsp
            <!-- 自定义链接 -->
            <a href="/help" title="Help">Help</a>&nbsp;&nbsp<em>·</em>&nbsp;&nbsp
            <a href="/links" title="Links">Links</a>&nbsp;&nbsp<em>·</em>&nbsp;&nbsp
            <a href="/sitemap.xml" title="SiteMap">SiteMap</a>
        </p>
        <p>
            Has been established&nbsp<a href="/timeline" id="siteBuildingTime"></a>&nbspDays，<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="licence">Based on Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)</a><br>
            ©2017-<span id="cpYear"></span> Based on&nbsp<a href="http://hexo.io" target="_blank" rel="nofollow">Hexo</a>
            ，Theme by&nbsp&nbsp<a href="https://github.com/tangkunyin/hexo-theme-jsimple" target="_blank" rel="bookmark">JSimple</a>
            ，Author&nbsp<a href="https://shuoit.net" target="_blank" rel="friend">纠结伦</a>
            ，Hosted by <a href="https://pages.github.com/" target="_blank" rel="nofollow">GitHub Pages</a>
        </p>
    </div>
</footer>
<script src="/js/SimpleCore.js"></script>

</div>
<!-- search pop -->
<div class="popup search-popup local-search-popup">
    <div class="local-search-header clearfix">
        <span class="search-icon">
            <i class="fa fa-search"></i>
        </span>
        <span class="popup-btn-close">
            <i class="fa fa-times-circle"></i>
        </span>
        <div class="local-search-input-wrapper">
            <input id="local-search-input" spellcheck="false" type="text" autocomplete="off" placeholder="Input query keywords here...">
        </div>
    </div>
    <div id="local-search-result"></div>
</div>
<div class="fixed-btn">
    <a class="btn-gotop" href="javascript:"> <i class="fa fa-angle-up"></i></a>
</div>
<script>
    $(function () {
        var jsi_config = {
            buildingTime: '01/20/2018',
            current: $('.post-tags').length > 0 ? 'post' : 'archive',
            snsQRCode: '/images/sns-qrcode.png',
            donateImg: '/images/donate-qr.png',
            localSearch: { dbPath: '' },
            readMode: 'day'
        };
        
            jsi_config.localSearch = {
                dbPath: '/search.xml',
                trigger: 'auto',
                topN: '1',
                unescape: 'false'
            }
        
        SimpleCore.init(jsi_config);
        
    });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
