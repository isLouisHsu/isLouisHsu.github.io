<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.4.2" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.4.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.4.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.4.2">


  <link rel="mask-icon" href="/images/logo.svg?v=6.4.2" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '6.4.2',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="论文可从Recent Advances in Deep Learning for Object Detection - arXiv.org下载。 Content Abstract Object detection is a fundamental visual recognition problem in computer vision and has been widely studied in">
<meta name="keywords" content="Object Detection">
<meta property="og:type" content="article">
<meta property="og:title" content="[解读] Recent Advances in Deep Learning for Object Detection">
<meta property="og:url" content="http://yoursite.com/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/index.html">
<meta property="og:site_name" content="LOUIS&#39; BLOG">
<meta property="og:description" content="论文可从Recent Advances in Deep Learning for Object Detection - arXiv.org下载。 Content Abstract Object detection is a fundamental visual recognition problem in computer vision and has been widely studied in">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://yoursite.com/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/contents.jpg">
<meta property="og:image" content="http://yoursite.com/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig1.jpg">
<meta property="og:image" content="http://yoursite.com/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig2.jpg">
<meta property="og:image" content="http://yoursite.com/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig3-R-CNN.png">
<meta property="og:image" content="http://yoursite.com/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig3-SPP-net.png">
<meta property="og:image" content="http://yoursite.com/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig3-Fast-R-CNN.png">
<meta property="og:image" content="http://yoursite.com/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig3-Faster-R-CNN.png">
<meta property="og:image" content="http://yoursite.com/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig3-R-FCN.png">
<meta property="og:image" content="http://yoursite.com/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig3-R-FCN-psROI.png">
<meta property="og:image" content="http://yoursite.com/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig4-YOLO.png">
<meta property="og:image" content="http://yoursite.com/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig4-SSD.png">
<meta property="og:image" content="http://yoursite.com/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig4-RetinaNet.png">
<meta property="og:image" content="http://yoursite.com/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig4-CornerNet.png">
<meta property="og:image" content="http://yoursite.com/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig7.png">
<meta property="og:image" content="http://yoursite.com/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig8.png">
<meta property="og:image" content="http://yoursite.com/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig10-nms.png">
<meta property="og:image" content="http://yoursite.com/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Tab1.png">
<meta property="og:image" content="http://yoursite.com/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Tab2.png">
<meta property="og:image" content="http://yoursite.com/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Tab3.png">
<meta property="og:updated_time" content="2019-08-31T11:16:39.906Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="[解读] Recent Advances in Deep Learning for Object Detection">
<meta name="twitter:description" content="论文可从Recent Advances in Deep Learning for Object Detection - arXiv.org下载。 Content Abstract Object detection is a fundamental visual recognition problem in computer vision and has been widely studied in">
<meta name="twitter:image" content="http://yoursite.com/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/contents.jpg">






  <link rel="canonical" href="http://yoursite.com/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/">



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>[解读] Recent Advances in Deep Learning for Object Detection | LOUIS' BLOG</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">LOUIS' BLOG</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">To be better</p>
      
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
    <a href="/categories/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-guestbook">
    <a href="/guestbook" rel="section">
      <i class="menu-item-icon fa fa-fw fa-guest"></i> <br>留言</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>
  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    
  
  
  
    
      
    
    <a href="https://github.com/isLouisHsu" class="github-corner" target="_blank" title="Follow me on GitHub" aria-label="Follow me on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#222; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg>
    
      </a>
    



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  
    <div class="reading-progress-bar"></div>
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Louis Hsu">
      <meta itemprop="description" content="技术博客？">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LOUIS' BLOG">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">[解读] Recent Advances in Deep Learning for Object Detection
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-08-31 19:15:12 / 修改时间：19:16:39" itemprop="dateCreated datePublished" datetime="2019-08-31T19:15:12+08:00">2019-08-31</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="post-meta-item-icon">
            <i class="fa fa-eye"></i>
             阅读次数： 
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>论文可从<a href="https://arxiv.org/abs/1908.03673?context=cs.CV" target="_blank" rel="noopener">Recent Advances in Deep Learning for Object Detection - arXiv.org</a>下载。</p>
<h1 id="Content"><a href="#Content" class="headerlink" title="Content"></a>Content</h1><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/contents.jpg" alt="contents"></p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><blockquote>
<p>Object detection is a fundamental visual recognition problem in computer vision and has been widely studied in the past decades. <strong>Visual object detection aims to find objects of certain target classes with precise localization in a given image and assign each object instance a corresponding class label.</strong> Due to the tremendous successes of deep learning based image classification, object detection techniques using deep learning have been actively studied in recent years. In this paper, we give a comprehensive survey of recent advances in visual object detection with deep learning. By reviewing a large body of recent related work in literature, we systematically analyze the existing object detection frameworks and organize the survey into three major parts: <strong>(i) detection components, (ii) learning strategies, and (iii) applications &amp; benchmarks.</strong> In the survey, we cover a variety of factors affecting the detection performance in detail, <strong>such as detector architectures, feature learning, proposal generation, sampling strategies, etc.</strong> Finally, we discuss <strong>several future directions to facilitate and spur future research</strong> for visual object detection with deep learning.<br><strong>Keywords:</strong> Object Detection, Deep Learning, Deep Convolutional Neural Networks</p>
</blockquote>
<p>目标检测：在给定图像中找到目标类对象的精确位置，并分配相应的类别标签。</p>
<p>本文介绍的主要分为以下几个部分</p>
<ul>
<li>组件组成部分；</li>
<li>学习策略；</li>
<li>应用及数据集、测试。</li>
</ul>
<p>影响检测的几个细节：</p>
<ul>
<li>检测器结构；</li>
<li>特征学习；</li>
<li>预选框生成；</li>
<li>采样策略等。</li>
</ul>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><h2 id="1-1-基本的视觉识别问题"><a href="#1-1-基本的视觉识别问题" class="headerlink" title="1.1 基本的视觉识别问题"></a>1.1 基本的视觉识别问题</h2><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig1.jpg" alt="Fig1"></p>
<ul>
<li>图像分类(image classification)：识别给定图像中对象的语义类别；</li>
<li>物体检测(object detection)：    识别给定图像中对象的语义类别，并用边界框表出物体位置；</li>
<li>语义分割(semantic segementation)：为每个像素分配特定的类别标签；</li>
<li>实例分割(instance segementation)：目标检测和语义分割交集，在语义分割基础上，在同类别物体中分辨出不同实例。</li>
</ul>
<h2 id="1-2-如何设计好的检测算法"><a href="#1-2-如何设计好的检测算法" class="headerlink" title="1.2 如何设计好的检测算法"></a>1.2 如何设计好的检测算法</h2><p>好的检测算法需要对语义及空间信息有很好的理解。</p>
<blockquote>
<p>A good detection algorithm should have a strong understanding of semantic cues as well as the spatial information about the image.</p>
</blockquote>
<h2 id="1-3-早期物体检测算法"><a href="#1-3-早期物体检测算法" class="headerlink" title="1.3 早期物体检测算法"></a>1.3 早期物体检测算法</h2><p>可分成三个步骤</p>
<ul>
<li>候选框生成(proposal generation)<br>一种直观方法：滑动窗口(sliding window)，将检测图像以不同比例缩放，并使用不同尺寸的滑动窗口选择感兴趣区域(regions of interest)。</li>
<li>特征提取(feature vector extraction)<br>从ROI区域提取固定长度的向量作为特征，如尺度不变特征变换(SIFT)，Haar，梯度直方图(HOG)，或加速鲁棒特征(SURF)。</li>
<li>区域分类(region classification)<br>例如支持向量机(SVM)，配合bagging，adaboost，级联学习(cascade learning)等方法。</li>
</ul>
<p>传统方法存在的局限性</p>
<ul>
<li>在候选框生成时，大量冗余的候选框造成误正(false positive)率高。窗口尺度是人为或启发式指定的，不能很好地匹配物体；</li>
<li>特征提取时，基于低级视觉特征，难以捕获上下文语义信息；</li>
<li>以上三个步骤单独设计优化，无法获得全局最优解。</li>
</ul>
<h2 id="1-4-深度学习方法"><a href="#1-4-深度学习方法" class="headerlink" title="1.4 深度学习方法"></a>1.4 深度学习方法</h2><p>模拟生物学分层结构，利用反向传播算法更新参数，但存在局限性：</p>
<ul>
<li>训练样本过少，造成过拟合；</li>
<li>计算资源的限制；</li>
<li>相比较于SVM等传统方法，缺少理论支持。</li>
</ul>
<p>其优点是</p>
<ul>
<li>深度卷积神经网络生成从原始像素到高级语义信息的分层特征表示，其从训练数据中自动学习，并且在复杂上下文中显示出更具辨别力的表达能力；</li>
<li>于传统视觉描述符相比，自动学习特征表示，而不是固定的；</li>
<li>可以以端到端的方式进行优化。</li>
</ul>
<h2 id="1-5-基于深度学习的对象检测框架"><a href="#1-5-基于深度学习的对象检测框架" class="headerlink" title="1.5 基于深度学习的对象检测框架"></a>1.5 基于深度学习的对象检测框架</h2><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig2.jpg" alt="Fig2"></p>
<p>两种系列</p>
<ul>
<li>两阶段检测器(two-stage detector)：通过候选框生成器，生成数目较少的候选框，并对每个候选框内内容进行特征提取；应用特征对候选区域进行分类。如 R-CNN(CNN + SVM/FNN)；</li>
<li>一阶段检测器(one-stage detector)：无虚级联区域分类步骤，对特征图的每个位置上的对象进行分类预测。如 YOLO 及其变体。<br>通常来说，一阶段检测器速度更快，可用于试试物体检测；二阶段检测器精度更高，在公共数据集等测试中效果更好。</li>
</ul>
<h1 id="2-Problem-Settings"><a href="#2-Problem-Settings" class="headerlink" title="2. Problem Settings"></a>2. Problem Settings</h1><p>目标检测涉及识别(如物体分类)和定位(如坐标回归)任务。给定数据集包含$N$张带标记的图片</p>
<script type="math/tex; mode=display">D = \{X^{(1)}, X^{(2)}, \cdots, X^{(N)}\}</script><p>对于第$i$张图片，若其内部包含$M_i$个物体，这些物体属于$C$类，第$j$个物体的标签包含所属类别$c^{(i)}_j$，所在位置$b^{(i)}_j$，则</p>
<script type="math/tex; mode=display">y^{(i)}_j = (c^{(i)}_j, b^{(i)}_j)</script><p>则该图片对应标签集为</p>
<script type="math/tex; mode=display">y^{(i)} = \{(c^{(i)}_1, b^{(i)}_1), (c^{(i)}_2, b^{(i)}_2), \cdots, (c^{(i)}_{M_i}, b^{(i)}_{M_i})\}</script><p>对于预测输出，为</p>
<script type="math/tex; mode=display">\hat{y}^{(i)} = \{(\hat{c}^{(i)}_1, \hat{b}^{(i)}_1), (\hat{c}^{(i)}_2, \hat{b}^{(i)}_2), \cdots, (\hat{c}^{(i)}_{M_i}, \hat{b}^{(i)}_{M_i})\}</script><p>总体损失定义为</p>
<script type="math/tex; mode=display">L(X, \theta) = \frac{1}{N} \sum_{i=1}^N L(X^{(i)}, y^{(i)}, \hat{y}^{(i)}; \theta) + \frac{\lambda}{2} ||\theta||_2^2</script><p>定义指标交并比IoU(intersection-over-union)</p>
<script type="math/tex; mode=display">\text{IoU} (b^{(i)}_j, \hat{b}^{(i)}_j) = \frac{\text{Area} (b^{(i)}_j \bigcap \hat{b}^{(i)}_j)}{\text{Area} (b^{(i)}_j \bigcup \hat{b}^{(i)}_j)}</script><p>则预测正确与否的判决为</p>
<script type="math/tex; mode=display">
\text{Prediction} = 
    \begin{cases} 
        \text{Positive} & c^{(i)}_j = \hat{c}^{(i)}_j \quad \text{and} \quad \text{IoU} (b^{(i)}_j, \hat{b}^{(i)}_j) > \Omega \\ 
        \text{Negative} & \text{otherwise} 
    \end{cases}</script><p>另外，在$C$类物体的检测问题中，需要计算mAP(mean average precision)进行评估。通常来说，达到20FPS的检测器可用于实时检测场景。</p>
<h1 id="3-Detection-Components"><a href="#3-Detection-Components" class="headerlink" title="3. Detection Components"></a>3. Detection Components</h1><h2 id="3-1-Detection-Settings-bbox-level-and-mask-level-algorithms"><a href="#3-1-Detection-Settings-bbox-level-and-mask-level-algorithms" class="headerlink" title="3.1. Detection Settings: bbox-level and mask-level algorithms"></a>3.1. Detection Settings: bbox-level and mask-level algorithms</h2><ul>
<li>bbox-level<br>只需要边界框注释，在评估时，计算预测与实际边界框的交并比IoU进行性能衡量。</li>
<li>mask-level<br>即实例分割，需要通过像素级掩码而不是粗略的边界框来分割对象，对空间信息的处理要求更改。</li>
</ul>
<h2 id="3-2-Detection-Paradigms-two-stage-detectors-and-one-stage-detectors"><a href="#3-2-Detection-Paradigms-two-stage-detectors-and-one-stage-detectors" class="headerlink" title="3.2. Detection Paradigms: two-stage detectors and one-stage detectors"></a>3.2. Detection Paradigms: two-stage detectors and one-stage detectors</h2><ul>
<li>两阶段检测器(two-stage detector)：通过候选框生成器，生成数目较少的候选框，并对每个候选框内内容进行特征提取；应用特征对候选区域进行分类。如 R-CNN(CNN + SVM/FNN)；</li>
<li>一阶段检测器(one-stage detector)：无虚级联区域分类步骤，对特征图的每个位置上的对象进行分类预测。如 YOLO 及其变体。<br>通常来说，一阶段检测器速度更快，可用于试试物体检测；二阶段检测器精度更高，在公共数据集等测试中效果更好。</li>
</ul>
<h3 id="3-2-1-Two-stage-Detectors"><a href="#3-2-1-Two-stage-Detectors" class="headerlink" title="3.2.1. Two-stage Detectors"></a>3.2.1. Two-stage Detectors</h3><p>将检测任务分成两部分：</p>
<ul>
<li>候选框生成<br>基本上思想是选出高召回率(Recall)的图像区域。</li>
<li>对候选框内图像进行判别<br>基于深度学习的模型，可将框内图像进行分类，此外，该模型输出也用以矫正第一阶段输出的候选框，使其更契合物体位置。</li>
</ul>
<h4 id="3-2-1-1-R-CNN"><a href="#3-2-1-1-R-CNN" class="headerlink" title="3.2.1.1. R-CNN"></a>3.2.1.1. R-CNN</h4><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig3-R-CNN.png" alt="Fig3-R-CNN"></p>
<ol>
<li>组成</li>
</ol>
<ul>
<li>候选框生成：通过选择性搜索(Selective Search)生成，可拒绝一些明显是背景的候选框，减少误正率(false positive)；</li>
<li>特征提取：通过裁剪、缩放获得固定尺寸图像，输入到深度卷积网络，得到4096维的向量；</li>
<li>区域分类：分类器选用一对多(one-vs-all)SVM，此外边界框回归器将候选框矫正。</li>
</ul>
<ol>
<li>细节</li>
</ol>
<ul>
<li>与传统方法相比，深度卷积网络生成分层特征，捕获不同尺度信息；</li>
<li>利用迁移学习方法，使用ImageNet与训练的卷积网络权重，全连接层重新初始化权值；</li>
</ul>
<ol>
<li>缺点</li>
</ol>
<ul>
<li>不共享权值，造成大量冗余计算；</li>
<li>各部分独立，不能进行端到端的方式优化，难以获得全局最优；</li>
<li>选择性搜索难以适应复杂背景的图片，并且无法使用GPU加速。</li>
</ul>
<h4 id="3-2-1-2-SPP-Net"><a href="#3-2-1-2-SPP-Net" class="headerlink" title="3.2.1.2. SPP-Net"></a>3.2.1.2. SPP-Net</h4><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig3-SPP-net.png" alt="Fig3-SPP-net"></p>
<ol>
<li><p>细节<br>在处理候选框内的图片时，并不将其进行裁剪缩放，而是定义空间金字塔池化层(Spatial Pyramid Pooling Layer)。<br>例如对于尺寸为$H \times W$的可见光图像，将其划分为$N \times N$个网格，在每个尺寸为$\frac{H}{N} \times \frac{W}{N}$网格内进行池化操作。选取不同的$N$重复采样，将不同划分数目$N$下得到的输出，合并为长度为$N \times N$向量。<br>SPP-layer可以接受不同尺度和纵横比的图像，故避免了信息丢失和几何失真等情况。</p>
</li>
<li><p>缺点</p>
</li>
</ol>
<ul>
<li>仍旧不能进行端到端的方式优化；</li>
<li>由于该层不能进行反向传播，此层之前的网络参数需要固定。</li>
</ul>
<h4 id="3-2-1-3-Fast-R-CNN"><a href="#3-2-1-3-Fast-R-CNN" class="headerlink" title="3.2.1.3. Fast R-CNN"></a>3.2.1.3. Fast R-CNN</h4><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig3-Fast-R-CNN.png" alt="Fig3-Fast-R-CNN"></p>
<ol>
<li><p>细节<br>计算了整个图像的特征图(同SPP-Net)，在特征图上提取固定长度的特征，这一步使用ROI Pooling Layer实现，是Spatial Pyramid Pooling Layer的特殊情况，仅在一种网格划分数目下采样，实际操作步骤如下：在<strong>整个图像</strong>计算得到的特征图中，根据实际边界框位置选取尺寸为$h \times w$的感兴趣区域(ROI)，指定网格的大小如$H \times W$，将ROI划分为$\frac{h}{H} \times \frac{w}{W}$个网格，在每个网格内进行池化操作，得到$\frac{h}{H} \times \frac{w}{W}$的向量。<br>分类与边界框回归为两个单独的全连接层网络，而不是采用SVM，分类器输出维数为$C+1$($C$种类别加背景)，回归器输出维数为$4 \times C$(各类别分别对应矫正参数)。<br>值得注意的是，该结构特征提取、区域分类与边界框回归这几个步骤可通过端到端的方式优化，</p>
</li>
<li><p>缺点<br>候选框仍使用传统方法生成，如选择性搜索(Selective Search)或边缘框(Edge Boxes)。</p>
</li>
</ol>
<h4 id="3-2-1-4-Faster-R-CNN"><a href="#3-2-1-4-Faster-R-CNN" class="headerlink" title="3.2.1.4. Faster R-CNN"></a>3.2.1.4. Faster R-CNN</h4><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig3-Faster-R-CNN.png" alt="Fig3-Faster-R-CNN"></p>
<ol>
<li><p>细节<br>提出新的候选框生成方法：Region Proposal Network(PRN)，为全卷积神经网络，接受任意大小的图像。在检测时，用大小为$n \times n$的滑动窗口在特征图上滑动，每个位置提取特征，送入分类层(cls)与回归层(reg)，最终结果用于确定候选框。<br>可将RPN插入到Fast R-CNN中从而以端到端的方式进行优化。</p>
</li>
<li><p>缺点<br>尽管Faster R-CNN在提取特征图时共享权值，但对于后续每个ROI内的特征图对应的向量，仍需要单独通过全连接计算。</p>
</li>
</ol>
<h4 id="3-2-1-5-R-FCN"><a href="#3-2-1-5-R-FCN" class="headerlink" title="3.2.1.5. R-FCN"></a>3.2.1.5. R-FCN</h4><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig3-R-FCN.png" alt="Fig3-R-FCN"></p>
<ol>
<li>细节<br>提出Positive-Sensitive ROP Pooling(PSROI Polling)，生成名为Position-Sensitive Score Map的特征图，保持了空间信息(to extract spatial-aware region features by encoding each relative position of the target regions)。如对于Stage 1生成的特征图，设其尺寸为$h \times w \times c$，用$k^2(C+1)$个$1 \times 1$的卷积核，即$k^2(C+1) \times 1 \times 1 \times c$进行运算，得到$h \times w \times k^2(C+1)$的Position-Sensitive Score Map。</li>
</ol>
<p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig3-R-FCN-psROI.png" alt="Fig3-R-FCN-psROI"></p>
<p>当取$k=3$时，表示将每个ROI划分为$3 \times 3$，在特征图平面位置为$(x, y)$处，可切片得到向量$f_{3^2 \times (C+1)}$，对于所属类别$C_j$的部分，又有$f^{(j)}_{3^2}$，表示该ROI内，左上、上、右上、左、中、右、左下、下、右下$9$个位置处，所属类别$C_j$的概率。Pooling 操作同Fast R-CNN与Faster R-CNN。</p>
<h4 id="3-2-1-6-FPN-MNC-Mask-R-CNN-Mask-Scoring-R-CNN-…"><a href="#3-2-1-6-FPN-MNC-Mask-R-CNN-Mask-Scoring-R-CNN-…" class="headerlink" title="3.2.1.6. FPN, MNC, Mask R-CNN, Mask Scoring R-CNN, …"></a>3.2.1.6. FPN, MNC, Mask R-CNN, Mask Scoring R-CNN, …</h4><p>略。</p>
<h3 id="3-2-2-One-stage-Detectors"><a href="#3-2-2-One-stage-Detectors" class="headerlink" title="3.2.2. One-stage Detectors"></a>3.2.2. One-stage Detectors</h3><p>该类检测方法没有设置单独的生成候选框的阶段，通常将图像上所有位置视为潜在对象，并尝试将每个感兴趣区域分类。</p>
<h4 id="3-2-2-1-YOLOv1"><a href="#3-2-2-1-YOLOv1" class="headerlink" title="3.2.2.1. YOLOv1"></a>3.2.2.1. YOLOv1</h4><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig4-YOLO.png" alt="Fig4-YOLO"></p>
<p>YOLOv1接受$448 \times 448$大小的$3$通道图像输入，得到尺寸为$7 \times 7 \times ((4 + 1) \times n_{boxes} + n_{classes})$，$n_{boxes}$表示该网格预测输出几个回归框(YOLOv1中设置为2)，每个回归框对应$5$维特征，即$(t_x, t_y, t_w, t_h, t_c)$。可达到45FPS，或者精简网络下155FPS。但存在一些缺点</p>
<ul>
<li>每个网格处，仅能预测两个物体，对于体型较小的或是聚集的物体，难以识别；</li>
<li>仅通过最后一个特征图来进行预测，没有考虑多尺度和不同横比。</li>
</ul>
<h4 id="3-2-2-2-SSD"><a href="#3-2-2-2-SSD" class="headerlink" title="3.2.2.2. SSD"></a>3.2.2.2. SSD</h4><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig4-SSD.png" alt="Fig4-SSD"></p>
<p>SSD将各层特征图均用于预测，分成固定数目的网格，每个网格设置一定尺度和纵横比的anchor。例如对于某层大小为$H \times W \times C$的特征图，经过卷积核$[n_{anchors} \times (n_{classes} + 4)] \times 3 \times 3 \times C$，得到$H \times W \times [n_{anchors} \times (n_{classes} + 4)]$的特征图，此特征图用于预测。</p>
<h4 id="3-2-2-3-RetinaNet"><a href="#3-2-2-3-RetinaNet" class="headerlink" title="3.2.2.3. RetinaNet"></a>3.2.2.3. RetinaNet</h4><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig4-RetinaNet.png" alt="Fig4-RetinaNet"></p>
<p>使用Focal Loss解决了不同类别样本数目不均衡问题，此外使用Feature Pyramid Network来检测多尺度物体。</p>
<h4 id="3-2-2-4-CornerNet"><a href="#3-2-2-4-CornerNet" class="headerlink" title="3.2.2.4 CornerNet"></a>3.2.2.4 CornerNet</h4><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig4-CornerNet.png" alt="Fig4-CornerNet"></p>
<p>该网络为anchor-free类，改变以往寻找anchor框内物体的思路，而预测回归框的关键点。它将物体检测为一对角，在特征图的每个位置上，预测了类热图(Classification Heatmaps)。</p>
<h2 id="3-3-Backbone-Architecture"><a href="#3-3-Backbone-Architecture" class="headerlink" title="3.3. Backbone Architecture"></a>3.3. Backbone Architecture</h2><p>采用大规模图像分类的预训练模型已经成为大多物体检测网络的默认策略。但直接使用分类模型是次优的</p>
<ul>
<li>分类需要更大的感受野，并希望保持空间不变性，故应用多个下采样操作以降低特征的映射分辨率，生成的特征图是低分辨率且空间不变的。但是定位任务需要较高的空间信息；</li>
<li>分类使用单个特征图即可，而检测任务考虑到多尺度问题，需要在多个尺寸的特征图上进行预测。</li>
</ul>
<h3 id="3-3-1-Basic-Architecture-of-a-CNN"><a href="#3-3-1-Basic-Architecture-of-a-CNN" class="headerlink" title="3.3.1. Basic Architecture of a CNN"></a>3.3.1. Basic Architecture of a CNN</h3><p>深度卷积神经网络通常由一系列的卷积层、池化层、非线性激活层和全连接层组成。</p>
<ul>
<li>卷积层：卷积层输入输出图像可视作通道数为$C_1, C_2$的图像，卷积核参数为$C_2 \times k \times k \times C_1$；每个特征图上像素点对应原图中的大小称为感受野。</li>
<li>池化岑：用于扩大感受野并降低计算成本，在一定程度上增加了对图像旋转等抗干扰性。</li>
<li>非线性激活层：用于添加非线性信息，若无非线性层，网络再深也是线性变化。</li>
<li>全连接层：一般设置一系列卷积操作后，最后输出时，添加全连接层“兜底”。</li>
</ul>
<h3 id="3-3-2-CNN-Backbone-for-Object-Detection"><a href="#3-3-2-CNN-Backbone-for-Object-Detection" class="headerlink" title="3.3.2. CNN Backbone for Object Detection"></a>3.3.2. CNN Backbone for Object Detection</h3><ol>
<li><p>VGG16</p>
<p> 包括$2 + 2 + 3 + 3 + 3$层卷积层和$3$层全连接层，每组卷积层间为最大池化层。网络层数的增加可增大网络的容量，但是超过20层时，难以使用SGD进行梯度下降反传。</p>
</li>
<li><p>ResNet</p>
<p> 引入shortcut connection，一定程度上解决了梯度消失问题，可使网络堆叠得更深，即</p>
<script type="math/tex; mode=display">x_{l+1} = x_l + f_{l+1}(x_l, \theta)</script></li>
<li><p>ResNet-v2</p>
<p> ResNet-v2增加Batch Normalization层。尽管shortcut解决了训练问题，但它没有充分利用前层特征图。底层特征在逐元素操作中逐渐丢失，因此提出DenseNet，通过通道组合得方法合并特征，并且层间密集连接。</p>
<script type="math/tex; mode=display">x_{l+1} = x_l \circ f_{l+1}(x_l, \theta)</script></li>
<li><p>Dual Path Network</p>
<p> Dual Path Network结合两者的特点，某一层的特征图通道可分为密集连接部分和逐元素相加元素，即$C=C^d + C^r$</p>
<script type="math/tex; mode=display">x_{l+1} = (x^r_l + f^r_{l+1}(x^r_l, \theta^r)) \circ (x^d_l \circ f^d_{l+1}(x^d_l, \theta^d))</script></li>
<li><p>ResNeXt</p>
<p> 采用分组分离卷积的方法，大大减少了参数量与计算量。</p>
</li>
<li><p>GoogLeNet</p>
<p> 除了增加网络深度，还通过增加支路来扩大网络容量。</p>
</li>
</ol>
<h2 id="3-4-Proposal-Generation"><a href="#3-4-Proposal-Generation" class="headerlink" title="3.4. Proposal Generation"></a>3.4. Proposal Generation</h2><p>One-stage Detectors与Two-stage Detectors都产生了候选框，区别是前者在特征图对应的每个位置生成指定大小的特征图，而后者仅产生前景或背景信息的候选框，得到的结果较为稀疏。</p>
<h3 id="3-4-1-Traditional-Computer-Vision-Methods"><a href="#3-4-1-Traditional-Computer-Vision-Methods" class="headerlink" title="3.4.1. Traditional Computer Vision Methods"></a>3.4.1. Traditional Computer Vision Methods</h3><p>略。</p>
<h3 id="3-4-2-Anchor-based-Methods"><a href="#3-4-2-Anchor-based-Methods" class="headerlink" title="3.4.2. Anchor-based Methods"></a>3.4.2. Anchor-based Methods</h3><p>anchor通过人为指定的方式</p>
<ul>
<li>根据不同特征图的感受野设置锚框(SSD)；</li>
<li>为检测细小物体，可增大图像尺寸和减少锚框stride的方式；</li>
<li>基于RPN，分解锚框的维度(DeRPN)；</li>
<li>用K-Means确定(YOLOv2)；</li>
<li>先指定锚框，再通过训练结果矫正锚框(RefineNet)；</li>
</ul>
<h3 id="3-4-3-Keypoints-based-Methods"><a href="#3-4-3-Keypoints-based-Methods" class="headerlink" title="3.4.3. Keypoints-based Methods"></a>3.4.3. Keypoints-based Methods</h3><p>略。暂时没有接触过，理解不深，后续可能补上(TODO:)。</p>
<h3 id="3-4-4-Other-Methods"><a href="#3-4-4-Other-Methods" class="headerlink" title="3.4.4. Other Methods"></a>3.4.4. Other Methods</h3><p>略。</p>
<h2 id="3-5-Feature-Representation-Learning"><a href="#3-5-Feature-Representation-Learning" class="headerlink" title="3.5. Feature Representation Learning"></a>3.5. Feature Representation Learning</h2><h3 id="3-5-1-Multi-scale-Feature-Learning"><a href="#3-5-1-Multi-scale-Feature-Learning" class="headerlink" title="3.5.1. Multi-scale Feature Learning"></a>3.5.1. Multi-scale Feature Learning</h3><p>如Fast R-CNN与Faster R-CNN，仅通过单个特征图预测输出，这对应多尺度和多个纵横比的情况下具有极大难度。</p>
<p>底层特征图比高层特征图具有更高的分辨率与更小的感受野，因此更适合于检测小物体。而深层的特征图具有更多的语义信息，对光照、偏移等鲁棒性更好，并具有更大的感受野，更适用于检测大体系物体。</p>
<p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig7.png" alt="Fig7"></p>
<p>有四种解决多尺度特征学习问题的方法</p>
<ol>
<li><p>Image Pyramid<br>通过将图像缩放为不同大小的方式，单独训练适应尺寸的网络，测试时将图片缩放为相应尺寸输入到不同的网络中，计算量较大。但是Singh等人提出，学习单个网络以适应不同尺寸比学习多个适应不同尺寸的网络更困难(SNIP)。</p>
</li>
<li><p>Integrated Features<br>通过组合多个层的特征图，以新构造的特征图来预测输出，常用的方法如跳跃连接(skip connection)。又如ION，通过ROI Pooling剪裁来自不同层的区域特征，将其组合后作为特征。又如HyperNet，用反卷积的方式，通过集成浅层及中间层的特征图，生成新的高分辨率特征用以输出候选框。又如Multi-scale Location-aware Kernel Representation (MLKP)，捕获高阶统计量，生成更多的判别特征表示，更具描述性，为分类和定位提供语义及空间信息。</p>
</li>
<li><p>Prediction Pyramid<br>如SSD，每一层特征图均用于预测输出，每个层设置一定比例的对象。Multi-Scale Deep Convolutional Neural Network(MSCNN)通过反卷积将特征图分辨率增大，用这些特征图预测输出。Reception Field Block Net(RFBNet)设置多分支，每个分支设置不同尺寸的卷积核，从而获得多尺度、不同感受野的特征图，从而用于预测输出。</p>
</li>
<li><p>Feature Pyramid<br>结合Integrated Features与Prediction Pyramid的优点。如Feature Pyramid Network(FPN)自上而下将不同尺度的特征通过逐元素相加或者组合的方式，利用深层特征丰富浅层特征的语义信息。</p>
</li>
</ol>
<p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig8.png" alt="Fig8"></p>
<h3 id="3-5-2-Region-Feature-Encoding"><a href="#3-5-2-Region-Feature-Encoding" class="headerlink" title="3.5.2. Region Feature Encoding"></a>3.5.2. Region Feature Encoding</h3><ul>
<li>R-CNN</li>
<li>ROI Pooling Layer</li>
<li>ROI Warping Layer</li>
<li>ROI Align Layer</li>
<li>Precise ROI Pooling Layer</li>
<li>Position Sensitive ROI Pooling Layer</li>
<li>Feature Selective Network</li>
<li>CoupleNet</li>
<li>Deformable ROI Pooling Layer</li>
</ul>
<h3 id="3-5-3-Contextual-Reasoning"><a href="#3-5-3-Contextual-Reasoning" class="headerlink" title="3.5.3. Contextual Reasoning"></a>3.5.3. Contextual Reasoning</h3><p>由于物体的出现与环境相关，且需要与其他物体进行交互，故上下文信息十分重要。深度卷积网络隐式地从分层特征表示中捕获上下文信息。</p>
<ul>
<li><p>全局上下文<br>从整个图像地上下文中学习，利用图像其余部分信息，来对感兴趣区域进行分类。</p>
<ul>
<li>循环神经网络编码整个图像的四个方向信息(ION)；</li>
<li>学习类别分数，用于作为与检测结果连接的上下文特征(Ouyang et al.);</li>
<li>从整个图像提取嵌入信息，将其与局部特征组合用以改善检测结果(He et al.)；</li>
<li>基于语义分割的方法；</li>
<li>将目标检测和语义分割作为多任务进行优化(He et al. and Dai et al.)；</li>
<li><u>伪分段</u>语义标注(Zhao et al.)；</li>
<li>通过学习的方式获得语义特征图(Detection with Enriched Semantics)；</li>
</ul>
</li>
<li><p>局部上下文<br>对周围区域上下文进行编码，并学习对象与周围区域的交互，直接学习不同位置和带有上下文信息的类别是很困难的。</p>
</li>
</ul>
<blockquote>
<p>Directly modeling different locations and categories objects relations with the contextual is very challenging. Chen et al. proposed <strong>Spatial Memory Network (SMN)</strong> [130] which introduced a spatial memory based module. The spatial memory module captured instance-level contexts by assembling object instances back into a pseudo ”image” representations which were later used for object relations reasoning. Liu et al. proposed <strong>Structure Inference Net (SIN)</strong> [137] which formulated object detection as a graph inference problem by considering scene contextual information and object relationships. In SIN, each object was treated as a graph node and the relationship between different objects were regarded as graph edges. Hu et al. [138] proposed a <strong>lightweight framework relation network</strong> which formulated the interaction between different objects between their appearance and image locations. The new proposed framework did not need additional annotation and showed improvements in object detection performance. Based on Hu et al., Gu et al. [139] proposed a <strong>fully learnable object detector</strong> which proposed a general viewpoint that unified existing region feature extraction methods. Their proposed method removed heuristic choices in ROI pooling methods and automatically select the most significant parts, including contexts beyond proposals. <strong>Another method</strong> to encode contextual information is to implicitly encode region features by adding image features surrounding region proposals and a large number of approaches have been proposed based on this idea [131, 106, 140, 141, 142, 143]. In addition to encode features from region proposals, <strong>Gidaris et al.</strong> [131] extracted features from a number of different sub-regions of the original object proposals(border regions, central regions, contextual regions etc.) and concatenated these features with the original region features. Similar to their method, [106] extracted local contexts by enlarging the proposal window size and concatenating these features with the original ones. Zeng et al. [142] proposed <strong>Gated Bi-Directional CNN (GBDNet)</strong> which extracted features from multi-scale subregions.Notably, GBDNet learned a gated function to control the transmission of different region information because not all contextual information is helpful for detection.</p>
</blockquote>
<h3 id="3-5-4-Deformable-Feature-Learning"><a href="#3-5-4-Deformable-Feature-Learning" class="headerlink" title="3.5.4. Deformable Feature Learning"></a>3.5.4. Deformable Feature Learning</h3><p>检测器应对图像中物体的非刚性变形具有鲁棒性。传统方式有Deformable Part based Models(DPMs)，使用可变形编码方法由多个组成表示对象。深度学习方法如DeepIDNet，开发Deformable-aware Pooling Layer;Deformable Convolutional Layers自动学习辅助位置，以增强常规采样的信息。</p>
<h1 id="4-Learning-Strategy"><a href="#4-Learning-Strategy" class="headerlink" title="4. Learning Strategy"></a>4. Learning Strategy</h1><h2 id="4-1-Training-Stage"><a href="#4-1-Training-Stage" class="headerlink" title="4.1. Training Stage"></a>4.1. Training Stage</h2><h3 id="4-1-1-Data-Augmentation"><a href="#4-1-1-Data-Augmentation" class="headerlink" title="4.1.1. Data Augmentation"></a>4.1.1. Data Augmentation</h3><p>为解决数据量少的问题，需要进行数据扩增。在物体检测中，常用方法有：水平翻转，旋转、随机裁剪、延申、颜色抖动(亮度，对比度，饱和度和色度)。注意图像变换后，需要对标签也作对应变换。</p>
<h3 id="4-1-2-Imbalance-Sampling"><a href="#4-1-2-Imbalance-Sampling" class="headerlink" title="4.1.2. Imbalance Sampling"></a>4.1.2. Imbalance Sampling</h3><p>候选框内图像大多数都是背景，而不是物体，有以下两个问题：1) 类别不平衡(class imbalance)，由于只有一小部分候选框内内容为物体，故负样本占大多数，导致梯度反传时负样本占主导。2) 困难不平衡(difficulty imbalance)，类似第一点，检测器更易分辨背景，难以分辨物体。</p>
<p>一些二阶段检测器，如R-CNN与Fast R-CNN会先拒绝大部分负样本，Fast R-CNN从$2000$个候选框中随机采样，直到在某批次的数据中，正负样本比例达到$1:3$。随机采样能解决类别不平衡问题，但是丢失的负样本可能包含丰富的语义信息。为解决这个问题，刘等人提出困难负样本策略(hard negative sampli  Strategy)，主要保留难以判别为负的负样本，具体地说来，就是选取损失值较大的负样本。</p>
<p>为了解决困难不平衡问题，大部分都是通过合理设置损失函数。对于标检测来说，多类别分类器需要分辨$C+1$个类别，即$C$类物体加背景。假定某区域真实标签为$u$，$p$是网络输出的$C+1$个类别的概率分布($p = \{p_0, \cdots, p_C\}$)，那么损失定义为</p>
<script type="math/tex; mode=display">L_{cls}(p, u) = - \log p_u</script><p>现阶段有一种改进的交叉熵损失：<a href="https://arxiv.org/abs/1904.09048" target="_blank" rel="noopener">Focal Loss</a>，即</p>
<script type="math/tex; mode=display">L_{FL} = - \alpha (1 - p_u)^{\gamma} \log p_u</script><p>其中参数$\alpha$与$\gamma$为超参数，该损失函数可根据网络输出的$p_u$计算权值，错误分类样本$p_u$更低，从而使权重更大，可以更多地关注误分类样本。梯度协调机制(gradient harmonizing mechanism - GHM)采用类似的思路，不仅抑制了易分类负样本，并避免了异常负样本的影响。此外，还有如在线困难样本挖掘策略(online hard example mining strategy)，自动选取困难样本用以训练，仅关注样本的困难度而不关注类别信息，即单批正负样本比例没有被考虑，他们认为，对于检测问题，样本困难度比类别不平衡更加重要。</p>
<h3 id="4-1-3-Localization-Refinement"><a href="#4-1-3-Localization-Refinement" class="headerlink" title="4.1.3. Localization Refinement"></a>4.1.3. Localization Refinement</h3><p>物体检测算法需要提供一个包含物体的最小矩形框，但精确定位困难，因为预测通常集中在物体更具分辨性特征的部位，而不一定是包含物体的区域。在一定场景下，需要进行高质量预测，可用IoU作为指标。一种解决方法是生成高质量的候选框，以下介绍一些其他方法。在R-CNN中使用L2惩罚作为损失，在Fast R-CNN中使用平滑L1惩罚作为损失，即</p>
<script type="math/tex; mode=display">L_{reg}(t^c, v) = \sum_{i \in \{x, y, w, h\}} \text{SmoothL1}(t^c_i - v_i)</script><script type="math/tex; mode=display">\text{SmoothL1}(x) = \begin{cases} 0.5x^2 & \text{if} |x| < 1 \\ |x| - 0.5 & \end{cases}</script><p>其中每类均具有回归的偏置，即$t^c = (t^c_x, t^c_y, t^c_w, t^c_h)$，真实边界框位置为$v=(v_x, v_y, v_w, v_h)$。</p>
<p>基于定位校准，一些方法采用辅助模型用于更好地矫正坐标。如Gidaris等人引入一种迭代地边框回归方法，使用R-CNN反复迭代候选框内容，多次矫正候选框；另外，提出LocNet，将每个边界框的分布进行建模。这些方法都需要单独模块，不能进行联合调优。</p>
<p>一些其他的，侧重于设计带有修改的目标函数(modified objective function)的统一网络框架。在多路网络(Multi-Path Network)中，采用一系列的分类器，这些分类器是通过不同的指标，以整体损失进行优化的，每个分类器都适应对应的IoU阈值，所有输出组成最终的预测结果。Fitness NMS学习了一种新式的IoU计算方法，他们认为现在的检测器旨在找到合格的预测，而不是最优的预测，因此高质量、低质量的候选有同等的重要性。Fitne-IoU更看重高度重合的候选框。他们也采用了基于一组IoU上限，来到处边界框的回归损失，以最大化具有对象的预测的IoU(They also derived a bounding box regression loss based on a set of IoU upper bounds to maximum the IoU of predictions with objects)。Grid R-CNN参照CornerNet与DeNet，用角点定位的机制取代边界框的线性回归。</p>
<h3 id="4-1-4-Cascade-Learning"><a href="#4-1-4-Cascade-Learning" class="headerlink" title="4.1.4. Cascade Learning"></a>4.1.4. Cascade Learning</h3><p>级联学习是一种粗到细的学习策略，从给定分类器的输出中收集信息，以级联的方式构建更强的分类器，首次被应用于训练人脸检测器。在深度学习算法方面，Cascade Region-proposal-network And FasT-rcnn(CRAFT)学习PRN以及带级联策略的分类器，首先学习一个标准的RPN网络，之后用二分类的Fast R-CNN拒绝一些容易判别的错误样本，剩余样本用来构建包含两个Fast R-CNN网络的级联区域分类器。Yang等人引入层级级联分类器，用于多尺度的物体检测，不同层的特征图设置分类器，浅层的分类器将拒绝易分辨的错误样本，剩余样本将送入更深的网络层。RefineDet和Cascade R-CNN将级联用于回归框的矫正，构建多阶段的边界框回归器，边界框输出将在各个阶段进行矫正，并且这些回归器是通过不同quality的指标训练得到的。Cheng等人通过观察Faster R-CNN的误检图片，注意到即使回归器定位准确，样本分类却存在错误，他们将此归结于特征共享(sharing of features)和联合的多任务优化(joint multi-task optimization)导致的次优特征表示(sub-optimal feature representation)；此外，他们认为Faster R-CNN的较大感受野引入了较多噪声。他们建立了一个基于Faster R-CNN与R-CNN的级联检测系统以互补，即用训练好的Faster R-CNN得到一些初始的预测，这些结果用以训练R-CNN。</p>
<h3 id="4-1-5-Others"><a href="#4-1-5-Others" class="headerlink" title="4.1.5. Others"></a>4.1.5. Others</h3><p>有一些其他的学习策略</p>
<ol>
<li><p>Adversarial Learning</p>
<p> 对抗性学习显示了生成模型的重要性，最主要的应用是生成对抗网络(GAN)。生成器对数据分布进行建模，根据给定的噪声输入，得到一副假的图片，送入鉴别器判断该图片是否为真。GAN在许多领域显示其有效性，在目标检测方面也不例外。Li等人提出的Perceptual GAN可用于细小物体检测，通过对抗过程，生成器将学习细小物体的高分辨率特征表征。A-Fast-R-CNN通过生成的对抗样本训练，他们认为困难样本是在分布的长尾上，因此他们映入了两个新式模块，自动生成具有遮挡和变形的特征。</p>
</li>
<li><p>Training from Scratch</p>
<p> 现在的检测器大多依赖于ImageNet与训练的分类模型，但是，由于分类和检测任务损失不同、数据分布不同，这可能会对结果造成负面影响。Finetuning可能解决这个问题，但是不能完全消除偏差。另外，将此分类模型用作新的领域可能不合适，如RGB图像到MRI数据。所以有必要从空白开始训练，这样的困难主要是物体检测训练数据过少，会造成过拟合。与图像分类不同，物体检测需要边界框级别的标注，这是非常耗时耗力的(ImageNet有1000个类别，但只有200类包含检测标注)。</p>
<p> Deeply Supervised Object Detectors(DSOD)的设计者认为，密集连接的网络结构用于监督学习可大大减少优化难度。基于DSOD，Shen等人提出了一种门控循环特征金字塔结构，动态调整中间层的监督强度以自适应不同尺度的物体，将空间和语义信息压缩到单个预测特征图，进一步减少了参数量从而加快了收敛速度。此外，门控特征金字塔结构可根据物体大小适应不同强度的监督，这种方法比原始DSOD更有效。但是随后He等人用从头开始训练的检测器在MSCOCO数据机上进行验证，发现vanilla detector可以获得等同于$10K$标注数据训练得到的检测能力，这表明从头开始训练的模型并不依赖特定结构，<strong>这与之前的工作相矛盾</strong>。</p>
</li>
<li><p>Knowledge Distillation</p>
<p> 通过师生教学方案(teacher-student training scheme)，将多个模型集成为一个模型。Li等人提出轻量级检测器，它的训练通过重量级有效的检测器指导进行训练，该检测器速度更快，精准度与后者相近。Cheng等人提出了一种基于RCNN的快速探测器，优化时，R-CNN模型作为教师网络来指导培训过程，与传统的单模型优化策略相比，他们的框架检测精度提高。</p>
</li>
</ol>
<h2 id="4-2-Testing-Stage"><a href="#4-2-Testing-Stage" class="headerlink" title="4.2. Testing Stage"></a>4.2. Testing Stage</h2><p>目标检测算法生成了众多预测结果，但是由于大量的冗余性，输出结果不能直接使用。因此有其他策略提升检测准确率，或加速推断过程。</p>
<h3 id="4-2-1-Duplicate-Removal"><a href="#4-2-1-Duplicate-Removal" class="headerlink" title="4.2.1. Duplicate Removal"></a>4.2.1. Duplicate Removal</h3><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig10-nms.png" alt="Fig10-nms"></p>
<p>Non-maximum suppression(NMS)是物体检测算法的一个组成部分，用于消除重复的假阳(false positive)预测，如上图。对于一阶段检测器来说，同一物体周围的预测边界框，可能具有相同的置信度，导致较高的假阳率；而二阶段检测算法产生较为稀疏的候选框，并且回归器会将这些候选框拉向相近的位置，同样会导致较高的假阳率。</p>
<p>具体说来，对于指定的一类物体，其预测框将按照置信度排序，选择当前置信度最高的候选框，记作$M$，然后计算该框与其余框的IoU，如果IoU值大于某阈值$\Omega_{test}$，该框将被从候选框中删除，即</p>
<script type="math/tex; mode=display">
\text{Score}_B = 
    \begin{cases}
        \text{Score}_B  & \text{IoU}(B, M) < \Omega_{test} \\
        0               & \text{otherwise}
    \end{cases}</script><p>但是对于聚集的物体，NMS算法将会把同类物体，靠近的物体框删除，这导致回归框预测确实，因此Navaneeth等人提出Soft-NMS算法，使用指定函数将靠近的框的置信度降低，而不是直接置为0</p>
<script type="math/tex; mode=display">
\text{Score}_B = 
    \begin{cases}
        \text{Score}_B              & \text{IoU}(B, M) < \Omega_{test} \\
        \text{F}(\text{IoU}(B, M))  & \text{otherwise}
    \end{cases}</script><blockquote>
<p>详细的实现代码可查看<a href="https://louishsu.xyz/2019/05/26/NMS-softer-NMS/" target="_blank" rel="noopener">该文</a></p>
</blockquote>
<p>此外，Hosong等人设计了一种网络结构，基于置信度和回归框来改进NMS，这是独立于检测器单独进行有监督训练的。他们认为，重复预测的原因是，检测器故意鼓励每个物体进行多次高分检测而不是奖励单个高分。因此他们根据两个动机来进行网络的设计：1)设置损失来惩罚二次检测，使得每个物体准确预测单个精确预测；2)处理附近的检测输出，给检测器提供是否物体已被多次检测的信息。新提出的模型没有舍弃检测结果，而是将NMS用作重估，降低重复检测结果的评分。</p>
<h3 id="4-2-2-Model-Acceleration"><a href="#4-2-2-Model-Acceleration" class="headerlink" title="4.2.2. Model Acceleration"></a>4.2.2. Model Acceleration</h3><p>目标检测应用需要较高的实时性，因此需要用指标评估检测器的速度。虽然当前state-of-art算法可以在公开数据集上得到一个较高的检测评价，但是他们的速度限制了他应用在实时场景中。通常来说，二阶段检测器比一阶段检测器速度更慢，因为候选框生成器与分类器单独运算，使得运算量较大。尽管R-FCN提出空间敏感的特征图，通过position-sensitive ROI Pooling来共享计算，但是随着物体种类增多，通道数也随之线性增加。</p>
<p>从检测器的主体结构可以看到，占计算量最多的主要是网络模型部分，因此一个简单的解决方法是用高效率的主体，比如MobileNet，通过分离卷积的方式，大大减少了参数量与计算量。PVANet是一种新的网络结构，他用到了CReLU，较少了非线性计算。</p>
<script type="math/tex; mode=display">\text{CReLU}(x) = [\text{ReLU}(x), \text{ReLU}(-x)]</script><p>另一种方法是，将模型进行线下优化，比如模型压缩。最后呢，如NVIDIA开发了一个加速工具TensorRT，在模型部署上进行优化，以加运算速度。</p>
<h3 id="4-2-3-Others"><a href="#4-2-3-Others" class="headerlink" title="4.2.3. Others"></a>4.2.3. Others</h3><p>其他方法，如将输入图像进行变换，以提高检测精度。图像金字塔是将待检测图像，以一定的比例生成一系列尺寸的图像，在每个图像上进行检测，最终结果合并于每张检测结果。Zhang等人，将图像缩放到指定尺寸，在某一特定尺寸下，仅检测某一指定大小的物体。水平翻转也可用于测试阶段。这些方法都可以增加准确率但是计算量也随之增加。</p>
<h1 id="5-Applications"><a href="#5-Applications" class="headerlink" title="5. Applications"></a>5. Applications</h1><h2 id="5-1-Face-Detection"><a href="#5-1-Face-Detection" class="headerlink" title="5.1. Face Detection"></a>5.1. Face Detection</h2><p>人脸检测是一个经典的计算机视觉问题，通常是人脸验证、对其、识别等算法的第一步。然而，人脸检测问题与通用检测之间存在关键差异：1)人脸检测中，目标的比例范围远大于通用物体；2)脸部对象包括强结构信息，并且仅有一类物体。考虑到这些特效，需要有一些先验来改进检测算法。</p>
<h2 id="5-2-Pedestrian-Detection"><a href="#5-2-Pedestrian-Detection" class="headerlink" title="5.2. Pedestrian Detection"></a>5.2. Pedestrian Detection</h2><p>行人检测是一个关键的重要任务。和通用检测不同，1)结构良好，具有几乎固定的纵横比，但也存在于较大范围内；2)行人检测是真实应用场景，有许多挑战：人群拥挤、遮挡、图像模糊等。</p>
<h2 id="5-3-Others"><a href="#5-3-Others" class="headerlink" title="5.3. Others"></a>5.3. Others</h2><p>其他应用如Logo检测和视频目标检测。</p>
<h1 id="6-Detection-Benchmarks"><a href="#6-Detection-Benchmarks" class="headerlink" title="6. Detection Benchmarks"></a>6. Detection Benchmarks</h1><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Tab1.png" alt="Tab1"></p>
<h2 id="6-1-Generic-Detection-Benchmarks"><a href="#6-1-Generic-Detection-Benchmarks" class="headerlink" title="6.1. Generic Detection Benchmarks"></a>6.1. Generic Detection Benchmarks</h2><ul>
<li>Pascal VOC2007</li>
<li>Pascal VOC2012</li>
<li>MSCOCO</li>
<li>Open Images</li>
<li>LVIS</li>
<li>ImageNet</li>
</ul>
<h2 id="6-2-Face-Detection-Benchmarks"><a href="#6-2-Face-Detection-Benchmarks" class="headerlink" title="6.2. Face Detection Benchmarks"></a>6.2. Face Detection Benchmarks</h2><ul>
<li>WIDER FACE</li>
<li>FDDB</li>
<li>PASCAL FACE</li>
</ul>
<h2 id="6-3-Pedestrian-Detection-Benchmarks"><a href="#6-3-Pedestrian-Detection-Benchmarks" class="headerlink" title="6.3. Pedestrian Detection Benchmarks"></a>6.3. Pedestrian Detection Benchmarks</h2><ul>
<li>CityPersons</li>
<li>Caltech</li>
<li>ETH</li>
<li>INRIA</li>
<li>KITTI</li>
</ul>
<h1 id="7-State-of-the-art-for-Generic-Object-Detection"><a href="#7-State-of-the-art-for-Generic-Object-Detection" class="headerlink" title="7. State-of-the-art for Generic Object Detection"></a>7. State-of-the-art for Generic Object Detection</h1><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Tab2.png" alt="Tab2"></p>
<p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Tab3.png" alt="Tab3"></p>
<h1 id="8-Concluding-Remarks-and-Future-Directions"><a href="#8-Concluding-Remarks-and-Future-Directions" class="headerlink" title="8. Concluding Remarks and Future Directions"></a>8. Concluding Remarks and Future Directions</h1><p>目标检测还存在许多挑战和未来的发展方向</p>
<ol>
<li><p>候选框生成策略</p>
<p> 如<a href="#34-proposal-generation">3.4.节</a>中所述，当前许多检测器是基于anchor的，这些anchor主要是手动设计的，难以匹配多尺度物体，基于IoU的匹配策略也是启发式的。而对于无anchor算法，具有大的改进空间，如计算成本告等。当前无锚框算法是一个热点。</p>
</li>
<li><p>有效的上下文信息编码</p>
<p> 上下文信息对物体检测十分重要，但是目前所作的工作对上下文信息的使用比较局限。</p>
</li>
<li><p>基于AutoML的检测方法</p>
<p> 给一个特定任务设计合适的网络结构是很重要的，但是也会消耗大量时间和人力。当前一个比较有趣和重要的研究方向是，通过学习的方法，自动设计网络结构。可通过AutoML方法进行网络结构的探索，但是这种算法需要大量的计算资源(more than 100 GPU cards to train a single model)。</p>
</li>
<li><p>目标检测新的数据集</p>
<p> 当前MSCOCO是目标检测最常用的数据库，但是它只包含80类物体，在真实世界中，这是远远不够的。最近有一个数据集LVIS旨在收集更多类别物体的图像数据，它包含超过1000个类别的16400张图片，其中还有许多高质量的语义分割标注。此外，它模拟真实世界的场景，其中存在大量类别但是有些类别数据很少。</p>
</li>
<li><p>Low-shot目标检测</p>
<p> 有限标记数据训练得到的模型，称作Low-shot。对于边界框级别的图像标注非常耗时耗力，现在有一些通过半监督学习的方法，来减少数据的使用。</p>
</li>
<li><p>检测任务的骨干架构</p>
<p> 当前检测器很多都基于分类模型。</p>
</li>
<li><p>其余研究问题</p>
<p> large batch learning和增量学习(incremental learning)等。</p>
</li>
</ol>

      
    </div>

    

    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div></div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="Louis Hsu 支付宝">
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Object-Detection/" rel="tag"># Object Detection</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/08/27/Batch-Normalization/" rel="next" title="Batch Normalization">
                <i class="fa fa-chevron-left"></i> Batch Normalization
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/09/01/YOLO-v1-v2-v3/" rel="prev" title="YOLO! v1, v2, v3">
                YOLO! v1, v2, v3 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div id="gitalk-container">
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="Louis Hsu">
            
              <p class="site-author-name" itemprop="name">Louis Hsu</p>
              <p class="site-description motion-element" itemprop="description">技术博客？</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">92</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">9</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">12</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/isLouisHsu" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://is.louishsu@foxmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.zhihu.com/people/islouishsu" target="_blank" title="Zhihu"><i class="fa fa-fw fa-zhihu"></i>Zhihu</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="http://weibo.com/islouishsu" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i>Weibo</a>
                  
                </span>
              
            </div>
          

          
          

          
          

          <div id="music163player">
            <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="110" src="//music.163.com/outchain/player?type=0&id=2703291040&auto=1&height=90">
            </iframe>
          </div>

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Content"><span class="nav-number">1.</span> <span class="nav-text">Content</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Abstract"><span class="nav-number">2.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-Introduction"><span class="nav-number">3.</span> <span class="nav-text">1. Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-基本的视觉识别问题"><span class="nav-number">3.1.</span> <span class="nav-text">1.1 基本的视觉识别问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-如何设计好的检测算法"><span class="nav-number">3.2.</span> <span class="nav-text">1.2 如何设计好的检测算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-早期物体检测算法"><span class="nav-number">3.3.</span> <span class="nav-text">1.3 早期物体检测算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-深度学习方法"><span class="nav-number">3.4.</span> <span class="nav-text">1.4 深度学习方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-5-基于深度学习的对象检测框架"><span class="nav-number">3.5.</span> <span class="nav-text">1.5 基于深度学习的对象检测框架</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-Problem-Settings"><span class="nav-number">4.</span> <span class="nav-text">2. Problem Settings</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-Detection-Components"><span class="nav-number">5.</span> <span class="nav-text">3. Detection Components</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-Detection-Settings-bbox-level-and-mask-level-algorithms"><span class="nav-number">5.1.</span> <span class="nav-text">3.1. Detection Settings: bbox-level and mask-level algorithms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-Detection-Paradigms-two-stage-detectors-and-one-stage-detectors"><span class="nav-number">5.2.</span> <span class="nav-text">3.2. Detection Paradigms: two-stage detectors and one-stage detectors</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-1-Two-stage-Detectors"><span class="nav-number">5.2.1.</span> <span class="nav-text">3.2.1. Two-stage Detectors</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-1-1-R-CNN"><span class="nav-number">5.2.1.1.</span> <span class="nav-text">3.2.1.1. R-CNN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-1-2-SPP-Net"><span class="nav-number">5.2.1.2.</span> <span class="nav-text">3.2.1.2. SPP-Net</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-1-3-Fast-R-CNN"><span class="nav-number">5.2.1.3.</span> <span class="nav-text">3.2.1.3. Fast R-CNN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-1-4-Faster-R-CNN"><span class="nav-number">5.2.1.4.</span> <span class="nav-text">3.2.1.4. Faster R-CNN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-1-5-R-FCN"><span class="nav-number">5.2.1.5.</span> <span class="nav-text">3.2.1.5. R-FCN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-1-6-FPN-MNC-Mask-R-CNN-Mask-Scoring-R-CNN-…"><span class="nav-number">5.2.1.6.</span> <span class="nav-text">3.2.1.6. FPN, MNC, Mask R-CNN, Mask Scoring R-CNN, …</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-One-stage-Detectors"><span class="nav-number">5.2.2.</span> <span class="nav-text">3.2.2. One-stage Detectors</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-2-1-YOLOv1"><span class="nav-number">5.2.2.1.</span> <span class="nav-text">3.2.2.1. YOLOv1</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-2-2-SSD"><span class="nav-number">5.2.2.2.</span> <span class="nav-text">3.2.2.2. SSD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-2-3-RetinaNet"><span class="nav-number">5.2.2.3.</span> <span class="nav-text">3.2.2.3. RetinaNet</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-2-4-CornerNet"><span class="nav-number">5.2.2.4.</span> <span class="nav-text">3.2.2.4 CornerNet</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-Backbone-Architecture"><span class="nav-number">5.3.</span> <span class="nav-text">3.3. Backbone Architecture</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-1-Basic-Architecture-of-a-CNN"><span class="nav-number">5.3.1.</span> <span class="nav-text">3.3.1. Basic Architecture of a CNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-2-CNN-Backbone-for-Object-Detection"><span class="nav-number">5.3.2.</span> <span class="nav-text">3.3.2. CNN Backbone for Object Detection</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-4-Proposal-Generation"><span class="nav-number">5.4.</span> <span class="nav-text">3.4. Proposal Generation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-1-Traditional-Computer-Vision-Methods"><span class="nav-number">5.4.1.</span> <span class="nav-text">3.4.1. Traditional Computer Vision Methods</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-2-Anchor-based-Methods"><span class="nav-number">5.4.2.</span> <span class="nav-text">3.4.2. Anchor-based Methods</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-3-Keypoints-based-Methods"><span class="nav-number">5.4.3.</span> <span class="nav-text">3.4.3. Keypoints-based Methods</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-4-Other-Methods"><span class="nav-number">5.4.4.</span> <span class="nav-text">3.4.4. Other Methods</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-5-Feature-Representation-Learning"><span class="nav-number">5.5.</span> <span class="nav-text">3.5. Feature Representation Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-1-Multi-scale-Feature-Learning"><span class="nav-number">5.5.1.</span> <span class="nav-text">3.5.1. Multi-scale Feature Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-2-Region-Feature-Encoding"><span class="nav-number">5.5.2.</span> <span class="nav-text">3.5.2. Region Feature Encoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-3-Contextual-Reasoning"><span class="nav-number">5.5.3.</span> <span class="nav-text">3.5.3. Contextual Reasoning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-4-Deformable-Feature-Learning"><span class="nav-number">5.5.4.</span> <span class="nav-text">3.5.4. Deformable Feature Learning</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-Learning-Strategy"><span class="nav-number">6.</span> <span class="nav-text">4. Learning Strategy</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-Training-Stage"><span class="nav-number">6.1.</span> <span class="nav-text">4.1. Training Stage</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-1-Data-Augmentation"><span class="nav-number">6.1.1.</span> <span class="nav-text">4.1.1. Data Augmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-2-Imbalance-Sampling"><span class="nav-number">6.1.2.</span> <span class="nav-text">4.1.2. Imbalance Sampling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-3-Localization-Refinement"><span class="nav-number">6.1.3.</span> <span class="nav-text">4.1.3. Localization Refinement</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-4-Cascade-Learning"><span class="nav-number">6.1.4.</span> <span class="nav-text">4.1.4. Cascade Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-5-Others"><span class="nav-number">6.1.5.</span> <span class="nav-text">4.1.5. Others</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-Testing-Stage"><span class="nav-number">6.2.</span> <span class="nav-text">4.2. Testing Stage</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-1-Duplicate-Removal"><span class="nav-number">6.2.1.</span> <span class="nav-text">4.2.1. Duplicate Removal</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-2-Model-Acceleration"><span class="nav-number">6.2.2.</span> <span class="nav-text">4.2.2. Model Acceleration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-3-Others"><span class="nav-number">6.2.3.</span> <span class="nav-text">4.2.3. Others</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-Applications"><span class="nav-number">7.</span> <span class="nav-text">5. Applications</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1-Face-Detection"><span class="nav-number">7.1.</span> <span class="nav-text">5.1. Face Detection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2-Pedestrian-Detection"><span class="nav-number">7.2.</span> <span class="nav-text">5.2. Pedestrian Detection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3-Others"><span class="nav-number">7.3.</span> <span class="nav-text">5.3. Others</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6-Detection-Benchmarks"><span class="nav-number">8.</span> <span class="nav-text">6. Detection Benchmarks</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#6-1-Generic-Detection-Benchmarks"><span class="nav-number">8.1.</span> <span class="nav-text">6.1. Generic Detection Benchmarks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-2-Face-Detection-Benchmarks"><span class="nav-number">8.2.</span> <span class="nav-text">6.2. Face Detection Benchmarks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-3-Pedestrian-Detection-Benchmarks"><span class="nav-number">8.3.</span> <span class="nav-text">6.3. Pedestrian Detection Benchmarks</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#7-State-of-the-art-for-Generic-Object-Detection"><span class="nav-number">9.</span> <span class="nav-text">7. State-of-the-art for Generic Object Detection</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#8-Concluding-Remarks-and-Future-Directions"><span class="nav-number">10.</span> <span class="nav-text">8. Concluding Remarks and Future Directions</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Louis Hsu</span>

  

  
</div>











        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv" title="总访客量">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="site-pv" title="总访问量">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>
























  



  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/reading_progress/reading_progress.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.4.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.4.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.4.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.4.2"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.4.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.4.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.2"></script>



  



  










  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
  <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
   <script type="text/javascript">
        var gitalk = new Gitalk({
          clientID: 'e65d27f7cf5c62feaf97',
          clientSecret: '356386826698e8b817ca076b08d7c0e9814f52ea',
          repo: 'isLouisHsu.github.io',
          owner: 'isLouisHsu',
          admin: ['isLouisHsu'],
          id: md5(window.location.pathname),
          distractionFreeMode: 'true'
        })
        gitalk.render('gitalk-container')
       </script>

  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('3');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  

  

  

  

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #eee;
      background-image: linear-gradient(#fcfcfc, #eee);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>
  <script>
    $('.highlight').each(function (i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap')
      $(e).after($wrap)
      $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function (e) {
        var code = $(this).parent().find('.code').find('.line').map(function (i, e) {
          return $(e).text()
        }).toArray().join('\n')
        var ta = document.createElement('textarea')
        document.body.appendChild(ta)
        ta.style.position = 'absolute'
        ta.style.top = '0px'
        ta.style.left = '0px'
        ta.value = code
        ta.select()
        ta.focus()
        var result = document.execCommand('copy')
        document.body.removeChild(ta)
        
        $(this).blur()
      })).on('mouseleave', function (e) {
        var $b = $(this).find('.copy-btn')
        setTimeout(function () {
          $b.text('复制')
        }, 300)
      }).append(e)
    })
  </script>



  <script type="text/javascript" src="/js/click_show_text.js"></script>
  <script type="text/javascript" src="/js/hone_hone_clock.js"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
