<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Arxiv每日速递(2021-12-13) | LOUIS' BLOG</title><meta name="author" content="徐耀彬"><meta name="copyright" content="徐耀彬"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。 统计 今日共更新250篇论文，其中：  82篇计算机视觉（cs.CV） 17篇自然语言处理（cs.CL） 100篇机器学习（cs.LG） 36篇人工智能（cs.AI）  计算机视觉    1. 标题：GAN-Supervised Dense Visual Alignment">
<meta property="og:type" content="article">
<meta property="og:title" content="Arxiv每日速递(2021-12-13)">
<meta property="og:url" content="http://louishsu.xyz/2021/12/13/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">
<meta property="og:site_name" content="LOUIS&#39; BLOG">
<meta property="og:description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。 统计 今日共更新250篇论文，其中：  82篇计算机视觉（cs.CV） 17篇自然语言处理（cs.CL） 100篇机器学习（cs.LG） 36篇人工智能（cs.AI）  计算机视觉    1. 标题：GAN-Supervised Dense Visual Alignment">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png">
<meta property="article:published_time" content="2021-12-13T00:25:58.485Z">
<meta property="article:modified_time" content="2021-12-13T00:27:44.783Z">
<meta property="article:author" content="徐耀彬">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://louishsu.xyz/2021/12/13/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-12-13 08:27:44'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/background.css"><script src="https://cdn.jsdelivr.net/npm/echarts@4.7.0/dist/echarts.min.js"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">11</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">LOUIS' BLOG</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Arxiv每日速递(2021-12-13)<a class="post-edit-link" href="https://github.com/isLouisHsu/blog/tree/master/source_posts/Arxiv每日速递.md" title="编辑" target="_blank"><i class="fas fa-pencil-square"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-12-13T00:25:58.485Z" title="发表于 2021-12-13 08:25:58">2021-12-13</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-12-13T00:27:44.783Z" title="更新于 2021-12-13 08:27:44">2021-12-13</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">阅读笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">60k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>359分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2021/12/13/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。</p>
<h1>统计</h1>
<p>今日共更新250篇论文，其中：</p>
<ul>
<li>82篇计算机视觉（<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>）</li>
<li>17篇自然语言处理（<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>）</li>
<li>100篇机器学习（cs.LG）</li>
<li>36篇人工智能（<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>）</li>
</ul>
<h1>计算机视觉</h1>
<details>
  <summary>1. <b>标题：GAN-Supervised Dense Visual Alignment</b></summary>
  <p><b>编号</b>：[1]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05143</p>
  <p><b>作者</b>：William Peebles,  Jun-Yan Zhu,  Richard Zhang,  Antonio Torralba,  Alexei Efros,  Eli Shechtman</p>
  <p><b>备注</b>：Code available at this https URL . Project page and videos available at this https URL</p>
  <p><b>关键词</b>：several datasets -- without making use, gangealing significantly outperforms past self, method successfully aligns complex data, generated training data jointly end, dense visual alignment problem</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose GAN-Supervised Learning, a framework for learning discriminative
models and their GAN-generated training data jointly end-to-end. We apply our
framework to the dense visual alignment problem. Inspired by the classic
Congealing method, our GANgealing algorithm trains a Spatial Transformer to map
random samples from a GAN trained on unaligned data to a common,
jointly-learned target mode. We show results on eight datasets, all of which
demonstrate our method successfully aligns complex data and discovers dense
correspondences. GANgealing significantly outperforms past self-supervised
correspondence algorithms and performs on-par with (and sometimes exceeds)
state-of-the-art supervised correspondence algorithms on several datasets --
without making use of any correspondence supervision or data augmentation and
despite being trained exclusively on GAN-generated data. For precise
correspondence, we improve upon state-of-the-art supervised methods by as much
as $3\times$. We show applications of our method for augmented reality, image
editing and automated pre-processing of image datasets for downstream GAN
training.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：HairCLIP: Design Your Hair by Text and Reference Image</b></summary>
  <p><b>编号</b>：[2]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05142</p>
  <p><b>作者</b>：Tianyi Wei,  Dongdong Chen,  Wenbo Zhou,  Jing Liao,  Zhentao Tan,  Lu Yuan,  Weiming Zhang,  Nenghai Yu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：many existing methods require well, enables manipulating hair attributes individually, powerful image text representation capability, new hair editing interaction mode, carefully designed network structures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Hair editing is an interesting and challenging problem in computer vision and
graphics. Many existing methods require well-drawn sketches or masks as
conditional inputs for editing, however these interactions are neither
straightforward nor efficient. In order to free users from the tedious
interaction process, this paper proposes a new hair editing interaction mode,
which enables manipulating hair attributes individually or jointly based on the
texts or reference images provided by users. For this purpose, we encode the
image and text conditions in a shared embedding space and propose a unified
hair editing framework by leveraging the powerful image text representation
capability of the Contrastive Language-Image Pre-Training (CLIP) model. With
the carefully designed network structures and loss functions, our framework can
perform high-quality hair editing in a disentangled manner. Extensive
experiments demonstrate the superiority of our approach in terms of
manipulation accuracy, visual realism of editing results, and irrelevant
attribute preservation. Project repo is this https URL.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Exploring the Equivalence of Siamese Self-Supervised Learning via A  Unified Gradient Framework</b></summary>
  <p><b>编号</b>：[3]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05141</p>
  <p><b>作者</b>：Chenxin Tao,  Honghui Wang,  Xizhou Zhu,  Jiahua Dong,  Shiji Song,  Gao Huang,  Jifeng Dai</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：extract powerful visual representations without human annotations, many downstream tasks also show, final accuracy numbers also vary, g ., barlow twins, g ., moco</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Self-supervised learning has shown its great potential to extract powerful
visual representations without human annotations. Various works are proposed to
deal with self-supervised learning from different perspectives: (1) contrastive
learning methods (e.g., MoCo, SimCLR) utilize both positive and negative
samples to guide the training direction; (2) asymmetric network methods (e.g.,
BYOL, SimSiam) get rid of negative samples via the introduction of a predictor
network and the stop-gradient operation; (3) feature decorrelation methods
(e.g., Barlow Twins, VICReg) instead aim to reduce the redundancy between
feature dimensions. These methods appear to be quite different in the designed
loss functions from various motivations. The final accuracy numbers also vary,
where different networks and tricks are utilized in different works. In this
work, we demonstrate that these methods can be unified into the same form.
Instead of comparing their loss functions, we derive a unified formula through
gradient analysis. Furthermore, we conduct fair and detailed experiments to
compare their performances. It turns out that there is little gap between these
methods, and the use of momentum encoder is the key factor to boost
performance. From this unified framework, we propose UniGrad, a simple but
effective gradient form for self-supervised learning. It does not require a
memory bank or a predictor network, but can still achieve state-of-the-art
performance and easily adopt other training strategies. Extensive experiments
on linear evaluation and many downstream tasks also show its effectiveness.
Code shall be released.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Neural Radiance Fields for Outdoor Scene Relighting</b></summary>
  <p><b>编号</b>：[4]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05140</p>
  <p><b>作者</b>：Viktor Rudnev,  Mohamed Elgharib,  William Smith,  Lingjie Liu,  Vladislav Golyanik,  Christian Theobalt</p>
  <p><b>备注</b>：13 pages, 8 figures, 2 tables; project web page: this https URL</p>
  <p><b>关键词</b>：360 degrees environment map, allow accurate numerical evaluations, technique allows simultaneous editing, outdoor scene relighting based, quality outdoor scene relighting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Photorealistic editing of outdoor scenes from photographs requires a profound
understanding of the image formation process and an accurate estimation of the
scene geometry, reflectance and illumination. A delicate manipulation of the
lighting can then be performed while keeping the scene albedo and geometry
unaltered. We present NeRF-OSR, i.e., the first approach for outdoor scene
relighting based on neural radiance fields. In contrast to the prior art, our
technique allows simultaneous editing of both scene illumination and camera
viewpoint using only a collection of outdoor photos shot in uncontrolled
settings. Moreover, it enables direct control over the scene illumination, as
defined through a spherical harmonics model. It also includes a dedicated
network for shadow reproduction, which is crucial for high-quality outdoor
scene relighting. To evaluate the proposed method, we collect a new benchmark
dataset of several outdoor sites, where each site is photographed from multiple
viewpoints and at different timings. For each timing, a 360 degrees environment
map is captured together with a colour-calibration chequerboard to allow
accurate numerical evaluations on real data against ground truth. Comparisons
against state of the art show that NeRF-OSR enables controllable lighting and
viewpoint editing at higher quality and with realistic self-shadowing
reproduction. Our method and the dataset will be made publicly available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields</b></summary>
  <p><b>编号</b>：[5]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05139</p>
  <p><b>作者</b>：Can Wang,  Menglei Chai,  Mingming He,  Dongdong Chen,  Jing Liao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：modal 3d object manipulation method, novel view synthesis capability, design two code mappers, shape conditioning via applying, disentangled conditional nerf architecture</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present CLIP-NeRF, a multi-modal 3D object manipulation method for neural
radiance fields (NeRF). By leveraging the joint language-image embedding space
of the recent Contrastive Language-Image Pre-Training (CLIP) model, we propose
a unified framework that allows manipulating NeRF in a user-friendly way, using
either a short text prompt or an exemplar image. Specifically, to combine the
novel view synthesis capability of NeRF and the controllable manipulation
ability of latent representations from generative models, we introduce a
disentangled conditional NeRF architecture that allows individual control over
both shape and appearance. This is achieved by performing the shape
conditioning via applying a learned deformation field to the positional
encoding and deferring color conditioning to the volumetric rendering stage. To
bridge this disentangled latent representation to the CLIP embedding, we design
two code mappers that take a CLIP embedding as input and update the latent
codes to reflect the targeted editing. The mappers are trained with a
CLIP-based matching loss to ensure the manipulation accuracy. Furthermore, we
propose an inverse optimization method that accurately projects an input image
to the latent codes for manipulation to enable editing on real images. We
evaluate our approach by extensive experiments on a variety of text prompts and
exemplar images and also provide an intuitive interface for interactive
editing. Our implementation is available at
this https URL</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Searching Parameterized AP Loss for Object Detection</b></summary>
  <p><b>编号</b>：[6]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05138</p>
  <p><b>作者</b>：Chenxin Tao,  Zizhang Li,  Xizhou Zhu,  Gao Huang,  Yong Liu,  Jifeng Dai</p>
  <p><b>备注</b>：Accepted by NeurIPS 2021</p>
  <p><b>关键词</b>：proposed parameterized ap loss consistently outperforms existing handcrafted losses, traditional object detectors adopt separate differentiable losses, alignment issue may well lead, propose parameterized ap loss, three different object detectors</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Loss functions play an important role in training deep-network-based object
detectors. The most widely used evaluation metric for object detection is
Average Precision (AP), which captures the performance of localization and
classification sub-tasks simultaneously. However, due to the non-differentiable
nature of the AP metric, traditional object detectors adopt separate
differentiable losses for the two sub-tasks. Such a mis-alignment issue may
well lead to performance degradation. To address this, existing works seek to
design surrogate losses for the AP metric manually, which requires expertise
and may still be sub-optimal. In this paper, we propose Parameterized AP Loss,
where parameterized functions are introduced to substitute the
non-differentiable components in the AP calculation. Different AP
approximations are thus represented by a family of parameterized functions in a
unified formula. Automatic parameter search algorithm is then employed to
search for the optimal parameters. Extensive experiments on the COCO benchmark
with three different object detectors (i.e., RetinaNet, Faster R-CNN, and
Deformable DETR) demonstrate that the proposed Parameterized AP Loss
consistently outperforms existing handcrafted losses. Code is released at
this https URL.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：PTR: A Benchmark for Part-based Conceptual, Relational, and Physical  Reasoning</b></summary>
  <p><b>编号</b>：[7]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05136</p>
  <p><b>作者</b>：Yining Hong,  Li Yi,  Joshua B. Tenenbaum,  Antonio Torralba,  Chuang Gan</p>
  <p><b>备注</b>：NeurIPS 2021. Project page: this http URL</p>
  <p><b>关键词</b>：ptr contains around 70k rgbd synthetic images, part level annotations regarding semantic instance segmentation, scale diagnostic visual reasoning dataset named ptr, existing visual reasoning benchmarks mostly focus, still make many surprising mistakes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A critical aspect of human visual perception is the ability to parse visual
scenes into individual objects and further into object parts, forming
part-whole hierarchies. Such composite structures could induce a rich set of
semantic concepts and relations, thus playing an important role in the
interpretation and organization of visual signals as well as for the
generalization of visual perception and reasoning. However, existing visual
reasoning benchmarks mostly focus on objects rather than parts. Visual
reasoning based on the full part-whole hierarchy is much more challenging than
object-centric reasoning due to finer-grained concepts, richer geometry
relations, and more complex physics. Therefore, to better serve for part-based
conceptual, relational and physical reasoning, we introduce a new large-scale
diagnostic visual reasoning dataset named PTR. PTR contains around 70k RGBD
synthetic images with ground truth object and part level annotations regarding
semantic instance segmentation, color attributes, spatial and geometric
relationships, and certain physical properties such as stability. These images
are paired with 700k machine-generated questions covering various types of
reasoning types, making them a good testbed for visual reasoning models. We
examine several state-of-the-art visual reasoning models on this dataset and
observe that they still make many surprising mistakes in situations where
humans can easily infer the correct answer. We believe this dataset will open
up new opportunities for part-based reasoning.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures</b></summary>
  <p><b>编号</b>：[8]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05135</p>
  <p><b>作者</b>：Dan Hendrycks,  Andy Zou,  Mantas Mazeika,  Leonard Tang,  Dawn Song,  Jacob Steinhardt</p>
  <p><b>备注</b>：Code and models are available at this https URL</p>
  <p><b>关键词</b>：performance beyond standard test set accuracy, methods cannot achieve without sacrificing performance, regularization techniques often improve ood robustness, safe systems must consider measures, adversarial training improves adversarial robustness</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In real-world applications of machine learning, reliable and safe systems
must consider measures of performance beyond standard test set accuracy. These
other goals include out-of-distribution (OOD) robustness, prediction
consistency, resilience to adversaries, calibrated uncertainty estimates, and
the ability to detect anomalous inputs. However, improving performance towards
these goals is often a balancing act that today's methods cannot achieve
without sacrificing performance on other safety axes. For instance, adversarial
training improves adversarial robustness but sharply degrades other classifier
performance metrics. Similarly, strong data augmentation and regularization
techniques often improve OOD robustness but harm anomaly detection, raising the
question of whether a Pareto improvement on all existing safety measures is
possible. To meet this challenge, we design a new data augmentation strategy
utilizing the natural structural complexity of pictures such as fractals, which
outperforms numerous baselines, is near Pareto-optimal, and roundly improves
safety measures.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：A Shared Representation for Photorealistic Driving Simulators</b></summary>
  <p><b>编号</b>：[9]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05134</p>
  <p><b>作者</b>：Saeed Saadatnejad,  Siyuan Li,  Taylor Mordan,  Alexandre Alahi</p>
  <p><b>备注</b>：Accepted to IEEE Transactions on Intelligent Transportation Systems (T-ITS)</p>
  <p><b>关键词</b>：human synthesis tasks across three different datasets, powerful simulator highly decreases, fine grained adversarial reasoning, cgans ), providing high, conditional generative adversarial networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A powerful simulator highly decreases the need for real-world tests when
training and evaluating autonomous vehicles. Data-driven simulators flourished
with the recent advancement of conditional Generative Adversarial Networks
(cGANs), providing high-fidelity images. The main challenge is synthesizing
photorealistic images while following given constraints. In this work, we
propose to improve the quality of generated images by rethinking the
discriminator architecture. The focus is on the class of problems where images
are generated given semantic inputs, such as scene segmentation maps or human
body poses. We build on successful cGAN models to propose a new
semantically-aware discriminator that better guides the generator. We aim to
learn a shared latent representation that encodes enough information to jointly
do semantic segmentation, content reconstruction, along with a coarse-to-fine
grained adversarial reasoning. The achieved improvements are generic and simple
enough to be applied to any architecture of conditional image synthesis. We
demonstrate the strength of our method on the scene, building, and human
synthesis tasks across three different datasets. The code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Spatio-temporal Relation Modeling for Few-shot Action Recognition</b></summary>
  <p><b>编号</b>：[10]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05132</p>
  <p><b>作者</b>：Anirudh Thatipelli,  Sanath Narayan,  Salman Khan,  Rao Muhammad Anwer,  Fahad Shahbaz Khan,  Bernard Ghanem</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：extensive ablation study reveals, shot action recognition framework, shot action recognition benchmarks, level enrichment explicitly encodes, level feature enrichment sub</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a novel few-shot action recognition framework, STRM, which
enhances class-specific feature discriminability while simultaneously learning
higher-order temporal representations. The focus of our approach is a novel
spatio-temporal enrichment module that aggregates spatial and temporal contexts
with dedicated local patch-level and global frame-level feature enrichment
sub-modules. Local patch-level enrichment captures the appearance-based
characteristics of actions. On the other hand, global frame-level enrichment
explicitly encodes the broad temporal context, thereby capturing the relevant
object features over time. The resulting spatio-temporally enriched
representations are then utilized to learn the relational matching between
query and support action sub-sequences. We further introduce a query-class
similarity classifier on the patch-level enriched features to enhance
class-specific feature discriminability by reinforcing the feature learning at
different stages in the proposed framework. Experiments are performed on four
few-shot action recognition benchmarks: Kinetics, SSv2, HMDB51 and UCF101. Our
extensive ablation study reveals the benefits of the proposed contributions.
Furthermore, our approach sets a new state-of-the-art on all four benchmarks.
On the challenging SSv2 benchmark, our approach achieves an absolute gain of
3.5% in classification accuracy, as compared to the best existing method in the
literature. Our code and models will be publicly released.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Plenoxels: Radiance Fields without Neural Networks</b></summary>
  <p><b>编号</b>：[11]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05131</p>
  <p><b>作者</b>：Alex Yu,  Sara Fridovich-Keil,  Matthew Tancik,  Qinhong Chen,  Benjamin Recht,  Angjoo Kanazawa</p>
  <p><b>备注</b>：For video and code, please see this https URL</p>
  <p><b>关键词</b>：calibrated images via gradient methods, sparse 3d grid, plenoptic voxels ),, photorealistic view synthesis, neural radiance fields</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce Plenoxels (plenoptic voxels), a system for photorealistic view
synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical
harmonics. This representation can be optimized from calibrated images via
gradient methods and regularization without any neural components. On standard,
benchmark tasks, Plenoxels are optimized two orders of magnitude faster than
Neural Radiance Fields with no loss in visual quality.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Multimodal Conditional Image Synthesis with Product-of-Experts GANs</b></summary>
  <p><b>编号</b>：[12]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05130</p>
  <p><b>作者</b>：Xun Huang,  Arun Mallya,  Ting-Chun Wang,  Ming-Yu Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：existing conditional image synthesis frameworks generate images based, best existing unimodal conditional image synthesis approaches, multimodal conditional image synthesis, multimodal multiscale projection discriminator, carefully designed training scheme</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing conditional image synthesis frameworks generate images based on user
inputs in a single modality, such as text, segmentation, sketch, or style
reference. They are often unable to leverage multimodal user inputs when
available, which reduces their practicality. To address this limitation, we
propose the Product-of-Experts Generative Adversarial Networks (PoE-GAN)
framework, which can synthesize images conditioned on multiple input modalities
or any subset of them, even the empty set. PoE-GAN consists of a
product-of-experts generator and a multimodal multiscale projection
discriminator. Through our carefully designed training scheme, PoE-GAN learns
to synthesize images with high quality and diversity. Besides advancing the
state of the art in multimodal conditional image synthesis, PoE-GAN also
outperforms the best existing unimodal conditional image synthesis approaches
when tested in the unimodal setting. The project website is available at
this https URL .</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：IterMVS: Iterative Probability Estimation for Efficient Multi-View  Stereo</b></summary>
  <p><b>编号</b>：[14]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05126</p>
  <p><b>作者</b>：Fangjinhua Wang,  Silvano Galliani,  Christoph Vogel,  Marc Pollefeys</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：model achieves competitive performance, scale matching information, combine traditional classification, better generalization ability, wise probability distributions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present IterMVS, a new data-driven method for high-resolution multi-view
stereo. We propose a novel GRU-based estimator that encodes pixel-wise
probability distributions of depth in its hidden state. Ingesting multi-scale
matching information, our model refines these distributions over multiple
iterations and infers depth and confidence. To extract the depth maps, we
combine traditional classification and regression in a novel manner. We verify
the efficiency and effectiveness of our method on DTU, Tanks&Temples and ETH3D.
While being the most efficient method in both memory and run-time, our model
achieves competitive performance on DTU and better generalization ability on
Tanks&Temples as well as ETH3D than most state-of-the-art methods. Code is
available at this https URL.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Neural Descriptor Fields: SE(3)-Equivariant Object Representations for  Manipulation</b></summary>
  <p><b>编号</b>：[16]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05124</p>
  <p><b>作者</b>：Anthony Simeonov,  Yilun Du,  Andrea Tagliasacchi,  Joshua B. Tenenbaum,  Alberto Rodriguez,  Pulkit Agrawal,  Vincent Sitzmann</p>
  <p><b>备注</b>：Website: this https URL First two authors contributed equally (order determined by coin flip), last two authors equal advising</p>
  <p><b>关键词</b>：present neural descriptor fields, pose whose descriptor matches, possible 3d object translations, 3 )- equivariant, new object instance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present Neural Descriptor Fields (NDFs), an object representation that
encodes both points and relative poses between an object and a target (such as
a robot gripper or a rack used for hanging) via category-level descriptors. We
employ this representation for object manipulation, where given a task
demonstration, we want to repeat the same task on a new object instance from
the same category. We propose to achieve this objective by searching (via
optimization) for the pose whose descriptor matches that observed in the
demonstration. NDFs are conveniently trained in a self-supervised fashion via a
3D auto-encoding task that does not rely on expert-labeled keypoints. Further,
NDFs are SE(3)-equivariant, guaranteeing performance that generalizes across
all possible 3D object translations and rotations. We demonstrate learning of
manipulation tasks from few (5-10) demonstrations both in simulation and on a
real robot. Our performance generalizes across both object instances and 6-DoF
object poses, and significantly outperforms a recent baseline that relies on 2D
descriptors. Project website: this https URL.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Self-Supervised Keypoint Discovery in Behavioral Videos</b></summary>
  <p><b>编号</b>：[17]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05121</p>
  <p><b>作者</b>：Jennifer J. Sun,  Serim Ryou,  Roni Goldshmid,  Brandon Weissbourd,  John Dabiri,  David J. Anderson,  Ann Kennedy,  Yisong Yue,  Pietro Perona</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：discovered keypoints represent semantically meaningful body parts, input videos without requiring manual annotations, discovered keypoints achieve comparable performance, keypoint regression among self, unlabelled behavioral videos</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a method for learning the posture and structure of agents from
unlabelled behavioral videos. Starting from the observation that behaving
agents are generally the main sources of movement in behavioral videos, our
method uses an encoder-decoder architecture with a geometric bottleneck to
reconstruct the difference between video frames. By focusing only on regions of
movement, our approach works directly on input videos without requiring manual
annotations, such as keypoints or bounding boxes. Experiments on a variety of
agent types (mouse, fly, human, jellyfish, and trees) demonstrate the
generality of our approach and reveal that our discovered keypoints represent
semantically meaningful body parts, which achieve state-of-the-art performance
on keypoint regression among self-supervised methods. Additionally, our
discovered keypoints achieve comparable performance to supervised keypoints on
downstream tasks, such as behavior classification, suggesting that our method
can dramatically reduce the cost of model training vis-a-vis supervised
methods.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：BLT: Bidirectional Layout Transformer for Controllable Layout Generation</b></summary>
  <p><b>编号</b>：[19]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05112</p>
  <p><b>作者</b>：Xiang Kong,  Lu Jiang,  Huiwen Chang,  Han Zhang,  Yuan Hao,  Haifeng Gong,  Irfan Essa</p>
  <p><b>备注</b>：14 pages, under review</p>
  <p><b>关键词</b>：results demonstrate two key advances, fulfill controllable layout generation, advance conditional layout generation, art layout transformer models, model empowers layout transformers</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Creating visual layouts is an important step in graphic design. Automatic
generation of such layouts is important as we seek scale-able and diverse
visual designs. Prior works on automatic layout generation focus on
unconditional generation, in which the models generate layouts while neglecting
user needs for specific problems. To advance conditional layout generation, we
introduce BLT, a bidirectional layout transformer. BLT differs from
autoregressive decoding as it first generates a draft layout that satisfies the
user inputs and then refines the layout iteratively. We verify the proposed
model on multiple benchmarks with various fidelity metrics. Our results
demonstrate two key advances to the state-of-the-art layout transformer models.
First, our model empowers layout transformers to fulfill controllable layout
generation. Second, our model slashes the linear inference time in
autoregressive decoding into a constant complexity, thereby achieving 4x-10x
speedups in generating a layout at inference time.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Extending the WILDS Benchmark for Unsupervised Adaptation</b></summary>
  <p><b>编号</b>：[21]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05090</p>
  <p><b>作者</b>：Shiori Sagawa,  Pang Wei Koh,  Tony Lee,  Irena Gao,  Sang Michael Xie,  Kendrick Shen,  Ananya Kumar,  Weihua Hu,  Michihiro Yasunaga,  Henrik Marklund,  Sara Beery,  Etienne David,  Ian Stavness,  Wei Guo,  Jure Leskovec,  Kate Saenko,  Tatsunori Hashimoto,  Sergey Levine,  Chelsea Finn,  Percy Liang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：wildlife conservation ), tasks, existing distribution shift benchmarks, machine learning systems deployed, include curated unlabeled data, molecular graphs ).</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine learning systems deployed in the wild are often trained on a source
distribution but deployed on a different target distribution. Unlabeled data
can be a powerful point of leverage for mitigating these distribution shifts,
as it is frequently much more available than labeled data. However, existing
distribution shift benchmarks for unlabeled data do not reflect the breadth of
scenarios that arise in real-world applications. In this work, we present the
WILDS 2.0 update, which extends 8 of the 10 datasets in the WILDS benchmark of
distribution shifts to include curated unlabeled data that would be
realistically obtainable in deployment. To maintain consistency, the labeled
training, validation, and test sets, as well as the evaluation metrics, are
exactly the same as in the original WILDS benchmark. These datasets span a wide
range of applications (from histology to wildlife conservation), tasks
(classification, regression, and detection), and modalities (photos, satellite
images, microscope slides, text, molecular graphs). We systematically benchmark
state-of-the-art methods that leverage unlabeled data, including
domain-invariant, self-training, and self-supervised methods, and show that
their success on WILDS 2.0 is limited. To facilitate method development and
evaluation, we provide an open-source package that automates data loading and
contains all of the model architectures and methods used in this paper. Code
and leaderboards are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Locally Shifted Attention With Early Global Integration</b></summary>
  <p><b>编号</b>：[24]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05080</p>
  <p><b>作者</b>：Shelly Sheynin,  Sagie Benaim,  Adam Polyak,  Lior Wolf</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：grained local interactions already, virtually located local patches, grained global interactions, dependent localization already, virtually located patches</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent work has shown the potential of transformers for computer vision
applications. An image is first partitioned into patches, which are then used
as input tokens for the attention mechanism. Due to the expensive quadratic
cost of the attention mechanism, either a large patch size is used, resulting
in coarse-grained global interactions, or alternatively, attention is applied
only on a local region of the image, at the expense of long-range interactions.
In this work, we propose an approach that allows for both coarse global
interactions and fine-grained local interactions already at early layers of a
vision transformer.
At the core of our method is the application of local and global attention
layers. In the local attention layer, we apply attention to each patch and its
local shifts, resulting in virtually located local patches, which are not bound
to a single, specific location. These virtually located patches are then used
in a global attention layer. The separation of the attention layer into local
and global counterparts allows for a low computational cost in the number of
patches, while still supporting data-dependent localization already at the
first layer, as opposed to the static positioning in other visual transformers.
Our method is shown to be superior to both convolutional and transformer-based
methods for image classification on CIFAR10, CIFAR100, and ImageNet. Code is
available at: this https URL.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Generating Useful Accident-Prone Driving Scenarios via a Learned Traffic  Prior</b></summary>
  <p><b>编号</b>：[25]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05077</p>
  <p><b>作者</b>：Davis Rempe,  Jonah Philion,  Leonidas J. Guibas,  Sanja Fidler,  Or Litany</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：autonomous vehicles requires scalable generation, analysis clusters generated scenarios based, strive successfully generates realistic, automatically generate challenging scenarios, based conditional vae</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Evaluating and improving planning for autonomous vehicles requires scalable
generation of long-tail traffic scenarios. To be useful, these scenarios must
be realistic and challenging, but not impossible to drive through safely. In
this work, we introduce STRIVE, a method to automatically generate challenging
scenarios that cause a given planner to produce undesirable behavior, like
collisions. To maintain scenario plausibility, the key idea is to leverage a
learned model of traffic motion in the form of a graph-based conditional VAE.
Scenario generation is formulated as an optimization in the latent space of
this traffic model, effected by perturbing an initial real-world scene to
produce trajectories that collide with a given planner. A subsequent
optimization is used to find a "solution" to the scenario, ensuring it is
useful to improve the given planner. Further analysis clusters generated
scenarios based on collision type. We attack two planners and show that STRIVE
successfully generates realistic, challenging scenarios in both cases. We
additionally "close the loop" and use these scenarios to optimize
hyperparameters of a rule-based planner.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Illumination and Temperature-Aware Multispectral Networks for  Edge-Computing-Enabled Pedestrian Detection</b></summary>
  <p><b>编号</b>：[33]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05053</p>
  <p><b>作者</b>：Yifan Zhuang,  Ziyuan Pu,  Jia Hu,  Yinhai Wang</p>
  <p><b>备注</b>：13 pages, 12 figures</p>
  <p><b>关键词</b>：intelligent transportation system regarding pedestrian safety, g ., advanced driver assistance systems, 21 seconds per image pair, 03 seconds per image pair, g ., low illumination conditions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Accurate and efficient pedestrian detection is crucial for the intelligent
transportation system regarding pedestrian safety and mobility, e.g., Advanced
Driver Assistance Systems, and smart pedestrian crosswalk systems. Among all
pedestrian detection methods, vision-based detection method is demonstrated to
be the most effective in previous studies. However, the existing vision-based
pedestrian detection algorithms still have two limitations that restrict their
implementations, those being real-time performance as well as the resistance to
the impacts of environmental factors, e.g., low illumination conditions. To
address these issues, this study proposes a lightweight Illumination and
Temperature-aware Multispectral Network (IT-MN) for accurate and efficient
pedestrian detection. The proposed IT-MN is an efficient one-stage detector.
For accommodating the impacts of environmental factors and enhancing the
sensing accuracy, thermal image data is fused by the proposed IT-MN with visual
images to enrich useful information when visual image quality is limited. In
addition, an innovative and effective late fusion strategy is also developed to
optimize the image fusion performance. To make the proposed model implementable
for edge computing, the model quantization is applied to reduce the model size
by 75% while shortening the inference time significantly. The proposed
algorithm is evaluated by comparing with the selected state-of-the-art
algorithms using a public dataset collected by in-vehicle cameras. The results
show that the proposed algorithm achieves a low miss rate and inference time at
14.19% and 0.03 seconds per image pair on GPU. Besides, the quantized IT-MN
achieves an inference time of 0.21 seconds per image pair on the edge device,
which also demonstrates the potentiality of deploying the proposed model on
edge devices as a highly efficient pedestrian detection algorithm.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Exploring Event-driven Dynamic Context for Accident Scene Segmentation</b></summary>
  <p><b>编号</b>：[42]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05006</p>
  <p><b>作者</b>：Jiaming Zhang,  Kailun Yang,  Rainer Stiefelhagen</p>
  <p><b>备注</b>：Accepted to IEEE Transactions on Intelligent Transportation Systems (T-ITS), extended version of arXiv:2008.08974, dataset and code will be made publicly available at this https URL</p>
  <p><b>关键词</b>：multiple source databases including cityscapes, enhance static rgb images, wise annotated accident dataset, art semantic segmentation methods, proposed accident dataset</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The robustness of semantic segmentation on edge cases of traffic scene is a
vital factor for the safety of intelligent transportation. However, most of the
critical scenes of traffic accidents are extremely dynamic and previously
unseen, which seriously harm the performance of semantic segmentation methods.
In addition, the delay of the traditional camera during high-speed driving will
further reduce the contextual information in the time dimension. Therefore, we
propose to extract dynamic context from event-based data with a higher temporal
resolution to enhance static RGB images, even for those from traffic accidents
with motion blur, collisions, deformations, overturns, etc. Moreover, in order
to evaluate the segmentation performance in traffic accidents, we provide a
pixel-wise annotated accident dataset, namely DADA-seg, which contains a
variety of critical scenarios from traffic accidents. Our experiments indicate
that event-based data can provide complementary information to stabilize
semantic segmentation under adverse conditions by preserving fine-grained
motion of fast-moving foreground (crash objects) in accidents. Our approach
achieves +8.2% performance gain on the proposed accident dataset, exceeding
more than 20 state-of-the-art semantic segmentation methods. The proposal has
been demonstrated to be consistently effective for models learned on multiple
source databases including Cityscapes, KITTI-360, BDD, and ApolloScape.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Mutual Adversarial Training: Learning together is better than going  alone</b></summary>
  <p><b>编号</b>：[43]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05005</p>
  <p><b>作者</b>：Jiang Liu,  Chun Pong Lau,  Hossein Souri,  Soheil Feizi,  Rama Chellappa</p>
  <p><b>备注</b>：Under submission</p>
  <p><b>关键词</b>：interactions among models affect robustness via knowledge distillation, among different perturbation types, effectively improve model robustness, propose mutual adversarial training, mat allows robust models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent studies have shown that robustness to adversarial attacks can be
transferred across networks. In other words, we can make a weak model more
robust with the help of a strong teacher model. We ask if instead of learning
from a static teacher, can models "learn together" and "teach each other" to
achieve better robustness? In this paper, we study how interactions among
models affect robustness via knowledge distillation. We propose mutual
adversarial training (MAT), in which multiple models are trained together and
share the knowledge of adversarial examples to achieve improved robustness. MAT
allows robust models to explore a larger space of adversarial samples, and find
more robust feature spaces and decision boundaries. Through extensive
experiments on CIFAR-10 and CIFAR-100, we demonstrate that MAT can effectively
improve model robustness and outperform state-of-the-art methods under
white-box attacks, bringing $\sim$8% accuracy gain to vanilla adversarial
training (AT) under PGD-100 attacks. In addition, we show that MAT can also
mitigate the robustness trade-off among different perturbation types, bringing
as much as 13.1% accuracy gain to AT baselines against the union of $l_\infty$,
$l_2$ and $l_1$ attacks. These results show the superiority of the proposed
method and demonstrate that collaborative learning is an effective strategy for
designing robust models.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：PE-former: Pose Estimation Transformer</b></summary>
  <p><b>编号</b>：[48]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04981</p>
  <p><b>作者</b>：Paschalis Panteleris,  Antonis Argyros</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：decoder transformer architecture yields state, evaluate two vit architectures, 2d body pose estimation, pure transformer architecture, vision transformer architectures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision transformer architectures have been demonstrated to work very
effectively for image classification tasks. Efforts to solve more challenging
vision tasks with transformers rely on convolutional backbones for feature
extraction. In this paper we investigate the use of a pure transformer
architecture (i.e., one with no CNN backbone) for the problem of 2D body pose
estimation. We evaluate two ViT architectures on the COCO dataset. We
demonstrate that using an encoder-decoder transformer architecture yields state
of the art results on this estimation problem.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：AdaStereo: An Efficient Domain-Adaptive Stereo Matching Approach</b></summary>
  <p><b>编号</b>：[51]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04974</p>
  <p><b>作者</b>：Xiao Song,  Guorun Yang,  Xinge Zhu,  Hui Zhou,  Yuexin Ma,  Zhe Wang,  Jianping Shi</p>
  <p><b>备注</b>：To be published in International Journal of Computer Vision (IJCV)</p>
  <p><b>关键词</b>：adversarial progressive color transfer algorithm, two additional evaluation metrics, perform intensive ablation studies, highly related auxiliary task, free cost normalization layer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, records on stereo matching benchmarks are constantly broken by
end-to-end disparity networks. However, the domain adaptation ability of these
deep models is quite limited. Addressing such problem, we present a novel
domain-adaptive approach called AdaStereo that aims to align multi-level
representations for deep stereo matching networks. Compared to previous
methods, our AdaStereo realizes a more standard, complete and effective domain
adaptation pipeline. Firstly, we propose a non-adversarial progressive color
transfer algorithm for input image-level alignment. Secondly, we design an
efficient parameter-free cost normalization layer for internal feature-level
alignment. Lastly, a highly related auxiliary task, self-supervised
occlusion-aware reconstruction is presented to narrow the gaps in output space.
We perform intensive ablation studies and break-down comparisons to validate
the effectiveness of each proposed module. With no extra inference overhead and
only a slight increase in training complexity, our AdaStereo models achieve
state-of-the-art cross-domain performance on multiple benchmarks, including
KITTI, Middlebury, ETH3D and DrivingStereo, even outperforming some
state-of-the-art disparity networks finetuned with target-domain ground-truths.
Moreover, based on two additional evaluation metrics, the superiority of our
domain-adaptive stereo matching pipeline is further uncovered from more
perspectives. Finally, we demonstrate that our method is robust to various
domain adaptation settings, and can be easily integrated into quick adaptation
application scenarios and real-world deployments.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：CaSP: Class-agnostic Semi-Supervised Pretraining for Detection and  Segmentation</b></summary>
  <p><b>编号</b>：[54]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04966</p>
  <p><b>作者</b>：Lu Qi,  Jason Kuen,  Zhe Lin,  Jiuxiang Gu,  Fengyun Rao,  Dian Li,  Weidong Guo,  Zhen Wen,  Jiaya Jia</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：pretrained model also demonstrates excellent transferability, specific training signals causes underfitting, supervised methods extract either, specific training signals, better avoid underfitting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To improve instance-level detection/segmentation performance, existing
self-supervised and semi-supervised methods extract either very task-unrelated
or very task-specific training signals from unlabeled data. We argue that these
two approaches, at the two extreme ends of the task-specificity spectrum, are
suboptimal for the task performance. Utilizing too little task-specific
training signals causes underfitting to the ground-truth labels of downstream
tasks, while the opposite causes overfitting to the ground-truth labels. To
this end, we propose a novel Class-agnostic Semi-supervised Pretraining (CaSP)
framework to achieve a more favorable task-specificity balance in extracting
training signals from unlabeled data. Compared to semi-supervised learning,
CaSP reduces the task specificity in training signals by ignoring class
information in the pseudo labels and having a separate pretraining stage that
uses only task-unrelated unlabeled data. On the other hand, CaSP preserves the
right amount of task specificity by leveraging box/mask-level pseudo labels. As
a result, our pretrained model can better avoid underfitting/overfitting to
ground-truth labels when finetuned on the downstream task. Using 3.6M unlabeled
data, we achieve a remarkable performance gain of 4.7% over ImageNet-pretrained
baseline on object detection. Our pretrained model also demonstrates excellent
transferability to other detection and segmentation tasks/frameworks.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：DVHN: A Deep Hashing Framework for Large-scale Vehicle Re-identification</b></summary>
  <p><b>编号</b>：[63]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04937</p>
  <p><b>作者</b>：Yongbiao Chen,  Sheng Zhang,  Fangxin Liu,  Chenggang Wu,  Kaicheng Guo,  Zhengwei Qi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：concretely ,~ dvhn directly learns discrete compact binary hash codes, reserving nearest neighbor search accuracy, hash code generating module, substantially reduces memory usage, art deep hash methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we make the very first attempt to investigate the integration
of deep hash learning with vehicle re-identification. We propose a deep
hash-based vehicle re-identification framework, dubbed DVHN, which
substantially reduces memory usage and promotes retrieval efficiency while
reserving nearest neighbor search accuracy. Concretely,~DVHN directly learns
discrete compact binary hash codes for each image by jointly optimizing the
feature learning network and the hash code generating module. Specifically, we
directly constrain the output from the convolutional neural network to be
discrete binary codes and ensure the learned binary codes are optimal for
classification. To optimize the deep discrete hashing framework, we further
propose an alternating minimization method for learning binary
similarity-preserved hashing codes. Extensive experiments on two widely-studied
vehicle re-identification datasets- \textbf{VehicleID} and \textbf{VeRi}-~have
demonstrated the superiority of our method against the state-of-the-art deep
hash methods. \textbf{DVHN} of $2048$ bits can achieve 13.94\% and 10.21\%
accuracy improvement in terms of \textbf{mAP} and \textbf{Rank@1} for
\textbf{VehicleID (800)} dataset. For \textbf{VeRi}, we achieve 35.45\% and
32.72\% performance gains for \textbf{Rank@1} and \textbf{mAP}, respectively.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Model Doctor: A Simple Gradient Aggregation Strategy for Diagnosing and  Treating CNN Classifiers</b></summary>
  <p><b>编号</b>：[64]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04934</p>
  <p><b>作者</b>：Zunlei Feng,  Jiacong Hu,  Sai Wu,  Xiaotian Yu,  Jie Song,  Mingli Song</p>
  <p><b>备注</b>：Accepted by AAAI 2022</p>
  <p><b>关键词</b>：first completely automatic model diagnosing, simple aggregate gradient constraint, 1 %- 5 %., proposed model doctor applies, aggregate gradient strategy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, Convolutional Neural Network (CNN) has achieved excellent
performance in the classification task. It is widely known that CNN is deemed
as a 'black-box', which is hard for understanding the prediction mechanism and
debugging the wrong prediction. Some model debugging and explanation works are
developed for solving the above drawbacks. However, those methods focus on
explanation and diagnosing possible causes for model prediction, based on which
the researchers handle the following optimization of models manually. In this
paper, we propose the first completely automatic model diagnosing and treating
tool, termed as Model Doctor. Based on two discoveries that 1) each category is
only correlated with sparse and specific convolution kernels, and 2)
adversarial samples are isolated while normal samples are successive in the
feature space, a simple aggregate gradient constraint is devised for
effectively diagnosing and optimizing CNN classifiers. The aggregate gradient
strategy is a versatile module for mainstream CNN classifiers. Extensive
experiments demonstrate that the proposed Model Doctor applies to all existing
CNN classifiers, and improves the accuracy of $16$ mainstream CNN classifiers
by 1%-5%.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Self-Supervised Image-to-Text and Text-to-Image Synthesis</b></summary>
  <p><b>编号</b>：[65]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04928</p>
  <p><b>作者</b>：Anindya Sundar Das,  Sriparna Saha</p>
  <p><b>备注</b>：ICONIP 2021 : The 28th International Conference on Neural Information Processing</p>
  <p><b>关键词</b>：supervised deep learning based approach towards learning, maximum mean discrepancy based generative networks, first obtain dense vector representations, level utilizing lstm based text, supervised generative deep architectures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A comprehensive understanding of vision and language and their interrelation
are crucial to realize the underlying similarities and differences between
these modalities and to learn more generalized, meaningful representations. In
recent years, most of the works related to Text-to-Image synthesis and
Image-to-Text generation, focused on supervised generative deep architectures
to solve the problems, where very little interest was placed on learning the
similarities between the embedding spaces across modalities. In this paper, we
propose a novel self-supervised deep learning based approach towards learning
the cross-modal embedding spaces; for both image to text and text to image
generations. In our approach, we first obtain dense vector representations of
images using StackGAN-based autoencoder model and also dense vector
representations on sentence-level utilizing LSTM based text-autoencoder; then
we study the mapping from embedding space of one modality to embedding space of
the other modality utilizing GAN and maximum mean discrepancy based generative
networks. We, also demonstrate that our model learns to generate textual
description from image data as well as images from textual data both
qualitatively and quantitatively.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Few-Shot Keypoint Detection as Task Adaptation via Latent Embeddings</b></summary>
  <p><b>编号</b>：[69]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04910</p>
  <p><b>作者</b>：Mel Vecerik,  Jackie Kay,  Raia Hadsell,  Lourdes Agapito,  Jon Scholz</p>
  <p><b>备注</b>：Supplementary material available at: this https URL</p>
  <p><b>关键词</b>：existing approaches either compute dense keypoint embeddings, important computer vision task, offering accuracy significantly closer, localize specific object points, dense object tracking</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Dense object tracking, the ability to localize specific object points with
pixel-level accuracy, is an important computer vision task with numerous
downstream applications in robotics. Existing approaches either compute dense
keypoint embeddings in a single forward pass, meaning the model is trained to
track everything at once, or allocate their full capacity to a sparse
predefined set of points, trading generality for accuracy. In this paper we
explore a middle ground based on the observation that the number of relevant
points at a given time are typically relatively few, e.g. grasp points on a
target object. Our main contribution is a novel architecture, inspired by
few-shot task adaptation, which allows a sparse-style network to condition on a
keypoint embedding that indicates which point to track. Our central finding is
that this approach provides the generality of dense-embedding models, while
offering accuracy significantly closer to sparse-keypoint approaches. We
present results illustrating this capacity vs. accuracy trade-off, and
demonstrate the ability to zero-shot transfer to new object instances
(within-class) using a real-robot pick-and-place task.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：PRA-Net: Point Relation-Aware Network for 3D Point Cloud Analysis</b></summary>
  <p><b>编号</b>：[72]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04903</p>
  <p><b>作者</b>：Silin Cheng,  Xiwu Chen,  Xinwei He,  Zhe Liu,  Xiang Bai</p>
  <p><b>备注</b>：13 pages</p>
  <p><b>关键词</b>：several 3d benchmarks covering shape classification, novel framework named point relation, differentiable region partition scheme, irl module captures inter, region relation learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Learning intra-region contexts and inter-region relations are two effective
strategies to strengthen feature representations for point cloud analysis.
However, unifying the two strategies for point cloud representation is not
fully emphasized in existing methods. To this end, we propose a novel framework
named Point Relation-Aware Network (PRA-Net), which is composed of an
Intra-region Structure Learning (ISL) module and an Inter-region Relation
Learning (IRL) module. The ISL module can dynamically integrate the local
structural information into the point features, while the IRL module captures
inter-region relations adaptively and efficiently via a differentiable region
partition scheme and a representative point-based strategy. Extensive
experiments on several 3D benchmarks covering shape classification, keypoint
estimation, and part segmentation have verified the effectiveness and the
generalization ability of PRA-Net. Code will be available at
this https URL .</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Learning Personal Representations from fMRIby Predicting Neurofeedback  Performance</b></summary>
  <p><b>编号</b>：[73]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04902</p>
  <p><b>作者</b>：Jhonathan Osin,  Lior Wolf,  Guy Gurevitch,  Jackob Nimrod Keynan,  Tom Fruchtman-Steinbok,  Ayelet Or-Borichev,  Shira Reznik Balter,  Talma Hendler</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：next fmri frame given recent fmri frames, supervised recurrent neural network, deep neural network method, frame prediction considerably, yields good performance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a deep neural network method for learning a personal
representation for individuals that are performing a self neuromodulation task,
guided by functional MRI (fMRI). This neurofeedback task (watch vs. regulate)
provides the subjects with a continuous feedback contingent on down regulation
of their Amygdala signal and the learning algorithm focuses on this region's
time-course of activity. The representation is learned by a self-supervised
recurrent neural network, that predicts the Amygdala activity in the next fMRI
frame given recent fMRI frames and is conditioned on the learned individual
representation. It is shown that the individuals' representation improves the
next-frame prediction considerably. Moreover, this personal representation,
learned solely from fMRI images, yields good performance in linear prediction
of psychiatric traits, which is better than performing such a prediction based
on clinical data and personality tests. Our code is attached as supplementary
and the data would be shared subject to ethical approvals.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Latent Space Explanation by Intervention</b></summary>
  <p><b>编号</b>：[75]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04895</p>
  <p><b>作者</b>：Itai Gat,  Guy Lorberbom,  Idan Schwartz,  Tamir Hazan</p>
  <p><b>备注</b>：Accepted to AAAI22</p>
  <p><b>关键词</b>：deep neural nets heavily relies, suggest different interventions, show various visualizations, hence providing interpretability, encode complex relations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The success of deep neural nets heavily relies on their ability to encode
complex relations between their input and their output. While this property
serves to fit the training data well, it also obscures the mechanism that
drives prediction. This study aims to reveal hidden concepts by employing an
intervention mechanism that shifts the predicted class based on discrete
variational autoencoders. An explanatory model then visualizes the encoded
information from any hidden layer and its corresponding intervened
representation. By the assessment of differences between the original
representation and the intervened representation, one can determine the
concepts that can alter the class, hence providing interpretability. We
demonstrate the effectiveness of our approach on CelebA, where we show various
visualizations for bias in the data and suggest different interventions to
reveal and change bias.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：A Bilingual, OpenWorld Video Text Dataset and End-to-end Video Text  Spotter with Transformer</b></summary>
  <p><b>编号</b>：[79]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04888</p>
  <p><b>作者</b>：Weijia Wu,  Yuanqiang Cai,  Debing Zhang,  Sibo Wang,  Zhuang Li,  Jiahong Li,  Yejun Tang,  Hong Zhou</p>
  <p><b>备注</b>：20 pages, 6 figures</p>
  <p><b>关键词</b>：existing video text spotting benchmarks focus, open world video text benchmark dataset, end video text spotting framework, bovtext provides bilingual text annotation, abundant text types annotation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Most existing video text spotting benchmarks focus on evaluating a single
language and scenario with limited data. In this work, we introduce a
large-scale, Bilingual, Open World Video text benchmark dataset(BOVText). There
are four features for BOVText. Firstly, we provide 2,000+ videos with more than
1,750,000+ frames, 25 times larger than the existing largest dataset with
incidental text in videos. Secondly, our dataset covers 30+ open categories
with a wide selection of various scenarios, e.g., Life Vlog, Driving, Movie,
etc. Thirdly, abundant text types annotation (i.e., title, caption or scene
text) are provided for the different representational meanings in video.
Fourthly, the BOVText provides bilingual text annotation to promote multiple
cultures live and communication. Besides, we propose an end-to-end video text
spotting framework with Transformer, termed TransVTSpotter, which solves the
multi-orient text spotting in video with a simple, but efficient
attention-based query-key mechanism. It applies object features from the
previous frame as a tracking query for the current frame and introduces a
rotation angle prediction to fit the multiorient text instance. On
ICDAR2015(video), TransVTSpotter achieves the state-of-the-art performance with
44.1% MOTA, 9 fps. The dataset and code of TransVTSpotter can be found at
github:com=weijiawu=BOVText and github:com=weijiawu=TransVTSpotter,
respectively.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：ScaleNet: A Shallow Architecture for Scale Estimation</b></summary>
  <p><b>编号</b>：[87]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04846</p>
  <p><b>作者</b>：Axel Barroso-Laguna,  Yurun Tian,  Krystian Mikolajczyk</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：improve camera pose estimation, sparse local features, significant performance improvements, exploits dilated convolutions, estimated scales leads</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we address the problem of estimating scale factors between
images. We formulate the scale estimation problem as a prediction of a
probability distribution over scale factors. We design a new architecture,
ScaleNet, that exploits dilated convolutions as well as self and
cross-correlation layers to predict the scale between images. We demonstrate
that rectifying images with estimated scales leads to significant performance
improvements for various tasks and methods. Specifically, we show how ScaleNet
can be combined with sparse local features and dense correspondence networks to
improve camera pose estimation, 3D reconstruction, or dense geometric matching
in different benchmarks and datasets. We provide an extensive evaluation on
several tasks and analyze the computational overhead of ScaleNet. The code,
evaluation protocols, and trained models are publicly available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Knowledge Distillation for Object Detection via Rank Mimicking and  Prediction-guided Feature Imitation</b></summary>
  <p><b>编号</b>：[90]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04840</p>
  <p><b>作者</b>：Gang Li,  Xiang Li,  Yujie Wang,  Shanshan Zhang,  Yichao Wu,  Ding Liang</p>
  <p><b>备注</b>：Accepted by AAAI 2022</p>
  <p><b>关键词</b>：making feature imitation directly help, detected candidate boxes quite differently, also outperforms previous kd methods, consequently realizing model compression, designing specific kd methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Knowledge Distillation (KD) is a widely-used technology to inherit
information from cumbersome teacher models to compact student models,
consequently realizing model compression and acceleration. Compared with image
classification, object detection is a more complex task, and designing specific
KD methods for object detection is non-trivial. In this work, we elaborately
study the behaviour difference between the teacher and student detection
models, and obtain two intriguing observations: First, the teacher and student
rank their detected candidate boxes quite differently, which results in their
precision discrepancy. Second, there is a considerable gap between the feature
response differences and prediction differences between teacher and student,
indicating that equally imitating all the feature maps of the teacher is the
sub-optimal choice for improving the student's accuracy. Based on the two
observations, we propose Rank Mimicking (RM) and Prediction-guided Feature
Imitation (PFI) for distilling one-stage detectors, respectively. RM takes the
rank of candidate boxes from teachers as a new form of knowledge to distill,
which consistently outperforms the traditional soft label distillation. PFI
attempts to correlate feature differences with prediction differences, making
feature imitation directly help to improve the student's accuracy. On MS COCO
and PASCAL VOC benchmarks, extensive experiments are conducted on various
detectors with different backbones to validate the effectiveness of our method.
Specifically, RetinaNet with ResNet50 achieves 40.4% mAP in MS COCO, which is
3.5% higher than its baseline, and also outperforms previous KD methods.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Explainability of the Implications of Supervised and Unsupervised Face  Image Quality Estimations Through Activation Map Variation Analyses in Face  Recognition Models</b></summary>
  <p><b>编号</b>：[94]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04827</p>
  <p><b>作者</b>：Biying Fu,  Naser Damer</p>
  <p><b>备注</b>：accepted at the IEEE Winter Conference on Applications of Computer Vision Workshops, WACV Workshops 2022</p>
  <p><b>关键词</b>：quality images typically cause consistent low activation, based fr solution using activation mapping, based face image quality assessment, general spatial activation mapping, issues like extreme poses</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>It is challenging to derive explainability for unsupervised or
statistical-based face image quality assessment (FIQA) methods. In this work,
we propose a novel set of explainability tools to derive reasoning for
different FIQA decisions and their face recognition (FR) performance
implications. We avoid limiting the deployment of our tools to certain FIQA
methods by basing our analyses on the behavior of FR models when processing
samples with different FIQA decisions. This leads to explainability tools that
can be applied for any FIQA method with any CNN-based FR solution using
activation mapping to exhibit the network's activation derived from the face
embedding. To avoid the low discrimination between the general spatial
activation mapping of low and high-quality images in FR models, we build our
explainability tools in a higher derivative space by analyzing the variation of
the FR activation maps of image sets with different quality decisions. We
demonstrate our tools and analyze the findings on four FIQA methods, by
presenting inter and intra-FIQA method analyses. Our proposed tools and the
analyses based on them point out, among other conclusions, that high-quality
images typically cause consistent low activation on the areas outside of the
central face region, while low-quality images, despite general low activation,
have high variations of activation in such areas. Our explainability tools also
extend to analyzing single images where we show that low-quality images tend to
have an FR model spatial activation that strongly differs from what is expected
from a high-quality image where this difference also tends to appear more in
areas outside of the central face region and does correspond to issues like
extreme poses and facial occlusions. The implementation of the proposed tools
is accessible here [link].</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Progressive Attention on Multi-Level Dense Difference Maps for Generic  Event Boundary Detection</b></summary>
  <p><b>编号</b>：[106]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04771</p>
  <p><b>作者</b>：Jiaqi Tang,  Zhaoyang Liu,  Chen Qian,  Wayne Wu,  Limin Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：humans naturally perceive event boundaries, perceiving various temporal variations, make three notable improvements, cvpr 2021 without bells, alleviate inadequate temporal modeling</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generic event boundary detection is an important yet challenging task in
video understanding, which aims at detecting the moments where humans naturally
perceive event boundaries. The main challenge of this task is perceiving
various temporal variations of diverse event boundaries. To this end, this
paper presents an effective and end-to-end learnable framework (DDM-Net). To
tackle the diversity and complicated semantics of event boundaries, we make
three notable improvements. First, we construct a feature bank to store
multi-level features of space and time, prepared for difference calculation at
multiple scales. Second, to alleviate inadequate temporal modeling of previous
methods, we present dense difference maps (DDM) to comprehensively characterize
the motion pattern. Finally, we exploit progressive attention on multi-level
DDM to jointly aggregate appearance and motion clues. As a result, DDM-Net
respectively achieves a significant boost of 14% and 8% on Kinetics-GEBD and
TAPOS benchmark, and outperforms the top-1 winner solution of LOVEU
Challenge@CVPR 2021 without bells and whistles. The state-of-the-art result
demonstrates the effectiveness of richer motion representation and more
sophisticated aggregation, in handling the diversity of generic event boundary
detection. Our codes will be made available soon.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Adaptive Methods for Aggregated Domain Generalization</b></summary>
  <p><b>编号</b>：[107]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04766</p>
  <p><b>作者</b>：Xavier Thomas,  Dhruv Mahajan,  Alex Pentland,  Abhimanyu Dubey</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：domain generalization benchmarks without using domain labels whatsoever, privacy concerns prohibit obtaining domain labels, domain generalization using cluster information, makes predictions using information, domain generalization involves learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Domain generalization involves learning a classifier from a heterogeneous
collection of training sources such that it generalizes to data drawn from
similar unknown target domains, with applications in large-scale learning and
personalized inference. In many settings, privacy concerns prohibit obtaining
domain labels for the training data samples, and instead only have an
aggregated collection of training points. Existing approaches that utilize
domain labels to create domain-invariant feature representations are
inapplicable in this setting, requiring alternative approaches to learn
generalizable classifiers. In this paper, we propose a domain-adaptive approach
to this problem, which operates in two steps: (a) we cluster training data
within a carefully chosen feature space to create pseudo-domains, and (b) using
these pseudo-domains we learn a domain-adaptive classifier that makes
predictions using information about both the input and the pseudo-domain it
belongs to. Our approach achieves state-of-the-art performance on a variety of
domain generalization benchmarks without using domain labels whatsoever.
Furthermore, we provide novel theoretical guarantees on domain generalization
using cluster information. Our approach is amenable to ensemble-based methods
and provides substantial gains even on large-scale benchmark datasets. The code
can be found at: this https URL</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：3D-VField: Learning to Adversarially Deform Point Clouds for Robust 3D  Object Detection</b></summary>
  <p><b>编号</b>：[108]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04764</p>
  <p><b>作者</b>：Alexander Lehner,  Stefano Gasperini,  Alvaro Marcos-Ramiro,  Michael Schmidt,  Mohammad-Ali Nikouei Mahani,  Nassir Navab,  Benjamin Busam,  Federico Tombari</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：plausibly deforms objects via vectors learned, share open source crashd, account deformed point clouds, approach constrains 3d points, differently shaped objects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As 3D object detection on point clouds relies on the geometrical
relationships between the points, non-standard object shapes can hinder a
method's detection capability. However, in safety-critical settings, robustness
on out-of-distribution and long-tail samples is fundamental to circumvent
dangerous issues, such as the misdetection of damaged or rare cars. In this
work, we substantially improve the generalization of 3D object detectors to
out-of-domain data by taking into account deformed point clouds during
training. We achieve this with 3D-VField: a novel method that plausibly deforms
objects via vectors learned in an adversarial fashion. Our approach constrains
3D points to slide along their sensor view rays while neither adding nor
removing any of them. The obtained vectors are transferrable,
sample-independent and preserve shape smoothness and occlusions. By augmenting
normal samples with the deformations produced by these vector fields during
training, we significantly improve robustness against differently shaped
objects, such as damaged/deformed cars, even while training only on KITTI.
Towards this end, we propose and share open source CrashD: a synthetic dataset
of realistic damaged and rare cars, with a variety of crash scenarios.
Extensive experiments on KITTI, Waymo, our CrashD and SUN RGB-D show the high
generalizability of our techniques to out-of-domain data, different models and
sensors, namely LiDAR and ToF cameras, for both indoor and outdoor scenes. Our
CrashD dataset is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：HBReID: Harder Batch for Re-identification</b></summary>
  <p><b>编号</b>：[109]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04761</p>
  <p><b>作者</b>：Wen Li,  Furong Xu,  Jianan Zhao,  Ruobing Zheng,  Cheng Zou,  Meng Wang,  Yuan Cheng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：learn scene invariant feature representations, adversarial scene removal module composed, hardest negative pairs far away, hardest positive pairs close, widely adopted loss function</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Triplet loss is a widely adopted loss function in ReID task which pulls the
hardest positive pairs close and pushes the hardest negative pairs far away.
However, the selected samples are not the hardest globally, but the hardest
only in a mini-batch, which will affect the performance. In this report, a hard
batch mining method is proposed to mine the hardest samples globally to make
triplet harder. More specifically, the most similar classes are selected into a
same mini-batch so that the similar classes could be pushed further away.
Besides, an adversarial scene removal module composed of a scene classifier and
an adversarial loss is used to learn scene invariant feature representations.
Experiments are conducted on dataset MSMT17 to prove the effectiveness, and our
method surpasses all of the previous methods and sets state-of-the-art result.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Does Redundancy in AI Perception Systems Help to Test for Super-Human  Automated Driving Performance?</b></summary>
  <p><b>编号</b>：[110]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04758</p>
  <p><b>作者</b>：Hanno Gottschalk,  Matthias Rottmann,  Maida Saltagic</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：labeled data needed would exceed dimensions, trained using special loss functions, 3d mnist data set, provide direct statistical evidence, commonly used strategy therefore</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While automated driving is often advertised with better-than-human driving
performance, this work reviews that it is nearly impossible to provide direct
statistical evidence on the system level that this is actually the case. The
amount of labeled data needed would exceed dimensions of present day technical
and economical capabilities. A commonly used strategy therefore is the use of
redundancy along with the proof of sufficient subsystems' performances. As it
is known, this strategy is efficient especially for the case of subsystems
operating independently, i.e. the occurrence of errors is independent in a
statistical sense. Here, we give some first considerations and experimental
evidence that this strategy is not a free ride as the errors of neural networks
fulfilling the same computer vision task, at least for some cases, show
correlated occurrences of errors. This remains true, if training data,
architecture, and training are kept separate or independence is trained using
special loss functions. Using data from different sensors (realized by up to
five 2D projections of the 3D MNIST data set) in our experiments is more
efficiently reducing correlations, however not to an extent that is realizing
the potential of reduction of testing data that can be obtained for redundant
and statistically independent subsystems.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：BLPnet: A New DNN model for Automatic License Plate Detection with  Bengali OCR</b></summary>
  <p><b>编号</b>：[112]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04752</p>
  <p><b>作者</b>：Md Saif Hassan Onim,  Hussain Nyeem,  Koushik Roy,  Mahmudul Hasan,  Abtahi Ishmam,  Md. Akiful Hoque Akif,  Tareque Bashar Ovi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：model feeding with17 frames per second, mean license plate character recognition accuracy, reasonably accurate automatic license plate recognition, developing robust license plate detection models, would significantly reduce computational cost</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep Neural Network (DNN) models with image processing and object
localization have the potential to advance the automatic traffic control and
monitoring system. Despite some notable progress in developing robust license
plate detection models, research endeavours continue to reduce computational
complexities with higher detection accuracy. This paper reports a
computationally efficient and reasonably accurate Automatic License Plate
Recognition (ALPR) system for Bengali characters with a new DNN model that we
call Bengali License Plate Network (BLPnet). Additionally, the cascaded
architectures for detecting vehicle regions prior to VLP in the proposed model,
would significantly reduce computational cost and false-positives making the
system faster and more accurate. Besides, with a new Bengali OCR engine and
word-mapping process, the model can readily extract, detect and output the
complete license-plate number of a vehicle. The model feeding with17 frames per
second (fps) on real-time video footage can detect a vehicle with the Mean
Squared Error (MSE) of 0.0152, and the mean license plate character recognition
accuracy of 95%. While compared to the other models, an improvement of 5% and
20% were recorded for the BLPnet over the prominent YOLO-based ALPR model and
Tesseract model for the number-plate detection accuracy and time requirement,
respectively.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：Superpixel-Based Building Damage Detection from Post-earthquake Very  High Resolution Imagery Using Deep Neural Networks</b></summary>
  <p><b>编号</b>：[117]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04744</p>
  <p><b>作者</b>：Jun Wang,  Zhoujing Li,  Yixuan Qiao,  Qiming Qin,  Peng Gao,  Guotong Xie</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：trained dnn using stacked denoising auto, vhr images using deep neural networks, initiating effective emergency response actions, improved semantic similarity criterion composed, dnn could boost detection accuracy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Building damage detection after natural disasters like earthquakes is crucial
for initiating effective emergency response actions. Remotely sensed very high
spatial resolution (VHR) imagery can provide vital information due to their
ability to map the affected buildings with high geometric precision. Many
approaches have been developed to detect damaged buildings due to earthquakes.
However, little attention has been paid to exploiting rich features represented
in VHR images using Deep Neural Networks (DNN). This paper presents a novel
super-pixel based approach combining DNN and a modified segmentation method, to
detect damaged buildings from VHR imagery. Firstly, a modified Fast Scanning
and Adaptive Merging method is extended to create initial over-segmentation.
Secondly, the segments are merged based on the Region Adjacent Graph (RAG),
considered an improved semantic similarity criterion composed of Local Binary
Patterns (LBP) texture, spectral, and shape features. Thirdly, a pre-trained
DNN using Stacked Denoising Auto-Encoders called SDAE-DNN is presented, to
exploit the rich semantic features for building damage detection. Deep-layer
feature abstraction of SDAE-DNN could boost detection accuracy through learning
more intrinsic and discriminative features, which outperformed other methods
using state-of-the-art alternative classifiers. We demonstrate the feasibility
and effectiveness of our method using a subset of WorldView-2 imagery, in the
complex urban areas of Bhaktapur, Nepal, which was affected by the Nepal
Earthquake of April 25, 2015.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Mimicking the Oracle: An Initial Phase Decorrelation Approach for Class  Incremental Learning</b></summary>
  <p><b>编号</b>：[122]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04731</p>
  <p><b>作者</b>：Yujun Shi,  Kuangqi Zhou,  Jian Liang,  Zihang Jiang,  Jiashi Feng,  Philip Torr,  Song Bai,  Vincent Y.F. Tan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：previous works mainly focus, various benchmark datasets show, directly encouraging cil learner, since one major difference, around 1 \%</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Class Incremental Learning (CIL) aims at learning a multi-class classifier in
a phase-by-phase manner, in which only data of a subset of the classes are
provided at each phase. Previous works mainly focus on mitigating forgetting in
phases after the initial one. However, we find that improving CIL at its
initial phase is also a promising direction. Specifically, we experimentally
show that directly encouraging CIL Learner at the initial phase to output
similar representations as the model jointly trained on all classes can greatly
boost the CIL performance. Motivated by this, we study the difference between a
naïvely-trained initial-phase model and the oracle model. Specifically, since
one major difference between these two models is the number of training
classes, we investigate how such difference affects the model representations.
We find that, with fewer training classes, the data representations of each
class lie in a long and narrow region; with more training classes, the
representations of each class scatter more uniformly. Inspired by this
observation, we propose Class-wise Decorrelation (CwD) that effectively
regularizes representations of each class to scatter more uniformly, thus
mimicking the model jointly trained with all classes (i.e., the oracle model).
Our CwD is simple to implement and easy to plug into existing methods.
Extensive experiments on various benchmark datasets show that CwD consistently
and significantly improves the performance of existing state-of-the-art methods
by around 1\% to 3\%. Code will be released.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Amicable Aid: Turning Adversarial Attack to Benefit Classification</b></summary>
  <p><b>编号</b>：[125]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04720</p>
  <p><b>作者</b>：Juyeop Kim,  Jun-Ho Choi,  Soobeom Jang,  Jong-Seok Lee</p>
  <p><b>备注</b>：16 pages (3 pages for appendix)</p>
  <p><b>关键词</b>：deep image classification models pose serious security concerns, also consider universal amicable perturbations, discuss several application scenarios, another yielding higher confidence, including secure image communication</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While adversarial attacks on deep image classification models pose serious
security concerns in practice, this paper suggests a novel paradigm where the
concept of adversarial attacks can benefit classification performance, which we
call amicable aid. We show that by taking the opposite search direction of
perturbation, an image can be converted to another yielding higher confidence
by the classification model and even a wrongly classified image can be made to
be correctly classified. Furthermore, with a large amount of perturbation, an
image can be made unrecognizable by human eyes, while it is correctly
recognized by the model. The mechanism of the amicable aid is explained in the
viewpoint of the underlying natural image manifold. We also consider universal
amicable perturbations, i.e., a fixed perturbation can be applied to multiple
images to improve their classification results. While it is challenging to find
such perturbations, we show that making the decision boundary as perpendicular
to the image manifold as possible via training with modified data is effective
to obtain a model for which universal amicable perturbations are more easily
found. Finally, we discuss several application scenarios where the amicable aid
can be useful, including secure image communication, privacy-preserving image
communication, and protection against adversarial attacks.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Learning with Nested Scene Modeling and Cooperative Architecture Search  for Low-Light Vision</b></summary>
  <p><b>编号</b>：[126]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04719</p>
  <p><b>作者</b>：Risheng Liu,  Long Ma,  Tengyu Ma,  Xin Fan,  Zhongxuan Luo</p>
  <p><b>备注</b>：Submitted to IEEE TPAMI. Code is available at this https URL</p>
  <p><b>关键词</b>：cooperatively search specific scene, light scenes often suffer, challenging downstream vision applications, nested optimization formulation, general learning framework</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Images captured from low-light scenes often suffer from severe degradations,
including low visibility, color cast and intensive noises, etc. These factors
not only affect image qualities, but also degrade the performance of downstream
Low-Light Vision (LLV) applications. A variety of deep learning methods have
been proposed to enhance the visual quality of low-light images. However, these
approaches mostly rely on significant architecture engineering to obtain proper
low-light models and often suffer from high computational burden. Furthermore,
it is still challenging to extend these enhancement techniques to handle other
LLVs. To partially address above issues, we establish Retinex-inspired
Unrolling with Architecture Search (RUAS), a general learning framework, which
not only can address low-light enhancement task, but also has the flexibility
to handle other more challenging downstream vision applications. Specifically,
we first establish a nested optimization formulation, together with an
unrolling strategy, to explore underlying principles of a series of LLV tasks.
Furthermore, we construct a differentiable strategy to cooperatively search
specific scene and task architectures for RUAS. Last but not least, we
demonstrate how to apply RUAS for both low- and high-level LLV applications
(e.g., enhancement, detection and segmentation). Extensive experiments verify
the flexibility, effectiveness, and efficiency of RUAS.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Auto-X3D: Ultra-Efficient Video Understanding via Finer-Grained Neural  Architecture Search</b></summary>
  <p><b>编号</b>：[131]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04710</p>
  <p><b>作者</b>：Yifan Jiang,  Xinyu Gong,  Junru Wu,  Humphrey Shi,  Zhicheng Yan,  Zhangyang Wang</p>
  <p><b>备注</b>：Accepted by WACV'2022</p>
  <p><b>关键词</b>：crafted image architecture along multiple axes, autox3d models outperform existing ones, probabilistic neural architecture search method, paper bypasses existing 2d architectures, x3d searches one axis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Efficient video architecture is the key to deploying video recognition
systems on devices with limited computing resources. Unfortunately, existing
video architectures are often computationally intensive and not suitable for
such applications. The recent X3D work presents a new family of efficient video
models by expanding a hand-crafted image architecture along multiple axes, such
as space, time, width, and depth. Although operating in a conceptually large
space, X3D searches one axis at a time, and merely explored a small set of 30
architectures in total, which does not sufficiently explore the space. This
paper bypasses existing 2D architectures, and directly searched for 3D
architectures in a fine-grained space, where block type, filter number,
expansion ratio and attention block are jointly searched. A probabilistic
neural architecture search method is adopted to efficiently search in such a
large space. Evaluations on Kinetics and Something-Something-V2 benchmarks
confirm our AutoX3D models outperform existing ones in accuracy up to 1.3%
under similar FLOPs, and reduce the computational cost up to x1.74 when
reaching similar performance.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Implicit Feature Refinement for Instance Segmentation</b></summary>
  <p><b>编号</b>：[132]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04709</p>
  <p><b>作者</b>：Lufan Ma,  Tiancai Wang,  Bin Dong,  Jiangpeng Yan,  Xiu Li,  Xiangyu Zhang</p>
  <p><b>备注</b>：Published at ACM MM 2021. Code is available at this https URL</p>
  <p><b>关键词</b>：sharing convolution blocks provides competitive performance, play general module easily extended, video instance segmentation methods rely, novel implicit feature refinement module, ifr achieves improved performance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a novel implicit feature refinement module for high-quality
instance segmentation. Existing image/video instance segmentation methods rely
on explicitly stacked convolutions to refine instance features before the final
prediction. In this paper, we first give an empirical comparison of different
refinement strategies,which reveals that the widely-used four consecutive
convolutions are not necessary. As an alternative, weight-sharing convolution
blocks provides competitive performance. When such block is iterated for
infinite times, the block output will eventually convergeto an equilibrium
state. Based on this observation, the implicit feature refinement (IFR) is
developed by constructing an implicit function. The equilibrium state of
instance features can be obtained by fixed-point iteration via a simulated
infinite-depth network. Our IFR enjoys several advantages: 1) simulates an
infinite-depth refinement network while only requiring parameters of single
residual block; 2) produces high-level equilibrium instance features of global
receptive field; 3) serves as a plug-and-play general module easily extended to
most object recognition frameworks. Experiments on the COCO and YouTube-VIS
benchmarks show that our IFR achieves improved performance on state-of-the-art
image/video instance segmentation frameworks, while reducing the parameter
burden (e.g.1% AP improvement on Mask R-CNN with only 30.0% parameters in mask
head). Code is made available at this https URL</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Fast Point Transformer</b></summary>
  <p><b>编号</b>：[136]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04702</p>
  <p><b>作者</b>：Chunghyun Park,  Yoonwoo Jeong,  Minsu Cho,  Jaesik Park</p>
  <p><b>备注</b>：16 pages, 8 figures</p>
  <p><b>关键词</b>：network achieves 136 times faster inference time, scheme inevitably involves additional stages, based architecture boosts computational efficiency, paper introduces fast point transformer, approach encodes continuous 3d coordinates</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The recent success of neural networks enables a better interpretation of 3D
point clouds, but processing a large-scale 3D scene remains a challenging
problem. Most current approaches divide a large-scale scene into small regions
and combine the local predictions together. However, this scheme inevitably
involves additional stages for pre- and post-processing and may also degrade
the final output due to predictions in a local perspective. This paper
introduces Fast Point Transformer that consists of a new lightweight
self-attention layer. Our approach encodes continuous 3D coordinates, and the
voxel hashing-based architecture boosts computational efficiency. The proposed
method is demonstrated with 3D semantic segmentation and 3D detection. The
accuracy of our approach is competitive to the best voxel-based method, and our
network achieves 136 times faster inference time than the state-of-the-art,
Point Transformer, with a reasonable accuracy trade-off.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Unsupervised Complementary-aware Multi-process Fusion for Visual Place  Recognition</b></summary>
  <p><b>编号</b>：[137]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04701</p>
  <p><b>作者</b>：Stephen Hausler,  Tobias Fischer,  Michael Milford</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：multiple complementary vpr techniques simultaneously, superior vpr performance compared, visual place recognition, specific deployment environment, proposed dynamic multi</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A recent approach to the Visual Place Recognition (VPR) problem has been to
fuse the place recognition estimates of multiple complementary VPR techniques
simultaneously. However, selecting the optimal set of techniques to use in a
specific deployment environment a-priori is a difficult and unresolved
challenge. Further, to the best of our knowledge, no method exists which can
select a set of techniques on a frame-by-frame basis in response to
image-to-image variations. In this work, we propose an unsupervised algorithm
that finds the most robust set of VPR techniques to use in the current
deployment environment, on a frame-by-frame basis. The selection of techniques
is determined by an analysis of the similarity scores between the current query
image and the collection of database images and does not require ground-truth
information. We demonstrate our approach on a wide variety of datasets and VPR
techniques and show that the proposed dynamic multi-process fusion (Dyn-MPF)
has superior VPR performance compared to a variety of challenging competitive
methods, some of which are given an unfair advantage through access to the
ground-truth information.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Trajectory-Constrained Deep Latent Visual Attention for Improved Local  Planning in Presence of Heterogeneous Terrain</b></summary>
  <p><b>编号</b>：[143]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04684</p>
  <p><b>作者</b>：Stefan Wapnick,  Travis Manderson,  David Meger,  Gregory Dudek</p>
  <p><b>备注</b>：Published in International Conference on Intelligent Robots and Systems (IROS) 2021 proceedings. Project website: this https URL</p>
  <p><b>关键词</b>：experiments involved randomized procedural generated simulation, based deep learning method featuring trajectory, latent feature map space instead, allowing adaptability yet encouraging, local visual navigation tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a reward-predictive, model-based deep learning method featuring
trajectory-constrained visual attention for use in mapless, local visual
navigation tasks. Our method learns to place visual attention at locations in
latent image space which follow trajectories caused by vehicle control actions
to enhance predictive accuracy during planning. The attention model is jointly
optimized by the task-specific loss and an additional trajectory-constraint
loss, allowing adaptability yet encouraging a regularized structure for
improved generalization and reliability. Importantly, visual attention is
applied in latent feature map space instead of raw image space to promote
efficient planning. We validated our model in visual navigation tasks of
planning low turbulence, collision-free trajectories in off-road settings and
hill climbing with locking differentials in the presence of slippery terrain.
Experiments involved randomized procedural generated simulation and real-world
environments. We found our method improved generalization and learning
efficiency when compared to no-attention and self-attention alternatives.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：SimIPU: Simple 2D Image and 3D Point Cloud Unsupervised Pre-Training for  Spatial-Aware Visual Representations</b></summary>
  <p><b>编号</b>：[146]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04680</p>
  <p><b>作者</b>：Zhenyu Li,  Zehui Chen,  Ang Li,  Liangji Fang,  Qinhong Jiang,  Xianming Liu,  Junjun Jiang,  Bolei Zhou,  Hang Zhao</p>
  <p><b>备注</b>：Accepted to 36th AAAI Conference on Artificial Intelligence (AAAI 2022)</p>
  <p><b>关键词</b>：simple yet effective 2d image, 3d point cloud unsupervised pre, modal feature interaction module, containing paired camera images, many computer vision tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pre-training has become a standard paradigm in many computer vision tasks.
However, most of the methods are generally designed on the RGB image domain.
Due to the discrepancy between the two-dimensional image plane and the
three-dimensional space, such pre-trained models fail to perceive spatial
information and serve as sub-optimal solutions for 3D-related tasks. To bridge
this gap, we aim to learn a spatial-aware visual representation that can
describe the three-dimensional space and is more suitable and effective for
these tasks. To leverage point clouds, which are much more superior in
providing spatial information compared to images, we propose a simple yet
effective 2D Image and 3D Point cloud Unsupervised pre-training strategy,
called SimIPU. Specifically, we develop a multi-modal contrastive learning
framework that consists of an intra-modal spatial perception module to learn a
spatial-aware representation from point clouds and an inter-modal feature
interaction module to transfer the capability of perceiving spatial information
from the point cloud encoder to the image encoder, respectively. Positive pairs
for contrastive losses are established by the matching algorithm and the
projection matrix. The whole framework is trained in an unsupervised end-to-end
fashion. To the best of our knowledge, this is the first study to explore
contrastive learning pre-training strategies for outdoor multi-modal datasets,
containing paired camera images and LIDAR point clouds. Codes and models are
available at this https URL.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：DualFormer: Local-Global Stratified Transformer for Efficient Video  Recognition</b></summary>
  <p><b>编号</b>：[147]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04674</p>
  <p><b>作者</b>：Yuxuan Liang,  Pan Zhou,  Roger Zimmermann,  Shuicheng Yan</p>
  <p><b>备注</b>：Preprint</p>
  <p><b>关键词</b>：often suffer high computational costs induced, restrict attention computations within local windows, time interactions among nearby 3d tokens, around 1000g inference flops, grained global pyramid contexts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While transformers have shown great potential on video recognition tasks with
their strong capability of capturing long-range dependencies, they often suffer
high computational costs induced by self-attention operation on the huge number
of 3D tokens in a video. In this paper, we propose a new transformer
architecture, termed DualFormer, which can effectively and efficiently perform
space-time attention for video recognition. Specifically, our DualFormer
stratifies the full space-time attention into dual cascaded levels, i.e., to
first learn fine-grained local space-time interactions among nearby 3D tokens,
followed by the capture of coarse-grained global dependencies between the query
token and the coarse-grained global pyramid contexts. Different from existing
methods that apply space-time factorization or restrict attention computations
within local windows for improving efficiency, our local-global stratified
strategy can well capture both short- and long-range spatiotemporal
dependencies, and meanwhile greatly reduces the number of keys and values in
attention computation to boost efficiency. Experimental results show the
superiority of DualFormer on five video benchmarks against existing methods. In
particular, DualFormer sets new state-of-the-art 82.9%/85.2% top-1 accuracy on
Kinetics-400/600 with around 1000G inference FLOPs which is at least 3.2 times
fewer than existing methods with similar performances.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Style Mixing and Patchwise Prototypical Matching for One-Shot  Unsupervised Domain Adaptive Semantic Segmentation</b></summary>
  <p><b>编号</b>：[150]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04665</p>
  <p><b>作者</b>：Xinyi Wu,  Zhenyao Wu,  Yuhang Lu,  Lili Ju,  Song Wang</p>
  <p><b>备注</b>：Accepted by AAAI 2022</p>
  <p><b>关键词</b>：traditional unsupervised domain adaptation models usually fail since, existing osuda methods usually integrate, see one unlabeled target image, shot unsupervised domain adaptation, two commonly used benchmarks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we tackle the problem of one-shot unsupervised domain
adaptation (OSUDA) for semantic segmentation where the segmentors only see one
unlabeled target image during training. In this case, traditional unsupervised
domain adaptation models usually fail since they cannot adapt to the target
domain with over-fitting to one (or few) target samples. To address this
problem, existing OSUDA methods usually integrate a style-transfer module to
perform domain randomization based on the unlabeled target sample, with which
multiple domains around the target sample can be explored during training.
However, such a style-transfer module relies on an additional set of images as
style reference for pre-training and also increases the memory demand for
domain adaptation. Here we propose a new OSUDA method that can effectively
relieve such computational burden. Specifically, we integrate several
style-mixing layers into the segmentor which play the role of style-transfer
module to stylize the source images without introducing any learned parameters.
Moreover, we propose a patchwise prototypical matching (PPM) method to weighted
consider the importance of source pixels during the supervised training to
relieve the negative adaptation. Experimental results show that our method
achieves new state-of-the-art performance on two commonly used benchmarks for
domain adaptive semantic segmentation under the one-shot setting and is more
efficient than all comparison approaches.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Dual Cluster Contrastive learning for Person Re-Identification</b></summary>
  <p><b>编号</b>：[151]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04662</p>
  <p><b>作者</b>：Hantao Yao,  Changsheng Xu</p>
  <p><b>备注</b>：10 pages, 6 figures</p>
  <p><b>关键词</b>：named dual cluster contrastive learning, unified cluster contrastive framework, unsupervised person reid demonstrate, centroid cluster memory banks, centroid cluster memory applies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, cluster contrastive learning has been proven effective for person
ReID by computing the contrastive loss between the individual feature and the
cluster memory. However, existing methods that use the individual feature to
momentum update the cluster memory are not robust to the noisy samples, such as
the samples with wrong annotated labels or the pseudo-labels. Unlike the
individual-based updating mechanism, the centroid-based updating mechanism that
applies the mean feature of each cluster to update the cluster memory is robust
against minority noisy samples. Therefore, we formulate the individual-based
updating and centroid-based updating mechanisms in a unified cluster
contrastive framework, named Dual Cluster Contrastive learning (DCC), which
maintains two types of memory banks: individual and centroid cluster memory
banks. Significantly, the individual cluster memory is momentum updated based
on the individual feature.The centroid cluster memory applies the mean feature
of each cluter to update the corresponding cluster memory. Besides the vallina
contrastive loss for each memory, a consistency constraint is applied to
guarantee the consistency of the output of two memories. Note that DCC can be
easily applied for unsupervised or supervised person ReID by using ground-truth
labels or pseudo-labels generated with clustering method, respectively.
Extensive experiments on two benchmarks under supervised person ReID and
unsupervised person ReID demonstrate the superior of the proposed DCC. Code is
available at: this https URL</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：BACON: Band-limited Coordinate Networks for Multiscale Scene  Representation</b></summary>
  <p><b>编号</b>：[153]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04645</p>
  <p><b>作者</b>：David B. Lindell,  Dave Van Veen,  Jeong Joon Park,  Gordon Wetzstein</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：3d scenes using signed distance functions, multiple scales without explicit supervision, map continuous input coordinates, analytical fourier spectrum, outperforms conventional single</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Coordinate-based networks have emerged as a powerful tool for 3D
representation and scene reconstruction. These networks are trained to map
continuous input coordinates to the value of a signal at each point. Still,
current architectures are black boxes: their spectral characteristics cannot be
easily analyzed, and their behavior at unsupervised points is difficult to
predict. Moreover, these networks are typically trained to represent a signal
at a single scale, and so naive downsampling or upsampling results in
artifacts. We introduce band-limited coordinate networks (BACON), a network
architecture with an analytical Fourier spectrum. BACON has predictable
behavior at unsupervised points, can be designed based on the spectral
characteristics of the represented signal, and can represent signals at
multiple scales without explicit supervision. We demonstrate BACON for
multiscale neural representation of images, radiance fields, and 3D scenes
using signed distance functions and show that it outperforms conventional
single-scale coordinate networks in terms of interpretability and quality.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Recurrent Glimpse-based Decoder for Detection with Transformer</b></summary>
  <p><b>编号</b>：[160]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04632</p>
  <p><b>作者</b>：Zhe Chen,  Jing Zhang,  Dacheng Tao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：rego helps deformable detr achieve 44, provide refined detection results based, mainly develop advanced feature, enlarged bounding box areas, extremely long training period</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Although detection with Transformer (DETR) is increasingly popular, its
global attention modeling requires an extremely long training period to
optimize and achieve promising detection performance. Alternative to existing
studies that mainly develop advanced feature or embedding designs to tackle the
training issue, we point out that the Region-of-Interest (RoI) based detection
refinement can easily help mitigate the difficulty of training for DETR
methods. Based on this, we introduce a novel REcurrent Glimpse-based decOder
(REGO) in this paper. In particular, the REGO employs a multi-stage recurrent
processing structure to help the attention of DETR gradually focus on
foreground objects more accurately. In each processing stage, visual features
are extracted as glimpse features from RoIs with enlarged bounding box areas of
detection results from the previous stage. Then, a glimpse-based decoder is
introduced to provide refined detection results based on both the glimpse
features and the attention modeling outputs of the previous stage. In practice,
REGO can be easily embedded in representative DETR variants while maintaining
their fully end-to-end training and inference pipelines. In particular, REGO
helps Deformable DETR achieve 44.8 AP on the MSCOCO dataset with only 36
training epochs, compared with the first DETR and the Deformable DETR that
require 500 and 50 epochs to achieve comparable performance, respectively.
Experiments also show that REGO consistently boosts the performance of
different DETR detectors by up to 7% relative gain at the same setting of 50
training epochs. Code is available via
this https URL.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Learning Auxiliary Monocular Contexts Helps Monocular 3D Object  Detection</b></summary>
  <p><b>编号</b>：[163]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04628</p>
  <p><b>作者</b>：Xianpeng Liu,  Nan Xue,  Tianfu Wu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：posed projected 2d supervision signals available, monocular 3d object detection without exploiting, monocular 3d object detection aims, help monocular 3d object detection, localize 3d bounding boxes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Monocular 3D object detection aims to localize 3D bounding boxes in an input
single 2D image. It is a highly challenging problem and remains open,
especially when no extra information (e.g., depth, lidar and/or multi-frames)
can be leveraged in training and/or inference. This paper proposes a simple yet
effective formulation for monocular 3D object detection without exploiting any
extra information. It presents the MonoCon method which learns Monocular
Contexts, as auxiliary tasks in training, to help monocular 3D object
detection. The key idea is that with the annotated 3D bounding boxes of objects
in an image, there is a rich set of well-posed projected 2D supervision signals
available in training, such as the projected corner keypoints and their
associated offset vectors with respect to the center of 2D bounding box, which
should be exploited as auxiliary tasks in training. The proposed MonoCon is
motivated by the Cramer-Wold theorem in measure theory at a high level. In
implementation, it utilizes a very simple end-to-end design to justify the
effectiveness of learning auxiliary monocular contexts, which consists of three
components: a Deep Neural Network (DNN) based feature backbone, a number of
regression head branches for learning the essential parameters used in the 3D
bounding box prediction, and a number of regression head branches for learning
auxiliary contexts. After training, the auxiliary context regression branches
are discarded for better inference efficiency. In experiments, the proposed
MonoCon is tested in the KITTI benchmark (car, pedestrain and cyclist). It
outperforms all prior arts in the leaderboard on car category and obtains
comparable performance on pedestrian and cyclist in terms of accuracy. Thanks
to the simple design, the proposed MonoCon method obtains the fastest inference
speed with 38.7 fps in comparisons</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：A Simple and efficient deep Scanpath Prediction</b></summary>
  <p><b>编号</b>：[170]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04610</p>
  <p><b>作者</b>：Mohamed Amine Kerkouri,  Aladine Chetouani</p>
  <p><b>备注</b>：Electronic Imaging Symposium 2022 (EI 2022)</p>
  <p><b>关键词</b>：literature using complex deep learning architectures, using common deep learning architectures, sometimes surpass previous complex architectures, simple fully convolutional regressive manner, different leveraged backbone architectures based</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Visual scanpath is the sequence of fixation points that the human gaze
travels while observing an image, and its prediction helps in modeling the
visual attention of an image. To this end several models were proposed in the
literature using complex deep learning architectures and frameworks. Here, we
explore the efficiency of using common deep learning architectures, in a simple
fully convolutional regressive manner. We experiment how well these models can
predict the scanpaths on 2 datasets. We compare with other models using
different metrics and show competitive results that sometimes surpass previous
complex architectures. We also compare the different leveraged backbone
architectures based on their performances on the experiment to deduce which
ones are the most suitable for the task.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Enhancing Food Intake Tracking in Long-Term Care with Automated Food  Imaging and Nutrient Intake Tracking (AFINI-T) Technology</b></summary>
  <p><b>编号</b>：[171]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04608</p>
  <p><b>作者</b>：Kaylen J. Pfisterer,  Robert Amelard,  Jennifer Boger,  Audrey G. Chung,  Heather H. Keller,  Alexander Wong</p>
  <p><b>备注</b>：Key words: Automatic segmentation, convolutional neural network, deep learning, food intake tracking, volume estimation, malnutrition prevention, long-term care, hospital</p>
  <p><b>关键词</b>：learning powered computational nutrient sensing system, objectively tracking ltc resident food intake, 9 %; mean intake error, simulated ltc food intake dataset, prevent malnutrition tracking strategies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Half of long-term care (LTC) residents are malnourished increasing
hospitalization, mortality, morbidity, with lower quality of life. Current
tracking methods are subjective and time consuming. This paper presents the
automated food imaging and nutrient intake tracking (AFINI-T) technology
designed for LTC. We propose a novel convolutional autoencoder for food
classification, trained on an augmented UNIMIB2016 dataset and tested on our
simulated LTC food intake dataset (12 meal scenarios; up to 15 classes each;
top-1 classification accuracy: 88.9%; mean intake error: -0.4 mL$\pm$36.7 mL).
Nutrient intake estimation by volume was strongly linearly correlated with
nutrient estimates from mass ($r^2$ 0.92 to 0.99) with good agreement between
methods ($\sigma$= -2.7 to -0.01; zero within each of the limits of agreement).
The AFINI-T approach is a deep-learning powered computational nutrient sensing
system that may provide a novel means for more accurately and objectively
tracking LTC resident food intake to support and prevent malnutrition tracking
strategies.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：Constrained Mean Shift Using Distant Yet Related Neighbors for  Representation Learning</b></summary>
  <p><b>编号</b>：[172]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04607</p>
  <p><b>作者</b>：Ajinkya Tejankar,  Soroush Abbasi Koohpayegani,  KL Navaneet,  Kossar Pourahmadi,  Akshayvarun Subramanya,  Hamed Pirsiavash</p>
  <p><b>备注</b>：Code is available at this https URL arXiv admin note: text overlap with arXiv:2110.10309</p>
  <p><b>关键词</b>：choosing far away neighbors, still semantically related, less training resources, generalize msf algorithm, method outperforms msf</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We are interested in representation learning in self-supervised, supervised,
or semi-supervised settings. The prior work on applying mean-shift idea for
self-supervised learning, MSF, generalizes the BYOL idea by pulling a query
image to not only be closer to its other augmentation, but also to the nearest
neighbors (NNs) of its other augmentation. We believe the learning can benefit
from choosing far away neighbors that are still semantically related to the
query. Hence, we propose to generalize MSF algorithm by constraining the search
space for nearest neighbors. We show that our method outperforms MSF in SSL
setting when the constraint utilizes a different augmentation of an image, and
outperforms PAWS in semi-supervised setting with less training resources when
the constraint ensures the NNs have the same pseudo-label as the query.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：A Unified Architecture of Semantic Segmentation and Hierarchical  Generative Adversarial Networks for Expression Manipulation</b></summary>
  <p><b>编号</b>：[175]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04603</p>
  <p><b>作者</b>：Rumeysa Bodur,  Binod Bhattarai,  Tae-Kyun Kim</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：two challenging facial expression translation benchmarks, hq across two popular architectures, face expression manipulation tasks validate, introduces unwanted artefacts degrading, bounding boxes centred around</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Editing facial expressions by only changing what we want is a long-standing
research problem in Generative Adversarial Networks (GANs) for image
manipulation. Most of the existing methods that rely only on a global generator
usually suffer from changing unwanted attributes along with the target
attributes. Recently, hierarchical networks that consist of both a global
network dealing with the whole image and multiple local networks focusing on
local parts are showing success. However, these methods extract local regions
by bounding boxes centred around the sparse facial key points which are
non-differentiable, inaccurate and unrealistic. Hence, the solution becomes
sub-optimal, introduces unwanted artefacts degrading the overall quality of the
synthetic images. Moreover, a recent study has shown strong correlation between
facial attributes and local semantic regions. To exploit this relationship, we
designed a unified architecture of semantic segmentation and hierarchical GANs.
A unique advantage of our framework is that on forward pass the semantic
segmentation network conditions the generative model, and on backward pass
gradients from hierarchical GANs are propagated to the semantic segmentation
network, which makes our framework an end-to-end differentiable architecture.
This allows both architectures to benefit from each other. To demonstrate its
advantages, we evaluate our method on two challenging facial expression
translation benchmarks, AffectNet and RaFD, and a semantic segmentation
benchmark, CelebAMask-HQ across two popular architectures, BiSeNet and UNet.
Our extensive quantitative and qualitative evaluations on both face semantic
segmentation and face expression manipulation tasks validate the effectiveness
of our work over existing state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：InvGAN: Invertable GANs</b></summary>
  <p><b>编号</b>：[177]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04598</p>
  <p><b>作者</b>：Partha Ghosh,  Dominik Zietlow,  Michael J. Black,  Larry S. Davis,  Xiaochen Hu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：high resolution generative models, successfully embeds real images, high quality generative model, real images using, generative model together</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generation of photo-realistic images, semantic editing and representation
learning are a few of many potential applications of high resolution generative
models. Recent progress in GANs have established them as an excellent choice
for such tasks. However, since they do not provide an inference model, image
editing or downstream tasks such as classification can not be done on real
images using the GAN latent space. Despite numerous efforts to train an
inference model or design an iterative method to invert a pre-trained
generator, previous methods are dataset (e.g. human face images) and
architecture (e.g. StyleGAN) specific. These methods are nontrivial to extend
to novel datasets or architectures. We propose a general framework that is
agnostic to architecture and datasets. Our key insight is that, by training the
inference and the generative model together, we allow them to adapt to each
other and to converge to a better quality model. Our \textbf{InvGAN}, short for
Invertable GAN, successfully embeds real images to the latent space of a high
quality generative model. This allows us to perform image inpainting, merging,
interpolation and online data augmentation. We demonstrate this with extensive
qualitative and quantitative experiments.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：STAF: A Spatio-Temporal Attention Fusion Network for Few-shot Video  Classification</b></summary>
  <p><b>编号</b>：[182]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04585</p>
  <p><b>作者</b>：Rex Liu,  Huanle Zhang,  Hamed Pirsiavash,  Xin Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：3d convolution neural networks embedding network, extracted features using self, staf first extracts coarse, g ., staf increases, temporal attention fusion network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose STAF, a Spatio-Temporal Attention Fusion network for few-shot
video classification. STAF first extracts coarse-grained spatial and temporal
features of videos by applying a 3D Convolution Neural Networks embedding
network. It then fine-tunes the extracted features using self-attention and
cross-attention networks. Last, STAF applies a lightweight fusion network and a
nearest neighbor classifier to classify each query video. To evaluate STAF, we
conduct extensive experiments on three benchmarks (UCF101, HMDB51, and
Something-Something-V2). The experimental results show that STAF improves
state-of-the-art accuracy by a large margin, e.g., STAF increases the five-way
one-shot accuracy by 5.3% and 7.0% for UCF101 and HMDB51, respectively.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：CoSSL: Co-Learning of Representation and Classifier for Imbalanced  Semi-Supervised Learning</b></summary>
  <p><b>编号</b>：[192]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04564</p>
  <p><b>作者</b>：Yue Fan,  Dengxin Dai,  Bernt Schiele</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：various shifted test distributions, balanced test sets, made publicly available, class feature enhancement, benchmark datasets ranging</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we propose a novel co-learning framework (CoSSL) with
decoupled representation learning and classifier learning for imbalanced SSL.
To handle the data imbalance, we devise Tail-class Feature Enhancement (TFE)
for classifier learning. Furthermore, the current evaluation protocol for
imbalanced SSL focuses only on balanced test sets, which has limited
practicality in real-world scenarios. Therefore, we further conduct a
comprehensive evaluation under various shifted test distributions. In
experiments, we show that our approach outperforms other methods over a large
range of shifted distributions, achieving state-of-the-art performance on
benchmark datasets ranging from CIFAR-10, CIFAR-100, ImageNet, to Food-101. Our
code will be made publicly available.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：SoK: Anti-Facial Recognition Technology</b></summary>
  <p><b>编号</b>：[195]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04558</p>
  <p><b>作者</b>：Emily Wenger,  Shawn Shan,  Haitao Zheng,  Ben Y. Zhao</p>
  <p><b>备注</b>：13 pages</p>
  <p><b>关键词</b>：help users avoid unwanted facial recognition, social challenges facing afr tools, first comprehensive analysis, broader design space, different afr approaches</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rapid adoption of facial recognition (FR) technology by both government
and commercial entities in recent years has raised concerns about civil
liberties and privacy. In response, a broad suite of so-called "anti-facial
recognition" (AFR) tools has been developed to help users avoid unwanted facial
recognition. The set of AFR tools proposed in the last few years is
wide-ranging and rapidly evolving, necessitating a step back to consider the
broader design space of AFR systems and long-term challenges. This paper aims
to fill that gap and provides the first comprehensive analysis of the AFR
research landscape. Using the operational stages of FR systems as a starting
point, we create a systematic framework for analyzing the benefits and
tradeoffs of different AFR approaches. We then consider both technical and
social challenges facing AFR tools and propose directions for future research
in this field.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：Segment and Complete: Defending Object Detectors against Adversarial  Patch Attacks with Robust Patch Detection</b></summary>
  <p><b>编号</b>：[204]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04532</p>
  <p><b>作者</b>：Jiang Liu,  Alexander Levine,  Chun Pong Lau,  Rama Chellappa,  Soheil Feizi</p>
  <p><b>备注</b>：Under submission</p>
  <p><b>关键词</b>：sac achieves superior robustness even, robust shape completion algorithm, targeted attack success rate, self adversarial training algorithm, xview datasets demonstrate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Object detection plays a key role in many security-critical systems.
Adversarial patch attacks, which are easy to implement in the physical world,
pose a serious threat to state-of-the-art object detectors. Developing reliable
defenses for object detectors against patch attacks is critical but severely
understudied. In this paper, we propose Segment and Complete defense (SAC), a
general framework for defending object detectors against patch attacks through
detecting and removing adversarial patches. We first train a patch segmenter
that outputs patch masks that provide pixel-level localization of adversarial
patches. We then propose a self adversarial training algorithm to robustify the
patch segmenter. In addition, we design a robust shape completion algorithm,
which is guaranteed to remove the entire patch from the images given the
outputs of the patch segmenter are within a certain Hamming distance of the
ground-truth patch masks. Our experiments on COCO and xView datasets
demonstrate that SAC achieves superior robustness even under strong adaptive
attacks with no performance drop on clean images, and generalizes well to
unseen patch shapes, attack budgets, and unseen attack methods. Furthermore, we
present the APRICOT-Mask dataset, which augments the APRICOT dataset with
pixel-level annotations of adversarial patches. We show SAC can significantly
reduce the targeted attack success rate of physical patch attacks.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：SIRfyN: Single Image Relighting from your Neighbors</b></summary>
  <p><b>编号</b>：[205]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04497</p>
  <p><b>作者</b>：D.A. Forsyth,  Anand Bhattad,  Pranav Asthana,  Yuanyi Zhong,  Yuxiong Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：procedure include generating training data, restore soft indoor shadows, resulting image looks like, ground truth data, use similar scenes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We show how to relight a scene, depicted in a single image, such that (a) the
overall shading has changed and (b) the resulting image looks like a natural
image of that scene. Applications for such a procedure include generating
training data and building authoring environments. Naive methods for doing this
fail. One reason is that shading and albedo are quite strongly related; for
example, sharp boundaries in shading tend to appear at depth discontinuities,
which usually apparent in albedo. The same scene can be lit in different ways,
and established theory shows the different lightings form a cone (the
illumination cone). Novel theory shows that one can use similar scenes to
estimate the different lightings that apply to a given scene, with bounded
expected error. Our method exploits this theory to estimate a representation of
the available lighting fields in the form of imputed generators of the
illumination cone. Our procedure does not require expensive "inverse graphics"
datasets, and sees no ground truth data of any kind.
Qualitative evaluation suggests the method can erase and restore soft indoor
shadows, and can "steer" light around a scene. We offer a summary quantitative
evaluation of the method with a novel application of the FID. An extension of
the FID allows per-generated-image evaluation. Furthermore, we offer
qualitative evaluation with a user study, and show that our method produces
images that can successfully be used for data augmentation.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：Critical configurations for two projective views, a new approach</b></summary>
  <p><b>编号</b>：[213]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05074</p>
  <p><b>作者</b>：Martin Bråtelund</p>
  <p><b>备注</b>：22 pages, 6 figures</p>
  <p><b>关键词</b>：two projective cameras, critical configurations lie, called critical configurations, paper also describes, critical configurations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The problem of structure from motion is concerned with recovering
3-dimensional structure of an object from a set of 2-dimensional images.
Generally, all information can be uniquely recovered if enough images and image
points are provided, but there are certain cases where unique recovery is
impossible; these are called critical configurations. In this paper we use an
algebraic approach to study the critical configurations for two projective
cameras. We show that all critical configurations lie on quadric surfaces, and
classify exactly which quadrics constitute a critical configuration. The paper
also describes the relation between the different reconstructions when unique
reconstruction is impossible.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：Sparse-View CT Reconstruction using Recurrent Stacked Back Projection</b></summary>
  <p><b>编号</b>：[218]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04998</p>
  <p><b>作者</b>：Wenrui Li,  Gregery T. Buzzard,  Charles A. Bouman</p>
  <p><b>备注</b>：5 pages, 5 pages, 2021 Asilomar Conference on Signals, Systems, and Computers</p>
  <p><b>关键词</b>：reconstruction dnn method called recurrent stacked back projection, recurrent convolutional lstm network, traditional direct reconstruction methods, based iterative reconstruction, recurrent processing exploits</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sparse-view CT reconstruction is important in a wide range of applications
due to limitations on cost, acquisition time, or dosage. However, traditional
direct reconstruction methods such as filtered back-projection (FBP) lead to
low-quality reconstructions in the sub-Nyquist regime. In contrast, deep neural
networks (DNNs) can produce high-quality reconstructions from sparse and noisy
data, e.g. through post-processing of FBP reconstructions, as can model-based
iterative reconstruction (MBIR), albeit at a higher computational cost.
In this paper, we introduce a direct-reconstruction DNN method called
Recurrent Stacked Back Projection (RSBP) that uses sequentially-acquired
backprojections of individual views as input to a recurrent convolutional LSTM
network. The SBP structure maintains all information in the sinogram, while the
recurrent processing exploits the correlations between adjacent views and
produces an updated reconstruction after each new view. We train our network on
simulated data and test on both simulated and real data and demonstrate that
RSBP outperforms both DNN post-processing of FBP images and basic MBIR, with a
lower computational cost than MBIR.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：Robust Weakly Supervised Learning for COVID-19 Recognition Using  Multi-Center CT Images</b></summary>
  <p><b>编号</b>：[219]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04984</p>
  <p><b>作者</b>：Qinghao Ye,  Yuan Gao,  Weiping Ding,  Zhangming Niu,  Chengjia Wang,  Yinghui Jiang,  Minhao Wang,  Evandro Fei Fang,  Wade Menpes-Smith,  Jun Xia,  Guang Yang</p>
  <p><b>备注</b>：32 pages, 8 figures, Applied Soft Computing</p>
  <p><b>关键词</b>：19 ct scan recognition model namely coronavirus information fusion, infectious disease named coronavirus disease 2019, new robust weakly supervised learning paradigm, severe acute respiratory syndrome coronavirus 2, automated 3d ct scan recognition tool</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The world is currently experiencing an ongoing pandemic of an infectious
disease named coronavirus disease 2019 (i.e., COVID-19), which is caused by the
severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). Computed
Tomography (CT) plays an important role in assessing the severity of the
infection and can also be used to identify those symptomatic and asymptomatic
COVID-19 carriers. With a surge of the cumulative number of COVID-19 patients,
radiologists are increasingly stressed to examine the CT scans manually.
Therefore, an automated 3D CT scan recognition tool is highly in demand since
the manual analysis is time-consuming for radiologists and their fatigue can
cause possible misjudgment. However, due to various technical specifications of
CT scanners located in different hospitals, the appearance of CT images can be
significantly different leading to the failure of many automated image
recognition approaches. The multi-domain shift problem for the multi-center and
multi-scanner studies is therefore nontrivial that is also crucial for a
dependable recognition and critical for reproducible and objective diagnosis
and prognosis. In this paper, we proposed a COVID-19 CT scan recognition model
namely coronavirus information fusion and diagnosis network (CIFD-Net) that can
efficiently handle the multi-domain shift problem via a new robust weakly
supervised learning paradigm. Our model can resolve the problem of different
appearance in CT scan images reliably and efficiently while attaining higher
accuracy compared to other state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：Semi-Supervised Medical Image Segmentation via Cross Teaching between  CNN and Transformer</b></summary>
  <p><b>编号</b>：[227]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04894</p>
  <p><b>作者</b>：Xiangde Luo,  Minhao Hu,  Tao Song,  Guotai Wang,  Shaoting Zhang</p>
  <p><b>备注</b>：A technical report about SSL4MIS:this https URL</p>
  <p><b>关键词</b>：method outperforms eight existing semi, fully supervised medical image segmentation, supervised medical image segmentation, supervised medical image segmentation, simple yet efficient framework</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, deep learning with Convolutional Neural Networks (CNNs) and
Transformers has shown encouraging results in fully supervised medical image
segmentation. However, it is still challenging for them to achieve good
performance with limited annotations for training. In this work, we present a
very simple yet efficient framework for semi-supervised medical image
segmentation by introducing the cross teaching between CNN and Transformer.
Specifically, we simplify the classical deep co-training from consistency
regularization to cross teaching, where the prediction of a network is used as
the pseudo label to supervise the other network directly end-to-end.
Considering the difference in learning paradigm between CNN and Transformer, we
introduce the Cross Teaching between CNN and Transformer rather than just using
CNNs. Experiments on a public benchmark show that our method outperforms eight
existing semi-supervised learning methods just with a simpler framework.
Notably, this work may be the first attempt to combine CNN and transformer for
semi-supervised medical image segmentation and achieve promising results on a
public benchmark. The code will be released at:
this https URL.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：Evaluating saliency methods on artificial data with different background  types</b></summary>
  <p><b>编号</b>：[229]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04882</p>
  <p><b>作者</b>：Céline Budding,  Fabian Eitel,  Kerstin Ritter,  Stefan Haufe</p>
  <p><b>备注</b>：6 pages, 2 figures. Presented at Medical Imaging meets NeurIPS 2021 (poster presentation)</p>
  <p><b>关键词</b>：known ground truth map, 2d brain mri slices, evaluated two data sets, generate artificial data, explainable artificial intelligence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Over the last years, many 'explainable artificial intelligence' (xAI)
approaches have been developed, but these have not always been objectively
evaluated. To evaluate the quality of heatmaps generated by various saliency
methods, we developed a framework to generate artificial data with synthetic
lesions and a known ground truth map. Using this framework, we evaluated two
data sets with different backgrounds, Perlin noise and 2D brain MRI slices, and
found that the heatmaps vary strongly between saliency methods and backgrounds.
We strongly encourage further evaluation of saliency maps and xAI methods using
this framework before applying these in clinical or other safety-critical
settings.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：3D Medical Point Transformer: Introducing Convolution to Attention  Networks for Medical Point Cloud Analysis</b></summary>
  <p><b>编号</b>：[230]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04863</p>
  <p><b>作者</b>：Jianhui Yu,  Chaoyi Zhang,  Heng Wang,  Dingxin Zhang,  Yang Song,  Tiange Xiang,  Dongnan Liu,  Weidong Cai</p>
  <p><b>备注</b>：technical report</p>
  <p><b>关键词</b>：general 3d point cloud benchmarks, namely 3d medical point transformer, medical data may lead, global content feature interactions, learn accurate local geometry</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>General point clouds have been increasingly investigated for different tasks,
and recently Transformer-based networks are proposed for point cloud analysis.
However, there are barely related works for medical point clouds, which are
important for disease detection and treatment. In this work, we propose an
attention-based model specifically for medical point clouds, namely 3D medical
point Transformer (3DMedPT), to examine the complex biological structures. By
augmenting contextual information and summarizing local responses at query, our
attention module can capture both local context and global content feature
interactions. However, the insufficient training samples of medical data may
lead to poor feature learning, so we apply position embeddings to learn
accurate local geometry and Multi-Graph Reasoning (MGR) to examine global
knowledge propagation over channel graphs to enrich feature representations.
Experiments conducted on IntrA dataset proves the superiority of 3DMedPT, where
we achieve the best classification and segmentation results. Furthermore, the
promising generalization ability of our method is validated on general 3D point
cloud benchmarks: ModelNet40 and ShapeNetPart. Code will be released soon.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：One-dimensional Deep Low-rank and Sparse Network for Accelerated MRI</b></summary>
  <p><b>编号</b>：[237]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04721</p>
  <p><b>作者</b>：Zi Wang,  Chen Qian,  Di Guo,  Hongwei Sun,  Rushuai Li,  Bo Zhao,  Xiaobo Qu</p>
  <p><b>备注</b>：16 pages</p>
  <p><b>关键词</b>：perform 2d convolution since many magnetic resonance images, odls also shows nice robustness, art deep learning reconstructions adopt, accelerated magnetic resonance imaging, powerful convolutional neural network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning has shown astonishing performance in accelerated magnetic
resonance imaging (MRI). Most state-of-the-art deep learning reconstructions
adopt the powerful convolutional neural network and perform 2D convolution
since many magnetic resonance images or their corresponding k-space are in 2D.
In this work, we present a new approach that explores the 1D convolution,
making the deep network much easier to be trained and generalized. We further
integrate the 1D convolution into the proposed deep network, named as
One-dimensional Deep Low-rank and Sparse network (ODLS), which unrolls the
iteration procedure of a low-rank and sparse reconstruction model. Extensive
results on in vivo knee and brain datasets demonstrate that, the proposed ODLS
is very suitable for the case of limited training subjects and provides
improved reconstruction performance than state-of-the-art methods both visually
and quantitatively. Additionally, ODLS also shows nice robustness to different
undersampling scenarios and some mismatches between the training and test data.
In summary, our work demonstrates that the 1D deep learning scheme is
memory-efficient and robust in fast MRI.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：Extending nn-UNet for brain tumor segmentation</b></summary>
  <p><b>编号</b>：[239]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04653</p>
  <p><b>作者</b>：Huan Minh Luu,  Sung-Hong Park</p>
  <p><b>备注</b>：12 pages, 4 figures, BraTS competition paper</p>
  <p><b>关键词</b>：brain tumor segmentation challenge, brain tumor segmentation, utilizing axial attention, fold cross validation, develop automatic algorithms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Brain tumor segmentation is essential for the diagnosis and prognosis of
patients with gliomas. The brain tumor segmentation challenge has continued to
provide a great source of data to develop automatic algorithms to perform the
task. This paper describes our contribution to the 2021 competition. We
developed our methods based on nn-UNet, the winning entry of last year
competition. We experimented with several modifications, including using a
larger network, replacing batch normalization with group normalization, and
utilizing axial attention in the decoder. Internal 5-fold cross validation as
well as online evaluation from the organizers showed the effectiveness of our
approach, with minor improvement in quantitative metrics when compared to the
baseline. The proposed models won first place in the final ranking on unseen
test data. The codes, pretrained weights, and docker image for the winning
submission are publicly available at
this https URL</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：Multiscale Softmax Cross Entropy for Fovea Localization on Color Fundus  Photography</b></summary>
  <p><b>编号</b>：[245]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04499</p>
  <p><b>作者</b>：Yuli Wu,  Peter Walter,  Dorit Merhof</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：proposed multiscale softmax cross entropy yields better performance, cross entropy loss function, ophthalmic medical image analysis, mean squared error loss, color fundus photography images</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Fovea localization is one of the most popular tasks in ophthalmic medical
image analysis, where the coordinates of the center point of the macula lutea,
i.e. fovea centralis, should be calculated based on color fundus images. In
this work, we treat the localization problem as a classification task, where
the coordinates of the x- and y-axis are considered as the target classes.
Moreover, the combination of the softmax activation function and the cross
entropy loss function is modified to its multiscale variation to encourage the
predicted coordinates to be located closely to the ground-truths. Based on
color fundus photography images, we empirically show that the proposed
multiscale softmax cross entropy yields better performance than the vanilla
version and than the mean squared error loss with sigmoid activation, which
provides a novel approach for coordinate regression.</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：Dynamic multi feature-class Gaussian process models</b></summary>
  <p><b>编号</b>：[246]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04495</p>
  <p><b>作者</b>：Jean-Rassaire Fouefack,  Bhushan Borotikar,  Marcel Lüthi,  Tania S. Douglas,  Valérie Burdin,  Tinashe E.M. Mutsvangwa</p>
  <p><b>备注</b>：16</p>
  <p><b>关键词</b>：method using controlled synthetic data, adding additional pose feature variability, image analysis tasks using dmfc, image intensity profiles representative, model performance results suggest</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In model-based medical image analysis, three features of interest are the
shape of structures of interest, their relative pose, and image intensity
profiles representative of some physical property. Often, these are modelled
separately through statistical models by decomposing the object's features into
a set of basis functions through principal geodesic analysis or principal
component analysis. This study presents a statistical modelling method for
automatic learning of shape, pose and intensity features in medical images
which we call the Dynamic multi feature-class Gaussian process models
(DMFC-GPM). A DMFC-GPM is a Gaussian process (GP)-based model with a shared
latent space that encodes linear and non-linear variation. Our method is
defined in a continuous domain with a principled way to represent shape, pose
and intensity feature classes in a linear space, based on deformation fields. A
deformation field-based metric is adapted in the method for modelling shape and
intensity feature variation as well as for comparing rigid transformations
(pose). Moreover, DMFC-GPMs inherit properties intrinsic to GPs including
marginalisation and regression. Furthermore, they allow for adding additional
pose feature variability on top of those obtained from the image acquisition
process; what we term as permutation modelling. For image analysis tasks using
DMFC-GPMs, we adapt Metropolis-Hastings algorithms making the prediction of
features fully probabilistic. We validate the method using controlled synthetic
data and we perform experiments on bone structures from CT images of the
shoulder to illustrate the efficacy of the model for pose and shape feature
prediction. The model performance results suggest that this new modelling
paradigm is robust, accurate, accessible, and has potential applications
including the management of musculoskeletal disorders and clinical decision
making</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：Binary Change Guided Hyperspectral Multiclass Change Detection</b></summary>
  <p><b>编号</b>：[247]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04493</p>
  <p><b>作者</b>：Meiqi Hu,  Chen Wu,  Bo Du,  Liangpei Zhang</p>
  <p><b>备注</b>：14 pages,17 figures</p>
  <p><b>关键词</b>：unsupervised binary change guided hyperspectral multiclass change detection network, cannot provide fine change classes information, mature binary change detection approaches, innovative binary change detection rule, groundbreaking temporal correlation constraint directed</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Characterized by tremendous spectral information, hyperspectral image is able
to detect subtle changes and discriminate various change classes for change
detection. The recent research works dominated by hyperspectral binary change
detection, however, cannot provide fine change classes information. And most
methods incorporating spectral unmixing for hyperspectral multiclass change
detection (HMCD), yet suffer from the neglection of temporal correlation and
error accumulation. In this study, we proposed an unsupervised Binary Change
Guided hyperspectral multiclass change detection Network (BCG-Net) for HMCD,
which aims at boosting the multiclass change detection result and unmixing
result with the mature binary change detection approaches. In BCG-Net, a novel
partial-siamese united-unmixing module is designed for multi-temporal spectral
unmixing, and a groundbreaking temporal correlation constraint directed by the
pseudo-labels of binary change detection result is developed to guide the
unmixing process from the perspective of change detection, encouraging the
abundance of the unchanged pixels more coherent and that of the changed pixels
more accurate. Moreover, an innovative binary change detection rule is put
forward to deal with the problem that traditional rule is susceptible to
numerical values. The iterative optimization of the spectral unmixing process
and the change detection process is proposed to eliminate the accumulated
errors and bias from unmixing result to change detection result. The
experimental results demonstrate that our proposed BCG-Net could achieve
comparative or even outstanding performance of multiclass change detection
among the state-of-the-art approaches and gain better spectral unmixing results
at the same time.</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：Revisiting Global Statistics Aggregation for Improving Image Restoration</b></summary>
  <p><b>编号</b>：[248]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04491</p>
  <p><b>作者</b>：Xiaojie Chu,  Liangyu Chen,  Chengpeng Chen,  Xin Lu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：testing phase respectively may distribute, aggregated along entire spatial dimensions, previous best result 0, time local statistics converter, global spatial statistics</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Global spatial statistics, which are aggregated along entire spatial
dimensions, are widely used in top-performance image restorers. For example,
mean, variance in Instance Normalization (IN) which is adopted by HINet, and
global average pooling (i.e. mean) in Squeeze and Excitation (SE) which is
applied to MPRNet. This paper first shows that statistics aggregated on the
patches-based/entire-image-based feature in the training/testing phase
respectively may distribute very differently and lead to performance
degradation in image restorers. It has been widely overlooked by previous
works. To solve this issue, we propose a simple approach, Test-time Local
Statistics Converter (TLSC), that replaces the region of statistics aggregation
operation from global to local, only in the test time. Without retraining or
finetuning, our approach significantly improves the image restorer's
performance. In particular, by extending SE with TLSC to the state-of-the-art
models, MPRNet boost by 0.65 dB in PSNR on GoPro dataset, achieves 33.31 dB,
exceeds the previous best result 0.6 dB. In addition, we simply apply TLSC to
the high-level vision task, i.e. semantic segmentation, and achieves
competitive results. Extensive quantity and quality experiments are conducted
to demonstrate TLSC solves the issue with marginal costs while significant
gain. The code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：A novel multi-view deep learning approach for BI-RADS and density  assessment of mammograms</b></summary>
  <p><b>编号</b>：[249]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04490</p>
  <p><b>作者</b>：Huyen T. X. Nguyen,  Sam B. Tran,  Dung B. Nguyen,  Hieu H. Pham,  Ha Q. Nguyen</p>
  <p><b>备注</b>：Under review by IEEE EMBC</p>
  <p><b>关键词</b>：proposed approach first deploys deep convolutional networks, light gradient boosting machine, developing breast cancer based, overall breast exam classification, public dataset digital database</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Advanced deep learning (DL) algorithms may predict the patient's risk of
developing breast cancer based on the Breast Imaging Reporting and Data System
(BI-RADS) and density standards. Recent studies have suggested that the
combination of multi-view analysis improved the overall breast exam
classification. In this paper, we propose a novel multi-view DL approach for
BI-RADS and density assessment of mammograms. The proposed approach first
deploys deep convolutional networks for feature extraction on each view
separately. The extracted features are then stacked and fed into a Light
Gradient Boosting Machine (LightGBM) classifier to predict BI-RADS and density
scores. We conduct extensive experiments on both the internal mammography
dataset and the public dataset Digital Database for Screening Mammography
(DDSM). The experimental results demonstrate that the proposed approach
outperforms the single-view classification approach on two benchmark datasets
by huge margins (5% on the internal dataset and 10% on the DDSM dataset). These
results highlight the vital role of combining multi-view information to improve
the performance of breast cancer risk prediction.</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：Learn2Reg: comprehensive multi-task medical image registration  challenge, dataset and evaluation in the era of deep learning</b></summary>
  <p><b>编号</b>：[250]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04489</p>
  <p><b>作者</b>：Alessa Hering,  Lasse Hansen,  Tony C. W. Mok,  Albert C. S. Chung,  Hanna Siebert,  Stephanie Häger,  Annkristin Lange,  Sven Kuckertz,  Stefan Heldmann,  Wei Shao,  Sulaiman Vesal,  Mirabela Rusu,  Geoffrey Sonn,  Théo Estienne,  Maria Vakalopoulou,  Luyi Han,  Yunzhi Huang,  Mikael Brudfors,  Yaël Balbastre,  Samuel Joutard,  Marc Modat,  Gal Lifshitz,  Dan Raviv,  Jinxin Lv,  Qiang Li,  Vincent Jaouen,  Dimitris Visvikis,  Constance Fourcade,  Mathieu Rubeaux,  Wentao Pan,  Zhe Xu,  Bailiang Jian,  Francesca De Benetti,  Marek Wodzinski,  Niklas Gunnarsson,  Huaqi Qiu,  Zeju Li,  Christoph Großbröhmer,  Andrew Hoopes,  Ingerid Reinertsen,  Yiming Xiao,  Bennett Landman,  Yuankai Huo,  Keelin Murphy,  Bram van Ginneken,  Adrian Dalca,  Mattias P. Heinrich</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：prevents fair benchmarks across competing approaches, comprehensively compared medical image registration approaches, primarily deep learning based approaches, open exiting new research directions, ideally suited remains open</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To date few studies have comprehensively compared medical image registration
approaches on a wide-range of complementary clinically relevant tasks. This
limits the adoption of advances in research into practice and prevents fair
benchmarks across competing approaches. Many newer learning-based methods have
been explored within the last five years, but the question which optimisation,
architectural or metric strategy is ideally suited remains open. Learn2Reg
covers a wide range of anatomies: brain, abdomen and thorax, modalities:
ultrasound, CT, MRI, populations: intra- and inter-patient and levels of
supervision. We established a lower entry barrier for training and validation
of 3D registration, which helped us compile results of over 65 individual
method submissions from more than 20 unique teams. Our complementary set of
metrics, including robustness, accuracy, plausibility and speed enables unique
insight into the current-state-of-the-art of medical image registration.
Further analyses into transferability, bias and importance of supervision
question the superiority of primarily deep learning based approaches and open
exiting new research directions into hybrid methods that leverage
GPU-accelerated conventional optimisation.</p>
  </details>
</details>
<h1>自然语言处理</h1>
<details>
  <summary>1. <b>标题：PTR: A Benchmark for Part-based Conceptual, Relational, and Physical  Reasoning</b></summary>
  <p><b>编号</b>：[7]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05136</p>
  <p><b>作者</b>：Yining Hong,  Li Yi,  Joshua B. Tenenbaum,  Antonio Torralba,  Chuang Gan</p>
  <p><b>备注</b>：NeurIPS 2021. Project page: this http URL</p>
  <p><b>关键词</b>：ptr contains around 70k rgbd synthetic images, part level annotations regarding semantic instance segmentation, scale diagnostic visual reasoning dataset named ptr, existing visual reasoning benchmarks mostly focus, still make many surprising mistakes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A critical aspect of human visual perception is the ability to parse visual
scenes into individual objects and further into object parts, forming
part-whole hierarchies. Such composite structures could induce a rich set of
semantic concepts and relations, thus playing an important role in the
interpretation and organization of visual signals as well as for the
generalization of visual perception and reasoning. However, existing visual
reasoning benchmarks mostly focus on objects rather than parts. Visual
reasoning based on the full part-whole hierarchy is much more challenging than
object-centric reasoning due to finer-grained concepts, richer geometry
relations, and more complex physics. Therefore, to better serve for part-based
conceptual, relational and physical reasoning, we introduce a new large-scale
diagnostic visual reasoning dataset named PTR. PTR contains around 70k RGBD
synthetic images with ground truth object and part level annotations regarding
semantic instance segmentation, color attributes, spatial and geometric
relationships, and certain physical properties such as stability. These images
are paired with 700k machine-generated questions covering various types of
reasoning types, making them a good testbed for visual reasoning models. We
examine several state-of-the-art visual reasoning models on this dataset and
observe that they still make many surprising mistakes in situations where
humans can easily infer the correct answer. We believe this dataset will open
up new opportunities for part-based reasoning.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Transferring BERT-like Transformers' Knowledge for Authorship  Verification</b></summary>
  <p><b>编号</b>：[15]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05125</p>
  <p><b>作者</b>：Andrei Manolache,  Florin Brad,  Elena Burceanu,  Antonio Barbalau,  Radu Ionescu,  Marius Popescu</p>
  <p><b>备注</b>：16 pages, 3 figures</p>
  <p><b>关键词</b>：natural language processing tasks, text spans several decades, impressive performance gains across, author writing style characteristics, different input data distribution</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The task of identifying the author of a text spans several decades and was
tackled using linguistics, statistics, and, more recently, machine learning.
Inspired by the impressive performance gains across a broad range of natural
language processing tasks and by the recent availability of the PAN large-scale
authorship dataset, we first study the effectiveness of several BERT-like
transformers for the task of authorship verification. Such models prove to
achieve very high scores consistently. Next, we empirically show that they
focus on topical clues rather than on author writing style characteristics,
taking advantage of existing biases in the dataset. To address this problem, we
provide new splits for PAN-2020, where training and test data are sampled from
disjoint topics or authors. Finally, we introduce DarkReddit, a dataset with a
different input data distribution. We further use it to analyze the domain
generalization performance of models in a low-data regime and how performance
varies when using the proposed PAN-2020 splits for fine-tuning. We show that
those splits can enhance the models' capability to transfer knowledge over a
new, significantly different dataset.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Opinion Extraction as A Structured Sentiment Analysis using Transformers</b></summary>
  <p><b>编号</b>：[31]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05056</p>
  <p><b>作者</b>：Yucheng Liu,  Tian Zhu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：require different input data, extract multiple opinion tuples, performed different experiments, structured sentiment analysis, named entity recognition</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Relationship extraction and named entity recognition have always been
considered as two distinct tasks that require different input data, labels, and
models. However, both are essential for structured sentiment analysis. We
believe that both tasks can be combined into a single stacked model with the
same input data. We performed different experiments to find the best model to
extract multiple opinion tuples from a single sentence. The opinion tuples will
consist of holders, targets, and expressions. With the opinion tuples, we will
be able to extract the relationship we need.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Few-Shot NLU with Vector Projection Distance and Abstract Triangular CRF</b></summary>
  <p><b>编号</b>：[46]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04999</p>
  <p><b>作者</b>：Su Zhu,  Lu Chen,  Ruisheng Cao,  Zhi Chen,  Qingliang Miao,  Kai Yu</p>
  <p><b>备注</b>：Accepted by NLPCC 2021</p>
  <p><b>关键词</b>：abstract triangular conditional random field, vector projection distance exploits projections, abstract triangular crf learns domain, significantly surpass strong baselines, arbitrary target domain directly</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Data sparsity problem is a key challenge of Natural Language Understanding
(NLU), especially for a new target domain. By training an NLU model in source
domains and applying the model to an arbitrary target domain directly (even
without fine-tuning), few-shot NLU becomes crucial to mitigate the data
scarcity issue. In this paper, we propose to improve prototypical networks with
vector projection distance and abstract triangular Conditional Random Field
(CRF) for the few-shot NLU. The vector projection distance exploits projections
of contextual word embeddings on label vectors as word-label similarities,
which is equivalent to a normalized linear model. The abstract triangular CRF
learns domain-agnostic label transitions for joint intent classification and
slot filling tasks. Extensive experiments demonstrate that our proposed methods
can significantly surpass strong baselines. Specifically, our approach can
achieve a new state-of-the-art on two few-shot NLU benchmarks (Few-Joint and
SNIPS) in Chinese and English without fine-tuning on target domains.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：How Universal is Genre in Universal Dependencies?</b></summary>
  <p><b>编号</b>：[52]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04971</p>
  <p><b>作者</b>：Max Müller-Eberstein,  Rob van der Goot,  Barbara Plank</p>
  <p><b>备注</b>：Accepted at SyntaxFest 2021</p>
  <p><b>关键词</b>：specificity spread across 114 languages, level genre using weak supervision, prior work using ud genre metadata, mono -/ bilingual setups, proposed methods recover instance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This work provides the first in-depth analysis of genre in Universal
Dependencies (UD). In contrast to prior work on genre identification which uses
small sets of well-defined labels in mono-/bilingual setups, UD contains 18
genres with varying degrees of specificity spread across 114 languages. As most
treebanks are labeled with multiple genres while lacking annotations about
which instances belong to which genre, we propose four methods for predicting
instance-level genre using weak supervision from treebank metadata. The
proposed methods recover instance-level genre better than competitive baselines
as measured on a subset of UD with labeled instances and adhere better to the
global expected distribution. Our analysis sheds light on prior work using UD
genre metadata for treebank selection, finding that metadata alone are a noisy
signal and must be disentangled within treebanks before it can be universally
applied.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：A Simple but Effective Bidirectional Extraction Framework for Relational  Triple Extraction</b></summary>
  <p><b>编号</b>：[62]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04940</p>
  <p><b>作者</b>：Feiliang Ren,  Longhui Zhang,  Xiaofeng Zhao,  Shujuan Yin,  Shilei Liu,  Bochao Li</p>
  <p><b>备注</b>：WSDM2022</p>
  <p><b>关键词</b>：tagging based relational triple extraction methods, attracting growing research attention recently, bidirectional extraction framework based method, convergence rate inconsistency issue, proposed bidirectional extraction framework</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Tagging based relational triple extraction methods are attracting growing
research attention recently. However, most of these methods take a
unidirectional extraction framework that first extracts all subjects and then
extracts objects and relations simultaneously based on the subjects extracted.
This framework has an obvious deficiency that it is too sensitive to the
extraction results of subjects. To overcome this deficiency, we propose a
bidirectional extraction framework based method that extracts triples based on
the entity pairs extracted from two complementary directions. Concretely, we
first extract all possible subject-object pairs from two paralleled directions.
These two extraction directions are connected by a shared encoder component,
thus the extraction features from one direction can flow to another direction
and vice versa. By this way, the extractions of two directions can boost and
complement each other. Next, we assign all possible relations for each entity
pair by a biaffine model. During training, we observe that the share structure
will lead to a convergence rate inconsistency issue which is harmful to
performance. So we propose a share-aware learning mechanism to address it. We
evaluate the proposed model on multiple benchmark datasets. Extensive
experimental results show that the proposed model is very effective and it
achieves state-of-the-art results on all of these datasets. Moreover,
experiments show that both the proposed bidirectional extraction framework and
the share-aware learning mechanism have good adaptability and can be used to
improve the performance of other tagging based methods. The source code of our
work is available at: this https URL.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Self-Supervised Image-to-Text and Text-to-Image Synthesis</b></summary>
  <p><b>编号</b>：[65]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04928</p>
  <p><b>作者</b>：Anindya Sundar Das,  Sriparna Saha</p>
  <p><b>备注</b>：ICONIP 2021 : The 28th International Conference on Neural Information Processing</p>
  <p><b>关键词</b>：supervised deep learning based approach towards learning, maximum mean discrepancy based generative networks, first obtain dense vector representations, level utilizing lstm based text, supervised generative deep architectures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A comprehensive understanding of vision and language and their interrelation
are crucial to realize the underlying similarities and differences between
these modalities and to learn more generalized, meaningful representations. In
recent years, most of the works related to Text-to-Image synthesis and
Image-to-Text generation, focused on supervised generative deep architectures
to solve the problems, where very little interest was placed on learning the
similarities between the embedding spaces across modalities. In this paper, we
propose a novel self-supervised deep learning based approach towards learning
the cross-modal embedding spaces; for both image to text and text to image
generations. In our approach, we first obtain dense vector representations of
images using StackGAN-based autoencoder model and also dense vector
representations on sentence-level utilizing LSTM based text-autoencoder; then
we study the mapping from embedding space of one modality to embedding space of
the other modality utilizing GAN and maximum mean discrepancy based generative
networks. We, also demonstrate that our model learns to generate textual
description from image data as well as images from textual data both
qualitatively and quantitatively.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：A Bilingual, OpenWorld Video Text Dataset and End-to-end Video Text  Spotter with Transformer</b></summary>
  <p><b>编号</b>：[79]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04888</p>
  <p><b>作者</b>：Weijia Wu,  Yuanqiang Cai,  Debing Zhang,  Sibo Wang,  Zhuang Li,  Jiahong Li,  Yejun Tang,  Hong Zhou</p>
  <p><b>备注</b>：20 pages, 6 figures</p>
  <p><b>关键词</b>：existing video text spotting benchmarks focus, open world video text benchmark dataset, end video text spotting framework, bovtext provides bilingual text annotation, abundant text types annotation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Most existing video text spotting benchmarks focus on evaluating a single
language and scenario with limited data. In this work, we introduce a
large-scale, Bilingual, Open World Video text benchmark dataset(BOVText). There
are four features for BOVText. Firstly, we provide 2,000+ videos with more than
1,750,000+ frames, 25 times larger than the existing largest dataset with
incidental text in videos. Secondly, our dataset covers 30+ open categories
with a wide selection of various scenarios, e.g., Life Vlog, Driving, Movie,
etc. Thirdly, abundant text types annotation (i.e., title, caption or scene
text) are provided for the different representational meanings in video.
Fourthly, the BOVText provides bilingual text annotation to promote multiple
cultures live and communication. Besides, we propose an end-to-end video text
spotting framework with Transformer, termed TransVTSpotter, which solves the
multi-orient text spotting in video with a simple, but efficient
attention-based query-key mechanism. It applies object features from the
previous frame as a tracking query for the current frame and introduces a
rotation angle prediction to fit the multiorient text instance. On
ICDAR2015(video), TransVTSpotter achieves the state-of-the-art performance with
44.1% MOTA, 9 fps. The dataset and code of TransVTSpotter can be found at
github:com=weijiawu=BOVText and github:com=weijiawu=TransVTSpotter,
respectively.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Semantic Search as Extractive Paraphrase Span Detection</b></summary>
  <p><b>编号</b>：[80]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04886</p>
  <p><b>作者</b>：Jenna Kanerva,  Hanna Kitti,  Li-Hsin Chang,  Teemu Vahtola,  Mathias Creutz,  Filip Ginter</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：paraphrase span detection model outperforms two strong retrieval baselines, 000 manually extracted finnish paraphrase pairs including, manually annotated paraphrase resources, creating artificial paraphrase data, span detection model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we approach the problem of semantic search by framing the
search task as paraphrase span detection, i.e. given a segment of text as a
query phrase, the task is to identify its paraphrase in a given document, the
same modelling setup as typically used in extractive question answering. On the
Turku Paraphrase Corpus of 100,000 manually extracted Finnish paraphrase pairs
including their original document context, we find that our paraphrase span
detection model outperforms two strong retrieval baselines (lexical similarity
and BERT sentence embeddings) by 31.9pp and 22.4pp respectively in terms of
exact match, and by 22.3pp and 12.9pp in terms of token-level F-score. This
demonstrates a strong advantage of modelling the task in terms of span
retrieval, rather than sentence similarity. Additionally, we introduce a method
for creating artificial paraphrase data through back-translation, suitable for
languages where manually annotated paraphrase resources for training the span
detection model are not available.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Nice perfume. How long did you marinate in it? Multimodal Sarcasm  Explanation</b></summary>
  <p><b>编号</b>：[81]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04873</p>
  <p><b>作者</b>：Poorav Desai,  Tanmoy Chakraborty,  Md Shad Akhtar</p>
  <p><b>备注</b>：Accepted for publication in AAAI-2022</p>
  <p><b>关键词</b>：fair agreement among 25 evaluators, empirical results demonstrate convincing results, novel problem -- multimodal sarcasm explanation, although recent approaches deal, also conduct human evaluation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sarcasm is a pervading linguistic phenomenon and highly challenging to
explain due to its subjectivity, lack of context and deeply-felt opinion. In
the multimodal setup, sarcasm is conveyed through the incongruity between the
text and visual entities. Although recent approaches deal with sarcasm as a
classification problem, it is unclear why an online post is identified as
sarcastic. Without proper explanation, end users may not be able to perceive
the underlying sense of irony. In this paper, we propose a novel problem --
Multimodal Sarcasm Explanation (MuSE) -- given a multimodal sarcastic post
containing an image and a caption, we aim to generate a natural language
explanation to reveal the intended sarcasm. To this end, we develop MORE, a new
dataset with explanation of 3510 sarcastic multimodal posts. Each explanation
is a natural language (English) sentence describing the hidden irony. We
benchmark MORE by employing a multimodal Transformer-based architecture. It
incorporates a cross-modal attention in the Transformer's encoder which attends
to the distinguishing features between the two modalities. Subsequently, a
BART-based auto-regressive decoder is used as the generator. Empirical results
demonstrate convincing results over various baselines (adopted for MuSE) across
five evaluation metrics. We also conduct human evaluation on predictions and
obtain Fleiss' Kappa score of 0.4 as a fair agreement among 25 evaluators.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：KGE-CL: Contrastive Learning of Knowledge Graph Embeddings</b></summary>
  <p><b>编号</b>：[82]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04871</p>
  <p><b>作者</b>：Wentao Xu,  Zhiping Luo,  Weiqing Liu,  Jiang Bian,  Jian Yin,  Tie-Yan Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：simple yet efficient contrastive learning framework, previous knowledge graph embedding methods ignore, three standard knowledge graph benchmarks, benefit various downstream applications, knowledge graph embedding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Learning the embeddings of knowledge graphs is vital in artificial
intelligence, and can benefit various downstream applications, such as
recommendation and question answering. In recent years, many research efforts
have been proposed for knowledge graph embedding. However, most previous
knowledge graph embedding methods ignore the semantic similarity between the
related entities and entity-relation couples in different triples since they
separately optimize each triple with the scoring function. To address this
problem, we propose a simple yet efficient contrastive learning framework for
knowledge graph embeddings, which can shorten the semantic distance of the
related entities and entity-relation couples in different triples and thus
improve the expressiveness of knowledge graph embeddings. We evaluate our
proposed method on three standard knowledge graph benchmarks. It is noteworthy
that our method can yield some new state-of-the-art results, achieving 51.2%
MRR, 46.8% Hits@1 on the WN18RR dataset, and 59.1% MRR, 51.8% Hits@1 on the
YAGO3-10 dataset.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Multimodal Fake News Detection</b></summary>
  <p><b>编号</b>：[93]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04831</p>
  <p><b>作者</b>：Santiago Alonso-Bartolome,  Isabel Segura-Bedmar</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：false connection strongly benefit, image data significantly improves, classify false content focus, typically classify news either, using images also improves</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Over the last years, there has been an unprecedented proliferation of fake
news. As a consequence, we are more susceptible to the pernicious impact that
misinformation and disinformation spreading can have in different segments of
our society. Thus, the development of tools for automatic detection of fake
news plays and important role in the prevention of its negative effects. Most
attempts to detect and classify false content focus only on using textual
information. Multimodal approaches are less frequent and they typically
classify news either as true or fake. In this work, we perform a fine-grained
classification of fake news on the Fakeddit dataset, using both unimodal and
multimodal approaches. Our experiments show that the multimodal approach based
on a Convolutional Neural Network (CNN) architecture combining text and image
data achieves the best results, with an accuracy of 87%. Some fake news
categories such as Manipulated content, Satire or False connection strongly
benefit from the use of images. Using images also improves the results of the
other categories, but with less impact. Regarding the unimodal approaches using
only text, Bidirectional Encoder Representations from Transformers (BERT) is
the best model with an accuracy of 78%. Therefore, exploiting both text and
image data significantly improves the performance of fake news detection.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Combining Textual Features for the Detection of Hateful and Offensive  Language</b></summary>
  <p><b>编号</b>：[99]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04803</p>
  <p><b>作者</b>：Sherzod Hakimov,  Ralph Ewerth</p>
  <p><b>备注</b>：HASOC 2021, Forum for Information Retrieval Evaluation, 2021</p>
  <p><b>关键词</b>：critical challenge since many users, contextual word embeddings combined, combining different textual features, compared different variants, character level embeddings</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The detection of offensive, hateful and profane language has become a
critical challenge since many users in social networks are exposed to
cyberbullying activities on a daily basis. In this paper, we present an
analysis of combining different textual features for the detection of hateful
or offensive posts on Twitter. We provide a detailed experimental evaluation to
understand the impact of each building block in a neural network architecture.
The proposed architecture is evaluated on the English Subtask 1A: Identifying
Hate, offensive and profane content from the post datasets of HASOC-2021
dataset under the team name TIB-VA. We compared different variants of the
contextual word embeddings combined with the character level embeddings and the
encoding of collected hate terms.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Detecting Potentially Harmful and Protective Suicide-related Content on  Twitter: A Machine Learning Approach</b></summary>
  <p><b>编号</b>：[102]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04796</p>
  <p><b>作者</b>：Hannah Metzler,  Hubert Baginski,  Thomas Niederkrotenthaler,  David Garcia</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：deep learning models reach accuracy scores, two deep learning models achieved, classified six main content categories, machine learning models including, art deep learning models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Research shows that exposure to suicide-related news media content is
associated with suicide rates, with some content characteristics likely having
harmful and others potentially protective effects. Although good evidence
exists for a few selected characteristics, systematic large scale
investigations are missing in general, and in particular for social media data.
We apply machine learning methods to automatically label large quantities of
Twitter data. We developed a novel annotation scheme that classifies
suicide-related tweets into different message types and problem- vs.
solution-focused perspectives. We then trained a benchmark of machine learning
models including a majority classifier, an approach based on word frequency
(TF-IDF with a linear SVM) and two state-of-the-art deep learning models (BERT,
XLNet). The two deep learning models achieved the best performance in two
classification tasks: First, we classified six main content categories,
including personal stories about either suicidal ideation and attempts or
coping, calls for action intending to spread either problem awareness or
prevention-related information, reportings of suicide cases, and other
suicide-related and off-topic tweets. The deep learning models reach accuracy
scores above 73% on average across the six categories, and F1-scores in between
69% and 85% for all but the suicidal ideation and attempts category (55%).
Second, in separating postings referring to actual suicide from off-topic
tweets, they correctly labelled around 88% of tweets, with BERT achieving
F1-scores of 93% and 74% for the two categories. These classification
performances are comparable to the state-of-the-art on similar tasks. By making
data labeling more efficient, this work enables future large-scale
investigations on harmful and protective effects of various kinds of social
media content on suicide rates and on help-seeking behavior.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Towards Neural Functional Program Evaluation</b></summary>
  <p><b>编号</b>：[161]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04630</p>
  <p><b>作者</b>：Torsten Scholak,  Jonathan Pilault,  Joey Velez-Ginorio</p>
  <p><b>备注</b>：9 pages. Accepted at the AIPLANS workshop at NeurIPS 2021</p>
  <p><b>关键词</b>：neural functional program evaluation performs surprisingly well, simple functional programming languages, new program generation mechanism, exact program match scores, using pretrained t5 weights</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper explores the capabilities of current transformer-based language
models for program evaluation of simple functional programming languages. We
introduce a new program generation mechanism that allows control over syntactic
sugar for semantically equivalent programs. T5 experiments reveal that neural
functional program evaluation performs surprisingly well, achieving high 90%
exact program match scores for most in-distribution and out-of-distribution
tests. Using pretrained T5 weights has significant advantages over random
initialization. We present and evaluate on three datasets to study
generalization abilities that are specific to functional programs based on:
type, function composition, and reduction steps. Code and data are publicly
available at this https URL.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Refined Commonsense Knowledge from Large-Scale Web Contents</b></summary>
  <p><b>编号</b>：[178]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04596</p>
  <p><b>作者</b>：Tuan-Phong Nguyen,  Simon Razniewski,  Julien Romero,  Gerhard Weikum</p>
  <p><b>备注</b>：This is a substantial extension of the WWW paper (arXiv:2011.00905). arXiv admin note: substantial text overlap with arXiv:2011.00905</p>
  <p><b>关键词</b>：ascent ++ combines open information extraction, ascent ++ goes beyond spo triples, others compiled large csk collections, prior works like conceptnet, ascent ++ kb</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Commonsense knowledge (CSK) about concepts and their properties is useful for
AI applications. Prior works like ConceptNet, COMET and others compiled large
CSK collections, but are restricted in their expressiveness to
subject-predicate-object (SPO) triples with simple concepts for S and strings
for P and O. This paper presents a method, called ASCENT++, to automatically
build a large-scale knowledge base (KB) of CSK assertions, with refined
expressiveness and both better precision and recall than prior works. ASCENT++
goes beyond SPO triples by capturing composite concepts with subgroups and
aspects, and by refining assertions with semantic facets. The latter is
important to express the temporal and spatial validity of assertions and
further qualifiers. ASCENT++ combines open information extraction with
judicious cleaning and ranking by typicality and saliency scores. For high
coverage, our method taps into the large-scale crawl C4 with broad web
contents. The evaluation with human judgements shows the superior quality of
the ASCENT++ KB, and an extrinsic evaluation for QA-support tasks underlines
the benefits of ASCENT++. A web interface, data and code can be accessed at
this https URL.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Prompt-based Zero-shot Relation Classification with Semantic Knowledge  Augmentation</b></summary>
  <p><b>编号</b>：[202]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04539</p>
  <p><b>作者</b>：Jiaying Gong,  Hoda Eldardiry</p>
  <p><b>备注</b>：11 pages, 7 figures</p>
  <p><b>关键词</b>：construct weighted virtual label words, integrate semantic knowledge information learned, three public datasets show, level sentence translation rule, experimental results also demonstrate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recognizing unseen relations with no training instances is a challenging task
in the real world. In this paper, we propose a prompt-based model with semantic
knowledge augmentation (ZS-SKA) to recognize unseen relations under the
zero-shot setting. We generate augmented instances with unseen relations from
instances with seen relations following a new word-level sentence translation
rule. We design prompts based on an external knowledge graph to integrate
semantic knowledge information learned from seen relations. Instead of using
the actual label sets in the prompt template, we construct weighted virtual
label words. By generating the representations of both seen and unseen
relations with augmented instances and prompts through prototypical networks,
distance is calculated to predict unseen relations. Extensive experiments
conducted on three public datasets show that ZS-SKA outperforms
state-of-the-art methods under the zero-shot scenarios. Our experimental
results also demonstrate the effectiveness and robustness of ZS-SKA.</p>
  </details>
</details>
<h1>机器学习</h1>
<details>
  <summary>1. <b>标题：PTR: A Benchmark for Part-based Conceptual, Relational, and Physical  Reasoning</b></summary>
  <p><b>编号</b>：[7]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05136</p>
  <p><b>作者</b>：Yining Hong,  Li Yi,  Joshua B. Tenenbaum,  Antonio Torralba,  Chuang Gan</p>
  <p><b>备注</b>：NeurIPS 2021. Project page: this http URL</p>
  <p><b>关键词</b>：ptr contains around 70k rgbd synthetic images, part level annotations regarding semantic instance segmentation, scale diagnostic visual reasoning dataset named ptr, existing visual reasoning benchmarks mostly focus, still make many surprising mistakes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A critical aspect of human visual perception is the ability to parse visual
scenes into individual objects and further into object parts, forming
part-whole hierarchies. Such composite structures could induce a rich set of
semantic concepts and relations, thus playing an important role in the
interpretation and organization of visual signals as well as for the
generalization of visual perception and reasoning. However, existing visual
reasoning benchmarks mostly focus on objects rather than parts. Visual
reasoning based on the full part-whole hierarchy is much more challenging than
object-centric reasoning due to finer-grained concepts, richer geometry
relations, and more complex physics. Therefore, to better serve for part-based
conceptual, relational and physical reasoning, we introduce a new large-scale
diagnostic visual reasoning dataset named PTR. PTR contains around 70k RGBD
synthetic images with ground truth object and part level annotations regarding
semantic instance segmentation, color attributes, spatial and geometric
relationships, and certain physical properties such as stability. These images
are paired with 700k machine-generated questions covering various types of
reasoning types, making them a good testbed for visual reasoning models. We
examine several state-of-the-art visual reasoning models on this dataset and
observe that they still make many surprising mistakes in situations where
humans can easily infer the correct answer. We believe this dataset will open
up new opportunities for part-based reasoning.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures</b></summary>
  <p><b>编号</b>：[8]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05135</p>
  <p><b>作者</b>：Dan Hendrycks,  Andy Zou,  Mantas Mazeika,  Leonard Tang,  Dawn Song,  Jacob Steinhardt</p>
  <p><b>备注</b>：Code and models are available at this https URL</p>
  <p><b>关键词</b>：performance beyond standard test set accuracy, methods cannot achieve without sacrificing performance, regularization techniques often improve ood robustness, safe systems must consider measures, adversarial training improves adversarial robustness</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In real-world applications of machine learning, reliable and safe systems
must consider measures of performance beyond standard test set accuracy. These
other goals include out-of-distribution (OOD) robustness, prediction
consistency, resilience to adversaries, calibrated uncertainty estimates, and
the ability to detect anomalous inputs. However, improving performance towards
these goals is often a balancing act that today's methods cannot achieve
without sacrificing performance on other safety axes. For instance, adversarial
training improves adversarial robustness but sharply degrades other classifier
performance metrics. Similarly, strong data augmentation and regularization
techniques often improve OOD robustness but harm anomaly detection, raising the
question of whether a Pareto improvement on all existing safety measures is
possible. To meet this challenge, we design a new data augmentation strategy
utilizing the natural structural complexity of pictures such as fractals, which
outperforms numerous baselines, is near Pareto-optimal, and roundly improves
safety measures.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Neural Descriptor Fields: SE(3)-Equivariant Object Representations for  Manipulation</b></summary>
  <p><b>编号</b>：[16]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05124</p>
  <p><b>作者</b>：Anthony Simeonov,  Yilun Du,  Andrea Tagliasacchi,  Joshua B. Tenenbaum,  Alberto Rodriguez,  Pulkit Agrawal,  Vincent Sitzmann</p>
  <p><b>备注</b>：Website: this https URL First two authors contributed equally (order determined by coin flip), last two authors equal advising</p>
  <p><b>关键词</b>：present neural descriptor fields, pose whose descriptor matches, possible 3d object translations, 3 )- equivariant, new object instance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present Neural Descriptor Fields (NDFs), an object representation that
encodes both points and relative poses between an object and a target (such as
a robot gripper or a rack used for hanging) via category-level descriptors. We
employ this representation for object manipulation, where given a task
demonstration, we want to repeat the same task on a new object instance from
the same category. We propose to achieve this objective by searching (via
optimization) for the pose whose descriptor matches that observed in the
demonstration. NDFs are conveniently trained in a self-supervised fashion via a
3D auto-encoding task that does not rely on expert-labeled keypoints. Further,
NDFs are SE(3)-equivariant, guaranteeing performance that generalizes across
all possible 3D object translations and rotations. We demonstrate learning of
manipulation tasks from few (5-10) demonstrations both in simulation and on a
real robot. Our performance generalizes across both object instances and 6-DoF
object poses, and significantly outperforms a recent baseline that relies on 2D
descriptors. Project website: this https URL.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Extending the WILDS Benchmark for Unsupervised Adaptation</b></summary>
  <p><b>编号</b>：[21]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05090</p>
  <p><b>作者</b>：Shiori Sagawa,  Pang Wei Koh,  Tony Lee,  Irena Gao,  Sang Michael Xie,  Kendrick Shen,  Ananya Kumar,  Weihua Hu,  Michihiro Yasunaga,  Henrik Marklund,  Sara Beery,  Etienne David,  Ian Stavness,  Wei Guo,  Jure Leskovec,  Kate Saenko,  Tatsunori Hashimoto,  Sergey Levine,  Chelsea Finn,  Percy Liang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：wildlife conservation ), tasks, existing distribution shift benchmarks, machine learning systems deployed, include curated unlabeled data, molecular graphs ).</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine learning systems deployed in the wild are often trained on a source
distribution but deployed on a different target distribution. Unlabeled data
can be a powerful point of leverage for mitigating these distribution shifts,
as it is frequently much more available than labeled data. However, existing
distribution shift benchmarks for unlabeled data do not reflect the breadth of
scenarios that arise in real-world applications. In this work, we present the
WILDS 2.0 update, which extends 8 of the 10 datasets in the WILDS benchmark of
distribution shifts to include curated unlabeled data that would be
realistically obtainable in deployment. To maintain consistency, the labeled
training, validation, and test sets, as well as the evaluation metrics, are
exactly the same as in the original WILDS benchmark. These datasets span a wide
range of applications (from histology to wildlife conservation), tasks
(classification, regression, and detection), and modalities (photos, satellite
images, microscope slides, text, molecular graphs). We systematically benchmark
state-of-the-art methods that leverage unlabeled data, including
domain-invariant, self-training, and self-supervised methods, and show that
their success on WILDS 2.0 is limited. To facilitate method development and
evaluation, we provide an open-source package that automates data loading and
contains all of the model architectures and methods used in this paper. Code
and leaderboards are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：A Survey on Echo Chambers on Social Media: Description, Detection and  Mitigation</b></summary>
  <p><b>编号</b>：[22]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05084</p>
  <p><b>作者</b>：Faisal Alatawi,  Lu Cheng,  Anique Tahir,  Mansooreh Karami,  Bohan Jiang,  Tyler Black,  Huan Liu</p>
  <p><b>备注</b>：21 pages, 5 figures</p>
  <p><b>关键词</b>：pertinent issues like political polarization, mainly based around recommender systems, echo chambers promote conspiracy theories, investigate different computational approaches, recommender systems take advantage</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Echo chambers on social media are a significant problem that can elicit a
number of negative consequences, most recently affecting the response to
COVID-19. Echo chambers promote conspiracy theories about the virus and are
found to be linked to vaccine hesitancy, less compliance with mask mandates,
and the practice of social distancing. Moreover, the problem of echo chambers
is connected to other pertinent issues like political polarization and the
spread of misinformation. An echo chamber is defined as a network of users in
which users only interact with opinions that support their pre-existing beliefs
and opinions, and they exclude and discredit other viewpoints. This survey aims
to examine the echo chamber phenomenon on social media from a social computing
perspective and provide a blueprint for possible solutions. We survey the
related literature to understand the attributes of echo chambers and how they
affect the individual and society at large. Additionally, we show the
mechanisms, both algorithmic and psychological, that lead to the formation of
echo chambers. These mechanisms could be manifested in two forms: (1) the bias
of social media's recommender systems and (2) internal biases such as
confirmation bias and homophily. While it is immensely challenging to mitigate
internal biases, there has been great efforts seeking to mitigate the bias of
recommender systems. These recommender systems take advantage of our own biases
to personalize content recommendations to keep us engaged in order to watch
more ads. Therefore, we further investigate different computational approaches
for echo chamber detection and prevention, mainly based around recommender
systems.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Generating Useful Accident-Prone Driving Scenarios via a Learned Traffic  Prior</b></summary>
  <p><b>编号</b>：[25]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05077</p>
  <p><b>作者</b>：Davis Rempe,  Jonah Philion,  Leonidas J. Guibas,  Sanja Fidler,  Or Litany</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：autonomous vehicles requires scalable generation, analysis clusters generated scenarios based, strive successfully generates realistic, automatically generate challenging scenarios, based conditional vae</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Evaluating and improving planning for autonomous vehicles requires scalable
generation of long-tail traffic scenarios. To be useful, these scenarios must
be realistic and challenging, but not impossible to drive through safely. In
this work, we introduce STRIVE, a method to automatically generate challenging
scenarios that cause a given planner to produce undesirable behavior, like
collisions. To maintain scenario plausibility, the key idea is to leverage a
learned model of traffic motion in the form of a graph-based conditional VAE.
Scenario generation is formulated as an optimization in the latent space of
this traffic model, effected by perturbing an initial real-world scene to
produce trajectories that collide with a given planner. A subsequent
optimization is used to find a "solution" to the scenario, ensuring it is
useful to improve the given planner. Further analysis clusters generated
scenarios based on collision type. We attack two planners and show that STRIVE
successfully generates realistic, challenging scenarios in both cases. We
additionally "close the loop" and use these scenarios to optimize
hyperparameters of a rule-based planner.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：A Novel Tropical Geometry-based Interpretable Machine Learning Method:  Application in Prognosis of Advanced Heart Failure</b></summary>
  <p><b>编号</b>：[26]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05071</p>
  <p><b>作者</b>：Heming Yao,  Harm Derksen,  Jessica R. Golbus,  Justin Zhang,  Keith D. Aaronson,  Jonathan Gryak,  Kayvan Najarian</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：novel interpretable machine learning method, proposed network achieved great performance, wherein variable encoding functions, experiments using synthetic datasets, durable mechanical circulatory support</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A model's interpretability is essential to many practical applications such
as clinical decision support systems. In this paper, a novel interpretable
machine learning method is presented, which can model the relationship between
input variables and responses in humanly understandable rules. The method is
built by applying tropical geometry to fuzzy inference systems, wherein
variable encoding functions and salient rules can be discovered by supervised
learning. Experiments using synthetic datasets were conducted to investigate
the performance and capacity of the proposed algorithm in classification and
rule discovery. Furthermore, the proposed method was applied to a clinical
application that identified heart failure patients that would benefit from
advanced therapies such as heart transplant or durable mechanical circulatory
support. Experimental results show that the proposed network achieved great
performance on the classification tasks. In addition to learning humanly
understandable rules from the dataset, existing fuzzy domain knowledge can be
easily transferred into the network and used to facilitate model training. From
our results, the proposed model and the ability of learning existing domain
knowledge can significantly improve the model generalizability. The
characteristics of the proposed network make it promising in applications
requiring model reliability and justification.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：A Bayesian Treatment of Real-to-Sim for Deformable Object Manipulation</b></summary>
  <p><b>编号</b>：[28]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05068</p>
  <p><b>作者</b>：Rika Antonova,  Jingyun Yang,  Priya Sundaresan,  Dieter Fox,  Fabio Ramos,  Jeannette Bohg</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：incorporate noisy state observations directly, state estimation typically rely, probabilistic inference task defined, deformable object manipulation remains, modern bayesian simulation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deformable object manipulation remains a challenging task in robotics
research. Conventional techniques for parameter inference and state estimation
typically rely on a precise definition of the state space and its dynamics.
While this is appropriate for rigid objects and robot states, it is challenging
to define the state space of a deformable object and how it evolves in time. In
this work, we pose the problem of inferring physical parameters of deformable
objects as a probabilistic inference task defined with a simulator. We propose
a novel methodology for extracting state information from image sequences via a
technique to represent the state of a deformable object as a distribution
embedding. This allows to incorporate noisy state observations directly into
modern Bayesian simulation-based inference tools in a principled manner. Our
experiments confirm that we can estimate posterior distributions of physical
properties, such as elasticity, friction and scale of highly deformable
objects, such as cloth and ropes. Overall, our method addresses the real-to-sim
problem probabilistically and helps to better represent the evolution of the
state of deformable objects.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Learning Transferable Motor Skills with Hierarchical Latent Mixture  Policies</b></summary>
  <p><b>编号</b>：[29]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05062</p>
  <p><b>作者</b>：Dushyant Rao,  Fereshteh Sadeghi,  Leonard Hasenclever,  Markus Wulfmeier,  Martina Zambelli,  Giulia Vezzani,  Dhruva Tirumala,  Yusuf Aytar,  Josh Merel,  Nicolas Heess,  Raia Hadsell</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：hierarchical mixture latent variable model, continuous latent variable model, yielding better sample efficiency, learn abstract motor skills, effectively cluster offline data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>For robots operating in the real world, it is desirable to learn reusable
behaviours that can effectively be transferred and adapted to numerous tasks
and scenarios. We propose an approach to learn abstract motor skills from data
using a hierarchical mixture latent variable model. In contrast to existing
work, our method exploits a three-level hierarchy of both discrete and
continuous latent variables, to capture a set of high-level behaviours while
allowing for variance in how they are executed. We demonstrate in manipulation
domains that the method can effectively cluster offline data into distinct,
executable behaviours, while retaining the flexibility of a continuous latent
variable model. The resulting skills can be transferred and fine-tuned on new
tasks, unseen objects, and from state to vision-based policies, yielding better
sample efficiency and asymptotic performance compared to existing skill- and
imitation-based methods. We further analyse how and when the skills are most
beneficial: they encourage directed exploration to cover large regions of the
state space relevant to the task, making them most effective in challenging
sparse-reward settings.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Opinion Extraction as A Structured Sentiment Analysis using Transformers</b></summary>
  <p><b>编号</b>：[31]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05056</p>
  <p><b>作者</b>：Yucheng Liu,  Tian Zhu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：require different input data, extract multiple opinion tuples, performed different experiments, structured sentiment analysis, named entity recognition</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Relationship extraction and named entity recognition have always been
considered as two distinct tasks that require different input data, labels, and
models. However, both are essential for structured sentiment analysis. We
believe that both tasks can be combined into a single stacked model with the
same input data. We performed different experiments to find the best model to
extract multiple opinion tuples from a single sentence. The opinion tuples will
consist of holders, targets, and expressions. With the opinion tuples, we will
be able to extract the relationship we need.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Gradient-matching coresets for continual learning</b></summary>
  <p><b>编号</b>：[38]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05025</p>
  <p><b>作者</b>：Lukas Balles,  Giovanni Zappella,  Cédric Archambeau</p>
  <p><b>备注</b>：Accepted at the NeurIPS '21 Workshop on Distribution Shifts</p>
  <p><b>关键词</b>：method performs strong competitors, coreset selection method based, reservoir sampling across, original training dataset, rehearsal memory</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We devise a coreset selection method based on the idea of gradient matching:
The gradients induced by the coreset should match, as closely as possible,
those induced by the original training dataset. We evaluate the method in the
context of continual learning, where it can be used to curate a rehearsal
memory. Our method performs strong competitors such as reservoir sampling
across a range of memory sizes.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Millimeter Wave Localization with Imperfect Training Data using Shallow  Neural Networks</b></summary>
  <p><b>编号</b>：[41]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05008</p>
  <p><b>作者</b>：Anish Shastri,  Joan Palacios,  Paolo Casari</p>
  <p><b>备注</b>：6 pages, 9 figures. The paper is submitted to IEEE WCNC 2022</p>
  <p><b>关键词</b>：model requires significantly fewer weights, relieve training data collection efforts, shallow neural network model, needs fewer training samples, yields sparse angular spectra</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Millimeter wave (mmWave) localization algorithms exploit the quasi-optical
propagation of mmWave signals, which yields sparse angular spectra at the
receiver. Geometric approaches to angle-based localization typically require to
know the map of the environment and the location of the access points. Thus,
several works have resorted to automated learning in order to infer a device's
location from the properties of the received mmWave signals. However,
collecting training data for such models is a significant burden. In this work,
we propose a shallow neural network model to localize mmWave devices indoors.
This model requires significantly fewer weights than those proposed in the
literature. Therefore, it is amenable for implementation in
resource-constrained hardware, and needs fewer training samples to converge. We
also propose to relieve training data collection efforts by retrieving
(inherently imperfect) location estimates from geometry-based mmWave
localization algorithms. Even in this case, our results show that the proposed
neural networks perform as good as or better than state-of-the-art algorithms.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Mutual Adversarial Training: Learning together is better than going  alone</b></summary>
  <p><b>编号</b>：[43]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05005</p>
  <p><b>作者</b>：Jiang Liu,  Chun Pong Lau,  Hossein Souri,  Soheil Feizi,  Rama Chellappa</p>
  <p><b>备注</b>：Under submission</p>
  <p><b>关键词</b>：interactions among models affect robustness via knowledge distillation, among different perturbation types, effectively improve model robustness, propose mutual adversarial training, mat allows robust models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent studies have shown that robustness to adversarial attacks can be
transferred across networks. In other words, we can make a weak model more
robust with the help of a strong teacher model. We ask if instead of learning
from a static teacher, can models "learn together" and "teach each other" to
achieve better robustness? In this paper, we study how interactions among
models affect robustness via knowledge distillation. We propose mutual
adversarial training (MAT), in which multiple models are trained together and
share the knowledge of adversarial examples to achieve improved robustness. MAT
allows robust models to explore a larger space of adversarial samples, and find
more robust feature spaces and decision boundaries. Through extensive
experiments on CIFAR-10 and CIFAR-100, we demonstrate that MAT can effectively
improve model robustness and outperform state-of-the-art methods under
white-box attacks, bringing $\sim$8% accuracy gain to vanilla adversarial
training (AT) under PGD-100 attacks. In addition, we show that MAT can also
mitigate the robustness trade-off among different perturbation types, bringing
as much as 13.1% accuracy gain to AT baselines against the union of $l_\infty$,
$l_2$ and $l_1$ attacks. These results show the superiority of the proposed
method and demonstrate that collaborative learning is an effective strategy for
designing robust models.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Wikidated 1.0: An Evolving Knowledge Graph Dataset of Wikidata's  Revision History</b></summary>
  <p><b>编号</b>：[44]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05003</p>
  <p><b>作者</b>：Lukas Schmelzeisen,  Corina Dima,  Steffen Staab</p>
  <p><b>备注</b>：15 pages, 4 figures. Published at Wikidata@ISWC 2021</p>
  <p><b>关键词</b>：thus evolved considerably since, recently emerging research subject, semantic web community, present wikidated 1, present statistical characteristics</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Wikidata is the largest general-interest knowledge base that is openly
available. It is collaboratively edited by thousands of volunteer editors and
has thus evolved considerably since its inception in 2012. In this paper, we
present Wikidated 1.0, a dataset of Wikidata's full revision history, which
encodes changes between Wikidata revisions as sets of deletions and additions
of RDF triples. To the best of our knowledge, it constitutes the first large
dataset of an evolving knowledge graph, a recently emerging research subject in
the Semantic Web community. We introduce the methodology for generating
Wikidated 1.0 from dumps of Wikidata, discuss its implementation and
limitations, and present statistical characteristics of the dataset.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：The Peril of Popular Deep Learning Uncertainty Estimation Methods</b></summary>
  <p><b>编号</b>：[45]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05000</p>
  <p><b>作者</b>：Yehao Liu,  Matteo Pagliardini,  Tatjana Chavdarova,  Sebastian U. Stich</p>
  <p><b>备注</b>：Presented at the Bayesian Deep Learning Workshop at NeurIPS 2021</p>
  <p><b>关键词</b>：gp methods always yield high uncertainty estimates, bnn ), monte carlo dropout, gp ), bayesian neural networks, give high uncertainty estimates, based methods -- instead</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Uncertainty estimation (UE) techniques -- such as the Gaussian process (GP),
Bayesian neural networks (BNN), Monte Carlo dropout (MCDropout) -- aim to
improve the interpretability of machine learning models by assigning an
estimated uncertainty value to each of their prediction outputs. However, since
too high uncertainty estimates can have fatal consequences in practice, this
paper analyzes the above techniques.
Firstly, we show that GP methods always yield high uncertainty estimates on
out of distribution (OOD) data. Secondly, we show on a 2D toy example that both
BNNs and MCDropout do not give high uncertainty estimates on OOD samples.
Finally, we show empirically that this pitfall of BNNs and MCDropout holds on
real world datasets as well. Our insights (i) raise awareness for the more
cautious use of currently popular UE methods in Deep Learning, (ii) encourage
the development of UE methods that approximate GP-based methods -- instead of
BNNs and MCDropout, and (iii) our empirical setups can be used for verifying
the OOD performances of any other UE method. The source code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：PE-former: Pose Estimation Transformer</b></summary>
  <p><b>编号</b>：[48]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04981</p>
  <p><b>作者</b>：Paschalis Panteleris,  Antonis Argyros</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：decoder transformer architecture yields state, evaluate two vit architectures, 2d body pose estimation, pure transformer architecture, vision transformer architectures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision transformer architectures have been demonstrated to work very
effectively for image classification tasks. Efforts to solve more challenging
vision tasks with transformers rely on convolutional backbones for feature
extraction. In this paper we investigate the use of a pure transformer
architecture (i.e., one with no CNN backbone) for the problem of 2D body pose
estimation. We evaluate two ViT architectures on the COCO dataset. We
demonstrate that using an encoder-decoder transformer architecture yields state
of the art results on this estimation problem.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Bringing Atomistic Deep Learning to Prime Time</b></summary>
  <p><b>编号</b>：[49]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04977</p>
  <p><b>作者</b>：Nathan C. Frey,  Siddharth Samsi,  Bharath Ramsundar,  Connor W. Coley,  Vijay Gadepally</p>
  <p><b>备注</b>：6 pages, 1 figure, NeurIPS 2021 AI for Science workshop</p>
  <p><b>关键词</b>：outline focused research efforts, identify four barriers preventing, atomistic deep learning, yet revolutionized, performance computing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Artificial intelligence has not yet revolutionized the design of materials
and molecules. In this perspective, we identify four barriers preventing the
integration of atomistic deep learning, molecular science, and high-performance
computing. We outline focused research efforts to address the opportunities
presented by these challenges.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Model-Agnostic Hybrid Numerical Weather Prediction and Machine Learning  Paradigm for Solar Forecasting in the Tropics</b></summary>
  <p><b>编号</b>：[55]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04963</p>
  <p><b>作者</b>：Nigel Yuan Yun Ng,  Harish Gopalan,  Venugopalan S.G. Raghavan,  Chin Chun Ooi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：results obtained using cam, multiple possible physical parameterizations, models incorporating nearby locations, ml error correction model, different possible parameterizations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Numerical weather prediction (NWP) and machine learning (ML) methods are
popular for solar forecasting. However, NWP models have multiple possible
physical parameterizations, which requires site-specific NWP optimization. This
is further complicated when regional NWP models are used with global climate
models with different possible parameterizations. In this study, an alternative
approach is proposed and evaluated for four radiation models. Weather Research
and Forecasting (WRF) model is run in both global and regional mode to provide
an estimate for solar irradiance. This estimate is then post-processed using ML
to provide a final prediction. Normalized root-mean-square error from WRF is
reduced by up to 40-50% with this ML error correction model. Results obtained
using CAM, GFDL, New Goddard and RRTMG radiation models were comparable after
this correction, negating the need for WRF parameterization tuning. Other
models incorporating nearby locations and sensor data are also evaluated, with
the latter being particularly promising.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Machine Learning for Utility Prediction in Argument-Based Computational  Persuasion</b></summary>
  <p><b>编号</b>：[58]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04953</p>
  <p><b>作者</b>：Ivan Donadello,  Anthony Hunter,  Stefano Teso,  Mauro Dragoni</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：realistic case study concerning healthy eating habits, predicting useful utility functions, develop two ml methods, use machine learning, party decision theory</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automated persuasion systems (APS) aim to persuade a user to believe
something by entering into a dialogue in which arguments and counterarguments
are exchanged. To maximize the probability that an APS is successful in
persuading a user, it can identify a global policy that will allow it to select
the best arguments it presents at each stage of the dialogue whatever arguments
the user presents. However, in real applications, such as for healthcare, it is
unlikely the utility of the outcome of the dialogue will be the same, or the
exact opposite, for the APS and user. In order to deal with this situation,
games in extended form have been harnessed for argumentation in Bi-party
Decision Theory. This opens new problems that we address in this paper: (1) How
can we use Machine Learning (ML) methods to predict utility functions for
different subpopulations of users? and (2) How can we identify for a new user
the best utility function from amongst those that we have learned? To this
extent, we develop two ML methods, EAI and EDS, that leverage information
coming from the users to predict their utilities. EAI is restricted to a fixed
amount of information, whereas EDS can choose the information that best detects
the subpopulations of a user. We evaluate EAI and EDS in a simulation setting
and in a realistic case study concerning healthy eating habits. Results are
promising in both cases, but EDS is more effective at predicting useful utility
functions.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：PARL: Enhancing Diversity of Ensemble Networks to Resist Adversarial  Attacks via Pairwise Adversarially Robust Loss Function</b></summary>
  <p><b>编号</b>：[59]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04948</p>
  <p><b>作者</b>：Manaar Alam,  Shubhajit Datta,  Debdeep Mukhopadhyay,  Arijit Mondal,  Partha Pratim Chakrabarti</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：present extensive experiments using standard image classification datasets like cifar, 100 trained using standard resnet20 classifier, previous ensemble methods without adversely affecting, constructs multiple diverse classifiers using, proposed training procedure enables parl</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The security of Deep Learning classifiers is a critical field of study
because of the existence of adversarial attacks. Such attacks usually rely on
the principle of transferability, where an adversarial example crafted on a
surrogate classifier tends to mislead the target classifier trained on the same
dataset even if both classifiers have quite different architecture. Ensemble
methods against adversarial attacks demonstrate that an adversarial example is
less likely to mislead multiple classifiers in an ensemble having diverse
decision boundaries. However, recent ensemble methods have either been shown to
be vulnerable to stronger adversaries or shown to lack an end-to-end
evaluation. This paper attempts to develop a new ensemble methodology that
constructs multiple diverse classifiers using a Pairwise Adversarially Robust
Loss (PARL) function during the training procedure. PARL utilizes gradients of
each layer with respect to input in every classifier within the ensemble
simultaneously. The proposed training procedure enables PARL to achieve higher
robustness against black-box transfer attacks compared to previous ensemble
methods without adversely affecting the accuracy of clean examples. We also
evaluate the robustness in the presence of white-box attacks, where adversarial
examples are crafted using parameters of the target classifier. We present
extensive experiments using standard image classification datasets like
CIFAR-10 and CIFAR-100 trained using standard ResNet20 classifier against
state-of-the-art adversarial attacks to demonstrate the robustness of the
proposed ensemble methodology.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Automated Side Channel Analysis of Media Software with Manifold Learning</b></summary>
  <p><b>编号</b>：[60]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04947</p>
  <p><b>作者</b>：Yuanyuan Yuan,  Qi Pang,  Shuai Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：highly effective defensive technique called perception blinding, evaluation exploits three popular media software, analyze three common side channels, framework successfully reconstructs high, launch side channel analyses</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The prosperous development of cloud computing and machine learning as a
service has led to the widespread use of media software to process confidential
media data. This paper explores an adversary's ability to launch side channel
analyses (SCA) against media software to reconstruct confidential media inputs.
Recent advances in representation learning and perceptual learning inspired us
to consider the reconstruction of media inputs from side channel traces as a
cross-modality manifold learning task that can be addressed in a unified manner
with an autoencoder framework trained to learn the mapping between media inputs
and side channel observations. We further enhance the autoencoder with
attention to localize the program points that make the primary contribution to
SCA, thus automatically pinpointing information-leakage points in media
software. We also propose a novel and highly effective defensive technique
called perception blinding that can perturb media inputs with perception masks
and mitigate manifold learning-based SCA.
Our evaluation exploits three popular media software to reconstruct inputs in
image, audio, and text formats. We analyze three common side channels - cache
bank, cache line, and page tables - and userspace-only cache set accesses
logged by standard Prime+Probe. Our framework successfully reconstructs
high-quality confidential inputs from the assessed media software and
automatically pinpoint their vulnerable program points, many of which are
unknown to the public. We further show that perception blinding can mitigate
manifold learning-based SCA with negligible extra cost.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Model Doctor: A Simple Gradient Aggregation Strategy for Diagnosing and  Treating CNN Classifiers</b></summary>
  <p><b>编号</b>：[64]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04934</p>
  <p><b>作者</b>：Zunlei Feng,  Jiacong Hu,  Sai Wu,  Xiaotian Yu,  Jie Song,  Mingli Song</p>
  <p><b>备注</b>：Accepted by AAAI 2022</p>
  <p><b>关键词</b>：first completely automatic model diagnosing, simple aggregate gradient constraint, 1 %- 5 %., proposed model doctor applies, aggregate gradient strategy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, Convolutional Neural Network (CNN) has achieved excellent
performance in the classification task. It is widely known that CNN is deemed
as a 'black-box', which is hard for understanding the prediction mechanism and
debugging the wrong prediction. Some model debugging and explanation works are
developed for solving the above drawbacks. However, those methods focus on
explanation and diagnosing possible causes for model prediction, based on which
the researchers handle the following optimization of models manually. In this
paper, we propose the first completely automatic model diagnosing and treating
tool, termed as Model Doctor. Based on two discoveries that 1) each category is
only correlated with sparse and specific convolution kernels, and 2)
adversarial samples are isolated while normal samples are successive in the
feature space, a simple aggregate gradient constraint is devised for
effectively diagnosing and optimizing CNN classifiers. The aggregate gradient
strategy is a versatile module for mainstream CNN classifiers. Extensive
experiments demonstrate that the proposed Model Doctor applies to all existing
CNN classifiers, and improves the accuracy of $16$ mainstream CNN classifiers
by 1%-5%.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Self-Supervised Image-to-Text and Text-to-Image Synthesis</b></summary>
  <p><b>编号</b>：[65]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04928</p>
  <p><b>作者</b>：Anindya Sundar Das,  Sriparna Saha</p>
  <p><b>备注</b>：ICONIP 2021 : The 28th International Conference on Neural Information Processing</p>
  <p><b>关键词</b>：supervised deep learning based approach towards learning, maximum mean discrepancy based generative networks, first obtain dense vector representations, level utilizing lstm based text, supervised generative deep architectures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A comprehensive understanding of vision and language and their interrelation
are crucial to realize the underlying similarities and differences between
these modalities and to learn more generalized, meaningful representations. In
recent years, most of the works related to Text-to-Image synthesis and
Image-to-Text generation, focused on supervised generative deep architectures
to solve the problems, where very little interest was placed on learning the
similarities between the embedding spaces across modalities. In this paper, we
propose a novel self-supervised deep learning based approach towards learning
the cross-modal embedding spaces; for both image to text and text to image
generations. In our approach, we first obtain dense vector representations of
images using StackGAN-based autoencoder model and also dense vector
representations on sentence-level utilizing LSTM based text-autoencoder; then
we study the mapping from embedding space of one modality to embedding space of
the other modality utilizing GAN and maximum mean discrepancy based generative
networks. We, also demonstrate that our model learns to generate textual
description from image data as well as images from textual data both
qualitatively and quantitatively.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Identification of Twitter Bots based on an Explainable ML Framework: the  US 2020 Elections Case Study</b></summary>
  <p><b>编号</b>：[67]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04913</p>
  <p><b>作者</b>：Alexander Shevtsov,  Christos Tzagkarakis,  Despoina Antonakaki,  Sotiris Ioannidis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：study also deploys shapley additive explanations, popular social networks attracting millions, art twitter bot detection method, efficient application programming interface, distinct twitter datasets demonstrate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Twitter is one of the most popular social networks attracting millions of
users, while a considerable proportion of online discourse is captured. It
provides a simple usage framework with short messages and an efficient
application programming interface (API) enabling the research community to
study and analyze several aspects of this social network. However, the Twitter
usage simplicity can lead to malicious handling by various bots. The malicious
handling phenomenon expands in online discourse, especially during the
electoral periods, where except the legitimate bots used for dissemination and
communication purposes, the goal is to manipulate the public opinion and the
electorate towards a certain direction, specific ideology, or political party.
This paper focuses on the design of a novel system for identifying Twitter bots
based on labeled Twitter data. To this end, a supervised machine learning (ML)
framework is adopted using an Extreme Gradient Boosting (XGBoost) algorithm,
where the hyper-parameters are tuned via cross-validation. Our study also
deploys Shapley Additive Explanations (SHAP) for explaining the ML model
predictions by calculating feature importance, using the game theoretic-based
Shapley values. Experimental evaluation on distinct Twitter datasets
demonstrate the superiority of our approach, in terms of bot detection
accuracy, when compared against a recent state-of-the-art Twitter bot detection
method.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Scalable and Decentralized Algorithms for Anomaly Detection via  Learning-Based Controlled Sensing</b></summary>
  <p><b>编号</b>：[68]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04912</p>
  <p><b>作者</b>：Geethu Joseph,  Chen Zhong,  M. Cenk Gursoy,  Senem Velipasalar,  Pramod K.Varshney</p>
  <p><b>备注</b>：13 pages, 4 figures. arXiv admin note: substantial text overlap with arXiv:2105.06289</p>
  <p><b>关键词</b>：markov decision process defined using, critic reinforcement learning framework, algorithms using numerical experiments, detection algorithms using, unlike prior work</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We address the problem of sequentially selecting and observing processes from
a given set to find the anomalies among them. The decision-maker observes a
subset of the processes at any given time instant and obtains a noisy binary
indicator of whether or not the corresponding process is anomalous. In this
setting, we develop an anomaly detection algorithm that chooses the processes
to be observed at a given time instant, decides when to stop taking
observations, and declares the decision on anomalous processes. The objective
of the detection algorithm is to identify the anomalies with an accuracy
exceeding the desired value while minimizing the delay in decision making. We
devise a centralized algorithm where the processes are jointly selected by a
common agent as well as a decentralized algorithm where the decision of whether
to select a process is made independently for each process. Our algorithms rely
on a Markov decision process defined using the marginal probability of each
process being normal or anomalous, conditioned on the observations. We
implement the detection algorithms using the deep actor-critic reinforcement
learning framework. Unlike prior work on this topic that has exponential
complexity in the number of processes, our algorithms have computational and
memory requirements that are both polynomial in the number of processes. We
demonstrate the efficacy of these algorithms using numerical experiments by
comparing them with state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：JueWu-MC: Playing Minecraft with Sample-efficient Hierarchical  Reinforcement Learning</b></summary>
  <p><b>编号</b>：[70]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04907</p>
  <p><b>作者</b>：Zichuan Lin,  Junyou Li,  Jianing Shi,  Deheng Ye,  Qiang Fu,  Wei Yang</p>
  <p><b>备注</b>：The champion solution of NeurIPS 2021 MineRL research competition ( this https URL )</p>
  <p><b>关键词</b>：world games like minecraft remains, neurips minerl 2021 research competition, mc significantly improves sample efficiency, efficient hierarchical rl approach equipped, approach includes two levels</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Learning rational behaviors in open-world games like Minecraft remains to be
challenging for Reinforcement Learning (RL) research due to the compound
challenge of partial observability, high-dimensional visual perception and
delayed reward. To address this, we propose JueWu-MC, a sample-efficient
hierarchical RL approach equipped with representation learning and imitation
learning to deal with perception and exploration. Specifically, our approach
includes two levels of hierarchy, where the high-level controller learns a
policy to control over options and the low-level workers learn to solve each
sub-task. To boost the learning of sub-tasks, we propose a combination of
techniques including 1) action-aware representation learning which captures
underlying relations between action and representation, 2) discriminator-based
self-imitation learning for efficient exploration, and 3) ensemble behavior
cloning with consistency filtering for policy robustness. Extensive experiments
show that JueWu-MC significantly improves sample efficiency and outperforms a
set of baselines by a large margin. Notably, we won the championship of the
NeurIPS MineRL 2021 research competition and achieved the highest performance
score ever.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：i-SpaSP: Structured Neural Pruning via Sparse Signal Recovery</b></summary>
  <p><b>编号</b>：[71]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04905</p>
  <p><b>作者</b>：Cameron R. Wolfe,  Anastasios Kyrillidis</p>
  <p><b>备注</b>：27 pages, 4 figures</p>
  <p><b>关键词</b>：polynomial becomes arbitrarily large based, e ., feed forward networks, achieves strong empirical results, sparse structured pruning algorithm, structured pruning algorithm</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a novel, structured pruning algorithm for neural networks -- the
iterative, Sparse Structured Pruning algorithm, dubbed as i-SpaSP. Inspired by
ideas from sparse signal recovery, i-SpaSP operates by iteratively identifying
a larger set of important parameter groups (e.g., filters or neurons) within a
network that contribute most to the residual between pruned and dense network
output, then thresholding these groups based on a smaller, pre-defined pruning
ratio. For both two-layer and multi-layer network architectures with ReLU
activations, we show the error induced by pruning with i-SpaSP decays
polynomially, where the degree of this polynomial becomes arbitrarily large
based on the sparsity of the dense network's hidden representations. In our
experiments, i-SpaSP is evaluated across a variety of datasets (i.e., MNIST and
ImageNet) and architectures (i.e., feed forward networks, ResNet34, and
MobileNetV2), where it is shown to discover high-performing sub-networks and
improve upon the pruning efficiency of provable baseline methodologies by
several orders of magnitude. Put simply, i-SpaSP is easy to implement with
automatic differentiation, achieves strong empirical results, comes with
theoretical convergence guarantees, and is efficient, thus distinguishing
itself as one of the few computationally efficient, practical, and provable
pruning algorithms.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Learning Personal Representations from fMRIby Predicting Neurofeedback  Performance</b></summary>
  <p><b>编号</b>：[73]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04902</p>
  <p><b>作者</b>：Jhonathan Osin,  Lior Wolf,  Guy Gurevitch,  Jackob Nimrod Keynan,  Tom Fruchtman-Steinbok,  Ayelet Or-Borichev,  Shira Reznik Balter,  Talma Hendler</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：next fmri frame given recent fmri frames, supervised recurrent neural network, deep neural network method, frame prediction considerably, yields good performance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a deep neural network method for learning a personal
representation for individuals that are performing a self neuromodulation task,
guided by functional MRI (fMRI). This neurofeedback task (watch vs. regulate)
provides the subjects with a continuous feedback contingent on down regulation
of their Amygdala signal and the learning algorithm focuses on this region's
time-course of activity. The representation is learned by a self-supervised
recurrent neural network, that predicts the Amygdala activity in the next fMRI
frame given recent fMRI frames and is conditioned on the learned individual
representation. It is shown that the individuals' representation improves the
next-frame prediction considerably. Moreover, this personal representation,
learned solely from fMRI images, yields good performance in linear prediction
of psychiatric traits, which is better than performing such a prediction based
on clinical data and personality tests. Our code is attached as supplementary
and the data would be shared subject to ethical approvals.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Assessing Fairness in the Presence of Missing Data</b></summary>
  <p><b>编号</b>：[74]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04899</p>
  <p><b>作者</b>：Yiliang Zhang,  Qi Long</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：complete case domain may show disproportionate bias towards, arbitrary model evaluated merely using complete cases, first known theoretical results, complete data may, complete data domain</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Missing data are prevalent and present daunting challenges in real data
analysis. While there is a growing body of literature on fairness in analysis
of fully observed data, there has been little theoretical work on investigating
fairness in analysis of incomplete data. In practice, a popular analytical
approach for dealing with missing data is to use only the set of complete
cases, i.e., observations with all features fully observed to train a
prediction algorithm. However, depending on the missing data mechanism, the
distribution of complete cases and the distribution of the complete data may be
substantially different. When the goal is to develop a fair algorithm in the
complete data domain where there are no missing values, an algorithm that is
fair in the complete case domain may show disproportionate bias towards some
marginalized groups in the complete data domain. To fill this significant gap,
we study the problem of estimating fairness in the complete data domain for an
arbitrary model evaluated merely using complete cases. We provide upper and
lower bounds on the fairness estimation error and conduct numerical experiments
to assess our theoretical results. Our work provides the first known
theoretical results on fairness guarantee in analysis of incomplete data.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Latent Space Explanation by Intervention</b></summary>
  <p><b>编号</b>：[75]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04895</p>
  <p><b>作者</b>：Itai Gat,  Guy Lorberbom,  Idan Schwartz,  Tamir Hazan</p>
  <p><b>备注</b>：Accepted to AAAI22</p>
  <p><b>关键词</b>：deep neural nets heavily relies, suggest different interventions, show various visualizations, hence providing interpretability, encode complex relations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The success of deep neural nets heavily relies on their ability to encode
complex relations between their input and their output. While this property
serves to fit the training data well, it also obscures the mechanism that
drives prediction. This study aims to reveal hidden concepts by employing an
intervention mechanism that shifts the predicted class based on discrete
variational autoencoders. An explanatory model then visualizes the encoded
information from any hidden layer and its corresponding intervened
representation. By the assessment of differences between the original
representation and the intervened representation, one can determine the
concepts that can alter the class, hence providing interpretability. We
demonstrate the effectiveness of our approach on CelebA, where we show various
visualizations for bias in the data and suggest different interventions to
reveal and change bias.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Real-World Dexterous Object Manipulation based Deep Reinforcement  Learning</b></summary>
  <p><b>编号</b>：[76]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04893</p>
  <p><b>作者</b>：Qingfeng Yao,  Jilong Wang,  Shuyu Yang</p>
  <p><b>备注</b>：Best Paper Award Runner Up winner submission for Real Robot Challenge 2021</p>
  <p><b>关键词</b>：level deep reinforcement learning model selects appropriate contact positions, level control module performs, deep reinforcement learning, deep reinforcement learning, traditional robot control methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep reinforcement learning has shown its advantages in real-time
decision-making based on the state of the agent. In this stage, we solved the
task of using a real robot to manipulate the cube to a given trajectory. The
task is broken down into different procedures and we propose a hierarchical
structure, the high-level deep reinforcement learning model selects appropriate
contact positions and the low-level control module performs the position
control under the corresponding trajectory. Our framework reduces the
disadvantage of low sample efficiency of deep reinforcement learning and
lacking adaptability of traditional robot control methods. Our algorithm is
trained in simulation and migrated to reality without fine-tuning. The
experimental results show the effectiveness of our method both simulation and
reality. Our code and video can be found at
this https URL and
this https URL.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Multi-Task Learning on Networks</b></summary>
  <p><b>编号</b>：[77]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04891</p>
  <p><b>作者</b>：Andrea Ponti</p>
  <p><b>备注</b>：94 pages, 53 figures, 8 tables</p>
  <p><b>关键词</b>：radically different approach based, conflicting objectives requires modelling, new computational approach represent, objective optimization problems arising, computational results show</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The multi-task learning (MTL) paradigm can be traced back to an early paper
of Caruana (1997) in which it was argued that data from multiple tasks can be
used with the aim to obtain a better performance over learning each task
independently. A solution of MTL with conflicting objectives requires modelling
the trade-off among them which is generally beyond what a straight linear
combination can achieve. A theoretically principled and computationally
effective strategy is finding solutions which are not dominated by others as it
is addressed in the Pareto analysis. Multi-objective optimization problems
arising in the multi-task learning context have specific features and require
adhoc methods. The analysis of these features and the proposal of a new
computational approach represent the focus of this work. Multi-objective
evolutionary algorithms (MOEAs) can easily include the concept of dominance and
therefore the Pareto analysis. The major drawback of MOEAs is a low sample
efficiency with respect to function evaluations. The key reason for this
drawback is that most of the evolutionary approaches do not use models for
approximating the objective function. Bayesian Optimization takes a radically
different approach based on a surrogate model, such as a Gaussian Process. In
this thesis the solutions in the Input Space are represented as probability
distributions encapsulating the knowledge contained in the function
evaluations. In this space of probability distributions, endowed with the
metric given by the Wasserstein distance, a new algorithm MOEA/WST can be
designed in which the model is not directly on the objective function but in an
intermediate Information Space where the objects from the input space are
mapped into histograms. Computational results show that the sample efficiency
and the quality of the Pareto set provided by MOEA/WST are significantly better
than in the standard MOEA.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：KGE-CL: Contrastive Learning of Knowledge Graph Embeddings</b></summary>
  <p><b>编号</b>：[82]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04871</p>
  <p><b>作者</b>：Wentao Xu,  Zhiping Luo,  Weiqing Liu,  Jiang Bian,  Jian Yin,  Tie-Yan Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：simple yet efficient contrastive learning framework, previous knowledge graph embedding methods ignore, three standard knowledge graph benchmarks, benefit various downstream applications, knowledge graph embedding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Learning the embeddings of knowledge graphs is vital in artificial
intelligence, and can benefit various downstream applications, such as
recommendation and question answering. In recent years, many research efforts
have been proposed for knowledge graph embedding. However, most previous
knowledge graph embedding methods ignore the semantic similarity between the
related entities and entity-relation couples in different triples since they
separately optimize each triple with the scoring function. To address this
problem, we propose a simple yet efficient contrastive learning framework for
knowledge graph embeddings, which can shorten the semantic distance of the
related entities and entity-relation couples in different triples and thus
improve the expressiveness of knowledge graph embeddings. We evaluate our
proposed method on three standard knowledge graph benchmarks. It is noteworthy
that our method can yield some new state-of-the-art results, achieving 51.2%
MRR, 46.8% Hits@1 on the WN18RR dataset, and 59.1% MRR, 51.8% Hits@1 on the
YAGO3-10 dataset.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：A New Measure of Model Redundancy for Compressed Convolutional Neural  Networks</b></summary>
  <p><b>编号</b>：[84]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04857</p>
  <p><b>作者</b>：Feiqing Huang,  Yuefeng Si,  Yao Zheng,  Guodong Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：rigorous sample complexity analysis, new model redundancy measure, derived sample complexity, weights across layers, naive parameter counting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While recently many designs have been proposed to improve the model
efficiency of convolutional neural networks (CNNs) on a fixed resource budget,
theoretical understanding of these designs is still conspicuously lacking. This
paper aims to provide a new framework for answering the question: Is there
still any remaining model redundancy in a compressed CNN? We begin by
developing a general statistical formulation of CNNs and compressed CNNs via
the tensor decomposition, such that the weights across layers can be summarized
into a single tensor. Then, through a rigorous sample complexity analysis, we
reveal an important discrepancy between the derived sample complexity and the
naive parameter counting, which serves as a direct indicator of the model
redundancy. Motivated by this finding, we introduce a new model redundancy
measure for compressed CNNs, called the $K/R$ ratio, which further allows for
nonlinear activations. The usefulness of this new measure is supported by
ablation studies on popular block designs and datasets.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Siamese Attribute-missing Graph Auto-encoder</b></summary>
  <p><b>编号</b>：[89]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04842</p>
  <p><b>作者</b>：Wenxuan Tu,  Sihang Zhou,  Yue Liu,  Xinwang Liu</p>
  <p><b>备注</b>：under review</p>
  <p><b>关键词</b>：structural constraint enhanced learning mechanism, six benchmark datasets demonstrate, recently attracted considerable attention, common yet challenging problem, less discriminative feature representations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Graph representation learning (GRL) on attribute-missing graphs, which is a
common yet challenging problem, has recently attracted considerable attention.
We observe that existing literature: 1) isolates the learning of attribute and
structure embedding thus fails to take full advantages of the two types of
information; 2) imposes too strict distribution assumption on the latent space
variables, leading to less discriminative feature representations. In this
paper, based on the idea of introducing intimate information interaction
between the two information sources, we propose our Siamese Attribute-missing
Graph Auto-encoder (SAGA). Specifically, three strategies have been conducted.
First, we entangle the attribute embedding and structure embedding by
introducing a siamese network structure to share the parameters learned by both
processes, which allows the network training to benefit from more abundant and
diverse information. Second, we introduce a K-nearest neighbor (KNN) and
structural constraint enhanced learning mechanism to improve the quality of
latent features of the missing attributes by filtering unreliable connections.
Third, we manually mask the connections on multiple adjacent matrices and force
the structural information embedding sub-network to recover the true adjacent
matrix, thus enforcing the resulting network to be able to selectively exploit
more high-order discriminative features for data completion. Extensive
experiments on six benchmark datasets demonstrate the superiority of our SAGA
against the state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Next Steps: Learning a Disentangled Gait Representation for Versatile  Quadruped Locomotion</b></summary>
  <p><b>编号</b>：[97]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04809</p>
  <p><b>作者</b>：Alexander L. Mitchell,  Wolfgang Merkt,  Mathieu Geisert,  Siddhant Gangapurwala,  Martin Engelcke,  Oiwi Parker Jones,  Ioannis Havoutis,  Ingmar Posner</p>
  <p><b>备注</b>：8 pages, 6 figures, under review at Robotics and Automation Letters (RA-L)</p>
  <p><b>关键词</b>：latent state induces holistic plans synthesising, vary key gait parameters continuously, relatively narrow behaviour seen, key stance phases constituting, dynamic manoeuvres lies beyond</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Quadruped locomotion is rapidly maturing to a degree where robots now
routinely traverse a variety of unstructured terrains. However, while gaits can
be varied typically by selecting from a range of pre-computed styles, current
planners are unable to vary key gait parameters continuously while the robot is
in motion. The synthesis, on-the-fly, of gaits with unexpected operational
characteristics or even the blending of dynamic manoeuvres lies beyond the
capabilities of the current state-of-the-art. In this work we address this
limitation by learning a latent space capturing the key stance phases
constituting a particular gait. This is achieved via a generative model trained
on a single trot style, which encourages disentanglement such that application
of a drive signal to a single dimension of the latent state induces holistic
plans synthesising a continuous variety of trot styles. We demonstrate that
specific properties of the drive signal map directly to gait parameters such as
cadence, foot step height and full stance duration. Due to the nature of our
approach these synthesised gaits are continuously variable online during robot
operation and robustly capture a richness of movement significantly exceeding
the relatively narrow behaviour seen during training. In addition, the use of a
generative model facilitates the detection and mitigation of disturbances to
provide a versatile and robust planning framework. We evaluate our approach on
a real ANYmal quadruped robot and demonstrate that our method achieves a
continuous blend of dynamic trot styles whilst being robust and reactive to
external perturbations.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Effective dimension of machine learning models</b></summary>
  <p><b>编号</b>：[98]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04807</p>
  <p><b>作者</b>：Amira Abbas,  David Sutter,  Alessio Figalli,  Stefan Woerner</p>
  <p><b>备注</b>：17 pages, 2 figures</p>
  <p><b>关键词</b>：tasks involving new data, various capacity measures try, local effective dimension bounds, local effective dimension, standard data sets</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Making statements about the performance of trained models on tasks involving
new data is one of the primary goals of machine learning, i.e., to understand
the generalization power of a model. Various capacity measures try to capture
this ability, but usually fall short in explaining important characteristics of
models that we observe in practice. In this study, we propose the local
effective dimension as a capacity measure which seems to correlate well with
generalization error on standard data sets. Importantly, we prove that the
local effective dimension bounds the generalization error and discuss the
aptness of this capacity measure for machine learning models.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Combining Textual Features for the Detection of Hateful and Offensive  Language</b></summary>
  <p><b>编号</b>：[99]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04803</p>
  <p><b>作者</b>：Sherzod Hakimov,  Ralph Ewerth</p>
  <p><b>备注</b>：HASOC 2021, Forum for Information Retrieval Evaluation, 2021</p>
  <p><b>关键词</b>：critical challenge since many users, contextual word embeddings combined, combining different textual features, compared different variants, character level embeddings</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The detection of offensive, hateful and profane language has become a
critical challenge since many users in social networks are exposed to
cyberbullying activities on a daily basis. In this paper, we present an
analysis of combining different textual features for the detection of hateful
or offensive posts on Twitter. We provide a detailed experimental evaluation to
understand the impact of each building block in a neural network architecture.
The proposed architecture is evaluated on the English Subtask 1A: Identifying
Hate, offensive and profane content from the post datasets of HASOC-2021
dataset under the team name TIB-VA. We compared different variants of the
contextual word embeddings combined with the character level embeddings and the
encoding of collected hate terms.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：GPU backed Data Mining on Android Devices</b></summary>
  <p><b>编号</b>：[100]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04800</p>
  <p><b>作者</b>：Robert Fritze,  Claudia Plant</p>
  <p><b>备注</b>：11 pages</p>
  <p><b>关键词</b>：computationally intensive machine learning, already written opencl programs, harsh environmental conditions, opencl supports thread, data mining tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Choosing an appropriate programming paradigm for high-performance computing
on low-power devices can be useful to speed up calculations. Many Android
devices have an integrated GPU and - although not officially supported - the
OpenCL framework can be used on Android devices for addressing these GPUs.
OpenCL supports thread and data parallelism. Applications that use the GPU must
account for the fact that they can be suspended by the user or the Android
operating system at any moment. We have created a wrapper library that allows
to use OpenCL on Android devices. Already written OpenCL programs can be
executed with almost no modification. We have used this library to compare the
performance of the DBSCAN and Kmeans algorithms on an integrated GPU of an
Arm-v7 tablet with other single and multithreaded implementations on the same
device. We have investigated which programming paradigm and language allows the
best tradeoff between execution speed and energy consumption. Using the GPU for
HPC on Android devices can help to carry out computationally intensive machine
learning or data mining tasks in remote areas, under harsh environmental
conditions and in areas where energy supply is an issue.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Detecting Potentially Harmful and Protective Suicide-related Content on  Twitter: A Machine Learning Approach</b></summary>
  <p><b>编号</b>：[102]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04796</p>
  <p><b>作者</b>：Hannah Metzler,  Hubert Baginski,  Thomas Niederkrotenthaler,  David Garcia</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：deep learning models reach accuracy scores, two deep learning models achieved, classified six main content categories, machine learning models including, art deep learning models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Research shows that exposure to suicide-related news media content is
associated with suicide rates, with some content characteristics likely having
harmful and others potentially protective effects. Although good evidence
exists for a few selected characteristics, systematic large scale
investigations are missing in general, and in particular for social media data.
We apply machine learning methods to automatically label large quantities of
Twitter data. We developed a novel annotation scheme that classifies
suicide-related tweets into different message types and problem- vs.
solution-focused perspectives. We then trained a benchmark of machine learning
models including a majority classifier, an approach based on word frequency
(TF-IDF with a linear SVM) and two state-of-the-art deep learning models (BERT,
XLNet). The two deep learning models achieved the best performance in two
classification tasks: First, we classified six main content categories,
including personal stories about either suicidal ideation and attempts or
coping, calls for action intending to spread either problem awareness or
prevention-related information, reportings of suicide cases, and other
suicide-related and off-topic tweets. The deep learning models reach accuracy
scores above 73% on average across the six categories, and F1-scores in between
69% and 85% for all but the suicidal ideation and attempts category (55%).
Second, in separating postings referring to actual suicide from off-topic
tweets, they correctly labelled around 88% of tweets, with BERT achieving
F1-scores of 93% and 74% for the two categories. These classification
performances are comparable to the state-of-the-art on similar tasks. By making
data labeling more efficient, this work enables future large-scale
investigations on harmful and protective effects of various kinds of social
media content on suicide rates and on help-seeking behavior.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：VMAgent: Scheduling Simulator for Reinforcement Learning</b></summary>
  <p><b>编号</b>：[104]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04785</p>
  <p><b>作者</b>：Junjie Sheng,  Shengliang Cai,  Haochuan Cui,  Wenhao Li,  Yun Hua,  Bo Jin,  Wenli Zhou,  Yiqiu Hu,  Lei Zhu,  Qian Peng,  Hongyuan Zha,  Xiangfeng Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：customized scheduling environments considering different problem features, help rl researchers better explore new methods, many reinforcement learning challenges, novel simulator called vmagent, vmagent provides flexible configurations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A novel simulator called VMAgent is introduced to help RL researchers better
explore new methods, especially for virtual machine scheduling. VMAgent is
inspired by practical virtual machine (VM) scheduling tasks and provides an
efficient simulation platform that can reflect the real situations of cloud
computing. Three scenarios (fading, recovering, and expansion) are concluded
from practical cloud computing and corresponds to many reinforcement learning
challenges (high dimensional state and action spaces, high non-stationarity,
and life-long demand). VMAgent provides flexible configurations for RL
researchers to design their customized scheduling environments considering
different problem features. From the VM scheduling perspective, VMAgent also
helps to explore better learning-based scheduling solutions.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Adaptive Methods for Aggregated Domain Generalization</b></summary>
  <p><b>编号</b>：[107]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04766</p>
  <p><b>作者</b>：Xavier Thomas,  Dhruv Mahajan,  Alex Pentland,  Abhimanyu Dubey</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：domain generalization benchmarks without using domain labels whatsoever, privacy concerns prohibit obtaining domain labels, domain generalization using cluster information, makes predictions using information, domain generalization involves learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Domain generalization involves learning a classifier from a heterogeneous
collection of training sources such that it generalizes to data drawn from
similar unknown target domains, with applications in large-scale learning and
personalized inference. In many settings, privacy concerns prohibit obtaining
domain labels for the training data samples, and instead only have an
aggregated collection of training points. Existing approaches that utilize
domain labels to create domain-invariant feature representations are
inapplicable in this setting, requiring alternative approaches to learn
generalizable classifiers. In this paper, we propose a domain-adaptive approach
to this problem, which operates in two steps: (a) we cluster training data
within a carefully chosen feature space to create pseudo-domains, and (b) using
these pseudo-domains we learn a domain-adaptive classifier that makes
predictions using information about both the input and the pseudo-domain it
belongs to. Our approach achieves state-of-the-art performance on a variety of
domain generalization benchmarks without using domain labels whatsoever.
Furthermore, we provide novel theoretical guarantees on domain generalization
using cluster information. Our approach is amenable to ensemble-based methods
and provides substantial gains even on large-scale benchmark datasets. The code
can be found at: this https URL</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：3D-VField: Learning to Adversarially Deform Point Clouds for Robust 3D  Object Detection</b></summary>
  <p><b>编号</b>：[108]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04764</p>
  <p><b>作者</b>：Alexander Lehner,  Stefano Gasperini,  Alvaro Marcos-Ramiro,  Michael Schmidt,  Mohammad-Ali Nikouei Mahani,  Nassir Navab,  Benjamin Busam,  Federico Tombari</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：plausibly deforms objects via vectors learned, share open source crashd, account deformed point clouds, approach constrains 3d points, differently shaped objects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As 3D object detection on point clouds relies on the geometrical
relationships between the points, non-standard object shapes can hinder a
method's detection capability. However, in safety-critical settings, robustness
on out-of-distribution and long-tail samples is fundamental to circumvent
dangerous issues, such as the misdetection of damaged or rare cars. In this
work, we substantially improve the generalization of 3D object detectors to
out-of-domain data by taking into account deformed point clouds during
training. We achieve this with 3D-VField: a novel method that plausibly deforms
objects via vectors learned in an adversarial fashion. Our approach constrains
3D points to slide along their sensor view rays while neither adding nor
removing any of them. The obtained vectors are transferrable,
sample-independent and preserve shape smoothness and occlusions. By augmenting
normal samples with the deformations produced by these vector fields during
training, we significantly improve robustness against differently shaped
objects, such as damaged/deformed cars, even while training only on KITTI.
Towards this end, we propose and share open source CrashD: a synthetic dataset
of realistic damaged and rare cars, with a variety of crash scenarios.
Extensive experiments on KITTI, Waymo, our CrashD and SUN RGB-D show the high
generalizability of our techniques to out-of-domain data, different models and
sensors, namely LiDAR and ToF cameras, for both indoor and outdoor scenes. Our
CrashD dataset is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Does Redundancy in AI Perception Systems Help to Test for Super-Human  Automated Driving Performance?</b></summary>
  <p><b>编号</b>：[110]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04758</p>
  <p><b>作者</b>：Hanno Gottschalk,  Matthias Rottmann,  Maida Saltagic</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：labeled data needed would exceed dimensions, trained using special loss functions, 3d mnist data set, provide direct statistical evidence, commonly used strategy therefore</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While automated driving is often advertised with better-than-human driving
performance, this work reviews that it is nearly impossible to provide direct
statistical evidence on the system level that this is actually the case. The
amount of labeled data needed would exceed dimensions of present day technical
and economical capabilities. A commonly used strategy therefore is the use of
redundancy along with the proof of sufficient subsystems' performances. As it
is known, this strategy is efficient especially for the case of subsystems
operating independently, i.e. the occurrence of errors is independent in a
statistical sense. Here, we give some first considerations and experimental
evidence that this strategy is not a free ride as the errors of neural networks
fulfilling the same computer vision task, at least for some cases, show
correlated occurrences of errors. This remains true, if training data,
architecture, and training are kept separate or independence is trained using
special loss functions. Using data from different sensors (realized by up to
five 2D projections of the 3D MNIST data set) in our experiments is more
efficiently reducing correlations, however not to an extent that is realizing
the potential of reduction of testing data that can be obtained for redundant
and statistically independent subsystems.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Asynchronous Semi-Decentralized Federated Edge Learning for  Heterogeneous Clients</b></summary>
  <p><b>编号</b>：[119]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04737</p>
  <p><b>作者</b>：Yuchang Sun,  Jiawei Shao,  Yuyi Mao,  Jun Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：preserving distributed learning framework, low training latency enabled, multiple edge servers collaborate, federated edge learning, simulation results demonstrate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Federated edge learning (FEEL) has drawn much attention as a
privacy-preserving distributed learning framework for mobile edge networks. In
this work, we investigate a novel semi-decentralized FEEL (SD-FEEL)
architecture where multiple edge servers collaborate to incorporate more data
from edge devices in training. Despite the low training latency enabled by fast
edge aggregation, the device heterogeneity in computational resources
deteriorates the efficiency. This paper proposes an asynchronous training
algorithm for SD-FEEL to overcome this issue, where edge servers can
independently set deadlines for the associated client nodes and trigger the
model aggregation. To deal with different levels of staleness, we design a
staleness-aware aggregation scheme and analyze its convergence performance.
Simulation results demonstrate the effectiveness of our proposed algorithm in
achieving faster convergence and better learning performance.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：From Good to Best: Two-Stage Training for Cross-lingual Machine Reading  Comprehension</b></summary>
  <p><b>编号</b>：[120]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04735</p>
  <p><b>作者</b>：Nuo Chen,  Linjun Shou,  Min Gong,  Jian Pei,  Daxin Jiang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：previous approaches may often fail, recent approaches use training data, rich language like english, lingual mrc benchmark datasets, lingual machine reading comprehension</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cross-lingual Machine Reading Comprehension (xMRC) is challenging due to the
lack of training data in low-resource languages. The recent approaches use
training data only in a resource-rich language like English to fine-tune
large-scale cross-lingual pre-trained language models. Due to the big
difference between languages, a model fine-tuned only by a source language may
not perform well for target languages. Interestingly, we observe that while the
top-1 results predicted by the previous approaches may often fail to hit the
ground-truth answers, the correct answers are often contained in the top-k
predicted results. Based on this observation, we develop a two-stage approach
to enhance the model performance. The first stage targets at recall: we design
a hard-learning (HL) algorithm to maximize the likelihood that the top-k
predictions contain the accurate answer. The second stage focuses on precision:
an answer-aware contrastive learning (AA-CL) mechanism is developed to learn
the fine difference between the accurate answer and other candidates. Our
extensive experiments show that our model significantly outperforms a series of
strong baselines on two cross-lingual MRC benchmark datasets.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：New Tight Relaxations of Rank Minimization for Multi-Task Learning</b></summary>
  <p><b>编号</b>：[121]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04734</p>
  <p><b>作者</b>：Wei Chang,  Feiping Nie,  Rong Wang,  Xuelong Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：rank common yet latent subspace, rank structure shared across tasks, means learning multiple tasks jointly, optimal shared latent subspace, weighted based iterative strategy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multi-task learning has been observed by many researchers, which supposes
that different tasks can share a low-rank common yet latent subspace. It means
learning multiple tasks jointly is better than learning them independently. In
this paper, we propose two novel multi-task learning formulations based on two
regularization terms, which can learn the optimal shared latent subspace by
minimizing the exactly $k$ minimal singular values. The proposed regularization
terms are the more tight approximations of rank minimization than trace norm.
But it's an NP-hard problem to solve the exact rank minimization problem.
Therefore, we design a novel re-weighted based iterative strategy to solve our
models, which can tactically handle the exact rank minimization problem by
setting a large penalizing parameter. Experimental results on benchmark
datasets demonstrate that our methods can correctly recover the low-rank
structure shared across tasks, and outperform related multi-task learning
methods.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Mimicking the Oracle: An Initial Phase Decorrelation Approach for Class  Incremental Learning</b></summary>
  <p><b>编号</b>：[122]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04731</p>
  <p><b>作者</b>：Yujun Shi,  Kuangqi Zhou,  Jian Liang,  Zihang Jiang,  Jiashi Feng,  Philip Torr,  Song Bai,  Vincent Y.F. Tan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：previous works mainly focus, various benchmark datasets show, directly encouraging cil learner, since one major difference, around 1 \%</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Class Incremental Learning (CIL) aims at learning a multi-class classifier in
a phase-by-phase manner, in which only data of a subset of the classes are
provided at each phase. Previous works mainly focus on mitigating forgetting in
phases after the initial one. However, we find that improving CIL at its
initial phase is also a promising direction. Specifically, we experimentally
show that directly encouraging CIL Learner at the initial phase to output
similar representations as the model jointly trained on all classes can greatly
boost the CIL performance. Motivated by this, we study the difference between a
naïvely-trained initial-phase model and the oracle model. Specifically, since
one major difference between these two models is the number of training
classes, we investigate how such difference affects the model representations.
We find that, with fewer training classes, the data representations of each
class lie in a long and narrow region; with more training classes, the
representations of each class scatter more uniformly. Inspired by this
observation, we propose Class-wise Decorrelation (CwD) that effectively
regularizes representations of each class to scatter more uniformly, thus
mimicking the model jointly trained with all classes (i.e., the oracle model).
Our CwD is simple to implement and easy to plug into existing methods.
Extensive experiments on various benchmark datasets show that CwD consistently
and significantly improves the performance of existing state-of-the-art methods
by around 1\% to 3\%. Code will be released.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Reducing Catastrophic Forgetting in Self Organizing Maps with  Internally-Induced Generative Replay</b></summary>
  <p><b>编号</b>：[123]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04728</p>
  <p><b>作者</b>：Hitesh Vaidya,  Travis Desell,  Alexander Ororbia</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：one major historic difficulty, internal neurons might carry, receives data points related, task incremental data, pattern sensory data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A lifelong learning agent is able to continually learn from potentially
infinite streams of pattern sensory data. One major historic difficulty in
building agents that adapt in this way is that neural systems struggle to
retain previously-acquired knowledge when learning from new samples. This
problem is known as catastrophic forgetting (interference) and remains an
unsolved problem in the domain of machine learning to this day. While
forgetting in the context of feedforward networks has been examined extensively
over the decades, far less has been done in the context of alternative
architectures such as the venerable self-organizing map (SOM), an unsupervised
neural model that is often used in tasks such as clustering and dimensionality
reduction. Although the competition among its internal neurons might carry the
potential to improve memory retention, we observe that a fixed-sized SOM
trained on task incremental data, i.e., it receives data points related to
specific classes at certain temporal increments, experiences significant
forgetting. In this study, we propose the continual SOM (c-SOM), a model that
is capable of reducing its own forgetting when processing information.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Amicable Aid: Turning Adversarial Attack to Benefit Classification</b></summary>
  <p><b>编号</b>：[125]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04720</p>
  <p><b>作者</b>：Juyeop Kim,  Jun-Ho Choi,  Soobeom Jang,  Jong-Seok Lee</p>
  <p><b>备注</b>：16 pages (3 pages for appendix)</p>
  <p><b>关键词</b>：deep image classification models pose serious security concerns, also consider universal amicable perturbations, discuss several application scenarios, another yielding higher confidence, including secure image communication</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While adversarial attacks on deep image classification models pose serious
security concerns in practice, this paper suggests a novel paradigm where the
concept of adversarial attacks can benefit classification performance, which we
call amicable aid. We show that by taking the opposite search direction of
perturbation, an image can be converted to another yielding higher confidence
by the classification model and even a wrongly classified image can be made to
be correctly classified. Furthermore, with a large amount of perturbation, an
image can be made unrecognizable by human eyes, while it is correctly
recognized by the model. The mechanism of the amicable aid is explained in the
viewpoint of the underlying natural image manifold. We also consider universal
amicable perturbations, i.e., a fixed perturbation can be applied to multiple
images to improve their classification results. While it is challenging to find
such perturbations, we show that making the decision boundary as perpendicular
to the image manifold as possible via training with modified data is effective
to obtain a model for which universal amicable perturbations are more easily
found. Finally, we discuss several application scenarios where the amicable aid
can be useful, including secure image communication, privacy-preserving image
communication, and protection against adversarial attacks.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：DR3: Value-Based Deep Reinforcement Learning Requires Explicit  Regularization</b></summary>
  <p><b>编号</b>：[127]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04716</p>
  <p><b>作者</b>：Aviral Kumar,  Rishabh Agarwal,  Tengyu Ma,  Aaron Courville,  George Tucker,  Sergey Levine</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：deep network value function trained via bootstrapping, deep networks trained via supervised learning, resulting derived regularizer favors degenerate solutions, overparameterized deep networks enjoy, offline deep rl setting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite overparameterization, deep networks trained via supervised learning
are easy to optimize and exhibit excellent generalization. One hypothesis to
explain this is that overparameterized deep networks enjoy the benefits of
implicit regularization induced by stochastic gradient descent, which favors
parsimonious solutions that generalize well on test inputs. It is reasonable to
surmise that deep reinforcement learning (RL) methods could also benefit from
this effect. In this paper, we discuss how the implicit regularization effect
of SGD seen in supervised learning could in fact be harmful in the offline deep
RL setting, leading to poor generalization and degenerate feature
representations. Our theoretical analysis shows that when existing models of
implicit regularization are applied to temporal difference learning, the
resulting derived regularizer favors degenerate solutions with excessive
"aliasing", in stark contrast to the supervised learning case. We back up these
findings empirically, showing that feature representations learned by a deep
network value function trained via bootstrapping can indeed become degenerate,
aliasing the representations for state-action pairs that appear on either side
of the Bellman backup. To address this issue, we derive the form of this
implicit regularizer and, inspired by this derivation, propose a simple and
effective explicit regularizer, called DR3, that counteracts the undesirable
effects of this implicit regularizer. When combined with existing offline RL
methods, DR3 substantially improves performance and stability, alleviating
unlearning in Atari 2600 games, D4RL domains and robotic manipulation from
images.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Ymir: A Supervised Ensemble Framework for Multivariate Time Series  Anomaly Detection</b></summary>
  <p><b>编号</b>：[134]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04704</p>
  <p><b>作者</b>：Zhanxiang Zhao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ymir integrates several currentlywidely used unsupervised anomaly detection models, multivariate time series anomaly detection frame, supervised classifier todo anomaly detection, provide robust frontalanomaly detection results, ymir leveragesthe aforementioned unsupervised methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We proposed a multivariate time series anomaly detection frame-work Ymir,
which leverages ensemble learning and supervisedlearning technology to
efficiently learn and adapt to anomaliesin real-world system applications. Ymir
integrates several currentlywidely used unsupervised anomaly detection models
through anensemble learning method, and thus can provide robust frontalanomaly
detection results in unsupervised scenarios. In a super-vised setting, domain
experts and system users discuss and providelabels (anomalous or not) for the
training data, which reflects theiranomaly detection criteria for the specific
system. Ymir leveragesthe aforementioned unsupervised methods to extract rich
and usefulfeature representations from the raw multivariate time series
data,then combines the features and labels with a supervised classifier todo
anomaly detection. We evaluated Ymir on internal multivariatetime series
datasets from large monitoring systems and achievedgood anomaly detection
performance.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Trajectory-Constrained Deep Latent Visual Attention for Improved Local  Planning in Presence of Heterogeneous Terrain</b></summary>
  <p><b>编号</b>：[143]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04684</p>
  <p><b>作者</b>：Stefan Wapnick,  Travis Manderson,  David Meger,  Gregory Dudek</p>
  <p><b>备注</b>：Published in International Conference on Intelligent Robots and Systems (IROS) 2021 proceedings. Project website: this https URL</p>
  <p><b>关键词</b>：experiments involved randomized procedural generated simulation, based deep learning method featuring trajectory, latent feature map space instead, allowing adaptability yet encouraging, local visual navigation tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a reward-predictive, model-based deep learning method featuring
trajectory-constrained visual attention for use in mapless, local visual
navigation tasks. Our method learns to place visual attention at locations in
latent image space which follow trajectories caused by vehicle control actions
to enhance predictive accuracy during planning. The attention model is jointly
optimized by the task-specific loss and an additional trajectory-constraint
loss, allowing adaptability yet encouraging a regularized structure for
improved generalization and reliability. Importantly, visual attention is
applied in latent feature map space instead of raw image space to promote
efficient planning. We validated our model in visual navigation tasks of
planning low turbulence, collision-free trajectories in off-road settings and
hill climbing with locking differentials in the presence of slippery terrain.
Experiments involved randomized procedural generated simulation and real-world
environments. We found our method improved generalization and learning
efficiency when compared to no-attention and self-attention alternatives.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Clairvoyance: Intelligent Route Planning for Electric Buses Based on  Urban Big Data</b></summary>
  <p><b>编号</b>：[145]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04682</p>
  <p><b>作者</b>：Xiangyong Lu,  Kaoru Ota,  Mianxiong Dong,  Chen Yu,  Hai Jin</p>
  <p><b>备注</b>：13 pages,12 figures</p>
  <p><b>关键词</b>：planning electric bus routes intelligently, nowadays many cities around, reduce local carbon emissions, future transportation carbon emission, recommend bus routes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Nowadays many cities around the world have introduced electric buses to
optimize urban traffic and reduce local carbon emissions. In order to cut
carbon emissions and maximize the utility of electric buses, it is important to
choose suitable routes for them. Traditionally, route selection is on the basis
of dedicated surveys, which are costly in time and labor. In this paper, we
mainly focus attention on planning electric bus routes intelligently, depending
on the unique needs of each region throughout the city. We propose
Clairvoyance, a route planning system that leverages a deep neural network and
a multilayer perceptron to predict the future people's trips and the future
transportation carbon emission in the whole city, respectively. Given the
future information of people's trips and transportation carbon emission, we
utilize a greedy mechanism to recommend bus routes for electric buses that will
depart in an ideal state. Furthermore, representative features of the two
neural networks are extracted from the heterogeneous urban datasets. We
evaluate our approach through extensive experiments on real-world data sources
in Zhuhai, China. The results show that our designed neural network-based
algorithms are consistently superior to the typical baselines. Additionally,
the recommended routes for electric buses are helpful in reducing the peak
value of carbon emissions and making full use of electric buses in the city.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：A Fully Single Loop Algorithm for Bilevel Optimization without Hessian  Inverse</b></summary>
  <p><b>编号</b>：[152]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04660</p>
  <p><b>作者</b>：Junyi Li,  Bin Gu,  Heng Huang</p>
  <p><b>备注</b>：To appear in AAAI 2022</p>
  <p><b>关键词</b>：new hessian inverse free fully single loop algorithm, multiple bilevel optimization based machine learning tasks, yet achieve fully single loop, (\ epsilon ^{- 2 })$., encompasses several previous common approaches</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we propose a new Hessian inverse free Fully Single Loop
Algorithm (FSLA) for bilevel optimization problems. Classic algorithms for
bilevel optimization admit a double loop structure which is computationally
expensive. Recently, several single loop algorithms have been proposed with
optimizing the inner and outer variable alternatively. However, these
algorithms not yet achieve fully single loop. As they overlook the loop needed
to evaluate the hyper-gradient for a given inner and outer state. In order to
develop a fully single loop algorithm, we first study the structure of the
hyper-gradient and identify a general approximation formulation of
hyper-gradient computation that encompasses several previous common approaches,
e.g. back-propagation through time, conjugate gradient, \emph{etc.} Based on
this formulation, we introduce a new state variable to maintain the historical
hyper-gradient information. Combining our new formulation with the alternative
update of the inner and outer variables, we propose an efficient fully single
loop algorithm. We theoretically show that the error generated by the new state
can be bounded and our algorithm converges with the rate of $O(\epsilon^{-2})$.
Finally, we verify the efficacy our algorithm empirically through multiple
bilevel optimization based machine learning tasks.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：BACON: Band-limited Coordinate Networks for Multiscale Scene  Representation</b></summary>
  <p><b>编号</b>：[153]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04645</p>
  <p><b>作者</b>：David B. Lindell,  Dave Van Veen,  Jeong Joon Park,  Gordon Wetzstein</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：3d scenes using signed distance functions, multiple scales without explicit supervision, map continuous input coordinates, analytical fourier spectrum, outperforms conventional single</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Coordinate-based networks have emerged as a powerful tool for 3D
representation and scene reconstruction. These networks are trained to map
continuous input coordinates to the value of a signal at each point. Still,
current architectures are black boxes: their spectral characteristics cannot be
easily analyzed, and their behavior at unsupervised points is difficult to
predict. Moreover, these networks are typically trained to represent a signal
at a single scale, and so naive downsampling or upsampling results in
artifacts. We introduce band-limited coordinate networks (BACON), a network
architecture with an analytical Fourier spectrum. BACON has predictable
behavior at unsupervised points, can be designed based on the spectral
characteristics of the represented signal, and can represent signals at
multiple scales without explicit supervision. We demonstrate BACON for
multiscale neural representation of images, radiance fields, and 3D scenes
using signed distance functions and show that it outperforms conventional
single-scale coordinate networks in terms of interpretability and quality.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Autoregressive Quantile Flows for Predictive Uncertainty Estimation</b></summary>
  <p><b>编号</b>：[154]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04643</p>
  <p><b>作者</b>：Phillip Si,  Allan Bishop,  Volodymyr Kuleshov</p>
  <p><b>备注</b>：9 pages, 4 figures, 6 tables (main body) additional 4 pages, 2 figures, 4 tables (appendix)</p>
  <p><b>关键词</b>：machine learning involve predicting flexible probability distributions, accurately capture predictive aleatoric uncertainties, parameterize predictive conditional distributions, propose autoregressive quantile flows, autoregressive flows trained using</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Numerous applications of machine learning involve predicting flexible
probability distributions over model outputs. We propose Autoregressive
Quantile Flows, a flexible class of probabilistic models over high-dimensional
variables that can be used to accurately capture predictive aleatoric
uncertainties. These models are instances of autoregressive flows trained using
a novel objective based on proper scoring rules, which simplifies the
calculation of computationally expensive determinants of Jacobians during
training and supports new types of neural architectures. We demonstrate that
these models can be used to parameterize predictive conditional distributions
and improve the quality of probabilistic predictions on time series forecasting
and object detection.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Differentially Private Ensemble Classifiers for Data Streams</b></summary>
  <p><b>编号</b>：[156]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04640</p>
  <p><b>作者</b>：Lovedeep Gondara,  Ke Wang,  Ricardo Silva Carvalho</p>
  <p><b>备注</b>：Accepted at WSDM 2022</p>
  <p><b>关键词</b>：continuous data streams via classification, trained differentially private classification, differentially private ensemble solution, ending data streams, protecting data owners</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Learning from continuous data streams via classification/regression is
prevalent in many domains. Adapting to evolving data characteristics (concept
drift) while protecting data owners' private information is an open challenge.
We present a differentially private ensemble solution to this problem with two
distinguishing features: it allows an \textit{unbounded} number of ensemble
updates to deal with the potentially never-ending data streams under a fixed
privacy budget, and it is \textit{model agnostic}, in that it treats any
pre-trained differentially private classification/regression model as a
black-box. Our method outperforms competitors on real-world and simulated
datasets for varying settings of privacy, concept drift, and data distribution.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Transferability Properties of Graph Neural Networks</b></summary>
  <p><b>编号</b>：[162]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04629</p>
  <p><b>作者</b>：Luana Ruiz,  Luiz F. O. Chamon,  Alejandro Ribeiro</p>
  <p><b>备注</b>：Submitted to IEEE TSP</p>
  <p><b>关键词</b>：deep convolutional architectures consisting, gnns -- graphon convolutions, requires matrix computations, graphon neural networks, define limit objects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Graph neural networks (GNNs) are deep convolutional architectures consisting
of layers composed by graph convolutions and pointwise nonlinearities. Due to
their invariance and stability properties, GNNs are provably successful at
learning representations from network data. However, training them requires
matrix computations which can be expensive for large graphs. To address this
limitation, we investigate the ability of GNNs to be transferred across graphs.
We consider graphons, which are both graph limits and generative models for
weighted and stochastic graphs, to define limit objects of graph convolutions
and GNNs -- graphon convolutions and graphon neural networks (WNNs) -- which we
use as generative models for graph convolutions and GNNs. We show that these
graphon filters and WNNs can be approximated by graph filters and GNNs sampled
from them on weighted and stochastic graphs. Using these results, we then
derive error bounds for transferring graph filters and GNNs across such graphs.
These bounds show that transferability increases with the graph size, and
reveal a tradeoff between transferability and spectral discriminability which
in GNNs is alleviated by the pointwise nonlinearities. These findings are
further verified empirically in numerical experiments in movie recommendation
and decentralized robot control.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Calibration Improves Bayesian Optimization</b></summary>
  <p><b>编号</b>：[166]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04620</p>
  <p><b>作者</b>：Shachi Deshpande,  Volodymyr Kuleshov</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：objective function violates assumptions made within, bayesian optimization makes better decisions, g ., gaussianity )., hyperparameter optimization tasks, standard benchmark functions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Bayesian optimization is a procedure that allows obtaining the global optimum
of black-box functions and that is useful in applications such as
hyper-parameter optimization. Uncertainty estimates over the shape of the
objective function are instrumental in guiding the optimization process.
However, these estimates can be inaccurate if the objective function violates
assumptions made within the underlying model (e.g., Gaussianity). We propose a
simple algorithm to calibrate the uncertainty of posterior distributions over
the objective function as part of the Bayesian optimization process. We show
that by improving the uncertainty estimates of the posterior distribution with
calibration, Bayesian optimization makes better decisions and arrives at the
global optimum in fewer steps. We show that this technique improves the
performance of Bayesian optimization on standard benchmark functions and
hyperparameter optimization tasks.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：Gaussian Process Constraint Learning for Scalable Chance-Constrained  Motion Planning from Demonstrations</b></summary>
  <p><b>编号</b>：[169]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04612</p>
  <p><b>作者</b>：Glen Chou,  Hao Wang,  Dmitry Berenson</p>
  <p><b>备注</b>：Under review at RA-L + ICRA 2022</p>
  <p><b>关键词</b>：outperforming previous constraint learning methods, requiring minimal prior information, learning constraints represented, specified safety probability, nonlinear constraints demonstrated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a method for learning constraints represented as Gaussian
processes (GPs) from locally-optimal demonstrations. Our approach uses the
Karush-Kuhn-Tucker (KKT) optimality conditions to determine where on the
demonstrations the constraint is tight, and a scaling of the constraint
gradient at those states. We then train a GP representation of the constraint
which is consistent with and which generalizes this information. We further
show that the GP uncertainty can be used within a kinodynamic RRT to plan
probabilistically-safe trajectories, and that we can exploit the GP structure
within the planner to exactly achieve a specified safety probability. We
demonstrate our method can learn complex, nonlinear constraints demonstrated on
a 5D nonholonomic car, a 12D quadrotor, and a 3-link planar arm, all while
requiring minimal prior information on the constraint. Our results suggest the
learned GP constraint is accurate, outperforming previous constraint learning
methods that require more a priori knowledge.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：Enhancing Food Intake Tracking in Long-Term Care with Automated Food  Imaging and Nutrient Intake Tracking (AFINI-T) Technology</b></summary>
  <p><b>编号</b>：[171]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04608</p>
  <p><b>作者</b>：Kaylen J. Pfisterer,  Robert Amelard,  Jennifer Boger,  Audrey G. Chung,  Heather H. Keller,  Alexander Wong</p>
  <p><b>备注</b>：Key words: Automatic segmentation, convolutional neural network, deep learning, food intake tracking, volume estimation, malnutrition prevention, long-term care, hospital</p>
  <p><b>关键词</b>：learning powered computational nutrient sensing system, objectively tracking ltc resident food intake, 9 %; mean intake error, simulated ltc food intake dataset, prevent malnutrition tracking strategies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Half of long-term care (LTC) residents are malnourished increasing
hospitalization, mortality, morbidity, with lower quality of life. Current
tracking methods are subjective and time consuming. This paper presents the
automated food imaging and nutrient intake tracking (AFINI-T) technology
designed for LTC. We propose a novel convolutional autoencoder for food
classification, trained on an augmented UNIMIB2016 dataset and tested on our
simulated LTC food intake dataset (12 meal scenarios; up to 15 classes each;
top-1 classification accuracy: 88.9%; mean intake error: -0.4 mL$\pm$36.7 mL).
Nutrient intake estimation by volume was strongly linearly correlated with
nutrient estimates from mass ($r^2$ 0.92 to 0.99) with good agreement between
methods ($\sigma$= -2.7 to -0.01; zero within each of the limits of agreement).
The AFINI-T approach is a deep-learning powered computational nutrient sensing
system that may provide a novel means for more accurately and objectively
tracking LTC resident food intake to support and prevent malnutrition tracking
strategies.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Prediction of Adverse Biological Effects of Chemicals Using Knowledge  Graph Embeddings</b></summary>
  <p><b>编号</b>：[173]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04605</p>
  <p><b>作者</b>：Erik B. Myklebust,  Ernesto Jiménez-Ruiz,  Jiaoyan Chen,  Raoul Wolf,  Knut Erik Tollefsen</p>
  <p><b>备注</b>：Accepted for publication in the Semantic Web Journal</p>
  <p><b>关键词</b>：evaluated nine knowledge graph embedding models, knowledge graph embedding models, major data sources used, using knowledge graph embeddings, namely chemical effect prediction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We have created a knowledge graph based on major data sources used in
ecotoxicological risk assessment. We have applied this knowledge graph to an
important task in risk assessment, namely chemical effect prediction. We have
evaluated nine knowledge graph embedding models from a selection of geometric,
decomposition, and convolutional models on this prediction task. We show that
using knowledge graph embeddings can increase the accuracy of effect prediction
with neural networks. Furthermore, we have implemented a fine-tuning
architecture which adapts the knowledge graph embeddings to the effect
prediction task and leads to a better performance. Finally, we evaluate certain
characteristics of the knowledge graph embedding models to shed light on the
individual model performance.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：Regularization methods for the short-term forecasting of the Italian  electric load</b></summary>
  <p><b>编号</b>：[174]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04604</p>
  <p><b>作者</b>：Alessandro Incremona,  Giuseppe De Nicolao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：control via alternative regularization methods, italian transmission system operator terna, daily mean absolute percentage error, hourly mean absolute percentage error, three test years considered</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The problem of forecasting the whole 24 profile of the Italian electric load
is addressed as a multitask learning problem, whose complexity is kept under
control via alternative regularization methods. In view of the quarter-hourly
samplings, 96 predictors are used, each of which linearly depends on 96
regressors. The 96x96 matrix weights form a 96x96 matrix, that can be seen and
displayed as a surface sampled on a square domain. Different regularization and
sparsity approaches to reduce the degrees of freedom of the surface were
explored, comparing the obtained forecasts with those of the Italian
Transmission System Operator Terna. Besides outperforming Terna in terms of
quarter-hourly mean absolute percentage error and mean absolute error, the
prediction residuals turned out to be weakly correlated with Terna, which
suggests that further improvement could ensue from forecasts aggregation. In
fact, the aggregated forecasts yielded further relevant drops in terms of
quarter-hourly and daily mean absolute percentage error, mean absolute error
and root mean square error (up to 30%) over the three test years considered.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：InvGAN: Invertable GANs</b></summary>
  <p><b>编号</b>：[177]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04598</p>
  <p><b>作者</b>：Partha Ghosh,  Dominik Zietlow,  Michael J. Black,  Larry S. Davis,  Xiaochen Hu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：high resolution generative models, successfully embeds real images, high quality generative model, real images using, generative model together</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generation of photo-realistic images, semantic editing and representation
learning are a few of many potential applications of high resolution generative
models. Recent progress in GANs have established them as an excellent choice
for such tasks. However, since they do not provide an inference model, image
editing or downstream tasks such as classification can not be done on real
images using the GAN latent space. Despite numerous efforts to train an
inference model or design an iterative method to invert a pre-trained
generator, previous methods are dataset (e.g. human face images) and
architecture (e.g. StyleGAN) specific. These methods are nontrivial to extend
to novel datasets or architectures. We propose a general framework that is
agnostic to architecture and datasets. Our key insight is that, by training the
inference and the generative model together, we allow them to adapt to each
other and to converge to a better quality model. Our \textbf{InvGAN}, short for
Invertable GAN, successfully embeds real images to the latent space of a high
quality generative model. This allows us to perform image inpainting, merging,
interpolation and online data augmentation. We demonstrate this with extensive
qualitative and quantitative experiments.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：Variational Regularization in Inverse Problems and Machine Learning</b></summary>
  <p><b>编号</b>：[179]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04591</p>
  <p><b>作者</b>：Martin Burger</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：quantitative estimates respectively needed ingredients, review basic properties needed, paper discusses basic results, also discuss variational regularization, convergent regularization scheme</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper discusses basic results and recent developments on variational
regularization methods, as developed for inverse problems. In a typical setup
we review basic properties needed to obtain a convergent regularization scheme
and further discuss the derivation of quantitative estimates respectively
needed ingredients such as Bregman distances for convex functionals.
In addition to the approach developed for inverse problems we will also
discuss variational regularization in machine learning and work out some
connections to the classical regularization theory. In particular we will
discuss a reinterpretation of machine learning problems in the framework of
regularization theory and a reinterpretation of variational methods for inverse
problems in the framework of risk minimization. Moreover, we establish some
previously unknown connections between error estimates in Bregman distances and
generalization errors.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：The perils of being unhinged: On the accuracy of classifiers minimizing  a noise-robust convex loss</b></summary>
  <p><b>编号</b>：[180]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04590</p>
  <p><b>作者</b>：Philip M. Long,  Rocco A. Servedio</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：simple linearly separable data distributions, van rooyen et al, random classification noise, convex loss functions, binary classifiers obtained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>van Rooyen et al. introduced a notion of convex loss functions being robust
to random classification noise, and established that the "unhinged" loss
function is robust in this sense. In this note we study the accuracy of binary
classifiers obtained by minimizing the unhinged loss, and observe that even for
simple linearly separable data distributions, minimizing the unhinged loss may
only yield a binary classifier with accuracy no better than random guessing.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：STAF: A Spatio-Temporal Attention Fusion Network for Few-shot Video  Classification</b></summary>
  <p><b>编号</b>：[182]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04585</p>
  <p><b>作者</b>：Rex Liu,  Huanle Zhang,  Hamed Pirsiavash,  Xin Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：3d convolution neural networks embedding network, extracted features using self, staf first extracts coarse, g ., staf increases, temporal attention fusion network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose STAF, a Spatio-Temporal Attention Fusion network for few-shot
video classification. STAF first extracts coarse-grained spatial and temporal
features of videos by applying a 3D Convolution Neural Networks embedding
network. It then fine-tunes the extracted features using self-attention and
cross-attention networks. Last, STAF applies a lightweight fusion network and a
nearest neighbor classifier to classify each query video. To evaluate STAF, we
conduct extensive experiments on three benchmarks (UCF101, HMDB51, and
Something-Something-V2). The experimental results show that STAF improves
state-of-the-art accuracy by a large margin, e.g., STAF increases the five-way
one-shot accuracy by 5.3% and 7.0% for UCF101 and HMDB51, respectively.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：Estimating Divergences in High Dimensions</b></summary>
  <p><b>编号</b>：[183]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04583</p>
  <p><b>作者</b>：Loong Kuan Lee,  Nico Piatkowski,  François Petitjean,  Geoffrey I. Webb</p>
  <p><b>备注</b>：13 pages, 6 Figures. Submitted to IEEE Transactions on Pattern Analysis and Machine Intelligence</p>
  <p><b>关键词</b>：maximum likelihood estimator outperforms existing methods, although previous methods perform well, leibler divergence using decomposable models, 2 high dimensional distributions, using decomposable models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The problem of estimating the divergence between 2 high dimensional
distributions with limited samples is an important problem in various fields
such as machine learning. Although previous methods perform well with moderate
dimensional data, their accuracy starts to degrade in situations with 100s of
binary variables. Therefore, we propose the use of decomposable models for
estimating divergences in high dimensional data. These allow us to factorize
the estimated density of the high-dimensional distribution into a product of
lower dimensional functions. We conduct formal and experimental analyses to
explore the properties of using decomposable models in the context of
divergence estimation. To this end, we show empirically that estimating the
Kullback-Leibler divergence using decomposable models from a maximum likelihood
estimator outperforms existing methods for divergence estimation in situations
where dimensionality is high and useful decomposable models can be learnt from
the available data.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：Adaptive Kernel Graph Neural Network</b></summary>
  <p><b>编号</b>：[186]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04575</p>
  <p><b>作者</b>：Mingxuan Ju,  Shifu Hou,  Yujie Fan,  Jianan Zhao,  Liang Zhao,  Yanfang Ye</p>
  <p><b>备注</b>：To be appear at AAAI2022</p>
  <p><b>关键词</b>：e ., namely adaptive kernel graph neural network, driven graph kernel learning mechanism, kernel would entail sub, defined kernels may restrain, graph neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Graph neural networks (GNNs) have demonstrated great success in
representation learning for graph-structured data. The layer-wise graph
convolution in GNNs is shown to be powerful at capturing graph topology. During
this process, GNNs are usually guided by pre-defined kernels such as Laplacian
matrix, adjacency matrix, or their variants. However, the adoptions of
pre-defined kernels may restrain the generalities to different graphs: mismatch
between graph and kernel would entail sub-optimal performance. For example,
GNNs that focus on low-frequency information may not achieve satisfactory
performance when high-frequency information is significant for the graphs, and
vice versa. To solve this problem, in this paper, we propose a novel framework
- i.e., namely Adaptive Kernel Graph Neural Network (AKGNN) - which learns to
adapt to the optimal graph kernel in a unified manner at the first attempt. In
the proposed AKGNN, we first design a data-driven graph kernel learning
mechanism, which adaptively modulates the balance between all-pass and low-pass
filters by modifying the maximal eigenvalue of the graph Laplacian. Through
this process, AKGNN learns the optimal threshold between high and low frequency
signals to relieve the generality problem. Later, we further reduce the number
of parameters by a parameterization trick and enhance the expressive power by a
global readout function. Extensive experiments are conducted on acknowledged
benchmark datasets and promising results demonstrate the outstanding
performance of our proposed AKGNN by comparison with state-of-the-art GNNs. The
source code is publicly available at: this https URL.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：Application of Artificial Intelligence and Machine Learning in  Libraries: A Systematic Review</b></summary>
  <p><b>编号</b>：[187]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04573</p>
  <p><b>作者</b>：Rajesh Kumar Das,  Mohammad Sharif Ul Islam</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：edge technologies like artificial intelligence, anticipating future innovation pathways, lis domain mainly focuses, information professionals involve research, empirical studies exploring application</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As the concept and implementation of cutting-edge technologies like
artificial intelligence and machine learning has become relevant, academics,
researchers and information professionals involve research in this area. The
objective of this systematic literature review is to provide a synthesis of
empirical studies exploring application of artificial intelligence and machine
learning in libraries. To achieve the objectives of the study, a systematic
literature review was conducted based on the original guidelines proposed by
Kitchenham et al. (2009). Data was collected from Web of Science, Scopus, LISA
and LISTA databases. Following the rigorous/ established selection process, a
total of thirty-two articles were finally selected, reviewed and analyzed to
summarize on the application of AI and ML domain and techniques which are most
often used in libraries. Findings show that the current state of the AI and ML
research that is relevant with the LIS domain mainly focuses on theoretical
works. However, some researchers also emphasized on implementation projects or
case studies. This study will provide a panoramic view of AI and ML in
libraries for researchers, practitioners and educators for furthering the more
technology-oriented approaches, and anticipating future innovation pathways.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：Merging Subject Matter Expertise and Deep Convolutional Neural Network  for State-Based Online Machine-Part Interaction Classification</b></summary>
  <p><b>编号</b>：[188]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04572</p>
  <p><b>作者</b>：Hao Wang,  Yassine Qamsane,  James Moyne,  Kira Barton</p>
  <p><b>备注</b>：Published at ASME Manufacturing Science and Engineering Conference (MSEC)</p>
  <p><b>关键词</b>：sme defined finite state machine, deep convolutional neural network, time series classification, time series classification, provides temporal information</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine-part interaction classification is a key capability required by
Cyber-Physical Systems (CPS), a pivotal enabler of Smart Manufacturing (SM).
While previous relevant studies on the subject have primarily focused on time
series classification, change point detection is equally important because it
provides temporal information on changes in behavior of the machine. In this
work, we address point detection and time series classification for
machine-part interactions with a deep Convolutional Neural Network (CNN) based
framework. The CNN in this framework utilizes a two-stage encoder-classifier
structure for efficient feature representation and convenient deployment
customization for CPS. Though data-driven, the design and optimization of the
framework are Subject Matter Expertise (SME) guided. An SME defined Finite
State Machine (FSM) is incorporated into the framework to prohibit intermittent
misclassifications. In the case study, we implement the framework to perform
machine-part interaction classification on a milling machine, and the
performance is evaluated using a testing dataset and deployment simulations.
The implementation achieved an average F1-Score of 0.946 across classes on the
testing dataset and an average delay of 0.24 seconds on the deployment
simulations.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：Ambiguous Dynamic Treatment Regimes: A Reinforcement Learning Approach</b></summary>
  <p><b>编号</b>：[189]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04571</p>
  <p><b>作者</b>：Soroush Saghafian</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：develop two reinforcement learning methods termed direct augmented v, new class termed ambiguous dynamic treatment regimes, ambiguous partially observable mark decision processes, one often faces ambiguity regarding, finding optimal dtrs often rely</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A main research goal in various studies is to use an observational data set
and provide a new set of counterfactual guidelines that can yield causal
improvements. Dynamic Treatment Regimes (DTRs) are widely studied to formalize
this process. However, available methods in finding optimal DTRs often rely on
assumptions that are violated in real-world applications (e.g., medical
decision-making or public policy), especially when (a) the existence of
unobserved confounders cannot be ignored, and (b) the unobserved confounders
are time-varying (e.g., affected by previous actions). When such assumptions
are violated, one often faces ambiguity regarding the underlying causal model
that is needed to be assumed to obtain an optimal DTR. This ambiguity is
inevitable, since the dynamics of unobserved confounders and their causal
impact on the observed part of the data cannot be understood from the observed
data. Motivated by a case study of finding superior treatment regimes for
patients who underwent transplantation in our partner hospital and faced a
medical condition known as New Onset Diabetes After Transplantation (NODAT), we
extend DTRs to a new class termed Ambiguous Dynamic Treatment Regimes (ADTRs),
in which the casual impact of treatment regimes is evaluated based on a "cloud"
of potential causal models. We then connect ADTRs to Ambiguous Partially
Observable Mark Decision Processes (APOMDPs) proposed by Saghafian (2018), and
develop two Reinforcement Learning methods termed Direct Augmented V-Learning
(DAV-Learning) and Safe Augmented V-Learning (SAV-Learning), which enable using
the observed data to efficiently learn an optimal treatment regime. We
establish theoretical results for these learning methods, including (weak)
consistency and asymptotic normality. We further evaluate the performance of
these learning methods both in our case study and in simulation experiments.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：CoSSL: Co-Learning of Representation and Classifier for Imbalanced  Semi-Supervised Learning</b></summary>
  <p><b>编号</b>：[192]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04564</p>
  <p><b>作者</b>：Yue Fan,  Dengxin Dai,  Bernt Schiele</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：various shifted test distributions, balanced test sets, made publicly available, class feature enhancement, benchmark datasets ranging</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we propose a novel co-learning framework (CoSSL) with
decoupled representation learning and classifier learning for imbalanced SSL.
To handle the data imbalance, we devise Tail-class Feature Enhancement (TFE)
for classifier learning. Furthermore, the current evaluation protocol for
imbalanced SSL focuses only on balanced test sets, which has limited
practicality in real-world scenarios. Therefore, we further conduct a
comprehensive evaluation under various shifted test distributions. In
experiments, we show that our approach outperforms other methods over a large
range of shifted distributions, achieving state-of-the-art performance on
benchmark datasets ranging from CIFAR-10, CIFAR-100, ImageNet, to Food-101. Our
code will be made publicly available.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：SoK: Anti-Facial Recognition Technology</b></summary>
  <p><b>编号</b>：[195]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04558</p>
  <p><b>作者</b>：Emily Wenger,  Shawn Shan,  Haitao Zheng,  Ben Y. Zhao</p>
  <p><b>备注</b>：13 pages</p>
  <p><b>关键词</b>：help users avoid unwanted facial recognition, social challenges facing afr tools, first comprehensive analysis, broader design space, different afr approaches</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rapid adoption of facial recognition (FR) technology by both government
and commercial entities in recent years has raised concerns about civil
liberties and privacy. In response, a broad suite of so-called "anti-facial
recognition" (AFR) tools has been developed to help users avoid unwanted facial
recognition. The set of AFR tools proposed in the last few years is
wide-ranging and rapidly evolving, necessitating a step back to consider the
broader design space of AFR systems and long-term challenges. This paper aims
to fill that gap and provides the first comprehensive analysis of the AFR
research landscape. Using the operational stages of FR systems as a starting
point, we create a systematic framework for analyzing the benefits and
tradeoffs of different AFR approaches. We then consider both technical and
social challenges facing AFR tools and propose directions for future research
in this field.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：Whose Ground Truth? Accounting for Individual and Collective Identities  Underlying Dataset Annotation</b></summary>
  <p><b>编号</b>：[196]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04554</p>
  <p><b>作者</b>：Emily Denton,  Mark Díaz,  Ian Kivlichan,  Vinodkumar Prabhakaran,  Rachel Rosen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ethical considerations around crowdsourced dataset annotation, space along two layers, received nearly enough attention, ethical considerations around, ml data pipeline</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Human annotations play a crucial role in machine learning (ML) research and
development. However, the ethical considerations around the processes and
decisions that go into building ML datasets has not received nearly enough
attention. In this paper, we survey an array of literature that provides
insights into ethical considerations around crowdsourced dataset annotation. We
synthesize these insights, and lay out the challenges in this space along two
layers: (1) who the annotator is, and how the annotators' lived experiences can
impact their annotations, and (2) the relationship between the annotators and
the crowdsourcing platforms and what that relationship affords them. Finally,
we put forth a concrete set of recommendations and considerations for dataset
developers at various stages of the ML data pipeline: task formulation,
selection of annotators, platform and infrastructure choices, dataset analysis
and evaluation, and dataset documentation and release.</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：PATO: Producibility-Aware Topology Optimization using Deep Learning for  Metal Additive Manufacturing</b></summary>
  <p><b>编号</b>：[197]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04552</p>
  <p><b>作者</b>：Naresh S. Iyer,  Amir M. Mirzendehdel,  Sathyanarayanan Raghavan,  Yang Jiao,  Erva Ulu,  Morad Behandish,  Saigopal Nelaturi,  Dean M. Robinson</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：components fabricated using metal additive manufacturing, high residual stress values generated, steep thermal gradients produced, laser powder bed fusion, deep convolutional neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we propose PATO-a producibility-aware topology optimization
(TO) framework to help efficiently explore the design space of components
fabricated using metal additive manufacturing (AM), while ensuring
manufacturability with respect to cracking. Specifically, parts fabricated
through Laser Powder Bed Fusion are prone to defects such as warpage or
cracking due to high residual stress values generated from the steep thermal
gradients produced during the build process. Maturing the design for such parts
and planning their fabrication can span months to years, often involving
multiple handoffs between design and manufacturing engineers. PATO is based on
the a priori discovery of crack-free designs, so that the optimized part can be
built defect-free at the outset. To ensure that the design is crack free during
optimization, producibility is explicitly encoded within the standard
formulation of TO, using a crack index. Multiple crack indices are explored and
using experimental validation, maximum shear strain index (MSSI) is shown to be
an accurate crack index. Simulating the build process is a coupled,
multi-physics computation and incorporating it in the TO loop can be
computationally prohibitive. We leverage the current advances in deep
convolutional neural networks and present a high-fidelity surrogate model based
on an Attention-based U-Net architecture to predict the MSSI values as a
spatially varying field over the part's domain. Further, we employ automatic
differentiation to directly compute the gradient of maximum MSSI with respect
to the input design variables and augment it with the performance-based
sensitivity field to optimize the design while considering the trade-off
between weight, manufacturability, and functionality. We demonstrate the
effectiveness of the proposed method through benchmark studies in 3D as well as
experimental validation.</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：Deep Q-Learning Market Makers in a Multi-Agent Simulated Stock Market</b></summary>
  <p><b>编号</b>：[206]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04494</p>
  <p><b>作者</b>：Oscar Fernández Vicente,  Fernando Fernández Rebollo,  Francisco Javier García Polo</p>
  <p><b>备注</b>：Presented at 2nd ACM International Conference on AI in Finance</p>
  <p><b>关键词</b>：provide traders alternative price levels, rl market maker agents behaves, one rl market maker learning, multiple rl market markers learning, profitable market maker approaches</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Market makers play a key role in financial markets by providing liquidity.
They usually fill order books with buy and sell limit orders in order to
provide traders alternative price levels to operate. This paper focuses
precisely on the study of these markets makers strategies from an agent-based
perspective. In particular, we propose the application of Reinforcement
Learning (RL) for the creation of intelligent market markers in simulated stock
markets. This research analyzes how RL market maker agents behaves in
non-competitive (only one RL market maker learning at the same time) and
competitive scenarios (multiple RL market markers learning at the same time),
and how they adapt their strategies in a Sim2Real scope with interesting
results. Furthermore, it covers the application of policy transfer between
different experiments, describing the impact of competing environments on RL
agents performance. RL and deep RL techniques are proven as profitable market
maker approaches, leading to a better understanding of their behavior in stock
markets.</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：Daily peak electrical load forecasting with a multi-resolution approach</b></summary>
  <p><b>编号</b>：[207]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04492</p>
  <p><b>作者</b>：Yvenn Amara-Ouali,  Matteo Fasiolo,  Yannig Goude,  Hui Yan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：different resolutions implemented via generalised additive models, forecast daily peak demand size, daily peak load forecasting, different model classes, uk electricity market</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the context of smart grids and load balancing, daily peak load forecasting
has become a critical activity for stakeholders of the energy industry. An
understanding of peak magnitude and timing is paramount for the implementation
of smart grid strategies such as peak shaving. The modelling approach proposed
in this paper leverages high-resolution and low-resolution information to
forecast daily peak demand size and timing. The resulting multi-resolution
modelling framework can be adapted to different model classes. The key
contributions of this paper are a) a general and formal introduction to the
multi-resolution modelling approach, b) a discussion on modelling approaches at
different resolutions implemented via Generalised Additive Models and Neural
Networks and c) experimental results on real data from the UK electricity
market. The results confirm that the predictive performance of the proposed
modelling approach is competitive with that of low- and high-resolution
alternatives.</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：Fair Structure Learning in Heterogeneous Graphical Models</b></summary>
  <p><b>编号</b>：[208]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05128</p>
  <p><b>作者</b>：Davoud Ataee Tarzanagh,  Laura Balzano,  Alfred O. Hero</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：novel $\ ell_1 $- regularized pseudo, probabilistic graphical models may, fair graphical model selection, gaussian graphical model, certain demographics may</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Inference of community structure in probabilistic graphical models may not be
consistent with fairness constraints when nodes have demographic attributes.
Certain demographics may be over-represented in some detected communities and
under-represented in others. This paper defines a novel $\ell_1$-regularized
pseudo-likelihood approach for fair graphical model selection. In particular,
we assume there is some community or clustering structure in the true
underlying graph, and we seek to learn a sparse undirected graph and its
communities from the data such that demographic groups are fairly represented
within the communities. Our optimization approach uses the demographic parity
definition of fairness, but the framework is easily extended to other
definitions of fairness. We establish statistical consistency of the proposed
method for both a Gaussian graphical model and an Ising model for,
respectively, continuous and binary data, proving that our method can recover
the graphs and their fair communities with high probability.</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：On Convergence of Federated Averaging Langevin Dynamics</b></summary>
  <p><b>编号</b>：[209]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05120</p>
  <p><b>作者</b>：Wei Deng,  Yi-An Ma,  Zhao Song,  Qian Zhang,  Guang Lin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：local devices may become inactive, generalize beyond normal posterior distributions, also show convergence results based, varying learning rates affect, federated averaging langevin algorithm</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a federated averaging Langevin algorithm (FA-LD) for uncertainty
quantification and mean predictions with distributed clients. In particular, we
generalize beyond normal posterior distributions and consider a general class
of models. We develop theoretical guarantees for FA-LD for strongly log-concave
distributions with non-i.i.d data and study how the injected noise and the
stochastic-gradient noise, the heterogeneity of data, and the varying learning
rates affect the convergence. Such an analysis sheds light on the optimal
choice of local updates to minimize communication costs. Important to our
approach is that the communication efficiency does not deteriorate with the
injected noise in the Langevin algorithms. In addition, we examine in our FA-LD
algorithm both independent and correlated noise used over different clients. We
observe that there is also a trade-off between federation and communication
cost there. As local devices may become inactive in the federated network, we
also show convergence results based on different averaging schemes where only
partial device updates are available.</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：Continuation Path with Linear Convergence Rate</b></summary>
  <p><b>编号</b>：[210]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05104</p>
  <p><b>作者</b>：Eugene Ndiaye,  Ichiro Takeuchi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：rather useful heuristic, primal dual analysis, linear convergence rate, better convergence speeds, active set approaches</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Path-following algorithms are frequently used in composite optimization
problems where a series of subproblems, with varying regularization
hyperparameters, are solved sequentially. By reusing the previous solutions as
initialization, better convergence speeds have been observed numerically. This
makes it a rather useful heuristic to speed up the execution of optimization
algorithms in machine learning. We present a primal dual analysis of the
path-following algorithm and explore how to design its hyperparameters as well
as determining how accurately each subproblem should be solved to guarantee a
linear convergence rate on a target problem. Furthermore, considering
optimization with a sparsity-inducing penalty, we analyze the change of the
active sets with respect to the regularization parameter. The latter can then
be adaptively calibrated to finely determine the number of features that will
be selected along the solution path. This leads to simple heuristics for
calibrating hyperparameters of active set approaches to reduce their complexity
and improve their execution time.</p>
  </details>
</details>
<details>
  <summary>83. <b>标题：Provable Continual Learning via Sketched Jacobian Approximations</b></summary>
  <p><b>编号</b>：[211]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05095</p>
  <p><b>作者</b>：Reinhard Heckel</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：based continual learning algorithms work, provably enables overcoming catastrophic forgetting, models forget previously learned tasks, provably suffer catastrophic forgetting, diagonal matrix build based</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>An important problem in machine learning is the ability to learn tasks in a
sequential manner. If trained with standard first-order methods most models
forget previously learned tasks when trained on a new task, which is often
referred to as catastrophic forgetting. A popular approach to overcome
forgetting is to regularize the loss function by penalizing models that perform
poorly on previous tasks. For example, elastic weight consolidation (EWC)
regularizes with a quadratic form involving a diagonal matrix build based on
past data. While EWC works very well for some setups, we show that, even under
otherwise ideal conditions, it can provably suffer catastrophic forgetting if
the diagonal matrix is a poor approximation of the Hessian matrix of previous
tasks. We propose a simple approach to overcome this: Regularizing training of
a new task with sketches of the Jacobian matrix of past data. This provably
enables overcoming catastrophic forgetting for linear models and for wide
neural networks, at the cost of memory. The overarching goal of this paper is
to provided insights on when regularization-based continual learning algorithms
work and under what memory costs.</p>
  </details>
</details>
<details>
  <summary>84. <b>标题：Robust Weakly Supervised Learning for COVID-19 Recognition Using  Multi-Center CT Images</b></summary>
  <p><b>编号</b>：[219]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04984</p>
  <p><b>作者</b>：Qinghao Ye,  Yuan Gao,  Weiping Ding,  Zhangming Niu,  Chengjia Wang,  Yinghui Jiang,  Minhao Wang,  Evandro Fei Fang,  Wade Menpes-Smith,  Jun Xia,  Guang Yang</p>
  <p><b>备注</b>：32 pages, 8 figures, Applied Soft Computing</p>
  <p><b>关键词</b>：19 ct scan recognition model namely coronavirus information fusion, infectious disease named coronavirus disease 2019, new robust weakly supervised learning paradigm, severe acute respiratory syndrome coronavirus 2, automated 3d ct scan recognition tool</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The world is currently experiencing an ongoing pandemic of an infectious
disease named coronavirus disease 2019 (i.e., COVID-19), which is caused by the
severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). Computed
Tomography (CT) plays an important role in assessing the severity of the
infection and can also be used to identify those symptomatic and asymptomatic
COVID-19 carriers. With a surge of the cumulative number of COVID-19 patients,
radiologists are increasingly stressed to examine the CT scans manually.
Therefore, an automated 3D CT scan recognition tool is highly in demand since
the manual analysis is time-consuming for radiologists and their fatigue can
cause possible misjudgment. However, due to various technical specifications of
CT scanners located in different hospitals, the appearance of CT images can be
significantly different leading to the failure of many automated image
recognition approaches. The multi-domain shift problem for the multi-center and
multi-scanner studies is therefore nontrivial that is also crucial for a
dependable recognition and critical for reproducible and objective diagnosis
and prognosis. In this paper, we proposed a COVID-19 CT scan recognition model
namely coronavirus information fusion and diagnosis network (CIFD-Net) that can
efficiently handle the multi-domain shift problem via a new robust weakly
supervised learning paradigm. Our model can resolve the problem of different
appearance in CT scan images reliably and efficiently while attaining higher
accuracy compared to other state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>85. <b>标题：A fully-differentiable compressible high-order computational fluid  dynamics solver</b></summary>
  <p><b>编号</b>：[220]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04979</p>
  <p><b>作者</b>：Deniz A. Bezgin,  Aaron B. Buhendwa,  Nikolaus A. Adams</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：improve existing numerical schemes inside computational fluid dynamics algorithms, paradigm shift towards machine learning supported design, fluid flows still introduces prohibitive computational cost, stokes equations govern compressible flows, compressible fluid flows using high</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Fluid flows are omnipresent in nature and engineering disciplines. The
reliable computation of fluids has been a long-lasting challenge due to
nonlinear interactions over multiple spatio-temporal scales. The compressible
Navier-Stokes equations govern compressible flows and allow for complex
phenomena like turbulence and shocks. Despite tremendous progress in hardware
and software, capturing the smallest length-scales in fluid flows still
introduces prohibitive computational cost for real-life applications. We are
currently witnessing a paradigm shift towards machine learning supported design
of numerical schemes as a means to tackle aforementioned problem. While prior
work has explored differentiable algorithms for one- or two-dimensional
incompressible fluid flows, we present a fully-differentiable three-dimensional
framework for the computation of compressible fluid flows using high-order
state-of-the-art numerical methods. Firstly, we demonstrate the efficiency of
our solver by computing classical two- and three-dimensional test cases,
including strong shocks and transition to turbulence. Secondly, and more
importantly, our framework allows for end-to-end optimization to improve
existing numerical schemes inside computational fluid dynamics algorithms. In
particular, we are using neural networks to substitute a conventional numerical
flux function.</p>
  </details>
</details>
<details>
  <summary>86. <b>标题：A Training Framework for Stereo-Aware Speech Enhancement using Deep  Neural Networks</b></summary>
  <p><b>编号</b>：[222]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04939</p>
  <p><b>作者</b>：Bahareh Tolooshams,  Kazuhito Koishida</p>
  <p><b>备注</b>：Submitted to ICASSP 2022</p>
  <p><b>关键词</b>：popular mono speech enhancement frameworks, incorporate spatial statistics along, deep learning based architecture, multichannel microphone recordings, growing computational power</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning-based speech enhancement has shown unprecedented performance in
recent years. The most popular mono speech enhancement frameworks are
end-to-end networks mapping the noisy mixture into an estimate of the clean
speech. With growing computational power and availability of multichannel
microphone recordings, prior works have aimed to incorporate spatial statistics
along with spectral information to boost up performance. Despite an improvement
in enhancement performance of mono output, the spatial image preservation and
subjective evaluations have not gained much attention in the literature. This
paper proposes a novel stereo-aware framework for speech enhancement, i.e., a
training loss for deep learning-based speech enhancement to preserve the
spatial image while enhancing the stereo mixture. The proposed framework is
model independent, hence it can be applied to any deep learning based
architecture. We provide an extensive objective and subjective evaluation of
the trained models through a listening test. We show that by regularizing for
an image preservation loss, the overall performance is improved, and the stereo
aspect of the speech is better preserved.</p>
  </details>
</details>
<details>
  <summary>87. <b>标题：Measuring Wind Turbine Health Using Drifting Concepts</b></summary>
  <p><b>编号</b>：[223]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04933</p>
  <p><b>作者</b>：Agnieszka Jastrzebska,  Alejandro Morales-Hernández,  Gonzalo Nápoles,  Yamisleydi Salgueiro,  Koen Vanhoof</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：process publicly available data describing four wind turbines, power production process undergoes fluctuations, labeled using linguistic labels, implemented using fuzzy sets, propose two new approaches</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Time series processing is an essential aspect of wind turbine health
monitoring. Despite the progress in this field, there is still room for new
methods to improve modeling quality. In this paper, we propose two new
approaches for the analysis of wind turbine health. Both approaches are based
on abstract concepts, implemented using fuzzy sets, which summarize and
aggregate the underlying raw data. By observing the change in concepts, we
infer about the change in the turbine's health. Analyzes are carried out
separately for different external conditions (wind speed and temperature). We
extract concepts that represent relative low, moderate, and high power
production. The first method aims at evaluating the decrease or increase in
relatively high and low power production. This task is performed using a
regression-like model. The second method evaluates the overall drift of the
extracted concepts. Large drift indicates that the power production process
undergoes fluctuations in time. Concepts are labeled using linguistic labels,
thus equipping our model with improved interpretability features. We applied
the proposed approach to process publicly available data describing four wind
turbines. The simulation results have shown that the aging process is not
homogeneous in all wind turbines.</p>
  </details>
</details>
<details>
  <summary>88. <b>标题：A More Stable Accelerated Gradient Method Inspired by Continuous-Time  Perspective</b></summary>
  <p><b>编号</b>：[224]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04922</p>
  <p><b>作者</b>：Yasong Feng,  Weiguo Gao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：machine learning background including deep learning, handwriting digit recognition demonstrate, large step size, higher computational speed, accelerated gradient method</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Nesterov's accelerated gradient method (NAG) is widely used in problems with
machine learning background including deep learning, and is corresponding to a
continuous-time differential equation. From this connection, the property of
the differential equation and its numerical approximation can be investigated
to improve the accelerated gradient method. In this work we present a new
improvement of NAG in terms of stability inspired by numerical analysis. We
give the precise order of NAG as a numerical approximation of its
continuous-time limit and then present a new method with higher order. We show
theoretically that our new method is more stable than NAG for large step size.
Experiments of matrix completion and handwriting digit recognition demonstrate
that the stability of our new method is better. Furthermore, better stability
leads to higher computational speed in experiments.</p>
  </details>
</details>
<details>
  <summary>89. <b>标题：End-to-end Alexa Device Arbitration</b></summary>
  <p><b>编号</b>：[225]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04914</p>
  <p><b>作者</b>：Jarred Barber,  Yifeng Fan,  Tao Zhang</p>
  <p><b>备注</b>：Submitted to ICASSP 2022</p>
  <p><b>关键词</b>：smart home devices ),, multiple distributed microphone arrays, end machine learning system, speaker localization problem, signal processing baseline</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce a variant of the speaker localization problem, which we call
device arbitration. In the device arbitration problem, a user utters a keyword
that is detected by multiple distributed microphone arrays (smart home
devices), and we want to determine which device was closest to the user. Rather
than solving the full localization problem, we propose an end-to-end machine
learning system. This system learns a feature embedding that is computed
independently on each device. The embeddings from each device are then
aggregated together to produce the final arbitration decision. We use a
large-scale room simulation to generate training and evaluation data, and
compare our system against a signal processing baseline.</p>
  </details>
</details>
<details>
  <summary>90. <b>标题：Enhancing Column Generation by a Machine-Learning-Based Pricing  Heuristic for Graph Coloring</b></summary>
  <p><b>编号</b>：[226]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04906</p>
  <p><b>作者</b>：Yunzhuang Shen,  Yuan Sun,  Xiaodong Li,  Andrew Eberhard,  Andreas Ernst</p>
  <p><b>备注</b>：Machine learning for column generation and branch-and-price; accepted to AAAI 2022</p>
  <p><b>关键词</b>：efficiently generate multiple high, gradually includes new columns, generate many high, substantially better performance, scale optimization problems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Column Generation (CG) is an effective method for solving large-scale
optimization problems. CG starts by solving a sub-problem with a subset of
columns (i.e., variables) and gradually includes new columns that can improve
the solution of the current subproblem. The new columns are generated as needed
by repeatedly solving a pricing problem, which is often NP-hard and is a
bottleneck of the CG approach. To tackle this, we propose a
Machine-Learning-based Pricing Heuristic (MLPH)that can generate many
high-quality columns efficiently. In each iteration of CG, our MLPH leverages
an ML model to predict the optimal solution of the pricing problem, which is
then used to guide a sampling method to efficiently generate multiple
high-quality columns. Using the graph coloring problem, we empirically show
that MLPH significantly enhancesCG as compared to six state-of-the-art methods,
and the improvement in CG can lead to substantially better performance of the
branch-and-price exact method.</p>
  </details>
</details>
<details>
  <summary>91. <b>标题：Forecast Evaluation in Large Cross-Sections of Realized Volatility</b></summary>
  <p><b>编号</b>：[228]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04887</p>
  <p><b>作者</b>：Christis Katsouris</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：section dependence using equal predictive accuracy testing procedures, augmented har model estimated via, sectional jump component measures, equal predictive accuracy, equal predictive accuracy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we consider the forecast evaluation of realized volatility
measures under cross-section dependence using equal predictive accuracy testing
procedures. We evaluate the predictive accuracy of the model based on the
augmented cross-section when forecasting Realized Volatility. Under the null
hypothesis of equal predictive accuracy the benchmark model employed is a
standard HAR model while under the alternative of non-equal predictive accuracy
the forecast model is an augmented HAR model estimated via the LASSO shrinkage.
We study the sensitivity of forecasts to the model specification by
incorporating a measurement error correction as well as cross-sectional jump
component measures. The out-of-sample forecast evaluation of the models is
assessed with numerical implementations.</p>
  </details>
</details>
<details>
  <summary>92. <b>标题：Evaluating saliency methods on artificial data with different background  types</b></summary>
  <p><b>编号</b>：[229]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04882</p>
  <p><b>作者</b>：Céline Budding,  Fabian Eitel,  Kerstin Ritter,  Stefan Haufe</p>
  <p><b>备注</b>：6 pages, 2 figures. Presented at Medical Imaging meets NeurIPS 2021 (poster presentation)</p>
  <p><b>关键词</b>：known ground truth map, 2d brain mri slices, evaluated two data sets, generate artificial data, explainable artificial intelligence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Over the last years, many 'explainable artificial intelligence' (xAI)
approaches have been developed, but these have not always been objectively
evaluated. To evaluate the quality of heatmaps generated by various saliency
methods, we developed a framework to generate artificial data with synthetic
lesions and a known ground truth map. Using this framework, we evaluated two
data sets with different backgrounds, Perlin noise and 2D brain MRI slices, and
found that the heatmaps vary strongly between saliency methods and backgrounds.
We strongly encourage further evaluation of saliency maps and xAI methods using
this framework before applying these in clinical or other safety-critical
settings.</p>
  </details>
</details>
<details>
  <summary>93. <b>标题：Evaluation of survival distribution predictions with discrimination  measures</b></summary>
  <p><b>编号</b>：[232]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04828</p>
  <p><b>作者</b>：Raphael Sonabend,  Andreas Bender,  Sebastian Vollmer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：machine learning survival analysis software implements clear transformations, evaluate survival distribution predictions, survival analysis, survey methods proposed, predicted cumulative hazard</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper we consider how to evaluate survival distribution predictions
with measures of discrimination. This is a non-trivial problem as
discrimination measures are the most commonly used in survival analysis and yet
there is no clear method to derive a risk prediction from a distribution
prediction. We survey methods proposed in literature and software and consider
their respective advantages and disadvantages. Whilst distributions are
frequently evaluated by discrimination measures, we find that the method for
doing so is rarely described in the literature and often leads to unfair
comparisons. We find that the most robust method of reducing a distribution to
a risk is to sum over the predicted cumulative hazard. We recommend that
machine learning survival analysis software implements clear transformations
between distribution and risk predictions in order to allow more transparent
and accessible model evaluation.</p>
  </details>
</details>
<details>
  <summary>94. <b>标题：Multimodal Pre-Training Model for Sequence-based Prediction of  Protein-Protein Interaction</b></summary>
  <p><b>编号</b>：[233]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04814</p>
  <p><b>作者</b>：Yang Xue,  Zijing Liu,  Xiaomin Fang,  Fan Wang</p>
  <p><b>备注</b>：MLCB 2021 Spotlight</p>
  <p><b>关键词</b>：driven binding affinity change prediction, proteins physically bind together, s2f learns protein embeddings, antigen affinity prediction, peptide drug discovery</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Protein-protein interactions (PPIs) are essentials for many biological
processes where two or more proteins physically bind together to achieve their
functions. Modeling PPIs is useful for many biomedical applications, such as
vaccine design, antibody therapeutics, and peptide drug discovery. Pre-training
a protein model to learn effective representation is critical for PPIs. Most
pre-training models for PPIs are sequence-based, which naively adopt the
language models used in natural language processing to amino acid sequences.
More advanced works utilize the structure-aware pre-training technique, taking
advantage of the contact maps of known protein structures. However, neither
sequences nor contact maps can fully characterize structures and functions of
the proteins, which are closely related to the PPI problem. Inspired by this
insight, we propose a multimodal protein pre-training model with three
modalities: sequence, structure, and function (S2F). Notably, instead of using
contact maps to learn the amino acid-level rigid structures, we encode the
structure feature with the topology complex of point clouds of heavy atoms. It
allows our model to learn structural information about not only the backbones
but also the side chains. Moreover, our model incorporates the knowledge from
the functional description of proteins extracted from literature or manual
annotations. Our experiments show that the S2F learns protein embeddings that
achieve good performances on a variety of PPIs tasks, including cross-species
PPI, antibody-antigen affinity prediction, antibody neutralization prediction
for SARS-CoV-2, and mutation-driven binding affinity change prediction.</p>
  </details>
</details>
<details>
  <summary>95. <b>标题：Regularized Modal Regression on Markov-dependent Observations: A  Theoretical Assessment</b></summary>
  <p><b>编号</b>：[234]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04779</p>
  <p><b>作者</b>：Tielang Gong,  Yuxin Dong,  Hong Chen,  Bo Dong,  Wei Feng,  Chen Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：machine learning communities due, widely used regression protocol, sample size would, multiplicative factor depending, important dependence structure</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Modal regression, a widely used regression protocol, has been extensively
investigated in statistical and machine learning communities due to its
robustness to outliers and heavy-tailed noises. Understanding modal
regression's theoretical behavior can be fundamental in learning theory.
Despite significant progress in characterizing its statistical property, the
majority of the results are based on the assumption that samples are
independent and identical distributed (i.i.d.), which is too restrictive for
real-world applications. This paper concerns the statistical property of
regularized modal regression (RMR) within an important dependence structure -
Markov dependent. Specifically, we establish the upper bound for RMR estimator
under moderate conditions and give an explicit learning rate. Our results show
that the Markov dependence impacts on the generalization error in the way that
sample size would be discounted by a multiplicative factor depending on the
spectral gap of underlying Markov chain. This result shed a new light on
characterizing the theoretical underpinning for robust regression.</p>
  </details>
</details>
<details>
  <summary>96. <b>标题：High-Dimensional Stock Portfolio Trading with Deep Reinforcement  Learning</b></summary>
  <p><b>编号</b>：[236]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04755</p>
  <p><b>作者</b>：Uta Pigorsch,  Sebastian Schäfer</p>
  <p><b>备注</b>：14 pages, 5 figures, 2 tables</p>
  <p><b>关键词</b>：48 us stock portfolio setups, may include data gaps, active benchmark investment strategies, financial portfolio trading based, deep reinforcement learning algorithm</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper proposes a Deep Reinforcement Learning algorithm for financial
portfolio trading based on Deep Q-learning. The algorithm is capable of trading
high-dimensional portfolios from cross-sectional datasets of any size which may
include data gaps and non-unique history lengths in the assets. We sequentially
set up environments by sampling one asset for each environment while rewarding
investments with the resulting asset's return and cash reservation with the
average return of the set of assets. This enforces the agent to strategically
assign capital to assets that it predicts to perform above-average. We apply
our methodology in an out-of-sample analysis to 48 US stock portfolio setups,
varying in the number of stocks from ten up to 500 stocks, in the selection
criteria and in the level of transaction costs. The algorithm on average
outperforms all considered passive and active benchmark investment strategies
by a large margin using only one hyperparameter setup for all portfolios.</p>
  </details>
</details>
<details>
  <summary>97. <b>标题：A Note on Comparison of F-measures</b></summary>
  <p><b>编号</b>：[238]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04677</p>
  <p><b>作者</b>：Wei Ju,  Wenxin Jiang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：imbalanced data sets ",, make two improvements related, two prediction rules, recent tkde paper, performance evaluation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We comment on a recent TKDE paper "Linear Approximation of F-measure for the
Performance Evaluation of Classification Algorithms on Imbalanced Data Sets",
and make two improvements related to comparison of F-measures for two
prediction rules.</p>
  </details>
</details>
<details>
  <summary>98. <b>标题：Deep Molecular Representation Learning via Fusing Physical and Chemical  Information</b></summary>
  <p><b>编号</b>：[240]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04624</p>
  <p><b>作者</b>：Shuwen Yang,  Ziyao Li,  Guojie Song,  Lingsheng Cai</p>
  <p><b>备注</b>：In NeurIPS-2021, 18 pages, 5 figures, appendix included</p>
  <p><b>关键词</b>：learns molecular representations via fusing physical, standard molecular machine learning benchmark, first yet vital step, learns molecular conformations, molecular representation learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Molecular representation learning is the first yet vital step in combining
deep learning and molecular science. To push the boundaries of molecular
representation learning, we present PhysChem, a novel neural architecture that
learns molecular representations via fusing physical and chemical information
of molecules. PhysChem is composed of a physicist network (PhysNet) and a
chemist network (ChemNet). PhysNet is a neural physical engine that learns
molecular conformations through simulating molecular dynamics with
parameterized forces; ChemNet implements geometry-aware deep message-passing to
learn chemical / biomedical properties of molecules. Two networks specialize in
their own tasks and cooperate by providing expertise to each other. By fusing
physical and chemical information, PhysChem achieved state-of-the-art
performances on MoleculeNet, a standard molecular machine learning benchmark.
The effectiveness of PhysChem was further corroborated on cutting-edge datasets
of SARS-CoV-2.</p>
  </details>
</details>
<details>
  <summary>99. <b>标题：Recent Advances in Reinforcement Learning in Finance</b></summary>
  <p><b>编号</b>：[243]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04553</p>
  <p><b>作者</b>：Ben Hambly,  Renyuan Xu,  Huining Yang</p>
  <p><b>备注</b>：60 pages, 1 figure</p>
  <p><b>关键词</b>：classical stochastic control theory, encompass deep rl algorithms, commonly used rl approaches, smart order routing, policy based methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rapid changes in the finance industry due to the increasing amount of
data have revolutionized the techniques on data processing and data analysis
and brought new theoretical and computational challenges. In contrast to
classical stochastic control theory and other analytical approaches for solving
financial decision-making problems that heavily reply on model assumptions, new
developments from reinforcement learning (RL) are able to make full use of the
large amount of financial data with fewer model assumptions and to improve
decisions in complex financial environments. This survey paper aims to review
the recent developments and use of RL approaches in finance. We give an
introduction to Markov decision processes, which is the setting for many of the
commonly used RL approaches. Various algorithms are then introduced with a
focus on value and policy based methods that do not require any model
assumptions. Connections are made with neural networks to extend the framework
to encompass deep RL algorithms. Our survey concludes by discussing the
application of these RL algorithms in a variety of decision-making problems in
finance, including optimal execution, portfolio optimization, option pricing
and hedging, market making, smart order routing, and robo-advising.</p>
  </details>
</details>
<details>
  <summary>100. <b>标题：Building Quantum Field Theories Out of Neurons</b></summary>
  <p><b>编号</b>：[244]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04527</p>
  <p><b>作者</b>：James Halverson</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large -$ n $, potentially explaining, infinite -$ n $,, finite -$ n $., invariant quantum field theory, infinite -$ n</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>An approach to field theory is studied in which fields are comprised of $N$
constituent random neurons. Gaussian theories arise in the infinite-$N$ limit
when neurons are independently distributed, via the Central Limit Theorem,
while interactions arise due to finite-$N$ effects or non-independently
distributed neurons. Euclidean-invariant ensembles of neurons are engineered,
with tunable two-point function, yielding families of Euclidean-invariant field
theories. Some Gaussian, Euclidean invariant theories are reflection positive,
which allows for analytic continuation to a Lorentz-invariant quantum field
theory. Examples are presented that yield dual theories at infinite-$N$, but
have different symmetries at finite-$N$. Landscapes of classical field
configurations are determined by local maxima of parameter distributions.
Predictions arise from mixed field-neuron correlators. Near-Gaussianity is
exhibited at large-$N$, potentially explaining a feature of field theories in
Nature.</p>
  </details>
</details>
<h1>人工智能</h1>
<details>
  <summary>1. <b>标题：PTR: A Benchmark for Part-based Conceptual, Relational, and Physical  Reasoning</b></summary>
  <p><b>编号</b>：[7]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05136</p>
  <p><b>作者</b>：Yining Hong,  Li Yi,  Joshua B. Tenenbaum,  Antonio Torralba,  Chuang Gan</p>
  <p><b>备注</b>：NeurIPS 2021. Project page: this http URL</p>
  <p><b>关键词</b>：ptr contains around 70k rgbd synthetic images, part level annotations regarding semantic instance segmentation, scale diagnostic visual reasoning dataset named ptr, existing visual reasoning benchmarks mostly focus, still make many surprising mistakes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A critical aspect of human visual perception is the ability to parse visual
scenes into individual objects and further into object parts, forming
part-whole hierarchies. Such composite structures could induce a rich set of
semantic concepts and relations, thus playing an important role in the
interpretation and organization of visual signals as well as for the
generalization of visual perception and reasoning. However, existing visual
reasoning benchmarks mostly focus on objects rather than parts. Visual
reasoning based on the full part-whole hierarchy is much more challenging than
object-centric reasoning due to finer-grained concepts, richer geometry
relations, and more complex physics. Therefore, to better serve for part-based
conceptual, relational and physical reasoning, we introduce a new large-scale
diagnostic visual reasoning dataset named PTR. PTR contains around 70k RGBD
synthetic images with ground truth object and part level annotations regarding
semantic instance segmentation, color attributes, spatial and geometric
relationships, and certain physical properties such as stability. These images
are paired with 700k machine-generated questions covering various types of
reasoning types, making them a good testbed for visual reasoning models. We
examine several state-of-the-art visual reasoning models on this dataset and
observe that they still make many surprising mistakes in situations where
humans can easily infer the correct answer. We believe this dataset will open
up new opportunities for part-based reasoning.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Neural Descriptor Fields: SE(3)-Equivariant Object Representations for  Manipulation</b></summary>
  <p><b>编号</b>：[16]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05124</p>
  <p><b>作者</b>：Anthony Simeonov,  Yilun Du,  Andrea Tagliasacchi,  Joshua B. Tenenbaum,  Alberto Rodriguez,  Pulkit Agrawal,  Vincent Sitzmann</p>
  <p><b>备注</b>：Website: this https URL First two authors contributed equally (order determined by coin flip), last two authors equal advising</p>
  <p><b>关键词</b>：present neural descriptor fields, pose whose descriptor matches, possible 3d object translations, 3 )- equivariant, new object instance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present Neural Descriptor Fields (NDFs), an object representation that
encodes both points and relative poses between an object and a target (such as
a robot gripper or a rack used for hanging) via category-level descriptors. We
employ this representation for object manipulation, where given a task
demonstration, we want to repeat the same task on a new object instance from
the same category. We propose to achieve this objective by searching (via
optimization) for the pose whose descriptor matches that observed in the
demonstration. NDFs are conveniently trained in a self-supervised fashion via a
3D auto-encoding task that does not rely on expert-labeled keypoints. Further,
NDFs are SE(3)-equivariant, guaranteeing performance that generalizes across
all possible 3D object translations and rotations. We demonstrate learning of
manipulation tasks from few (5-10) demonstrations both in simulation and on a
real robot. Our performance generalizes across both object instances and 6-DoF
object poses, and significantly outperforms a recent baseline that relies on 2D
descriptors. Project website: this https URL.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Extending the WILDS Benchmark for Unsupervised Adaptation</b></summary>
  <p><b>编号</b>：[21]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05090</p>
  <p><b>作者</b>：Shiori Sagawa,  Pang Wei Koh,  Tony Lee,  Irena Gao,  Sang Michael Xie,  Kendrick Shen,  Ananya Kumar,  Weihua Hu,  Michihiro Yasunaga,  Henrik Marklund,  Sara Beery,  Etienne David,  Ian Stavness,  Wei Guo,  Jure Leskovec,  Kate Saenko,  Tatsunori Hashimoto,  Sergey Levine,  Chelsea Finn,  Percy Liang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：wildlife conservation ), tasks, existing distribution shift benchmarks, machine learning systems deployed, include curated unlabeled data, molecular graphs ).</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine learning systems deployed in the wild are often trained on a source
distribution but deployed on a different target distribution. Unlabeled data
can be a powerful point of leverage for mitigating these distribution shifts,
as it is frequently much more available than labeled data. However, existing
distribution shift benchmarks for unlabeled data do not reflect the breadth of
scenarios that arise in real-world applications. In this work, we present the
WILDS 2.0 update, which extends 8 of the 10 datasets in the WILDS benchmark of
distribution shifts to include curated unlabeled data that would be
realistically obtainable in deployment. To maintain consistency, the labeled
training, validation, and test sets, as well as the evaluation metrics, are
exactly the same as in the original WILDS benchmark. These datasets span a wide
range of applications (from histology to wildlife conservation), tasks
(classification, regression, and detection), and modalities (photos, satellite
images, microscope slides, text, molecular graphs). We systematically benchmark
state-of-the-art methods that leverage unlabeled data, including
domain-invariant, self-training, and self-supervised methods, and show that
their success on WILDS 2.0 is limited. To facilitate method development and
evaluation, we provide an open-source package that automates data loading and
contains all of the model architectures and methods used in this paper. Code
and leaderboards are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Locally Shifted Attention With Early Global Integration</b></summary>
  <p><b>编号</b>：[24]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05080</p>
  <p><b>作者</b>：Shelly Sheynin,  Sagie Benaim,  Adam Polyak,  Lior Wolf</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：grained local interactions already, virtually located local patches, grained global interactions, dependent localization already, virtually located patches</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent work has shown the potential of transformers for computer vision
applications. An image is first partitioned into patches, which are then used
as input tokens for the attention mechanism. Due to the expensive quadratic
cost of the attention mechanism, either a large patch size is used, resulting
in coarse-grained global interactions, or alternatively, attention is applied
only on a local region of the image, at the expense of long-range interactions.
In this work, we propose an approach that allows for both coarse global
interactions and fine-grained local interactions already at early layers of a
vision transformer.
At the core of our method is the application of local and global attention
layers. In the local attention layer, we apply attention to each patch and its
local shifts, resulting in virtually located local patches, which are not bound
to a single, specific location. These virtually located patches are then used
in a global attention layer. The separation of the attention layer into local
and global counterparts allows for a low computational cost in the number of
patches, while still supporting data-dependent localization already at the
first layer, as opposed to the static positioning in other visual transformers.
Our method is shown to be superior to both convolutional and transformer-based
methods for image classification on CIFAR10, CIFAR100, and ImageNet. Code is
available at: this https URL.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Learning Transferable Motor Skills with Hierarchical Latent Mixture  Policies</b></summary>
  <p><b>编号</b>：[29]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05062</p>
  <p><b>作者</b>：Dushyant Rao,  Fereshteh Sadeghi,  Leonard Hasenclever,  Markus Wulfmeier,  Martina Zambelli,  Giulia Vezzani,  Dhruva Tirumala,  Yusuf Aytar,  Josh Merel,  Nicolas Heess,  Raia Hadsell</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：hierarchical mixture latent variable model, continuous latent variable model, yielding better sample efficiency, learn abstract motor skills, effectively cluster offline data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>For robots operating in the real world, it is desirable to learn reusable
behaviours that can effectively be transferred and adapted to numerous tasks
and scenarios. We propose an approach to learn abstract motor skills from data
using a hierarchical mixture latent variable model. In contrast to existing
work, our method exploits a three-level hierarchy of both discrete and
continuous latent variables, to capture a set of high-level behaviours while
allowing for variance in how they are executed. We demonstrate in manipulation
domains that the method can effectively cluster offline data into distinct,
executable behaviours, while retaining the flexibility of a continuous latent
variable model. The resulting skills can be transferred and fine-tuned on new
tasks, unseen objects, and from state to vision-based policies, yielding better
sample efficiency and asymptotic performance compared to existing skill- and
imitation-based methods. We further analyse how and when the skills are most
beneficial: they encourage directed exploration to cover large regions of the
state space relevant to the task, making them most effective in challenging
sparse-reward settings.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：End-to-End Learning of Joint Geometric and Probabilistic Constellation  Shaping</b></summary>
  <p><b>编号</b>：[35]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05050</p>
  <p><b>作者</b>：Vahid Aref,  Mathieu Chagnon</p>
  <p><b>备注</b>：Will be presented at OFC 2022 (invited talk)</p>
  <p><b>关键词</b>：probabilistic constellation shaping, metric decoding )., generalized mutual information, mutual information, metric decoding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a novel autoencoder-based learning of joint geometric and
probabilistic constellation shaping for coded-modulation systems. It can
maximize either the mutual information (for symbol-metric decoding) or the
generalized mutual information (for bit-metric decoding).</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Wikidated 1.0: An Evolving Knowledge Graph Dataset of Wikidata's  Revision History</b></summary>
  <p><b>编号</b>：[44]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05003</p>
  <p><b>作者</b>：Lukas Schmelzeisen,  Corina Dima,  Steffen Staab</p>
  <p><b>备注</b>：15 pages, 4 figures. Published at Wikidata@ISWC 2021</p>
  <p><b>关键词</b>：thus evolved considerably since, recently emerging research subject, semantic web community, present wikidated 1, present statistical characteristics</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Wikidata is the largest general-interest knowledge base that is openly
available. It is collaboratively edited by thousands of volunteer editors and
has thus evolved considerably since its inception in 2012. In this paper, we
present Wikidated 1.0, a dataset of Wikidata's full revision history, which
encodes changes between Wikidata revisions as sets of deletions and additions
of RDF triples. To the best of our knowledge, it constitutes the first large
dataset of an evolving knowledge graph, a recently emerging research subject in
the Semantic Web community. We introduce the methodology for generating
Wikidated 1.0 from dumps of Wikidata, discuss its implementation and
limitations, and present statistical characteristics of the dataset.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Smart Support for Mission Success</b></summary>
  <p><b>编号</b>：[57]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04957</p>
  <p><b>作者</b>：Juliette Mattioli,  Pierre-Olivier Robic</p>
  <p><b>备注</b>：8 pages, 2 figures</p>
  <p><b>关键词</b>：provide supported equipment able, efficient decision support system, requires efficient support, criteria decision support, support agencies need</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Today's battlefield environment is complex, dynamic and uncertain, and
requires efficient support to ensure mission success. This relies on a proper
support strategy to provide supported equipment able to fulfill the mission. In
the context of defense where both systems and organization are complex, having
a holistic approach is challenging by nature, forces and support agencies need
to rely on an efficient decision support system. Logistics, readiness and
sustainability are critical factors for asset management, which can benefit
from AI to reach "Smart In Service" level relying especially on predictive and
prescriptive approaches and on effective management of operational re-sources.
Smart Support capacities can be then monitored by appropriate metrics and
improved by multi-criteria decision support and knowledge management system.
Depending on the operational context in terms of information and the objective,
different AI paradigms (data-driven AI, knowledge-based AI) are suitable even a
combination through hybrid AI.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Machine Learning for Utility Prediction in Argument-Based Computational  Persuasion</b></summary>
  <p><b>编号</b>：[58]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04953</p>
  <p><b>作者</b>：Ivan Donadello,  Anthony Hunter,  Stefano Teso,  Mauro Dragoni</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：realistic case study concerning healthy eating habits, predicting useful utility functions, develop two ml methods, use machine learning, party decision theory</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automated persuasion systems (APS) aim to persuade a user to believe
something by entering into a dialogue in which arguments and counterarguments
are exchanged. To maximize the probability that an APS is successful in
persuading a user, it can identify a global policy that will allow it to select
the best arguments it presents at each stage of the dialogue whatever arguments
the user presents. However, in real applications, such as for healthcare, it is
unlikely the utility of the outcome of the dialogue will be the same, or the
exact opposite, for the APS and user. In order to deal with this situation,
games in extended form have been harnessed for argumentation in Bi-party
Decision Theory. This opens new problems that we address in this paper: (1) How
can we use Machine Learning (ML) methods to predict utility functions for
different subpopulations of users? and (2) How can we identify for a new user
the best utility function from amongst those that we have learned? To this
extent, we develop two ML methods, EAI and EDS, that leverage information
coming from the users to predict their utilities. EAI is restricted to a fixed
amount of information, whereas EDS can choose the information that best detects
the subpopulations of a user. We evaluate EAI and EDS in a simulation setting
and in a realistic case study concerning healthy eating habits. Results are
promising in both cases, but EDS is more effective at predicting useful utility
functions.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：DVHN: A Deep Hashing Framework for Large-scale Vehicle Re-identification</b></summary>
  <p><b>编号</b>：[63]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04937</p>
  <p><b>作者</b>：Yongbiao Chen,  Sheng Zhang,  Fangxin Liu,  Chenggang Wu,  Kaicheng Guo,  Zhengwei Qi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：concretely ,~ dvhn directly learns discrete compact binary hash codes, reserving nearest neighbor search accuracy, hash code generating module, substantially reduces memory usage, art deep hash methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we make the very first attempt to investigate the integration
of deep hash learning with vehicle re-identification. We propose a deep
hash-based vehicle re-identification framework, dubbed DVHN, which
substantially reduces memory usage and promotes retrieval efficiency while
reserving nearest neighbor search accuracy. Concretely,~DVHN directly learns
discrete compact binary hash codes for each image by jointly optimizing the
feature learning network and the hash code generating module. Specifically, we
directly constrain the output from the convolutional neural network to be
discrete binary codes and ensure the learned binary codes are optimal for
classification. To optimize the deep discrete hashing framework, we further
propose an alternating minimization method for learning binary
similarity-preserved hashing codes. Extensive experiments on two widely-studied
vehicle re-identification datasets- \textbf{VehicleID} and \textbf{VeRi}-~have
demonstrated the superiority of our method against the state-of-the-art deep
hash methods. \textbf{DVHN} of $2048$ bits can achieve 13.94\% and 10.21\%
accuracy improvement in terms of \textbf{mAP} and \textbf{Rank@1} for
\textbf{VehicleID (800)} dataset. For \textbf{VeRi}, we achieve 35.45\% and
32.72\% performance gains for \textbf{Rank@1} and \textbf{mAP}, respectively.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：JueWu-MC: Playing Minecraft with Sample-efficient Hierarchical  Reinforcement Learning</b></summary>
  <p><b>编号</b>：[70]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04907</p>
  <p><b>作者</b>：Zichuan Lin,  Junyou Li,  Jianing Shi,  Deheng Ye,  Qiang Fu,  Wei Yang</p>
  <p><b>备注</b>：The champion solution of NeurIPS 2021 MineRL research competition ( this https URL )</p>
  <p><b>关键词</b>：world games like minecraft remains, neurips minerl 2021 research competition, mc significantly improves sample efficiency, efficient hierarchical rl approach equipped, approach includes two levels</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Learning rational behaviors in open-world games like Minecraft remains to be
challenging for Reinforcement Learning (RL) research due to the compound
challenge of partial observability, high-dimensional visual perception and
delayed reward. To address this, we propose JueWu-MC, a sample-efficient
hierarchical RL approach equipped with representation learning and imitation
learning to deal with perception and exploration. Specifically, our approach
includes two levels of hierarchy, where the high-level controller learns a
policy to control over options and the low-level workers learn to solve each
sub-task. To boost the learning of sub-tasks, we propose a combination of
techniques including 1) action-aware representation learning which captures
underlying relations between action and representation, 2) discriminator-based
self-imitation learning for efficient exploration, and 3) ensemble behavior
cloning with consistency filtering for policy robustness. Extensive experiments
show that JueWu-MC significantly improves sample efficiency and outperforms a
set of baselines by a large margin. Notably, we won the championship of the
NeurIPS MineRL 2021 research competition and achieved the highest performance
score ever.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：i-SpaSP: Structured Neural Pruning via Sparse Signal Recovery</b></summary>
  <p><b>编号</b>：[71]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04905</p>
  <p><b>作者</b>：Cameron R. Wolfe,  Anastasios Kyrillidis</p>
  <p><b>备注</b>：27 pages, 4 figures</p>
  <p><b>关键词</b>：polynomial becomes arbitrarily large based, e ., feed forward networks, achieves strong empirical results, sparse structured pruning algorithm, structured pruning algorithm</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a novel, structured pruning algorithm for neural networks -- the
iterative, Sparse Structured Pruning algorithm, dubbed as i-SpaSP. Inspired by
ideas from sparse signal recovery, i-SpaSP operates by iteratively identifying
a larger set of important parameter groups (e.g., filters or neurons) within a
network that contribute most to the residual between pruned and dense network
output, then thresholding these groups based on a smaller, pre-defined pruning
ratio. For both two-layer and multi-layer network architectures with ReLU
activations, we show the error induced by pruning with i-SpaSP decays
polynomially, where the degree of this polynomial becomes arbitrarily large
based on the sparsity of the dense network's hidden representations. In our
experiments, i-SpaSP is evaluated across a variety of datasets (i.e., MNIST and
ImageNet) and architectures (i.e., feed forward networks, ResNet34, and
MobileNetV2), where it is shown to discover high-performing sub-networks and
improve upon the pruning efficiency of provable baseline methodologies by
several orders of magnitude. Put simply, i-SpaSP is easy to implement with
automatic differentiation, achieves strong empirical results, comes with
theoretical convergence guarantees, and is efficient, thus distinguishing
itself as one of the few computationally efficient, practical, and provable
pruning algorithms.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Assessing Fairness in the Presence of Missing Data</b></summary>
  <p><b>编号</b>：[74]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04899</p>
  <p><b>作者</b>：Yiliang Zhang,  Qi Long</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：complete case domain may show disproportionate bias towards, arbitrary model evaluated merely using complete cases, first known theoretical results, complete data may, complete data domain</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Missing data are prevalent and present daunting challenges in real data
analysis. While there is a growing body of literature on fairness in analysis
of fully observed data, there has been little theoretical work on investigating
fairness in analysis of incomplete data. In practice, a popular analytical
approach for dealing with missing data is to use only the set of complete
cases, i.e., observations with all features fully observed to train a
prediction algorithm. However, depending on the missing data mechanism, the
distribution of complete cases and the distribution of the complete data may be
substantially different. When the goal is to develop a fair algorithm in the
complete data domain where there are no missing values, an algorithm that is
fair in the complete case domain may show disproportionate bias towards some
marginalized groups in the complete data domain. To fill this significant gap,
we study the problem of estimating fairness in the complete data domain for an
arbitrary model evaluated merely using complete cases. We provide upper and
lower bounds on the fairness estimation error and conduct numerical experiments
to assess our theoretical results. Our work provides the first known
theoretical results on fairness guarantee in analysis of incomplete data.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Multi-Task Learning on Networks</b></summary>
  <p><b>编号</b>：[77]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04891</p>
  <p><b>作者</b>：Andrea Ponti</p>
  <p><b>备注</b>：94 pages, 53 figures, 8 tables</p>
  <p><b>关键词</b>：radically different approach based, conflicting objectives requires modelling, new computational approach represent, objective optimization problems arising, computational results show</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The multi-task learning (MTL) paradigm can be traced back to an early paper
of Caruana (1997) in which it was argued that data from multiple tasks can be
used with the aim to obtain a better performance over learning each task
independently. A solution of MTL with conflicting objectives requires modelling
the trade-off among them which is generally beyond what a straight linear
combination can achieve. A theoretically principled and computationally
effective strategy is finding solutions which are not dominated by others as it
is addressed in the Pareto analysis. Multi-objective optimization problems
arising in the multi-task learning context have specific features and require
adhoc methods. The analysis of these features and the proposal of a new
computational approach represent the focus of this work. Multi-objective
evolutionary algorithms (MOEAs) can easily include the concept of dominance and
therefore the Pareto analysis. The major drawback of MOEAs is a low sample
efficiency with respect to function evaluations. The key reason for this
drawback is that most of the evolutionary approaches do not use models for
approximating the objective function. Bayesian Optimization takes a radically
different approach based on a surrogate model, such as a Gaussian Process. In
this thesis the solutions in the Input Space are represented as probability
distributions encapsulating the knowledge contained in the function
evaluations. In this space of probability distributions, endowed with the
metric given by the Wasserstein distance, a new algorithm MOEA/WST can be
designed in which the model is not directly on the objective function but in an
intermediate Information Space where the objects from the input space are
mapped into histograms. Computational results show that the sample efficiency
and the quality of the Pareto set provided by MOEA/WST are significantly better
than in the standard MOEA.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Artificial Intelligence and Design of Experiments for Assessing Security  of Electricity Supply: A Review and Strategic Outlook</b></summary>
  <p><b>编号</b>：[78]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04889</p>
  <p><b>作者</b>：Jan Priesmann,  Justin Münch,  Elias Ridha,  Thomas Spiegel,  Marius Reich,  Mario Adam,  Lars Nolting,  Aaron Praktiknjo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：electricity supply models using ai methods, energy systems requires adequate methods, uncertainty increases likewise calling, energy system modeling leading, accelerate current methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Assessing the effects of the energy transition and liberalization of energy
markets on resource adequacy is an increasingly important and demanding task.
The rising complexity in energy systems requires adequate methods for energy
system modeling leading to increased computational requirements. Furthermore,
with complexity, uncertainty increases likewise calling for probabilistic
assessments and scenario analyses. To adequately and efficiently address these
various requirements, new methods from the field of data science are needed to
accelerate current methods. With our systematic literature review, we want to
close the gap between the three disciplines (1) assessment of security of
electricity supply, (2) artificial intelligence, and (3) design of experiments.
For this, we conduct a large-scale quantitative review on selected fields of
application and methods and make a synthesis that relates the different
disciplines to each other. Among other findings, we identify metamodeling of
complex security of electricity supply models using AI methods and applications
of AI-based methods for forecasts of storage dispatch and (non-)availabilities
as promising fields of application that have not sufficiently been covered,
yet. We end with deriving a new methodological pipeline for adequately and
efficiently addressing the present and upcoming challenges in the assessment of
security of electricity supply.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：KGE-CL: Contrastive Learning of Knowledge Graph Embeddings</b></summary>
  <p><b>编号</b>：[82]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04871</p>
  <p><b>作者</b>：Wentao Xu,  Zhiping Luo,  Weiqing Liu,  Jiang Bian,  Jian Yin,  Tie-Yan Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：simple yet efficient contrastive learning framework, previous knowledge graph embedding methods ignore, three standard knowledge graph benchmarks, benefit various downstream applications, knowledge graph embedding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Learning the embeddings of knowledge graphs is vital in artificial
intelligence, and can benefit various downstream applications, such as
recommendation and question answering. In recent years, many research efforts
have been proposed for knowledge graph embedding. However, most previous
knowledge graph embedding methods ignore the semantic similarity between the
related entities and entity-relation couples in different triples since they
separately optimize each triple with the scoring function. To address this
problem, we propose a simple yet efficient contrastive learning framework for
knowledge graph embeddings, which can shorten the semantic distance of the
related entities and entity-relation couples in different triples and thus
improve the expressiveness of knowledge graph embeddings. We evaluate our
proposed method on three standard knowledge graph benchmarks. It is noteworthy
that our method can yield some new state-of-the-art results, achieving 51.2%
MRR, 46.8% Hits@1 on the WN18RR dataset, and 59.1% MRR, 51.8% Hits@1 on the
YAGO3-10 dataset.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Siamese Attribute-missing Graph Auto-encoder</b></summary>
  <p><b>编号</b>：[89]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04842</p>
  <p><b>作者</b>：Wenxuan Tu,  Sihang Zhou,  Yue Liu,  Xinwang Liu</p>
  <p><b>备注</b>：under review</p>
  <p><b>关键词</b>：structural constraint enhanced learning mechanism, six benchmark datasets demonstrate, recently attracted considerable attention, common yet challenging problem, less discriminative feature representations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Graph representation learning (GRL) on attribute-missing graphs, which is a
common yet challenging problem, has recently attracted considerable attention.
We observe that existing literature: 1) isolates the learning of attribute and
structure embedding thus fails to take full advantages of the two types of
information; 2) imposes too strict distribution assumption on the latent space
variables, leading to less discriminative feature representations. In this
paper, based on the idea of introducing intimate information interaction
between the two information sources, we propose our Siamese Attribute-missing
Graph Auto-encoder (SAGA). Specifically, three strategies have been conducted.
First, we entangle the attribute embedding and structure embedding by
introducing a siamese network structure to share the parameters learned by both
processes, which allows the network training to benefit from more abundant and
diverse information. Second, we introduce a K-nearest neighbor (KNN) and
structural constraint enhanced learning mechanism to improve the quality of
latent features of the missing attributes by filtering unreliable connections.
Third, we manually mask the connections on multiple adjacent matrices and force
the structural information embedding sub-network to recover the true adjacent
matrix, thus enforcing the resulting network to be able to selectively exploit
more high-order discriminative features for data completion. Extensive
experiments on six benchmark datasets demonstrate the superiority of our SAGA
against the state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Explainability of the Implications of Supervised and Unsupervised Face  Image Quality Estimations Through Activation Map Variation Analyses in Face  Recognition Models</b></summary>
  <p><b>编号</b>：[94]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04827</p>
  <p><b>作者</b>：Biying Fu,  Naser Damer</p>
  <p><b>备注</b>：accepted at the IEEE Winter Conference on Applications of Computer Vision Workshops, WACV Workshops 2022</p>
  <p><b>关键词</b>：quality images typically cause consistent low activation, based fr solution using activation mapping, based face image quality assessment, general spatial activation mapping, issues like extreme poses</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>It is challenging to derive explainability for unsupervised or
statistical-based face image quality assessment (FIQA) methods. In this work,
we propose a novel set of explainability tools to derive reasoning for
different FIQA decisions and their face recognition (FR) performance
implications. We avoid limiting the deployment of our tools to certain FIQA
methods by basing our analyses on the behavior of FR models when processing
samples with different FIQA decisions. This leads to explainability tools that
can be applied for any FIQA method with any CNN-based FR solution using
activation mapping to exhibit the network's activation derived from the face
embedding. To avoid the low discrimination between the general spatial
activation mapping of low and high-quality images in FR models, we build our
explainability tools in a higher derivative space by analyzing the variation of
the FR activation maps of image sets with different quality decisions. We
demonstrate our tools and analyze the findings on four FIQA methods, by
presenting inter and intra-FIQA method analyses. Our proposed tools and the
analyses based on them point out, among other conclusions, that high-quality
images typically cause consistent low activation on the areas outside of the
central face region, while low-quality images, despite general low activation,
have high variations of activation in such areas. Our explainability tools also
extend to analyzing single images where we show that low-quality images tend to
have an FR model spatial activation that strongly differs from what is expected
from a high-quality image where this difference also tends to appear more in
areas outside of the central face region and does correspond to issues like
extreme poses and facial occlusions. The implementation of the proposed tools
is accessible here [link].</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Complexity assessments for decidable fragments of Set Theory. III: A  quadratic reduction of constraints over nested sets to Boolean formulae</b></summary>
  <p><b>编号</b>：[101]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04797</p>
  <p><b>作者</b>：Domenico Cantone,  Andrea De Domenico,  Pietro Maugeri,  Eugenio G. Omodeo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：rather simple conjunctive normal form, target language involve variables ranging, z =\{ x \}$,, von neumann universe, relators designating equality</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As a contribution to quantitative set-theoretic inferencing, a translation is
proposed of conjunctions of literals of the forms $x=y\setminus z$, $x \neq
y\setminus z$, and $z =\{x\}$, where $x,y,z$ stand for variables ranging over
the von Neumann universe of sets, into unquantified Boolean formulae of a
rather simple conjunctive normal form. The formulae in the target language
involve variables ranging over a Boolean ring of sets, along with a difference
operator and relators designating equality, non-disjointness and inclusion.
Moreover, the result of each translation is a conjunction of literals of the
forms $x=y\setminus z$, $x\neq y\setminus z$ and of implications whose
antecedents are isolated literals and whose consequents are either inclusions
(strict or non-strict) between variables, or equalities between variables.
Besides reflecting a simple and natural semantics, which ensures
satisfiability-preservation, the proposed translation has quadratic algorithmic
time-complexity, and bridges two languages both of which are known to have an
NP-complete satisfiability problem.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：VMAgent: Scheduling Simulator for Reinforcement Learning</b></summary>
  <p><b>编号</b>：[104]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04785</p>
  <p><b>作者</b>：Junjie Sheng,  Shengliang Cai,  Haochuan Cui,  Wenhao Li,  Yun Hua,  Bo Jin,  Wenli Zhou,  Yiqiu Hu,  Lei Zhu,  Qian Peng,  Hongyuan Zha,  Xiangfeng Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：customized scheduling environments considering different problem features, help rl researchers better explore new methods, many reinforcement learning challenges, novel simulator called vmagent, vmagent provides flexible configurations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A novel simulator called VMAgent is introduced to help RL researchers better
explore new methods, especially for virtual machine scheduling. VMAgent is
inspired by practical virtual machine (VM) scheduling tasks and provides an
efficient simulation platform that can reflect the real situations of cloud
computing. Three scenarios (fading, recovering, and expansion) are concluded
from practical cloud computing and corresponds to many reinforcement learning
challenges (high dimensional state and action spaces, high non-stationarity,
and life-long demand). VMAgent provides flexible configurations for RL
researchers to design their customized scheduling environments considering
different problem features. From the VM scheduling perspective, VMAgent also
helps to explore better learning-based scheduling solutions.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Co-evolutionary hybrid intelligence</b></summary>
  <p><b>编号</b>：[113]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04751</p>
  <p><b>作者</b>：Kirill Krinkin,  Yulia Shichkina,  Andrey Ignatyev</p>
  <p><b>备注</b>：4 pages</p>
  <p><b>关键词</b>：training neural networks requires huge computational, artificial intelligence systems based, modeling complex objects, modern technological development, artificial intelligence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Artificial intelligence is one of the drivers of modern technological
development. The current approach to the development of intelligent systems is
data-centric. It has several limitations: it is fundamentally impossible to
collect data for modeling complex objects and processes; training neural
networks requires huge computational and energy resources; solutions are not
explainable. The article discusses an alternative approach to the development
of artificial intelligence systems based on human-machine hybridization and
their co-evolution.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：LipSound2: Self-Supervised Pre-Training for Lip-to-Speech Reconstruction  and Lip Reading</b></summary>
  <p><b>编号</b>：[115]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04748</p>
  <p><b>作者</b>：Leyuan Qu,  Cornelius Weber,  Stefan Wermter</p>
  <p><b>备注</b>：SUBMITTED TO IEEE Transaction on Neural Networks and Learning Systems</p>
  <p><b>关键词</b>：scale spectrograms directly without requiring, map face image sequences, conduct chinese speech reconstruction, trained speech recognition system, chinese benchmark datasets</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The aim of this work is to investigate the impact of crossmodal
self-supervised pre-training for speech reconstruction (video-to-audio) by
leveraging the natural co-occurrence of audio and visual streams in videos. We
propose LipSound2 which consists of an encoder-decoder architecture and
location-aware attention mechanism to map face image sequences to mel-scale
spectrograms directly without requiring any human annotations. The proposed
LipSound2 model is firstly pre-trained on $\sim$2400h multi-lingual (e.g.
English and German) audio-visual data (VoxCeleb2). To verify the
generalizability of the proposed method, we then fine-tune the pre-trained
model on domain-specific datasets (GRID, TCD-TIMIT) for English speech
reconstruction and achieve a significant improvement on speech quality and
intelligibility compared to previous approaches in speaker-dependent and
-independent settings. In addition to English, we conduct Chinese speech
reconstruction on the CMLR dataset to verify the impact on transferability.
Lastly, we train the cascaded lip reading (video-to-text) system by fine-tuning
the generated audios on a pre-trained speech recognition system and achieve
state-of-the-art performance on both English and Chinese benchmark datasets.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Learning multiple gaits of quadruped robot using hierarchical  reinforcement learning</b></summary>
  <p><b>编号</b>：[118]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04741</p>
  <p><b>作者</b>：Yunho Kim,  Bukun Son,  Dongjun Lee</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：quadruped robot using reinforcement learning due, experiment results show 1, specific velocity range 2, could generate multiple gaits, velocity command tracking controller</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>There is a growing interest in learning a velocity command tracking
controller of quadruped robot using reinforcement learning due to its
robustness and scalability. However, a single policy, trained end-to-end,
usually shows a single gait regardless of the command velocity. This could be a
suboptimal solution considering the existence of optimal gait according to the
velocity for quadruped animals. In this work, we propose a hierarchical
controller for quadruped robot that could generate multiple gaits (i.e. pace,
trot, bound) while tracking velocity command. Our controller is composed of two
policies, each working as a central pattern generator and local feedback
controller, and trained with hierarchical reinforcement learning. Experiment
results show 1) the existence of optimal gait for specific velocity range 2)
the efficiency of our hierarchical controller compared to a controller composed
of a single policy, which usually shows a single gait. Codes are publicly
available.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：From Good to Best: Two-Stage Training for Cross-lingual Machine Reading  Comprehension</b></summary>
  <p><b>编号</b>：[120]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04735</p>
  <p><b>作者</b>：Nuo Chen,  Linjun Shou,  Min Gong,  Jian Pei,  Daxin Jiang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：previous approaches may often fail, recent approaches use training data, rich language like english, lingual mrc benchmark datasets, lingual machine reading comprehension</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cross-lingual Machine Reading Comprehension (xMRC) is challenging due to the
lack of training data in low-resource languages. The recent approaches use
training data only in a resource-rich language like English to fine-tune
large-scale cross-lingual pre-trained language models. Due to the big
difference between languages, a model fine-tuned only by a source language may
not perform well for target languages. Interestingly, we observe that while the
top-1 results predicted by the previous approaches may often fail to hit the
ground-truth answers, the correct answers are often contained in the top-k
predicted results. Based on this observation, we develop a two-stage approach
to enhance the model performance. The first stage targets at recall: we design
a hard-learning (HL) algorithm to maximize the likelihood that the top-k
predictions contain the accurate answer. The second stage focuses on precision:
an answer-aware contrastive learning (AA-CL) mechanism is developed to learn
the fine difference between the accurate answer and other candidates. Our
extensive experiments show that our model significantly outperforms a series of
strong baselines on two cross-lingual MRC benchmark datasets.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Explainable AI for B5G/6G: Technical Aspects, Use Cases, and Research  Challenges</b></summary>
  <p><b>编号</b>：[139]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04698</p>
  <p><b>作者</b>：Shen Wang,  M.Atif Qureshi,  Luis Miralles-Pechuaán,  Thien Huynh-The,  Thippa Reddy Gadekallu,  Madhusanka Liyanage</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：united nations sustainable development goals, automated decisions made every second, sdg ), promoting innovation, g ., intelligent radio, g ., industry 5</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>When 5G began its commercialisation journey around 2020, the discussion on
the vision of 6G also surfaced. Researchers expect 6G to have higher bandwidth,
coverage, reliability, energy efficiency, lower latency, and, more importantly,
an integrated "human-centric" network system powered by artificial intelligence
(AI). Such a 6G network will lead to an excessive number of automated decisions
made every second. These decisions can range widely, from network resource
allocation to collision avoidance for self-driving cars. However, the risk of
losing control over decision-making may increase due to high-speed
data-intensive AI decision-making beyond designers and users' comprehension.
The promising explainable AI (XAI) methods can mitigate such risks by enhancing
the transparency of the black box AI decision-making process. This survey paper
highlights the need for XAI towards the upcoming 6G age in every aspect,
including 6G technologies (e.g., intelligent radio, zero-touch network
management) and 6G use cases (e.g., industry 5.0). Moreover, we summarised the
lessons learned from the recent attempts and outlined important research
challenges in applying XAI for building 6G systems. This research aligns with
goals 9, 11, 16, and 17 of the United Nations Sustainable Development Goals
(UN-SDG), promoting innovation and building infrastructure, sustainable and
inclusive human settlement, advancing justice and strong institutions, and
fostering partnership at the global level.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：CWS-PResUNet: Music Source Separation with Channel-wise Subband  Phase-aware ResUNet</b></summary>
  <p><b>编号</b>：[142]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04685</p>
  <p><b>作者</b>：Haohe Liu,  Qiuqiang Kong,  Jiafeng Liu</p>
  <p><b>备注</b>：Published at MDX Workshop @ ISMIR 2021</p>
  <p><b>关键词</b>：challenge limited training data track, limit unnecessary global weights sharing, unbound complex ideal ratio mask, many mss models perform separations, using convolutional neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Music source separation (MSS) shows active progress with deep learning models
in recent years. Many MSS models perform separations on spectrograms by
estimating bounded ratio masks and reusing the phases of the mixture. When
using convolutional neural networks (CNN), weights are usually shared within a
spectrogram during convolution regardless of the different patterns between
frequency bands. In this study, we propose a new MSS model, channel-wise
subband phase-aware ResUNet (CWS-PResUNet), to decompose signals into subbands
and estimate an unbound complex ideal ratio mask (cIRM) for each source.
CWS-PResUNet utilizes a channel-wise subband (CWS) feature to limit unnecessary
global weights sharing on the spectrogram and reduce computational resource
consumptions. The saved computational cost and memory can in turn allow for a
larger architecture. On the MUSDB18HQ test set, we propose a 276-layer
CWS-PResUNet and achieve state-of-the-art (SoTA) performance on vocals with an
8.92 signal-to-distortion ratio (SDR) score. By combining CWS-PResUNet and
Demucs, our ByteMSS system ranks the 2nd on vocals score and 5th on average
score in the 2021 ISMIR Music Demixing (MDX) Challenge limited training data
track (leaderboard A). Our code and pre-trained models are publicly available
at: this https URL</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：DualFormer: Local-Global Stratified Transformer for Efficient Video  Recognition</b></summary>
  <p><b>编号</b>：[147]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04674</p>
  <p><b>作者</b>：Yuxuan Liang,  Pan Zhou,  Roger Zimmermann,  Shuicheng Yan</p>
  <p><b>备注</b>：Preprint</p>
  <p><b>关键词</b>：often suffer high computational costs induced, restrict attention computations within local windows, time interactions among nearby 3d tokens, around 1000g inference flops, grained global pyramid contexts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While transformers have shown great potential on video recognition tasks with
their strong capability of capturing long-range dependencies, they often suffer
high computational costs induced by self-attention operation on the huge number
of 3D tokens in a video. In this paper, we propose a new transformer
architecture, termed DualFormer, which can effectively and efficiently perform
space-time attention for video recognition. Specifically, our DualFormer
stratifies the full space-time attention into dual cascaded levels, i.e., to
first learn fine-grained local space-time interactions among nearby 3D tokens,
followed by the capture of coarse-grained global dependencies between the query
token and the coarse-grained global pyramid contexts. Different from existing
methods that apply space-time factorization or restrict attention computations
within local windows for improving efficiency, our local-global stratified
strategy can well capture both short- and long-range spatiotemporal
dependencies, and meanwhile greatly reduces the number of keys and values in
attention computation to boost efficiency. Experimental results show the
superiority of DualFormer on five video benchmarks against existing methods. In
particular, DualFormer sets new state-of-the-art 82.9%/85.2% top-1 accuracy on
Kinetics-400/600 with around 1000G inference FLOPs which is at least 3.2 times
fewer than existing methods with similar performances.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Enhancing Food Intake Tracking in Long-Term Care with Automated Food  Imaging and Nutrient Intake Tracking (AFINI-T) Technology</b></summary>
  <p><b>编号</b>：[171]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04608</p>
  <p><b>作者</b>：Kaylen J. Pfisterer,  Robert Amelard,  Jennifer Boger,  Audrey G. Chung,  Heather H. Keller,  Alexander Wong</p>
  <p><b>备注</b>：Key words: Automatic segmentation, convolutional neural network, deep learning, food intake tracking, volume estimation, malnutrition prevention, long-term care, hospital</p>
  <p><b>关键词</b>：learning powered computational nutrient sensing system, objectively tracking ltc resident food intake, 9 %; mean intake error, simulated ltc food intake dataset, prevent malnutrition tracking strategies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Half of long-term care (LTC) residents are malnourished increasing
hospitalization, mortality, morbidity, with lower quality of life. Current
tracking methods are subjective and time consuming. This paper presents the
automated food imaging and nutrient intake tracking (AFINI-T) technology
designed for LTC. We propose a novel convolutional autoencoder for food
classification, trained on an augmented UNIMIB2016 dataset and tested on our
simulated LTC food intake dataset (12 meal scenarios; up to 15 classes each;
top-1 classification accuracy: 88.9%; mean intake error: -0.4 mL$\pm$36.7 mL).
Nutrient intake estimation by volume was strongly linearly correlated with
nutrient estimates from mass ($r^2$ 0.92 to 0.99) with good agreement between
methods ($\sigma$= -2.7 to -0.01; zero within each of the limits of agreement).
The AFINI-T approach is a deep-learning powered computational nutrient sensing
system that may provide a novel means for more accurately and objectively
tracking LTC resident food intake to support and prevent malnutrition tracking
strategies.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Prediction of Adverse Biological Effects of Chemicals Using Knowledge  Graph Embeddings</b></summary>
  <p><b>编号</b>：[173]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04605</p>
  <p><b>作者</b>：Erik B. Myklebust,  Ernesto Jiménez-Ruiz,  Jiaoyan Chen,  Raoul Wolf,  Knut Erik Tollefsen</p>
  <p><b>备注</b>：Accepted for publication in the Semantic Web Journal</p>
  <p><b>关键词</b>：evaluated nine knowledge graph embedding models, knowledge graph embedding models, major data sources used, using knowledge graph embeddings, namely chemical effect prediction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We have created a knowledge graph based on major data sources used in
ecotoxicological risk assessment. We have applied this knowledge graph to an
important task in risk assessment, namely chemical effect prediction. We have
evaluated nine knowledge graph embedding models from a selection of geometric,
decomposition, and convolutional models on this prediction task. We show that
using knowledge graph embeddings can increase the accuracy of effect prediction
with neural networks. Furthermore, we have implemented a fine-tuning
architecture which adapts the knowledge graph embeddings to the effect
prediction task and leads to a better performance. Finally, we evaluate certain
characteristics of the knowledge graph embedding models to shed light on the
individual model performance.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Refined Commonsense Knowledge from Large-Scale Web Contents</b></summary>
  <p><b>编号</b>：[178]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04596</p>
  <p><b>作者</b>：Tuan-Phong Nguyen,  Simon Razniewski,  Julien Romero,  Gerhard Weikum</p>
  <p><b>备注</b>：This is a substantial extension of the WWW paper (arXiv:2011.00905). arXiv admin note: substantial text overlap with arXiv:2011.00905</p>
  <p><b>关键词</b>：ascent ++ combines open information extraction, ascent ++ goes beyond spo triples, others compiled large csk collections, prior works like conceptnet, ascent ++ kb</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Commonsense knowledge (CSK) about concepts and their properties is useful for
AI applications. Prior works like ConceptNet, COMET and others compiled large
CSK collections, but are restricted in their expressiveness to
subject-predicate-object (SPO) triples with simple concepts for S and strings
for P and O. This paper presents a method, called ASCENT++, to automatically
build a large-scale knowledge base (KB) of CSK assertions, with refined
expressiveness and both better precision and recall than prior works. ASCENT++
goes beyond SPO triples by capturing composite concepts with subgroups and
aspects, and by refining assertions with semantic facets. The latter is
important to express the temporal and spatial validity of assertions and
further qualifiers. ASCENT++ combines open information extraction with
judicious cleaning and ranking by typicality and saliency scores. For high
coverage, our method taps into the large-scale crawl C4 with broad web
contents. The evaluation with human judgements shows the superior quality of
the ASCENT++ KB, and an extrinsic evaluation for QA-support tasks underlines
the benefits of ASCENT++. A web interface, data and code can be accessed at
this https URL.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Application of Artificial Intelligence and Machine Learning in  Libraries: A Systematic Review</b></summary>
  <p><b>编号</b>：[187]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04573</p>
  <p><b>作者</b>：Rajesh Kumar Das,  Mohammad Sharif Ul Islam</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：edge technologies like artificial intelligence, anticipating future innovation pathways, lis domain mainly focuses, information professionals involve research, empirical studies exploring application</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As the concept and implementation of cutting-edge technologies like
artificial intelligence and machine learning has become relevant, academics,
researchers and information professionals involve research in this area. The
objective of this systematic literature review is to provide a synthesis of
empirical studies exploring application of artificial intelligence and machine
learning in libraries. To achieve the objectives of the study, a systematic
literature review was conducted based on the original guidelines proposed by
Kitchenham et al. (2009). Data was collected from Web of Science, Scopus, LISA
and LISTA databases. Following the rigorous/ established selection process, a
total of thirty-two articles were finally selected, reviewed and analyzed to
summarize on the application of AI and ML domain and techniques which are most
often used in libraries. Findings show that the current state of the AI and ML
research that is relevant with the LIS domain mainly focuses on theoretical
works. However, some researchers also emphasized on implementation projects or
case studies. This study will provide a panoramic view of AI and ML in
libraries for researchers, practitioners and educators for furthering the more
technology-oriented approaches, and anticipating future innovation pathways.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：PATO: Producibility-Aware Topology Optimization using Deep Learning for  Metal Additive Manufacturing</b></summary>
  <p><b>编号</b>：[197]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04552</p>
  <p><b>作者</b>：Naresh S. Iyer,  Amir M. Mirzendehdel,  Sathyanarayanan Raghavan,  Yang Jiao,  Erva Ulu,  Morad Behandish,  Saigopal Nelaturi,  Dean M. Robinson</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：components fabricated using metal additive manufacturing, high residual stress values generated, steep thermal gradients produced, laser powder bed fusion, deep convolutional neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we propose PATO-a producibility-aware topology optimization
(TO) framework to help efficiently explore the design space of components
fabricated using metal additive manufacturing (AM), while ensuring
manufacturability with respect to cracking. Specifically, parts fabricated
through Laser Powder Bed Fusion are prone to defects such as warpage or
cracking due to high residual stress values generated from the steep thermal
gradients produced during the build process. Maturing the design for such parts
and planning their fabrication can span months to years, often involving
multiple handoffs between design and manufacturing engineers. PATO is based on
the a priori discovery of crack-free designs, so that the optimized part can be
built defect-free at the outset. To ensure that the design is crack free during
optimization, producibility is explicitly encoded within the standard
formulation of TO, using a crack index. Multiple crack indices are explored and
using experimental validation, maximum shear strain index (MSSI) is shown to be
an accurate crack index. Simulating the build process is a coupled,
multi-physics computation and incorporating it in the TO loop can be
computationally prohibitive. We leverage the current advances in deep
convolutional neural networks and present a high-fidelity surrogate model based
on an Attention-based U-Net architecture to predict the MSSI values as a
spatially varying field over the part's domain. Further, we employ automatic
differentiation to directly compute the gradient of maximum MSSI with respect
to the input design variables and augment it with the performance-based
sensitivity field to optimize the design while considering the trade-off
between weight, manufacturability, and functionality. We demonstrate the
effectiveness of the proposed method through benchmark studies in 3D as well as
experimental validation.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Deep Q-Learning Market Makers in a Multi-Agent Simulated Stock Market</b></summary>
  <p><b>编号</b>：[206]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04494</p>
  <p><b>作者</b>：Oscar Fernández Vicente,  Fernando Fernández Rebollo,  Francisco Javier García Polo</p>
  <p><b>备注</b>：Presented at 2nd ACM International Conference on AI in Finance</p>
  <p><b>关键词</b>：provide traders alternative price levels, rl market maker agents behaves, one rl market maker learning, multiple rl market markers learning, profitable market maker approaches</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Market makers play a key role in financial markets by providing liquidity.
They usually fill order books with buy and sell limit orders in order to
provide traders alternative price levels to operate. This paper focuses
precisely on the study of these markets makers strategies from an agent-based
perspective. In particular, we propose the application of Reinforcement
Learning (RL) for the creation of intelligent market markers in simulated stock
markets. This research analyzes how RL market maker agents behaves in
non-competitive (only one RL market maker learning at the same time) and
competitive scenarios (multiple RL market markers learning at the same time),
and how they adapt their strategies in a Sim2Real scope with interesting
results. Furthermore, it covers the application of policy transfer between
different experiments, describing the impact of competing environments on RL
agents performance. RL and deep RL techniques are proven as profitable market
maker approaches, leading to a better understanding of their behavior in stock
markets.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Provable Continual Learning via Sketched Jacobian Approximations</b></summary>
  <p><b>编号</b>：[211]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.05095</p>
  <p><b>作者</b>：Reinhard Heckel</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：based continual learning algorithms work, provably enables overcoming catastrophic forgetting, models forget previously learned tasks, provably suffer catastrophic forgetting, diagonal matrix build based</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>An important problem in machine learning is the ability to learn tasks in a
sequential manner. If trained with standard first-order methods most models
forget previously learned tasks when trained on a new task, which is often
referred to as catastrophic forgetting. A popular approach to overcome
forgetting is to regularize the loss function by penalizing models that perform
poorly on previous tasks. For example, elastic weight consolidation (EWC)
regularizes with a quadratic form involving a diagonal matrix build based on
past data. While EWC works very well for some setups, we show that, even under
otherwise ideal conditions, it can provably suffer catastrophic forgetting if
the diagonal matrix is a poor approximation of the Hessian matrix of previous
tasks. We propose a simple approach to overcome this: Regularizing training of
a new task with sketches of the Jacobian matrix of past data. This provably
enables overcoming catastrophic forgetting for linear models and for wide
neural networks, at the cost of memory. The overarching goal of this paper is
to provided insights on when regularization-based continual learning algorithms
work and under what memory costs.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Enhancing Column Generation by a Machine-Learning-Based Pricing  Heuristic for Graph Coloring</b></summary>
  <p><b>编号</b>：[226]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04906</p>
  <p><b>作者</b>：Yunzhuang Shen,  Yuan Sun,  Xiaodong Li,  Andrew Eberhard,  Andreas Ernst</p>
  <p><b>备注</b>：Machine learning for column generation and branch-and-price; accepted to AAAI 2022</p>
  <p><b>关键词</b>：efficiently generate multiple high, gradually includes new columns, generate many high, substantially better performance, scale optimization problems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Column Generation (CG) is an effective method for solving large-scale
optimization problems. CG starts by solving a sub-problem with a subset of
columns (i.e., variables) and gradually includes new columns that can improve
the solution of the current subproblem. The new columns are generated as needed
by repeatedly solving a pricing problem, which is often NP-hard and is a
bottleneck of the CG approach. To tackle this, we propose a
Machine-Learning-based Pricing Heuristic (MLPH)that can generate many
high-quality columns efficiently. In each iteration of CG, our MLPH leverages
an ML model to predict the optimal solution of the pricing problem, which is
then used to guide a sampling method to efficiently generate multiple
high-quality columns. Using the graph coloring problem, we empirically show
that MLPH significantly enhancesCG as compared to six state-of-the-art methods,
and the improvement in CG can lead to substantially better performance of the
branch-and-price exact method.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：One-dimensional Deep Low-rank and Sparse Network for Accelerated MRI</b></summary>
  <p><b>编号</b>：[237]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2112.04721</p>
  <p><b>作者</b>：Zi Wang,  Chen Qian,  Di Guo,  Hongwei Sun,  Rushuai Li,  Bo Zhao,  Xiaobo Qu</p>
  <p><b>备注</b>：16 pages</p>
  <p><b>关键词</b>：perform 2d convolution since many magnetic resonance images, odls also shows nice robustness, art deep learning reconstructions adopt, accelerated magnetic resonance imaging, powerful convolutional neural network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning has shown astonishing performance in accelerated magnetic
resonance imaging (MRI). Most state-of-the-art deep learning reconstructions
adopt the powerful convolutional neural network and perform 2D convolution
since many magnetic resonance images or their corresponding k-space are in 2D.
In this work, we present a new approach that explores the 1D convolution,
making the deep network much easier to be trained and generalized. We further
integrate the 1D convolution into the proposed deep network, named as
One-dimensional Deep Low-rank and Sparse network (ODLS), which unrolls the
iteration procedure of a low-rank and sparse reconstruction model. Extensive
results on in vivo knee and brain datasets demonstrate that, the proposed ODLS
is very suitable for the case of limited training subjects and provides
improved reconstruction performance than state-of-the-art methods both visually
and quantitatively. Additionally, ODLS also shows nice robustness to different
undersampling scenarios and some mismatches between the training and test data.
In summary, our work demonstrates that the 1D deep learning scheme is
memory-efficient and robust in fast MRI.</p>
  </details>
</details>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">徐耀彬</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://louishsu.xyz/2021/12/13/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">http://louishsu.xyz/2021/12/13/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://louishsu.xyz" target="_blank">LOUIS' BLOG</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2021/10/22/%E4%B8%AD%E5%9B%BD%E6%B3%95%E5%BE%8B%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E8%AF%84%E6%B5%8B(CAIL2021)%EF%BC%9A%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96(Rank2).html"><img class="next-cover" src="http://cail.cipsc.org.cn/img/index_mainpic.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">徐耀彬</div><div class="author-info__description">做知识的原创者！</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">11</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/isLouisHsu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/isLouisHsu" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:is.louishsu@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">记录和分享一些学习和开源内容，若有问题可通过邮箱is.louishsu@foxmail.com联系，欢迎交流！！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">统计</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">计算机视觉</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">自然语言处理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">机器学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">人工智能</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/12/13/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2021-12-13)"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv每日速递(2021-12-13)"/></a><div class="content"><a class="title" href="/2021/12/13/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2021-12-13)">Arxiv每日速递(2021-12-13)</a><time datetime="2021-12-13T00:25:58.485Z" title="发表于 2021-12-13 08:25:58">2021-12-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/10/22/%E4%B8%AD%E5%9B%BD%E6%B3%95%E5%BE%8B%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E8%AF%84%E6%B5%8B(CAIL2021)%EF%BC%9A%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96(Rank2).html" title="中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)"><img src="http://cail.cipsc.org.cn/img/index_mainpic.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)"/></a><div class="content"><a class="title" href="/2021/10/22/%E4%B8%AD%E5%9B%BD%E6%B3%95%E5%BE%8B%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E8%AF%84%E6%B5%8B(CAIL2021)%EF%BC%9A%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96(Rank2).html" title="中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)">中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)</a><time datetime="2021-10-22T14:29:06.000Z" title="发表于 2021-10-22 22:29:06">2021-10-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/05/19/%E5%85%A8%E7%90%83%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0%E5%A4%A7%E8%B5%9B%E3%80%90%E8%B5%9B%E9%81%93%E4%B8%80%E3%80%91%EF%BC%9A%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E6%8A%A5%E5%91%8A%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B.html" title="全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测"><img src="https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161037709574435991610377095138.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测"/></a><div class="content"><a class="title" href="/2021/05/19/%E5%85%A8%E7%90%83%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0%E5%A4%A7%E8%B5%9B%E3%80%90%E8%B5%9B%E9%81%93%E4%B8%80%E3%80%91%EF%BC%9A%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E6%8A%A5%E5%91%8A%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B.html" title="全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测">全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测</a><time datetime="2021-05-19T09:57:06.000Z" title="发表于 2021-05-19 17:57:06">2021-05-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/09/16/%E8%AF%A6%E8%A7%A3%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%EF%BC%9ALSTM-CRF.html" title="详解命名实体识别模型：LSTM-CRF"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fpic1.zhimg.com%2Fv2-1ed6a10dbb239a148e42df088c5870a6_1440w.jpg%3Fsource%3D172ae18b&amp;refer=http%3A%2F%2Fpic1.zhimg.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1638183974&amp;t=4632f13fd487ed1cfcb12d67a6b71e52" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="详解命名实体识别模型：LSTM-CRF"/></a><div class="content"><a class="title" href="/2020/09/16/%E8%AF%A6%E8%A7%A3%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%EF%BC%9ALSTM-CRF.html" title="详解命名实体识别模型：LSTM-CRF">详解命名实体识别模型：LSTM-CRF</a><time datetime="2020-09-16T09:26:44.000Z" title="发表于 2020-09-16 17:26:44">2020-09-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/06/14/%E8%AE%B0%E4%B8%80%E6%AC%A1MySQL%E8%A7%A3%E5%86%B3%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E6%97%B6%E7%9A%84%E5%88%86%E8%A1%A8%E9%97%AE%E9%A2%98.html" title="记一次MySQL解决大数据存储时的分表问题"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="记一次MySQL解决大数据存储时的分表问题"/></a><div class="content"><a class="title" href="/2020/06/14/%E8%AE%B0%E4%B8%80%E6%AC%A1MySQL%E8%A7%A3%E5%86%B3%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E6%97%B6%E7%9A%84%E5%88%86%E8%A1%A8%E9%97%AE%E9%A2%98.html" title="记一次MySQL解决大数据存储时的分表问题">记一次MySQL解决大数据存储时的分表问题</a><time datetime="2020-06-14T15:24:27.000Z" title="发表于 2020-06-14 23:24:27">2020-06-14</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2021 By 徐耀彬</div><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender"title="主题采用butterfly"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr"title="本站使用JsDelivr为静态资源提供CDN加速"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub"title="本站项目由Gtihub托管"alt="img"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"alt="img"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></br></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    let initData = {
      el: '#twikoo-wrap',
      envId: 'blog-',
      region: ''
    }

    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    twikoo.init(initData)
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'blog-',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/机器学习/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐱 机器学习 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/自然语言处理/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 自然语言处理 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/竞赛相关/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 竞赛相关 (2)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/阅读笔记/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 阅读笔记 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="http://louishsu.xyz/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style>
  <script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/hexo-github-calendar@1.21/hexo_githubcalendar.js"></script>
  <script data-pjax>
        function GithubCalendarConfig(){
            var git_githubapiurl ="https://python-github-calendar-api.vercel.app/api?isLouisHsu";
            var git_color =['#ebedf0', '#fdcdec', '#fc9bd9', '#fa6ac5', '#f838b2', '#f5089f', '#c4067e', '#92055e', '#540336', '#48022f', '#30021f'];
            var git_user ="isLouisHsu";
            var parent_div_git = document.getElementById('recent-posts');
            var git_div_html = '<div class="recent-post-item" style="width:100%;height:auto;padding:10px;"><div id="github_loading" style="width:10%;height:100%;margin:0 auto;display: block"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"  viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animateTransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animateTransform></path></svg></div><div id="github_container"></div></div>';
            if(parent_div_git && location.pathname =='/'){
                console.log('已挂载github calendar')
                // parent_div_git.innerHTML=git_div_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",git_div_html) // 有报错，但不影响使用(支持pjax跳转)
            };
            GithubCalendar(git_githubapiurl,git_color,git_user)
        }
        if(document.getElementById('recent-posts')){
            GithubCalendarConfig()
        }
    </script>
    <style>#github_container{min-height:280px}@media screen and (max-width:650px) {#github_container{background-image:;min-height:0px}}</style>
    <style></style><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>