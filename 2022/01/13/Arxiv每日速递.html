<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Arxiv每日速递(2022-01-13) | LOUIS' BLOG</title><meta name="author" content="徐耀彬"><meta name="copyright" content="徐耀彬"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。 统计 今日共更新207篇论文，其中：  45篇计算机视觉（cs.CV） 14篇自然语言处理（cs.CL） 62篇机器学习（cs.LG） 30篇人工智能（cs.AI）  计算机视觉    1. 标题：HumanNeRF: Free-viewpoint Rendering of">
<meta property="og:type" content="article">
<meta property="og:title" content="Arxiv每日速递(2022-01-13)">
<meta property="og:url" content="http://louishsu.xyz/2022/01/13/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">
<meta property="og:site_name" content="LOUIS&#39; BLOG">
<meta property="og:description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。 统计 今日共更新207篇论文，其中：  45篇计算机视觉（cs.CV） 14篇自然语言处理（cs.CL） 62篇机器学习（cs.LG） 30篇人工智能（cs.AI）  计算机视觉    1. 标题：HumanNeRF: Free-viewpoint Rendering of">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png">
<meta property="article:published_time" content="2022-01-13T00:29:47.465Z">
<meta property="article:modified_time" content="2022-01-13T00:31:31.233Z">
<meta property="article:author" content="徐耀彬">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://louishsu.xyz/2022/01/13/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-01-13 08:31:31'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/background.css"><script src="https://cdn.jsdelivr.net/npm/echarts@4.7.0/dist/echarts.min.js"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">11</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">LOUIS' BLOG</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Arxiv每日速递(2022-01-13)<a class="post-edit-link" href="https://github.com/isLouisHsu/blog/tree/master/source_posts/Arxiv每日速递.md" title="编辑" target="_blank"><i class="fas fa-pencil-square"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-01-13T00:29:47.465Z" title="发表于 2022-01-13 08:29:47">2022-01-13</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-01-13T00:31:31.233Z" title="更新于 2022-01-13 08:31:31">2022-01-13</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">阅读笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">39.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>236分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2022/01/13/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。</p>
<h1>统计</h1>
<p>今日共更新207篇论文，其中：</p>
<ul>
<li>45篇计算机视觉（<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>）</li>
<li>14篇自然语言处理（<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>）</li>
<li>62篇机器学习（cs.LG）</li>
<li>30篇人工智能（<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>）</li>
</ul>
<h1>计算机视觉</h1>
<details>
  <summary>1. <b>标题：HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular  Video</b></summary>
  <p><b>编号</b>：[3]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04127</p>
  <p><b>作者</b>：Chung-Yi Weng,  Brian Curless,  Pratul P. Srinivasan,  Jonathan T. Barron,  Ira Kemelmacher-Shlizerman</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：viewpoint rendering method -- humannerf --, human performing complex body motions, show significant performance improvements, arbitrary new camera viewpoints, requires synthesizing photorealistic details</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce a free-viewpoint rendering method -- HumanNeRF -- that works on
a given monocular video of a human performing complex body motions, e.g. a
video from YouTube. Our method enables pausing the video at any frame and
rendering the subject from arbitrary new camera viewpoints or even a full
360-degree camera path for that particular frame and body pose. This task is
particularly challenging, as it requires synthesizing photorealistic details of
the body, as seen from various camera angles that may not exist in the input
video, as well as synthesizing fine details such as cloth folds and facial
appearance. Our method optimizes for a volumetric representation of the person
in a canonical T-pose, in concert with a motion field that maps the estimated
canonical representation to every frame of the video via backward warps. The
motion field is decomposed into skeletal rigid and non-rigid motions, produced
by deep networks. We show significant performance improvements over prior work,
and compelling examples of free-viewpoint renderings from monocular video of
moving humans in challenging uncontrolled capture scenarios.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：gDNA: Towards Generative Detailed Neural Avatars</b></summary>
  <p><b>编号</b>：[5]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04123</p>
  <p><b>作者</b>：Xu Chen,  Tianjian Jiang,  Jie Song,  Jinlong Yang,  Michael J. Black,  Andreas Geiger,  Otmar Hilliges</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：make 3d human avatars widely available, generate natural human avatars wearing diverse, subject forward skinning module, yet stochastic geometric detail, rigged scans per subject</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To make 3D human avatars widely available, we must be able to generate a
variety of 3D virtual humans with varied identities and shapes in arbitrary
poses. This task is challenging due to the diversity of clothed body shapes,
their complex articulations, and the resulting rich, yet stochastic geometric
detail in clothing. Hence, current methods to represent 3D people do not
provide a full generative model of people in clothing. In this paper, we
propose a novel method that learns to generate detailed 3D shapes of people in
a variety of garments with corresponding skinning weights. Specifically, we
devise a multi-subject forward skinning module that is learned from only a few
posed, un-rigged scans per subject. To capture the stochastic nature of
high-frequency details in garments, we leverage an adversarial loss formulation
that encourages the model to capture the underlying statistics. We provide
empirical evidence that this leads to realistic generation of local details
such as wrinkles. We show that our model is able to generate natural human
avatars wearing diverse and detailed clothing. Furthermore, we show that our
method can be used on the task of fitting human models to raw scans,
outperforming the previous state-of-the-art.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：In Defense of the Unitary Scalarization for Deep Multi-Task Learning</b></summary>
  <p><b>编号</b>：[6]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04122</p>
  <p><b>作者</b>：Vitaly Kurin,  Alessandro De Palma,  Ilya Kostrikov,  Shimon Whiteson,  M. Pawan Kumar</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：task learning research argues, training simply minimizes, theoretical analysis suggesting, reinforcement learning settings, introduce significant memory</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent multi-task learning research argues against unitary scalarization,
where training simply minimizes the sum of the task losses. Several ad-hoc
multi-task optimization algorithms have instead been proposed, inspired by
various hypotheses about what makes multi-task settings difficult. The majority
of these optimizers require per-task gradients, and introduce significant
memory, runtime, and implementation overhead. We present a theoretical analysis
suggesting that many specialized multi-task optimizers can be interpreted as
forms of regularization. Moreover, we show that, when coupled with standard
regularization and stabilization techniques from single-task learning, unitary
scalarization matches or improves upon the performance of complex multi-task
optimizers in both supervised and reinforcement learning settings. We believe
our results call for a critical reevaluation of recent research in the area.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：DM-VIO: Delayed Marginalization Visual-Inertial Odometry</b></summary>
  <p><b>编号</b>：[8]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04114</p>
  <p><b>作者</b>：Lukas von Stumberg,  Daniel Cremers</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：two novel techniques called delayed marginalization, vio performs photometric bundle adjustment, proposed pose graph bundle adjustment, pose graph bundle adjustment, delayed marginalization enables us</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present DM-VIO, a monocular visual-inertial odometry system based on two
novel techniques called delayed marginalization and pose graph bundle
adjustment. DM-VIO performs photometric bundle adjustment with a dynamic weight
for visual residuals. We adopt marginalization, which is a popular strategy to
keep the update time constrained, but it cannot easily be reversed, and
linearization points of connected variables have to be fixed. To overcome this
we propose delayed marginalization: The idea is to maintain a second factor
graph, where marginalization is delayed. This allows us to later readvance this
delayed graph, yielding an updated marginalization prior with new and
consistent linearization points. In addition, delayed marginalization enables
us to inject IMU information into already marginalized states. This is the
foundation of the proposed pose graph bundle adjustment, which we use for IMU
initialization. In contrast to prior works on IMU initialization, it is able to
capture the full photometric uncertainty, improving the scale estimation. In
order to cope with initially unobservable scale, we continue to optimize scale
and gravity direction in the main system after IMU initialization is complete.
We evaluate our system on the EuRoC, TUM-VI, and 4Seasons datasets, which
comprise flying drone, large-scale handheld, and automotive scenarios. Thanks
to the proposed IMU initialization, our system exceeds the state of the art in
visual-inertial odometry, even outperforming stereo-inertial methods while
using only a single camera and IMU. The code will be published at
this http URL</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Learning to Denoise Raw Mobile UI Layouts for ImprovingDatasets at Scale</b></summary>
  <p><b>编号</b>：[9]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04100</p>
  <p><b>作者</b>：Gang Li,  Gilles Baechler,  Manuel Tragut,  Yang Li</p>
  <p><b>备注</b>：Accepted to ACM CHI 2022</p>
  <p><b>关键词</b>：deep models achieve high accuracy withf1 scores, scale high quality ui layout datasetsfor data, deep learning approach fordenoising ui layouts, automatically improve existingmobile ui layout datasets, raw layout byremoving incorrect nodes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The layout of a mobile screen is a critical data source for UI designresearch
and semantic understanding of the screen. However, UIlayouts in existing
datasets are often noisy, have mismatches withtheir visual representation, or
consists of generic or app-specifictypes that are difficult to analyze and
model. In this paper, wepropose the CLAY pipeline that uses a deep learning
approach fordenoising UI layouts, allowing us to automatically improve
existingmobile UI layout datasets at scale. Our pipeline takes both
thescreenshot and the raw UI layout, and annotates the raw layout byremoving
incorrect nodes and assigning a semantically meaningfultype to each node. To
experiment with our data-cleaning pipeline,we create the CLAY dataset of 59,555
human-annotated screenlayouts, based on screenshots and raw layouts from Rico,
a publicmobile UI corpus. Our deep models achieve high accuracy withF1 scores
of 82.7% for detecting layout objects that do not have avalid visual
representation and 85.9% for recognizing object types,which significantly
outperforms a heuristic baseline. Our work laysa foundation for creating
large-scale high quality UI layout datasetsfor data-driven mobile UI research
and reduces the need of manuallabeling efforts that are prohibitively
expensive.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Identification of chicken egg fertility using SVM classifier based on  first-order statistical feature extraction</b></summary>
  <p><b>编号</b>：[17]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04063</p>
  <p><b>作者</b>：Shoffan Saifullah,  Andiko Putro Suryotomo</p>
  <p><b>备注</b>：9 Pages, 5 Figures, 2 Tables</p>
  <p><b>关键词</b>：chicken egg image data became input, contrast limited adaptive histogram equalization, identify chicken eggs fertility using, sample data uses datasets, repaired using image preprocessing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This study aims to identify chicken eggs fertility using the support vector
machine (SVM) classifier method. The classification basis used the first-order
statistical (FOS) parameters as feature extraction in the identification
process. This research was developed based on the process's identification
process, which is still manual (conventional). Although currently there are
many technologies in the identification process, they still need development.
Thus, this research is one of the developments in the field of image processing
technology. The sample data uses datasets from previous studies with a total of
100 egg images. The egg object in the image is a single object. From these
data, the classification of each fertile and infertile egg is 50 image data.
Chicken egg image data became input in image processing, with the initial
process is segmentation. This initial segmentation aims to get the cropped
image according to the object. The cropped image is repaired using image
preprocessing with grayscaling and image enhancement methods. This method
(image enhancement) used two combination methods: contrast limited adaptive
histogram equalization (CLAHE) and histogram equalization (HE). The improved
image becomes the input for feature extraction using the FOS method. The FOS
uses five parameters, namely mean, entropy, variance, skewness, and kurtosis.
The five parameters entered into the SVM classifier method to identify the
fertility of chicken eggs. The results of these experiments, the method
proposed in the identification process has a success percentage of 84.57%.
Thus, the implementation of this method can be used as a reference for future
research improvements. In addition, it may be possible to use a second-order
feature extraction method to improve its accuracy and improve supervised
learning for classification.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Towards Lightweight Neural Animation : Exploration of Neural Network  Pruning in Mixture of Experts-based Animation Models</b></summary>
  <p><b>编号</b>：[26]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04042</p>
  <p><b>作者</b>：Antoine Maiorca,  Nathan Hubens,  Sohaib Laraba,  Thierry Dutoit</p>
  <p><b>备注</b>：8 pages, 4 figures, 2 tables, 17th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications</p>
  <p><b>关键词</b>：pruned model produces less motion artifacts, level motion features, given impressive results, defined control signal, apply pruning algorithms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the past few years, neural character animation has emerged and offered an
automatic method for animating virtual characters. Their motion is synthesized
by a neural network. Controlling this movement in real time with a user-defined
control signal is also an important task in video games for example. Solutions
based on fully-connected layers (MLPs) and Mixture-of-Experts (MoE) have given
impressive results in generating and controlling various movements with
close-range interactions between the environment and the virtual character.
However, a major shortcoming of fully-connected layers is their computational
and memory cost which may lead to sub-optimized solution. In this work, we
apply pruning algorithms to compress an MLP- MoE neural network in the context
of interactive character animation, which reduces its number of parameters and
accelerates its computation time with a trade-off between this acceleration and
the synthesized motion quality. This work demonstrates that, with the same
number of experts and parameters, the pruned model produces less motion
artifacts than the dense model and the learned high-level motion features are
similar for both</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：MobilePhys: Personalized Mobile Camera-Based Contactless Physiological  Sensing</b></summary>
  <p><b>编号</b>：[27]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04039</p>
  <p><b>作者</b>：Xin Liu,  Yuntao Wang,  Sinan Xie,  Xiaoyu Zhang,  Zixian Ma,  Daniel McDuff,  Shwetak Patel</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：first mobile personalized remote physiological sensing system, novel mobile sensing system called mobilephys, many personalization techniques still require, supervised manner using videos accompanied, based contactless ppg models generated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Camera-based contactless photoplethysmography refers to a set of popular
techniques for contactless physiological measurement. The current
state-of-the-art neural models are typically trained in a supervised manner
using videos accompanied by gold standard physiological measurements. However,
they often generalize poorly out-of-domain examples (i.e., videos that are
unlike those in the training set). Personalizing models can help improve model
generalizability, but many personalization techniques still require some gold
standard data. To help alleviate this dependency, in this paper, we present a
novel mobile sensing system called MobilePhys, the first mobile personalized
remote physiological sensing system, that leverages both front and rear cameras
on a smartphone to generate high-quality self-supervised labels for training
personalized contactless camera-based PPG models. To evaluate the robustness of
MobilePhys, we conducted a user study with 39 participants who completed a set
of tasks under different mobile devices, lighting conditions/intensities,
motion tasks, and skin types. Our results show that MobilePhys significantly
outperforms the state-of-the-art on-device supervised training and few-shot
adaptation methods. Through extensive user studies, we further examine how does
MobilePhys perform in complex real-world settings. We envision that calibrated
or personalized camera-based contactless PPG models generated from our proposed
dual-camera mobile sensing system will open the door for numerous future
applications such as smart mirrors, fitness and mobile health applications.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Motion-Focused Contrastive Learning of Video Representations</b></summary>
  <p><b>编号</b>：[30]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04029</p>
  <p><b>作者</b>：Rui Li,  Yiheng Zhang,  Zhaofan Qiu,  Ting Yao,  Dong Liu,  Tao Mei</p>
  <p><b>备注</b>：ICCV 2021 (Oral); Code is publicly available at: this https URL</p>
  <p><b>关键词</b>：associated frame patches across time, outperforming imagenet supervised pre, supervised video representation learning, extensive experiments conducted, e ., sequences</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Motion, as the most distinct phenomenon in a video to involve the changes
over time, has been unique and critical to the development of video
representation learning. In this paper, we ask the question: how important is
the motion particularly for self-supervised video representation learning. To
this end, we compose a duet of exploiting the motion for data augmentation and
feature learning in the regime of contrastive learning. Specifically, we
present a Motion-focused Contrastive Learning (MCL) method that regards such
duet as the foundation. On one hand, MCL capitalizes on optical flow of each
frame in a video to temporally and spatially sample the tubelets (i.e.,
sequences of associated frame patches across time) as data augmentations. On
the other hand, MCL further aligns gradient maps of the convolutional layers to
optical flow maps from spatial, temporal and spatio-temporal perspectives, in
order to ground motion information in feature learning. Extensive experiments
conducted on R(2+1)D backbone demonstrate the effectiveness of our MCL. On
UCF101, the linear classifier trained on the representations learnt by MCL
achieves 81.91% top-1 accuracy, outperforming ImageNet supervised pre-training
by 6.78%. On Kinetics-400, MCL achieves 66.62% top-1 accuracy under the linear
protocol. Code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Representing Videos as Discriminative Sub-graphs for Action Recognition</b></summary>
  <p><b>编号</b>：[31]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04027</p>
  <p><b>作者</b>：Dong Li,  Zhaofan Qiu,  Yingwei Pan,  Ting Yao,  Houqiang Li,  Tao Mei</p>
  <p><b>备注</b>：CVPR 2021</p>
  <p><b>关键词</b>：musle produces 3d bounding boxes, learning gaussian mixture layer, something v2 validation set, takes dense connectivity, novelly builds space</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Human actions are typically of combinatorial structures or patterns, i.e.,
subjects, objects, plus spatio-temporal interactions in between. Discovering
such structures is therefore a rewarding way to reason about the dynamics of
interactions and recognize the actions. In this paper, we introduce a new
design of sub-graphs to represent and encode the discriminative patterns of
each action in the videos. Specifically, we present MUlti-scale Sub-graph
LEarning (MUSLE) framework that novelly builds space-time graphs and clusters
the graphs into compact sub-graphs on each scale with respect to the number of
nodes. Technically, MUSLE produces 3D bounding boxes, i.e., tubelets, in each
video clip, as graph nodes and takes dense connectivity as graph edges between
tubelets. For each action category, we execute online clustering to decompose
the graph into sub-graphs on each scale through learning Gaussian Mixture Layer
and select the discriminative sub-graphs as action prototypes for recognition.
Extensive experiments are conducted on both Something-Something V1 & V2 and
Kinetics-400 datasets, and superior results are reported when comparing to
state-of-the-art methods. More remarkably, our MUSLE achieves to-date the best
reported accuracy of 65.0% on Something-Something V2 validation set.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Uni-EDEN: Universal Encoder-Decoder Network by Multi-Granular  Vision-Language Pre-training</b></summary>
  <p><b>编号</b>：[32]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04026</p>
  <p><b>作者</b>：Yehao Li,  Jiahao Fan,  Yingwei Pan,  Ting Yao,  Weiyao Lin,  Tao Mei</p>
  <p><b>备注</b>：ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</p>
  <p><b>关键词</b>：moc ), masked region phrase generation, g ., visual question answering, g ., image captioning )., stream transformer based structure, sentence generation via inter</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision-language pre-training has been an emerging and fast-developing
research topic, which transfers multi-modal knowledge from rich-resource
pre-training task to limited-resource downstream tasks. Unlike existing works
that predominantly learn a single generic encoder, we present a pre-trainable
Universal Encoder-DEcoder Network (Uni-EDEN) to facilitate both vision-language
perception (e.g., visual question answering) and generation (e.g., image
captioning). Uni-EDEN is a two-stream Transformer based structure, consisting
of three modules: object and sentence encoders that separately learns the
representations of each modality, and sentence decoder that enables both
multi-modal reasoning and sentence generation via inter-modal interaction.
Considering that the linguistic representations of each image can span
different granularities in this hierarchy including, from simple to
comprehensive, individual label, a phrase, and a natural sentence, we pre-train
Uni-EDEN through multi-granular vision-language proxy tasks: Masked Object
Classification (MOC), Masked Region Phrase Generation (MRPG), Image-Sentence
Matching (ISM), and Masked Sentence Generation (MSG). In this way, Uni-EDEN is
endowed with the power of both multi-modal representation extraction and
language modeling. Extensive experiments demonstrate the compelling
generalizability of Uni-EDEN by fine-tuning it to four vision-language
perception and generation downstream tasks.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Smart Director: An Event-Driven Directing System for Live Broadcasting</b></summary>
  <p><b>编号</b>：[33]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04024</p>
  <p><b>作者</b>：Yingwei Pan,  Yue Chen,  Qian Bao,  Ning Zhang,  Ting Yao,  Jingen Liu,  Tao Mei</p>
  <p><b>备注</b>：ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</p>
  <p><b>关键词</b>：innovative automated sports broadcast directing system, live video broadcasting normally requires, view video analysis algorithms, end automated directing system, three consecutive novel components</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Live video broadcasting normally requires a multitude of skills and expertise
with domain knowledge to enable multi-camera productions. As the number of
cameras keep increasing, directing a live sports broadcast has now become more
complicated and challenging than ever before. The broadcast directors need to
be much more concentrated, responsive, and knowledgeable, during the
production. To relieve the directors from their intensive efforts, we develop
an innovative automated sports broadcast directing system, called Smart
Director, which aims at mimicking the typical human-in-the-loop broadcasting
process to automatically create near-professional broadcasting programs in
real-time by using a set of advanced multi-view video analysis algorithms.
Inspired by the so-called "three-event" construction of sports broadcast, we
build our system with an event-driven pipeline consisting of three consecutive
novel components: 1) the Multi-view Event Localization to detect events by
modeling multi-view correlations, 2) the Multi-view Highlight Detection to rank
camera views by the visual importance for view selection, 3) the
Auto-Broadcasting Scheduler to control the production of broadcasting videos.
To our best knowledge, our system is the first end-to-end automated directing
system for multi-camera sports broadcasting, completely driven by the semantic
understanding of sports events. It is also the first system to solve the novel
problem of multi-view joint event detection by cross-view relation modeling. We
conduct both objective and subjective evaluations on a real-world multi-camera
soccer dataset, which demonstrate the quality of our auto-generated videos is
comparable to that of the human-directed. Thanks to its faster response, our
system is able to capture more fast-passing and short-duration events which are
usually missed by human directors.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Boosting Video Representation Learning with Multi-Faceted Integration</b></summary>
  <p><b>编号</b>：[34]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04023</p>
  <p><b>作者</b>：Zhaofan Qiu,  Ting Yao,  Chong-Wah Ngo,  Xiao-Ping Zhang,  Dong Wu,  Tao Mei</p>
  <p><b>备注</b>：CVPR 2021</p>
  <p><b>关键词</b>：scale video datasets plus two image datasets leads, mufi also shows clear improvements, existing datasets mostly label, several downstream video applications, rich semantic embedding space</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Video content is multifaceted, consisting of objects, scenes, interactions or
actions. The existing datasets mostly label only one of the facets for model
training, resulting in the video representation that biases to only one facet
depending on the training dataset. There is no study yet on how to learn a
video representation from multifaceted labels, and whether multifaceted
information is helpful for video representation learning. In this paper, we
propose a new learning framework, MUlti-Faceted Integration (MUFI), to
aggregate facets from different datasets for learning a representation that
could reflect the full spectrum of video content. Technically, MUFI formulates
the problem as visual-semantic embedding learning, which explicitly maps video
representation into a rich semantic embedding space, and jointly optimizes
video representation from two perspectives. One is to capitalize on the
intra-facet supervision between each video and its own label descriptions, and
the second predicts the "semantic representation" of each video from the facets
of other datasets as the inter-facet supervision. Extensive experiments
demonstrate that learning 3D CNN via our MUFI framework on a union of four
large-scale video datasets plus two image datasets leads to superior capability
of video representation. The pre-learnt 3D CNN with MUFI also shows clear
improvements over other approaches on several downstream video applications.
More remarkably, MUFI achieves 98.1%/80.9% on UCF101/HMDB51 for action
recognition and 101.5% in terms of CIDEr-D score on MSVD for video captioning.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Condensing a Sequence to One Informative Frame for Video Recognition</b></summary>
  <p><b>编号</b>：[35]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04022</p>
  <p><b>作者</b>：Zhaofan Qiu,  Ting Yao,  Yan Shu,  Chong-Wah Ngo,  Tao Mei</p>
  <p><b>备注</b>：ICCV 2021</p>
  <p><b>关键词</b>：intensive media requires exhaustive computing resources, ifs consistently demonstrates evident improvements, incorporates three objective tasks, e ., appearance reconstruction, e ., adversarial learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Video is complex due to large variations in motion and rich content in
fine-grained visual details. Abstracting useful information from such
information-intensive media requires exhaustive computing resources. This paper
studies a two-step alternative that first condenses the video sequence to an
informative "frame" and then exploits off-the-shelf image recognition system on
the synthetic frame. A valid question is how to define "useful information" and
then distill it from a video sequence down to one synthetic frame. This paper
presents a novel Informative Frame Synthesis (IFS) architecture that
incorporates three objective tasks, i.e., appearance reconstruction, video
categorization, motion estimation, and two regularizers, i.e., adversarial
learning, color consistency. Each task equips the synthetic frame with one
ability, while each regularizer enhances its visual quality. With these, by
jointly learning the frame synthesis in an end-to-end manner, the generated
frame is expected to encapsulate the required spatio-temporal information
useful for video analysis. Extensive experiments are conducted on the
large-scale Kinetics dataset. When comparing to baseline methods that map video
sequence to a single image, IFS shows superior performance. More remarkably,
IFS consistently demonstrates evident improvements on image-based 2D networks
and clip-based 3D networks, and achieves comparable performance with the
state-of-the-art methods with less computational cost.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Optimization Planning for 3D ConvNets</b></summary>
  <p><b>编号</b>：[36]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04021</p>
  <p><b>作者</b>：Zhaofan Qiu,  Ting Yao,  Chong-Wah Ngo,  Tao Mei</p>
  <p><b>备注</b>：ICML 2021; Code is publicly available at: this https URL</p>
  <p><b>关键词</b>：learning 3d convnets using short video clips, seven public video recognition benchmarks demonstrate, term temporal dependency using lengthy clips, 3d convnets achieves superior results, 3d convolutional neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>It is not trivial to optimally learn a 3D Convolutional Neural Networks (3D
ConvNets) due to high complexity and various options of the training scheme.
The most common hand-tuning process starts from learning 3D ConvNets using
short video clips and then is followed by learning long-term temporal
dependency using lengthy clips, while gradually decaying the learning rate from
high to low as training progresses. The fact that such process comes along with
several heuristic settings motivates the study to seek an optimal "path" to
automate the entire training. In this paper, we decompose the path into a
series of training "states" and specify the hyper-parameters, e.g., learning
rate and the length of input clips, in each state. The estimation of the knee
point on the performance-epoch curve triggers the transition from one state to
another. We perform dynamic programming over all the candidate states to plan
the optimal permutation of states, i.e., optimization path. Furthermore, we
devise a new 3D ConvNets with a unique design of dual-head classifier to
improve spatial and temporal discrimination. Extensive experiments on seven
public video recognition benchmarks demonstrate the advantages of our proposal.
With the optimization planning, our 3D ConvNets achieves superior results when
comparing to the state-of-the-art recognition methods. More remarkably, we
obtain the top-1 accuracy of 80.5% and 82.7% on Kinetics-400 and Kinetics-600
datasets, respectively. Source code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Pyramid Fusion Transformer for Semantic Segmentation</b></summary>
  <p><b>编号</b>：[38]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04019</p>
  <p><b>作者</b>：Zipeng Qin,  Jianbo Liu,  Xiaolin Zhang,  Maoqing Tian,  Aojun Zhou,  Shuai Yi,  Hongsheng Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：different resolutions without incurring, segmentation quality thus relies, efficiently utilize image features, rich semantic information across, based pyramid fusion transformer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The recently proposed MaskFormer \cite{maskformer} gives a refreshed
perspective on the task of semantic segmentation: it shifts from the popular
pixel-level classification paradigm to a mask-level classification method. In
essence, it generates paired probabilities and masks corresponding to category
segments and combines them during inference for the segmentation maps. The
segmentation quality thus relies on how well the queries can capture the
semantic information for categories and their spatial locations within the
images. In our study, we find that per-mask classification decoder on top of a
single-scale feature is not effective enough to extract reliable probability or
mask. To mine for rich semantic information across the feature pyramid, we
propose a transformer-based Pyramid Fusion Transformer (PFT) for per-mask
approach semantic segmentation on top of multi-scale features. To efficiently
utilize image features of different resolutions without incurring too much
computational overheads, PFT uses a multi-scale transformer decoder with
cross-scale inter-query attention to exchange complimentary information.
Extensive experimental evaluations and ablations demonstrate the efficacy of
our framework. In particular, we achieve a 3.2 mIoU improvement on COCO-Stuff
10K dataset with ResNet-101c compared to MaskFormer. Besides, on ADE20K
validation set, our result with Swin-B backbone matches that of MaskFormer's
with a much larger Swin-L backbone in both single-scale and multi-scale
inference, achieving 54.1 mIoU and 55.3 mIoU respectively. Using a Swin-L
backbone, we achieve 56.0 mIoU single-scale result on the ADE20K validation set
and 57.2 multi-scale result, obtaining state-of-the-art performance on the
dataset.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Captcha Attack:Turning Captchas Against Humanity</b></summary>
  <p><b>编号</b>：[40]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04014</p>
  <p><b>作者</b>：Mauro Conti,  Luca Pajola,  Pier Paolo Tricomi</p>
  <p><b>备注</b>：Currently under submission</p>
  <p><b>关键词</b>：9 billion daily active facebook users posted around 150 thousand photos every minute, help human moderators handle high data volume, content moderators constantly monitor, generating custom textual captchas, g ., social networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Nowadays, people generate and share massive content on online platforms
(e.g., social networks, blogs). In 2021, the 1.9 billion daily active Facebook
users posted around 150 thousand photos every minute. Content moderators
constantly monitor these online platforms to prevent the spreading of
inappropriate content (e.g., hate speech, nudity images). Based on deep
learning (DL) advances, Automatic Content Moderators (ACM) help human
moderators handle high data volume. Despite their advantages, attackers can
exploit weaknesses of DL components (e.g., preprocessing, model) to affect
their performance. Therefore, an attacker can leverage such techniques to
spread inappropriate content by evading ACM.
In this work, we propose CAPtcha Attack (CAPA), an adversarial technique that
allows users to spread inappropriate text online by evading ACM controls. CAPA,
by generating custom textual CAPTCHAs, exploits ACM's careless design
implementations and internal procedures vulnerabilities. We test our attack on
real-world ACM, and the results confirm the ferocity of our simple yet
effective attack, reaching up to a 100% evasion success in most cases. At the
same time, we demonstrate the difficulties in designing CAPA mitigations,
opening new challenges in CAPTCHAs research area.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Similarity-based Gray-box Adversarial Attack Against Deep Face  Recognition</b></summary>
  <p><b>编号</b>：[42]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04011</p>
  <p><b>作者</b>：Hanrui Wang,  Shuo Wang,  Zhe Jin,  Yandan Wang,  Cunjian Chen,  Massimo Tistarell</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：adversarial example could satisfactorily cater, adversarial attack techniques perform well, proposed method significantly outperforms, newly developed objective function, existing adversarial attack techniques</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The majority of adversarial attack techniques perform well against deep face
recognition when the full knowledge of the system is revealed
(\emph{white-box}). However, such techniques act unsuccessfully in the gray-box
setting where the face templates are unknown to the attackers. In this work, we
propose a similarity-based gray-box adversarial attack (SGADV) technique with a
newly developed objective function. SGADV utilizes the dissimilarity score to
produce the optimized adversarial example, i.e., similarity-based adversarial
attack. This technique applies to both white-box and gray-box attacks against
authentication systems that determine genuine or imposter users using the
dissimilarity score. To validate the effectiveness of SGADV, we conduct
extensive experiments on face datasets of LFW, CelebA, and CelebA-HQ against
deep face recognition models of FaceNet and InsightFace in both white-box and
gray-box settings. The results suggest that the proposed method significantly
outperforms the existing adversarial attack techniques in the gray-box setting.
We hence summarize that the similarity-base approaches to develop the
adversarial example could satisfactorily cater to the gray-box attack scenarios
for de-authentication.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：A Novel Home-Built Metrology to Analyze Oral Fluid Droplets and Quantify  the Efficacy of Masks</b></summary>
  <p><b>编号</b>：[49]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03993</p>
  <p><b>作者</b>：Ava Tan Bhowmik</p>
  <p><b>备注</b>：9 pages, 12 figures</p>
  <p><b>关键词</b>：402 nm wavelength uv tube lights, test subject first wets, project includes setup optimization, oral fluid droplet generation, experiments evaluating mask efficacy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Wearing masks is crucial to preventing the spread of potentially
pathogen-containing droplets, especially amidst the COVID-19 pandemic. However,
not all face coverings are equally effective and most experiments evaluating
mask efficacy are very expensive and complex to operate. In this work, a novel,
home-built, low-cost, and accurate metrology to visualize orally-generated
fluid droplets has been developed. The project includes setup optimization,
data collection, data analysis, and applications. The final materials chosen
were quinine-containing tonic water, 397-402 nm wavelength UV tube lights, an
iPhone and tripod, string, and a spray bottle. The experiment took place in a
dark closet with a dark background. During data collection, the test subject
first wets their mouth with an ingestible fluorescent liquid (tonic water) and
speaks, sneezes, or coughs under UV darklight. The fluorescence from the tonic
water droplets generated can be visualized, recorded by an iPhone 8+ camera in
slo-mo (240 fps), and analyzed. The software VLC is used for frame separation
and Fiji/ImageJ is used for image processing and analysis. The dependencies of
oral fluid droplet generation and propagation on different phonics, the
loudness of speech, and the type of expiratory event were studied in detail and
established using the metrology developed. The efficacy of different types of
masks was evaluated and correlated with fabric microstructures. All masks
blocked droplets to varying extent. Masks with smaller-sized pores and thicker
material were found to block the most droplets. This low-cost technique can be
easily constructed at home using materials that total to a cost of less than
$50. Despite the minimal cost, the method is very accurate and the data is
quantifiable.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Multimodal Representations Learning Based on Mutual Information  Maximization and Minimization and Identity Embedding for Multimodal Sentiment  Analysis</b></summary>
  <p><b>编号</b>：[53]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03969</p>
  <p><b>作者</b>：Jiahao Zheng,  Sen Zhang,  Xiaoping Wang,  Zhigang Zeng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：fundamental complex research problem due, two public datasets demonstrate, robust multimodal representation needs, multimodal representation model based, combine mutual information maximization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multimodal sentiment analysis (MSA) is a fundamental complex research problem
due to the heterogeneity gap between different modalities and the ambiguity of
human emotional expression. Although there have been many successful attempts
to construct multimodal representations for MSA, there are still two challenges
to be addressed: 1) A more robust multimodal representation needs to be
constructed to bridge the heterogeneity gap and cope with the complex
multimodal interactions, and 2) the contextual dynamics must be modeled
effectively throughout the information flow. In this work, we propose a
multimodal representation model based on Mutual information Maximization and
Minimization and Identity Embedding (MMMIE). We combine mutual information
maximization between modal pairs, and mutual information minimization between
input data and corresponding features to mine the modal-invariant and
task-related information. Furthermore, Identity Embedding is proposed to prompt
the downstream network to perceive the contextual information. Experimental
results on two public datasets demonstrate the effectiveness of the proposed
model.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：On the Efficacy of Co-Attention Transformer Layers in Visual Question  Answering</b></summary>
  <p><b>编号</b>：[56]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03965</p>
  <p><b>作者</b>：Ankur Sikarwar,  Gabriel Kreiman</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：vqa ), outperforming previous architectures, generate visual attention maps using, neural network attention maps, conditioned image attention scores, human attention maps</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, multi-modal transformers have shown significant progress in
Vision-Language tasks, such as Visual Question Answering (VQA), outperforming
previous architectures by a considerable margin. This improvement in VQA is
often attributed to the rich interactions between vision and language streams.
In this work, we investigate the efficacy of co-attention transformer layers in
helping the network focus on relevant regions while answering the question. We
generate visual attention maps using the question-conditioned image attention
scores in these co-attention layers. We evaluate the effect of the following
critical components on visual attention of a state-of-the-art VQA model: (i)
number of object region proposals, (ii) question part of speech (POS) tags,
(iii) question semantics, (iv) number of co-attention layers, and (v) answer
accuracy. We compare the neural network attention maps against human attention
maps both qualitatively and quantitatively. Our findings indicate that
co-attention transformer modules are crucial in attending to relevant regions
of the image given a question. Importantly, we observe that the semantic
meaning of the question is not what drives visual attention, but specific
keywords in the question do. Our work sheds light on the function and
interpretation of co-attention transformer layers, highlights gaps in current
networks, and can guide the development of future VQA models and networks that
simultaneously process visual and language streams.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Feature Extraction Framework based on Contrastive Learning with Adaptive  Positive and Negative Samples</b></summary>
  <p><b>编号</b>：[64]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03942</p>
  <p><b>作者</b>：Hongjie Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：final numerical experiments prove, traditional feature extraction methods, feature extraction framework based, infonce loss based, view feature extraction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this study, we propose a feature extraction framework based on contrastive
learning with adaptive positive and negative samples (CL-FEFA) that is suitable
for unsupervised, supervised, and semi-supervised single-view feature
extraction. CL-FEFA constructs adaptively the positive and negative samples
from the results of feature extraction, which makes it more appropriate and
accurate. Thereafter, the discriminative features are re extracted to according
to InfoNCE loss based on previous positive and negative samples, which will
make the intra-class samples more compact and the inter-class samples more
dispersed. At the same time, using the potential structure information of
subspace samples to dynamically construct positive and negative samples can
make our framework more robust to noisy data. Furthermore, CL-FEFA considers
the mutual information between positive samples, that is, similar samples in
potential structures, which provides theoretical support for its advantages in
feature extraction. The final numerical experiments prove that the proposed
framework has a strong advantage over the traditional feature extraction
methods and contrastive learning methods.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Where Is My Mind (looking at)? Predicting Visual Attention from Brain  Activity</b></summary>
  <p><b>编号</b>：[74]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03902</p>
  <p><b>作者</b>：Victor Delvigne,  Noé Tits,  Luca La Fisca,  Nathan Hubens,  Antoine Maiorca,  Hazem Wannous,  Thierry Dutoit,  Jean-Philippe Vandeborre</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：saliency map representing attention, approaches estimating attention, visual attention estimation, visual attention, visual attention</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Visual attention estimation is an active field of research at the crossroads
of different disciplines: computer vision, artificial intelligence and
medicine. One of the most common approaches to estimate a saliency map
representing attention is based on the observed images. In this paper, we show
that visual attention can be retrieved from EEG acquisition. The results are
comparable to traditional predictions from observed images, which is of great
interest. For this purpose, a set of signals has been recorded and different
models have been developed to study the relationship between visual attention
and brain activity. The results are encouraging and comparable with other
approaches estimating attention with other modalities. The codes and dataset
considered in this paper have been made available at
\url{this https URL} to promote research in the
field.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Emotion Estimation from EEG -- A Dual Deep Learning Approach Combined  with Saliency</b></summary>
  <p><b>编号</b>：[77]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03891</p>
  <p><b>作者</b>：Victor Delvigne,  Antoine Facchini,  Hazem Wannous,  Thierry Dutoit,  Laurence Ris,  Jean-Philippe Vandeborre</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：electrical brain activity presented motivating results, four publicly available datasets, achieves similar results, reflects higher stability, physiological knowledge defined</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Emotion estimation is an active field of research that has an important
impact on the interaction between human and computer. Among the different
modality to assess emotion, electroencephalogram (EEG) representing the
electrical brain activity presented motivating results during the last decade.
Emotion estimation from EEG could help in the diagnosis or rehabilitation of
certain diseases. In this paper, we propose a dual method considering the
physiological knowledge defined by specialists combined with novel deep
learning (DL) models initially dedicated to computer vision. The joint learning
has been enhanced with model saliency analysis. To present a global approach,
the model has been evaluated on four publicly available datasets and achieves
similar results to the state-of-theart approaches and outperforming results for
two of the proposed datasets with a lower standard deviation that reflects
higher stability. For sake of reproducibility, the codes and models proposed in
this paper are available at this http URL.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：On Exploring Pose Estimation as an Auxiliary Learning Task for  Visible-Infrared Person Re-identification</b></summary>
  <p><b>编号</b>：[86]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03859</p>
  <p><b>作者</b>：Yunqi Miao,  Nianchang Huang,  Xiao Ma,  Qiang Zhang,  Jungong Han</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：method achieves nearly 20 $\%$ map improvements, existing feature learning paradigms imposed constraints, model learns higher quality modality, proposed method consistently improves state, pioneering approaches reduce intra</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Visible-infrared person re-identification (VI-ReID) has been challenging due
to the existence of large discrepancies between visible and infrared
modalities. Most pioneering approaches reduce intra-class variations and
inter-modality discrepancies by learning modality-shared and ID-related
features. However, an explicit modality-shared cue, i.e., body keypoints, has
not been fully exploited in VI-ReID. Additionally, existing feature learning
paradigms imposed constraints on either global features or partitioned feature
stripes, which neglect the prediction consistency of global and part features.
To address the above problems, we exploit Pose Estimation as an auxiliary
learning task to assist the VI-ReID task in an end-to-end framework. By jointly
training these two tasks in a mutually beneficial manner, our model learns
higher quality modality-shared and ID-related features. On top of it, the
learnings of global features and local features are seamlessly synchronized by
Hierarchical Feature Constraint (HFC), where the former supervises the latter
using the knowledge distillation strategy. Experimental results on two
benchmark VI-ReID datasets show that the proposed method consistently improves
state-of-the-art methods by significant margins. Specifically, our method
achieves nearly 20$\%$ mAP improvements against the state-of-the-art method on
the RegDB dataset. Our intriguing findings highlight the usage of auxiliary
task learning in VI-ReID.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：MobileFaceSwap: A Lightweight Framework for Video Face Swapping</b></summary>
  <p><b>编号</b>：[103]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03808</p>
  <p><b>作者</b>：Zhiliang Xu,  Zhibin Hong,  Changxing Ding,  Zhen Zhu,  Junyu Han,  Jingtuo Liu,  Errui Ding</p>
  <p><b>备注</b>：AAAI 2022</p>
  <p><b>关键词</b>：introducing two dynamic neural network techniques, edge devices like mobile phones, 33g flops per frame, obtain better synthesized results, method achieves comparable results</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Advanced face swapping methods have achieved appealing results. However, most
of these methods have many parameters and computations, which makes it
challenging to apply them in real-time applications or deploy them on edge
devices like mobile phones. In this work, we propose a lightweight
Identity-aware Dynamic Network (IDN) for subject-agnostic face swapping by
dynamically adjusting the model parameters according to the identity
information. In particular, we design an efficient Identity Injection Module
(IIM) by introducing two dynamic neural network techniques, including the
weights prediction and weights modulation. Once the IDN is updated, it can be
applied to swap faces given any target image or video. The presented IDN
contains only 0.50M parameters and needs 0.33G FLOPs per frame, making it
capable for real-time video face swapping on mobile phones. In addition, we
introduce a knowledge distillation-based method for stable training, and a loss
reweighting module is employed to obtain better synthesized results. Finally,
our method achieves comparable results with the teacher models and other
state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Unsupervised Domain Adaptive Person Re-id with Local-enhance and  Prototype Dictionary Learning</b></summary>
  <p><b>编号</b>：[106]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03803</p>
  <p><b>作者</b>：Haopeng Hou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：two large datasets demonstrate, general domain adaptive tasks, level prototype dictionary learning, 3 \% improvement compared, propose prototype dictionary learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The unsupervised domain adaptive person re-identification (re-ID) task has
been a challenge because, unlike the general domain adaptive tasks, there is no
overlap between the classes of source and target domain data in the person
re-ID, which leads to a significant domain gap. State-of-the-art unsupervised
re-ID methods train the neural networks using a memory-based contrastive loss.
However, performing contrastive learning by treating each unlabeled instance as
a class will lead to the problem of class collision, and the updating intensity
is inconsistent due to the difference in the number of instances of different
categories when updating in the memory bank. To address such problems, we
propose Prototype Dictionary Learning for person re-ID which is able to utilize
both source domain data and target domain data by one training stage while
avoiding the problem of class collision and the problem of updating intensity
inconsistency by cluster-level prototype dictionary learning. In order to
reduce the interference of domain gap on the model, we propose a local-enhance
module to improve the domain adaptation of the model without increasing the
number of model parameters. Our experiments on two large datasets demonstrate
the effectiveness of the prototype dictionary learning. 71.5\% mAP is achieved
in the Market-to-Duke task, which is a 2.3\% improvement compared to the
state-of-the-art unsupervised domain adaptive re-ID methods. It achieves 83.9\%
mAP in the Duke-to-Market task, which improves by 4.4\% compared to the
state-of-the-art unsupervised adaptive re-ID methods.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Efficient Non-Local Contrastive Attention for Image Super-Resolution</b></summary>
  <p><b>编号</b>：[108]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03794</p>
  <p><b>作者</b>：Bin Xia,  Yucheng Hang,  Yapeng Tian,  Wenming Yang,  Qingmin Liao,  Jie Zhou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：nla gives noisy information large weights, obtains linear computation complexity, leveraging intrinsic feature correlation, extensive experimental results show, consumes quadratic computation resources</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Non-Local Attention (NLA) brings significant improvement for Single Image
Super-Resolution (SISR) by leveraging intrinsic feature correlation in natural
images. However, NLA gives noisy information large weights and consumes
quadratic computation resources with respect to the input size, limiting its
performance and application. In this paper, we propose a novel Efficient
Non-Local Contrastive Attention (ENLCA) to perform long-range visual modeling
and leverage more relevant non-local features. Specifically, ENLCA consists of
two parts, Efficient Non-Local Attention (ENLA) and Sparse Aggregation. ENLA
adopts the kernel method to approximate exponential function and obtains linear
computation complexity. For Sparse Aggregation, we multiply inputs by an
amplification factor to focus on informative features, yet the variance of
approximation increases exponentially. Therefore, contrastive learning is
applied to further separate relevant and irrelevant features. To demonstrate
the effectiveness of ENLCA, we build an architecture called Efficient Non-Local
Contrastive Network (ENLCN) by adding a few of our modules in a simple
backbone. Extensive experimental results show that ENLCN reaches superior
performance over state-of-the-art approaches on both quantitative and
qualitative evaluations.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Classification of Beer Bottles using Object Detection and Transfer  Learning</b></summary>
  <p><b>编号</b>：[109]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03791</p>
  <p><b>作者</b>：Philipp Hohlfeld,  Tobias Ostermeier,  Dominik Brandl</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：image contains exactly one beer bottle, classic one step transfer learning approach, cnn detects image sections relevant, 5207 beer bottle images, master course deep learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Classification problems are common in Computer Vision. Despite this, there is
no dedicated work for the classification of beer bottles. As part of the
challenge of the master course Deep Learning, a dataset of 5207 beer bottle
images and brand labels was created. An image contains exactly one beer bottle.
In this paper we present a deep learning model which classifies pictures of
beer bottles in a two step approach. As the first step, a Faster-R-CNN detects
image sections relevant for classification independently of the brand. In the
second step, the relevant image sections are classified by a ResNet-18. The
image section with the highest confidence is returned as class label. We
propose a model, with which we surpass the classic one step transfer learning
approach and reached an accuracy of 99.86% during the challenge on the final
test dataset. We were able to achieve 100% accuracy after the challenge ended</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Drone Object Detection Using RGB/IR Fusion</b></summary>
  <p><b>编号</b>：[111]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03786</p>
  <p><b>作者</b>：Lizhi Yang,  Ruhang Ma,  Avideh Zakhor</p>
  <p><b>备注</b>：Accepted to Electronic Imaging Symposium, Computational Imaging XX Conference, 2022</p>
  <p><b>关键词</b>：object detection using aerial drone imagery, creating synthetic ir images using, nvidia jetson xavier running, applying deep learning methods, drone ir imagery</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Object detection using aerial drone imagery has received a great deal of
attention in recent years. While visible light images are adequate for
detecting objects in most scenarios, thermal cameras can extend the
capabilities of object detection to night-time or occluded objects. As such,
RGB and Infrared (IR) fusion methods for object detection are useful and
important. One of the biggest challenges in applying deep learning methods to
RGB/IR object detection is the lack of available training data for drone IR
imagery, especially at night. In this paper, we develop several strategies for
creating synthetic IR images using the AIRSim simulation engine and CycleGAN.
Furthermore, we utilize an illumination-aware fusion framework to fuse RGB and
IR images for object detection on the ground. We characterize and test our
methods for both simulated and actual data. Our solution is implemented on an
NVIDIA Jetson Xavier running on an actual drone, requiring about 28
milliseconds of processing per RGB/IR image pair.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：TSA-Net: Tube Self-Attention Network for Action Quality Assessment</b></summary>
  <p><b>编号</b>：[121]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03746</p>
  <p><b>作者</b>：Shunli Wang,  Dingkang Yang,  Peng Zhai,  Chixiao Chen,  Lihua Zhang</p>
  <p><b>备注</b>：9 pages, 7 figures, conference paper</p>
  <p><b>关键词</b>：popular action quality assessment datasets including aqa, efficiently generate rich spatio, existing approaches usually tackle, dataset named fall recognition, adopting sparse feature interactions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, assessing action quality from videos has attracted growing
attention in computer vision community and human computer interaction. Most
existing approaches usually tackle this problem by directly migrating the model
from action recognition tasks, which ignores the intrinsic differences within
the feature map such as foreground and background information. To address this
issue, we propose a Tube Self-Attention Network (TSA-Net) for action quality
assessment (AQA). Specifically, we introduce a single object tracker into AQA
and propose the Tube Self-Attention Module (TSA), which can efficiently
generate rich spatio-temporal contextual information by adopting sparse feature
interactions. The TSA module is embedded in existing video networks to form
TSA-Net. Overall, our TSA-Net is with the following merits: 1) High
computational efficiency, 2) High flexibility, and 3) The state-of-the art
performance. Extensive experiments are conducted on popular action quality
assessment datasets including AQA-7 and MTL-AQA. Besides, a dataset named Fall
Recognition in Figure Skating (FR-FS) is proposed to explore the basic action
assessment in the figure skating scene.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：NFANet: A Novel Method for Weakly Supervised Water Extraction from  High-Resolution Remote Sensing Imagery</b></summary>
  <p><b>编号</b>：[140]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03686</p>
  <p><b>作者</b>：Ming Lu,  Leyuan Fang,  Muxing Li,  Bob Zhang,  Yi Zhang,  Pedram Ghamisi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：method utilizes neighboring features instead, water extraction requires precise pixel, studied weakly supervised approaches, improved recursive training algorithm, also obtains similar results</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The use of deep learning for water extraction requires precise pixel-level
labels. However, it is very difficult to label high-resolution remote sensing
images at the pixel level. Therefore, we study how to utilize point labels to
extract water bodies and propose a novel method called the neighbor feature
aggregation network (NFANet). Compared with pixellevel labels, point labels are
much easier to obtain, but they will lose much information. In this paper, we
take advantage of the similarity between the adjacent pixels of a local
water-body, and propose a neighbor sampler to resample remote sensing images.
Then, the sampled images are sent to the network for feature aggregation. In
addition, we use an improved recursive training algorithm to further improve
the extraction accuracy, making the water boundary more natural. Furthermore,
our method utilizes neighboring features instead of global or local features to
learn more representative features. The experimental results show that the
proposed NFANet method not only outperforms other studied weakly supervised
approaches, but also obtains similar results as the state-of-the-art ones.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：PrintsGAN: Synthetic Fingerprint Generator</b></summary>
  <p><b>编号</b>：[144]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03674</p>
  <p><b>作者</b>：Joshua J. Engelsma,  Steven A. Grosz,  Anil K. Jain</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：g ., using deep networks, prevailing synthetic fingerprint generation methods, generate multiple impressions per finger, learn fixed length fingerprint embeddings, generating unique fingerprints along</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A major impediment to researchers working in the area of fingerprint
recognition is the lack of publicly available, large-scale, fingerprint
datasets. The publicly available datasets that do exist contain very few
identities and impressions per finger. This limits research on a number of
topics, including e.g., using deep networks to learn fixed length fingerprint
embeddings. Therefore, we propose PrintsGAN, a synthetic fingerprint generator
capable of generating unique fingerprints along with multiple impressions for a
given fingerprint. Using PrintsGAN, we synthesize a database of 525,000
fingerprints (35,000 distinct fingers, each with 15 impressions). Next, we show
the utility of the PrintsGAN generated dataset by training a deep network to
extract a fixed-length embedding from a fingerprint. In particular, an
embedding model trained on our synthetic fingerprints and fine-tuned on a small
number of publicly available real fingerprints (25,000 prints from NIST SD302)
obtains a TAR of 87.03% @ FAR=0.01% on the NIST SD4 database (a boost from
TAR=73.37% when only trained on NIST SD302). Prevailing synthetic fingerprint
generation methods do not enable such performance gains due to i) lack of
realism or ii) inability to generate multiple impressions per finger. We plan
to release our database of synthetic fingerprints to the public.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Towards Group Robustness in the presence of Partial Group Labels</b></summary>
  <p><b>编号</b>：[145]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03668</p>
  <p><b>作者</b>：Vishnu Suresh Lokhande,  Kihyuk Sohn,  Jinsung Yoon,  Madeleine Udell,  Chen-Yu Lee,  Tomas Pfister</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：preserving overall aggregate accuracy across groups, contain partially labeled group information, neural network predictions resulting, data collection efforts results, leverage partially available sensitive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Learning invariant representations is an important requirement when training
machine learning models that are driven by spurious correlations in the
datasets. These spurious correlations, between input samples and the target
labels, wrongly direct the neural network predictions resulting in poor
performance on certain groups, especially the minority groups. Robust training
against these spurious correlations requires the knowledge of group membership
for every sample. Such a requirement is impractical in situations where the
data labeling efforts for minority or rare groups are significantly laborious
or where the individuals comprising the dataset choose to conceal sensitive
information. On the other hand, the presence of such data collection efforts
results in datasets that contain partially labeled group information. Recent
works have tackled the fully unsupervised scenario where no labels for groups
are available. Thus, we aim to fill the missing gap in the literature by
tackling a more realistic setting that can leverage partially available
sensitive or group information during training. First, we construct a
constraint set and derive a high probability bound for the group assignment to
belong to the set. Second, we propose an algorithm that optimizes for the
worst-off group assignments from the constraint set. Through experiments on
image and tabular datasets, we show improvements in the minority group's
performance while preserving overall aggregate accuracy across groups.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Multi-query Video Retrieval</b></summary>
  <p><b>编号</b>：[155]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03639</p>
  <p><b>作者</b>：Zeyu Wang,  Yu Wu,  Karthik Narasimhan,  Olga Russakovsky</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：models consistently outperform several competitive baselines, propose several new methods, simply combining similarity outputs, retrieving target videos based, better evaluates retrieval capabilities</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Retrieving target videos based on text descriptions is a task of great
practical value and has received increasing attention over the past few years.
In this paper, we focus on the less-studied setting of multi-query video
retrieval, where multiple queries are provided to the model for searching over
the video archive. We first show that the multi-query retrieval task is more
pragmatic and representative of real-world use cases and better evaluates
retrieval capabilities of current models, thereby deserving of further
investigation alongside the more prevalent single-query retrieval setup. We
then propose several new methods for leveraging multiple queries at training
time to improve over simply combining similarity outputs of multiple queries
from regular single-query trained models. Our models consistently outperform
several competitive baselines over three different datasets. For instance,
Recall@1 can be improved by 4.7 points on MSR-VTT, 4.1 points on MSVD and 11.7
points on VATEX over a strong baseline built on the state-of-the-art CLIP4Clip
model. We believe further modeling efforts will bring new insights to this
direction and spark new systems that perform better in real-world video
retrieval applications. Code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Cross-Modality Sub-Image Retrieval using Contrastive Multimodal Image  Representations</b></summary>
  <p><b>编号</b>：[166]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03597</p>
  <p><b>作者</b>：Eva Breznik,  Elisabeth Wetzer,  Joakim Lindblad,  Nataša Sladoje</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：different modalities may display little common information, independent approach shows promising results, second harmonic generation microscopy images, proposed method significantly outperforms, proposed method performs better</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In tissue characterization and cancer diagnostics, multimodal imaging has
emerged as a powerful technique. Thanks to computational advances, large
datasets can be exploited to improve diagnosis and discover patterns in
pathologies. However, this requires efficient and scalable image retrieval
methods. Cross-modality image retrieval is particularly demanding, as images of
the same content captured in different modalities may display little common
information. We propose a content-based image retrieval system (CBIR) for
reverse (sub-)image search to retrieve microscopy images in one modality given
a corresponding image captured by a different modality, where images are not
aligned and share only few structures. We propose to combine deep learning to
generate representations which embed both modalities in a common space, with
classic, fast, and robust feature extractors (SIFT, SURF) to create a
bag-of-words model for efficient and reliable retrieval. Our
application-independent approach shows promising results on a publicly
available dataset of brightfield and second harmonic generation microscopy
images. We obtain 75.4% and 83.6% top-10 retrieval success for retrieval in one
or the other direction. Our proposed method significantly outperforms both
direct retrieval of the original multimodal (sub-)images, as well as their
corresponding generative adversarial network (GAN)-based image-to-image
translations. We establish that the proposed method performs better in
comparison with a recent sub-image retrieval toolkit, GAN-based image-to-image
translations, and learnt feature extractors for the downstream task of
cross-modal image retrieval. We highlight the shortcomings of the latter
methods and observe the importance of equivariance and invariance properties of
the learnt representations and feature extractors in the CBIR pipeline. Code
will be available at this http URL.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Reproducing BowNet: Learning Representations by Predicting Bags of  Visual Words</b></summary>
  <p><b>编号</b>：[169]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03556</p>
  <p><b>作者</b>：Harry Nguyen,  Stone Yun,  Hisham Mohammad</p>
  <p><b>备注</b>：This is a reproducibility project. Original work is by Gidaris et al. published in CVPR 2020. Pytorch implementation is public on Github</p>
  <p><b>关键词</b>：softmax layer $\ omega (\ cdot )$ trained, convolutional feature extractor $\ phi (\ cdot )$, 100 accuracy improvements reported, $\ phi, deep feature descriptors</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This work aims to reproduce results from the CVPR 2020 paper by Gidaris et
al. Self-supervised learning (SSL) is used to learn feature representations of
an image using an unlabeled dataset. This work proposes to use bag-of-words
(BoW) deep feature descriptors as a self-supervised learning target to learn
robust, deep representations. BowNet is trained to reconstruct the histogram of
visual words (ie. the deep BoW descriptor) of a reference image when presented
a perturbed version of the image as input. Thus, this method aims to learn
perturbation-invariant and context-aware image features that can be useful for
few-shot tasks or supervised downstream tasks. In the paper, the author
describes BowNet as a network consisting of a convolutional feature extractor
$\Phi(\cdot)$ and a Dense-softmax layer $\Omega(\cdot)$ trained to predict BoW
features from images. After BoW training, the features of $\Phi$ are used in
downstream tasks. For this challenge we were trying to build and train a
network that could reproduce the CIFAR-100 accuracy improvements reported in
the original paper. However, we were unsuccessful in reproducing an accuracy
improvement comparable to what the authors mentioned.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Image quality measurements and denoising using Fourier Ring Correlations</b></summary>
  <p><b>编号</b>：[180]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03992</p>
  <p><b>作者</b>：J. Kaczmar-Michalska,  N.R. Hajizadeh,  A.J. Rzepiela,  S.F. Nørrelykke</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：humans perceive image similarities, google open images dataset, based loss function allows, loss function based, structural similarity index</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image quality is a nebulous concept with different meanings to different
people. To quantify image quality a relative difference is typically calculated
between a corrupted image and a ground truth image. But what metric should we
use for measuring this difference? Ideally, the metric should perform well for
both natural and scientific images. The structural similarity index (SSIM) is a
good measure for how humans perceive image similarities, but is not sensitive
to differences that are scientifically meaningful in microscopy. In electron
and super-resolution microscopy, the Fourier Ring Correlation (FRC) is often
used, but is little known outside of these fields. Here we show that the FRC
can equally well be applied to natural images, e.g. the Google Open Images
dataset. We then define a loss function based on the FRC, show that it is
analytically differentiable, and use it to train a U-net for denoising of
images. This FRC-based loss function allows the network to train faster and
achieve similar or better results than when using L1- or L2- based losses. We
also investigate the properties and limitations of neural network denoising
with the FRC analysis.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：COROLLA: An Efficient Multi-Modality Fusion Framework with Supervised  Contrastive Learning for Glaucoma Grading</b></summary>
  <p><b>编号</b>：[193]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03795</p>
  <p><b>作者</b>：Zhiyuan Cai,  Li Lin,  Huaqing He,  Xiaoying Tang</p>
  <p><b>备注</b>：5 pages, To be published in ISBI 2022</p>
  <p><b>关键词</b>：corolla framework achieves overwhelming glaucoma grading performance compared, distribution similarities across medical image samples, existing glaucoma grading approaches mainly utilize, modality supervised contrastive learning framework, employ supervised contrastive learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Glaucoma is one of the ophthalmic diseases that may cause blindness, for
which early detection and treatment are very important. Fundus images and
optical coherence tomography (OCT) images are both widely-used modalities in
diagnosing glaucoma. However, existing glaucoma grading approaches mainly
utilize a single modality, ignoring the complementary information between
fundus and OCT. In this paper, we propose an efficient multi-modality
supervised contrastive learning framework, named COROLLA, for glaucoma grading.
Through layer segmentation as well as thickness calculation and projection,
retinal thickness maps are extracted from the original OCT volumes and used as
a replacing modality, resulting in more efficient calculations with less memory
usage. Given the high structure and distribution similarities across medical
image samples, we employ supervised contrastive learning to increase our
models' discriminative power with better convergence. Moreover, feature-level
fusion of paired fundus image and thickness map is conducted for enhanced
diagnosis accuracy. On the GAMMA dataset, our COROLLA framework achieves
overwhelming glaucoma grading performance compared to state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Reciprocal Adversarial Learning for Brain Tumor Segmentation: A Solution  to BraTS Challenge 2021 Segmentation Task</b></summary>
  <p><b>编号</b>：[195]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03777</p>
  <p><b>作者</b>：Himashi Peiris,  Zhaolin Chen,  Gary Egan,  Mehrtash Harandi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：dual reciprocal adversarial learning approaches, adversarial learning based training approach, proposed approach yielded better performance, virtual adversarial training approach, adversarial examples via adding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper proposes an adversarial learning based training approach for brain
tumor segmentation task. In this concept, the 3D segmentation network learns
from dual reciprocal adversarial learning approaches. To enhance the
generalization across the segmentation predictions and to make the segmentation
network robust, we adhere to the Virtual Adversarial Training approach by
generating more adversarial examples via adding some noise on original patient
data. By incorporating a critic that acts as a quantitative subjective referee,
the segmentation network learns from the uncertainty information associated
with segmentation results. We trained and evaluated network architecture on the
RSNA-ASNR-MICCAI BraTS 2021 dataset. Our performance on the online validation
dataset is as follows: Dice Similarity Score of 81.38%, 90.77% and 85.39%;
Hausdorff Distance (95\%) of 21.83 mm, 5.37 mm, 8.56 mm for the enhancing
tumor, whole tumor and tumor core, respectively. Similarly, our approach
achieved a Dice Similarity Score of 84.55%, 90.46% and 85.30%, as well as
Hausdorff Distance (95\%) of 13.48 mm, 6.32 mm and 16.98 mm on the final test
dataset. Overall, our proposed approach yielded better performance in
segmentation accuracy for each tumor sub-region. Our code implementation is
publicly available at this https URL</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：An analysis of reconstruction noise from undersampled 4D flow MRI</b></summary>
  <p><b>编号</b>：[196]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03715</p>
  <p><b>作者</b>：Lauren Partin,  Daniele E. Schiavazzi,  Carlos A. Sing Long</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：hemodynamic images may present visual artifacts, $\ ell_1 $- norm minimization, correlation length may increase significantly, correlation length remains limited, require long acquisition times</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Novel Magnetic Resonance (MR) imaging modalities can quantify hemodynamics
but require long acquisition times, precluding its widespread use for early
diagnosis of cardiovascular disease. To reduce the acquisition times,
reconstruction methods from undersampled measurements are routinely used, that
leverage representations designed to increase image compressibility.
Reconstructed anatomical and hemodynamic images may present visual artifacts.
Although some of these artifact are essentially reconstruction errors, and thus
a consequence of undersampling, others may be due to measurement noise or the
random choice of the sampled frequencies. Said otherwise, a reconstructed image
becomes a random variable, and both its bias and its covariance can lead to
visual artifacts; the latter leads to spatial correlations that may be
misconstrued for visual information. Although the nature of the former has been
studied in the literature, the latter has not received as much attention.
In this study, we investigate the theoretical properties of the random
perturbations arising from the reconstruction process, and perform a number of
numerical experiments on simulated and MR aortic flow. Our results show that
the correlation length remains limited to two to three pixels when a Gaussian
undersampling pattern is combined with recovery algorithms based on
$\ell_1$-norm minimization. However, the correlation length may increase
significantly for other undersampling patterns, higher undersampling factors
(i.e., 8x or 16x compression), and different reconstruction methods.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Neuroplastic graph attention networks for nuclei segmentation in  histopathology images</b></summary>
  <p><b>编号</b>：[199]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03669</p>
  <p><b>作者</b>：Yoav Alon,  Huiyu Zhou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：art deep learning approaches predominantly apply convolutional layers, novel neuroplastic graph attention network based, graph structure representing multiple magnification levels, modern histopathological image analysis relies, residual graph attention layers</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Modern histopathological image analysis relies on the segmentation of cell
structures to derive quantitative metrics required in biomedical research and
clinical diagnostics. State-of-the-art deep learning approaches predominantly
apply convolutional layers in segmentation and are typically highly customized
for a specific experimental configuration; often unable to generalize to
unknown data. As the model capacity of classical convolutional layers is
limited by a finite set of learned kernels, our approach uses a graph
representation of the image and focuses on the node transitions in multiple
magnifications. We propose a novel architecture for semantic segmentation of
cell nuclei robust to differences in experimental configuration such as
staining and variation of cell types. The architecture is comprised of a novel
neuroplastic graph attention network based on residual graph attention layers
and concurrent optimization of the graph structure representing multiple
magnification levels of the histopathological image. The modification of graph
structure, which generates the node features by projection, is as important to
the architecture as the graph neural network itself. It determines the possible
message flow and critical properties to optimize attention, graph structure,
and node updates in a balanced magnification loss. In experimental evaluation,
our framework outperforms ensembles of state-of-the-art neural networks, with a
fraction of the neurons typically required, and sets new standards for the
segmentation of new nuclei datasets.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：3D Segmentation with Fully Trainable Gabor Kernels and Pearson's  Correlation Coefficient</b></summary>
  <p><b>编号</b>：[201]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03644</p>
  <p><b>作者</b>：Ken C. L. Wong,  Mehdi Moradi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：less versatile gabor kernels become less popular despite, 43 3d brain magnetic resonance images, 19 anatomical structures show, using learnable parametric kernels, require manual weight selections</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The convolutional layer and loss function are two fundamental components in
deep learning. Because of the success of conventional deep learning kernels,
the less versatile Gabor kernels become less popular despite the fact that they
can provide abundant features at different frequencies, orientations, and
scales with much fewer parameters. For existing loss functions for multi-class
image segmentation, there is usually a tradeoff among accuracy, robustness to
hyperparameters, and manual weight selections for combining different losses.
Therefore, to gain the benefits of using Gabor kernels while keeping the
advantage of automatic feature generation in deep learning, we propose a fully
trainable Gabor-based convolutional layer where all Gabor parameters are
trainable through backpropagation. Furthermore, we propose a loss function
based on the Pearson's correlation coefficient, which is accurate, robust to
learning rates, and does not require manual weight selections. Experiments on
43 3D brain magnetic resonance images with 19 anatomical structures show that,
using the proposed loss function with a proper combination of conventional and
Gabor-based kernels, we can train a network with only 1.6 million parameters to
achieve an average Dice coefficient of 83%. This size is 44 times smaller than
the V-Net which has 71 million parameters. This paper demonstrates the
potentials of using learnable parametric kernels in deep learning for 3D
segmentation.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Iterative RAKI with Complex-Valued Convolution for Improved Image  Reconstruction with Limited Scan-Specific Training Samples</b></summary>
  <p><b>编号</b>：[204]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03560</p>
  <p><b>作者</b>：Peter Dawood,  Martin Blaimer,  Felix Breuer,  Paul R. Burd,  István Homolya,  Peter M. Jakob,  Johannes Oberberger</p>
  <p><b>备注</b>：Submitted to Magnetic Resonance in Medicine</p>
  <p><b>关键词</b>：raki provides superior reconstruction quality compared, 8 %), iraki outperforms raki, includes training data augmentation via, grappa method interpolates missing k, suppressing residual artefacts occurring</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>MRI scan time reduction is commonly achieved by Parallel Imaging methods,
typically based on uniform undersampling of the inverse image space (a.k.a.
k-space) and simultaneous signal reception with multiple receiver coils. The
GRAPPA method interpolates missing k-space signals by linear combination of
adjacent, acquired signals across all coils, and can be described by a
convolution in k-space. Recently, a more generalized method called RAKI was
introduced. RAKI is a deep-learning method that generalizes GRAPPA with
additional convolution layers, on which a non-linear activation function is
applied. This enables non-linear estimation of missing signals by convolutional
neural networks. In analogy to GRAPPA, the convolution kernels in RAKI are
trained using scan-specific training samples obtained from
auto-calibration-signals (ACS). RAKI provides superior reconstruction quality
compared to GRAPPA, however, often requires much more ACS due to its increased
number of unknown parameters. In order to overcome this limitation, this study
investigates the influence of training data on the reconstruction quality for
standard 2D imaging, with particular focus on its amount and contrast
information. Furthermore, an iterative k-space interpolation approach (iRAKI)
is evaluated, which includes training data augmentation via an initial GRAPPA
reconstruction, and refinement of convolution filters by iterative training.
Using only 18, 20 and 25 ACS lines (8%), iRAKI outperforms RAKI by suppressing
residual artefacts occurring at accelerations factors R=4 and R=5, and yields
strong noise suppression in comparison to GRAPPA, underlined by quantitative
quality metrics. Combination with a phase-constraint yields further
improvement. Additionally, iRAKI shows better performance than GRAPPA and RAKI
in case of pre-scan calibration and strongly varying contrast between training-
and undersampled data.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Demonstrating The Risk of Imbalanced Datasets in Chest X-ray Image-based  Diagnostics by Prototypical Relevance Propagation</b></summary>
  <p><b>编号</b>：[205]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03559</p>
  <p><b>作者</b>：Srishti Gautam,  Marina M.-C. Höhne,  Stine Hansen,  Robert Jenssen,  Michael Kampffmeyer</p>
  <p><b>备注</b>：To appear in ISBI 2022</p>
  <p><b>关键词</b>：improve automated diagnostics raises concerns, balanced source domain datasets, widely used chestx, source chest x, learning spurious correlations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The recent trend of integrating multi-source Chest X-Ray datasets to improve
automated diagnostics raises concerns that models learn to exploit
source-specific correlations to improve performance by recognizing the source
domain of an image rather than the medical pathology. We hypothesize that this
effect is enforced by and leverages label-imbalance across the source domains,
i.e, prevalence of a disease corresponding to a source. Therefore, in this
work, we perform a thorough study of the effect of label-imbalance in
multi-source training for the task of pneumonia detection on the widely used
ChestX-ray14 and CheXpert datasets. The results highlight and stress the
importance of using more faithful and transparent self-explaining models for
automated diagnosis, thus enabling the inherent detection of spurious learning.
They further illustrate that this undesirable effect of learning spurious
correlations can be reduced considerably when ensuring label-balanced source
domain datasets.</p>
  </details>
</details>
<h1>自然语言处理</h1>
<details>
  <summary>1. <b>标题：Uni-EDEN: Universal Encoder-Decoder Network by Multi-Granular  Vision-Language Pre-training</b></summary>
  <p><b>编号</b>：[32]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04026</p>
  <p><b>作者</b>：Yehao Li,  Jiahao Fan,  Yingwei Pan,  Ting Yao,  Weiyao Lin,  Tao Mei</p>
  <p><b>备注</b>：ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</p>
  <p><b>关键词</b>：moc ), masked region phrase generation, g ., visual question answering, g ., image captioning )., stream transformer based structure, sentence generation via inter</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision-language pre-training has been an emerging and fast-developing
research topic, which transfers multi-modal knowledge from rich-resource
pre-training task to limited-resource downstream tasks. Unlike existing works
that predominantly learn a single generic encoder, we present a pre-trainable
Universal Encoder-DEcoder Network (Uni-EDEN) to facilitate both vision-language
perception (e.g., visual question answering) and generation (e.g., image
captioning). Uni-EDEN is a two-stream Transformer based structure, consisting
of three modules: object and sentence encoders that separately learns the
representations of each modality, and sentence decoder that enables both
multi-modal reasoning and sentence generation via inter-modal interaction.
Considering that the linguistic representations of each image can span
different granularities in this hierarchy including, from simple to
comprehensive, individual label, a phrase, and a natural sentence, we pre-train
Uni-EDEN through multi-granular vision-language proxy tasks: Masked Object
Classification (MOC), Masked Region Phrase Generation (MRPG), Image-Sentence
Matching (ISM), and Masked Sentence Generation (MSG). In this way, Uni-EDEN is
endowed with the power of both multi-modal representation extraction and
language modeling. Extensive experiments demonstrate the compelling
generalizability of Uni-EDEN by fine-tuning it to four vision-language
perception and generation downstream tasks.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Multimodal Representations Learning Based on Mutual Information  Maximization and Minimization and Identity Embedding for Multimodal Sentiment  Analysis</b></summary>
  <p><b>编号</b>：[53]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03969</p>
  <p><b>作者</b>：Jiahao Zheng,  Sen Zhang,  Xiaoping Wang,  Zhigang Zeng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：fundamental complex research problem due, two public datasets demonstrate, robust multimodal representation needs, multimodal representation model based, combine mutual information maximization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multimodal sentiment analysis (MSA) is a fundamental complex research problem
due to the heterogeneity gap between different modalities and the ambiguity of
human emotional expression. Although there have been many successful attempts
to construct multimodal representations for MSA, there are still two challenges
to be addressed: 1) A more robust multimodal representation needs to be
constructed to bridge the heterogeneity gap and cope with the complex
multimodal interactions, and 2) the contextual dynamics must be modeled
effectively throughout the information flow. In this work, we propose a
multimodal representation model based on Mutual information Maximization and
Minimization and Identity Embedding (MMMIE). We combine mutual information
maximization between modal pairs, and mutual information minimization between
input data and corresponding features to mine the modal-invariant and
task-related information. Furthermore, Identity Embedding is proposed to prompt
the downstream network to perceive the contextual information. Experimental
results on two public datasets demonstrate the effectiveness of the proposed
model.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Emotion Intensity and its Control for Emotional Voice Conversion</b></summary>
  <p><b>编号</b>：[55]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03967</p>
  <p><b>作者</b>：Kun Zhou,  Berrak Sisman,  Rajib Rana,  Björn W. Schuller,  Haizhou Li</p>
  <p><b>备注</b>：Submitted to IEEE Transactions on Affective Computing</p>
  <p><b>关键词</b>：incorporate emotion classification loss, speech also conveys emotions, emotion embedding similarity loss, discrete categories overlooking, actual emotion encoder</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Emotional voice conversion (EVC) seeks to convert the emotional state of an
utterance while preserving the linguistic content and speaker identity. In EVC,
emotions are usually treated as discrete categories overlooking the fact that
speech also conveys emotions with various intensity levels that the listener
can perceive. In this paper, we aim to explicitly characterize and control the
intensity of emotion. We propose to disentangle the speaker style from
linguistic content and encode the speaker style into a style embedding in a
continuous space that forms the prototype of emotion embedding. We further
learn the actual emotion encoder from an emotion-labelled database and study
the use of relative attributes to represent fine-grained emotion intensity. To
ensure emotional intelligibility, we incorporate emotion classification loss
and emotion embedding similarity loss into the training of the EVC network. As
desired, the proposed network controls the fine-grained emotion intensity in
the output speech. Through both objective and subjective evaluations, we
validate the effectiveness of the proposed network for emotional expressiveness
and emotion intensity control.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Sentiment Analysis with Deep Learning Models: A Comparative Study on a  Decade of Sinhala Language Facebook Data</b></summary>
  <p><b>编号</b>：[65]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03941</p>
  <p><b>作者</b>：Gihan Weeraprameshwara,  Vihanga Jayawickrama,  Nisansa de Silva,  Yudhanjaya Wijeratne</p>
  <p><b>备注</b>：8 pages</p>
  <p><b>关键词</b>：3 layer bidirectional lstm model achieves, deep learning models show f1 scores, deep learning models catered, art sinhala sentiment analysis models, sinhala sentiment analysis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The relationship between Facebook posts and the corresponding reaction
feature is an interesting subject to explore and understand. To archive this
end, we test state-of-the-art Sinhala sentiment analysis models against a data
set containing a decade worth of Sinhala posts with millions of reactions. For
the purpose of establishing benchmarks and with the goal of identifying the
best model for Sinhala sentiment analysis, we also test, on the same data set
configuration, other deep learning models catered for sentiment analysis. In
this study we report that the 3 layer Bidirectional LSTM model achieves an F1
score of 84.58% for Sinhala sentiment analysis, surpassing the current
state-of-the-art model; Capsule B, which only manages to get an F1 score of
82.04%. Further, since all the deep learning models show F1 scores above 75% we
conclude that it is safe to claim that Facebook reactions are suitable to
predict the sentiment of a text.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：The GINCO Training Dataset for Web Genre Identification of Documents Out  in the Wild</b></summary>
  <p><b>编号</b>：[87]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03857</p>
  <p><b>作者</b>：Taja Kuzman,  Peter Rupnik,  Nikola Ljubešić</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：one document etc ., enabling evaluation, macro f1 metrics ranging around 0, 125 crawled slovenian web documents, builds upon existing schemata, initial machine learning experiments</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a new training dataset for automatic genre identification
GINCO, which is based on 1,125 crawled Slovenian web documents that consist of
650 thousand words. Each document was manually annotated for genre with a new
annotation schema that builds upon existing schemata, having primarily clarity
of labels and inter-annotator agreement in mind. The dataset consists of
various challenges related to web-based data, such as machine translated
content, encoding errors, multiple contents presented in one document etc.,
enabling evaluation of classifiers in realistic conditions. The initial machine
learning experiments on the dataset show that (1) pre-Transformer models are
drastically less able to model the phenomena, with macro F1 metrics ranging
around 0.22, while Transformer-based models achieve scores of around 0.58, and
(2) multilingual Transformer models work as well on the task as the monolingual
models that were previously proven to be superior to multilingual models on
standard NLP tasks.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Turkish Sentiment Analysis Using Machine Learning Methods: Application  on Online Food Order Site Reviews</b></summary>
  <p><b>编号</b>：[89]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03848</p>
  <p><b>作者</b>：Özlem Aktaş,  Berkay Coşkuner,  İlker Soner</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：3 different natural language processing methods applied resulted, various natural language processing methods used, various machine learning algorithms, every sector today, measuring customer satisfaction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Satisfaction measurement, which emerges in every sector today, is a very
important factor for many companies. In this study, it is aimed to reach the
highest accuracy rate with various machine learning algorithms by using the
data on Yemek Sepeti and variations of this data. The accuracy values of each
algorithm were calculated together with the various natural language processing
methods used. While calculating these accuracy values, the parameters of the
algorithms used were tried to be optimized. The models trained in this study on
labeled data can be used on unlabeled data and can give companies an idea in
measuring customer satisfaction. It was observed that 3 different natural
language processing methods applied resulted in approximately 5% accuracy
increase in most of the developed models.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Quantifying Robustness to Adversarial Word Substitutions</b></summary>
  <p><b>编号</b>：[93]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03829</p>
  <p><b>作者</b>：Yuting Yang,  Pei Huang,  FeiFei Ma,  Juan Cao,  Meishan Zhang,  Jian Zhang,  Jintao Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：art models like bert, metric helps us figure, based nlp models, rigorous statistical guarantee, repurpose attack methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep-learning-based NLP models are found to be vulnerable to word
substitution perturbations. Before they are widely adopted, the fundamental
issues of robustness need to be addressed. Along this line, we propose a formal
framework to evaluate word-level robustness. First, to study safe regions for a
model, we introduce robustness radius which is the boundary where the model can
resist any perturbation. As calculating the maximum robustness radius is
computationally hard, we estimate its upper and lower bound. We repurpose
attack methods as ways of seeking upper bound and design a pseudo-dynamic
programming algorithm for a tighter upper bound. Then verification method is
utilized for a lower bound. Further, for evaluating the robustness of regions
outside a safe radius, we reexamine robustness from another view:
quantification. A robustness metric with a rigorous statistical guarantee is
introduced to measure the quantification of adversarial examples, which
indicates the model's susceptibility to perturbations outside the safe radius.
The metric helps us figure out why state-of-the-art models like BERT can be
easily fooled by a few word substitutions, but generalize well in the presence
of real-world noises.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：CI-AVSR: A Cantonese Audio-Visual Speech Dataset for In-car Command  Recognition</b></summary>
  <p><b>编号</b>：[105]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03804</p>
  <p><b>作者</b>：Wenliang Dai,  Samuel Cahyawijaya,  Tiezheng Yu,  Elham J. Barezi,  Peng Xu,  Cheuk Tung Shadow Yiu,  Rita Frieske,  Holy Lovenia,  Genta Indra Winata,  Qifeng Chen,  Xiaojuan Ma,  Bertram E. Shi,  Pascale Fung</p>
  <p><b>备注</b>：6 pages</p>
  <p><b>关键词</b>：implement two multimodal baselines, 30 native cantonese speakers, dataset 10 times larger, car speech recognition systems, visual speech recognition</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the rise of deep learning and intelligent vehicle, the smart assistant
has become an essential in-car component to facilitate driving and provide
extra functionalities. In-car smart assistants should be able to process
general as well as car-related commands and perform corresponding actions,
which eases driving and improves safety. However, there is a data scarcity
issue for low resource languages, hindering the development of research and
applications. In this paper, we introduce a new dataset, Cantonese In-car
Audio-Visual Speech Recognition (CI-AVSR), for in-car command recognition in
the Cantonese language with both video and audio data. It consists of 4,984
samples (8.3 hours) of 200 in-car commands recorded by 30 native Cantonese
speakers. Furthermore, we augment our dataset using common in-car background
noises to simulate real environments, producing a dataset 10 times larger than
the collected one. We provide detailed statistics of both the clean and the
augmented versions of our dataset. Moreover, we implement two multimodal
baselines to demonstrate the validity of CI-AVSR. Experiment results show that
leveraging the visual signal improves the overall performance of the model.
Although our best model can achieve a considerable quality on the clean test
set, the speech recognition quality on the noisy data is still inferior and
remains as an extremely challenging task for real in-car speech recognition
systems. The dataset and code will be released at
this https URL.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Prior Knowledge Enhances Radiology Report Generation</b></summary>
  <p><b>编号</b>：[117]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03761</p>
  <p><b>作者</b>：Song Wang,  Liyan Tang,  Mingquan Lin,  George Shih,  Ying Ding,  Yifan Peng</p>
  <p><b>备注</b>：10 pages, 4 figures, accepted by AMIA 2022 Informatics Summit</p>
  <p><b>关键词</b>：previous deep learning methods tend, drawn increasing attention recently, radiology report generation aims, accurate radiology report generation, associations among medical findings</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Radiology report generation aims to produce computer-aided diagnoses to
alleviate the workload of radiologists and has drawn increasing attention
recently. However, previous deep learning methods tend to neglect the mutual
influences between medical findings, which can be the bottleneck that limits
the quality of generated reports. In this work, we propose to mine and
represent the associations among medical findings in an informative knowledge
graph and incorporate this prior knowledge with radiology report generation to
help improve the quality of generated reports. Experiment results demonstrate
the superior performance of our proposed method on the IU X-ray dataset with a
ROUGE-L of 0.384$\pm$0.007 and CIDEr of 0.340$\pm$0.011. Compared with previous
works, our model achieves an average of 1.6% improvement (2.0% and 1.5%
improvements in CIDEr and ROUGE-L, respectively). The experiments suggest that
prior knowledge can bring performance gains to accurate radiology report
generation. We will make the code publicly available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Explaining Prediction Uncertainty of Pre-trained Language Models by  Detecting Uncertain Words in Inputs</b></summary>
  <p><b>编号</b>：[123]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03742</p>
  <p><b>作者</b>：Hanjie Chen,  Yangfeng Ji</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：although many previous works focus, methods consistently capture words, trained language models, trained language models, natural language inference</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Estimating the predictive uncertainty of pre-trained language models is
important for increasing their trustworthiness in NLP. Although many previous
works focus on quantifying prediction uncertainty, there is little work on
explaining the uncertainty. This paper pushes a step further on explaining
uncertain predictions of post-calibrated pre-trained language models. We adapt
two perturbation-based post-hoc interpretation methods, Leave-one-out and
Sampling Shapley, to identify words in inputs that cause the uncertainty in
predictions. We test the proposed methods on BERT and RoBERTa with three tasks:
sentiment classification, natural language inference, and paraphrase
identification, in both in-domain and out-of-domain settings. Experiments show
that both methods consistently capture words in inputs that cause prediction
uncertainty.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：CVSS Corpus and Massively Multilingual Speech-to-Speech Translation</b></summary>
  <p><b>编号</b>：[132]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03713</p>
  <p><b>作者</b>：Ye Jia,  Michelle Tadmor Ramanovich,  Quan Wang,  Heiga Zen</p>
  <p><b>备注</b>：Submitted to LREC 2022</p>
  <p><b>关键词</b>：built baseline multilingual direct s2st models, build strong cascade s2st baselines, cvss provides normalized translation text, direct s2st models approaches, level parallel s2st pairs</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce CVSS, a massively multilingual-to-English speech-to-speech
translation (S2ST) corpus, covering sentence-level parallel S2ST pairs from 21
languages into English. CVSS is derived from the Common Voice speech corpus and
the CoVoST 2 speech-to-text translation (ST) corpus, by synthesizing the
translation text from CoVoST 2 into speech using state-of-the-art TTS systems.
Two versions of translation speeches are provided: 1) CVSS-C: All the
translation speeches are in a single high-quality canonical voice; 2) CVSS-T:
The translation speeches are in voices transferred from the corresponding
source speeches. In addition, CVSS provides normalized translation text which
matches the pronunciation in the translation speech. On each version of CVSS,
we built baseline multilingual direct S2ST models and cascade S2ST models,
verifying the effectiveness of the corpus. To build strong cascade S2ST
baselines, we trained an ST model on CoVoST 2, which outperforms the previous
state-of-the-art trained on the corpus without extra data by 5.8 BLEU.
Nevertheless, the performance of the direct S2ST models approaches the strong
cascade baselines when trained from scratch, and with only 0.1 or 0.7 BLEU
difference on ASR transcribed translation when initialized from matching ST
models.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Informal Persian Universal Dependency Treebank</b></summary>
  <p><b>编号</b>：[142]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03679</p>
  <p><b>作者</b>：Roya Kabiri,  Simin Karimi,  Mihai Surdeanu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：natural language processing tools across languages, source informal persian universal dependency treebank, dependency relations whose performance deteriorates, informal persian exhibits particular characteristics, new treebank annotated within</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents the phonological, morphological, and syntactic
distinctions between formal and informal Persian, showing that these two
variants have fundamental differences that cannot be attributed solely to
pronunciation discrepancies. Given that informal Persian exhibits particular
characteristics, any computational model trained on formal Persian is unlikely
to transfer well to informal Persian, necessitating the creation of dedicated
treebanks for this variety. We thus detail the development of the open-source
Informal Persian Universal Dependency Treebank, a new treebank annotated within
the Universal Dependencies scheme. We then investigate the parsing of informal
Persian by training two dependency parsers on existing formal treebanks and
evaluating them on out-of-domain data, i.e. the development set of our informal
treebank. Our results show that parsers experience a substantial performance
drop when we move across the two domains, as they face more unknown tokens and
structures and fail to generalize well. Furthermore, the dependency relations
whose performance deteriorates the most represent the unique properties of the
informal variant. The ultimate goal of this study that demonstrates a broader
impact is to provide a stepping-stone to reveal the significance of informal
variants of languages, which have been widely overlooked in natural language
processing tools across languages.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Language-Agnostic Website Embedding and Classification</b></summary>
  <p><b>编号</b>：[143]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03677</p>
  <p><b>作者</b>：Sylvain Lugeon,  Tiziano Piccardi,  Robert West</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：dataset contains 14 website categories aligned across languages, curated curlie dataset aligned across languages, largest multilingual crowdsourced web directory, stable performance across low, efficiently computable features suffices</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Currently, publicly available models for website classification do not offer
an embedding method and have limited support for languages beyond English. We
release a dataset with more than 1M websites in 92 languages with relative
labels collected from Curlie, the largest multilingual crowdsourced Web
directory. The dataset contains 14 website categories aligned across languages.
Alongside it, we introduce Homepage2Vec, a machine-learned pre-trained model
for classifying and embedding websites based on their homepage in a
language-agnostic way. Homepage2Vec, thanks to its feature set (textual
content, metadata tags, and visual attributes) and recent progress in natural
language representation, is language-independent by design and can generate
embeddings representation. We show that Homepage2Vec correctly classifies
websites with a macro-averaged F1-score of 0.90, with stable performance across
low- as well as high-resource languages. Feature analysis shows that a small
subset of efficiently computable features suffices to achieve high performance
even with limited computational resources. We make publicly available the
curated Curlie dataset aligned across languages, the pre-trained Homepage2Vec
model, and libraries.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：A Likelihood Ratio based Domain Adaptation Method for E2E Models</b></summary>
  <p><b>编号</b>：[149]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03655</p>
  <p><b>作者</b>：Chhavi Choudhury,  Ankur Gandhe,  Xiaohan Ding,  Ivan Bulyko</p>
  <p><b>备注</b>：Submitted to ICASSP 2022</p>
  <p><b>关键词</b>：automatic speech recognition models like recurrent neural networks transducer, pass rescoring model gives additive wer improvements, streaming asr applications like voice assistants, contextual biasing approach using likelihood, improving rare words recognition</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>End-to-end (E2E) automatic speech recognition models like Recurrent Neural
Networks Transducer (RNN-T) are becoming a popular choice for streaming ASR
applications like voice assistants. While E2E models are very effective at
learning representation of the training data they are trained on, their
accuracy on unseen domains remains a challenging problem. Additionally, these
models require paired audio and text training data, are computationally
expensive and are difficult to adapt towards the fast evolving nature of
conversational speech. In this work, we explore a contextual biasing approach
using likelihood-ratio that leverages text data sources to adapt RNN-T model to
new domains and entities. We show that this method is effective in improving
rare words recognition, and results in a relative improvement of 10% in 1-best
word error rate (WER) and 10% in n-best Oracle WER (n=8) on multiple
out-of-domain datasets without any degradation on a general dataset. We also
show that complementing the contextual biasing adaptation with adaptation of a
second-pass rescoring model gives additive WER improvements.</p>
  </details>
</details>
<h1>机器学习</h1>
<details>
  <summary>1. <b>标题：Optimally compressing VC classes</b></summary>
  <p><b>编号</b>：[1]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04131</p>
  <p><b>作者</b>：Zachary Chase</p>
  <p><b>备注</b>：4 pages</p>
  <p><b>关键词</b>：sample compression scheme, concept class, warmuth, vc, size</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Resolving a conjecture of Littlestone and Warmuth, we show that any concept
class of VC-dimension $d$ has a sample compression scheme of size $d$.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：In Defense of the Unitary Scalarization for Deep Multi-Task Learning</b></summary>
  <p><b>编号</b>：[6]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04122</p>
  <p><b>作者</b>：Vitaly Kurin,  Alessandro De Palma,  Ilya Kostrikov,  Shimon Whiteson,  M. Pawan Kumar</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：task learning research argues, training simply minimizes, theoretical analysis suggesting, reinforcement learning settings, introduce significant memory</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent multi-task learning research argues against unitary scalarization,
where training simply minimizes the sum of the task losses. Several ad-hoc
multi-task optimization algorithms have instead been proposed, inspired by
various hypotheses about what makes multi-task settings difficult. The majority
of these optimizers require per-task gradients, and introduce significant
memory, runtime, and implementation overhead. We present a theoretical analysis
suggesting that many specialized multi-task optimizers can be interpreted as
forms of regularization. Moreover, we show that, when coupled with standard
regularization and stabilization techniques from single-task learning, unitary
scalarization matches or improves upon the performance of complex multi-task
optimizers in both supervised and reinforcement learning settings. We believe
our results call for a critical reevaluation of recent research in the area.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Learning to Denoise Raw Mobile UI Layouts for ImprovingDatasets at Scale</b></summary>
  <p><b>编号</b>：[9]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04100</p>
  <p><b>作者</b>：Gang Li,  Gilles Baechler,  Manuel Tragut,  Yang Li</p>
  <p><b>备注</b>：Accepted to ACM CHI 2022</p>
  <p><b>关键词</b>：deep models achieve high accuracy withf1 scores, scale high quality ui layout datasetsfor data, deep learning approach fordenoising ui layouts, automatically improve existingmobile ui layout datasets, raw layout byremoving incorrect nodes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The layout of a mobile screen is a critical data source for UI designresearch
and semantic understanding of the screen. However, UIlayouts in existing
datasets are often noisy, have mismatches withtheir visual representation, or
consists of generic or app-specifictypes that are difficult to analyze and
model. In this paper, wepropose the CLAY pipeline that uses a deep learning
approach fordenoising UI layouts, allowing us to automatically improve
existingmobile UI layout datasets at scale. Our pipeline takes both
thescreenshot and the raw UI layout, and annotates the raw layout byremoving
incorrect nodes and assigning a semantically meaningfultype to each node. To
experiment with our data-cleaning pipeline,we create the CLAY dataset of 59,555
human-annotated screenlayouts, based on screenshots and raw layouts from Rico,
a publicmobile UI corpus. Our deep models achieve high accuracy withF1 scores
of 82.7% for detecting layout objects that do not have avalid visual
representation and 85.9% for recognizing object types,which significantly
outperforms a heuristic baseline. Our work laysa foundation for creating
large-scale high quality UI layout datasetsfor data-driven mobile UI research
and reduces the need of manuallabeling efforts that are prohibitively
expensive.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Data transformation based optimized customer churn prediction model for  the telecommunication industry</b></summary>
  <p><b>编号</b>：[12]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04088</p>
  <p><b>作者</b>：Joydeb Kumar Sana,  Mohammad Zoynul Abedin,  M. Sohel Rahman,  M. Saifur Rahman</p>
  <p><b>备注</b>：24 pages</p>
  <p><b>关键词</b>：proposed data transformation based optimized models improve, investigated various data transform methods, widely used evaluation measures, publicly available tci datasets, combining data transformation methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Data transformation (DT) is a process that transfers the original data into a
form which supports a particular classification algorithm and helps to analyze
the data for a special purpose. To improve the prediction performance we
investigated various data transform methods. This study is conducted in a
customer churn prediction (CCP) context in the telecommunication industry
(TCI), where customer attrition is a common phenomenon. We have proposed a
novel approach of combining data transformation methods with the machine
learning models for the CCP problem. We conducted our experiments on publicly
available TCI datasets and assessed the performance in terms of the widely used
evaluation measures (e.g. AUC, precision, recall, and F-measure). In this
study, we presented comprehensive comparisons to affirm the effect of the
transformation methods. The comparison results and statistical test proved that
most of the proposed data transformation based optimized models improve the
performance of CCP significantly. Overall, an efficient and optimized CCP model
for the telecommunication industry has been presented through this manuscript.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：VGAER: graph neural network reconstruction based community detection</b></summary>
  <p><b>编号</b>：[15]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04066</p>
  <p><b>作者</b>：Chenyang Qiu,  Zhaoci Huang,  Wenzhe Xu,  Huijia Li</p>
  <p><b>备注</b>：Accepted by AAAI-22: DLG-AAAI'22 (this https URL)</p>
  <p><b>关键词</b>：variational graph autoencoder reconstruction based community detection vgaer, carefully designed corresponding input features, community detection algorithms based, powerful network modularity ability, graph neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Community detection is a fundamental and important issue in network science,
but there are only a few community detection algorithms based on graph neural
networks, among which unsupervised algorithms are almost blank. By fusing the
high-order modularity information with network features, this paper proposes a
Variational Graph AutoEncoder Reconstruction based community detection VGAER
for the first time, and gives its non-probabilistic version. They do not need
any prior information. We have carefully designed corresponding input features,
decoder, and downstream tasks based on the community detection task and these
designs are concise, natural, and perform well (NMI values under our design are
improved by 59.1% - 565.9%). Based on a series of experiments with wide range
of datasets and advanced methods, VGAER has achieved superior performance and
shows strong competitiveness and potential with a simpler design. Finally, we
report the results of algorithm convergence analysis and t-SNE visualization,
which clearly depicted the stable performance and powerful network modularity
ability of VGAER. Our codes are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Differentially Describing Groups of Graphs</b></summary>
  <p><b>编号</b>：[16]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04064</p>
  <p><b>作者</b>：Corinna Coupette,  Sebastian Dalleiger,  Jilles Vreeken</p>
  <p><b>备注</b>：9 pages, 6 figures, accepted for publication at AAAI22</p>
  <p><b>关键词</b>：uses maximum entropy modeling, perform graph group analysis, graph group analysis, statistically significant associations, shared across classes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>How does neural connectivity in autistic children differ from neural
connectivity in healthy children or autistic youths? What patterns in global
trade networks are shared across classes of goods, and how do these patterns
change over time? Answering questions like these requires us to differentially
describe groups of graphs: Given a set of graphs and a partition of these
graphs into groups, discover what graphs in one group have in common, how they
systematically differ from graphs in other groups, and how multiple groups of
graphs are related. We refer to this task as graph group analysis, which seeks
to describe similarities and differences between graph groups by means of
statistically significant subgraphs. To perform graph group analysis, we
introduce Gragra, which uses maximum entropy modeling to identify a
non-redundant set of subgraphs with statistically significant associations to
one or more graph groups. Through an extensive set of experiments on a wide
range of synthetic and real-world graph groups, we confirm that Gragra works
well in practice.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：State Estimation in Electric Power Systems Leveraging Graph Neural  Networks</b></summary>
  <p><b>编号</b>：[21]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04056</p>
  <p><b>作者</b>：Ognjen Kundacina,  Mirsad Cosovic,  Dejan Vukobratovic</p>
  <p><b>备注</b>：6 pages, 6 figures, conference paper</p>
  <p><b>关键词</b>：estimate complex bus voltages, trained using synthetic datasets, pmu high sampling rates, solution obtained using, randomly sampling sets</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The goal of the state estimation (SE) algorithm is to estimate complex bus
voltages as state variables based on the available set of measurements in the
power system. Because phasor measurement units (PMUs) are increasingly being
used in transmission power systems, there is a need for a fast SE solver that
can take advantage of PMU high sampling rates. This paper proposes training a
graph neural network (GNN) to learn the estimates given the PMU voltage and
current measurements as inputs, with the intent of obtaining fast and accurate
predictions during the evaluation phase. GNN is trained using synthetic
datasets, created by randomly sampling sets of measurements in the power system
and labelling them with a solution obtained using a linear SE with PMUs solver.
The presented results display the accuracy of GNN predictions in various test
scenarios and tackle the sensitivity of the predictions to the missing input
data.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：DDG-DA: Data Distribution Generation for Predictable Concept Drift  Adaptation</b></summary>
  <p><b>编号</b>：[28]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04038</p>
  <p><b>作者</b>：Wendi Li,  Xiao Yang,  Weiqing Liu,  Yingce Xia,  Jiang Bian</p>
  <p><b>备注</b>：Accepted by AAAI'22</p>
  <p><b>关键词</b>：streaming data distribution may change, previous methods first detect, future concept drift trend, stock price trend, obtain significant improvement</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In many real-world scenarios, we often deal with streaming data that is
sequentially collected over time. Due to the non-stationary nature of the
environment, the streaming data distribution may change in unpredictable ways,
which is known as concept drift. To handle concept drift, previous methods
first detect when/where the concept drift happens and then adapt models to fit
the distribution of the latest data. However, there are still many cases that
some underlying factors of environment evolution are predictable, making it
possible to model the future concept drift trend of the streaming data, while
such cases are not fully explored in previous work.
In this paper, we propose a novel method DDG-DA, that can effectively
forecast the evolution of data distribution and improve the performance of
models. Specifically, we first train a predictor to estimate the future data
distribution, then leverage it to generate training samples, and finally train
models on the generated data. We conduct experiments on three real-world tasks
(forecasting on stock price trend, electricity load and solar irradiance) and
obtain significant improvement on multiple widely-used models.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Captcha Attack:Turning Captchas Against Humanity</b></summary>
  <p><b>编号</b>：[40]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04014</p>
  <p><b>作者</b>：Mauro Conti,  Luca Pajola,  Pier Paolo Tricomi</p>
  <p><b>备注</b>：Currently under submission</p>
  <p><b>关键词</b>：9 billion daily active facebook users posted around 150 thousand photos every minute, help human moderators handle high data volume, content moderators constantly monitor, generating custom textual captchas, g ., social networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Nowadays, people generate and share massive content on online platforms
(e.g., social networks, blogs). In 2021, the 1.9 billion daily active Facebook
users posted around 150 thousand photos every minute. Content moderators
constantly monitor these online platforms to prevent the spreading of
inappropriate content (e.g., hate speech, nudity images). Based on deep
learning (DL) advances, Automatic Content Moderators (ACM) help human
moderators handle high data volume. Despite their advantages, attackers can
exploit weaknesses of DL components (e.g., preprocessing, model) to affect
their performance. Therefore, an attacker can leverage such techniques to
spread inappropriate content by evading ACM.
In this work, we propose CAPtcha Attack (CAPA), an adversarial technique that
allows users to spread inappropriate text online by evading ACM controls. CAPA,
by generating custom textual CAPTCHAs, exploits ACM's careless design
implementations and internal procedures vulnerabilities. We test our attack on
real-world ACM, and the results confirm the ferocity of our simple yet
effective attack, reaching up to a 100% evasion success in most cases. At the
same time, we demonstrate the difficulties in designing CAPA mitigations,
opening new challenges in CAPTCHAs research area.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Multimodal Representations Learning Based on Mutual Information  Maximization and Minimization and Identity Embedding for Multimodal Sentiment  Analysis</b></summary>
  <p><b>编号</b>：[53]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03969</p>
  <p><b>作者</b>：Jiahao Zheng,  Sen Zhang,  Xiaoping Wang,  Zhigang Zeng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：fundamental complex research problem due, two public datasets demonstrate, robust multimodal representation needs, multimodal representation model based, combine mutual information maximization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multimodal sentiment analysis (MSA) is a fundamental complex research problem
due to the heterogeneity gap between different modalities and the ambiguity of
human emotional expression. Although there have been many successful attempts
to construct multimodal representations for MSA, there are still two challenges
to be addressed: 1) A more robust multimodal representation needs to be
constructed to bridge the heterogeneity gap and cope with the complex
multimodal interactions, and 2) the contextual dynamics must be modeled
effectively throughout the information flow. In this work, we propose a
multimodal representation model based on Mutual information Maximization and
Minimization and Identity Embedding (MMMIE). We combine mutual information
maximization between modal pairs, and mutual information minimization between
input data and corresponding features to mine the modal-invariant and
task-related information. Furthermore, Identity Embedding is proposed to prompt
the downstream network to perceive the contextual information. Experimental
results on two public datasets demonstrate the effectiveness of the proposed
model.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Optimal and Differentially Private Data Acquisition: Central and Local  Mechanisms</b></summary>
  <p><b>编号</b>：[54]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03968</p>
  <p><b>作者</b>：Alireza Fallah,  Ali Makhdoumi,  Azarakhsh Malekian,  Asuman Ozdaglar</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：consider two popular differential privacy settings, given heterogeneous privacy loss levels, quantify using differential privacy, establish minimax lower bounds, develop efficient algorithmic mechanisms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider a platform's problem of collecting data from privacy sensitive
users to estimate an underlying parameter of interest. We formulate this
question as a Bayesian-optimal mechanism design problem, in which an individual
can share her (verifiable) data in exchange for a monetary reward or services,
but at the same time has a (private) heterogeneous privacy cost which we
quantify using differential privacy. We consider two popular differential
privacy settings for providing privacy guarantees for the users: central and
local. In both settings, we establish minimax lower bounds for the estimation
error and derive (near) optimal estimators for given heterogeneous privacy loss
levels for users. Building on this characterization, we pose the mechanism
design problem as the optimal selection of an estimator and payments that will
elicit truthful reporting of users' privacy sensitivities. Under a regularity
condition on the distribution of privacy sensitivities we develop efficient
algorithmic mechanisms to solve this problem in both privacy settings. Our
mechanism in the central setting can be implemented in time $\mathcal{O}(n \log
n)$ where $n$ is the number of users and our mechanism in the local setting
admits a Polynomial Time Approximation Scheme (PTAS).</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Emotion Intensity and its Control for Emotional Voice Conversion</b></summary>
  <p><b>编号</b>：[55]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03967</p>
  <p><b>作者</b>：Kun Zhou,  Berrak Sisman,  Rajib Rana,  Björn W. Schuller,  Haizhou Li</p>
  <p><b>备注</b>：Submitted to IEEE Transactions on Affective Computing</p>
  <p><b>关键词</b>：incorporate emotion classification loss, speech also conveys emotions, emotion embedding similarity loss, discrete categories overlooking, actual emotion encoder</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Emotional voice conversion (EVC) seeks to convert the emotional state of an
utterance while preserving the linguistic content and speaker identity. In EVC,
emotions are usually treated as discrete categories overlooking the fact that
speech also conveys emotions with various intensity levels that the listener
can perceive. In this paper, we aim to explicitly characterize and control the
intensity of emotion. We propose to disentangle the speaker style from
linguistic content and encode the speaker style into a style embedding in a
continuous space that forms the prototype of emotion embedding. We further
learn the actual emotion encoder from an emotion-labelled database and study
the use of relative attributes to represent fine-grained emotion intensity. To
ensure emotional intelligibility, we incorporate emotion classification loss
and emotion embedding similarity loss into the training of the EVC network. As
desired, the proposed network controls the fine-grained emotion intensity in
the output speech. Through both objective and subjective evaluations, we
validate the effectiveness of the proposed network for emotional expressiveness
and emotion intensity control.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：On the Efficacy of Co-Attention Transformer Layers in Visual Question  Answering</b></summary>
  <p><b>编号</b>：[56]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03965</p>
  <p><b>作者</b>：Ankur Sikarwar,  Gabriel Kreiman</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：vqa ), outperforming previous architectures, generate visual attention maps using, neural network attention maps, conditioned image attention scores, human attention maps</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, multi-modal transformers have shown significant progress in
Vision-Language tasks, such as Visual Question Answering (VQA), outperforming
previous architectures by a considerable margin. This improvement in VQA is
often attributed to the rich interactions between vision and language streams.
In this work, we investigate the efficacy of co-attention transformer layers in
helping the network focus on relevant regions while answering the question. We
generate visual attention maps using the question-conditioned image attention
scores in these co-attention layers. We evaluate the effect of the following
critical components on visual attention of a state-of-the-art VQA model: (i)
number of object region proposals, (ii) question part of speech (POS) tags,
(iii) question semantics, (iv) number of co-attention layers, and (v) answer
accuracy. We compare the neural network attention maps against human attention
maps both qualitatively and quantitatively. Our findings indicate that
co-attention transformer modules are crucial in attending to relevant regions
of the image given a question. Importantly, we observe that the semantic
meaning of the question is not what drives visual attention, but specific
keywords in the question do. Our work sheds light on the function and
interpretation of co-attention transformer layers, highlights gaps in current
networks, and can guide the development of future VQA models and networks that
simultaneously process visual and language streams.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Multi-granularity Relabeled Under-sampling Algorithm for Imbalanced Data</b></summary>
  <p><b>编号</b>：[58]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03957</p>
  <p><b>作者</b>：Qi Dai,  Jian-wei Liu,  Yang Liu</p>
  <p><b>备注</b>：35 pages</p>
  <p><b>关键词</b>：optimal global relabeled index value, global relabeled index value, potential local overlapping instances, local potential overlapping instances, imbalanced classification problem turns</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The imbalanced classification problem turns out to be one of the important
and challenging problems in data mining and machine learning. The performances
of traditional classifiers will be severely affected by many data problems,
such as class imbalanced problem, class overlap and noise. The Tomek-Link
algorithm was only used to clean data when it was proposed. In recent years,
there have been reports of combining Tomek-Link algorithm with sampling
technique. The Tomek-Link sampling algorithm can effectively reduce the class
overlap on data, remove the majority instances that are difficult to
distinguish, and improve the algorithm classification accuracy. However, the
Tomek-Links under-sampling algorithm only considers the boundary instances that
are the nearest neighbors to each other globally and ignores the potential
local overlapping instances. When the number of minority instances is small,
the under-sampling effect is not satisfactory, and the performance improvement
of the classification model is not obvious. Therefore, on the basis of
Tomek-Link, a multi-granularity relabeled under-sampling algorithm (MGRU) is
proposed. This algorithm fully considers the local information of the data set
in the local granularity subspace, and detects the local potential overlapping
instances in the data set. Then, the overlapped majority instances are
eliminated according to the global relabeled index value, which effectively
expands the detection range of Tomek-Links. The simulation results show that
when we select the optimal global relabeled index value for under-sampling, the
classification accuracy and generalization performance of the proposed
under-sampling algorithm are significantly better than other baseline
algorithms.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：The Dataset Nutrition Label (2nd Gen): Leveraging Context to Mitigate  Harms in Artificial Intelligence</b></summary>
  <p><b>编号</b>：[59]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03954</p>
  <p><b>作者</b>：Kasia S. Chmielinski,  Sarah Newman,  Matt Taylor,  Josh Joseph,  Kemi Thomas,  Jessica Yurkofsky,  Yue Chelsea Qiu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：user interface targeted towards, new label includes context, work including new datasets, specific use cases, produce automated decision</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As the production of and reliance on datasets to produce automated
decision-making systems (ADS) increases, so does the need for processes for
evaluating and interrogating the underlying data. After launching the Dataset
Nutrition Label in 2018, the Data Nutrition Project has made significant
updates to the design and purpose of the Label, and is launching an updated
Label in late 2020, which is previewed in this paper. The new Label includes
context-specific Use Cases &Alerts presented through an updated design and user
interface targeted towards the data scientist profile. This paper discusses the
harm and bias from underlying training data that the Label is intended to
mitigate, the current state of the work including new datasets being labeled,
new and existing challenges, and further directions of the work, as well as
Figures previewing the new label.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Active Reinforcement Learning -- A Roadmap Towards Curious Classifier  Systems for Self-Adaptation</b></summary>
  <p><b>编号</b>：[62]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03947</p>
  <p><b>作者</b>：Simon Reichhuber,  Sven Tomforde</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：fundamental reinforcement learning approaches come, traditional approaches separate, time taking observations, research agenda towards, make isolated use</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Intelligent systems have the ability to improve their behaviour over time
taking observations, experiences or explicit feedback into account. Traditional
approaches separate the learning problem and make isolated use of techniques
from different field of machine learning such as reinforcement learning, active
learning, anomaly detection or transfer learning, for instance. In this
context, the fundamental reinforcement learning approaches come with several
drawbacks that hinder their application to real-world systems: trial-and-error,
purely reactive behaviour or isolated problem handling. The idea of this
article is to present a concept for alleviating these drawbacks by setting up a
research agenda towards what we call "active reinforcement learning" in
intelligent systems.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Feature Extraction Framework based on Contrastive Learning with Adaptive  Positive and Negative Samples</b></summary>
  <p><b>编号</b>：[64]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03942</p>
  <p><b>作者</b>：Hongjie Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：final numerical experiments prove, traditional feature extraction methods, feature extraction framework based, infonce loss based, view feature extraction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this study, we propose a feature extraction framework based on contrastive
learning with adaptive positive and negative samples (CL-FEFA) that is suitable
for unsupervised, supervised, and semi-supervised single-view feature
extraction. CL-FEFA constructs adaptively the positive and negative samples
from the results of feature extraction, which makes it more appropriate and
accurate. Thereafter, the discriminative features are re extracted to according
to InfoNCE loss based on previous positive and negative samples, which will
make the intra-class samples more compact and the inter-class samples more
dispersed. At the same time, using the potential structure information of
subspace samples to dynamically construct positive and negative samples can
make our framework more robust to noisy data. Furthermore, CL-FEFA considers
the mutual information between positive samples, that is, similar samples in
potential structures, which provides theoretical support for its advantages in
feature extraction. The final numerical experiments prove that the proposed
framework has a strong advantage over the traditional feature extraction
methods and contrastive learning methods.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Sentiment Analysis with Deep Learning Models: A Comparative Study on a  Decade of Sinhala Language Facebook Data</b></summary>
  <p><b>编号</b>：[65]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03941</p>
  <p><b>作者</b>：Gihan Weeraprameshwara,  Vihanga Jayawickrama,  Nisansa de Silva,  Yudhanjaya Wijeratne</p>
  <p><b>备注</b>：8 pages</p>
  <p><b>关键词</b>：3 layer bidirectional lstm model achieves, deep learning models show f1 scores, deep learning models catered, art sinhala sentiment analysis models, sinhala sentiment analysis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The relationship between Facebook posts and the corresponding reaction
feature is an interesting subject to explore and understand. To archive this
end, we test state-of-the-art Sinhala sentiment analysis models against a data
set containing a decade worth of Sinhala posts with millions of reactions. For
the purpose of establishing benchmarks and with the goal of identifying the
best model for Sinhala sentiment analysis, we also test, on the same data set
configuration, other deep learning models catered for sentiment analysis. In
this study we report that the 3 layer Bidirectional LSTM model achieves an F1
score of 84.58% for Sinhala sentiment analysis, surpassing the current
state-of-the-art model; Capsule B, which only manages to get an F1 score of
82.04%. Further, since all the deep learning models show F1 scores above 75% we
conclude that it is safe to claim that Facebook reactions are suitable to
predict the sentiment of a text.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Automated Reinforcement Learning (AutoRL): A Survey and Open Problems</b></summary>
  <p><b>编号</b>：[67]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03916</p>
  <p><b>作者</b>：Jack Parker-Holder,  Raghu Rajan,  Xingyou Song,  André Biedenkapp,  Yingjie Miao,  Theresa Eimer,  Baohe Zhang,  Vu Nguyen,  Roberto Calandra,  Aleksandra Faust,  Frank Hutter,  Marius Lindauer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：also yielded promising initial results, also includes additional challenges unique, path towards generally capable agents, researchers going forward, prone manual tuning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The combination of Reinforcement Learning (RL) with deep learning has led to
a series of impressive feats, with many believing (deep) RL provides a path
towards generally capable agents. However, the success of RL agents is often
highly sensitive to design choices in the training process, which may require
tedious and error-prone manual tuning. This makes it challenging to use RL for
new problems, while also limits its full potential. In many other areas of
machine learning, AutoML has shown it is possible to automate such design
choices and has also yielded promising initial results when applied to RL.
However, Automated Reinforcement Learning (AutoRL) involves not only standard
applications of AutoML but also includes additional challenges unique to RL,
that naturally produce a different set of methods. As such, AutoRL has been
emerging as an important area of research in RL, providing promise in a variety
of applications from RNA design to playing games such as Go. Given the
diversity of methods and environments considered in RL, much of the research
has been conducted in distinct subfields, ranging from meta-learning to
evolution. In this survey we seek to unify the field of AutoRL, we provide a
common taxonomy, discuss each area in detail and pose open problems which would
be of interest to researchers going forward.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：An Introduction to Autoencoders</b></summary>
  <p><b>编号</b>：[75]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03898</p>
  <p><b>作者</b>：Umberto Michelucci</p>
  <p><b>备注</b>：26 pages; lecture notes; introductory paper</p>
  <p><b>关键词</b>：typical use cases, typical applications, reconstruction error, paper contains, output layer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this article, we will look at autoencoders. This article covers the
mathematics and the fundamental concepts of autoencoders. We will discuss what
they are, what the limitations are, the typical use cases, and we will look at
some examples. We will start with a general introduction to autoencoders, and
we will discuss the role of the activation function in the output layer and the
loss function. We will then discuss what the reconstruction error is. Finally,
we will look at typical applications as dimensionality reduction,
classification, denoising, and anomaly detection. This paper contains the notes
of a PhD-level lecture on autoencoders given in 2021.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Dictionary Learning with Uniform Sparse Representations for Anomaly  Detection</b></summary>
  <p><b>编号</b>：[82]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03869</p>
  <p><b>作者</b>：Paul Irofti,  Cristian Rusu,  Andrei Pătraşcu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：seeks uniform sparse representations model, many applications like audio, efficient signal modeling technique, smallest approximation error, numerical simulations show</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Many applications like audio and image processing show that sparse
representations are a powerful and efficient signal modeling technique. Finding
an optimal dictionary that generates at the same time the sparsest
representations of data and the smallest approximation error is a hard problem
approached by dictionary learning (DL). We study how DL performs in detecting
abnormal samples in a dataset of signals. In this paper we use a particular DL
formulation that seeks uniform sparse representations model to detect the
underlying subspace of the majority of samples in a dataset, using a K-SVD-type
algorithm. Numerical simulations show that one can efficiently use this
resulted subspace to discriminate the anomalies over the regular data points.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：End-To-End Optimization of LiDAR Beam Configuration for 3D Object  Detection and Localization</b></summary>
  <p><b>编号</b>：[85]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03860</p>
  <p><b>作者</b>：Niclas Vödisch,  Ozan Unal,  Ke Li,  Luc Van Gool,  Dengxin Dai</p>
  <p><b>备注</b>：To appear in: IEEE Robotics and Automation Letters (RA-L)</p>
  <p><b>关键词</b>：based applications use 3d points scanned, 3d object detection, two important tasks, tasks significantly compared, often evenly distributed</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing learning methods for LiDAR-based applications use 3D points scanned
under a pre-determined beam configuration, e.g., the elevation angles of beams
are often evenly distributed. Those fixed configurations are task-agnostic, so
simply using them can lead to sub-optimal performance. In this work, we take a
new route to learn to optimize the LiDAR beam configuration for a given
application. Specifically, we propose a reinforcement learning-based
learning-to-optimize (RL-L2O) framework to automatically optimize the beam
configuration in an end-to-end manner for different LiDAR-based applications.
The optimization is guided by the final performance of the target task and thus
our method can be integrated easily with any LiDAR-based application as a
simple drop-in module. The method is especially useful when a low-resolution
(low-cost) LiDAR is needed, for instance, for system deployment at a massive
scale. We use our method to search for the beam configuration of a
low-resolution LiDAR for two important tasks: 3D object detection and
localization. Experiments show that the proposed RL-L2O method improves the
performance in both tasks significantly compared to the baseline methods. We
believe that a combination of our method with the recent advances of
programmable LiDARs can start a new research direction for LiDAR-based active
perception. The code is publicly available at
this https URL</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：DANNTe: a case study of a turbo-machinery sensor virtualization under  domain shift</b></summary>
  <p><b>编号</b>：[88]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03850</p>
  <p><b>作者</b>：Luca Strazzera,  Valentina Gori,  Giacomo Veneri</p>
  <p><b>备注</b>：5 pages, 3 figures, NeurIPS 2021</p>
  <p><b>关键词</b>：domain classifier neural networks, time series regression task, time series applications, unlabeled target dataset, ganin et al</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose an adversarial learning method to tackle a Domain Adaptation (DA)
time series regression task (DANNTe). The regression aims at building a virtual
copy of a sensor installed on a gas turbine, to be used in place of the
physical sensor which can be missing in certain situations. Our DA approach is
to search for a domain-invariant representation of the features. The learner
has access to both a labelled source dataset and an unlabeled target dataset
(unsupervised DA) and is trained on both, exploiting the minmax game between a
task regressor and a domain classifier Neural Networks. Both models share the
same feature representation, learnt by a feature extractor. This work is based
on the results published by Ganin et al. arXiv:1505.07818; indeed, we present
an extension suitable to time series applications. We report a significant
improvement in regression performance, compared to the baseline model trained
on the source domain only.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Turkish Sentiment Analysis Using Machine Learning Methods: Application  on Online Food Order Site Reviews</b></summary>
  <p><b>编号</b>：[89]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03848</p>
  <p><b>作者</b>：Özlem Aktaş,  Berkay Coşkuner,  İlker Soner</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：3 different natural language processing methods applied resulted, various natural language processing methods used, various machine learning algorithms, every sector today, measuring customer satisfaction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Satisfaction measurement, which emerges in every sector today, is a very
important factor for many companies. In this study, it is aimed to reach the
highest accuracy rate with various machine learning algorithms by using the
data on Yemek Sepeti and variations of this data. The accuracy values of each
algorithm were calculated together with the various natural language processing
methods used. While calculating these accuracy values, the parameters of the
algorithms used were tried to be optimized. The models trained in this study on
labeled data can be used on unlabeled data and can give companies an idea in
measuring customer satisfaction. It was observed that 3 different natural
language processing methods applied resulted in approximately 5% accuracy
increase in most of the developed models.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Reward Relabelling for combined Reinforcement and Imitation Learning on  sparse-reward tasks</b></summary>
  <p><b>编号</b>：[91]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03834</p>
  <p><b>作者</b>：Jesus Bujalance Martin,  Fabien Moutarde</p>
  <p><b>备注</b>：arXiv admin note: substantial text overlap with arXiv:2110.14464</p>
  <p><b>关键词</b>：minimizing additional cost functions, make good use, freedom robotic arm, deep reinforcement learning, reward relabeling improves</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>During recent years, deep reinforcement learning (DRL) has made successful
incursions into complex decision-making applications such as robotics,
autonomous driving or video games. In the search for more sample-efficient
algorithms, a promising direction is to leverage as much external off-policy
data as possible. One staple of this data-driven approach is to learn from
expert demonstrations. In the past, multiple ideas have been proposed to make
good use of the demonstrations added to the replay buffer, such as pretraining
on demonstrations only or minimizing additional cost functions. We present a
new method, able to leverage demonstrations and episodes collected online in
any sparse-reward environment with any off-policy algorithm. Our method is
based on a reward bonus given to demonstrations and successful episodes,
encouraging expert imitation and self-imitation. First, we give a reward bonus
to the transitions coming from demonstrations to encourage the agent to match
the demonstrated behaviour. Then, upon collecting a successful episode, we
relabel its transitions with the same bonus before adding them to the replay
buffer, encouraging the agent to also match its previous successes. Our
experiments focus on manipulation robotics, specifically on three tasks for a 6
degrees-of-freedom robotic arm in simulation. We show that our method based on
reward relabeling improves the performance of the base algorithm (SAC and DDPG)
on these tasks, even in the absence of demonstrations. Furthermore, integrating
into our method two improvements from previous works allows our approach to
outperform all baselines.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Path differentiability of ODE flows</b></summary>
  <p><b>编号</b>：[96]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03819</p>
  <p><b>作者</b>：Swann Marx (LS2N),  Edouard Pauwels (IRIT)</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：small step first order methods based, small step first order methods, sensitivity differential inclusions provide, path differentiable vector fields, path differentiable functions constitute</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider flows of ordinary differential equations (ODEs) driven by path
differentiable vector fields. Path differentiable functions constitute a proper
subclass of Lipschitz functions which admit conservative gradients, a notion of
generalized derivative compatible with basic calculus rules. Our main result
states that such flows inherit the path differentiability property of the
driving vector field. We show indeed that forward propagation of derivatives
given by the sensitivity differential inclusions provide a conservative
Jacobian for the flow. This allows to propose a nonsmooth version of the
adjoint method, which can be applied to integral costs under an ODE constraint.
This result constitutes a theoretical ground to the application of small step
first order methods to solve a broad class of nonsmooth optimization problems
with parametrized ODE constraints. This is illustrated with the convergence of
small step first order methods based on the proposed nonsmooth adjoint.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Bootstrapping Informative Graph Augmentation via A Meta Learning  Approach</b></summary>
  <p><b>编号</b>：[100]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03812</p>
  <p><b>作者</b>：Hang Gao,  Jiangmeng Li,  Wenwen Qiang,  Lingyu Si,  Changwen Zheng,  Fuchun Sun</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：benchmark methods apply various graph augmentation approaches, experiments across multiple benchmark datasets demonstrate, recent works explore learning graph representations, generating unbeneficial augmented graphs, graph contrastive learning methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent works explore learning graph representations in a self-supervised
manner. In graph contrastive learning, benchmark methods apply various graph
augmentation approaches. However, most of the augmentation methods are
non-learnable, which causes the issue of generating unbeneficial augmented
graphs. Such augmentation may degenerate the representation ability of graph
contrastive learning methods. Therefore, we motivate our method to generate
augmented graph by a learnable graph augmenter, called MEta Graph Augmentation
(MEGA). We then clarify that a "good" graph augmentation must have uniformity
at the instance-level and informativeness at the feature-level. To this end, we
propose a novel approach to learning a graph augmenter that can generate an
augmentation with uniformity and informativeness. The objective of the graph
augmenter is to promote our feature extraction network to learn a more
discriminative feature representation, which motivates us to propose a
meta-learning paradigm. Empirically, the experiments across multiple benchmark
datasets demonstrate that MEGA outperforms the state-of-the-art methods in
graph self-supervised learning tasks. Further experimental studies prove the
effectiveness of different terms of MEGA.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Learning what to remember</b></summary>
  <p><b>编号</b>：[104]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03806</p>
  <p><b>作者</b>：Robi Bhattacharjee,  Gaurav Mahajan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：alternative scheme whose regret guarantees, multiplicative weights update algorithm, reflect different policies, online learning framework, mathematical model based</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider a lifelong learning scenario in which a learner faces a
neverending and arbitrary stream of facts and has to decide which ones to
retain in its limited memory. We introduce a mathematical model based on the
online learning framework, in which the learner measures itself against a
collection of experts that are also memory-constrained and that reflect
different policies for what to remember. Interspersed with the stream of facts
are occasional questions, and on each of these the learner incurs a loss if it
has not remembered the corresponding fact. Its goal is to do almost as well as
the best expert in hindsight, while using roughly the same amount of memory. We
identify difficulties with using the multiplicative weights update algorithm in
this memory-constrained scenario, and design an alternative scheme whose regret
guarantees are close to the best possible.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Winning solutions and post-challenge analyses of the ChaLearn AutoDL  challenge 2019</b></summary>
  <p><b>编号</b>：[107]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03801</p>
  <p><b>作者</b>：Zhengying Liu,  Adrien Pavao,  Zhen Xu,  Sergio Escalera,  Fabio Ferreira,  Isabelle Guyon,  Sirui Hong,  Frank Hutter,  Rongrong Ji,  Julio C. S. Jacques Junior,  Ge Li,  Marius Lindauer,  Zhipeng Luo,  Meysam Madadi,  Thomas Nierhoff,  Kangning Niu,  Chunguang Pan,  Danny Stoll,  Sebastien Treguer,  Jin Wang,  Peng Wang,  Chenglin Wu,  Youcheng Xiong,  Arbe r Zela,  Yang Zhang</p>
  <p><b>备注</b>：The first three authors contributed equally; This is only a draft version</p>
  <p><b>关键词</b>：high level modular organization emerged featuring, though popular neural architecture search, modularity enabled ablation studies, architectures matching data modality, input data modalities</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper reports the results and post-challenge analyses of ChaLearn's
AutoDL challenge series, which helped sorting out a profusion of AutoML
solutions for Deep Learning (DL) that had been introduced in a variety of
settings, but lacked fair comparisons. All input data modalities (time series,
images, videos, text, tabular) were formatted as tensors and all tasks were
multi-label classification problems. Code submissions were executed on hidden
tasks, with limited time and computational resources, pushing solutions that
get results quickly. In this setting, DL methods dominated, though popular
Neural Architecture Search (NAS) was impractical. Solutions relied on
fine-tuned pre-trained networks, with architectures matching data modality.
Post-challenge tests did not reveal improvements beyond the imposed time limit.
While no component is particularly original or novel, a high level modular
organization emerged featuring a "meta-learner", "data ingestor", "model
selector", "model/learner", and "evaluator". This modularity enabled ablation
studies, which revealed the importance of (off-platform) meta-learning,
ensembling, and efficient data management. Experiments on heterogeneous module
combinations further confirm the (local) optimality of the winning solutions.
Our challenge legacy includes an ever-lasting benchmark
(this http URL), the open-sourced code of the winners, and a free
"AutoDL self-service".</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Classification of Beer Bottles using Object Detection and Transfer  Learning</b></summary>
  <p><b>编号</b>：[109]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03791</p>
  <p><b>作者</b>：Philipp Hohlfeld,  Tobias Ostermeier,  Dominik Brandl</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：image contains exactly one beer bottle, classic one step transfer learning approach, cnn detects image sections relevant, 5207 beer bottle images, master course deep learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Classification problems are common in Computer Vision. Despite this, there is
no dedicated work for the classification of beer bottles. As part of the
challenge of the master course Deep Learning, a dataset of 5207 beer bottle
images and brand labels was created. An image contains exactly one beer bottle.
In this paper we present a deep learning model which classifies pictures of
beer bottles in a two step approach. As the first step, a Faster-R-CNN detects
image sections relevant for classification independently of the brand. In the
second step, the relevant image sections are classified by a ResNet-18. The
image section with the highest confidence is returned as class label. We
propose a model, with which we surpass the classic one step transfer learning
approach and reached an accuracy of 99.86% during the challenge on the final
test dataset. We were able to achieve 100% accuracy after the challenge ended</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Partial Model Averaging in Federated Learning: Performance Guarantees  and Benefits</b></summary>
  <p><b>编号</b>：[110]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03789</p>
  <p><b>作者</b>：Sunwoo Lee,  Anit Kumar Sahu,  Chaoyang He,  Salman Avestimehr</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：recent advanced optimization methods tackle, local stochastic gradient descent, significant model discrepancy across, model discrepancy issue due, partial model averaging framework</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Local Stochastic Gradient Descent (SGD) with periodic model averaging
(FedAvg) is a foundational algorithm in Federated Learning. The algorithm
independently runs SGD on multiple workers and periodically averages the model
across all the workers. When local SGD runs with many workers, however, the
periodic averaging causes a significant model discrepancy across the workers
making the global loss converge slowly. While recent advanced optimization
methods tackle the issue focused on non-IID settings, there still exists the
model discrepancy issue due to the underlying periodic model averaging. We
propose a partial model averaging framework that mitigates the model
discrepancy issue in Federated Learning. The partial averaging encourages the
local models to stay close to each other on parameter space, and it enables to
more effectively minimize the global loss. Given a fixed number of iterations
and a large number of workers (128), the partial averaging achieves up to 2.2%
higher validation accuracy than the periodic full averaging.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Deep Neural Network Approximation For Hölder Functions</b></summary>
  <p><b>编号</b>：[120]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03747</p>
  <p><b>作者</b>：Ahmed Abdeljawad</p>
  <p><b>备注</b>：22 pages, 2 figures. This is the first version. Some revisions in the near future is expected to be performed</p>
  <p><b>关键词</b>：deep rectified quadratic unit neural networks, theoretical approximation heavily depends, selected activation function, neural network, approximation capability</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we explore the approximation capability of deep Rectified
Quadratic Unit neural networks for Hölder-regular functions, with respect to
the uniform norm. We find that theoretical approximation heavily depends on the
selected activation function in the neural network.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Online Changepoint Detection on a Budget</b></summary>
  <p><b>编号</b>：[134]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03710</p>
  <p><b>作者</b>：Zhaohui Wang,  Xiao Lin,  Abhinav Mishra,  Ram Sriharsha</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：simple online hyperparameter auto tuning technique, case computational complexity per observation, online changepoint detection algorithm, offline changepoint detection algorithms, constrained computational model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Changepoints are abrupt variations in the underlying distribution of data.
Detecting changes in a data stream is an important problem with many
applications. In this paper, we are interested in changepoint detection
algorithms which operate in an online setting in the sense that both its
storage requirements and worst-case computational complexity per observation
are independent of the number of previous observations. We propose an online
changepoint detection algorithm for both univariate and multivariate data which
compares favorably with offline changepoint detection algorithms while also
operating in a strictly more constrained computational model. In addition, we
present a simple online hyperparameter auto tuning technique for these
algorithms.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Pavlovian Signalling with General Value Functions in Agent-Agent  Temporal Decision Making</b></summary>
  <p><b>编号</b>：[135]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03709</p>
  <p><b>作者</b>：Andrew Butcher,  Michael Bradley Johanson,  Elnaz Davoodi,  Dylan J. A. Brenneis,  Leslie Acker,  Adam S. R. Parker,  Adam White,  Joseph Modayil,  Patrick M. Pilarski</p>
  <p><b>备注</b>：9 pages, 7 figures</p>
  <p><b>关键词</b>：constructivist path towards communication learning, determine time since past events, different temporal processes impact coordination, temporally extended predictions made, fully adaptive communication learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we contribute a multi-faceted study into Pavlovian signalling
-- a process by which learned, temporally extended predictions made by one
agent inform decision-making by another agent. Signalling is intimately
connected to time and timing. In service of generating and receiving signals,
humans and other animals are known to represent time, determine time since past
events, predict the time until a future stimulus, and both recognize and
generate patterns that unfold in time. We investigate how different temporal
processes impact coordination and signalling between learning agents by
introducing a partially observable decision-making domain we call the Frost
Hollow. In this domain, a prediction learning agent and a reinforcement
learning agent are coupled into a two-part decision-making system that works to
acquire sparse reward while avoiding time-conditional hazards. We evaluate two
domain variations: machine agents interacting in a seven-state linear walk, and
human-machine interaction in a virtual-reality environment. Our results
showcase the speed of learning for Pavlovian signalling, the impact that
different temporal representations do (and do not) have on agent-agent
coordination, and how temporal aliasing impacts agent-agent and human-agent
interactions differently. As a main contribution, we establish Pavlovian
signalling as a natural bridge between fixed signalling paradigms and fully
adaptive communication learning between two agents. We further show how to
computationally build this adaptive signalling process out of a fixed
signalling process, characterized by fast continual prediction learning and
minimal constraints on the nature of the agent receiving signals. Our results
therefore suggest an actionable, constructivist path towards communication
learning between reinforcement learning agents.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Verified Probabilistic Policies for Deep Reinforcement Learning</b></summary>
  <p><b>编号</b>：[137]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03698</p>
  <p><b>作者</b>：Edoardo Bacci,  David Parker</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：models using abstract interpretation, interval markov decision processes, yields probabilistic guarantees, state dynamical systems, reinforcement learning benchmarks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep reinforcement learning is an increasingly popular technique for
synthesising policies to control an agent's interaction with its environment.
There is also growing interest in formally verifying that such policies are
correct and execute safely. Progress has been made in this area by building on
existing work for verification of deep neural networks and of continuous-state
dynamical systems. In this paper, we tackle the problem of verifying
probabilistic policies for deep reinforcement learning, which are used to, for
example, tackle adversarial environments, break symmetries and manage
trade-offs. We propose an abstraction approach, based on interval Markov
decision processes, that yields probabilistic guarantees on a policy's
execution, and present techniques to build and solve these models using
abstract interpretation, mixed-integer linear programming, entropy-based
refinement and probabilistic model checking. We implement our approach and
illustrate its effectiveness on a selection of reinforcement learning
benchmarks.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Stratified Graph Spectra</b></summary>
  <p><b>编号</b>：[138]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03696</p>
  <p><b>作者</b>：Fanchao Meng,  Mark Orr,  Samarth Swarup</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：art graph learning modeling, classic graph signal processing, graph learning models, graph fourier transform, new tool assisting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In classic graph signal processing, given a real-valued graph signal, its
graph Fourier transform is typically defined as the series of inner products
between the signal and each eigenvector of the graph Laplacian. Unfortunately,
this definition is not mathematically valid in the cases of vector-valued graph
signals which however are typical operands in the state-of-the-art graph
learning modeling and analyses. Seeking a generalized transformation decoding
the magnitudes of eigencomponents from vector-valued signals is thus the main
objective of this paper. Several attempts are explored, and also it is found
that performing the transformation at hierarchical levels of adjacency help
profile the spectral characteristics of signals more insightfully. The proposed
methods are introduced as a new tool assisting on diagnosing and profiling
behaviors of graph learning models.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：NFANet: A Novel Method for Weakly Supervised Water Extraction from  High-Resolution Remote Sensing Imagery</b></summary>
  <p><b>编号</b>：[140]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03686</p>
  <p><b>作者</b>：Ming Lu,  Leyuan Fang,  Muxing Li,  Bob Zhang,  Yi Zhang,  Pedram Ghamisi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：method utilizes neighboring features instead, water extraction requires precise pixel, studied weakly supervised approaches, improved recursive training algorithm, also obtains similar results</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The use of deep learning for water extraction requires precise pixel-level
labels. However, it is very difficult to label high-resolution remote sensing
images at the pixel level. Therefore, we study how to utilize point labels to
extract water bodies and propose a novel method called the neighbor feature
aggregation network (NFANet). Compared with pixellevel labels, point labels are
much easier to obtain, but they will lose much information. In this paper, we
take advantage of the similarity between the adjacent pixels of a local
water-body, and propose a neighbor sampler to resample remote sensing images.
Then, the sampled images are sent to the network for feature aggregation. In
addition, we use an improved recursive training algorithm to further improve
the extraction accuracy, making the water boundary more natural. Furthermore,
our method utilizes neighboring features instead of global or local features to
learn more representative features. The experimental results show that the
proposed NFANet method not only outperforms other studied weakly supervised
approaches, but also obtains similar results as the state-of-the-art ones.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：FairEdit: Preserving Fairness in Graph Neural Networks through Greedy  Graph Editing</b></summary>
  <p><b>编号</b>：[141]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03681</p>
  <p><b>作者</b>：Donald Loveland,  Jiayi Pan,  Aaresh Farrokh Bhathena,  Yiyang Lu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：fairedit performs efficient edge editing, improve fairness across many domains, inherently missing fair connections, fairedit outperforms standard training, perform edge editing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Graph Neural Networks (GNNs) have proven to excel in predictive modeling
tasks where the underlying data is a graph. However, as GNNs are extensively
used in human-centered applications, the issue of fairness has arisen. While
edge deletion is a common method used to promote fairness in GNNs, it fails to
consider when data is inherently missing fair connections. In this work we
consider the unexplored method of edge addition, accompanied by deletion, to
promote fairness. We propose two model-agnostic algorithms to perform edge
editing: a brute force approach and a continuous approximation approach,
FairEdit. FairEdit performs efficient edge editing by leveraging gradient
information of a fairness loss to find edges that improve fairness. We find
that FairEdit outperforms standard training for many data sets and GNN methods,
while performing comparably to many state-of-the-art methods, demonstrating
FairEdit's ability to improve fairness across many domains and models.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：Towards Group Robustness in the presence of Partial Group Labels</b></summary>
  <p><b>编号</b>：[145]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03668</p>
  <p><b>作者</b>：Vishnu Suresh Lokhande,  Kihyuk Sohn,  Jinsung Yoon,  Madeleine Udell,  Chen-Yu Lee,  Tomas Pfister</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：preserving overall aggregate accuracy across groups, contain partially labeled group information, neural network predictions resulting, data collection efforts results, leverage partially available sensitive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Learning invariant representations is an important requirement when training
machine learning models that are driven by spurious correlations in the
datasets. These spurious correlations, between input samples and the target
labels, wrongly direct the neural network predictions resulting in poor
performance on certain groups, especially the minority groups. Robust training
against these spurious correlations requires the knowledge of group membership
for every sample. Such a requirement is impractical in situations where the
data labeling efforts for minority or rare groups are significantly laborious
or where the individuals comprising the dataset choose to conceal sensitive
information. On the other hand, the presence of such data collection efforts
results in datasets that contain partially labeled group information. Recent
works have tackled the fully unsupervised scenario where no labels for groups
are available. Thus, we aim to fill the missing gap in the literature by
tackling a more realistic setting that can leverage partially available
sensitive or group information during training. First, we construct a
constraint set and derive a high probability bound for the group assignment to
belong to the set. Second, we propose an algorithm that optimizes for the
worst-off group assignments from the constraint set. Through experiments on
image and tabular datasets, we show improvements in the minority group's
performance while preserving overall aggregate accuracy across groups.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Learning Fair Node Representations with Graph Counterfactual Fairness</b></summary>
  <p><b>编号</b>：[146]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03662</p>
  <p><b>作者</b>：Jing Ma,  Ruocheng Guo,  Mengting Wan,  Longqi Yang,  Aidong Zhang,  Jundong Li</p>
  <p><b>备注</b>：9 pages, 4 figures</p>
  <p><b>关键词</b>：learn node representations towards graph counterfactual fairness, also achieves comparable prediction performance, certain subpopulations regarding sensitive attributes, sensitive attributes may causally affect, neighbors may causally affect</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Fair machine learning aims to mitigate the biases of model predictions
against certain subpopulations regarding sensitive attributes such as race and
gender. Among the many existing fairness notions, counterfactual fairness
measures the model fairness from a causal perspective by comparing the
predictions of each individual from the original data and the counterfactuals.
In counterfactuals, the sensitive attribute values of this individual had been
modified. Recently, a few works extend counterfactual fairness to graph data,
but most of them neglect the following facts that can lead to biases: 1) the
sensitive attributes of each node's neighbors may causally affect the
prediction w.r.t. this node; 2) the sensitive attributes may causally affect
other features and the graph structure. To tackle these issues, in this paper,
we propose a novel fairness notion - graph counterfactual fairness, which
considers the biases led by the above facts. To learn node representations
towards graph counterfactual fairness, we propose a novel framework based on
counterfactual data augmentation. In this framework, we generate
counterfactuals corresponding to perturbations on each node's and their
neighbors' sensitive attributes. Then we enforce fairness by minimizing the
discrepancy between the representations learned from the original graph and the
counterfactuals for each node. Experiments on both synthetic and real-world
graphs show that our framework outperforms the state-of-the-art baselines in
graph counterfactual fairness, and also achieves comparable prediction
performance.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：A Likelihood Ratio based Domain Adaptation Method for E2E Models</b></summary>
  <p><b>编号</b>：[149]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03655</p>
  <p><b>作者</b>：Chhavi Choudhury,  Ankur Gandhe,  Xiaohan Ding,  Ivan Bulyko</p>
  <p><b>备注</b>：Submitted to ICASSP 2022</p>
  <p><b>关键词</b>：automatic speech recognition models like recurrent neural networks transducer, pass rescoring model gives additive wer improvements, streaming asr applications like voice assistants, contextual biasing approach using likelihood, improving rare words recognition</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>End-to-end (E2E) automatic speech recognition models like Recurrent Neural
Networks Transducer (RNN-T) are becoming a popular choice for streaming ASR
applications like voice assistants. While E2E models are very effective at
learning representation of the training data they are trained on, their
accuracy on unseen domains remains a challenging problem. Additionally, these
models require paired audio and text training data, are computationally
expensive and are difficult to adapt towards the fast evolving nature of
conversational speech. In this work, we explore a contextual biasing approach
using likelihood-ratio that leverages text data sources to adapt RNN-T model to
new domains and entities. We show that this method is effective in improving
rare words recognition, and results in a relative improvement of 10% in 1-best
word error rate (WER) and 10% in n-best Oracle WER (n=8) on multiple
out-of-domain datasets without any degradation on a general dataset. We also
show that complementing the contextual biasing adaptation with adaptation of a
second-pass rescoring model gives additive WER improvements.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：An Accelerator for Rule Induction in Fuzzy Rough Theory</b></summary>
  <p><b>编号</b>：[150]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03649</p>
  <p><b>作者</b>：Suyun Zhao,  Zhigang Dai,  Xizhao Wang,  Peng Ni,  Hengheng Luo,  Hong Chen,  Cuiping Li</p>
  <p><b>备注</b>：15 pages,9 figures</p>
  <p><b>关键词</b>：compacted search space termed key set, key set ensures consistency, key set ensures, rule induction method based, key instances required</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Rule-based classifier, that extract a subset of induced rules to efficiently
learn/mine while preserving the discernibility information, plays a crucial
role in human-explainable artificial intelligence. However, in this era of big
data, rule induction on the whole datasets is computationally intensive. So
far, to the best of our knowledge, no known method focusing on accelerating
rule induction has been reported. This is first study to consider the
acceleration technique to reduce the scale of computation in rule induction. We
propose an accelerator for rule induction based on fuzzy rough theory; the
accelerator can avoid redundant computation and accelerate the building of a
rule classifier. First, a rule induction method based on consistence degree,
called Consistence-based Value Reduction (CVR), is proposed and used as basis
to accelerate. Second, we introduce a compacted search space termed Key Set,
which only contains the key instances required to update the induced rule, to
conduct value reduction. The monotonicity of Key Set ensures the feasibility of
our accelerator. Third, a rule-induction accelerator is designed based on Key
Set, and it is theoretically guaranteed to display the same results as the
unaccelerated version. Specifically, the rank preservation property of Key Set
ensures consistency between the rule induction achieved by the accelerator and
the unaccelerated method. Finally, extensive experiments demonstrate that the
proposed accelerator can perform remarkably faster than the unaccelerated
rule-based classifier methods, especially on datasets with numerous instances.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：Competing Mutual Information Constraints with Stochastic  Competition-based Activations for Learning Diversified Representations</b></summary>
  <p><b>编号</b>：[157]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03624</p>
  <p><b>作者</b>：Konstantinos P. Panousis,  Anastasios Antoniadis,  Sotirios Chatzis</p>
  <p><b>备注</b>：Accepted AAAI-22</p>
  <p><b>关键词</b>：resulting networks yield significant discriminative representation learning abilities, conventional deep architectures commonly used, network layer yields sparse outputs, approach using benchmark datasets, stochastic variational bayes framework</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This work aims to address the long-established problem of learning
diversified representations. To this end, we combine information-theoretic
arguments with stochastic competition-based activations, namely Stochastic
Local Winner-Takes-All (LWTA) units. In this context, we ditch the conventional
deep architectures commonly used in Representation Learning, that rely on
non-linear activations; instead, we replace them with sets of locally and
stochastically competing linear units. In this setting, each network layer
yields sparse outputs, determined by the outcome of the competition between
units that are organized into blocks of competitors. We adopt stochastic
arguments for the competition mechanism, which perform posterior sampling to
determine the winner of each block. We further endow the considered networks
with the ability to infer the sub-part of the network that is essential for
modeling the data at hand; we impose appropriate stick-breaking priors to this
end. To further enrich the information of the emerging representations, we
resort to information-theoretic principles, namely the Information Competing
Process (ICP). Then, all the components are tied together under the stochastic
Variational Bayes framework for inference. We perform a thorough experimental
investigation for our approach using benchmark datasets on image
classification. As we experimentally show, the resulting networks yield
significant discriminative representation learning abilities. In addition, the
introduced paradigm allows for a principled investigation mechanism of the
emerging intermediate network representations.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：SpectraNet: Learned Recognition of Artificial Satellites From High  Contrast Spectroscopic Imagery</b></summary>
  <p><b>编号</b>：[159]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03614</p>
  <p><b>作者</b>：J. Zachary Gazak,  Ian McQuaid,  Ryan Swindle,  Matthew Phelps,  Justin Fletcher</p>
  <p><b>备注</b>：8 pages, 8 figures, 5 tables. Published at WACV 2022</p>
  <p><b>关键词</b>：object identification solution leveraging modified residual convolutional neural networks, effective space traffic management requires positive identification, routine decisions risk expensive space assets, measure classification uncertainties -- critical components, observed data require spatially resolved imagery</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Effective space traffic management requires positive identification of
artificial satellites. Current methods for extracting object identification
from observed data require spatially resolved imagery which limits
identification to objects in low earth orbits. Most artificial satellites,
however, operate in geostationary orbits at distances which prohibit ground
based observatories from resolving spatial information. This paper demonstrates
an object identification solution leveraging modified residual convolutional
neural networks to map distance-invariant spectroscopic data to object
identity. We report classification accuracies exceeding 80% for a simulated
64-class satellite problem--even in the case of satellites undergoing constant,
random re-orientation. An astronomical observing campaign driven by these
results returned accuracies of 72% for a nine-class problem with an average of
100 examples per class, performing as expected from simulation. We demonstrate
the application of variational Bayesian inference by dropout, stochastic weight
averaging (SWA), and SWA-focused deep ensembling to measure classification
uncertainties--critical components in space traffic management where routine
decisions risk expensive space assets and carry geopolitical consequences.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Evaluating Bayesian Model Visualisations</b></summary>
  <p><b>编号</b>：[163]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03604</p>
  <p><b>作者</b>：Sebastian Stein (1),  John H. Williamson (1) ((1) School of Computing Science, University of Glasgow, Scotland, United Kingdom)</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：software framework development progress facilitate, policy decisions ultimately made, explores whether making boxplots, hypothetical outcome plots interactive, software framework implementing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Probabilistic models inform an increasingly broad range of business and
policy decisions ultimately made by people. Recent algorithmic, computational,
and software framework development progress facilitate the proliferation of
Bayesian probabilistic models, which characterise unobserved parameters by
their joint distribution instead of point estimates. While they can empower
decision makers to explore complex queries and to perform what-if-style
conditioning in theory, suitable visualisations and interactive tools are
needed to maximise users' comprehension and rational decision making under
uncertainty. In this paper, propose a protocol for quantitative evaluation of
Bayesian model visualisations and introduce a software framework implementing
this protocol to support standardisation in evaluation practice and facilitate
reproducibility. We illustrate the evaluation and analysis workflow on a user
study that explores whether making Boxplots and Hypothetical Outcome Plots
interactive can increase comprehension or rationality and conclude with design
guidelines for researchers looking to conduct similar studies in the future.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Reproducing BowNet: Learning Representations by Predicting Bags of  Visual Words</b></summary>
  <p><b>编号</b>：[169]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03556</p>
  <p><b>作者</b>：Harry Nguyen,  Stone Yun,  Hisham Mohammad</p>
  <p><b>备注</b>：This is a reproducibility project. Original work is by Gidaris et al. published in CVPR 2020. Pytorch implementation is public on Github</p>
  <p><b>关键词</b>：softmax layer $\ omega (\ cdot )$ trained, convolutional feature extractor $\ phi (\ cdot )$, 100 accuracy improvements reported, $\ phi, deep feature descriptors</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This work aims to reproduce results from the CVPR 2020 paper by Gidaris et
al. Self-supervised learning (SSL) is used to learn feature representations of
an image using an unlabeled dataset. This work proposes to use bag-of-words
(BoW) deep feature descriptors as a self-supervised learning target to learn
robust, deep representations. BowNet is trained to reconstruct the histogram of
visual words (ie. the deep BoW descriptor) of a reference image when presented
a perturbed version of the image as input. Thus, this method aims to learn
perturbation-invariant and context-aware image features that can be useful for
few-shot tasks or supervised downstream tasks. In the paper, the author
describes BowNet as a network consisting of a convolutional feature extractor
$\Phi(\cdot)$ and a Dense-softmax layer $\Omega(\cdot)$ trained to predict BoW
features from images. After BoW training, the features of $\Phi$ are used in
downstream tasks. For this challenge we were trying to build and train a
network that could reproduce the CIFAR-100 accuracy improvements reported in
the original paper. However, we were unsuccessful in reproducing an accuracy
improvement comparable to what the authors mentioned.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Machine learning enabling high-throughput and remote operations at  large-scale user facilities</b></summary>
  <p><b>编号</b>：[170]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03550</p>
  <p><b>作者</b>：Tatiana Konstantinova,  Phillip M. Maffettone,  Bruce Ravel,  Stuart I. Campbell,  Andi M. Barbour,  Daniel Olds</p>
  <p><b>备注</b>：12 pages, 5 figures</p>
  <p><b>关键词</b>：national synchrotron light source ii, thus producing vast amounts, feedback loops via integration, discovering new functional materials, facility general user community</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Imaging, scattering, and spectroscopy are fundamental in understanding and
discovering new functional materials. Contemporary innovations in automation
and experimental techniques have led to these measurements being performed much
faster and with higher resolution, thus producing vast amounts of data for
analysis. These innovations are particularly pronounced at user facilities and
synchrotron light sources. Machine learning (ML) methods are regularly
developed to process and interpret large datasets in real-time with
measurements. However, there remain conceptual barriers to entry for the
facility general user community, whom often lack expertise in ML, and technical
barriers for deploying ML models. Herein, we demonstrate a variety of
archetypal ML models for on-the-fly analysis at multiple beamlines at the
National Synchrotron Light Source II (NSLS-II). We describe these examples
instructively, with a focus on integrating the models into existing
experimental workflows, such that the reader can easily include their own ML
techniques into experiments at NSLS-II or facilities with a common
infrastructure. The framework presented here shows how with little effort,
diverse ML models operate in conjunction with feedback loops via integration
into the existing Bluesky Suite for experimental orchestration and data
management.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Spectrum Surveying: Active Radio Map Estimation with Autonomous UAVs</b></summary>
  <p><b>编号</b>：[171]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04125</p>
  <p><b>作者</b>：Raju Shrestha,  Daniel Romero,  Sundeep Prabhakar Chepuri</p>
  <p><b>备注</b>：30 pages, 10 figures, journal to be submitted to the IEEE on Wireless Communications</p>
  <p><b>关键词</b>：proposed scheme constructs accurate radio maps quickly, radio maps find numerous applications, feature constant complexity per measurement, driven deep learning algorithm, based online bayesian estimator</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Radio maps find numerous applications in wireless communications and mobile
robotics tasks, including resource allocation, interference coordination, and
mission planning. Although numerous techniques have been proposed to construct
radio maps from spatially distributed measurements, the locations of such
measurements are assumed predetermined beforehand. In contrast, this paper
proposes spectrum surveying, where a mobile robot such as an unmanned aerial
vehicle (UAV) collects measurements at a set of locations that are actively
selected to obtain high-quality map estimates in a short surveying time. This
is performed in two steps. First, two novel algorithms, a model-based online
Bayesian estimator and a data-driven deep learning algorithm, are devised for
updating a map estimate and an uncertainty metric that indicates the
informativeness of measurements at each possible location. These algorithms
offer complementary benefits and feature constant complexity per measurement.
Second, the uncertainty metric is used to plan the trajectory of the UAV to
gather measurements at the most informative locations. To overcome the
combinatorial complexity of this problem, a dynamic programming approach is
proposed to obtain lists of waypoints through areas of large uncertainty in
linear time. Numerical experiments conducted on a realistic dataset confirm
that the proposed scheme constructs accurate radio maps quickly.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Systematic Literature Review: Quantum Machine Learning and its  applications</b></summary>
  <p><b>编号</b>：[172]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04093</p>
  <p><b>作者</b>：David Peral García,  Juan Cruz-Benito,  Francisco José García-Peñalvo</p>
  <p><b>备注</b>：28 pages, 25 figures</p>
  <p><b>关键词</b>：existing quantum computers lack enough quality, used quantum machine learning techniques, performing calculations using quantum mechanics, solve problems currently answered, study identified 52 articles</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Quantum computing is the process of performing calculations using quantum
mechanics. This field studies the quantum behavior of certain subatomic
particles for subsequent use in performing calculations, as well as for
large-scale information processing. These capabilities can give quantum
computers an advantage in terms of computational time and cost over classical
computers. Nowadays, there are scientific challenges that are impossible to
perform by classical computation due to computational complexity or the time
the calculation would take, and quantum computation is one of the possible
answers. However, current quantum devices have not yet the necessary qubits and
are not fault-tolerant enough to achieve these goals. Nonetheless, there are
other fields like machine learning or chemistry where quantum computation could
be useful with current quantum devices. This manuscript aims to present a
Systematic Literature Review of the papers published between 2017 and 2021 to
identify, analyze and classify the different algorithms used in quantum machine
learning and their applications. Consequently, this study identified 52
articles that used quantum machine learning techniques and algorithms. The main
types of found algorithms are quantum implementations of classical machine
learning algorithms, such as support vector machines or the k-nearest neighbor
model, and classical deep learning algorithms, like quantum neural networks.
Many articles try to solve problems currently answered by classical machine
learning but using quantum devices and algorithms. Even though results are
promising, quantum machine learning is far from achieving its full potential.
An improvement in the quantum hardware is required since the existing quantum
computers lack enough quality, speed, and scale to allow quantum computing to
achieve its full potential.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Application of Common Spatial Patterns in Gravitational Waves Detection</b></summary>
  <p><b>编号</b>：[174]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04086</p>
  <p><b>作者</b>：Damodar Dahal</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：feature extraction algorithm widely used, gravitational wave transient catalog, detector gravitational wave, time series data, signal processing techniques</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Common Spatial Patterns (CSP) is a feature extraction algorithm widely used
in Brain-Computer Interface (BCI) Systems for detecting Event-Related
Potentials (ERPs) in multi-channel magneto/electroencephalography (MEG/EEG)
time series data. In this article, we develop and apply a CSP algorithm to the
problem of identifying whether a given epoch of multi-detector Gravitational
Wave (GW) strains contains coalescenses. Paired with Signal Processing
techniques and a Logistic Regression classifier, we find that our pipeline is
correctly able to detect 76 out of 82 confident events from Gravitational Wave
Transient Catalog, using H1 and L1 strains, with a classification score of
$93.72 \pm 0.04\%$ using $10 \times 5$ cross validation. The false negative
events were: GW170817-v3, GW191219 163120-v1, GW200115 042309-v2, GW200210
092254-v1, GW200220 061928-v1, and GW200322 091133-v1.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Improving ECG Classification Interpretability using Saliency Maps</b></summary>
  <p><b>编号</b>：[175]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04070</p>
  <p><b>作者</b>：Ms Yola Jones,  Dr Fani Deligianni,  Dr Jeff Dalton</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：using adapted saliency maps averaged across complete classes, comparing saliency maps across complete classes gives, automatic ecg classification systems using machine learning, machine learning algorithms suffer, visualizing model decisions across</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cardiovascular disease is a large worldwide healthcare issue; symptoms often
present suddenly with minimal warning. The electrocardiogram (ECG) is a fast,
simple and reliable method of evaluating the health of the heart, by measuring
electrical activity recorded through electrodes placed on the skin. ECGs often
need to be analyzed by a cardiologist, taking time which could be spent on
improving patient care and outcomes. Because of this, automatic ECG
classification systems using machine learning have been proposed, which can
learn complex interactions between ECG features and use this to detect
abnormalities. However, algorithms built for this purpose often fail to
generalize well to unseen data, reporting initially impressive results which
drop dramatically when applied to new environments. Additionally, machine
learning algorithms suffer a "black-box" issue, in which it is difficult to
determine how a decision has been made. This is vital for applications in
healthcare, as clinicians need to be able to verify the process of evaluation
in order to trust the algorithm. This paper proposes a method for visualizing
model decisions across each class in the MIT-BIH arrhythmia dataset, using
adapted saliency maps averaged across complete classes to determine what
patterns are being learned. We do this by building two algorithms based on
state-of-the-art models. This paper highlights how these maps can be used to
find problems in the model which could be affecting generalizability and model
performance. Comparing saliency maps across complete classes gives an overall
impression of confounding variables or other biases in the model, unlike what
would be highlighted when comparing saliency maps on an ECG-by-ECG basis.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：A novel method for error analysis in radiation thermometry with  application to industrial furnaces</b></summary>
  <p><b>编号</b>：[176]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04069</p>
  <p><b>作者</b>：Iñigo Martinez,  Urtzi Otamendi,  Igor G. Olaizola,  Roger Solsona,  Mikel Maiza,  Elisabeth Viles,  Arturo Fernandez,  Ignacio Arzua</p>
  <p><b>备注</b>：14 pages, 14 figures, 4 tables. Accepted for publication on Measurement journal</p>
  <p><b>关键词</b>：thereby increasing operational security, band radiation thermometry techniques, based measurement correction model, isolated using measurement models, precise industrial furnace monitoring</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Accurate temperature measurements are essential for the proper monitoring and
control of industrial furnaces. However, measurement uncertainty is a risk for
such a critical parameter. Certain instrumental and environmental errors must
be considered when using spectral-band radiation thermometry techniques, such
as the uncertainty in the emissivity of the target surface, reflected radiation
from surrounding objects, or atmospheric absorption and emission, to name a
few. Undesired contributions to measured radiation can be isolated using
measurement models, also known as error-correction models. This paper presents
a methodology for budgeting significant sources of error and uncertainty during
temperature measurements in a petrochemical furnace scenario. A continuous
monitoring system is also presented, aided by a deep-learning-based measurement
correction model, to allow domain experts to analyze the furnace's operation in
real-time. To validate the proposed system's functionality, a real-world
application case in a petrochemical plant is presented. The proposed solution
demonstrates the viability of precise industrial furnace monitoring, thereby
increasing operational security and improving the efficiency of such
energy-intensive systems.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：A hybrid estimation of distribution algorithm for joint stratification  and sample allocation</b></summary>
  <p><b>编号</b>：[177]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04068</p>
  <p><b>作者</b>：Mervyn O'Luing,  Steven Prestwich,  S. Armagan Tarim</p>
  <p><b>备注</b>：32</p>
  <p><b>关键词</b>：sample probability models, continuous strata show, box optimization algorithms, simulated annealing algorithm, simulated annealing algorithm</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this study we propose a hybrid estimation of distribution algorithm (HEDA)
to solve the joint stratification and sample allocation problem. This is a
complex problem in which each the quality of each stratification from the set
of all possible stratifications is measured its optimal sample allocation. EDAs
are stochastic black-box optimization algorithms which can be used to estimate,
build and sample probability models in the search for an optimal
stratification. In this paper we enhance the exploitation properties of the EDA
by adding a simulated annealing algorithm to make it a hybrid EDA. Results of
empirical comparisons for atomic and continuous strata show that the HEDA
attains the bests results found so far when compared to benchmark tests on the
same data using a grouping genetic algorithm, simulated annealing algorithm or
hill-climbing algorithm. However, the execution times and total execution are,
in general, higher for the HEDA.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：ExBrainable: An Open-Source GUI for CNN-based EEG Decoding and Model  Interpretation</b></summary>
  <p><b>编号</b>：[178]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04065</p>
  <p><b>作者</b>：Ya-Lin Huang,  Chia-Ying Hsieh,  Jian-Xue Huang,  Chun-Shu Wei</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：available functions include model training, studied public dataset, investigators across disciplines, convolutional neural networks, gui ), exbrainable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We have developed a graphic user interface (GUI), ExBrainable, dedicated to
convolutional neural networks (CNN) model training and visualization in
electroencephalography (EEG) decoding. Available functions include model
training, evaluation, and parameter visualization in terms of temporal and
spatial representations. We demonstrate these functions using a well-studied
public dataset of motor-imagery EEG and compare the results with existing
knowledge of neuroscience. The primary objective of ExBrainable is to provide a
fast, simplified, and user-friendly solution of EEG decoding for investigators
across disciplines to leverage cutting-edge methods in brain/neuroscience
research.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：PEPit: computer-assisted worst-case analyses of first-order optimization  methods in Python</b></summary>
  <p><b>编号</b>：[179]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04040</p>
  <p><b>作者</b>：Baptiste Goujaud,  Céline Moucer,  François Glineur,  Julien Hendrickx,  Adrien Taylor,  Aymeric Dieuleveut</p>
  <p><b>备注</b>：Reference work for the PEPit package (available at this https URL)</p>
  <p><b>关键词</b>：order optimization methods possibly involving gradient, order optimization methods, order methods nearly, linear optimization oracles, key underlying idea</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>PEPit is a Python package aiming at simplifying the access to worst-case
analyses of a large family of first-order optimization methods possibly
involving gradient, projection, proximal, or linear optimization oracles, along
with their approximate, or Bregman variants.
In short, PEPit is a package enabling computer-assisted worst-case analyses
of first-order optimization methods. The key underlying idea is to cast the
problem of performing a worst-case analysis, often referred to as a performance
estimation problem (PEP), as a semidefinite program (SDP) which can be solved
numerically. For doing that, the package users are only required to write
first-order methods nearly as they would have implemented them. The package
then takes care of the SDP modelling parts, and the worst-case analysis is
performed numerically via a standard solver.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：Image quality measurements and denoising using Fourier Ring Correlations</b></summary>
  <p><b>编号</b>：[180]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03992</p>
  <p><b>作者</b>：J. Kaczmar-Michalska,  N.R. Hajizadeh,  A.J. Rzepiela,  S.F. Nørrelykke</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：humans perceive image similarities, google open images dataset, based loss function allows, loss function based, structural similarity index</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image quality is a nebulous concept with different meanings to different
people. To quantify image quality a relative difference is typically calculated
between a corrupted image and a ground truth image. But what metric should we
use for measuring this difference? Ideally, the metric should perform well for
both natural and scientific images. The structural similarity index (SSIM) is a
good measure for how humans perceive image similarities, but is not sensitive
to differences that are scientifically meaningful in microscopy. In electron
and super-resolution microscopy, the Fourier Ring Correlation (FRC) is often
used, but is little known outside of these fields. Here we show that the FRC
can equally well be applied to natural images, e.g. the Google Open Images
dataset. We then define a loss function based on the FRC, show that it is
analytically differentiable, and use it to train a U-net for denoising of
images. This FRC-based loss function allows the network to train faster and
achieve similar or better results than when using L1- or L2- based losses. We
also investigate the properties and limitations of neural network denoising
with the FRC analysis.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Entropic Optimal Transport in Random Graphs</b></summary>
  <p><b>编号</b>：[185]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03949</p>
  <p><b>作者</b>：Nicolas Keriven</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：called universal singular value thresholding estimator, prove new concentration results, $\ epsilon $- graphs, latent space random graphs, regularized optimal transport</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In graph analysis, a classic task consists in computing similarity measures
between (groups of) nodes. In latent space random graphs, nodes are associated
to unknown latent variables. One may then seek to compute distances directly in
the latent space, using only the graph structure. In this paper, we show that
it is possible to consistently estimate entropic-regularized Optimal Transport
(OT) distances between groups of nodes in the latent space. We provide a
general stability result for entropic OT with respect to perturbations of the
cost matrix. We then apply it to several examples of random graphs, such as
graphons or $\epsilon$-graphs on manifolds. Along the way, we prove new
concentration results for the so-called Universal Singular Value Thresholding
estimator, and for the estimation of geodesic distances on a manifold.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Atomistic Simulations for Reactions and Spectroscopy in the Era of  Machine Learning -- Quo Vadis?</b></summary>
  <p><b>编号</b>：[192]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03822</p>
  <p><b>作者</b>：M. Meuwly</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：atomistic simulations using accurate energy functions, machine learning techniques provides, dynamics simulations closer, discusses open questions, currently pursued efforts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Atomistic simulations using accurate energy functions can provide
molecular-level insight into functional motions of molecules in the gas- and in
the condensed phase. Together with recently developed and currently pursued
efforts in integrating and combining this with machine learning techniques
provides a unique opportunity to bring such dynamics simulations closer to
reality. This perspective delineates the present status of the field from
efforts of others in the field and some of your own work and discusses open
questions and future prospects.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：A Physics-Informed Vector Quantized Autoencoder for Data Compression of  Turbulent Flow</b></summary>
  <p><b>编号</b>：[202]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03617</p>
  <p><b>作者</b>：Mohammadreza Momenifar,  Enmao Diao,  Vahid Tarokh,  Andrew D. Bragg</p>
  <p><b>备注</b>：arXiv admin note: substantial text overlap with arXiv:2103.01074</p>
  <p><b>关键词</b>：informed deep learning technique based, 10 ^{- 3 })$,, offer cr $= 85, lossy data compression scheme, deep learning framework</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Analyzing large-scale data from simulations of turbulent flows is memory
intensive, requiring significant resources. This major challenge highlights the
need for data compression techniques. In this study, we apply a
physics-informed Deep Learning technique based on vector quantization to
generate a discrete, low-dimensional representation of data from simulations of
three-dimensional turbulent flows. The deep learning framework is composed of
convolutional layers and incorporates physical constraints on the flow, such as
preserving incompressibility and global statistical characteristics of the
velocity gradients. The accuracy of the model is assessed using statistical,
comparison-based similarity and physics-based metrics. The training data set is
produced from Direct Numerical Simulation of an incompressible, statistically
stationary, isotropic turbulent flow. The performance of this lossy data
compression scheme is evaluated not only with unseen data from the stationary,
isotropic turbulent flow, but also with data from decaying isotropic
turbulence, and a Taylor-Green vortex flow. Defining the compression ratio (CR)
as the ratio of original data size to the compressed one, the results show that
our model based on vector quantization can offer CR $=85$ with a mean square
error (MSE) of $O(10^{-3})$, and predictions that faithfully reproduce the
statistics of the flow, except at the very smallest scales where there is some
loss. Compared to the recent study based on a conventional autoencoder where
compression is performed in a continuous space, our model improves the CR by
more than $30$ percent, and reduces the MSE by an order of magnitude. Our
compression model is an attractive solution for situations where fast, high
quality and low-overhead encoding and decoding of large data are required.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Iterative RAKI with Complex-Valued Convolution for Improved Image  Reconstruction with Limited Scan-Specific Training Samples</b></summary>
  <p><b>编号</b>：[204]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03560</p>
  <p><b>作者</b>：Peter Dawood,  Martin Blaimer,  Felix Breuer,  Paul R. Burd,  István Homolya,  Peter M. Jakob,  Johannes Oberberger</p>
  <p><b>备注</b>：Submitted to Magnetic Resonance in Medicine</p>
  <p><b>关键词</b>：raki provides superior reconstruction quality compared, 8 %), iraki outperforms raki, includes training data augmentation via, grappa method interpolates missing k, suppressing residual artefacts occurring</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>MRI scan time reduction is commonly achieved by Parallel Imaging methods,
typically based on uniform undersampling of the inverse image space (a.k.a.
k-space) and simultaneous signal reception with multiple receiver coils. The
GRAPPA method interpolates missing k-space signals by linear combination of
adjacent, acquired signals across all coils, and can be described by a
convolution in k-space. Recently, a more generalized method called RAKI was
introduced. RAKI is a deep-learning method that generalizes GRAPPA with
additional convolution layers, on which a non-linear activation function is
applied. This enables non-linear estimation of missing signals by convolutional
neural networks. In analogy to GRAPPA, the convolution kernels in RAKI are
trained using scan-specific training samples obtained from
auto-calibration-signals (ACS). RAKI provides superior reconstruction quality
compared to GRAPPA, however, often requires much more ACS due to its increased
number of unknown parameters. In order to overcome this limitation, this study
investigates the influence of training data on the reconstruction quality for
standard 2D imaging, with particular focus on its amount and contrast
information. Furthermore, an iterative k-space interpolation approach (iRAKI)
is evaluated, which includes training data augmentation via an initial GRAPPA
reconstruction, and refinement of convolution filters by iterative training.
Using only 18, 20 and 25 ACS lines (8%), iRAKI outperforms RAKI by suppressing
residual artefacts occurring at accelerations factors R=4 and R=5, and yields
strong noise suppression in comparison to GRAPPA, underlined by quantitative
quality metrics. Combination with a phase-constraint yields further
improvement. Additionally, iRAKI shows better performance than GRAPPA and RAKI
in case of pre-scan calibration and strongly varying contrast between training-
and undersampled data.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：Demonstrating The Risk of Imbalanced Datasets in Chest X-ray Image-based  Diagnostics by Prototypical Relevance Propagation</b></summary>
  <p><b>编号</b>：[205]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03559</p>
  <p><b>作者</b>：Srishti Gautam,  Marina M.-C. Höhne,  Stine Hansen,  Robert Jenssen,  Michael Kampffmeyer</p>
  <p><b>备注</b>：To appear in ISBI 2022</p>
  <p><b>关键词</b>：improve automated diagnostics raises concerns, balanced source domain datasets, widely used chestx, source chest x, learning spurious correlations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The recent trend of integrating multi-source Chest X-Ray datasets to improve
automated diagnostics raises concerns that models learn to exploit
source-specific correlations to improve performance by recognizing the source
domain of an image rather than the medical pathology. We hypothesize that this
effect is enforced by and leverages label-imbalance across the source domains,
i.e, prevalence of a disease corresponding to a source. Therefore, in this
work, we perform a thorough study of the effect of label-imbalance in
multi-source training for the task of pneumonia detection on the widely used
ChestX-ray14 and CheXpert datasets. The results highlight and stress the
importance of using more faithful and transparent self-explaining models for
automated diagnosis, thus enabling the inherent detection of spurious learning.
They further illustrate that this undesirable effect of learning spurious
correlations can be reduced considerably when ensuring label-balanced source
domain datasets.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：A multi-scale sampling method for accurate and robust deep neural  network to predict combustion chemical kinetics</b></summary>
  <p><b>编号</b>：[206]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03549</p>
  <p><b>作者</b>：Tianhan Zhang,  Yuxiao Yi,  Yifan Xu,  Zhi X. Chen,  Yaoyu Zhang,  Weinan E,  Zhi-Qin John Xu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：scale method without specific flame simulation data, cannot remain robust toward perturbation, understand two basic questions regarding, work compares different sampling methods, current work proposes using box</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine learning has long been considered as a black box for predicting
combustion chemical kinetics due to the extremely large number of parameters
and the lack of evaluation standards and reproducibility. The current work aims
to understand two basic questions regarding the deep neural network (DNN)
method: what data the DNN needs and how general the DNN method can be. Sampling
and preprocessing determine the DNN training dataset, further affect DNN
prediction ability. The current work proposes using Box-Cox transformation
(BCT) to preprocess the combustion data. In addition, this work compares
different sampling methods with or without preprocessing, including the Monte
Carlo method, manifold sampling, generative neural network method (cycle-GAN),
and newly-proposed multi-scale sampling. Our results reveal that the DNN
trained by the manifold data can capture the chemical kinetics in limited
configurations but cannot remain robust toward perturbation, which is
inevitable for the DNN coupled with the flow field. The Monte Carlo and
cycle-GAN samplings can cover a wider phase space but fail to capture
small-scale intermediate species, producing poor prediction results. A
three-hidden-layer DNN, based on the multi-scale method without specific flame
simulation data, allows predicting chemical kinetics in various scenarios and
being stable during the temporal evolutions. This single DNN is readily
implemented with several CFD codes and validated in various combustors,
including (1). zero-dimensional autoignition, (2). one-dimensional freely
propagating flame, (3). two-dimensional jet flame with triple-flame structure,
and (4). three-dimensional turbulent lifted flames. The results demonstrate the
satisfying accuracy and generalization ability of the pre-trained DNN. The
Fortran and Python versions of DNN and example code are attached in the
supplementary for reproducibility.</p>
  </details>
</details>
<h1>人工智能</h1>
<details>
  <summary>1. <b>标题：In Defense of the Unitary Scalarization for Deep Multi-Task Learning</b></summary>
  <p><b>编号</b>：[6]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04122</p>
  <p><b>作者</b>：Vitaly Kurin,  Alessandro De Palma,  Ilya Kostrikov,  Shimon Whiteson,  M. Pawan Kumar</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：task learning research argues, training simply minimizes, theoretical analysis suggesting, reinforcement learning settings, introduce significant memory</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent multi-task learning research argues against unitary scalarization,
where training simply minimizes the sum of the task losses. Several ad-hoc
multi-task optimization algorithms have instead been proposed, inspired by
various hypotheses about what makes multi-task settings difficult. The majority
of these optimizers require per-task gradients, and introduce significant
memory, runtime, and implementation overhead. We present a theoretical analysis
suggesting that many specialized multi-task optimizers can be interpreted as
forms of regularization. Moreover, we show that, when coupled with standard
regularization and stabilization techniques from single-task learning, unitary
scalarization matches or improves upon the performance of complex multi-task
optimizers in both supervised and reinforcement learning settings. We believe
our results call for a critical reevaluation of recent research in the area.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：VGAER: graph neural network reconstruction based community detection</b></summary>
  <p><b>编号</b>：[15]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04066</p>
  <p><b>作者</b>：Chenyang Qiu,  Zhaoci Huang,  Wenzhe Xu,  Huijia Li</p>
  <p><b>备注</b>：Accepted by AAAI-22: DLG-AAAI'22 (this https URL)</p>
  <p><b>关键词</b>：variational graph autoencoder reconstruction based community detection vgaer, carefully designed corresponding input features, community detection algorithms based, powerful network modularity ability, graph neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Community detection is a fundamental and important issue in network science,
but there are only a few community detection algorithms based on graph neural
networks, among which unsupervised algorithms are almost blank. By fusing the
high-order modularity information with network features, this paper proposes a
Variational Graph AutoEncoder Reconstruction based community detection VGAER
for the first time, and gives its non-probabilistic version. They do not need
any prior information. We have carefully designed corresponding input features,
decoder, and downstream tasks based on the community detection task and these
designs are concise, natural, and perform well (NMI values under our design are
improved by 59.1% - 565.9%). Based on a series of experiments with wide range
of datasets and advanced methods, VGAER has achieved superior performance and
shows strong competitiveness and potential with a simpler design. Finally, we
report the results of algorithm convergence analysis and t-SNE visualization,
which clearly depicted the stable performance and powerful network modularity
ability of VGAER. Our codes are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：DDG-DA: Data Distribution Generation for Predictable Concept Drift  Adaptation</b></summary>
  <p><b>编号</b>：[28]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04038</p>
  <p><b>作者</b>：Wendi Li,  Xiao Yang,  Weiqing Liu,  Yingce Xia,  Jiang Bian</p>
  <p><b>备注</b>：Accepted by AAAI'22</p>
  <p><b>关键词</b>：streaming data distribution may change, previous methods first detect, future concept drift trend, stock price trend, obtain significant improvement</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In many real-world scenarios, we often deal with streaming data that is
sequentially collected over time. Due to the non-stationary nature of the
environment, the streaming data distribution may change in unpredictable ways,
which is known as concept drift. To handle concept drift, previous methods
first detect when/where the concept drift happens and then adapt models to fit
the distribution of the latest data. However, there are still many cases that
some underlying factors of environment evolution are predictable, making it
possible to model the future concept drift trend of the streaming data, while
such cases are not fully explored in previous work.
In this paper, we propose a novel method DDG-DA, that can effectively
forecast the evolution of data distribution and improve the performance of
models. Specifically, we first train a predictor to estimate the future data
distribution, then leverage it to generate training samples, and finally train
models on the generated data. We conduct experiments on three real-world tasks
(forecasting on stock price trend, electricity load and solar irradiance) and
obtain significant improvement on multiple widely-used models.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Pyramid Fusion Transformer for Semantic Segmentation</b></summary>
  <p><b>编号</b>：[38]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04019</p>
  <p><b>作者</b>：Zipeng Qin,  Jianbo Liu,  Xiaolin Zhang,  Maoqing Tian,  Aojun Zhou,  Shuai Yi,  Hongsheng Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：different resolutions without incurring, segmentation quality thus relies, efficiently utilize image features, rich semantic information across, based pyramid fusion transformer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The recently proposed MaskFormer \cite{maskformer} gives a refreshed
perspective on the task of semantic segmentation: it shifts from the popular
pixel-level classification paradigm to a mask-level classification method. In
essence, it generates paired probabilities and masks corresponding to category
segments and combines them during inference for the segmentation maps. The
segmentation quality thus relies on how well the queries can capture the
semantic information for categories and their spatial locations within the
images. In our study, we find that per-mask classification decoder on top of a
single-scale feature is not effective enough to extract reliable probability or
mask. To mine for rich semantic information across the feature pyramid, we
propose a transformer-based Pyramid Fusion Transformer (PFT) for per-mask
approach semantic segmentation on top of multi-scale features. To efficiently
utilize image features of different resolutions without incurring too much
computational overheads, PFT uses a multi-scale transformer decoder with
cross-scale inter-query attention to exchange complimentary information.
Extensive experimental evaluations and ablations demonstrate the efficacy of
our framework. In particular, we achieve a 3.2 mIoU improvement on COCO-Stuff
10K dataset with ResNet-101c compared to MaskFormer. Besides, on ADE20K
validation set, our result with Swin-B backbone matches that of MaskFormer's
with a much larger Swin-L backbone in both single-scale and multi-scale
inference, achieving 54.1 mIoU and 55.3 mIoU respectively. Using a Swin-L
backbone, we achieve 56.0 mIoU single-scale result on the ADE20K validation set
and 57.2 multi-scale result, obtaining state-of-the-art performance on the
dataset.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Feature Space Hijacking Attacks against Differentially Private Split  Learning</b></summary>
  <p><b>编号</b>：[39]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04018</p>
  <p><b>作者</b>：Grzegorz Gawron,  Philip Stubbings</p>
  <p><b>备注</b>：To appear at the Third AAAI Workshop on Privacy-Preserving Artificial Intelligence (PPAI-22). (4 pages, short paper.)</p>
  <p><b>关键词</b>：receiving increased research attention recently, recent feature space hijacking attack, arbitrarily set dp epsilon levels, split neural network enhanced, potential attack risk mitigation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Split learning and differential privacy are technologies with growing
potential to help with privacy-compliant advanced analytics on distributed
datasets. Attacks against split learning are an important evaluation tool and
have been receiving increased research attention recently. This work's
contribution is applying a recent feature space hijacking attack (FSHA) to the
learning process of a split neural network enhanced with differential privacy
(DP), using a client-side off-the-shelf DP optimizer. The FSHA attack obtains
client's private data reconstruction with low error rates at arbitrarily set DP
epsilon levels. We also experiment with dimensionality reduction as a potential
attack risk mitigation and show that it might help to some extent. We discuss
the reasons why differential privacy is not an effective protection in this
setting and mention potential other risk mitigation methods.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：The Dataset Nutrition Label (2nd Gen): Leveraging Context to Mitigate  Harms in Artificial Intelligence</b></summary>
  <p><b>编号</b>：[59]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03954</p>
  <p><b>作者</b>：Kasia S. Chmielinski,  Sarah Newman,  Matt Taylor,  Josh Joseph,  Kemi Thomas,  Jessica Yurkofsky,  Yue Chelsea Qiu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：user interface targeted towards, new label includes context, work including new datasets, specific use cases, produce automated decision</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As the production of and reliance on datasets to produce automated
decision-making systems (ADS) increases, so does the need for processes for
evaluating and interrogating the underlying data. After launching the Dataset
Nutrition Label in 2018, the Data Nutrition Project has made significant
updates to the design and purpose of the Label, and is launching an updated
Label in late 2020, which is previewed in this paper. The new Label includes
context-specific Use Cases &Alerts presented through an updated design and user
interface targeted towards the data scientist profile. This paper discusses the
harm and bias from underlying training data that the Label is intended to
mitigate, the current state of the work including new datasets being labeled,
new and existing challenges, and further directions of the work, as well as
Figures previewing the new label.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Feature Extraction Framework based on Contrastive Learning with Adaptive  Positive and Negative Samples</b></summary>
  <p><b>编号</b>：[64]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03942</p>
  <p><b>作者</b>：Hongjie Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：final numerical experiments prove, traditional feature extraction methods, feature extraction framework based, infonce loss based, view feature extraction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this study, we propose a feature extraction framework based on contrastive
learning with adaptive positive and negative samples (CL-FEFA) that is suitable
for unsupervised, supervised, and semi-supervised single-view feature
extraction. CL-FEFA constructs adaptively the positive and negative samples
from the results of feature extraction, which makes it more appropriate and
accurate. Thereafter, the discriminative features are re extracted to according
to InfoNCE loss based on previous positive and negative samples, which will
make the intra-class samples more compact and the inter-class samples more
dispersed. At the same time, using the potential structure information of
subspace samples to dynamically construct positive and negative samples can
make our framework more robust to noisy data. Furthermore, CL-FEFA considers
the mutual information between positive samples, that is, similar samples in
potential structures, which provides theoretical support for its advantages in
feature extraction. The final numerical experiments prove that the proposed
framework has a strong advantage over the traditional feature extraction
methods and contrastive learning methods.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：pymdp: A Python library for active inference in discrete state spaces</b></summary>
  <p><b>编号</b>：[73]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03904</p>
  <p><b>作者</b>：Conor Heins,  Beren Millidge,  Daphne Demekas,  Brennan Klein,  Karl Friston,  Iain Couzin,  Alexander Tschantz</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：source scientific computing languages like python, diversifying applications across scientific disciplines, open source languages like python, advantages like modular design, observable markov decision processes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Active inference is an account of cognition and behavior in complex systems
which brings together action, perception, and learning under the theoretical
mantle of Bayesian inference. Active inference has seen growing applications in
academic research, especially in fields that seek to model human or animal
behavior. While in recent years, some of the code arising from the active
inference literature has been written in open source languages like Python and
Julia, to-date, the most popular software for simulating active inference
agents is the DEM toolbox of SPM, a MATLAB library originally developed for the
statistical analysis and modelling of neuroimaging data. Increasing interest in
active inference, manifested both in terms of sheer number as well as
diversifying applications across scientific disciplines, has thus created a
need for generic, widely-available, and user-friendly code for simulating
active inference in open-source scientific computing languages like Python. The
Python package we present here, pymdp (see
this https URL), represents a significant step in this
direction: namely, we provide the first open-source package for simulating
active inference with partially-observable Markov Decision Processes or POMDPs.
We review the package's structure and explain its advantages like modular
design and customizability, while providing in-text code blocks along the way
to demonstrate how it can be used to build and run active inference processes
with ease. We developed pymdp to increase the accessibility and exposure of the
active inference framework to researchers, engineers, and developers with
diverse disciplinary backgrounds. In the spirit of open-source software, we
also hope that it spurs new innovation, development, and collaboration in the
growing active inference community.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Where Is My Mind (looking at)? Predicting Visual Attention from Brain  Activity</b></summary>
  <p><b>编号</b>：[74]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03902</p>
  <p><b>作者</b>：Victor Delvigne,  Noé Tits,  Luca La Fisca,  Nathan Hubens,  Antoine Maiorca,  Hazem Wannous,  Thierry Dutoit,  Jean-Philippe Vandeborre</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：saliency map representing attention, approaches estimating attention, visual attention estimation, visual attention, visual attention</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Visual attention estimation is an active field of research at the crossroads
of different disciplines: computer vision, artificial intelligence and
medicine. One of the most common approaches to estimate a saliency map
representing attention is based on the observed images. In this paper, we show
that visual attention can be retrieved from EEG acquisition. The results are
comparable to traditional predictions from observed images, which is of great
interest. For this purpose, a set of signals has been recorded and different
models have been developed to study the relationship between visual attention
and brain activity. The results are encouraging and comparable with other
approaches estimating attention with other modalities. The codes and dataset
considered in this paper have been made available at
\url{this https URL} to promote research in the
field.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：An Introduction to Autoencoders</b></summary>
  <p><b>编号</b>：[75]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03898</p>
  <p><b>作者</b>：Umberto Michelucci</p>
  <p><b>备注</b>：26 pages; lecture notes; introductory paper</p>
  <p><b>关键词</b>：typical use cases, typical applications, reconstruction error, paper contains, output layer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this article, we will look at autoencoders. This article covers the
mathematics and the fundamental concepts of autoencoders. We will discuss what
they are, what the limitations are, the typical use cases, and we will look at
some examples. We will start with a general introduction to autoencoders, and
we will discuss the role of the activation function in the output layer and the
loss function. We will then discuss what the reconstruction error is. Finally,
we will look at typical applications as dimensionality reduction,
classification, denoising, and anomaly detection. This paper contains the notes
of a PhD-level lecture on autoencoders given in 2021.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Emotion Estimation from EEG -- A Dual Deep Learning Approach Combined  with Saliency</b></summary>
  <p><b>编号</b>：[77]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03891</p>
  <p><b>作者</b>：Victor Delvigne,  Antoine Facchini,  Hazem Wannous,  Thierry Dutoit,  Laurence Ris,  Jean-Philippe Vandeborre</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：electrical brain activity presented motivating results, four publicly available datasets, achieves similar results, reflects higher stability, physiological knowledge defined</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Emotion estimation is an active field of research that has an important
impact on the interaction between human and computer. Among the different
modality to assess emotion, electroencephalogram (EEG) representing the
electrical brain activity presented motivating results during the last decade.
Emotion estimation from EEG could help in the diagnosis or rehabilitation of
certain diseases. In this paper, we propose a dual method considering the
physiological knowledge defined by specialists combined with novel deep
learning (DL) models initially dedicated to computer vision. The joint learning
has been enhanced with model saliency analysis. To present a global approach,
the model has been evaluated on four publicly available datasets and achieves
similar results to the state-of-theart approaches and outperforming results for
two of the proposed datasets with a lower standard deviation that reflects
higher stability. For sake of reproducibility, the codes and models proposed in
this paper are available at this http URL.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Acquisition and Representation of User Preferences Guided by an Ontology</b></summary>
  <p><b>编号</b>：[94]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03824</p>
  <p><b>作者</b>：Rahma Dandan,  Sylvie Despres,  Karima Sedki</p>
  <p><b>备注</b>：in French, JFO 2016 - 6{\`e}mes Journ{\'e}es Francophones sur les Ontologies, Nov 2016, Bordeaux, France</p>
  <p><b>关键词</b>：domain knowledge represented, domain ontology expressed, food preferences guide, domain ontology, food choices</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Our food preferences guide our food choices and in turn affect our personal
health and our social life. In this paper, we adopt an approach using a domain
ontology expressed in OWL2 to support the acquisition and representation of
preferences in formalism CP-Net. Specifically, we present the construction of
the domain ontology and questionnaire design to acquire and represent the
preferences. The acquisition and representation of preferences are implemented
in the field of university canteen. Our main contribution in this preliminary
work is to acquire preferences and enrich the model preferably with domain
knowledge represented in the ontology.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Bootstrapping Informative Graph Augmentation via A Meta Learning  Approach</b></summary>
  <p><b>编号</b>：[100]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03812</p>
  <p><b>作者</b>：Hang Gao,  Jiangmeng Li,  Wenwen Qiang,  Lingyu Si,  Changwen Zheng,  Fuchun Sun</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：benchmark methods apply various graph augmentation approaches, experiments across multiple benchmark datasets demonstrate, recent works explore learning graph representations, generating unbeneficial augmented graphs, graph contrastive learning methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent works explore learning graph representations in a self-supervised
manner. In graph contrastive learning, benchmark methods apply various graph
augmentation approaches. However, most of the augmentation methods are
non-learnable, which causes the issue of generating unbeneficial augmented
graphs. Such augmentation may degenerate the representation ability of graph
contrastive learning methods. Therefore, we motivate our method to generate
augmented graph by a learnable graph augmenter, called MEta Graph Augmentation
(MEGA). We then clarify that a "good" graph augmentation must have uniformity
at the instance-level and informativeness at the feature-level. To this end, we
propose a novel approach to learning a graph augmenter that can generate an
augmentation with uniformity and informativeness. The objective of the graph
augmenter is to promote our feature extraction network to learn a more
discriminative feature representation, which motivates us to propose a
meta-learning paradigm. Empirically, the experiments across multiple benchmark
datasets demonstrate that MEGA outperforms the state-of-the-art methods in
graph self-supervised learning tasks. Further experimental studies prove the
effectiveness of different terms of MEGA.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Ancestral instrument method for causal inference without a causal graph</b></summary>
  <p><b>编号</b>：[101]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03810</p>
  <p><b>作者</b>：Debo Cheng (1),  Jiuyong Li (1),  Lin Liu (1),  Jiji Zhang (2),  Thuc duy Le (1),  Jixue Liu (1) ((1) STEM, University of South Australia, Adelaide, SA, Australia, (2) Department of Religion and Philosophy, Hong Kong Baptist University, Hong Kong, China)</p>
  <p><b>备注</b>：11 pages, 5 figures</p>
  <p><b>关键词</b>：conditional iv needs complete causal structure knowledge, leveraging maximal ancestral graphs, unbiased causal effect estimation, causal effect estimation, causal effect estimation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Unobserved confounding is the main obstacle to causal effect estimation from
observational data. Instrumental variables (IVs) are widely used for causal
effect estimation when there exist latent confounders. With the standard IV
method, when a given IV is valid, unbiased estimation can be obtained, but the
validity requirement of standard IV is strict and untestable. Conditional IV
has been proposed to relax the requirement of standard IV by conditioning on a
set of observed variables (known as a conditioning set for a conditional IV).
However, the criterion for finding a conditioning set for a conditional IV
needs complete causal structure knowledge or a directed acyclic graph (DAG)
representing the causal relationships of both observed and unobserved
variables. This makes it impossible to discover a conditioning set directly
from data. In this paper, by leveraging maximal ancestral graphs (MAGs) in
causal inference with latent variables, we propose a new type of IV, ancestral
IV in MAG, and develop the theory to support data-driven discovery of the
conditioning set for a given ancestral IV in MAG. Based on the theory, we
develop an algorithm for unbiased causal effect estimation with an ancestral IV
in MAG and observational data. Extensive experiments on synthetic and
real-world datasets have demonstrated the performance of the algorithm in
comparison with existing IV methods.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：CI-AVSR: A Cantonese Audio-Visual Speech Dataset for In-car Command  Recognition</b></summary>
  <p><b>编号</b>：[105]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03804</p>
  <p><b>作者</b>：Wenliang Dai,  Samuel Cahyawijaya,  Tiezheng Yu,  Elham J. Barezi,  Peng Xu,  Cheuk Tung Shadow Yiu,  Rita Frieske,  Holy Lovenia,  Genta Indra Winata,  Qifeng Chen,  Xiaojuan Ma,  Bertram E. Shi,  Pascale Fung</p>
  <p><b>备注</b>：6 pages</p>
  <p><b>关键词</b>：implement two multimodal baselines, 30 native cantonese speakers, dataset 10 times larger, car speech recognition systems, visual speech recognition</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the rise of deep learning and intelligent vehicle, the smart assistant
has become an essential in-car component to facilitate driving and provide
extra functionalities. In-car smart assistants should be able to process
general as well as car-related commands and perform corresponding actions,
which eases driving and improves safety. However, there is a data scarcity
issue for low resource languages, hindering the development of research and
applications. In this paper, we introduce a new dataset, Cantonese In-car
Audio-Visual Speech Recognition (CI-AVSR), for in-car command recognition in
the Cantonese language with both video and audio data. It consists of 4,984
samples (8.3 hours) of 200 in-car commands recorded by 30 native Cantonese
speakers. Furthermore, we augment our dataset using common in-car background
noises to simulate real environments, producing a dataset 10 times larger than
the collected one. We provide detailed statistics of both the clean and the
augmented versions of our dataset. Moreover, we implement two multimodal
baselines to demonstrate the validity of CI-AVSR. Experiment results show that
leveraging the visual signal improves the overall performance of the model.
Although our best model can achieve a considerable quality on the clean test
set, the speech recognition quality on the noisy data is still inferior and
remains as an extremely challenging task for real in-car speech recognition
systems. The dataset and code will be released at
this https URL.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Winning solutions and post-challenge analyses of the ChaLearn AutoDL  challenge 2019</b></summary>
  <p><b>编号</b>：[107]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03801</p>
  <p><b>作者</b>：Zhengying Liu,  Adrien Pavao,  Zhen Xu,  Sergio Escalera,  Fabio Ferreira,  Isabelle Guyon,  Sirui Hong,  Frank Hutter,  Rongrong Ji,  Julio C. S. Jacques Junior,  Ge Li,  Marius Lindauer,  Zhipeng Luo,  Meysam Madadi,  Thomas Nierhoff,  Kangning Niu,  Chunguang Pan,  Danny Stoll,  Sebastien Treguer,  Jin Wang,  Peng Wang,  Chenglin Wu,  Youcheng Xiong,  Arbe r Zela,  Yang Zhang</p>
  <p><b>备注</b>：The first three authors contributed equally; This is only a draft version</p>
  <p><b>关键词</b>：high level modular organization emerged featuring, though popular neural architecture search, modularity enabled ablation studies, architectures matching data modality, input data modalities</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper reports the results and post-challenge analyses of ChaLearn's
AutoDL challenge series, which helped sorting out a profusion of AutoML
solutions for Deep Learning (DL) that had been introduced in a variety of
settings, but lacked fair comparisons. All input data modalities (time series,
images, videos, text, tabular) were formatted as tensors and all tasks were
multi-label classification problems. Code submissions were executed on hidden
tasks, with limited time and computational resources, pushing solutions that
get results quickly. In this setting, DL methods dominated, though popular
Neural Architecture Search (NAS) was impractical. Solutions relied on
fine-tuned pre-trained networks, with architectures matching data modality.
Post-challenge tests did not reveal improvements beyond the imposed time limit.
While no component is particularly original or novel, a high level modular
organization emerged featuring a "meta-learner", "data ingestor", "model
selector", "model/learner", and "evaluator". This modularity enabled ablation
studies, which revealed the importance of (off-platform) meta-learning,
ensembling, and efficient data management. Experiments on heterogeneous module
combinations further confirm the (local) optimality of the winning solutions.
Our challenge legacy includes an ever-lasting benchmark
(this http URL), the open-sourced code of the winners, and a free
"AutoDL self-service".</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Efficient Non-Local Contrastive Attention for Image Super-Resolution</b></summary>
  <p><b>编号</b>：[108]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03794</p>
  <p><b>作者</b>：Bin Xia,  Yucheng Hang,  Yapeng Tian,  Wenming Yang,  Qingmin Liao,  Jie Zhou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：nla gives noisy information large weights, obtains linear computation complexity, leveraging intrinsic feature correlation, extensive experimental results show, consumes quadratic computation resources</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Non-Local Attention (NLA) brings significant improvement for Single Image
Super-Resolution (SISR) by leveraging intrinsic feature correlation in natural
images. However, NLA gives noisy information large weights and consumes
quadratic computation resources with respect to the input size, limiting its
performance and application. In this paper, we propose a novel Efficient
Non-Local Contrastive Attention (ENLCA) to perform long-range visual modeling
and leverage more relevant non-local features. Specifically, ENLCA consists of
two parts, Efficient Non-Local Attention (ENLA) and Sparse Aggregation. ENLA
adopts the kernel method to approximate exponential function and obtains linear
computation complexity. For Sparse Aggregation, we multiply inputs by an
amplification factor to focus on informative features, yet the variance of
approximation increases exponentially. Therefore, contrastive learning is
applied to further separate relevant and irrelevant features. To demonstrate
the effectiveness of ENLCA, we build an architecture called Efficient Non-Local
Contrastive Network (ENLCN) by adding a few of our modules in a simple
backbone. Extensive experimental results show that ENLCN reaches superior
performance over state-of-the-art approaches on both quantitative and
qualitative evaluations.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Improved Neural Distinguishers with (Related-key) Differentials:  Applications in SIMON and SIMECK</b></summary>
  <p><b>编号</b>：[115]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03767</p>
  <p><b>作者</b>：Jinyu Lu,  Guoqiang Liu,  Yunwen Liu,  Bing Sun,  Chao Li,  Li Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：high probability compatible differential characteristics, 64 reach 11 -, 11, nsa block cipher speck32, successfully applied deep learning, 11 -, 14</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In CRYPTO 2019, Gohr made a pioneering attempt, and successfully applied deep
learning to the differential cryptanalysis against NSA block cipher Speck32/64,
achieving higher accuracy than the pure differential distinguishers. By its
very nature, mining effective features in data plays a crucial role in
data-driven deep learning. In this paper, in addition to considering the
integrity of the information from the training data of the ciphertext pair,
domain knowledge about the structure of differential cryptanalysis is also
considered into the training process of deep learning to improve the
performance. Besides, based on the SAT/SMT solvers, we find other high
probability compatible differential characteristics which effectively improve
the performance compared with previous work. We build neural distinguishers
(NDs) and related-key neural distinguishers (RKNDs) against Simon and Simeck.
The ND and RKND for Simon32/64 reach 11-, 11-round with an accuracy of 59.55%
and 97.90%, respectively. For Simon64/128, the ND achieve an accuracy of 60.32%
in 13-round, while it is 95.49% for the RKND. For Simeck32/64, ND and RKND of
11-, 14-round are obtained, reaching an accuracy of 63.32% and 87.06%,
respectively. And we build 17-round ND and 21-round RKND for Simeck64/128 with
an accuracy of 64.24% and 62.96%, respectively. Currently, these are the
longest (related-key) neural distinguishers with higher accuracy for
Simon32/64, Simon64/128, Simeck32/64 and Simeck64/128.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Prior Knowledge Enhances Radiology Report Generation</b></summary>
  <p><b>编号</b>：[117]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03761</p>
  <p><b>作者</b>：Song Wang,  Liyan Tang,  Mingquan Lin,  George Shih,  Ying Ding,  Yifan Peng</p>
  <p><b>备注</b>：10 pages, 4 figures, accepted by AMIA 2022 Informatics Summit</p>
  <p><b>关键词</b>：previous deep learning methods tend, drawn increasing attention recently, radiology report generation aims, accurate radiology report generation, associations among medical findings</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Radiology report generation aims to produce computer-aided diagnoses to
alleviate the workload of radiologists and has drawn increasing attention
recently. However, previous deep learning methods tend to neglect the mutual
influences between medical findings, which can be the bottleneck that limits
the quality of generated reports. In this work, we propose to mine and
represent the associations among medical findings in an informative knowledge
graph and incorporate this prior knowledge with radiology report generation to
help improve the quality of generated reports. Experiment results demonstrate
the superior performance of our proposed method on the IU X-ray dataset with a
ROUGE-L of 0.384$\pm$0.007 and CIDEr of 0.340$\pm$0.011. Compared with previous
works, our model achieves an average of 1.6% improvement (2.0% and 1.5%
improvements in CIDEr and ROUGE-L, respectively). The experiments suggest that
prior knowledge can bring performance gains to accurate radiology report
generation. We will make the code publicly available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Pavlovian Signalling with General Value Functions in Agent-Agent  Temporal Decision Making</b></summary>
  <p><b>编号</b>：[135]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03709</p>
  <p><b>作者</b>：Andrew Butcher,  Michael Bradley Johanson,  Elnaz Davoodi,  Dylan J. A. Brenneis,  Leslie Acker,  Adam S. R. Parker,  Adam White,  Joseph Modayil,  Patrick M. Pilarski</p>
  <p><b>备注</b>：9 pages, 7 figures</p>
  <p><b>关键词</b>：constructivist path towards communication learning, determine time since past events, different temporal processes impact coordination, temporally extended predictions made, fully adaptive communication learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we contribute a multi-faceted study into Pavlovian signalling
-- a process by which learned, temporally extended predictions made by one
agent inform decision-making by another agent. Signalling is intimately
connected to time and timing. In service of generating and receiving signals,
humans and other animals are known to represent time, determine time since past
events, predict the time until a future stimulus, and both recognize and
generate patterns that unfold in time. We investigate how different temporal
processes impact coordination and signalling between learning agents by
introducing a partially observable decision-making domain we call the Frost
Hollow. In this domain, a prediction learning agent and a reinforcement
learning agent are coupled into a two-part decision-making system that works to
acquire sparse reward while avoiding time-conditional hazards. We evaluate two
domain variations: machine agents interacting in a seven-state linear walk, and
human-machine interaction in a virtual-reality environment. Our results
showcase the speed of learning for Pavlovian signalling, the impact that
different temporal representations do (and do not) have on agent-agent
coordination, and how temporal aliasing impacts agent-agent and human-agent
interactions differently. As a main contribution, we establish Pavlovian
signalling as a natural bridge between fixed signalling paradigms and fully
adaptive communication learning between two agents. We further show how to
computationally build this adaptive signalling process out of a fixed
signalling process, characterized by fast continual prediction learning and
minimal constraints on the nature of the agent receiving signals. Our results
therefore suggest an actionable, constructivist path towards communication
learning between reinforcement learning agents.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Learning Logic Programs From Noisy Failures</b></summary>
  <p><b>编号</b>：[136]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03702</p>
  <p><b>作者</b>：John Wahlig</p>
  <p><b>备注</b>：Thesis for MSc in Computer Science</p>
  <p><b>关键词</b>：art ml methods typically produces highly interpretable, avoid allowing noisy training data, hypothesis space wherein failed hypotheses, novel noisy popper ilp system, partially misclassified training data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Inductive Logic Programming (ILP) is a form of machine learning (ML) which in
contrast to many other state of the art ML methods typically produces highly
interpretable and reusable models. However, many ILP systems lack the ability
to naturally learn from any noisy or partially misclassified training data. We
introduce the relaxed learning from failures approach to ILP, a noise handling
modification of the previously introduced learning from failures (LFF) approach
which is incapable of handling noise. We additionally introduce the novel Noisy
Popper ILP system which implements this relaxed approach and is a modification
of the existing Popper system. Like Popper, Noisy Popper takes a
generate-test-constrain loop to search its hypothesis space wherein failed
hypotheses are used to construct hypothesis constraints. These constraints are
used to prune the hypothesis space, making the hypothesis search more
efficient. However, in the relaxed setting, constraints are generated in a more
lax fashion as to avoid allowing noisy training data to lead to hypothesis
constraints which prune optimal hypotheses. Constraints unique to the relaxed
setting are generated via hypothesis comparison. Additional constraints are
generated by weighing the accuracy of hypotheses against their sizes to avoid
overfitting through an application of the minimum description length. We
support this new setting through theoretical proofs as well as experimental
results which suggest that Noisy Popper improves the noise handling
capabilities of Popper but at the cost of overall runtime efficiency.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Verified Probabilistic Policies for Deep Reinforcement Learning</b></summary>
  <p><b>编号</b>：[137]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03698</p>
  <p><b>作者</b>：Edoardo Bacci,  David Parker</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：models using abstract interpretation, interval markov decision processes, yields probabilistic guarantees, state dynamical systems, reinforcement learning benchmarks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep reinforcement learning is an increasingly popular technique for
synthesising policies to control an agent's interaction with its environment.
There is also growing interest in formally verifying that such policies are
correct and execute safely. Progress has been made in this area by building on
existing work for verification of deep neural networks and of continuous-state
dynamical systems. In this paper, we tackle the problem of verifying
probabilistic policies for deep reinforcement learning, which are used to, for
example, tackle adversarial environments, break symmetries and manage
trade-offs. We propose an abstraction approach, based on interval Markov
decision processes, that yields probabilistic guarantees on a policy's
execution, and present techniques to build and solve these models using
abstract interpretation, mixed-integer linear programming, entropy-based
refinement and probabilistic model checking. We implement our approach and
illustrate its effectiveness on a selection of reinforcement learning
benchmarks.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Language-Agnostic Website Embedding and Classification</b></summary>
  <p><b>编号</b>：[143]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03677</p>
  <p><b>作者</b>：Sylvain Lugeon,  Tiziano Piccardi,  Robert West</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：dataset contains 14 website categories aligned across languages, curated curlie dataset aligned across languages, largest multilingual crowdsourced web directory, stable performance across low, efficiently computable features suffices</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Currently, publicly available models for website classification do not offer
an embedding method and have limited support for languages beyond English. We
release a dataset with more than 1M websites in 92 languages with relative
labels collected from Curlie, the largest multilingual crowdsourced Web
directory. The dataset contains 14 website categories aligned across languages.
Alongside it, we introduce Homepage2Vec, a machine-learned pre-trained model
for classifying and embedding websites based on their homepage in a
language-agnostic way. Homepage2Vec, thanks to its feature set (textual
content, metadata tags, and visual attributes) and recent progress in natural
language representation, is language-independent by design and can generate
embeddings representation. We show that Homepage2Vec correctly classifies
websites with a macro-averaged F1-score of 0.90, with stable performance across
low- as well as high-resource languages. Feature analysis shows that a small
subset of efficiently computable features suffices to achieve high performance
even with limited computational resources. We make publicly available the
curated Curlie dataset aligned across languages, the pre-trained Homepage2Vec
model, and libraries.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Towards Group Robustness in the presence of Partial Group Labels</b></summary>
  <p><b>编号</b>：[145]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03668</p>
  <p><b>作者</b>：Vishnu Suresh Lokhande,  Kihyuk Sohn,  Jinsung Yoon,  Madeleine Udell,  Chen-Yu Lee,  Tomas Pfister</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：preserving overall aggregate accuracy across groups, contain partially labeled group information, neural network predictions resulting, data collection efforts results, leverage partially available sensitive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Learning invariant representations is an important requirement when training
machine learning models that are driven by spurious correlations in the
datasets. These spurious correlations, between input samples and the target
labels, wrongly direct the neural network predictions resulting in poor
performance on certain groups, especially the minority groups. Robust training
against these spurious correlations requires the knowledge of group membership
for every sample. Such a requirement is impractical in situations where the
data labeling efforts for minority or rare groups are significantly laborious
or where the individuals comprising the dataset choose to conceal sensitive
information. On the other hand, the presence of such data collection efforts
results in datasets that contain partially labeled group information. Recent
works have tackled the fully unsupervised scenario where no labels for groups
are available. Thus, we aim to fill the missing gap in the literature by
tackling a more realistic setting that can leverage partially available
sensitive or group information during training. First, we construct a
constraint set and derive a high probability bound for the group assignment to
belong to the set. Second, we propose an algorithm that optimizes for the
worst-off group assignments from the constraint set. Through experiments on
image and tabular datasets, we show improvements in the minority group's
performance while preserving overall aggregate accuracy across groups.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Learning Fair Node Representations with Graph Counterfactual Fairness</b></summary>
  <p><b>编号</b>：[146]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03662</p>
  <p><b>作者</b>：Jing Ma,  Ruocheng Guo,  Mengting Wan,  Longqi Yang,  Aidong Zhang,  Jundong Li</p>
  <p><b>备注</b>：9 pages, 4 figures</p>
  <p><b>关键词</b>：learn node representations towards graph counterfactual fairness, also achieves comparable prediction performance, certain subpopulations regarding sensitive attributes, sensitive attributes may causally affect, neighbors may causally affect</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Fair machine learning aims to mitigate the biases of model predictions
against certain subpopulations regarding sensitive attributes such as race and
gender. Among the many existing fairness notions, counterfactual fairness
measures the model fairness from a causal perspective by comparing the
predictions of each individual from the original data and the counterfactuals.
In counterfactuals, the sensitive attribute values of this individual had been
modified. Recently, a few works extend counterfactual fairness to graph data,
but most of them neglect the following facts that can lead to biases: 1) the
sensitive attributes of each node's neighbors may causally affect the
prediction w.r.t. this node; 2) the sensitive attributes may causally affect
other features and the graph structure. To tackle these issues, in this paper,
we propose a novel fairness notion - graph counterfactual fairness, which
considers the biases led by the above facts. To learn node representations
towards graph counterfactual fairness, we propose a novel framework based on
counterfactual data augmentation. In this framework, we generate
counterfactuals corresponding to perturbations on each node's and their
neighbors' sensitive attributes. Then we enforce fairness by minimizing the
discrepancy between the representations learned from the original graph and the
counterfactuals for each node. Experiments on both synthetic and real-world
graphs show that our framework outperforms the state-of-the-art baselines in
graph counterfactual fairness, and also achieves comparable prediction
performance.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：An Accelerator for Rule Induction in Fuzzy Rough Theory</b></summary>
  <p><b>编号</b>：[150]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03649</p>
  <p><b>作者</b>：Suyun Zhao,  Zhigang Dai,  Xizhao Wang,  Peng Ni,  Hengheng Luo,  Hong Chen,  Cuiping Li</p>
  <p><b>备注</b>：15 pages,9 figures</p>
  <p><b>关键词</b>：compacted search space termed key set, key set ensures consistency, key set ensures, rule induction method based, key instances required</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Rule-based classifier, that extract a subset of induced rules to efficiently
learn/mine while preserving the discernibility information, plays a crucial
role in human-explainable artificial intelligence. However, in this era of big
data, rule induction on the whole datasets is computationally intensive. So
far, to the best of our knowledge, no known method focusing on accelerating
rule induction has been reported. This is first study to consider the
acceleration technique to reduce the scale of computation in rule induction. We
propose an accelerator for rule induction based on fuzzy rough theory; the
accelerator can avoid redundant computation and accelerate the building of a
rule classifier. First, a rule induction method based on consistence degree,
called Consistence-based Value Reduction (CVR), is proposed and used as basis
to accelerate. Second, we introduce a compacted search space termed Key Set,
which only contains the key instances required to update the induced rule, to
conduct value reduction. The monotonicity of Key Set ensures the feasibility of
our accelerator. Third, a rule-induction accelerator is designed based on Key
Set, and it is theoretically guaranteed to display the same results as the
unaccelerated version. Specifically, the rank preservation property of Key Set
ensures consistency between the rule induction achieved by the accelerator and
the unaccelerated method. Finally, extensive experiments demonstrate that the
proposed accelerator can perform remarkably faster than the unaccelerated
rule-based classifier methods, especially on datasets with numerous instances.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：CausalKG: Causal Knowledge Graph Explainability using interventional and  counterfactual reasoning</b></summary>
  <p><b>编号</b>：[152]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03647</p>
  <p><b>作者</b>：Utkarshani Jaimini,  Amit Sheth</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：existing kgs represent causal relationships extracted, complex causal relations using, proposed causal knowledge graph, domain adaptable causal model, relational graph representation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Humans use causality and hypothetical retrospection in their daily
decision-making, planning, and understanding of life events. The human mind,
while retrospecting a given situation, think about questions such as "What was
the cause of the given situation?", "What would be the effect of my action?",
or "Which action led to this effect?". It develops a causal model of the world,
which learns with fewer data points, makes inferences, and contemplates
counterfactual scenarios. The unseen, unknown, scenarios are known as
counterfactuals. AI algorithms use a representation based on knowledge graphs
(KG) to represent the concepts of time, space, and facts. A KG is a graphical
data model which captures the semantic relationships between entities such as
events, objects, or concepts. The existing KGs represent causal relationships
extracted from texts based on linguistic patterns of noun phrases for causes
and effects as in ConceptNet and WordNet. The current causality representation
in KGs makes it challenging to support counterfactual reasoning. A richer
representation of causality in AI systems using a KG-based approach is needed
for better explainability, and support for intervention and counterfactuals
reasoning, leading to improved understanding of AI systems by humans. The
causality representation requires a higher representation framework to define
the context, the causal information, and the causal effects. The proposed
Causal Knowledge Graph (CausalKG) framework, leverages recent progress of
causality and KG towards explainability. CausalKG intends to address the lack
of a domain adaptable causal model and represent the complex causal relations
using the hyper-relational graph representation in the KG. We show that the
CausalKG's interventional and counterfactual reasoning can be used by the AI
system for the domain explainability.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Resource recommender system performance improvement by exploring similar  tags and detecting tags communities</b></summary>
  <p><b>编号</b>：[158]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03622</p>
  <p><b>作者</b>：Zeinab Shokrzadeh,  Mohammad-Reza Feizi-Derakhshi,  Mohammad-Ali Balafar,  Jamshid Bagherzadeh Mohasefi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：done using two criteria, using community detection methods, evaluation results show, used tag information, many data sets</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Many researchers have used tag information to improve the performance of
recommendation techniques in recommender systems. Examining the tags of users
will help to get their interests and leads to more accuracy in the
recommendations. Since user-defined tags are chosen freely and without any
restrictions, problems arise in determining their exact meaning and the
similarity of tags. On the other hand, using thesauruses and ontologies to find
the meaning of tags is not very efficient due to their free definition by users
and the use of different languages in many data sets. Therefore, this article
uses the mathematical and statistical methods to determine lexical similarity
and co-occurrence tags solution to assign semantic similarity. On the other
hand, due to the change of users' interests over time this article have
considered the time of tag assignments in co-occurrence tags for determined
similarity of tags. Then the graph is created based on these similarities. For
modeling the interests of the users, the communities of tags are determined by
using community detection methods. So recommendations based on the communities
of tags and similarity between resources are done. The performance of the
proposed method has been done using two criteria of precision and recall based
on evaluations with "Delicious" dataset. The evaluation results show that, the
precision and recall of the proposed method have significantly improved,
compared to the other methods.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：A novel method for error analysis in radiation thermometry with  application to industrial furnaces</b></summary>
  <p><b>编号</b>：[176]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.04069</p>
  <p><b>作者</b>：Iñigo Martinez,  Urtzi Otamendi,  Igor G. Olaizola,  Roger Solsona,  Mikel Maiza,  Elisabeth Viles,  Arturo Fernandez,  Ignacio Arzua</p>
  <p><b>备注</b>：14 pages, 14 figures, 4 tables. Accepted for publication on Measurement journal</p>
  <p><b>关键词</b>：thereby increasing operational security, band radiation thermometry techniques, based measurement correction model, isolated using measurement models, precise industrial furnace monitoring</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Accurate temperature measurements are essential for the proper monitoring and
control of industrial furnaces. However, measurement uncertainty is a risk for
such a critical parameter. Certain instrumental and environmental errors must
be considered when using spectral-band radiation thermometry techniques, such
as the uncertainty in the emissivity of the target surface, reflected radiation
from surrounding objects, or atmospheric absorption and emission, to name a
few. Undesired contributions to measured radiation can be isolated using
measurement models, also known as error-correction models. This paper presents
a methodology for budgeting significant sources of error and uncertainty during
temperature measurements in a petrochemical furnace scenario. A continuous
monitoring system is also presented, aided by a deep-learning-based measurement
correction model, to allow domain experts to analyze the furnace's operation in
real-time. To validate the proposed system's functionality, a real-world
application case in a petrochemical plant is presented. The proposed solution
demonstrates the viability of precise industrial furnace monitoring, thereby
increasing operational security and improving the efficiency of such
energy-intensive systems.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Quantum activation functions for quantum neural networks</b></summary>
  <p><b>编号</b>：[198]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2201.03700</p>
  <p><b>作者</b>：Marco Maronese,  Claudio Destri,  Enrico Prati</p>
  <p><b>备注</b>：28 pages, 4 figures</p>
  <p><b>关键词</b>：unlike previous proposals providing irreversible measurement -- based, forward neural network may acquire, forward neural networks consists, creating trainable neural networks, universal approximation properties according</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The field of artificial neural networks is expected to strongly benefit from
recent developments of quantum computers. In particular, quantum machine
learning, a class of quantum algorithms which exploit qubits for creating
trainable neural networks, will provide more power to solve problems such as
pattern recognition, clustering and machine learning in general. The building
block of feed-forward neural networks consists of one layer of neurons
connected to an output neuron that is activated according to an arbitrary
activation function. The corresponding learning algorithm goes under the name
of Rosenblatt perceptron. Quantum perceptrons with specific activation
functions are known, but a general method to realize arbitrary activation
functions on a quantum computer is still lacking. Here we fill this gap with a
quantum algorithm which is capable to approximate any analytic activation
functions to any given order of its power series. Unlike previous proposals
providing irreversible measurement--based and simplified activation functions,
here we show how to approximate any analytic function to any required accuracy
without the need to measure the states encoding the information. Thanks to the
generality of this construction, any feed-forward neural network may acquire
the universal approximation properties according to Hornik's theorem. Our
results recast the science of artificial neural networks in the architecture of
gate-model quantum computers.</p>
  </details>
</details>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">徐耀彬</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://louishsu.xyz/2022/01/13/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">http://louishsu.xyz/2022/01/13/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://louishsu.xyz" target="_blank">LOUIS' BLOG</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2021/10/22/%E4%B8%AD%E5%9B%BD%E6%B3%95%E5%BE%8B%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E8%AF%84%E6%B5%8B(CAIL2021)%EF%BC%9A%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96(Rank2).html"><img class="next-cover" src="http://cail.cipsc.org.cn/img/index_mainpic.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">徐耀彬</div><div class="author-info__description">做知识的原创者！</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">11</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/isLouisHsu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/isLouisHsu" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:is.louishsu@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">记录和分享一些学习和开源内容，若有问题可通过邮箱is.louishsu@foxmail.com联系，欢迎交流！！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">统计</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">计算机视觉</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">自然语言处理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">机器学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">人工智能</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/01/13/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2022-01-13)"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv每日速递(2022-01-13)"/></a><div class="content"><a class="title" href="/2022/01/13/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2022-01-13)">Arxiv每日速递(2022-01-13)</a><time datetime="2022-01-13T00:29:47.465Z" title="发表于 2022-01-13 08:29:47">2022-01-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/10/22/%E4%B8%AD%E5%9B%BD%E6%B3%95%E5%BE%8B%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E8%AF%84%E6%B5%8B(CAIL2021)%EF%BC%9A%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96(Rank2).html" title="中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)"><img src="http://cail.cipsc.org.cn/img/index_mainpic.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)"/></a><div class="content"><a class="title" href="/2021/10/22/%E4%B8%AD%E5%9B%BD%E6%B3%95%E5%BE%8B%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E8%AF%84%E6%B5%8B(CAIL2021)%EF%BC%9A%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96(Rank2).html" title="中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)">中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)</a><time datetime="2021-10-22T14:29:06.000Z" title="发表于 2021-10-22 22:29:06">2021-10-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/05/19/%E5%85%A8%E7%90%83%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0%E5%A4%A7%E8%B5%9B%E3%80%90%E8%B5%9B%E9%81%93%E4%B8%80%E3%80%91%EF%BC%9A%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E6%8A%A5%E5%91%8A%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B.html" title="全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测"><img src="https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161037709574435991610377095138.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测"/></a><div class="content"><a class="title" href="/2021/05/19/%E5%85%A8%E7%90%83%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0%E5%A4%A7%E8%B5%9B%E3%80%90%E8%B5%9B%E9%81%93%E4%B8%80%E3%80%91%EF%BC%9A%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E6%8A%A5%E5%91%8A%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B.html" title="全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测">全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测</a><time datetime="2021-05-19T09:57:06.000Z" title="发表于 2021-05-19 17:57:06">2021-05-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/09/16/%E8%AF%A6%E8%A7%A3%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%EF%BC%9ALSTM-CRF.html" title="详解命名实体识别模型：LSTM-CRF"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fpic1.zhimg.com%2Fv2-1ed6a10dbb239a148e42df088c5870a6_1440w.jpg%3Fsource%3D172ae18b&amp;refer=http%3A%2F%2Fpic1.zhimg.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1638183974&amp;t=4632f13fd487ed1cfcb12d67a6b71e52" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="详解命名实体识别模型：LSTM-CRF"/></a><div class="content"><a class="title" href="/2020/09/16/%E8%AF%A6%E8%A7%A3%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%EF%BC%9ALSTM-CRF.html" title="详解命名实体识别模型：LSTM-CRF">详解命名实体识别模型：LSTM-CRF</a><time datetime="2020-09-16T09:26:44.000Z" title="发表于 2020-09-16 17:26:44">2020-09-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/06/14/%E8%AE%B0%E4%B8%80%E6%AC%A1MySQL%E8%A7%A3%E5%86%B3%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E6%97%B6%E7%9A%84%E5%88%86%E8%A1%A8%E9%97%AE%E9%A2%98.html" title="记一次MySQL解决大数据存储时的分表问题"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="记一次MySQL解决大数据存储时的分表问题"/></a><div class="content"><a class="title" href="/2020/06/14/%E8%AE%B0%E4%B8%80%E6%AC%A1MySQL%E8%A7%A3%E5%86%B3%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E6%97%B6%E7%9A%84%E5%88%86%E8%A1%A8%E9%97%AE%E9%A2%98.html" title="记一次MySQL解决大数据存储时的分表问题">记一次MySQL解决大数据存储时的分表问题</a><time datetime="2020-06-14T15:24:27.000Z" title="发表于 2020-06-14 23:24:27">2020-06-14</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2022 By 徐耀彬</div><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender"title="主题采用butterfly"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr"title="本站使用JsDelivr为静态资源提供CDN加速"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub"title="本站项目由Gtihub托管"alt="img"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"alt="img"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></br></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    let initData = {
      el: '#twikoo-wrap',
      envId: 'blog-',
      region: ''
    }

    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    twikoo.init(initData)
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'blog-',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/机器学习/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐱 机器学习 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/自然语言处理/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 自然语言处理 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/竞赛相关/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 竞赛相关 (2)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/阅读笔记/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 阅读笔记 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="http://louishsu.xyz/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style>
  <script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/hexo-github-calendar@1.21/hexo_githubcalendar.js"></script>
  <script data-pjax>
        function GithubCalendarConfig(){
            var git_githubapiurl ="https://python-github-calendar-api.vercel.app/api?isLouisHsu";
            var git_color =['#ebedf0', '#fdcdec', '#fc9bd9', '#fa6ac5', '#f838b2', '#f5089f', '#c4067e', '#92055e', '#540336', '#48022f', '#30021f'];
            var git_user ="isLouisHsu";
            var parent_div_git = document.getElementById('recent-posts');
            var git_div_html = '<div class="recent-post-item" style="width:100%;height:auto;padding:10px;"><div id="github_loading" style="width:10%;height:100%;margin:0 auto;display: block"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"  viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animateTransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animateTransform></path></svg></div><div id="github_container"></div></div>';
            if(parent_div_git && location.pathname =='/'){
                console.log('已挂载github calendar')
                // parent_div_git.innerHTML=git_div_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",git_div_html) // 有报错，但不影响使用(支持pjax跳转)
            };
            GithubCalendar(git_githubapiurl,git_color,git_user)
        }
        if(document.getElementById('recent-posts')){
            GithubCalendarConfig()
        }
    </script>
    <style>#github_container{min-height:280px}@media screen and (max-width:650px) {#github_container{background-image:;min-height:0px}}</style>
    <style></style><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>