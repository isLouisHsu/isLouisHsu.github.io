<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Arxiv每日速递(2022-04-06) | LOUIS' BLOG</title><meta name="author" content="徐耀彬"><meta name="copyright" content="徐耀彬"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。 统计 今日共更新484篇论文，其中：  130篇计算机视觉（cs.CV） 46篇自然语言处理（cs.CL） 142篇机器学习（cs.LG） 64篇人工智能（cs.AI）  计算机视觉    1. 标题：MaxViT: Multi-Axis Vision Transforme">
<meta property="og:type" content="article">
<meta property="og:title" content="Arxiv每日速递(2022-04-06)">
<meta property="og:url" content="http://louishsu.xyz/2022/04/06/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">
<meta property="og:site_name" content="LOUIS&#39; BLOG">
<meta property="og:description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。 统计 今日共更新484篇论文，其中：  130篇计算机视觉（cs.CV） 46篇自然语言处理（cs.CL） 142篇机器学习（cs.LG） 64篇人工智能（cs.AI）  计算机视觉    1. 标题：MaxViT: Multi-Axis Vision Transforme">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png">
<meta property="article:published_time" content="2022-04-06T00:41:11.786Z">
<meta property="article:modified_time" content="2022-04-06T00:42:48.171Z">
<meta property="article:author" content="徐耀彬">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://louishsu.xyz/2022/04/06/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-04-06 08:42:48'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/background.css"><script src="https://cdn.jsdelivr.net/npm/echarts@4.7.0/dist/echarts.min.js"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.1"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">11</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">LOUIS' BLOG</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Arxiv每日速递(2022-04-06)<a class="post-edit-link" href="https://github.com/isLouisHsu/blog/tree/master/source_posts/Arxiv每日速递.md" title="编辑" target="_blank"><i class="fas fa-pencil-square"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-04-06T00:41:11.786Z" title="发表于 2022-04-06 08:41:11">2022-04-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-04-06T00:42:48.171Z" title="更新于 2022-04-06 08:42:48">2022-04-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">阅读笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">98.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>589分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2022/04/06/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。</p>
<h1>统计</h1>
<p>今日共更新484篇论文，其中：</p>
<ul>
<li>130篇计算机视觉（<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>）</li>
<li>46篇自然语言处理（<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>）</li>
<li>142篇机器学习（cs.LG）</li>
<li>64篇人工智能（<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>）</li>
</ul>
<h1>计算机视觉</h1>
<details>
  <summary>1. <b>标题：MaxViT: Multi-Axis Vision Transformer</b></summary>
  <p><b>编号</b>：[1]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01697</p>
  <p><b>作者</b>：Zhengzhong Tu,  Hossein Talebi,  Han Zhang,  Feng Yang,  Peyman Milanfar,  Alan Bovik,  Yinxiao Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：proposed model expresses strong generative modeling capability, design choices allow global, simple hierarchical vision backbone, backbone delivers favorable performance, recently gained significant attention</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transformers have recently gained significant attention in the computer
vision community. However, the lack of scalability of self-attention mechanisms
with respect to image size has limited their wide adoption in state-of-the-art
vision backbones. In this paper we introduce an efficient and scalable
attention model we call multi-axis attention, which consists of two aspects:
blocked local and dilated global attention. These design choices allow
global-local spatial interactions on arbitrary input resolutions with only
linear complexity. We also present a new architectural element by effectively
blending our proposed attention model with convolutions, and accordingly
propose a simple hierarchical vision backbone, dubbed MaxViT, by simply
repeating the basic building block over multiple stages. Notably, MaxViT is
able to "see" globally throughout the entire network, even in earlier,
high-resolution stages. We demonstrate the effectiveness of our model on a
broad spectrum of vision tasks. On image classification, MaxViT achieves
state-of-the-art performance under various settings: without extra data, MaxViT
attains 86.5\% ImageNet-1K top-1 accuracy; with ImageNet-21K pre-training, our
model achieves 88.7\% top-1 accuracy. For downstream tasks, MaxViT as a
backbone delivers favorable performance on object detection as well as visual
aesthetic assessment. We also show that our proposed model expresses strong
generative modeling capability on ImageNet, demonstrating the superior
potential of MaxViT blocks as a universal vision module. We will make the code
and models publicly available.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Joint Hand Motion and Interaction Hotspots Prediction from Egocentric  Videos</b></summary>
  <p><b>编号</b>：[2]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01696</p>
  <p><b>作者</b>：Shaowei Liu,  Subarna Tripathi,  Somdeb Majumdar,  Xiaolong Wang</p>
  <p><b>备注</b>：CVPR 2022, Project page: this https URL</p>
  <p><b>关键词</b>：e ., interaction hotspots )., object interaction reasoning via, oct significantly outperforms state, dimensional representation provides, next active object</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose to forecast future hand-object interactions given an egocentric
video. Instead of predicting action labels or pixels, we directly predict the
hand motion trajectory and the future contact points on the next active object
(i.e., interaction hotspots). This relatively low-dimensional representation
provides a concrete description of future interactions. To tackle this task, we
first provide an automatic way to collect trajectory and hotspots labels on
large-scale data. We then use this data to train an Object-Centric Transformer
(OCT) model for prediction. Our model performs hand and object interaction
reasoning via the self-attention mechanism in Transformers. OCT also provides a
probabilistic framework to sample the future trajectory and hotspots to handle
uncertainty in prediction. We perform experiments on the Epic-Kitchens-55,
Epic-Kitchens-100, and EGTEA Gaze+ datasets, and show that OCT significantly
outperforms state-of-the-art approaches by a large margin. Project page is
available at this https URL .</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：LISA: Learning Implicit Shape and Appearance of Hands</b></summary>
  <p><b>编号</b>：[3]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01695</p>
  <p><b>作者</b>：Enric Corona,  Tomas Hodan,  Minh Vo,  Francesc Moreno-Noguer,  Chris Sweeney,  Richard Newcombe,  Lingni Ma</p>
  <p><b>备注</b>：Published at CVPR 2022</p>
  <p><b>关键词</b>：bone predictions using predicted skinning weights, view rgb image sequences annotated, provide dense surface correspondences, reconstructed hand shapes compared, capture accurate hand shape</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper proposes a do-it-all neural model of human hands, named LISA. The
model can capture accurate hand shape and appearance, generalize to arbitrary
hand subjects, provide dense surface correspondences, be reconstructed from
images in the wild and easily animated. We train LISA by minimizing the shape
and appearance losses on a large set of multi-view RGB image sequences
annotated with coarse 3D poses of the hand skeleton. For a 3D point in the hand
local coordinate, our model predicts the color and the signed distance with
respect to each hand bone independently, and then combines the per-bone
predictions using predicted skinning weights. The shape, color and pose
representations are disentangled by design, allowing to estimate or animate
only selected parameters. We experimentally demonstrate that LISA can
accurately reconstruct a dynamic hand from monocular or multi-view sequences,
achieving a noticeably higher quality of reconstructed hand shapes compared to
baseline approaches. Project page:
this https URL.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题："This is my unicorn, Fluffy": Personalizing frozen vision-language  representations</b></summary>
  <p><b>编号</b>：[4]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01694</p>
  <p><b>作者</b>：Niv Cohen,  Rinon Gal,  Eli A. Meirom,  Gal Chechik,  Yuval Atzmon</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：semantic segmentation using rich textual queries, new learning setup called personalized vision, approach learns personalized visual concepts, two new benchmark datasets, scale data provide representations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Vision & Language models pretrained on web-scale data provide
representations that are invaluable for numerous V&L problems. However, it is
unclear how they can be used for reasoning about user-specific visual concepts
in unstructured language. This problem arises in multiple domains, from
personalized image retrieval to personalized interaction with smart devices. We
introduce a new learning setup called Personalized Vision & Language (PerVL)
with two new benchmark datasets for retrieving and segmenting user-specific
"personalized" concepts "in the wild". In PerVL, one should learn personalized
concepts (1) independently of the downstream task (2) allowing a pretrained
model to reason about them with free language, and (3) does not require
personalized negative examples. We propose an architecture for solving PerVL
that operates by extending the input vocabulary of a pretrained model with new
word embeddings for the new personalized concepts. The model can then reason
about them by simply using them in a sentence. We demonstrate that our approach
learns personalized visual concepts from a few examples and can effectively
apply them in image retrieval and semantic segmentation using rich textual
queries.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Monitoring social distancing with single image depth estimation</b></summary>
  <p><b>编号</b>：[5]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01693</p>
  <p><b>作者</b>：Alessio Mingozzi,  Andrea Conti,  Filippo Aleotti,  Matteo Poggi,  Stefano Mattoccia</p>
  <p><b>备注</b>：Accepted for pubblication on IEEE Transactions on Emerging Topics in Computational Intelligence (TETCI)</p>
  <p><b>关键词</b>：recent pandemic emergency raised many challenges regarding, single rgb frame without additional depth sensors, scale ambiguity affecting single image depth estimation, appropriately driven single image depth estimation, called social distance gained much interest</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The recent pandemic emergency raised many challenges regarding the
countermeasures aimed at containing the virus spread, and constraining the
minimum distance between people resulted in one of the most effective
strategies. Thus, the implementation of autonomous systems capable of
monitoring the so-called social distance gained much interest. In this paper,
we aim to address this task leveraging a single RGB frame without additional
depth sensors. In contrast to existing single-image alternatives failing when
ground localization is not available, we rely on single image depth estimation
to perceive the 3D structure of the observed scene and estimate the distance
between people. During the setup phase, a straightforward calibration
procedure, leveraging a scale-aware SLAM algorithm available even on consumer
smartphones, allows us to address the scale ambiguity affecting single image
depth estimation. We validate our approach through indoor and outdoor images
employing a calibrated LiDAR + RGB camera asset. Experimental results highlight
that our proposal enables sufficiently reliable estimation of the
inter-personal distance to monitor social distancing effectively. This fact
confirms that despite its intrinsic ambiguity, if appropriately driven single
image depth estimation can be a viable alternative to other depth perception
techniques, more expensive and not always feasible in practical applications.
Our evaluation also highlights that our framework can run reasonably fast and
comparably to competitors, even on pure CPU systems. Moreover, its practical
deployment on low-power systems is around the corner.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Long Movie Clip Classification with State-Space Video Models</b></summary>
  <p><b>编号</b>：[6]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01692</p>
  <p><b>作者</b>：Md Mohaiminul Islam,  Gedas Bertasius</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：recently introduced video transformers partially address, form movie video classification tasks, recently introduced structured state, coin procedural activity datasets, long movie understanding tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Most modern video recognition models are designed to operate on short video
clips (e.g., 5-10s in length). Because of this, it is challenging to apply such
models to long movie understanding tasks, which typically require sophisticated
long-range temporal reasoning capabilities. The recently introduced video
transformers partially address this issue by using long-range temporal
self-attention. However, due to the quadratic cost of self-attention, such
models are often costly and impractical to use. Instead, we propose ViS4mer, an
efficient long-range video model that combines the strengths of self-attention
and the recently introduced structured state-space sequence (S4) layer. Our
model uses a standard Transformer encoder for short-range spatiotemporal
feature extraction, and a multi-scale temporal S4 decoder for subsequent
long-range temporal reasoning. By progressively reducing the spatiotemporal
feature resolution and channel dimension at each decoder layer, ViS4mer learns
complex long-range spatiotemporal dependencies in a video. Furthermore, ViS4mer
is $2.63\times$ faster and requires $8\times$ less GPU memory than the
corresponding pure self-attention-based model. Additionally, ViS4mer achieves
state-of-the-art results in $7$ out of $9$ long-form movie video classification
tasks on the LVU benchmark. Furthermore, we also show that our approach
successfully generalizes to other domains, achieving competitive results on the
Breakfast and the COIN procedural activity datasets. The code will be made
publicly available.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：TALLFormer: Temporal Action Localization with Long-memory Transformer</b></summary>
  <p><b>编号</b>：[10]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01680</p>
  <p><b>作者</b>：Feng Cheng,  Gedas Bertasius</p>
  <p><b>备注</b>：15 pages, 2 figures</p>
  <p><b>关键词</b>：end trainable temporal action localization transformer, range temporal boundary localization capability, high gpu memory cost caused, based feature extractor without freezing, range temporal boundary localization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Most modern approaches in temporal action localization divide this problem
into two parts: (i) short-term feature extraction and (ii) long-range temporal
boundary localization. Due to the high GPU memory cost caused by processing
long untrimmed videos, many methods sacrifice the representational power of the
short-term feature extractor by either freezing the backbone or using a very
small spatial video resolution. This issue becomes even worse with the recent
video transformer models, many of which have quadratic memory complexity. To
address these issues, we propose TALLFormer, a memory-efficient and end-to-end
trainable Temporal Action Localization transformer with Long-term memory. Our
long-term memory mechanism eliminates the need for processing hundreds of
redundant video frames during each training iteration, thus, significantly
reducing the GPU memory consumption and training time. These efficiency savings
allow us (i) to use a powerful video transformer-based feature extractor
without freezing the backbone or reducing the spatial video resolution, while
(ii) also maintaining long-range temporal boundary localization capability.
With only RGB frames as input and no external action recognition classifier,
TALLFormer outperforms previous state-of-the-art methods by a large margin,
achieving an average mAP of 59.1% on THUMOS14 and 35.6% on ActivityNet-1.3. The
code will be available in this https URL.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：MultiMAE: Multi-modal Multi-task Masked Autoencoders</b></summary>
  <p><b>编号</b>：[12]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01678</p>
  <p><b>作者</b>：Roman Bachmann,  David Mizrahi,  Andrei Atanov,  Amir Zamir</p>
  <p><b>备注</b>：Project page at this https URL</p>
  <p><b>关键词</b>：training objective accordingly includes predicting multiple outputs besides, train multimae entirely using pseudo labeling, additional information besides rgb images, make training multimae tractable, optionally accept additional modalities</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a pre-training strategy called Multi-modal Multi-task Masked
Autoencoders (MultiMAE). It differs from standard Masked Autoencoding in two
key aspects: I) it can optionally accept additional modalities of information
in the input besides the RGB image (hence "multi-modal"), and II) its training
objective accordingly includes predicting multiple outputs besides the RGB
image (hence "multi-task").
We make use of masking (across image patches and input modalities) to make
training MultiMAE tractable as well as to ensure cross-modality predictive
coding is indeed learned by the network. We show this pre-training strategy
leads to a flexible, simple, and efficient framework with improved transfer
results to downstream tasks. In particular, the same exact pre-trained network
can be flexibly used when additional information besides RGB images is
available or when no information other than RGB is available - in all
configurations yielding competitive to or significantly better results than the
baselines. To avoid needing training datasets with multiple modalities and
tasks, we train MultiMAE entirely using pseudo labeling, which makes the
framework widely applicable to any RGB dataset.
The experiments are performed on multiple transfer tasks (image
classification, semantic segmentation, depth estimation) and datasets
(ImageNet, ADE20K, Taskonomy, Hypersim, NYUv2). The results show an
intriguingly impressive capability by the model in cross-modal/task predictive
coding and transfer.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Residual-guided Personalized Speech Synthesis based on Face Image</b></summary>
  <p><b>编号</b>：[15]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01672</p>
  <p><b>作者</b>：Jianrong Wang,  Zixuan Wang,  Xiaosheng Hu,  Xuewei Li,  Qiang Fang,  Li Liu</p>
  <p><b>备注</b>：ICASSP 2022</p>
  <p><b>关键词</b>：synthesize personalized speech using neural vocoder, previous works derive personalized speech features, based residual personalized speech synthesis model, innovatively extract personalized speech features, designing two speech priors</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Previous works derive personalized speech features by training the model on a
large dataset composed of his/her audio sounds. It was reported that face
information has a strong link with the speech sound. Thus in this work, we
innovatively extract personalized speech features from human faces to
synthesize personalized speech using neural vocoder. A Face-based Residual
Personalized Speech Synthesis Model (FR-PSS) containing a speech encoder, a
speech synthesizer and a face encoder is designed for PSS. In this model, by
designing two speech priors, a residual-guided strategy is introduced to guide
the face feature to approach the true speech feature in the training. Moreover,
considering the error of feature's absolute values and their directional bias,
we formulate a novel tri-item loss function for face encoder. Experimental
results show that the speech synthesized by our model is comparable to the
personalized speech synthesized by training a large amount of audio data in
previous works.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Exemplar-bsaed Pattern Synthesis with Implicit Periodic Field Network</b></summary>
  <p><b>编号</b>：[16]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01671</p>
  <p><b>作者</b>：Haiwei Chen,  Jiayi Liu,  Weikai Chen,  Shichen Liu,  Yajie Zhao</p>
  <p><b>备注</b>：8 pages, CVPR 2022</p>
  <p><b>关键词</b>：continuously designed gan training procedures, periodic encoding scheme encourages diversity, based visual pattern synthesis framework, implicit formulation directly maps, present novel experimental results</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Synthesis of ergodic, stationary visual patterns is widely applicable in
texturing, shape modeling, and digital content creation. The wide applicability
of this technique thus requires the pattern synthesis approaches to be
scalable, diverse, and authentic. In this paper, we propose an exemplar-based
visual pattern synthesis framework that aims to model the inner statistics of
visual patterns and generate new, versatile patterns that meet the
aforementioned requirements. To this end, we propose an implicit network based
on generative adversarial network (GAN) and periodic encoding, thus calling our
network the Implicit Periodic Field Network (IPFN). The design of IPFN ensures
scalability: the implicit formulation directly maps the input coordinates to
features, which enables synthesis of arbitrary size and is computationally
efficient for 3D shape synthesis. Learning with a periodic encoding scheme
encourages diversity: the network is constrained to model the inner statistics
of the exemplar based on spatial latent codes in a periodic field. Coupled with
continuously designed GAN training procedures, IPFN is shown to synthesize
tileable patterns with smooth transitions and local variations. Last but not
least, thanks to both the adversarial training technique and the encoded
Fourier features, IPFN learns high-frequency functions that produce authentic,
high-quality results. To validate our approach, we present novel experimental
results on various applications in 2D texture synthesis and 3D shape synthesis.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：A Novel Capsule Neural Network Based Model for Drowsiness Detection  Using Electroencephalography Signals</b></summary>
  <p><b>编号</b>：[19]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01666</p>
  <p><b>作者</b>：Luis Guarda,  Juan Tapia,  Enrique Lopez Droguett,  Marcelo Ramos</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：new deep learning algorithm proposed, electroencephalography signals allow us, electroencephalography signals channels, gives specific information, convolutional neural network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The early detection of drowsiness has become vital to ensure the correct and
safe development of several industries' tasks. Due to the transient mental
state of a human subject between alertness and drowsiness, automated drowsiness
detection is a complex problem to tackle. The electroencephalography signals
allow us to record variations in an individual's brain's electrical potential,
where each of them gives specific information about a subject's mental state.
However, due to this type of signal's nature, its acquisition, in general, is
complex, so it is hard to have a large volume of data to apply techniques of
Deep Learning for processing and classification optimally. Nevertheless,
Capsule Neural Networks are a brand-new Deep Learning algorithm proposed for
work with reduced amounts of data. It is a robust algorithm to handle the
data's hierarchical relationships, which is an essential characteristic for
work with biomedical signals. Therefore, this paper presents a Deep
Learning-based method for drowsiness detection with CapsNet by using a
concatenation of spectrogram images of the electroencephalography signals
channels. The proposed CapsNet model is compared with a Convolutional Neural
Network, which is outperformed by the proposed model, which obtains an average
accuracy of 86,44% and 87,57% of sensitivity against an average accuracy of
75,86% and 79,47% sensitivity for the CNN, showing that CapsNet is more
suitable for this kind of datasets and tasks.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Evolving Neural Selection with Adaptive Regularization</b></summary>
  <p><b>编号</b>：[20]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01662</p>
  <p><b>作者</b>：Li Ding,  Lee Spector</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ablation studies also validate, modern deep neural networks, used neural network architectures, deep neural networks evolves, standard image recognition benchmarks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Over-parameterization is one of the inherent characteristics of modern deep
neural networks, which can often be overcome by leveraging regularization
methods, such as Dropout. Usually, these methods are applied globally and all
the input cases are treated equally. However, given the natural variation of
the input space for real-world tasks such as image recognition and natural
language understanding, it is unlikely that a fixed regularization pattern will
have the same effectiveness for all the input cases. In this work, we
demonstrate a method in which the selection of neurons in deep neural networks
evolves, adapting to the difficulty of prediction. We propose the Adaptive
Neural Selection (ANS) framework, which evolves to weigh neurons in a layer to
form network variants that are suitable to handle different input cases.
Experimental results show that the proposed method can significantly improve
the performance of commonly-used neural network architectures on standard image
recognition benchmarks. Ablation studies also validate the effectiveness and
contribution of each component in the proposed framework.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：APP: Anytime Progressive Pruning</b></summary>
  <p><b>编号</b>：[23]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01640</p>
  <p><b>作者</b>：Diganta Misra,  Bharat Runwal,  Tianlong Chen,  Zhangyang Wang,  Irina Rish</p>
  <p><b>备注</b>：21 pages including 4 pages of references. Preprint version</p>
  <p><b>关键词</b>：anytime osp models across multiple architectures, proposed approach significantly outperforms, observe interesting nonmonotonic transitions, $\ approx 7 \%$, $\ approx 22 \%$,</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the latest advances in deep learning, there has been a lot of focus on
the online learning paradigm due to its relevance in practical settings.
Although many methods have been investigated for optimal learning settings in
scenarios where the data stream is continuous over time, sparse networks
training in such settings have often been overlooked. In this paper, we explore
the problem of training a neural network with a target sparsity in a particular
case of online learning: the anytime learning at macroscale paradigm (ALMA). We
propose a novel way of progressive pruning, referred to as \textit{Anytime
Progressive Pruning} (APP); the proposed approach significantly outperforms the
baseline dense and Anytime OSP models across multiple architectures and
datasets under short, moderate, and long-sequence training. Our method, for
example, shows an improvement in accuracy of $\approx 7\%$ and a reduction in
the generalization gap by $\approx 22\%$, while being $\approx 1/3$ rd the size
of the dense baseline model in few-shot restricted imagenet training. We
further observe interesting nonmonotonic transitions in the generalization gap
in the high number of megabatches-based ALMA. The code and experiment
dashboards can be accessed at
\url{this https URL} and
\url{this https URL}, respectively.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：DODA: Data-oriented Sim-to-Real Domain Adaptation for 3D Indoor Semantic  Segmentation</b></summary>
  <p><b>编号</b>：[42]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01599</p>
  <p><b>作者</b>：Runyu Ding,  Jihan Yang,  Li Jiang,  Xiaojuan Qi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：deep learning approaches achieve prominent success, doda encompasses virtual scan simulation, doda surpasses existing uda approaches, 7 popular unsupervised domain adaptation, layout placements across domains</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning approaches achieve prominent success in 3D semantic
segmentation. However, collecting densely annotated real-world 3D datasets is
extremely time-consuming and expensive. Training models on synthetic data and
generalizing on real-world scenarios becomes an appealing alternative, but
unfortunately suffers from notorious domain shifts. In this work, we propose a
Data-Oriented Domain Adaptation (DODA) framework to mitigate pattern and
context gaps caused by different sensing mechanisms and layout placements
across domains. Our DODA encompasses virtual scan simulation to imitate
real-world point cloud patterns and tail-aware cuboid mixing to alleviate the
interior context gap with a cuboid-based intermediate domain. The first
unsupervised sim-to-real adaptation benchmark on 3D indoor semantic
segmentation is also built on 3D-FRONT, ScanNet and S3DIS along with 7 popular
Unsupervised Domain Adaptation (UDA) methods. Our DODA surpasses existing UDA
approaches by over 13% on both 3D-FRONT $\rightarrow$ ScanNet and 3D-FRONT
$\rightarrow$ S3DIS. Code will be available.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：FIFO: Learning Fog-invariant Features for Foggy Scene Segmentation</b></summary>
  <p><b>编号</b>：[47]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01587</p>
  <p><b>作者</b>：Sohyun Lee,  Taeyoung Son,  Suha Kwak</p>
  <p><b>备注</b>：Accepted to CVPR 2022 (Oral)</p>
  <p><b>关键词</b>：method substantially outperforms previous work, existing methods often degrade performance, learning semantic segmentation models robust, segmentation model alternately gradually closes, three real foggy image datasets</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Robust visual recognition under adverse weather conditions is of great
importance in real-world applications. In this context, we propose a new method
for learning semantic segmentation models robust against fog. Its key idea is
to consider the fog condition of an image as its style and close the gap
between images with different fog conditions in neural style spaces of a
segmentation model. In particular, since the neural style of an image is in
general affected by other factors as well as fog, we introduce a fog-pass
filter module that learns to extract a fog-relevant factor from the style.
Optimizing the fog-pass filter and the segmentation model alternately gradually
closes the style gap between different fog conditions and allows to learn
fog-invariant features in consequence. Our method substantially outperforms
previous work on three real foggy image datasets. Moreover, it improves
performance on both foggy and clear weather images, while existing methods
often degrade performance on clear scenes.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Object Level Depth Reconstruction for Category Level 6D Object Pose  Estimation From Monocular RGB Image</b></summary>
  <p><b>编号</b>：[48]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01586</p>
  <p><b>作者</b>：Zhaoxin Fan,  Zhenbo Song,  Jian Xu,  Zhicheng Wang,  Kejian Wu,  Hongyan Liu,  Jun He</p>
  <p><b>备注</b>：19 pages, 7 figures, 4 tables</p>
  <p><b>关键词</b>：two novel modules named normalized global position hints, novel approach named object level depth reconstruction network, depth information prohibits broader applications, level 6d object pose estimation, level 6d object pose estimation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, RGBD-based category-level 6D object pose estimation has achieved
promising improvement in performance, however, the requirement of depth
information prohibits broader applications. In order to relieve this problem,
this paper proposes a novel approach named Object Level Depth reconstruction
Network (OLD-Net) taking only RGB images as input for category-level 6D object
pose estimation. We propose to directly predict object-level depth from a
monocular RGB image by deforming the category-level shape prior into
object-level depth and the canonical NOCS representation. Two novel modules
named Normalized Global Position Hints (NGPH) and Shape-aware Decoupled Depth
Reconstruction (SDDR) module are introduced to learn high fidelity object-level
depth and delicate shape representations. At last, the 6D object pose is solved
by aligning the predicted canonical representation with the back-projected
object-level depth. Extensive experiments on the challenging CAMERA25 and
REAL275 datasets indicate that our model, though simple, achieves
state-of-the-art performance.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Coarse-to-Fine Q-attention with Learned Path Ranking</b></summary>
  <p><b>编号</b>：[50]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01571</p>
  <p><b>作者</b>：Stephen James,  Pieter Abbeel</p>
  <p><b>备注</b>：Project page and code: this https URL</p>
  <p><b>关键词</b>：approach across 16 rlbench tasks, propose learned path ranking, reaching paths generated, path generation modules, path generating methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose Learned Path Ranking (LPR), a method that accepts an end-effector
goal pose, and learns to rank a set of goal-reaching paths generated from an
array of path generating methods, including: path planning, Bezier curve
sampling, and a learned policy. The core idea being that each of the path
generation modules will be useful in different tasks, or at different stages in
a task. When LPR is added as an extension to C2F-ARM, our new system,
C2F-ARM+LPR, retains the sample efficiency of its predecessor, while also being
able to accomplish a larger set of tasks; in particular, tasks that require
very specific motions (e.g. opening toilet seat) that need to be inferred from
both demonstrations and exploration data. In addition to benchmarking our
approach across 16 RLBench tasks, we also learn real-world tasks, tabula rasa,
in 10-15 minutes, with only 3 demonstrations.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：DAD: Data-free Adversarial Defense at Test Time</b></summary>
  <p><b>编号</b>：[51]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01568</p>
  <p><b>作者</b>：Gaurav Kumar Nayak,  Ruchit Rawal,  Anirban Chakraborty</p>
  <p><b>备注</b>：WACV 2022. Project page: this https URL</p>
  <p><b>关键词</b>：proposed technique via extensive experiments, detection method correctly identifies 91, carefully crafted imperceptible noises, adversarial sample detection framework, model requires training data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep models are highly susceptible to adversarial attacks. Such attacks are
carefully crafted imperceptible noises that can fool the network and can cause
severe consequences when deployed. To encounter them, the model requires
training data for adversarial training or explicit regularization-based
techniques. However, privacy has become an important concern, restricting
access to only trained models but not the training data (e.g. biometric data).
Also, data curation is expensive and companies may have proprietary rights over
it. To handle such situations, we propose a completely novel problem of
'test-time adversarial defense in absence of training data and even their
statistics'. We solve it in two stages: a) detection and b) correction of
adversarial samples. Our adversarial sample detection framework is initially
trained on arbitrary data and is subsequently adapted to the unlabelled test
data through unsupervised domain adaptation. We further correct the predictions
on detected adversarial samples by transforming them in Fourier domain and
obtaining their low frequency component at our proposed suitable radius for
model prediction. We demonstrate the efficacy of our proposed technique via
extensive experiments against several adversarial attacks and for different
model architectures and datasets. For a non-robust Resnet-18 model pre-trained
on CIFAR-10, our detection method correctly identifies 91.42% adversaries.
Also, we significantly improve the adversarial accuracy from 0% to 37.37% with
a minimal drop of 0.02% in clean accuracy on state-of-the-art 'Auto Attack'
without having to retrain the model.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：HiT-DVAE: Human Motion Generation via Hierarchical Transformer Dynamical  VAE</b></summary>
  <p><b>编号</b>：[52]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01565</p>
  <p><b>作者</b>：Xiaoyu Bie,  Wen Guo,  Simon Leglaive,  Lauren Girin,  Francesc Moreno-Noguer,  Xavier Alameda-Pineda</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：propose hierarchical transformer dynamical variational autoencoder, diverse future human poses following, previous works rarely explore, predict arbitrary large sequences, observed 3d pose sequence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Studies on the automatic processing of 3D human pose data have flourished in
the recent past. In this paper, we are interested in the generation of
plausible and diverse future human poses following an observed 3D pose
sequence. Current methods address this problem by injecting random variables
from a single latent space into a deterministic motion prediction framework,
which precludes the inherent multi-modality in human motion generation. In
addition, previous works rarely explore the use of attention to select which
frames are to be used to inform the generation process up to our knowledge. To
overcome these limitations, we propose Hierarchical Transformer Dynamical
Variational Autoencoder, HiT-DVAE, which implements auto-regressive generation
with transformer-like attention mechanisms. HiT-DVAE simultaneously learns the
evolution of data and latent space distribution with time correlated
probabilistic dependencies, thus enabling the generative model to learn a more
complex and time-varying latent space as well as diverse and realistic human
motions. Furthermore, the auto-regressive generation brings more flexibility on
observation and prediction, i.e. one can have any length of observation and
predict arbitrary large sequences of poses with a single pre-trained model. We
evaluate the proposed method on HumanEva-I and Human3.6M with various
evaluation methods, and outperform the state-of-the-art methods on most of the
metrics.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Con$^{2}$DA: Simplifying Semi-supervised Domain Adaptation by Learning  Consistent and Contrastive Feature Representations</b></summary>
  <p><b>编号</b>：[55]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01558</p>
  <p><b>作者</b>：Manuel Pérez-Carrasco,  Pavlos Protopapas,  Guillermo Cabrera-Vives</p>
  <p><b>备注</b>：11 pages, 3 figures, 4 tables</p>
  <p><b>关键词</b>：extract good discriminative features across different domains, present con $^{ 2 }$ da, use different loss functions, performing stochastic data transformations, feature representation space using</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we present Con$^{2}$DA, a simple framework that extends recent
advances in semi-supervised learning to the semi-supervised domain adaptation
(SSDA) problem. Our framework generates pairs of associated samples by
performing stochastic data transformations to a given input. Associated data
pairs are mapped to a feature representation space using a feature extractor.
We use different loss functions to enforce consistency between the feature
representations of associated data pairs of samples. We show that these learned
representations are useful to deal with differences in data distributions in
the domain adaptation problem. We performed experiments to study the main
components of our model and we show that (i) learning of the consistent and
contrastive feature representations is crucial to extract good discriminative
features across different domains, and ii) our model benefits from the use of
strong augmentation policies. With these findings, our method achieves
state-of-the-art performances in three benchmark datasets for SSDA.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Internet-of-Things Architectures for Secure Cyber-Physical Spaces: the  VISOR Experience Report</b></summary>
  <p><b>编号</b>：[65]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01531</p>
  <p><b>作者</b>：Daniel De Pascale,  Giuseppe Cascavilla,  Mirella Sangiovanni,  Damian A. Tamburri,  Willem-Jan van den Heuvel</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：select optimal iot architecture configurations --, national interest project called visor, multiple federated devices encompassing drones, physical spaces using iot devices, dutch easter music festival</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Internet of things (IoT) technologies are becoming a more and more widespread
part of civilian life in common urban spaces, which are rapidly turning into
cyber-physical spaces. Simultaneously, the fear of terrorism and crime in such
public spaces is ever-increasing. Due to the resulting increased demand for
security, video-based IoT surveillance systems have become an important area
for research. Considering the large number of devices involved in the illicit
recognition task, we conducted a field study in a Dutch Easter music festival
in a national interest project called VISOR to select the most appropriate
device configuration in terms of performance and results. We iteratively
architected solutions for the security of cyber-physical spaces using IoT
devices. We tested the performance of multiple federated devices encompassing
drones, closed-circuit television, smart phone cameras, and smart glasses to
detect real-case scenarios of potentially malicious activities such as
mosh-pits and pick-pocketing. Our results pave the way to select optimal IoT
architecture configurations -- i.e., a mix of CCTV, drones, smart glasses, and
camera phones in our case -- to make safer cyber-physical spaces' a reality.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Bi-directional Loop Closure for Visual SLAM</b></summary>
  <p><b>编号</b>：[67]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01524</p>
  <p><b>作者</b>：Ihtisham Ali,  Sari Peltonen,  Atanas Gotchev</p>
  <p><b>备注</b>：11 pages, 11 figures</p>
  <p><b>关键词</b>：validate two different cnn architectures, offer direct loop closure opportunities, thus significantly reducing long, art methods still approach, loop closure detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A key functional block of visual navigation system for intelligent autonomous
vehicles is Loop Closure detection and subsequent relocalisation.
State-of-the-Art methods still approach the problem as uni-directional along
the direction of the previous motion. As a result, most of the methods fail in
the absence of a significantly similar overlap of perspectives. In this study,
we propose an approach for bi-directional loop closure. This will, for the
first time, provide us with the capability to relocalize to a location even
when traveling in the opposite direction, thus significantly reducing long-term
odometry drift in the absence of a direct loop. We present a technique to
select training data from large datasets in order to make them usable for the
bi-directional problem. The data is used to train and validate two different
CNN architectures for loop closure detection and subsequent regression of 6-DOF
camera pose between the views in an end-to-end manner. The outcome packs a
considerable impact and aids significantly to real-world scenarios that do not
offer direct loop closure opportunities. We provide a rigorous empirical
comparison against other established approaches and evaluate our method on both
outdoor and indoor data from the FinnForest dataset and PennCOSYVIO dataset.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Context-aware Visual Tracking with Joint Meta-updating</b></summary>
  <p><b>编号</b>：[71]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01513</p>
  <p><b>作者</b>：Qiuhong Shen,  Xin Li,  Fanyang Meng,  Yongsheng Liang</p>
  <p><b>备注</b>：9 pages, 8 figures</p>
  <p><b>关键词</b>：various emerging video applications, updater optimizes trackers directly, proposed tracking method achieves, visual object tracking acts, existing deep trackers</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Visual object tracking acts as a pivotal component in various emerging video
applications. Despite the numerous developments in visual tracking, existing
deep trackers are still likely to fail when tracking against objects with
dramatic variation. These deep trackers usually do not perform online update or
update single sub-branch of the tracking model, for which they cannot adapt to
the appearance variation of objects. Efficient updating methods are therefore
crucial for tracking while previous meta-updater optimizes trackers directly
over parameter space, which is prone to over-fit even collapse on longer
sequences. To address these issues, we propose a context-aware tracking model
to optimize the tracker over the representation space, which jointly
meta-update both branches by exploiting information along the whole sequence,
such that it can avoid the over-fitting problem. First, we note that the
embedded features of the localization branch and the box-estimation branch,
focusing on the local and global information of the target, are effective
complements to each other. Based on this insight, we devise a
context-aggregation module to fuse information in historical frames, followed
by a context-aware module to learn affinity vectors for both branches of the
tracker. Besides, we develop a dedicated meta-learning scheme, on account of
fast and stable updating with limited training samples. The proposed tracking
method achieves an EAO score of 0.514 on VOT2018 with the speed of 40FPS,
demonstrating its capability of improving the accuracy and robustness of the
underlying tracker with little speed drop.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：The Group Loss++: A deeper look into group loss for deep metric learning</b></summary>
  <p><b>编号</b>：[73]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01509</p>
  <p><b>作者</b>：Ismail Elezi,  Jenny Seidenschwarz,  Laurin Wagner,  Sebastiano Vascon,  Alessandro Torcinovich,  Marcello Pelillo,  Laura Leal-Taixe</p>
  <p><b>备注</b>：Accepted to IEEE Transactions on Pattern Analysis and Machine Intelligence (tPAMI), 2022. Includes supplementary material</p>
  <p><b>关键词</b>：density regions amongst data points belonging, obtain highly discriminative feature embeddings, consistent labelling amongst samples within, enforces embedding similarity across, inference strategies tailored towards</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep metric learning has yielded impressive results in tasks such as
clustering and image retrieval by leveraging neural networks to obtain highly
discriminative feature embeddings, which can be used to group samples into
different classes. Much research has been devoted to the design of smart loss
functions or data mining strategies for training such networks. Most methods
consider only pairs or triplets of samples within a mini-batch to compute the
loss function, which is commonly based on the distance between embeddings. We
propose Group Loss, a loss function based on a differentiable label-propagation
method that enforces embedding similarity across all samples of a group while
promoting, at the same time, low-density regions amongst data points belonging
to different groups. Guided by the smoothness assumption that "similar objects
should belong to the same group", the proposed loss trains the neural network
for a classification task, enforcing a consistent labelling amongst samples
within a class. We design a set of inference strategies tailored towards our
algorithm, named Group Loss++ that further improve the results of our model. We
show state-of-the-art results on clustering and image retrieval on four
retrieval datasets, and present competitive results on two person
re-identification datasets, providing a unified framework for retrieval and
re-identification.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Adaptive Network Combination for Single-Image Reflection Removal: A  Domain Generalization Perspective</b></summary>
  <p><b>编号</b>：[74]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01505</p>
  <p><b>作者</b>：Ming Liu,  Jianan Pan,  Zifei Yan,  Wangmeng Zuo,  Lei Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：deep single image reflection removal, learn deep models generalizing well, e ., output fusion, two representative adanec methods, handling different reflection types</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, multiple synthetic and real-world datasets have been built to
facilitate the training of deep single image reflection removal (SIRR) models.
Meanwhile, diverse testing sets are also provided with different types of
reflection and scenes. However, the non-negligible domain gaps between training
and testing sets make it difficult to learn deep models generalizing well to
testing images. The diversity of reflections and scenes further makes it a
mission impossible to learn a single model being effective to all testing sets
and real-world reflections. In this paper, we tackle these issues by learning
SIRR models from a domain generalization perspective. Particularly, for each
source set, a specific SIRR model is trained to serve as a domain expert of
relevant reflection types. For a given reflection-contaminated image, we
present a reflection type-aware weighting (RTAW) module to predict expert-wise
weights. RTAW can then be incorporated with adaptive network combination
(AdaNEC) for handling different reflection types and scenes, i.e., generalizing
to unknown domains. Two representative AdaNEC methods, i.e., output fusion (OF)
and network interpolation (NI), are provided by considering both adaptation
levels and efficiency. For images from one source set, we train RTAW to only
predict expert-wise weights of other domain experts for improving
generalization ability, while the weights of all experts are predicted and
employed during testing. An in-domain expert (IDE) loss is presented for
training RTAW. Extensive experiments show the appealing performance gain of our
AdaNEC on different state-of-the-art SIRR networks. Source code and pre-trained
models will available at this https URL.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Satellite Monitoring of Terrestrial Plastic Waste</b></summary>
  <p><b>编号</b>：[83]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01485</p>
  <p><b>作者</b>：Caleb Kruse,  Edward Boyda,  Sully Chen,  Krishna Karra,  Tristan Bou-Nahra,  Dan Hammer,  Jennifer Mathis,  Taylor Maddalene,  Jenna Jambeck,  Fabien Laurier</p>
  <p><b>备注</b>：14 pages, 14 figures</p>
  <p><b>关键词</b>：southeast asia identifies 996 subsequently confirmed waste sites, system deployed across twelve countries, algorithmically monitor waste site footprints, numerous sites sit directly, detected 374 waste aggregations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Plastic waste is a significant environmental pollutant that is difficult to
monitor. We created a system of neural networks to analyze spectral, spatial,
and temporal components of Sentinel-2 satellite data to identify terrestrial
aggregations of waste. The system works at continental scale. We evaluated
performance in Indonesia and detected 374 waste aggregations, more than double
the number of sites found in public databases. The same system deployed across
twelve countries in Southeast Asia identifies 996 subsequently confirmed waste
sites. For each detected site, we algorithmically monitor waste site footprints
through time and cross-reference other datasets to generate physical and social
metadata. 19% of detected waste sites are located within 200 m of a waterway.
Numerous sites sit directly on riverbanks, with high risk of ocean leakage.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Unsupervised Learning of Accurate Siamese Tracking</b></summary>
  <p><b>编号</b>：[88]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01475</p>
  <p><b>作者</b>：Qiuhong Shen,  Lei Qiao,  Jinyang Guo,  Peixia Li,  Xin Li,  Bo Li,  Weitao Feng,  Weihao Gan,  Wei Wu,  Wanli Ouyang</p>
  <p><b>备注</b>：13 pages, 7 figures, to appear in CVPR 2022</p>
  <p><b>关键词</b>：since noisy labels may degrade training, prior unsupervised tracking approaches rely heavily, tracker outperforms preceding unsupervised methods, various computer vision tasks, guided loss reweighting strategy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Unsupervised learning has been popular in various computer vision tasks,
including visual object tracking. However, prior unsupervised tracking
approaches rely heavily on spatial supervision from template-search pairs and
are still unable to track objects with strong variation over a long time span.
As unlimited self-supervision signals can be obtained by tracking a video along
a cycle in time, we investigate evolving a Siamese tracker by tracking videos
forward-backward. We present a novel unsupervised tracking framework, in which
we can learn temporal correspondence both on the classification branch and
regression branch. Specifically, to propagate reliable template feature in the
forward propagation process so that the tracker can be trained in the cycle, we
first propose a consistency propagation transformation. We then identify an
ill-posed penalty problem in conventional cycle training in backward
propagation process. Thus, a differentiable region mask is proposed to select
features as well as to implicitly penalize tracking errors on intermediate
frames. Moreover, since noisy labels may degrade training, we propose a
mask-guided loss reweighting strategy to assign dynamic weights based on the
quality of pseudo labels. In extensive experiments, our tracker outperforms
preceding unsupervised methods by a substantial margin, performing on par with
supervised methods on large-scale datasets such as TrackingNet and LaSOT. Code
is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Optimizing the Consumption of Spiking Neural Networks with Activity  Regularization</b></summary>
  <p><b>编号</b>：[95]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01460</p>
  <p><b>作者</b>：Simon Narduzzi,  Siavash A. Bigdeli,  Shih-Chii Liu,  L. Andrea Dunbar</p>
  <p><b>备注</b>：5 pages, 3 figures; accepted at IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Singapore, 2022</p>
  <p><b>关键词</b>：neural network activation maps, neural network models running, using binary activations, rate coding therefore, deep neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reducing energy consumption is a critical point for neural network models
running on edge devices. In this regard, reducing the number of
multiply-accumulate (MAC) operations of Deep Neural Networks (DNNs) running on
edge hardware accelerators will reduce the energy consumption during inference.
Spiking Neural Networks (SNNs) are an example of bio-inspired techniques that
can further save energy by using binary activations, and avoid consuming energy
when not spiking. The networks can be configured for equivalent accuracy on a
task through DNN-to-SNN conversion frameworks but their conversion is based on
rate coding therefore the synaptic operations can be high. In this work, we
look into different techniques to enforce sparsity on the neural network
activation maps and compare the effect of different training regularizers on
the efficiency of the optimized DNNs and SNNs.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Correlation Verification for Image Retrieval</b></summary>
  <p><b>编号</b>：[96]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01458</p>
  <p><b>作者</b>：Seongwon Lee,  Hongje Seong,  Suhyeon Lee,  Euntai Kim</p>
  <p><b>备注</b>：Accepted to CVPR 2022 (Oral Presentation)</p>
  <p><b>关键词</b>：comprising deeply stacked 4d convolutional layers, handle hard samples without losing generality, ranking network named correlation verification networks, gradually compresses dense feature correlation, learning diverse geometric matching patterns</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Geometric verification is considered a de facto solution for the re-ranking
task in image retrieval. In this study, we propose a novel image retrieval
re-ranking network named Correlation Verification Networks (CVNet). Our
proposed network, comprising deeply stacked 4D convolutional layers, gradually
compresses dense feature correlation into image similarity while learning
diverse geometric matching patterns from various image pairs. To enable
cross-scale matching, it builds feature pyramids and constructs cross-scale
feature correlations within a single inference, replacing costly multi-scale
inferences. In addition, we use curriculum learning with the hard negative
mining and Hide-and-Seek strategy to handle hard samples without losing
generality. Our proposed re-ranking network shows state-of-the-art performance
on several retrieval benchmarks with a significant margin (+12.6% in mAP on
ROxford-Hard+1M set) over state-of-the-art methods. The source code and models
are available online: this https URL.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Learning Commonsense-aware Moment-Text Alignment for Fast Video Temporal  Grounding</b></summary>
  <p><b>编号</b>：[99]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01450</p>
  <p><b>作者</b>：Ziyue Wu,  Junyu Gao,  Shucheng Huang,  Changsheng Xu</p>
  <p><b>备注</b>：Code is available at this https URL</p>
  <p><b>关键词</b>：existing approaches adopt elaborately designed cross, grounding temporal video segments described, fast video temporal grounding, fast video temporal grounding, two challenging benchmarks show</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Grounding temporal video segments described in natural language queries
effectively and efficiently is a crucial capability needed in
vision-and-language fields. In this paper, we deal with the fast video temporal
grounding (FVTG) task, aiming at localizing the target segment with high speed
and favorable accuracy. Most existing approaches adopt elaborately designed
cross-modal interaction modules to improve the grounding performance, which
suffer from the test-time bottleneck. Although several common space-based
methods enjoy the high-speed merit during inference, they can hardly capture
the comprehensive and explicit relations between visual and textual modalities.
In this paper, to tackle the dilemma of speed-accuracy tradeoff, we propose a
commonsense-aware cross-modal alignment (CCA) framework, which incorporates
commonsense-guided visual and text representations into a complementary common
space for fast video temporal grounding. Specifically, the commonsense concepts
are explored and exploited by extracting the structural semantic information
from a language corpus. Then, a commonsense-aware interaction module is
designed to obtain bridged visual and text features by utilizing the learned
commonsense concepts. Finally, to maintain the original semantic information of
textual queries, a cross-modal complementary common space is optimized to
obtain matching scores for performing FVTG. Extensive results on two
challenging benchmarks show that our CCA method performs favorably against
state-of-the-arts while running at high speed. Our code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：WildNet: Learning Domain Generalized Semantic Segmentation from the Wild</b></summary>
  <p><b>编号</b>：[101]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01446</p>
  <p><b>作者</b>：Suhyeon Lee,  Hongje Seong,  Seongwon Lee,  Euntai Kim</p>
  <p><b>备注</b>：Accepted to CVPR 2022</p>
  <p><b>关键词</b>：new domain generalized semantic segmentation network named wildnet, capture consistent semantic information even, providing semantic variations borrowed, five different datasets validate, generalized semantic information</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a new domain generalized semantic segmentation network named
WildNet, which learns domain-generalized features by leveraging a variety of
contents and styles from the wild. In domain generalization, the low
generalization ability for unseen target domains is clearly due to overfitting
to the source domain. To address this problem, previous works have focused on
generalizing the domain by removing or diversifying the styles of the source
domain. These alleviated overfitting to the source-style but overlooked
overfitting to the source-content. In this paper, we propose to diversify both
the content and style of the source domain with the help of the wild. Our main
idea is for networks to naturally learn domain-generalized semantic information
from the wild. To this end, we diversify styles by augmenting source features
to resemble wild styles and enable networks to adapt to a variety of styles.
Furthermore, we encourage networks to learn class-discriminant features by
providing semantic variations borrowed from the wild to source contents in the
feature space. Finally, we regularize networks to capture consistent semantic
information even when both the content and style of the source domain are
extended to the wild. Extensive experiments on five different datasets validate
the effectiveness of our WildNet, and we significantly outperform
state-of-the-art methods. The source code and model are available online:
this https URL.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Degradation-agnostic Correspondence from Resolution-asymmetric Stereo</b></summary>
  <p><b>编号</b>：[111]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01429</p>
  <p><b>作者</b>：Xihao Chen,  Zhiwei Xiong,  Zhen Cheng,  Jiayong Peng,  Yueyi Zhang,  Zheng-Jun Zha</p>
  <p><b>备注</b>：Accepted to CVPR 2022</p>
  <p><b>关键词</b>：stereo matching network trained, generally assumed photometric consistency, wide camera system, unsupervised learning perspective, truth disparity labels</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we study the problem of stereo matching from a pair of images
with different resolutions, e.g., those acquired with a tele-wide camera
system. Due to the difficulty of obtaining ground-truth disparity labels in
diverse real-world systems, we start from an unsupervised learning perspective.
However, resolution asymmetry caused by unknown degradations between two views
hinders the effectiveness of the generally assumed photometric consistency. To
overcome this challenge, we propose to impose the consistency between two views
in a feature space instead of the image space, named feature-metric
consistency. Interestingly, we find that, although a stereo matching network
trained with the photometric loss is not optimal, its feature extractor can
produce degradation-agnostic and matching-specific features. These features can
then be utilized to formulate a feature-metric loss to avoid the photometric
inconsistency. Moreover, we introduce a self-boosting strategy to optimize the
feature extractor progressively, which further strengthens the feature-metric
consistency. Experiments on both simulated datasets with various degradations
and a self-collected real-world dataset validate the superior performance of
the proposed method over existing solutions.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Re-examining Distillation For Continual Object Detection</b></summary>
  <p><b>编号</b>：[115]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01407</p>
  <p><b>作者</b>：Eli Verwimp,  Kuo Yang,  Sarah Parisot,  Hong Lanqing,  Steven McDonagh,  Eduardo Pérez-Pellitero,  Matthias De Lange,  Tinne Tuytelaars</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：yet overly confident teacher predictions prevent student models, object detection models forget catastrophically, contemporary continual object detection work, detecting incorrect teacher predictions, training models continually</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Training models continually to detect and classify objects, from new classes
and new domains, remains an open problem. In this work, we conduct a thorough
analysis of why and how object detection models forget catastrophically. We
focus on distillation-based approaches in two-stage networks; the most-common
strategy employed in contemporary continual object detection work.Distillation
aims to transfer the knowledge of a model trained on previous tasks -- the
teacher -- to a new model -- the student -- while it learns the new task. We
show that this works well for the region proposal network, but that wrong, yet
overly confident teacher predictions prevent student models from effective
learning of the classification head. Our analysis provides a foundation that
allows us to propose improvements for existing techniques by detecting
incorrect teacher predictions, based on current ground-truth labels, and by
employing an adaptive Huber loss as opposed to the mean squared error for the
distillation loss in the classification heads. We evidence that our strategy
works not only in a class incremental setting, but also in domain incremental
settings, which constitute a realistic context, likely to be the setting of
representative real-world problems.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：How stable are Transferability Metrics evaluations?</b></summary>
  <p><b>编号</b>：[116]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01403</p>
  <p><b>作者</b>：Andrea Agostinelli,  Michal Pándy,  Jasper Uijlings,  Thomas Mensink,  Vittorio Ferrari</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：single transferability metric works best, aggregating across many experiments, transferability metrics work best, selecting good source datasets, selecting good source architectures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transferability metrics is a maturing field with increasing interest, which
aims at providing heuristics for selecting the most suitable source models to
transfer to a given target dataset, without fine-tuning them all. However,
existing works rely on custom experimental setups which differ across papers,
leading to inconsistent conclusions about which transferability metrics work
best. In this paper we conduct a large-scale study by systematically
constructing a broad range of 715k experimental setup variations. We discover
that even small variations to an experimental setup lead to different
conclusions about the superiority of a transferability metric over another.
Then we propose better evaluations by aggregating across many experiments,
enabling to reach more stable conclusions. As a result, we reveal the
superiority of LogME at selecting good source datasets to transfer from in a
semantic segmentation scenario, NLEEP at selecting good source architectures in
an image classification scenario, and GBC at determining which target task
benefits most from a given source model. Yet, no single transferability metric
works best in all scenarios.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Dressi: A Hardware-Agnostic Differentiable Renderer with Reactive Shader  Packing and Soft Rasterization</b></summary>
  <p><b>编号</b>：[121]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01386</p>
  <p><b>作者</b>：Yusuke Takimoto,  Hiroyuki Sato,  Hikari Takehara,  Keishiro Uragaki,  Takehiro Tawara,  Xiao Liang,  Kentaro Oku,  Wataru Kishimoto,  Bo Zheng</p>
  <p><b>备注</b>：13 pages, 17 figures, EUROGRAPHICS 2022</p>
  <p><b>关键词</b>：system design mixes dr algorithm implementation, specific modules handcrafted using cuda, efficiently execute complex computational graphs, agnostic differentiable renderer called dressi, new full ad design</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Differentiable rendering (DR) enables various computer graphics and computer
vision applications through gradient-based optimization with derivatives of the
rendering equation. Most rasterization-based approaches are built on
general-purpose automatic differentiation (AD) libraries and DR-specific
modules handcrafted using CUDA. Such a system design mixes DR algorithm
implementation and algorithm building blocks, resulting in hardware dependency
and limited performance. In this paper, we present a practical
hardware-agnostic differentiable renderer called Dressi, which is based on a
new full AD design. The DR algorithms of Dressi are fully written in our
Vulkan-based AD for DR, Dressi-AD, which supports all primitive operations for
DR. Dressi-AD and our inverse UV technique inside it bring hardware
independence and acceleration by graphics hardware. Stage packing, our runtime
optimization technique, can adapt hardware constraints and efficiently execute
complex computational graphs of DR with reactive cache considering the render
pass hierarchy of Vulkan. HardSoftRas, our novel rendering process, is designed
for inverse rendering with a graphics pipeline. Under the limited
functionalities of the graphics pipeline, HardSoftRas can propagate the
gradients of pixels from the screen space to far-range triangle attributes. Our
experiments and applications demonstrate that Dressi establishes hardware
independence, high-quality and robust optimization with fast speed, and
photorealistic rendering.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Learning to solve Minimum Cost Multicuts efficiently using Edge-Weighted  Graph Convolutional Neural Networks</b></summary>
  <p><b>编号</b>：[127]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01366</p>
  <p><b>作者</b>：Steffen Jung,  Margret Keuper</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：therefore adapt various gnn architectures including graph convolutional networks, signed graph convolutional networks, graph convolutional neural networks, providing lower computation times, largely improved scalability compared</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The minimum cost multicut problem is the NP-hard/APX-hard combinatorial
optimization problem of partitioning a real-valued edge-weighted graph such as
to minimize the total cost of the partition. While graph convolutional neural
networks (GNN) have proven to be promising in the context of combinatorial
optimization, most of them are only tailored to or tested on positive-valued
edge weights, i.e. they do not comply to the nature of the multicut problem. We
therefore adapt various GNN architectures including Graph Convolutional
Networks, Signed Graph Convolutional Networks and Graph Isomorphic Networks to
facilitate the efficient encoding of real-valued edge costs. Moreover, we
employ a reformulation of the multicut ILP constraints to a polynomial program
as loss function that allows to learn feasible multicut solutions in a scalable
way. Thus, we provide the first approach towards end-to-end trainable
multicuts. Our findings support that GNN approaches can produce good solutions
in practice while providing lower computation times and largely improved
scalability compared to LP solvers and optimized heuristics, especially when
considering large instances.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：MGRR-Net: Multi-level Graph Relational Reasoning Network for Facial  Action Units Detection</b></summary>
  <p><b>编号</b>：[130]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01349</p>
  <p><b>作者</b>：Xuri Ge,  Joemon M. Jose,  Songpei Xu,  Xiao Liu,  Hu Han</p>
  <p><b>备注</b>：10 pages, 4 figures, 9 tables; submitted to IEEE TMM for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible</p>
  <p><b>关键词</b>：local face patches features via graph neural network, wise feature learning via graph attention network, level graph relational reasoning network, proposed approach achieves superior performance, attracted extensive research attention due</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The Facial Action Coding System (FACS) encodes the action units (AUs) in
facial images, which has attracted extensive research attention due to its wide
use in facial expression analysis. Many methods that perform well on automatic
facial action unit (AU) detection primarily focus on modeling various types of
AU relations between corresponding local muscle areas, or simply mining global
attention-aware facial features, however, neglect the dynamic interactions
among local-global features. We argue that encoding AU features just from one
perspective may not capture the rich contextual information between regional
and global face features, as well as the detailed variability across AUs,
because of the diversity in expression and individual characteristics. In this
paper, we propose a novel Multi-level Graph Relational Reasoning Network
(termed MGRR-Net) for facial AU detection. Each layer of MGRR-Net performs a
multi-level (i.e., region-level, pixel-wise and channel-wise level) feature
learning. While the region-level feature learning from local face patches
features via graph neural network can encode the correlation across different
AUs, the pixel-wise and channel-wise feature learning via graph attention
network can enhance the discrimination ability of AU features from global face
features. The fused features from the three levels lead to improved AU
discriminative ability. Extensive experiments on DISFA and BP4D AU datasets
show that the proposed approach achieves superior performance than the
state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Extended Reality for Anxiety and Depression Therapy amidst Mental  Disorders -- A Systematic Review</b></summary>
  <p><b>编号</b>：[131]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01348</p>
  <p><b>作者</b>：Omisore Olatunji,  Ifeanyi Odenigbo,  Joseph Orji,  Amelia Beltran,  Rita Orji,  Nilufar Baghaei,  Meier Sandra</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：computing machinery digital library yielded 689 articles, three databases namely google scholar, could rapidly aid intervention, vr ), augmented reality, review study could aid</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This systematic study is aimed to investigate the implementation level of
different extended reality (XR) techniques in the care of mental disorder. We
point out some XR technologies used to deliver care for mental disorders, and
to evaluate the effectiveness of using XR systems for anxiety and depression
amidst other mental disorders. A search period of May 2017 and August 2021 was
defined to filter out articles related to the usage of virtual reality (VR),
augmented reality (AR) and mixed reality (AR) in a mental health context.
Search done on three databases namely Google Scholar, PubMED, and Association
for Computing Machinery Digital Library yielded 689 articles. Also, 10 articles
were recommended. Upon eligibility filtering, only 72 articles were found
relevant and were utilized for the study. Results show that the 72 studies were
done in only 23 countries across the globe, with the majority of studies being
reported for developed countries such as USA (20.64%) and Germany (11.11%).
Thus this could rapidly aid intervention of mental health disorder with XR.
Meanwhile, none of the studies observed was from an African country. The
majority of the articles reported that XR techniques led to significant
reduction in symptoms of anxiety or depression. The majority of studies (23,
36.51%) were published in the year 2021 of the total studies included. In a
sense, this data might be attributed to COVID-19 pandemic. Most studies (30,
47.62%) focused a population with age range of 18 to 65 years, while fewer
studies (4, 6.35%) focused on each of adolescents (10 - 19 years) and seniors
(over 64 years). Also, more studies were done experimentally (52, 82.54%)
rather than by analytical and modeling approach (5, 7.94%) as found in other XR
studies domain. This review study could aid the development of XR systems for
effective cognitive behavioral and exposure therapies of mental disorders.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：An application of Pixel Interval Down-sampling (PID) for dense tiny  microorganism counting on environmental microorganism images</b></summary>
  <p><b>编号</b>：[134]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01341</p>
  <p><b>作者</b>：Jiawei Zhang,  Ning Xu,  Chen Li,  Md Mamunur Rahaman,  Yu-Dong Yao,  Yu-Hao Lin,  Jinghua Zhang,  Tao Jiang,  Wenjun Qin,  Marcin Grzegorzek</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：done using classical segmentation metrics, art approaches like attention u, dense tiny objects counting tasks, 2448 yeast cell images, dense tiny objects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper proposes a novel pixel interval down-sampling network (PID-Net)
for dense tiny objects (yeast cells) counting tasks with higher accuracy. The
PID-Net is an end-to-end CNN model with encoder to decoder architecture. The
pixel interval down-sampling operations are concatenated with max-pooling
operations to combine the sparse and dense features. It addresses the
limitation of contour conglutination of dense objects while counting.
Evaluation was done using classical segmentation metrics (Dice, Jaccard,
Hausdorff distance) as well as counting metrics. Experimental result shows that
the proposed PID-Net has the best performance and potential for dense tiny
objects counting tasks, which achieves 96.97% counting accuracy on the dataset
with 2448 yeast cell images. By comparing with the state-of-the-art approaches
like Attention U-Net, Swin U-Net and Trans U-Net, the proposed PID-Net can
segment the dense tiny objects with clearer boundaries and fewer incorrect
debris, which shows the great potential of PID-Net in the task of accurate
counting tasks.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：IMOT: General-Purpose, Fast and Robust Estimation for Spatial Perception  Problems with Outliers</b></summary>
  <p><b>编号</b>：[142]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01324</p>
  <p><b>作者</b>：Lei Sun</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：return robust results even without noise bound information, 5 different spatial perception problems including, 3 -- 125 times faster, 3 -- 10 iterations, purpose robust estimator imot</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Spatial perception problems are the fundamental building blocks of robotics
and computer vision. However, in many real-world situations, they inevitably
suffer from the issue of outliers, which hinders traditional solvers from
making correct estimates. In this paper, we present a novel, general-purpose
robust estimator IMOT (Iterative Multi-layered Otsu's Thresholding) using
standard non-minimal solvers to rapidly reject outliers for spatial perception
problems. First, we propose a new outlier-robust iterative optimizing framework
where in each iteration all the measurement data are separated into two groups
according to the residual errors and only the group with lower residual errors
can be preserved for estimation in the next iteration. Second, we introduce and
employ the well-known Otsu's method (from image processing) to conduct
thresholding on the residual errors so as to obtain the best separation
(grouping) statistically which maximizes the between-class variance. Third, to
enhance robustness, we design a multi-layered Otsu's thresholding approach in
combination with our framework to sift out the true inliers from outliers that
might even occupy the majority of measurements. We test our robust estimator
IMOT on 5 different spatial perception problems including: rotation averaging,
rotation search, point cloud registration, category-level registration, and
SLAM. Experiments show that IMOT is robust against 70%--90% of outliers and can
typically converge in only 3--10 iterations, being 3--125 times faster than
existing robust estimators: GNC and ADAPT. Moreover, IMOT is able to return
robust results even without noise bound information.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：RayMVSNet: Learning Ray-based 1D Implicit Fields for Accurate Multi-View  Stereo</b></summary>
  <p><b>编号</b>：[144]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01320</p>
  <p><b>作者</b>：Junhua Xi,  Yifei Shi,  Yijie Wang,  Yulan Guo,  Kai Xu</p>
  <p><b>备注</b>：cvpr 2022, 11 pages</p>
  <p><b>关键词</b>：far centered around 3d convolution, crossing point indicating scene depth, 1d implicit field along, achieving overall reconstruction score, full cost volume optimization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Learning-based multi-view stereo (MVS) has by far centered around 3D
convolution on cost volumes. Due to the high computation and memory consumption
of 3D CNN, the resolution of output depth is often considerably limited.
Different from most existing works dedicated to adaptive refinement of cost
volumes, we opt to directly optimize the depth value along each camera ray,
mimicking the range (depth) finding of a laser scanner. This reduces the MVS
problem to ray-based depth optimization which is much more light-weight than
full cost volume optimization. In particular, we propose RayMVSNet which learns
sequential prediction of a 1D implicit field along each camera ray with the
zero-crossing point indicating scene depth. This sequential modeling, conducted
based on transformer features, essentially learns the epipolar line search in
traditional multi-view stereo. We also devise a multi-task learning for better
optimization convergence and depth accuracy. Our method ranks top on both the
DTU and the Tanks \& Temples datasets over all previous learning-based methods,
achieving overall reconstruction score of 0.33mm on DTU and f-score of 59.48%
on Tanks & Temples.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Flexible Portrait Image Editing with Fine-Grained Control</b></summary>
  <p><b>编号</b>：[145]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01318</p>
  <p><b>作者</b>：Linlin Liu,  Qian Fu,  Fei Hou,  Ying He</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：novel asymmetric conditional gan architecture, single neural network model, guide controllable image generation, also present ablation studies, contain positional information</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We develop a new method for portrait image editing, which supports
fine-grained editing of geometries, colors, lights and shadows using a single
neural network model. We adopt a novel asymmetric conditional GAN architecture:
the generators take the transformed conditional inputs, such as edge maps,
color palette, sliders and masks, that can be directly edited by the user; the
discriminators take the conditional inputs in the way that can guide
controllable image generation more effectively. Taking color editing as an
example, we feed color palettes (which can be edited easily) into the
generator, and color maps (which contain positional information of colors) into
the discriminator. We also design a region-weighted discriminator so that
higher weights are assigned to more important regions, like eyes and skin.
Using a color palette, the user can directly specify the desired colors of
hair, skin, eyes, lip and background. Color sliders allow the user to blend
colors in an intuitive manner. The user can also edit lights and shadows by
modifying the corresponding masks. We demonstrate the effectiveness of our
method by evaluating it on the CelebAMask-HQ dataset with a wide range of
tasks, including geometry/color/shadow/light editing, hand-drawn sketch to
image translation, and color transfer. We also present ablation studies to
justify our design.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：REM: Routing Entropy Minimization for Capsule Networks</b></summary>
  <p><b>编号</b>：[150]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01298</p>
  <p><b>作者</b>：Riccardo Renzulli,  Enzo Tartaglione,  Marco Grangetto</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：model parameters distribution towards low entropy configurations, also generate static parse trees, capsule networks build stronger relationships, inspired neural network model, capsule networks ambition</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Capsule Networks ambition is to build an explainable and
biologically-inspired neural network model. One of their main innovations
relies on the routing mechanism which extracts a parse tree: its main purpose
is to explicitly build relationships between capsules. However, their true
potential in terms of explainability has not surfaced yet: these relationships
are extremely heterogeneous and difficult to understand. This paper proposes
REM, a technique which minimizes the entropy of the parse tree-like structure,
improving its explainability. We accomplish this by driving the model
parameters distribution towards low entropy configurations, using a pruning
mechanism as a proxy. We also generate static parse trees with no performance
loss, showing that, with REM, Capsule Networks build stronger relationships
between capsules.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Learning Dynamic Correlations in Spatiotemporal Graphs for Motion  Prediction</b></summary>
  <p><b>编号</b>：[151]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01297</p>
  <p><b>作者</b>：Jiajun Fu,  Fuxing Yang,  Jianqin Yin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：powerful spatiotemporal graph convolution network called dstd, also mathematically reformulating spatiotemporal graph convolutions, present constrained dynamic correlation modeling strategy, gc decomposes dynamic spatiotemporal graph modeling, common constraints like body connections</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Human motion prediction is a challenge task due to the dynamic spatiotemporal
graph correlations in different motion sequences. How to efficiently represent
spatiotemporal graph correlations and model dynamic correlation variances
between different motion sequences is a challenge for spatiotemporal graph
representation in motion prediction. In this work, we present Dynamic
SpatioTemporal Graph Convolution (DSTD-GC). The proposed DSTD-GC decomposes
dynamic spatiotemporal graph modeling into a combination of Dynamic Spatial
Graph Convolution (DS-GC) and Dynamic Temporal Graph Convolution (DT-GC). As
human motions are subject to common constraints like body connections and
present dynamic motion patterns from different samples, we present Constrained
Dynamic Correlation Modeling strategy to represent the spatial/temporal graph
as a shared spatial/temporal correlation and a function to extract
temporal-specific /spatial-specific adjustments for each sample. The modeling
strategy represents the spatiotemporal graph with 28.6\% parameters of the
state-of-the-art static decomposition representation while also explicitly
models sample-specific spatiotemporal correlation variances. Moreover, we also
mathematically reformulating spatiotemporal graph convolutions and their
decomposed variants into a unified form and find that DSTD-GC relaxes strict
constraints of other graph convolutions, leading to a stronger representation
capability. Combining DSTD-GC with prior knowledge, we propose a powerful
spatiotemporal graph convolution network called DSTD-GCN which outperforms
state-of-the-art methods on the Human3.6M and CMU Mocap datasets in prediction
accuracy with fewest parameters.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：SPFNet:Subspace Pyramid Fusion Network for Semantic Segmentation</b></summary>
  <p><b>编号</b>：[160]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01278</p>
  <p><b>作者</b>：Mohammed A. M. Elhassan,  Chenhui Yang,  Chenxi Huang,  Tewodros Legesse Munea</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：help select category localization details, enhance communication across different sub, egca adopts shuffle attention mechanism, hardly extract sufficient context information, propose subspace pyramid fusion network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The encoder-decoder structure has significantly improved performance in many
vision tasks by fusing low-level and high-level feature maps. However, this
approach can hardly extract sufficient context information for pixel-wise
segmentation. In addition, extracting similar low-level features at multiple
scales could lead to redundant information. To tackle these issues, we propose
Subspace Pyramid Fusion Network (SPFNet). Specifically, we combine pyramidal
module and context aggregation module to exploit the impact of
multi-scale/global context information. At first, we construct a Subspace
Pyramid Fusion Module (SPFM) based on Reduced Pyramid Pooling (RPP). Then, we
propose the Efficient Global Context Aggregation (EGCA) module to capture
discriminative features by fusing multi-level global context features. Finally,
we add decoder-based subpixel convolution to retrieve the high-resolution
feature maps, which can help select category localization details. SPFM learns
separate RPP for each feature subspace to capture multi-scale feature
representations, which is more useful for semantic segmentation. EGCA adopts
shuffle attention mechanism to enhance communication across different
sub-features. Experimental results on two well-known semantic segmentation
datasets, including Camvid and Cityscapes, show that our proposed method is
competitive with other state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Aligning Silhouette Topology for Self-Adaptive 3D Human Pose Recovery</b></summary>
  <p><b>编号</b>：[161]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01276</p>
  <p><b>作者</b>：Mugalodi Rakesh,  Jogendra Nath Kundu,  Varun Jampani,  R. Venkatesh Babu</p>
  <p><b>备注</b>：NeurIPS 2021</p>
  <p><b>关键词</b>：alignment loss via distance field computation, existing 3d human pose estimation techniques, standard foreground silhouette estimation techniques, novel target adaptation framework, 3d pose supervision forms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Articulation-centric 2D/3D pose supervision forms the core training objective
in most existing 3D human pose estimation techniques. Except for synthetic
source environments, acquiring such rich supervision for each real target
domain at deployment is highly inconvenient. However, we realize that standard
foreground silhouette estimation techniques (on static camera feeds) remain
unaffected by domain-shifts. Motivated by this, we propose a novel target
adaptation framework that relies only on silhouette supervision to adapt a
source-trained model-based regressor. However, in the absence of any auxiliary
cue (multi-view, depth, or 2D pose), an isolated silhouette loss fails to
provide a reliable pose-specific gradient and requires to be employed in tandem
with a topology-centric loss. To this end, we develop a series of
convolution-friendly spatial transformations in order to disentangle a
topological-skeleton representation from the raw silhouette. Such a design
paves the way to devise a Chamfer-inspired spatial topological-alignment loss
via distance field computation, while effectively avoiding any gradient
hindering spatial-to-pointset mapping. Experimental results demonstrate our
superiority against prior-arts in self-adapting a source trained model to
diverse unlabeled target domains, such as a) in-the-wild datasets, b)
low-resolution image domains, and c) adversarially perturbed image domains (via
UAP).</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Improving Monocular Visual Odometry Using Learned Depth</b></summary>
  <p><b>编号</b>：[164]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01268</p>
  <p><b>作者</b>：Libo Sun,  Wei Yin,  Enze Xie,  Zhengrong Li,  Changming Sun,  Chunhua Shen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：diverse scenarios remains largely unsolved, two separate working modes, single monocular image input, exploit monocular depth estimation, robust monocular vo systems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Monocular visual odometry (VO) is an important task in robotics and computer
vision. Thus far, how to build accurate and robust monocular VO systems that
can work well in diverse scenarios remains largely unsolved. In this paper, we
propose a framework to exploit monocular depth estimation for improving VO. The
core of our framework is a monocular depth estimation module with a strong
generalization capability for diverse scenes. It consists of two separate
working modes to assist the localization and mapping. With a single monocular
image input, the depth estimation module predicts a relative depth to help the
localization module on improving the accuracy. With a sparse depth map and an
RGB image input, the depth estimation module can generate accurate
scale-consistent depth for dense mapping. Compared with current learning-based
VO methods, our method demonstrates a stronger generalization ability to
diverse scenes. More significantly, our framework is able to boost the
performances of existing geometry-based VO methods by a large margin.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：FoV-Net: Field-of-View Extrapolation Using Self-Attention and  Uncertainty</b></summary>
  <p><b>编号</b>：[165]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01267</p>
  <p><b>作者</b>：Liqian Ma,  Stamatios Georgoulis,  Xu Jia,  Luc Van Gool</p>
  <p><b>备注</b>：Accepted to IEEE Robotics and Automation Letters and ICRA2021. Project page this http URL</p>
  <p><b>关键词</b>：may benefit critical decision, based feature aggregation module, temporally consistent wide field, temporally consistent field, making downstream applications</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The ability to make educated predictions about their surroundings, and
associate them with certain confidence, is important for intelligent systems,
like autonomous vehicles and robots. It allows them to plan early and decide
accordingly. Motivated by this observation, in this paper we utilize
information from a video sequence with a narrow field-of-view to infer the
scene at a wider field-of-view. To this end, we propose a temporally consistent
field-of-view extrapolation framework, namely FoV-Net, that: (1) leverages 3D
information to propagate the observed scene parts from past frames; (2)
aggregates the propagated multi-frame information using an attention-based
feature aggregation module and a gated self-attention module, simultaneously
hallucinating any unobserved scene parts; and (3) assigns an interpretable
uncertainty value at each pixel. Extensive experiments show that FoV-Net does
not only extrapolate the temporally consistent wide field-of-view scene better
than existing alternatives, but also provides the associated uncertainty which
may benefit critical decision-making downstream applications. Project page is
at this http URL.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Multi-modality Associative Bridging through Memory: Speech Sound  Recollected from Face Video</b></summary>
  <p><b>编号</b>：[167]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01265</p>
  <p><b>作者</b>：Minsu Kim,  Joanna Hong,  Se Jin Park,  Yong Man Ro</p>
  <p><b>备注</b>：Published at ICCV 2021</p>
  <p><b>关键词</b>：associative bridge properly relates, target modal representations inside, modal bridging framework, provides rich information, e ., visual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we introduce a novel audio-visual multi-modal bridging
framework that can utilize both audio and visual information, even with
uni-modal inputs. We exploit a memory network that stores source (i.e., visual)
and target (i.e., audio) modal representations, where source modal
representation is what we are given, and target modal representations are what
we want to obtain from the memory network. We then construct an associative
bridge between source and target memories that considers the interrelationship
between the two memories. By learning the interrelationship through the
associative bridge, the proposed bridging framework is able to obtain the
target modal representations inside the memory network, even with the source
modal input only, and it provides rich information for its downstream tasks. We
apply the proposed framework to two tasks: lip reading and speech
reconstruction from silent video. Through the proposed associative bridge and
modality-specific memories, each task knowledge is enriched with the recalled
audio context, achieving state-of-the-art performance. We also verify that the
associative bridge properly relates the source and target memories.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Probabilistic Implicit Scene Completion</b></summary>
  <p><b>编号</b>：[168]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01264</p>
  <p><b>作者</b>：Dongsu Zhang,  Changwoon Choi,  Inbum Park,  Young Min Kim</p>
  <p><b>备注</b>：Accepted to ICLR 2022 as spotlight, code available at this https URL</p>
  <p><b>关键词</b>：model successfully generates diverse plausible scenes faithful, approach outperforms deterministic models even, quality result requires scalable solutions, probabilistic shape completion method extended, consider multiple possible outcomes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a probabilistic shape completion method extended to the continuous
geometry of large-scale 3D scenes. Real-world scans of 3D scenes suffer from a
considerable amount of missing data cluttered with unsegmented objects. The
problem of shape completion is inherently ill-posed, and high-quality result
requires scalable solutions that consider multiple possible outcomes. We employ
the Generative Cellular Automata that learns the multi-modal distribution and
transform the formulation to process large-scale continuous geometry. The local
continuous shape is incrementally generated as a sparse voxel embedding, which
contains the latent code for each occupied cell. We formally derive that our
training objective for the sparse voxel embedding maximizes the variational
lower bound of the complete shape distribution and therefore our progressive
generation constitutes a valid generative model. Experiments show that our
model successfully generates diverse plausible scenes faithful to the input,
especially when the input suffers from a significant amount of missing data. We
also demonstrate that our approach outperforms deterministic models even in
less ambiguous cases with a small amount of missing data, which infers that
probabilistic formulation is crucial for high-quality geometry completion on
input scans exhibiting any levels of completeness.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Direct Dense Pose Estimation</b></summary>
  <p><b>编号</b>：[169]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01263</p>
  <p><b>作者</b>：Liqian Ma,  Lingjie Liu,  Christian Theobalt,  Luc Van Gool</p>
  <p><b>备注</b>：Accepted to 3DV 2021. Project page this http URL</p>
  <p><b>关键词</b>：simple yet effective 2d temporal, prior dense pose estimation methods, previous dense pose estimation methods, called direct dense pose, global iuv representation separately</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Dense human pose estimation is the problem of learning dense correspondences
between RGB images and the surfaces of human bodies, which finds various
applications, such as human body reconstruction, human pose transfer, and human
action recognition. Prior dense pose estimation methods are all based on Mask
R-CNN framework and operate in a top-down manner of first attempting to
identify a bounding box for each person and matching dense correspondences in
each bounding box. Consequently, these methods lack robustness due to their
critical dependence on the Mask R-CNN detection, and the runtime increases
drastically as the number of persons in the image increases. We therefore
propose a novel alternative method for solving the dense pose estimation
problem, called Direct Dense Pose (DDP). DDP first predicts the instance mask
and global IUV representation separately and then combines them together. We
also propose a simple yet effective 2D temporal-smoothing scheme to alleviate
the temporal jitters when dealing with video data. Experiments demonstrate that
DDP overcomes the limitations of previous top-down baseline methods and
achieves competitive accuracy. In addition, DDP is computationally more
efficient than previous dense pose estimation methods, and it reduces jitters
when applied to a video sequence, which is a problem plaguing the previous
methods.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：BatchFormerV2: Exploring Sample Relationships for Dense Representation  Learning</b></summary>
  <p><b>编号</b>：[173]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01254</p>
  <p><b>作者</b>：Zhi Hou,  Baosheng Yu,  Chaoyue Wang,  Yibing Zhan,  Dacheng Tao</p>
  <p><b>备注</b>：Tech report</p>
  <p><b>关键词</b>：two important dense prediction tasks, batchformerv2 consistently improves current detr, overcoming data scarcity challenges, also visual recognition applications, popular visual recognition tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Attention mechanisms have been very popular in deep neural networks, where
the Transformer architecture has achieved great success in not only natural
language processing but also visual recognition applications. Recently, a new
Transformer module, applying on batch dimension rather than spatial/channel
dimension, i.e., BatchFormer [18], has been introduced to explore sample
relationships for overcoming data scarcity challenges. However, it only works
with image-level representations for classification. In this paper, we devise a
more general batch Transformer module, BatchFormerV2, which further enables
exploring sample relationships for dense representation learning. Specifically,
when applying the proposed module, it employs a two-stream pipeline during
training, i.e., either with or without a BatchFormerV2 module, where the
batchformer stream can be removed for testing. Therefore, the proposed method
is a plug-and-play module and can be easily integrated into different vision
Transformers without any extra inference cost. Without bells and whistles, we
show the effectiveness of the proposed method for a variety of popular visual
recognition tasks, including image classification and two important dense
prediction tasks: object detection and panoptic segmentation. Particularly,
BatchFormerV2 consistently improves current DETR-based detection methods (e.g.,
DETR, Deformable-DETR, Conditional DETR, and SMCA) by over 1.3%. Code will be
made publicly available.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Dynamic Focus-aware Positional Queries for Semantic Segmentation</b></summary>
  <p><b>编号</b>：[174]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01244</p>
  <p><b>作者</b>：Haoyu He,  Jianfei Cai,  Zizheng Pan,  Jing Liu,  Jing Zhang,  Dacheng Tao,  Bohan Zhuang</p>
  <p><b>备注</b>：Tech report</p>
  <p><b>关键词</b>：latest top semantic segmentation approaches, generate positional queries dynamically conditioned, simple yet effective solution, perform local relation aggregation, often encode dataset statistics</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Most of the latest top semantic segmentation approaches are based on vision
Transformers, particularly DETR-like frameworks, which employ a set of queries
in the Transformer decoder. Each query is composed of a content query that
preserves semantic information and a positional query that provides positional
guidance for aggregating the query-specific context. However, the positional
queries in the Transformer decoder layers are typically represented as fixed
learnable weights, which often encode dataset statistics for segments and can
be inaccurate for individual samples. Therefore, in this paper, we propose to
generate positional queries dynamically conditioned on the cross-attention
scores and the localization information of the preceding layer. By doing so,
each query is aware of its previous focus, thus providing more accurate
positional guidance and encouraging the cross-attention consistency across the
decoder layers. In addition, we also propose an efficient way to deal with
high-resolution cross-attention by dynamically determining the contextual
tokens based on the low-resolution cross-attention maps to perform local
relation aggregation. Our overall framework termed FASeg (Focus-Aware semantic
Segmentation) provides a simple yet effective solution for semantic
segmentation. Extensive experiments on ADE20K and Cityscapes show that our
FASeg achieves state-of-the-art performance, e.g., obtaining 48.3% and 49.6%
mIoU respectively for single-scale inference on ADE20K validation set with
ResNet-50 and Swin-T backbones, and barely increases the computation
consumption from Mask2former. Source code will be made publicly available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Soft Threshold Ternary Networks</b></summary>
  <p><b>编号</b>：[177]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01234</p>
  <p><b>作者</b>：Weixiang Xu,  Xiangyu He,  Tianli Zhao,  Qinghao Hu,  Peisong Wang,  Jian Cheng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：method dramatically outperforms current state, automatically determine quantization intervals instead, 2 %) achieves new state, previous works estimate {\ delta, previous ternarized neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large neural networks are difficult to deploy on mobile devices because of
intensive computation and storage. To alleviate it, we study ternarization, a
balance between efficiency and accuracy that quantizes both weights and
activations into ternary values. In previous ternarized neural networks, a hard
threshold {\Delta} is introduced to determine quantization intervals. Although
the selection of {\Delta} greatly affects the training results, previous works
estimate {\Delta} via an approximation or treat it as a hyper-parameter, which
is suboptimal. In this paper, we present the Soft Threshold Ternary Networks
(STTN), which enables the model to automatically determine quantization
intervals instead of depending on a hard threshold. Concretely, we replace the
original ternary kernel with the addition of two binary kernels at training
time, where ternary values are determined by the combination of two
corresponding binary values. At inference time, we add up the two binary
kernels to obtain a single ternary kernel. Our method dramatically outperforms
current state-of-the-arts, lowering the performance gap between full-precision
networks and extreme low bit networks. Experiments on ImageNet with ResNet-18
(Top-1 66.2%) achieves new state-of-the-art.
Update: In this version, we further fine-tune the experimental
hyperparameters and training procedure. The latest STTN shows that ResNet-18
with ternary weights and ternary activations achieves up to 68.2% Top-1
accuracy on ImageNet. Code is available at: this http URL.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Animatable Neural Radiance Fields from Monocular RGB-D</b></summary>
  <p><b>编号</b>：[182]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01218</p>
  <p><b>作者</b>：Tiantian Wang,  Nikolaos Sarafianos,  Ming-Hsuan Yang,  Tony Tung</p>
  <p><b>备注</b>：16 pages</p>
  <p><b>关键词</b>：exploring shared canonical neural radiance fields, point cloud based code predicts details, method significantly outperforms existing works, novel views given monocular rgb, incomplete point clouds generated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper aims at representing animatable photo-realistic humans under novel
views and poses. Recent work has shown significant progress with dynamic scenes
by exploring shared canonical neural radiance fields. However learning a
user-controlled model for novel poses remains a challenging task. To tackle
this problem, we introduce a novel method to integrate observations across
frames and encode the appearance at each individual frame by utilizing the
human pose that models the body shape and point clouds which cover partial part
of the human as the input. Specifically, our method simultaneously learns a
shared set of latent codes anchored to the human pose among frames, and learns
an appearance-dependent code anchored to incomplete point clouds generated by
monocular RGB-D at each frame. A human pose-based code models the shape of the
performer whereas a point cloud based code predicts details and reasons about
missing structures at the unseen poses. To further recover non-visible regions
in query frames, we utilize a temporal transformer to integrate features of
points in query frames and tracked body points from automatically-selected key
frames. Experiments on various sequences of humans in motion show that our
method significantly outperforms existing works under unseen poses and novel
views given monocular RGB-D videos as input.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：Co-Teaching for Unsupervised Domain Adaptation and Expansion</b></summary>
  <p><b>编号</b>：[185]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01210</p>
  <p><b>作者</b>：Kaibin Tian,  Qijie Wei,  Xirong Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：knowledge distillation based ct, mixup based ct, crystally clear boundary, segmentation benchmarks justify, kdct transfers knowledge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Unsupervised Domain Adaptation (UDA) is known to trade a model's performance
on a source domain for improving its performance on a target domain. To resolve
the issue, Unsupervised Domain Expansion (UDE) has been proposed recently to
adapt the model for the target domain as UDA does, and in the meantime maintain
its performance on the source domain. For both UDA and UDE, a model tailored to
a given domain, let it be the source or the target domain, is assumed to well
handle samples from the given domain. We question the assumption by reporting
the existence of cross-domain visual ambiguity: Due to the lack of a crystally
clear boundary between the two domains, samples from one domain can be visually
close to the other domain. We exploit this finding and accordingly propose in
this paper Co-Teaching (CT) that consists of knowledge distillation based CT
(kdCT) and mixup based CT (miCT). Specifically, kdCT transfers knowledge from a
leader-teacher network and an assistant-teacher network to a student network,
so the cross-domain visual ambiguity will be better handled by the student.
Meanwhile, miCT further enhances the generalization ability of the student.
Comprehensive experiments on two image-classification benchmarks and two
driving-scene-segmentation benchmarks justify the viability of the proposed
method.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Rediscovery of the Effectiveness of Standard Convolution for Lightweight  Face Detection</b></summary>
  <p><b>编号</b>：[186]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01209</p>
  <p><b>作者</b>：Joonhyun Jeong,  Beomyoung Kim,  Joonsang Yu,  Youngjoon Yoo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：v1 ,- v2 ,- v3 )., heavily utilizes depthwise separable convolution layers, new feature aggregation method maximizing, surprisingly allows high efficiency compared, proposed detector eresfd obtained 80</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper analyses the design choices of face detection architecture that
improve efficiency between computation cost and accuracy. Specifically, we
re-examine the effectiveness of the standard convolutional block as a
lightweight backbone architecture on face detection. Unlike the current
tendency of lightweight architecture design, which heavily utilizes depthwise
separable convolution layers, we show that heavily channel-pruned standard
convolution layer can achieve better accuracy and inference speed when using a
similar parameter size. This observation is supported by the analyses
concerning the characteristics of the target data domain, face. Based on our
observation, we propose to employ ResNet with a highly reduced channel, which
surprisingly allows high efficiency compared to other mobile-friendly networks
(e.g., MobileNet-V1,-V2,-V3). From the extensive experiments, we show that the
proposed backbone can replace that of the state-of-the-art face detector with a
faster inference speed. Also, we further propose a new feature aggregation
method maximizing the detection performance. Our proposed detector EResFD
obtained 80.4% mAP on WIDER FACE Hard subset which only takes 37.7 ms for VGA
image inference in on CPU. Code will be available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Attribute Prototype Network for Any-Shot Learning</b></summary>
  <p><b>编号</b>：[187]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01208</p>
  <p><b>作者</b>：Wenjia Xu,  Yongqin Xian,  Jiuniu Wang,  Bernt Schiele,  Zeynep Akata</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2008.08290</p>
  <p><b>关键词</b>：semantic embedding layer learns global features, locality augmented image representations achieve, integrated attribute localization ability would, jointly learns discriminative global, ground truth part annotations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Any-shot image classification allows to recognize novel classes with only a
few or even zero samples. For the task of zero-shot learning, visual attributes
have been shown to play an important role, while in the few-shot regime, the
effect of attributes is under-explored. To better transfer attribute-based
knowledge from seen to unseen classes, we argue that an image representation
with integrated attribute localization ability would be beneficial for
any-shot, i.e. zero-shot and few-shot, image classification tasks. To this end,
we propose a novel representation learning framework that jointly learns
discriminative global and local features using only class-level attributes.
While a visual-semantic embedding layer learns global features, local features
are learned through an attribute prototype network that simultaneously
regresses and decorrelates attributes from intermediate features. Furthermore,
we introduce a zoom-in module that localizes and crops the informative regions
to encourage the network to learn informative features explicitly. We show that
our locality augmented image representations achieve a new state-of-the-art on
challenging benchmarks, i.e. CUB, AWA2, and SUN. As an additional benefit, our
model points to the visual evidence of the attributes in an image, confirming
the improved attribute localization ability of our image representation. The
attribute localization is evaluated quantitatively with ground truth part
annotations, qualitatively with visualizations, and through well-designed user
studies.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Unsupervised Change Detection Based on Image Reconstruction Loss</b></summary>
  <p><b>编号</b>：[190]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01200</p>
  <p><b>作者</b>：Hyeoncheol Noh,  Jingi Ju,  Minseok Seo,  Jongchan Park,  Dong-Geol Choi</p>
  <p><b>备注</b>：10 pages, 7 figures</p>
  <p><b>关键词</b>：various change detection benchmark datasets even though, temporal images shows high reconstruction loss, various unsupervised change detection methods, propose unsupervised change detection based, change detector showed significant performance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To train the change detector, bi-temporal images taken at different times in
the same area are used. However, collecting labeled bi-temporal images is
expensive and time consuming. To solve this problem, various unsupervised
change detection methods have been proposed, but they still require unlabeled
bi-temporal images. In this paper, we propose unsupervised change detection
based on image reconstruction loss using only unlabeled single temporal single
image. The image reconstruction model is trained to reconstruct the original
source image by receiving the source image and the photometrically transformed
source image as a pair. During inference, the model receives bi-temporal images
as the input, and tries to reconstruct one of the inputs. The changed region
between bi-temporal images shows high reconstruction loss. Our change detector
showed significant performance in various change detection benchmark datasets
even though only a single temporal single source image was used. The code and
trained models will be publicly available for reproducibility.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Revisiting Sliced Wasserstein on Images: From Vectorization to  Convolution</b></summary>
  <p><b>编号</b>：[193]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01188</p>
  <p><b>作者</b>：Khai Nguyen,  Nhat Ho</p>
  <p><b>备注</b>：34 pages, 12 figures, 10 tables</p>
  <p><b>关键词</b>：later slicing process becomes harder, variants via incorporating stride, training deep generative modeling, propose novel slicing methods, dimensional projected probability measures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The conventional sliced Wasserstein is defined between two probability
measures that have realizations as vectors. When comparing two probability
measures over images, practitioners first need to vectorize images and then
project them to one-dimensional space by using matrix multiplication between
the sample matrix and the projection matrix. After that, the sliced Wasserstein
is evaluated by averaging the two corresponding one-dimensional projected
probability measures. However, this approach has two limitations. The first
limitation is that the spatial structure of images is not captured efficiently
by the vectorization step; therefore, the later slicing process becomes harder
to gather the discrepancy information. The second limitation is memory
inefficiency since each slicing direction is a vector that has the same
dimension as the images. To address these limitations, we propose novel slicing
methods for sliced Wasserstein between probability measures over images that
are based on the convolution operators. We derive convolution sliced
Wasserstein (CSW) and its variants via incorporating stride, dilation, and
non-linear activation function into the convolution operators. We investigate
the metricity of CSW as well as its sample complexity, its computational
complexity, and its connection to conventional sliced Wasserstein distances.
Finally, we demonstrate the favorable performance of CSW over the conventional
sliced Wasserstein in comparing probability measures over images and in
training deep generative modeling on images.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：Revisiting a kNN-based Image Classification System with High-capacity  Storage</b></summary>
  <p><b>编号</b>：[194]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01186</p>
  <p><b>作者</b>：Kengo Nakata,  Youyang Ng,  Daisuke Miyashita,  Asuka Maki,  Yu-Chieh Lin,  Jun Deguchi</p>
  <p><b>备注</b>：16 pages, 7 figures, 6 tables</p>
  <p><b>关键词</b>：use deep neural networks, task incremental learning setting, existing image classification systems, imagenet dataset without fine, incremental learning scenarios</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In existing image classification systems that use deep neural networks, the
knowledge needed for image classification is implicitly stored in model
parameters. If users want to update this knowledge, then they need to fine-tune
the model parameters. Moreover, users cannot verify the validity of inference
results or evaluate the contribution of knowledge to the results. In this
paper, we investigate a system that stores knowledge for image classification,
such as image feature maps, labels, and original images, not in model
parameters but in external high-capacity storage. Our system refers to the
storage like a database when classifying input images. To increase knowledge,
our system updates the database instead of fine-tuning model parameters, which
avoids catastrophic forgetting in incremental learning scenarios. We revisit a
kNN (k-Nearest Neighbor) classifier and employ it in our system. By analyzing
the neighborhood samples referred by the kNN algorithm, we can interpret how
knowledge learned in the past is used for inference results. Our system
achieves 79.8% top-1 accuracy on the ImageNet dataset without fine-tuning model
parameters after pretraining, and 90.8% accuracy on the Split CIFAR-100 dataset
in the task incremental learning setting.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：Exploiting Temporal Relations on Radar Perception for Autonomous Driving</b></summary>
  <p><b>编号</b>：[195]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01184</p>
  <p><b>作者</b>：Peizhao Li,  Pu Wang,  Karl Berntorp,  Hongfu Liu</p>
  <p><b>备注</b>：To appear in CVPR 2022</p>
  <p><b>关键词</b>：autonomous driving using automotive radar sensors, objects within successive radar images, view radar image frames, recognizing surrounding objects, radar signals suffer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider the object recognition problem in autonomous driving using
automotive radar sensors. Comparing to Lidar sensors, radar is cost-effective
and robust in all-weather conditions for perception in autonomous driving.
However, radar signals suffer from low angular resolution and precision in
recognizing surrounding objects. To enhance the capacity of automotive radar,
in this work, we exploit the temporal information from successive ego-centric
bird-eye-view radar image frames for radar object recognition. We leverage the
consistency of an object's existence and attributes (size, orientation, etc.),
and propose a temporal relational layer to explicitly model the relations
between objects within successive radar images. In both object detection and
multiple object tracking, we show the superiority of our method compared to
several baseline approaches.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Shape-Pose Disentanglement using SE(3)-equivariant Vector Neurons</b></summary>
  <p><b>编号</b>：[203]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01159</p>
  <p><b>作者</b>：Oren Katzir,  Dani Lischinski,  Daniel Cohen-Or</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：semantically align different input shapes, resulting encoder produces pose, vector neuron networks, qualitative experiments validate, equivariant neural network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce an unsupervised technique for encoding point clouds into a
canonical shape representation, by disentangling shape and pose. Our encoder is
stable and consistent, meaning that the shape encoding is purely
pose-invariant, while the extracted rotation and translation are able to
semantically align different input shapes of the same class to a common
canonical pose. Specifically, we design an auto-encoder based on Vector Neuron
Networks, a rotation-equivariant neural network, whose layers we extend to
provide translation-equivariance in addition to rotation-equivariance only. The
resulting encoder produces pose-invariant shape encoding by construction,
enabling our approach to focus on learning a consistent canonical pose for a
class of objects. Quantitative and qualitative experiments validate the
superior stability and consistency of our approach.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：Indoor Navigation Assistance for Visually Impaired People via Dynamic  SLAM and Panoptic Segmentation with an RGB-D Sensor</b></summary>
  <p><b>编号</b>：[206]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01154</p>
  <p><b>作者</b>：Wenyan Ou,  Jiaming Zhang,  Kunyu Peng,  Kailun Yang,  Gerhard Jaworek,  Karin Müller,  Rainer Stiefelhagen</p>
  <p><b>备注</b>：Accepted to ICCHP 2022</p>
  <p><b>关键词</b>：sparse feature points extracted, prior dynamic object information, visually impaired people, several approaches achieve, detect dynamic information</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Exploring an unfamiliar indoor environment and avoiding obstacles is
challenging for visually impaired people. Currently, several approaches achieve
the avoidance of static obstacles based on the mapping of indoor scenes. To
solve the issue of distinguishing dynamic obstacles, we propose an assistive
system with an RGB-D sensor to detect dynamic information of a scene. Once the
system captures an image, panoptic segmentation is performed to obtain the
prior dynamic object information. With sparse feature points extracted from
images and the depth information, poses of the user can be estimated. After the
ego-motion estimation, the dynamic object can be identified and tracked. Then,
poses and speed of tracked dynamic objects can be estimated, which are passed
to the users through acoustic feedback.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：BNV-Fusion: Dense 3D Reconstruction using Bi-level Neural Volume Fusion</b></summary>
  <p><b>编号</b>：[210]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01139</p>
  <p><b>作者</b>：Kejie Li,  Yansong Tang,  Victor Adrian Prisacariu,  Philip H.S. Torr</p>
  <p><b>备注</b>：Accepted at CVPR 2022</p>
  <p><b>关键词</b>：incrementally integrate new depth maps, truncated signed distance function, global neural implicit representation, level neural volume fusion, neural implicit representations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Dense 3D reconstruction from a stream of depth images is the key to many
mixed reality and robotic applications. Although methods based on Truncated
Signed Distance Function (TSDF) Fusion have advanced the field over the years,
the TSDF volume representation is confronted with striking a balance between
the robustness to noisy measurements and maintaining the level of detail. We
present Bi-level Neural Volume Fusion (BNV-Fusion), which leverages recent
advances in neural implicit representations and neural rendering for dense 3D
reconstruction. In order to incrementally integrate new depth maps into a
global neural implicit representation, we propose a novel bi-level fusion
strategy that considers both efficiency and reconstruction quality by design.
We evaluate the proposed method on multiple datasets quantitatively and
qualitatively, demonstrating a significant improvement over existing methods.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：Adjusting for Bias with Procedural Data</b></summary>
  <p><b>编号</b>：[219]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01108</p>
  <p><b>作者</b>：Shesh Narayan Gupta,  Nicholas Bear Brown</p>
  <p><b>备注</b>：11 pages, 9 figures, 4 tables, presented in RISE 2022 Northeastern University</p>
  <p><b>关键词</b>：better classify poorly performing breeds, 3d rendered procedural data generation, good performance requires care, datasets become increasingly unwieldy, producing highly realistic images</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>3D softwares are now capable of producing highly realistic images that look
nearly indistinguishable from the real images. This raises the question: can
real datasets be enhanced with 3D rendered data? We investigate this question.
In this paper we demonstrate the use of 3D rendered data, procedural, data for
the adjustment of bias in image datasets. We perform error analysis of images
of animals which shows that the misclassification of some animal breeds is
largely a data issue. We then create procedural images of the poorly classified
breeds and that model further trained on procedural data can better classify
poorly performing breeds on real data. We believe that this approach can be
used for the enhancement of visual data for any underrepresented group,
including rare diseases, or any data bias potentially improving the accuracy
and fairness of models. We find that the resulting representations rival or
even out-perform those learned directly from real data, but that good
performance requires care in the 3D rendered procedural data generation. 3D
image dataset can be viewed as a compressed and organized copy of a real
dataset, and we envision a future where more and more procedural data
proliferate while datasets become increasingly unwieldy, missing, or private.
This paper suggests several techniques for dealing with visual representation
learning in such a future.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：Adversarially robust segmentation models learn perceptually-aligned  gradients</b></summary>
  <p><b>编号</b>：[226]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01099</p>
  <p><b>作者</b>：Pedro Sandoval-Segura</p>
  <p><b>备注</b>：12 pages, 3 figures</p>
  <p><b>关键词</b>：adversarially robust models exhibit gradients, place additional weight behind, producing plausible image inpaintings, trained semantic segmentation networks, semantic segmentation networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The effects of adversarial training on semantic segmentation networks has not
been thoroughly explored. While previous work has shown that
adversarially-trained image classifiers can be used to perform image synthesis,
we have yet to understand how best to leverage an adversarially-trained
segmentation network to do the same. Using a simple optimizer, we demonstrate
that adversarially-trained semantic segmentation networks can be used to
perform image inpainting and generation. Our experiments demonstrate that
adversarially-trained segmentation networks are more robust and indeed exhibit
perceptually-aligned gradients which help in producing plausible image
inpaintings. We seek to place additional weight behind the hypothesis that
adversarially robust models exhibit gradients that are more
perceptually-aligned with human vision. Through image synthesis, we argue that
perceptually-aligned gradients promote a better understanding of a neural
network's learned representations and aid in making neural networks more
interpretable.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：Faces: AI Blitz XIII Solutions</b></summary>
  <p><b>编号</b>：[234]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01081</p>
  <p><b>作者</b>：Andrew Melnik,  Eren Akbulut,  Jannik Sheikh,  Kira Loos,  Michael Buettner,  Tobias Lenze</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ai blitz xiii faces challenge hosted, team glados took second place, http url platform consisted, https url, sentiment classification</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>AI Blitz XIII Faces challenge hosted on this http URL platform consisted of
five problems: Sentiment Classification, Age Prediction, Mask Prediction, Face
Recognition, and Face De-Blurring. Our team GLaDOS took second place. Here we
present our solutions and results. Code implementation:
this https URL</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：ES6D: A Computation Efficient and Symmetry-Aware 6D Pose Regression  Framework</b></summary>
  <p><b>编号</b>：[235]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01080</p>
  <p><b>作者</b>：Ningkai Mo,  Wanshui Gan,  Naoto Yokoya,  Shifeng Chen</p>
  <p><b>备注</b>：Accepted by CVPR 2022</p>
  <p><b>关键词</b>：many relationship may lead, invariant pose distance metric, computation efficient regression framework, regression network converge, grouped primitives distance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, a computation efficient regression framework is presented for
estimating the 6D pose of rigid objects from a single RGB-D image, which is
applicable to handling symmetric objects. This framework is designed in a
simple architecture that efficiently extracts point-wise features from RGB-D
data using a fully convolutional network, called XYZNet, and directly regresses
the 6D pose without any post refinement. In the case of symmetric object, one
object has multiple ground-truth poses, and this one-to-many relationship may
lead to estimation ambiguity. In order to solve this ambiguity problem, we
design a symmetry-invariant pose distance metric, called average (maximum)
grouped primitives distance or A(M)GPD. The proposed A(M)GPD loss can make the
regression network converge to the correct state, i.e., all minima in the
A(M)GPD loss surface are mapped to the correct poses. Extensive experiments on
YCB-Video and T-LESS datasets demonstrate the proposed framework's
substantially superior performance in top accuracy and low computational cost.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：In Rain or Shine: Understanding and Overcoming Dataset Bias for  Improving Robustness Against Weather Corruptions for Autonomous Vehicles</b></summary>
  <p><b>编号</b>：[241]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01062</p>
  <p><b>作者</b>：Aboli Marathe,  Rahee Walambe,  Ketan Kotecha,  Deepak Kumar Jain</p>
  <p><b>备注</b>：Under review</p>
  <p><b>关键词</b>：autonomous driving tasks exhibit biases due, may prove extremely high risk, simple yet effective od framework, proposed techniques outperform baseline methods, synthetic image corruption technique</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Several popular computer vision (CV) datasets, specifically employed for
Object Detection (OD) in autonomous driving tasks exhibit biases due to a range
of factors including weather and lighting conditions. These biases may impair a
model's generalizability, rendering it ineffective for OD in novel and unseen
datasets. Especially, in autonomous driving, it may prove extremely high risk
and unsafe for the vehicle and its surroundings. This work focuses on
understanding these datasets better by identifying such "good-weather" bias.
Methods to mitigate such bias which allows the OD models to perform better and
improve the robustness are also demonstrated. A simple yet effective OD
framework for studying bias mitigation is proposed. Using this framework, the
performance on popular datasets is analyzed and a significant difference in
model performance is observed. Additionally, a knowledge transfer technique and
a synthetic image corruption technique are proposed to mitigate the identified
bias. Finally, using the DAWN dataset, the findings are validated on the OD
task, demonstrating the effectiveness of our techniques in mitigating
real-world "good-weather" bias. The experiments show that the proposed
techniques outperform baseline methods by averaged fourfold improvement.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：Style-Based Global Appearance Flow for Virtual Try-On</b></summary>
  <p><b>编号</b>：[244]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01046</p>
  <p><b>作者</b>：Sen He,  Yi-Zhe Song,  Tao Xiang</p>
  <p><b>备注</b>：CVPR 2022</p>
  <p><b>关键词</b>：novel global appearance flow estimation model, local appearance flow estimation model, prior methods typically adopt, method achieves new state, appearance flow estimation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image-based virtual try-on aims to fit an in-shop garment into a clothed
person image. To achieve this, a key step is garment warping which spatially
aligns the target garment with the corresponding body parts in the person
image. Prior methods typically adopt a local appearance flow estimation model.
They are thus intrinsically susceptible to difficult body poses/occlusions and
large mis-alignments between person and garment images (see
Fig.~\ref{fig:fig1}). To overcome this limitation, a novel global appearance
flow estimation model is proposed in this work. For the first time, a StyleGAN
based architecture is adopted for appearance flow estimation. This enables us
to take advantage of a global style vector to encode a whole-image context to
cope with the aforementioned challenges. To guide the StyleGAN flow generator
to pay more attention to local garment deformation, a flow refinement module is
introduced to add local context. Experiment results on a popular virtual try-on
benchmark show that our method achieves new state-of-the-art performance. It is
particularly effective in a `in-the-wild' application scenario where the
reference image is full-body resulting in a large mis-alignment with the
garment image (Fig.~\ref{fig:fig1} Top). Code is available at:
\url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：Distortion-Aware Self-Supervised 360° Depth Estimation from A  Single Equirectangular Projection Image</b></summary>
  <p><b>编号</b>：[247]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01027</p>
  <p><b>作者</b>：Yuya Hasegawa,  Ikehata Satoshi,  Kiyoharu Aizawa</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：single 360 ° image depth prediction, supervised learning using motion pictures, 360 ° single image, uses cube map projection, produce six perspective images</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>360° images are widely available over the last few years. This paper
proposes a new technique for single 360° image depth prediction under open
environments. Depth prediction from a 360° single image is not easy for
two reasons. One is the limitation of supervision datasets - the currently
available dataset is limited to indoor scenes. The other is the problems caused
by Equirectangular Projection Format (ERP), commonly used for 360° images,
that are coordinate and distortion. There is only one method existing that uses
cube map projection to produce six perspective images and apply self-supervised
learning using motion pictures for perspective depth prediction to deal with
these problems. Different from the existing method, we directly use the ERP
format. We propose a framework of direct use of ERP with coordinate conversion
of correspondences and distortion-aware upsampling module to deal with the ERP
related problems and extend a self-supervised learning method for open
environments. For the experiments, we firstly built a dataset for the
evaluation, and quantitatively evaluate the depth prediction in outdoor scenes.
We show that it outperforms the state-of-the-art technique</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：STCrowd: A Multimodal Dataset for Pedestrian Perception in Crowded  Scenes</b></summary>
  <p><b>编号</b>：[248]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01026</p>
  <p><b>作者</b>：Peishan Cong,  Xinge Zhu,  Feng Qiao,  Yiming Ren,  Xidong Peng,  Yuenan Hou,  Lan Xu,  Ruigang Yang,  Dinesh Manocha,  Yuexin Ma</p>
  <p><b>备注</b>：accepted at CVPR2022</p>
  <p><b>关键词</b>：reliable pedestrian perception system especially, better evaluate pedestrian perception algorithms, provide synchronized lidar point clouds, situation becomes even worse, aware hierarchical heatmap aggregation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Accurately detecting and tracking pedestrians in 3D space is challenging due
to large variations in rotations, poses and scales. The situation becomes even
worse for dense crowds with severe occlusions. However, existing benchmarks
either only provide 2D annotations, or have limited 3D annotations with
low-density pedestrian distribution, making it difficult to build a reliable
pedestrian perception system especially in crowded scenes. To better evaluate
pedestrian perception algorithms in crowded scenarios, we introduce a
large-scale multimodal dataset,STCrowd. Specifically, in STCrowd, there are a
total of 219 K pedestrian instances and 20 persons per frame on average, with
various levels of occlusion. We provide synchronized LiDAR point clouds and
camera images as well as their corresponding 3D labels and joint IDs. STCrowd
can be used for various tasks, including LiDAR-only, image-only, and
sensor-fusion based pedestrian detection and tracking. We provide baselines for
most of the tasks. In addition, considering the property of sparse global
distribution and density-varying local distribution of pedestrians, we further
propose a novel method, Density-aware Hierarchical heatmap Aggregation (DHA),
to enhance pedestrian perception in crowded scenes. Extensive experiments show
that our new method achieves state-of-the-art performance for pedestrian
detection on various datasets.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：TransRAC: Encoding Multi-scale Temporal Correlation with Transformers  for Repetitive Action Counting</b></summary>
  <p><b>编号</b>：[251]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01018</p>
  <p><b>作者</b>：Huazhang Hu,  Sixun Dong,  Yiqun Zhao,  Dongze Lian,  Zhengxin Li,  Shenghua Gao</p>
  <p><b>备注</b>：(Revised) CVPR 2022 Oral. RepCount dataset: this https URL , Code: this https URL</p>
  <p><b>关键词</b>：scale repetitive action counting dataset covering, repetitive action counting towards, performing repetitive action counting, proposed method outperforms state, unseen dataset without fine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Counting repetitive actions are widely seen in human activities such as
physical exercise. Existing methods focus on performing repetitive action
counting in short videos, which is tough for dealing with longer videos in more
realistic scenarios. In the data-driven era, the degradation of such
generalization capability is mainly attributed to the lack of long video
datasets. To complement this margin, we introduce a new large-scale repetitive
action counting dataset covering a wide variety of video lengths, along with
more realistic situations where action interruption or action inconsistencies
occur in the video. Besides, we also provide a fine-grained annotation of the
action cycles instead of just counting annotation along with a numerical value.
Such a dataset contains 1,451 videos with about 20,000 annotations, which is
more challenging. For repetitive action counting towards more realistic
scenarios, we further propose encoding multi-scale temporal correlation with
transformers that can take into account both performance and efficiency.
Furthermore, with the help of fine-grained annotation of action cycles, we
propose a density map regression-based method to predict the action period,
which yields better performance with sufficient interpretability. Our proposed
method outperforms state-of-the-art methods on all datasets and also achieves
better performance on the unseen dataset without fine-tuning. The dataset and
code are available.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：Region-aware Attention for Image Inpainting</b></summary>
  <p><b>编号</b>：[256]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01004</p>
  <p><b>作者</b>：Zhilin Huang,  Chujun Qin,  Zhenyu Weng,  Yuesheng Zhu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：methodscan generate semantically plausible results, generate blurry contents since, paris streetview datasets validate, also avoids information redundancy, based image inpainting methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent attention-based image inpainting methods have made inspiring progress
by modeling long-range dependencies within a single image. However, they tend
to generate blurry contents since the correlation between each pixel pairs is
always misled by ill-predicted features in holes. To handle this problem, we
propose a novel region-aware attention (RA) module. By avoiding the directly
calculating corralation between each pixel pair in a single samples and
considering the correlation between different samples, the misleading of
invalid information in holes can be avoided. Meanwhile, a learnable region
dictionary (LRD) is introduced to store important information in the entire
dataset, which not only simplifies correlation modeling, but also avoids
information redundancy. By applying RA in our architecture, our methodscan
generate semantically plausible results with realistic details. Extensive
experiments on CelebA, Places2 and Paris StreetView datasets validate the
superiority of our method compared with existing methods.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：Improving Vision Transformers by Revisiting High-frequency Components</b></summary>
  <p><b>编号</b>：[261]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00993</p>
  <p><b>作者</b>：Jiawang Bai,  Li Yuan,  Shu-Tao Xia,  Shuicheng Yan,  Zhifeng Li,  Wei Liu</p>
  <p><b>备注</b>：18 pages, 7 figures</p>
  <p><b>关键词</b>：training convolutional neural network, images via adversarial training, scale training set, training vision transformer, shown promising effectiveness</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The transformer models have shown promising effectiveness in dealing with
various vision tasks. However, compared with training Convolutional Neural
Network (CNN) models, training Vision Transformer (ViT) models is more
difficult and relies on the large-scale training set. To explain this
observation we make a hypothesis that ViT models are less effective in
capturing the high-frequency components of images than CNN models, and verify
it by a frequency analysis. Inspired by this finding, we first investigate the
effects of existing techniques for improving ViT models from a new frequency
perspective, and find that the success of some techniques (e.g., RandAugment)
can be attributed to the better usage of the high-frequency components. Then,
to compensate for this insufficient ability of ViT models, we propose HAT,
which directly augments high-frequency components of images via adversarial
training. We show that HAT can consistently boost the performance of various
ViT models (e.g., +1.2% for ViT-B, +0.5% for Swin-B), and especially enhance
the advanced model VOLO-D5 to 87.3% that only uses ImageNet-1K data, and the
superiority can also be maintained on out-of-distribution data and transferred
to downstream tasks.</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：POS-BERT: Point Cloud One-Stage BERT Pre-Training</b></summary>
  <p><b>编号</b>：[263]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00989</p>
  <p><b>作者</b>：Kexue Fu,  Peng Gao,  ShaoLei Liu,  Renrui Zhang,  Yu Qiao,  Manning Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：significantly improved many downstream tasks, fixed discrete variational autoencoder, recover masked patches information, dynamically updated momentum encoder, dynamic supervision signal along</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, the pre-training paradigm combining Transformer and masked language
modeling has achieved tremendous success in NLP, images, and point clouds, such
as BERT. However, directly extending BERT from NLP to point clouds requires
training a fixed discrete Variational AutoEncoder (dVAE) before pre-training,
which results in a complex two-stage method called Point-BERT. Inspired by BERT
and MoCo, we propose POS-BERT, a one-stage BERT pre-training method for point
clouds. Specifically, we use the mask patch modeling (MPM) task to perform
point cloud pre-training, which aims to recover masked patches information
under the supervision of the corresponding tokenizer output. Unlike Point-BERT,
its tokenizer is extra-trained and frozen. We propose to use the dynamically
updated momentum encoder as the tokenizer, which is updated and outputs the
dynamic supervision signal along with the training process. Further, in order
to learn high-level semantic representation, we combine contrastive learning to
maximize the class token consistency between different transformation point
clouds. Extensive experiments have demonstrated that POS-BERT can extract
high-quality pre-training features and promote downstream tasks to improve
performance. Using the pre-training model without any fine-tuning to extract
features and train linear SVM on ModelNet40, POS-BERT achieves the
state-of-the-art classification accuracy, which exceeds Point-BERT by 3.5\%. In
addition, our approach has significantly improved many downstream tasks, such
as fine-tuned classification, few-shot classification, part segmentation. The
code and trained-models will be available at:
\url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：BinsFormer: Revisiting Adaptive Bins for Monocular Depth Estimation</b></summary>
  <p><b>编号</b>：[265]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00987</p>
  <p><b>作者</b>：Zhenyu Li,  Xuyang Wang,  Xianming Liu,  Junjun Jiang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：art monocular depth estimation methods, novel framework called binsformer, implicitly learn useful information, extra scene understanding query, auxiliary environment classification task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Monocular depth estimation is a fundamental task in computer vision and has
drawn increasing attention. Recently, some methods reformulate it as a
classification-regression task to boost the model performance, where continuous
depth is estimated via a linear combination of predicted probability
distributions and discrete bins. In this paper, we present a novel framework
called BinsFormer, tailored for the classification-regression-based depth
estimation. It mainly focuses on two crucial components in the specific task:
1) proper generation of adaptive bins and 2) sufficient interaction between
probability distribution and bins predictions. To specify, we employ the
Transformer decoder to generate bins, novelly viewing it as a direct set-to-set
prediction problem. We further integrate a multi-scale decoder structure to
achieve a comprehensive understanding of spatial geometry information and
estimate depth maps in a coarse-to-fine manner. Moreover, an extra scene
understanding query is proposed to improve the estimation accuracy, which turns
out that models can implicitly learn useful information from an auxiliary
environment classification task. Extensive experiments on the KITTI, NYU, and
SUN RGB-D datasets demonstrate that BinsFormer surpasses state-of-the-art
monocular depth estimation methods with prominent margins. Code and pretrained
models will be made publicly available at
\url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：Question-Driven Graph Fusion Network For Visual Question Answering</b></summary>
  <p><b>编号</b>：[271]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00975</p>
  <p><b>作者</b>：Yuxi Qian,  Yuncong Hu,  Ruonan Wang,  Fangxiang Feng,  Xiaojie Wang</p>
  <p><b>备注</b>：Accepted by ICME 2022</p>
  <p><b>关键词</b>：inevitably introduces irrelevant information brought, driven graph fusion network, explored various visual relationships, three graph attention networks, novel graph aggregation method</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing Visual Question Answering (VQA) models have explored various visual
relationships between objects in the image to answer complex questions, which
inevitably introduces irrelevant information brought by inaccurate object
detection and text grounding. To address the problem, we propose a
Question-Driven Graph Fusion Network (QD-GFN). It first models semantic,
spatial, and implicit visual relations in images by three graph attention
networks, then question information is utilized to guide the aggregation
process of the three graphs, further, our QD-GFN adopts an object filtering
mechanism to remove question-irrelevant objects contained in the image.
Experiment results demonstrate that our QD-GFN outperforms the prior
state-of-the-art on both VQA 2.0 and VQA-CP v2 datasets. Further analysis shows
that both the novel graph aggregation method and object filtering mechanism
play a significant role in improving the performance of the model.</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：Neural Global Shutter: Learn to Restore Video from a Rolling Shutter  Camera with Global Reset Feature</b></summary>
  <p><b>编号</b>：[272]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00974</p>
  <p><b>作者</b>：Zhixiang Wang,  Xiang Ji,  Jia-Bin Huang,  Shin'ichi Satoh,  Xiao Zhou,  Yinqiang Zheng</p>
  <p><b>备注</b>：CVPR2022, this https URL</p>
  <p><b>关键词</b>：computer vision systems assume distortion, existing work relies heavily, costly explicit motion estimation, restore clean global shutter, investigate using rolling shutter</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Most computer vision systems assume distortion-free images as inputs. The
widely used rolling-shutter (RS) image sensors, however, suffer from geometric
distortion when the camera and object undergo motion during capture. Extensive
researches have been conducted on correcting RS distortions. However, most of
the existing work relies heavily on the prior assumptions of scenes or motions.
Besides, the motion estimation steps are either oversimplified or
computationally inefficient due to the heavy flow warping, limiting their
applicability. In this paper, we investigate using rolling shutter with a
global reset feature (RSGR) to restore clean global shutter (GS) videos. This
feature enables us to turn the rectification problem into a deblur-like one,
getting rid of inaccurate and costly explicit motion estimation. First, we
build an optic system that captures paired RSGR/GS videos. Second, we develop a
novel algorithm incorporating spatial and temporal designs to correct the
spatial-varying RSGR distortion. Third, we demonstrate that existing
image-to-image translation algorithms can recover clean GS videos from
distorted RSGR inputs, yet our algorithm achieves the best performance with the
specific designs. Our rendered results are not only visually appealing but also
beneficial to downstream tasks. Compared to the state-of-the-art RS solution,
our RSGR solution is superior in both effectiveness and efficiency. Considering
it is easy to realize without changing the hardware, we believe our RSGR
solution can potentially replace the RS solution in taking distortion-free
videos with low noise and low budget.</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：Kernel Extreme Learning Machine Optimized by the Sparrow Search  Algorithm for Hyperspectral Image Classification</b></summary>
  <p><b>编号</b>：[273]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00973</p>
  <p><b>作者</b>：Zhixin Yan,  Jiawei Huang,  Kehua Xiang</p>
  <p><b>备注</b>：17 pages</p>
  <p><b>关键词</b>：multiscale fusion feature hyperspectral image classification method, new swarm intelligence optimization method, hyperspectral image classification algorithm, kernel extreme learning machine, strong global search capability</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To improve the classification performance and generalization ability of the
hyperspectral image classification algorithm, this paper uses Multi-Scale Total
Variation (MSTV) to extract the spectral features, local binary pattern (LBP)
to extract spatial features, and feature superposition to obtain the fused
features of hyperspectral images. A new swarm intelligence optimization method
with high convergence and strong global search capability, the Sparrow Search
Algorithm (SSA), is used to optimize the kernel parameters and regularization
coefficients of the Kernel Extreme Learning Machine (KELM). In summary, a
multiscale fusion feature hyperspectral image classification method (MLS-KELM)
is proposed in this paper. The Indian Pines, Pavia University and Houston 2013
datasets were selected to validate the classification performance of MLS-KELM,
and the method was applied to ZY1-02D hyperspectral data. The experimental
results show that MLS-KELM has better classification performance and
generalization ability compared with other popular classification methods, and
MLS-KELM shows its strong robustness in the small sample case.</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：DST: Dynamic Substitute Training for Data-free Black-box Attack</b></summary>
  <p><b>编号</b>：[274]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00972</p>
  <p><b>作者</b>：Wenxuan Wang,  Xuelin Qian,  Yanwei Fu,  Xiangyang Xue</p>
  <p><b>备注</b>：Accepted by CVPR2022</p>
  <p><b>关键词</b>：adaptively generate optimal substitute model structure via, novel dynamic substitute training attack method, based structure information learning constrain, dynamic substitute structure learning strategy, free black box attack scenario</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the wide applications of deep neural network models in various computer
vision tasks, more and more works study the model vulnerability to adversarial
examples. For data-free black box attack scenario, existing methods are
inspired by the knowledge distillation, and thus usually train a substitute
model to learn knowledge from the target model using generated data as input.
However, the substitute model always has a static network structure, which
limits the attack ability for various target models and tasks. In this paper,
we propose a novel dynamic substitute training attack method to encourage
substitute model to learn better and faster from the target model.
Specifically, a dynamic substitute structure learning strategy is proposed to
adaptively generate optimal substitute model structure via a dynamic gate
according to different target models and tasks. Moreover, we introduce a
task-driven graph-based structure information learning constrain to improve the
quality of generated training data, and facilitate the substitute model
learning structural relationships from the target model multiple outputs.
Extensive experiments have been conducted to verify the efficacy of the
proposed attack method, which can achieve better performance compared with the
state-of-the-art competitors on several datasets.</p>
  </details>
</details>
<details>
  <summary>83. <b>标题：AdaFace: Quality Adaptive Margin for Face Recognition</b></summary>
  <p><b>编号</b>：[276]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00964</p>
  <p><b>作者</b>：Minchul Kim,  Anil K. Jain,  Xiaoming Liu</p>
  <p><b>备注</b>：to be published in CVPR2022 (Oral)</p>
  <p><b>关键词</b>：low quality face datasets, introduce another aspect, extensive experiments show, face recognition performance, different difficulties based</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recognition in low quality face datasets is challenging because facial
attributes are obscured and degraded. Advances in margin-based loss functions
have resulted in enhanced discriminability of faces in the embedding space.
Further, previous studies have studied the effect of adaptive losses to assign
more importance to misclassified (hard) examples. In this work, we introduce
another aspect of adaptiveness in the loss function, namely the image quality.
We argue that the strategy to emphasize misclassified samples should be
adjusted according to their image quality. Specifically, the relative
importance of easy or hard samples should be based on the sample's image
quality. We propose a new loss function that emphasizes samples of different
difficulties based on their image quality. Our method achieves this in the form
of an adaptive margin function by approximating the image quality with feature
norms. Extensive experiments show that our method, AdaFace, improves the face
recognition performance over the state-of-the-art (SoTA) on four datasets
(IJB-B, IJB-C, IJB-S and TinyFace). Code and models are released in
this https URL.</p>
  </details>
</details>
<details>
  <summary>84. <b>标题：A Sentinel-2 multi-year, multi-country benchmark dataset for crop  classification and segmentation with deep learning</b></summary>
  <p><b>编号</b>：[282]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00951</p>
  <p><b>作者</b>：Dimitrios Sykas,  Maria Sdraka,  Dimitrios Zografakis,  Ioannis Papoutsis</p>
  <p><b>备注</b>：This work has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. Copyright may be transferred without notice, after which this version may no longer be accessible</p>
  <p><b>关键词</b>：2 based time series multi country benchmark dataset, new crop type taxonomy across europe, temporal variability across different years, harmonizing country wide labels, indicative crop classification scheme</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work we introduce Sen4AgriNet, a Sentinel-2 based time series multi
country benchmark dataset, tailored for agricultural monitoring applications
with Machine and Deep Learning. Sen4AgriNet dataset is annotated from farmer
declarations collected via the Land Parcel Identification System (LPIS) for
harmonizing country wide labels. These declarations have only recently been
made available as open data, allowing for the first time the labeling of
satellite imagery from ground truth data. We proceed to propose and standardise
a new crop type taxonomy across Europe that address Common Agriculture Policy
(CAP) needs, based on the Food and Agriculture Organization (FAO) Indicative
Crop Classification scheme. Sen4AgriNet is the only multi-country, multi-year
dataset that includes all spectral information. It is constructed to cover the
period 2016-2020 for Catalonia and France, while it can be extended to include
additional countries. Currently, it contains 42.5 million parcels, which makes
it significantly larger than other available archives. We extract two
sub-datasets to highlight its value for diverse Deep Learning applications; the
Object Aggregated Dataset (OAD) and the Patches Assembled Dataset (PAD). OAD
capitalizes zonal statistics of each parcel, thus creating a powerful
label-to-features instance for classification algorithms. On the other hand,
PAD structure generalizes the classification problem to parcel extraction and
semantic segmentation and labeling. The PAD and OAD are examined under three
different scenarios to showcase and model the effects of spatial and temporal
variability across different years and different countries.</p>
  </details>
</details>
<details>
  <summary>85. <b>标题：Matching Feature Sets for Few-Shot Image Classification</b></summary>
  <p><b>编号</b>：[283]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00949</p>
  <p><b>作者</b>：Arman Afrasiyabi,  Hugo Larochelle,  Jean-François Lalonde,  Christian Gagné</p>
  <p><b>备注</b>：International Conference on Computer Vision and Pattern Recognition (CVPR), 2022</p>
  <p><b>关键词</b>：attention mechanisms inside existing encoder architectures, shot classification methods also mostly follow, single feature vector per input image, shot datasets -- namely miniimagenet, adapt existing feature extractors</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In image classification, it is common practice to train deep networks to
extract a single feature vector per input image. Few-shot classification
methods also mostly follow this trend. In this work, we depart from this
established direction and instead propose to extract sets of feature vectors
for each image. We argue that a set-based representation intrinsically builds a
richer representation of images from the base classes, which can subsequently
better transfer to the few-shot classes. To do so, we propose to adapt existing
feature extractors to instead produce sets of feature vectors from images. Our
approach, dubbed SetFeat, embeds shallow self-attention mechanisms inside
existing encoder architectures. The attention modules are lightweight, and as
such our method results in encoders that have approximately the same number of
parameters as their original versions. During training and inference, a
set-to-set matching metric is used to perform image classification. The
effectiveness of our proposed architecture and metrics is demonstrated via
thorough experiments on standard few-shot datasets -- namely miniImageNet,
tieredImageNet, and CUB -- in both the 1- and 5-shot scenarios. In all cases
but one, our method outperforms the state-of-the-art.</p>
  </details>
</details>
<details>
  <summary>86. <b>标题：Progressive Minimal Path Method with Embedded CNN</b></summary>
  <p><b>编号</b>：[284]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00944</p>
  <p><b>作者</b>：Wei Liao</p>
  <p><b>备注</b>：Accepted to IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, 2022</p>
  <p><b>关键词</b>：cnns use strong image features, cnns employ learned image features, provides strong geometric priors, embedding convolutional neural networks, cnn achieves better performance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose Path-CNN, a method for the segmentation of centerlines of tubular
structures by embedding convolutional neural networks (CNNs) into the
progressive minimal path method. Minimal path methods are widely used for
topology-aware centerline segmentation, but usually these methods rely on weak,
hand-tuned image features. In contrast, CNNs use strong image features which
are learned automatically from images. But CNNs usually do not take the
topology of the results into account, and often require a large amount of
annotations for training. We integrate CNNs into the minimal path method, so
that both techniques benefit from each other: CNNs employ learned image
features to improve the determination of minimal paths, while the minimal path
method ensures the correct topology of the segmented centerlines, provides
strong geometric priors to increase the performance of CNNs, and reduces the
amount of annotations for the training of CNNs significantly. Our method has
lower hardware requirements than many recent methods. Qualitative and
quantitative comparison with other methods shows that Path-CNN achieves better
performance, especially when dealing with tubular structures with complex
shapes in challenging environments.</p>
  </details>
</details>
<details>
  <summary>87. <b>标题：TripleNet: A Low Computing Power Platform of Low-Parameter Network</b></summary>
  <p><b>编号</b>：[285]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00943</p>
  <p><b>作者</b>：Rui-Yang Ju,  Ting-Yu Lin,  Jia-Hao Jian,  Jen-Shiun Chiang</p>
  <p><b>备注</b>：4 pages, 2 figures</p>
  <p><b>关键词</b>：triplenet uses three different convolutional layers combined, improved convolutional neural network based, lightweight convolutional neural network model, neural network models based, convolutional neural network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the excellent performance of deep learning technology in the field of
computer vision, convolutional neural network (CNN) architecture has become the
main backbone of computer vision task technology. With the widespread use of
mobile devices, neural network models based on platforms with low computing
power are gradually being paid attention. This paper proposes a lightweight
convolutional neural network model, TripleNet, an improved convolutional neural
network based on HarDNet and ThreshNet, inheriting the advantages of small
memory usage and low power consumption of the mentioned two models. TripleNet
uses three different convolutional layers combined into a new model
architecture, which has less number of parameters than that of HarDNet and
ThreshNet. CIFAR-10 and SVHN datasets were used for image classification by
employing HarDNet, ThreshNet, and our proposed TripleNet for verification.
Experimental results show that, compared with HarDNet, TripleNet's parameters
are reduced by 66% and its accuracy rate is increased by 18%; compared with
ThreshNet, TripleNet's parameters are reduced by 37% and its accuracy rate is
increased by 5%.</p>
  </details>
</details>
<details>
  <summary>88. <b>标题：A-ACT: Action Anticipation through Cycle Transformations</b></summary>
  <p><b>编号</b>：[286]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00942</p>
  <p><b>作者</b>：Akash Gupta,  Jingen Liu,  Liefeng Bo,  Amit K. Roy-Chowdhury,  Tao Mei</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：action anticipation model learned using, question worth pondering upon, simulating possible scenarios based, cycle transformation performs favorably, anticipating future action directly</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While action anticipation has garnered a lot of research interest recently,
most of the works focus on anticipating future action directly through observed
visual cues only. In this work, we take a step back to analyze how the human
capability to anticipate the future can be transferred to machine learning
algorithms. To incorporate this ability in intelligent systems a question worth
pondering upon is how exactly do we anticipate? Is it by anticipating future
actions from past experiences? Or is it by simulating possible scenarios based
on cues from the present? A recent study on human psychology explains that, in
anticipating an occurrence, the human brain counts on both systems. In this
work, we study the impact of each system for the task of action anticipation
and introduce a paradigm to integrate them in a learning framework. We believe
that intelligent systems designed by leveraging the psychological anticipation
models will do a more nuanced job at the task of human action prediction.
Furthermore, we introduce cyclic transformation in the temporal dimension in
feature and semantic label space to instill the human ability of reasoning of
past actions based on the predicted future. Experiments on Epic-Kitchen,
Breakfast, and 50Salads dataset demonstrate that the action anticipation model
learned using a combination of the two systems along with the cycle
transformation performs favorably against various state-of-the-art approaches.</p>
  </details>
</details>
<details>
  <summary>89. <b>标题：SinNeRF: Training Neural Radiance Fields on Complex Scenes from a Single  Image</b></summary>
  <p><b>编号</b>：[292]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00928</p>
  <p><b>作者</b>：Dejia Xu,  Yifan Jiang,  Peihao Wang,  Zhiwen Fan,  Humphrey Shi,  Zhangyang Wang</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：local light field fusion dataset, dense covers largely prohibits, including nerf synthetic dataset, realistically complex visual scenes, propagate geometry pseudo labels</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite the rapid development of Neural Radiance Field (NeRF), the necessity
of dense covers largely prohibits its wider applications. While several recent
works have attempted to address this issue, they either operate with sparse
views (yet still, a few of them) or on simple objects/scenes. In this work, we
consider a more ambitious task: training neural radiance field, over
realistically complex visual scenes, by "looking only once", i.e., using only a
single view. To attain this goal, we present a Single View NeRF (SinNeRF)
framework consisting of thoughtfully designed semantic and geometry
regularizations. Specifically, SinNeRF constructs a semi-supervised learning
process, where we introduce and propagate geometry pseudo labels and semantic
pseudo labels to guide the progressive training process. Extensive experiments
are conducted on complex scene benchmarks, including NeRF synthetic dataset,
Local Light Field Fusion dataset, and DTU dataset. We show that even without
pre-training on multi-view datasets, SinNeRF can yield photo-realistic
novel-view synthesis results. Under the single image setting, SinNeRF
significantly outperforms the current state-of-the-art NeRF baselines in all
cases. Project page: this https URL</p>
  </details>
</details>
<details>
  <summary>90. <b>标题：Word separation in continuous sign language using isolated signs and  post-processing</b></summary>
  <p><b>编号</b>：[295]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00923</p>
  <p><b>作者</b>：Razieh Rastgoo,  Kourosh Kiani,  Sergio Escalera</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：continuous sign videos confirm, continuous sign language recognition, isolated sign language recognition, isolated sign boundaries detection, softmax outputs obtained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Continuous Sign Language Recognition (CSLR) is a long challenging task in
Computer Vision due to the difficulties in detecting the explicit boundaries
between the words in a sign sentence. To deal with this challenge, we propose a
two-stage model. In the first stage, the predictor model, which includes a
combination of CNN, SVD, and LSTM, is trained with the isolated signs. In the
second stage, we apply a post-processing algorithm to the Softmax outputs
obtained from the first part of the model in order to separate the isolated
signs in the continuous signs. Due to the lack of a large dataset, including
both the sign sequences and the corresponding isolated signs, two public
datasets in Isolated Sign Language Recognition (ISLR), RKS-PERSIANSIGN and
ASLVID, are used for evaluation. Results of the continuous sign videos confirm
the efficiency of the proposed model to deal with isolated sign boundaries
detection.</p>
  </details>
</details>
<details>
  <summary>91. <b>标题：Deep Algebraic Fitting for Multiple Circle Primitives Extraction from  Raw Point Clouds</b></summary>
  <p><b>编号</b>：[297]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00920</p>
  <p><b>作者</b>：Zeyong Wei,  Honghua Chen,  Hao Tang,  Qian Xie,  Mingqiang Wei,  Jun Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：deep feature based circle parameter learning module, end point cloud circle algebraic fitting network, scanned point clouds exhibit clear improvements, existing circle extraction methods either, boundary point feature learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The shape of circle is one of fundamental geometric primitives of man-made
engineering objects. Thus, extraction of circles from scanned point clouds is a
quite important task in 3D geometry data processing. However, existing circle
extraction methods either are sensitive to the quality of raw point clouds when
classifying circle-boundary points, or require well-designed fitting functions
when regressing circle parameters. To relieve the challenges, we propose an
end-to-end Point Cloud Circle Algebraic Fitting Network (Circle-Net) based on a
synergy of deep circle-boundary point feature learning and weighted algebraic
fitting. First, we design a circle-boundary learning module, which considers
local and global neighboring contexts of each point, to detect all potential
circle-boundary points. Second, we develop a deep feature based circle
parameter learning module for weighted algebraic fitting, without designing any
weight metric, to avoid the influence of outliers during fitting. Unlike most
of the cutting-edge circle extraction wisdoms, the proposed
classification-and-fitting modules are originally co-trained with a
comprehensive loss to enhance the quality of extracted circles.Comparisons on
the established dataset and real-scanned point clouds exhibit clear
improvements of Circle-Net over SOTAs in terms of both noise-robustness and
extraction accuracy. We will release our code, model, and data for both
training and evaluation on GitHub upon publication.</p>
  </details>
</details>
<details>
  <summary>92. <b>标题：Mix-up Self-Supervised Learning for Contrast-agnostic Applications</b></summary>
  <p><b>编号</b>：[304]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00901</p>
  <p><b>作者</b>：Yichen Zhang,  Yifang Yin,  Ying Zhang,  Roger Zimmermann</p>
  <p><b>备注</b>：Accepted by ICME 2021</p>
  <p><b>关键词</b>：attracted significant research attention recently, low variance across images based, g ., medical image classification, learns effective visual representations, two benchmark datasets validate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Contrastive self-supervised learning has attracted significant research
attention recently. It learns effective visual representations from unlabeled
data by embedding augmented views of the same image close to each other while
pushing away embeddings of different images. Despite its great success on
ImageNet classification, COCO object detection, etc., its performance degrades
on contrast-agnostic applications, e.g., medical image classification, where
all images are visually similar to each other. This creates difficulties in
optimizing the embedding space as the distance between images is rather small.
To solve this issue, we present the first mix-up self-supervised learning
framework for contrast-agnostic applications. We address the low variance
across images based on cross-domain mix-up and build the pretext task based on
two synergistic objectives: image reconstruction and transparency prediction.
Experimental results on two benchmark datasets validate the effectiveness of
our method, where an improvement of 2.5% ~ 7.4% in top-1 accuracy was obtained
compared to existing self-supervised learning methods.</p>
  </details>
</details>
<details>
  <summary>93. <b>标题：A Free Lunch to Person Re-identification: Learning from Automatically  Generated Noisy Tracklets</b></summary>
  <p><b>编号</b>：[308]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00891</p>
  <p><b>作者</b>：Hehan Teng,  Tao He,  Yuchen Guo,  Zhenhua Guo,  Guiguang Ding</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：reduce id switch noise within tracklets, various manually generated noises show, proposed framework achieved map 53, high labor cost required, automatically generated person tracklets</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A series of unsupervised video-based re-identification (re-ID) methods have
been proposed to solve the problem of high labor cost required to annotate
re-ID datasets. But their performance is still far lower than the supervised
counterparts. In the mean time, clean datasets without noise are used in these
methods, which is not realistic. In this paper, we propose to tackle this
problem by learning re-ID models from automatically generated person tracklets
by multiple objects tracking (MOT) algorithm. To this end, we design a
tracklet-based multi-level clustering (TMC) framework to effectively learn the
re-ID model from the noisy person tracklets. First, intra-tracklet isolation to
reduce ID switch noise within tracklets; second, alternates between using
inter-tracklet association to eliminate ID fragmentation noise and network
training using the pseudo label. Extensive experiments on MARS with various
manually generated noises show the effectiveness of the proposed framework.
Specifically, the proposed framework achieved mAP 53.4% and rank-1 63.7% on the
simulated tracklets with strongest noise, even outperforming the best existing
method on clean tracklets. Based on the results, we believe that building re-ID
models from automatically generated noisy tracklets is a reasonable approach
and will also be an important way to make re-ID models feasible in real-world
applications.</p>
  </details>
</details>
<details>
  <summary>94. <b>标题：Moment-based Adversarial Training for Embodied Language Comprehension</b></summary>
  <p><b>编号</b>：[309]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00889</p>
  <p><b>作者</b>：Shintaro Ishikawa,  Komei Sugiura</p>
  <p><b>备注</b>：Accepted for presentation at ICPR2022</p>
  <p><b>关键词</b>：existing methods sometimes fail, uses two types, still far lower, execute household tasks, coffee maker ,"</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we focus on a vision-and-language task in which a robot is
instructed to execute household tasks. Given an instruction such as "Rinse off
a mug and place it in the coffee maker," the robot is required to locate the
mug, wash it, and put it in the coffee maker. This is challenging because the
robot needs to break down the instruction sentences into subgoals and execute
them in the correct order. On the ALFRED benchmark, the performance of
state-of-the-art methods is still far lower than that of humans. This is
partially because existing methods sometimes fail to infer subgoals that are
not explicitly specified in the instruction sentences. We propose Moment-based
Adversarial Training (MAT), which uses two types of moments for perturbation
updates in adversarial training. We introduce MAT to the embedding spaces of
the instruction, subgoals, and state representations to handle their varieties.
We validated our method on the ALFRED benchmark, and the results demonstrated
that our method outperformed the baseline method for all the metrics on the
benchmark.</p>
  </details>
</details>
<details>
  <summary>95. <b>标题：Acoustic-to-articulatory Inversion based on Speech Decomposition and  Auxiliary Feature</b></summary>
  <p><b>编号</b>：[315]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00873</p>
  <p><b>作者</b>：Jianrong Wang,  Jinyu Liu,  Longxuan Zhao,  Shanyu Wang,  Ruiguo Yu,  Li Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：three public datasets show, novel auxiliary feature network, new personalized speech features, average correlation coefficient increases, average correlation coefficient</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Acoustic-to-articulatory inversion (AAI) is to obtain the movement of
articulators from speech signals. Until now, achieving a speaker-independent
AAI remains a challenge given the limited data. Besides, most current works
only use audio speech as input, causing an inevitable performance bottleneck.
To solve these problems, firstly, we pre-train a speech decomposition network
to decompose audio speech into speaker embedding and content embedding as the
new personalized speech features to adapt to the speaker-independent case.
Secondly, to further improve the AAI, we propose a novel auxiliary feature
network to estimate the lip auxiliary features from the above personalized
speech features. Experimental results on three public datasets show that,
compared with the state-of-the-art only using the audio speech feature, the
proposed method reduces the average RMSE by 0.25 and increases the average
correlation coefficient by 2.0% in the speaker-dependent case. More
importantly, the average RMSE decreases by 0.29 and the average correlation
coefficient increases by 5.0% in the speaker-independent case.</p>
  </details>
</details>
<details>
  <summary>96. <b>标题：Adversarial Neon Beam: Robust Physical-World Adversarial Attack to DNNs</b></summary>
  <p><b>编号</b>：[323]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00853</p>
  <p><b>作者</b>：Chengyin Hu,  Kalibinuer Tiliwalidi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：attack method called adversarial neon beam, achieve better physical perturbation concealment, adversarial neon beam attack, achieve advanced attack effect, advanced physical attack methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the physical world, light affects the performance of deep neural networks.
Nowadays, many products based on deep neural network have been put into daily
life. There are few researches on the effect of light on the performance of
deep neural network models. However, the adversarial perturbations generated by
light may have extremely dangerous effects on these systems. In this work, we
propose an attack method called adversarial neon beam (AdvNB), which can
execute the physical attack by obtaining the physical parameters of adversarial
neon beams with very few queries. Experiments show that our algorithm can
achieve advanced attack effect in both digital test and physical test. In the
digital environment, 99.3% attack success rate was achieved, and in the
physical environment, 100% attack success rate was achieved. Compared with the
most advanced physical attack methods, our method can achieve better physical
perturbation concealment. In addition, by analyzing the experimental data, we
reveal some new phenomena brought about by the adversarial neon beam attack.</p>
  </details>
</details>
<details>
  <summary>97. <b>标题：Rotated Object Detection via Scale-invariant Mahalanobis Distance in  Aerial Images</b></summary>
  <p><b>编号</b>：[328]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00840</p>
  <p><b>作者</b>：Siyang Wen,  Wei Guo,  Ruijie Wu,  Yi Liu</p>
  <p><b>备注</b>：5 pages, 6 figures</p>
  <p><b>关键词</b>：new loss function called mahalanobis distance loss, rotated object detection usually use ln, meaningful yet challenging task, detection metric rotational intersection, parameter rotated object detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Rotated object detection in aerial images is a meaningful yet challenging
task as objects are densely arranged and have arbitrary orientations. The
eight-parameter (coordinates of box vectors) methods in rotated object
detection usually use ln-norm losses (L1 loss, L2 loss, and smooth L1 loss) as
loss functions. As ln-norm losses are mainly based on non-scale-invariant
Minkowski distance, using ln-norm losses will lead to inconsistency with the
detection metric rotational Intersection-over-Union (IoU) and training
instability. To address the problems, we use Mahalanobis distance to calculate
loss between the predicted and the target box vertices' vectors, proposing a
new loss function called Mahalanobis Distance Loss (MDL) for eight-parameter
rotated object detection. As Mahalanobis distance is scale-invariant, MDL is
more consistent with detection metric than ln-norm losses and more stable
during training. To alleviate the problem of boundary discontinuity like all
other eight-parameter methods, we further take the minimum loss value to make
MDL continuous at boundary cases. We achieve state-of-art performance on
DOTA-v1.0 with the proposed method MDL. Furthermore, with the comparative
experiment of smooth L1 loss under the same condi-tion, we find that MDL
performs better in rotated object detection.</p>
  </details>
</details>
<details>
  <summary>98. <b>标题：PixelFolder: An Efficient Progressive Pixel Synthesis Network for Image  Generation</b></summary>
  <p><b>编号</b>：[329]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00833</p>
  <p><b>作者</b>：Jing He,  Yiyi Zhou,  Qi Zhang,  Yunhang Shen,  Xiaoshuai Sun,  Chao Chen,  Rongrong Ji</p>
  <p><b>备注</b>：11 pages, 7 figures</p>
  <p><b>关键词</b>：progressive pixel synthesis network towards efficient image generation, latest pixel synthesis method called cips, introduce novel pixel folding operations, progressive pixel regression problem, existing methods still suffer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pixel synthesis is a promising research paradigm for image generation, which
can well exploit pixel-wise prior knowledge for generation. However, existing
methods still suffer from excessive memory footprint and computation overhead.
In this paper, we propose a progressive pixel synthesis network towards
efficient image generation, coined as PixelFolder. Specifically, PixelFolder
formulates image generation as a progressive pixel regression problem and
synthesizes images by a multi-stage paradigm, which can greatly reduce the
overhead caused by large tensor transformations. In addition, we introduce
novel pixel folding operations to further improve model efficiency while
maintaining pixel-wise prior knowledge for end-to-end regression. With these
innovative designs, we greatly reduce the expenditure of pixel synthesis, e.g.,
reducing 90% computation and 57% parameters compared to the latest pixel
synthesis method called CIPS. To validate our approach, we conduct extensive
experiments on two benchmark datasets, namely FFHQ and LSUN Church. The
experimental results show that with much less expenditure, PixelFolder obtains
new state-of-the-art (SOTA) performance on two benchmark datasets, i.e., 3.77
FID and 2.45 FID on FFHQ and LSUN Church, respectively. Meanwhile, PixelFolder
is also more efficient than the SOTA methods like StyleGAN2, reducing about 74%
computation and 36% parameters, respectively. These results greatly validate
the effectiveness of the proposed PixelFolder.</p>
  </details>
</details>
<details>
  <summary>99. <b>标题：Online Convolutional Re-parameterization</b></summary>
  <p><b>编号</b>：[331]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00826</p>
  <p><b>作者</b>：Mu Hu,  Junyi Feng,  Jiashen Hua,  Baisheng Lai,  Jianqiang Huang,  Xiaojin Gong,  Xiansheng Hua</p>
  <p><b>备注</b>：Accepted by CVPR 2022</p>
  <p><b>关键词</b>：models outperform previous methods, deep models without introducing, various computer vision tasks, large extra training cost, models rely heavily</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Structural re-parameterization has drawn increasing attention in various
computer vision tasks. It aims at improving the performance of deep models
without introducing any inference-time cost. Though efficient during inference,
such models rely heavily on the complicated training-time blocks to achieve
high accuracy, leading to large extra training cost. In this paper, we present
online convolutional re-parameterization (OREPA), a two-stage pipeline, aiming
to reduce the huge training overhead by squeezing the complex training-time
block into a single convolution. To achieve this goal, we introduce a linear
scaling layer for better optimizing the online blocks. Assisted with the
reduced training cost, we also explore some more effective re-param components.
Compared with the state-of-the-art re-param models, OREPA is able to save the
training-time memory cost by about 70% and accelerate the training speed by
around 2x. Meanwhile, equipped with OREPA, the models outperform previous
methods on ImageNet by up to +0.6%.We also conduct experiments on object
detection and semantic segmentation and show consistent improvements on the
downstream tasks. Codes are available at
this https URL .</p>
  </details>
</details>
<details>
  <summary>100. <b>标题：Semantic-Aware Domain Generalized Segmentation</b></summary>
  <p><b>编号</b>：[335]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00822</p>
  <p><b>作者</b>：Duo Peng,  Yinjie Lei,  Munawar Hayat,  Yulan Guo,  Wen Li</p>
  <p><b>备注</b>：16 pages, 7 figures, accepted at CVPR 2022 (Oral Presentation)</p>
  <p><b>关键词</b>：framework including two novel modules, address domain generalized semantic segmentation, get clear segmentation boundaries, source domain lack generalization, simultaneously promoting domain invariance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep models trained on source domain lack generalization when evaluated on
unseen target domains with different data distributions. The problem becomes
even more pronounced when we have no access to target domain samples for
adaptation. In this paper, we address domain generalized semantic segmentation,
where a segmentation model is trained to be domain-invariant without using any
target domain data. Existing approaches to tackle this problem standardize data
into a unified distribution. We argue that while such a standardization
promotes global normalization, the resulting features are not discriminative
enough to get clear segmentation boundaries. To enhance separation between
categories while simultaneously promoting domain invariance, we propose a
framework including two novel modules: Semantic-Aware Normalization (SAN) and
Semantic-Aware Whitening (SAW). Specifically, SAN focuses on category-level
center alignment between features from different image styles, while SAW
enforces distributed alignment for the already center-aligned features. With
the help of SAN and SAW, we encourage both intra-category compactness and
inter-category separability. We validate our approach through extensive
experiments on widely-used datasets (i.e. GTAV, SYNTHIA, Cityscapes, Mapillary
and BDDS). Our approach shows significant improvements over existing
state-of-the-art on various backbone networks. Code is available at
this https URL</p>
  </details>
</details>
<details>
  <summary>101. <b>标题：Unsupervised Coherent Video Cartoonization with Perceptual Motion  Consistency</b></summary>
  <p><b>编号</b>：[346]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00795</p>
  <p><b>作者</b>：Zhenhuan Liu,  Liang Li,  Huajie Jiang,  Xin Jin,  Dandan Tu,  Shuhui Wang,  Zheng-Jun Zha</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：captures global semantic information beyond raw pixel, creative content generations like style transfer, temporal consistency without hurting style effects, similarity measurement disentangles temporal relationships, adaptive semantic alignment framework</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, creative content generations like style transfer and neural
photo editing have attracted more and more attention. Among these,
cartoonization of real-world scenes has promising applications in entertainment
and industry. Different from image translations focusing on improving the style
effect of generated images, video cartoonization has additional requirements on
the temporal consistency. In this paper, we propose a spatially-adaptive
semantic alignment framework with perceptual motion consistency for coherent
video cartoonization in an unsupervised manner. The semantic alignment module
is designed to restore deformation of semantic structure caused by spatial
information lost in the encoder-decoder architecture. Furthermore, we devise
the spatio-temporal correlative map as a style-independent, global-aware
regularization on the perceptual motion consistency. Deriving from similarity
measurement of high-level features in photo and cartoon frames, it captures
global semantic information beyond raw pixel-value in optical flow. Besides,
the similarity measurement disentangles temporal relationships from
domain-specific style properties, which helps regularize the temporal
consistency without hurting style effects of cartoon images. Qualitative and
quantitative experiments demonstrate our method is able to generate highly
stylistic and temporal consistent cartoon videos.</p>
  </details>
</details>
<details>
  <summary>102. <b>标题：R(Det)^2: Randomized Decision Routing for Object Detection</b></summary>
  <p><b>编号</b>：[347]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00794</p>
  <p><b>作者</b>：Ya-Li Li,  Shengjin Wang</p>
  <p><b>备注</b>：10 pages, 5 figures; Accepted by CVPR2022</p>
  <p><b>关键词</b>：6 $\% ap improvement, plugging soft decision trees, det )$^ 2 $., affects detection performance significantly, performance decision head remains</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the paradigm of object detection, the decision head is an important part,
which affects detection performance significantly. Yet how to design a
high-performance decision head remains to be an open issue. In this paper, we
propose a novel approach to combine decision trees and deep neural networks in
an end-to-end learning manner for object detection. First, we disentangle the
decision choices and prediction values by plugging soft decision trees into
neural networks. To facilitate effective learning, we propose randomized
decision routing with node selective and associative losses, which can boost
the feature representative learning and network decision simultaneously.
Second, we develop the decision head for object detection with narrow branches
to generate the routing probabilities and masks, for the purpose of obtaining
divergent decisions from different nodes. We name this approach as the
randomized decision routing for object detection, abbreviated as R(Det)$^2$.
Experiments on MS-COCO dataset demonstrate that R(Det)$^2$ is effective to
improve the detection performance. Equipped with existing detectors, it
achieves $1.4\sim 3.6$\% AP improvement.</p>
  </details>
</details>
<details>
  <summary>103. <b>标题：IR-GAN: Image Manipulation with Linguistic Instruction by Increment  Reasoning</b></summary>
  <p><b>编号</b>：[348]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00792</p>
  <p><b>作者</b>：Zhenhuan Liu,  Jincan Deng,  Liang Li,  Shaofei Cai,  Qianqian Xu,  Shuhui Wang,  Qingming Huang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：traditional conditional image generation models mainly focus, active research topic including text2image, linguistic instruction brings new challenges, increment reasoning generative adversarial network, multimodal conditional generation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Conditional image generation is an active research topic including text2image
and image translation.
Recently image manipulation with linguistic instruction brings new challenges
of multimodal conditional generation.
However, traditional conditional image generation models mainly focus on
generating high-quality and visually realistic images, and lack resolving the
partial consistency between image and instruction.
To address this issue, we propose an Increment Reasoning Generative
Adversarial Network (IR-GAN), which aims to reason the consistency between
visual increment in images and semantic increment in instructions.
First, we introduce the word-level and instruction-level instruction encoders
to learn user's intention from history-correlated instructions as semantic
increment.
Second, we embed the representation of semantic increment into that of source
image for generating target image, where source image plays the role of
referring auxiliary.
Finally, we propose a reasoning discriminator to measure the consistency
between visual increment and semantic increment, which purifies user's
intention and guarantees the good logic of generated target image.
Extensive experiments and visualization conducted on two datasets show the
effectiveness of IR-GAN.</p>
  </details>
</details>
<details>
  <summary>104. <b>标题：SAD: A Large-scale Dataset towards Airport Detection in Synthetic  Aperture Radar Images</b></summary>
  <p><b>编号</b>：[350]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00790</p>
  <p><b>作者</b>：Fan Zhang,  Daochang Wang,  Fei Ma,  Qiang Yin,  Deliang Xiang,  Yongsheng Zhou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：art airport area detection algorithms, covers 104 airfield instances, multiple deep learning approach, publicly available sar dataset, contains 624 sar images</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Airports have an important role in both military and civilian domains. The
synthetic aperture radar (SAR) based airport detection has received increasing
attention in recent years. However, due to the high cost of SAR imaging and
annotation process, there is no publicly available SAR dataset for airport
detection. As a result, deep learning methods have not been fully used in
airport detection tasks. To provide a benchmark for airport detection research
in SAR images, this paper introduces a large-scale SAR Airport Dataset (SAD).
In order to adequately reflect the demands of real world applications, it
contains 624 SAR images from Sentinel 1B and covers 104 airfield instances with
different scales, orientations and shapes. The experiments of multiple deep
learning approach on this dataset proves its effectiveness. It developing
state-of-the-art airport area detection algorithms or other relevant tasks.</p>
  </details>
</details>
<details>
  <summary>105. <b>标题：Do learned representations respect causal relationships?</b></summary>
  <p><b>编号</b>：[359]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00762</p>
  <p><b>作者</b>：Lan Wang,  Vishnu Naresh Boddeti</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ncinet significantly outperforms existing observational causal discovery approaches, observational causal discovery, consider image representations learned, various design choices, underlying causal relation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Data often has many semantic attributes that are causally associated with
each other. But do attribute-specific learned representations of data also
respect the same causal relations? We answer this question in three steps.
First, we introduce NCINet, an approach for observational causal discovery from
high-dimensional data. It is trained purely on synthetically generated
representations and can be applied to real representations, and is specifically
designed to mitigate the domain gap between the two. Second, we apply NCINet to
identify the causal relations between image representations of different pairs
of attributes with known and unknown causal relations between the labels. For
this purpose, we consider image representations learned for predicting
attributes on the 3D Shapes, CelebA, and the CASIA-WebFace datasets, which we
annotate with multiple multi-class attributes. Third, we analyze the effect on
the underlying causal relation between learned representations induced by
various design choices in representation learning. Our experiments indicate
that (1) NCINet significantly outperforms existing observational causal
discovery approaches for estimating the causal relation between pairs of random
samples, both in the presence and absence of an unobserved confounder, (2)
under controlled scenarios, learned representations can indeed satisfy the
underlying causal relations between their respective labels, and (3) the causal
relations are positively correlated with the predictive capability of the
representations.</p>
  </details>
</details>
<details>
  <summary>106. <b>标题：Homography Loss for Monocular 3D Object Detection</b></summary>
  <p><b>编号</b>：[362]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00754</p>
  <p><b>作者</b>：Jiaqi Gu,  Bojian Wu,  Lubin Fan,  Jianqiang Huang,  Shen Cao,  Zhiyu Xiang,  Xian-Sheng Hua</p>
  <p><b>备注</b>：8 pages, 5 figures. Accepted to CVPR 2022</p>
  <p><b>关键词</b>：mature monocular 3d detector, corresponding predicted 3d boxes, accurately predicted 3d boxes, monocular 3d object detection, kitti 3d datasets</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Monocular 3D object detection is an essential task in autonomous driving.
However, most current methods consider each 3D object in the scene as an
independent training sample, while ignoring their inherent geometric relations,
thus inevitably resulting in a lack of leveraging spatial constraints. In this
paper, we propose a novel method that takes all the objects into consideration
and explores their mutual relationships to help better estimate the 3D boxes.
Moreover, since 2D detection is more reliable currently, we also investigate
how to use the detected 2D boxes as guidance to globally constrain the
optimization of the corresponding predicted 3D boxes. To this end, a
differentiable loss function, termed as Homography Loss, is proposed to achieve
the goal, which exploits both 2D and 3D information, aiming at balancing the
positional relationships between different objects by global constraints, so as
to obtain more accurately predicted 3D boxes. Thanks to the concise design, our
loss function is universal and can be plugged into any mature monocular 3D
detector, while significantly boosting the performance over their baseline.
Experiments demonstrate that our method yields the best performance (Nov. 2021)
compared with the other state-of-the-arts by a large margin on KITTI 3D
datasets.</p>
  </details>
</details>
<details>
  <summary>107. <b>标题：What to look at and where: Semantic and Spatial Refined Transformer for  detecting human-object interactions</b></summary>
  <p><b>编号</b>：[366]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00746</p>
  <p><b>作者</b>：A S M Iftekhar,  Hao Chen,  Kaustav Kundu,  Xinyu Li,  Joseph Tighe,  Davide Modolo</p>
  <p><b>备注</b>：CVPR 2022 Oral</p>
  <p><b>关键词</b>：ssrt introduces two new modules, representation using rich semantic, object interaction detection task, popular hoi benchmarks, action pairs within</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a novel one-stage Transformer-based semantic and spatial refined
transformer (SSRT) to solve the Human-Object Interaction detection task, which
requires to localize humans and objects, and predicts their interactions.
Differently from previous Transformer-based HOI approaches, which mostly focus
at improving the design of the decoder outputs for the final detection, SSRT
introduces two new modules to help select the most relevant object-action pairs
within an image and refine the queries' representation using rich semantic and
spatial features. These enhancements lead to state-of-the-art results on the
two most popular HOI benchmarks: V-COCO and HICO-DET.</p>
  </details>
</details>
<details>
  <summary>108. <b>标题：SkeleVision: Towards Adversarial Resiliency of Person Tracking with  Multi-Task Learning</b></summary>
  <p><b>编号</b>：[370]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00734</p>
  <p><b>作者</b>：Nilaksh Das,  Sheng-Yun Peng,  Duen Horng Chau</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：adversarial attacks raises serious concerns regarding, person tracking using computer vision techniques, widely used siamrpn tracker, powerful adversarial attacks, world datasets reveals</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Person tracking using computer vision techniques has wide ranging
applications such as autonomous driving, home security and sports analytics.
However, the growing threat of adversarial attacks raises serious concerns
regarding the security and reliability of such techniques. In this work, we
study the impact of multi-task learning (MTL) on the adversarial robustness of
the widely used SiamRPN tracker, in the context of person tracking.
Specifically, we investigate the effect of jointly learning with semantically
analogous tasks of person tracking and human keypoint detection. We conduct
extensive experiments with more powerful adversarial attacks that can be
physically realizable, demonstrating the practical value of our approach. Our
empirical study with simulated as well as real-world datasets reveals that
training with MTL consistently makes it harder to attack the SiamRPN tracker,
compared to typically training only on the single task of person tracking.</p>
  </details>
</details>
<details>
  <summary>109. <b>标题：Learning Audio-Video Modalities from Image Captions</b></summary>
  <p><b>编号</b>：[390]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00679</p>
  <p><b>作者</b>：Arsha Nagrani,  Paul Hongsuck Seo,  Bryan Seybold,  Anja Hauth,  Santiago Manen,  Chen Sun,  Cordelia Schmid</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：multimodal transformed based model, data achieves competitive performance, even outperforming howto100m pretraining, video captioning dataset consisting, new video mining pipeline</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A major challenge in text-video and text-audio retrieval is the lack of
large-scale training data. This is unlike image-captioning, where datasets are
in the order of millions of samples. To close this gap we propose a new video
mining pipeline which involves transferring captions from image captioning
datasets to video clips with no additional manual effort. Using this pipeline,
we create a new large-scale, weakly labelled audio-video captioning dataset
consisting of millions of paired clips and captions. We show that training a
multimodal transformed based model on this data achieves competitive
performance on video retrieval and video captioning, matching or even
outperforming HowTo100M pretraining with 20x fewer clips. We also show that our
mined clips are suitable for text-audio pretraining, and achieve state of the
art results for the task of audio retrieval.</p>
  </details>
</details>
<details>
  <summary>110. <b>标题：Hazard Detection And Avoidance For The Nova-C Lander</b></summary>
  <p><b>编号</b>：[395]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00660</p>
  <p><b>作者</b>：Joel Getchius,  Devin Renshaw,  Daniel Posada,  Troy Henderson,  Lillian Hong,  Shen Ge,  Giovanni Molina</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：completed within 15 seconds, c lander must ensure, c utilizes intuitive machines, precision navigation system, performed 400 meters</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In early 2022, Intuitive Machines' NOVA-C Lander will touch down on the lunar
surface becoming the first commercial endeavor to visit a celestial body.
NOVA-C will deliver six payloads to the lunar surface with various scientific
and engineering objectives, ushering in a new era of commercial space
exploration and utilization. However, to safely accomplish the mission, the
NOVA-C lander must ensure its landing site is free of hazards larger than 30 cm
and the slope of local terrain at touchdown is less than 10 degrees off
vertical. To accomplish this, NOVA-C utilizes Intuitive Machines' precision
navigation system, coupled with machine vision algorithms for scene reduction
and landing site characterization. A unique aspect to the NOVA-C approach is
the real-time nature of the hazard detection and avoidance algorithms--which
are performed 400 meters above and down range of the intended landing site and
completed within 15 seconds. In this paper, we review the theoretical
foundations for the hazard detection and avoidance algorithms, describe the
practical challenges of implementation on the NOVA-C flight computer, and
present test and analysis results.</p>
  </details>
</details>
<details>
  <summary>111. <b>标题：Consistency driven Sequential Transformers Attention Model for Partially  Observable Scenes</b></summary>
  <p><b>编号</b>：[396]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00656</p>
  <p><b>作者</b>：Samrudhdhi B. Rangrej,  Chetan L. Srinidhi,  James J. Clark</p>
  <p><b>备注</b>：Accepted to CVPR 2022</p>
  <p><b>关键词</b>：g ., aerial imaging ), observing, predicts informative glimpse locations solely based, hard attention models initially observe, sequential transformers attention model, training objective yields 3</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Most hard attention models initially observe a complete scene to locate and
sense informative glimpses, and predict class-label of a scene based on
glimpses. However, in many applications (e.g., aerial imaging), observing an
entire scene is not always feasible due to the limited time and resources
available for acquisition. In this paper, we develop a Sequential Transformers
Attention Model (STAM) that only partially observes a complete image and
predicts informative glimpse locations solely based on past glimpses. We design
our agent using DeiT-distilled and train it with a one-step actor-critic
algorithm. Furthermore, to improve classification performance, we introduce a
novel training objective, which enforces consistency between the class
distribution predicted by a teacher model from a complete image and the class
distribution predicted by our agent using glimpses. When the agent senses only
4% of the total image area, the inclusion of the proposed consistency loss in
our training objective yields 3% and 8% higher accuracy on ImageNet and fMoW
datasets, respectively. Moreover, our agent outperforms previous
state-of-the-art by observing nearly 27% and 42% fewer pixels in glimpses on
ImageNet and fMoW.</p>
  </details>
</details>
<details>
  <summary>112. <b>标题：Robust Neonatal Face Detection in Real-world Clinical Settings</b></summary>
  <p><b>编号</b>：[397]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00655</p>
  <p><b>作者</b>：Jacqueline Hausmann,  Md Sirajus Salekin,  Ghada Zamzmi,  Dmitry Goldgof,  Yu Sun</p>
  <p><b>备注</b>：Accepted at IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR Workshops 2021)</p>
  <p><b>关键词</b>：work achieves near real time neonate face detection, proprietary dataset containing labelled neonate faces, neonatal faces would benefit wide range, neonate infant whose face composition, neonate intensive care unit</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Current face detection algorithms are extremely generalized and can obtain
decent accuracy when detecting the adult faces. These approaches are
insufficient when handling outlier cases, for example when trying to detect the
face of a neonate infant whose face composition and expressions are relatively
different than that of the adult. It is furthermore difficult when applied to
detect faces in a complicated setting such as the Neonate Intensive Care Unit.
By training a state-of-the-art face detection model, You-Only-Look-Once, on a
proprietary dataset containing labelled neonate faces in a clinical setting,
this work achieves near real time neonate face detection. Our preliminary
findings show an accuracy of 68.7%, compared to the off the shelf solution
which detected neonate faces with an accuracy of 7.37%. Although further
experiments are needed to validate our model, our results are promising and
prove the feasibility of detecting neonatal faces in challenging real-world
settings. The robust and real-time detection of neonatal faces would benefit
wide range of automated systems (e.g., pain recognition and surveillance) who
currently suffer from the time and effort due to the necessity of manual
annotations. To benefit the research community, we make our trained weights
publicly available at github(this https URL).</p>
  </details>
</details>
<details>
  <summary>113. <b>标题：SIMBAR: Single Image-Based Scene Relighting For Effective Data  Augmentation For Automated Driving Vision Tasks</b></summary>
  <p><b>编号</b>：[405]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00644</p>
  <p><b>作者</b>：Xianling Zhang,  Nathan Tseng,  Ameerah Syed,  Rohan Bhasin,  Nikita Jaipuria</p>
  <p><b>备注</b>：Accepted to CVPR 2022. Project page: this https URL</p>
  <p><b>关键词</b>：scene relighting leveraging explicit geometric representations, world autonomous driving datasets comprise, view scene relighting baselines, automated driving vision tasks, multiple object tracking accuracy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Real-world autonomous driving datasets comprise of images aggregated from
different drives on the road. The ability to relight captured scenes to unseen
lighting conditions, in a controllable manner, presents an opportunity to
augment datasets with a richer variety of lighting conditions, similar to what
would be encountered in the real-world. This paper presents a novel image-based
relighting pipeline, SIMBAR, that can work with a single image as input. To the
best of our knowledge, there is no prior work on scene relighting leveraging
explicit geometric representations from a single image. We present qualitative
comparisons with prior multi-view scene relighting baselines. To further
validate and effectively quantify the benefit of leveraging SIMBAR for data
augmentation for automated driving vision tasks, object detection and tracking
experiments are conducted with a state-of-the-art method, a Multiple Object
Tracking Accuracy (MOTA) of 93.3% is achieved with CenterTrack on
SIMBAR-augmented KITTI - an impressive 9.0% relative improvement over the
baseline MOTA of 85.6% with CenterTrack on original KITTI, both models trained
from scratch and tested on Virtual KITTI. For more details and SIMBAR relit
datasets, please visit our project website (this https URL).</p>
  </details>
</details>
<details>
  <summary>114. <b>标题：Learning Neural Acoustic Fields</b></summary>
  <p><b>编号</b>：[407]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00628</p>
  <p><b>作者</b>：Andrew Luo,  Yilun Du,  Michael J. Tarr,  Joshua B. Tenenbaum,  Antonio Torralba,  Chuang Gan</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：neural impulse response function, increasingly higher quality representations, object moves around us, introduce neural acoustic fields, learning spatial auditory representations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Our environment is filled with rich and dynamic acoustic information. When we
walk into a cathedral, the reverberations as much as appearance inform us of
the sanctuary's wide open space. Similarly, as an object moves around us, we
expect the sound emitted to also exhibit this movement. While recent advances
in learned implicit functions have led to increasingly higher quality
representations of the visual world, there have not been commensurate advances
in learning spatial auditory representations. To address this gap, we introduce
Neural Acoustic Fields (NAFs), an implicit representation that captures how
sounds propagate in a physical scene. By modeling acoustic propagation in a
scene as a linear time-invariant system, NAFs learn to continuously map all
emitter and listener location pairs to a neural impulse response function that
can then be applied to arbitrary sounds. We demonstrate that the continuous
nature of NAFs enables us to render spatial acoustics for a listener at an
arbitrary location, and can predict sound propagation at novel locations. We
further show that the representation learned by NAFs can help improve visual
learning with sparse views. Finally, we show that a representation informative
of scene structure emerges during the learning of NAFs.</p>
  </details>
</details>
<details>
  <summary>115. <b>标题：Explainable and Interpretable Diabetic Retinopathy Classification Based  on Neural-Symbolic Learning</b></summary>
  <p><b>编号</b>：[409]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00624</p>
  <p><b>作者</b>：Se-In Jang,  Michael J.A. Girard,  Alexandre H. Thiery</p>
  <p><b>备注</b>：Published in AAAI-22 Workshop</p>
  <p><b>关键词</b>：proposed explaindr method exhibits promising performance, diabetic retinopathy classification dataset show, include humanreadable features obtained, diabetic retinopathy characteristics related, interpretable diabetic retinopathy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we propose an explainable and interpretable diabetic
retinopathy (ExplainDR) classification model based on neural-symbolic learning.
To gain explainability, a highlevel symbolic representation should be
considered in decision making. Specifically, we introduce a human-readable
symbolic representation, which follows a taxonomy style of diabetic retinopathy
characteristics related to eye health conditions to achieve explainability. We
then include humanreadable features obtained from the symbolic representation
in the disease prediction. Experimental results on a diabetic retinopathy
classification dataset show that our proposed ExplainDR method exhibits
promising performance when compared to that from state-of-the-art methods
applied to the IDRiD dataset, while also providing interpretability and
explainability.</p>
  </details>
</details>
<details>
  <summary>116. <b>标题：End-to-end multi-particle reconstruction in high occupancy imaging  calorimeters with graph neural networks</b></summary>
  <p><b>编号</b>：[412]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01681</p>
  <p><b>作者</b>：Shah Rukh Qasim,  Nadezda Chernyavskaya,  Jan Kieseler,  Kenneth Long,  Oleksandr Viazlo,  Maurizio Pierini,  Raheel Nawaz</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：}( 1000 )$ particles, generation granular calorimeters similar, weighted graph neural network, graph segmentation technique, inference computational cost</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present an end-to-end reconstruction algorithm to build particle
candidates from detector hits in next-generation granular calorimeters similar
to that foreseen for the high-luminosity upgrade of the CMS detector. The
algorithm exploits a distance-weighted graph neural network, trained with
object condensation, a graph segmentation technique. Through a single-shot
approach, the reconstruction task is paired with energy regression. We describe
the reconstruction performance in terms of efficiency as well as in terms of
energy resolution. In addition, we show the jet reconstruction performance of
our method and discuss its inference computational cost. This work is the
first-ever example of single-shot calorimetric reconstruction of ${\cal
O}(1000)$ particles in high-luminosity conditions with 200 pileup to our
knowledge.</p>
  </details>
</details>
<details>
  <summary>117. <b>标题：Three-dimensional Microstructural Image Synthesis from 2D Backscattered  Electron Image of Cement Paste</b></summary>
  <p><b>编号</b>：[414]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01645</p>
  <p><b>作者</b>：Xin Zhao,  Xu Wu,  Lin Wang,  Pengkun Hou,  Qinfei Li,  Yuxuan Zhang,  Bo Yang</p>
  <p><b>备注</b>：25 pages, 9 figures</p>
  <p><b>关键词</b>：focused ion beam scanning electron microscopy, method includes 2d backscattered electron, quality 3d microstructural image, acquired 2d bse image, 3d microstructure synthesis phases</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The microstructure is significant for exploring the physical properties of
hardened cement paste. In general, the microstructures of hardened cement paste
are obtained by microscopy. As a popular method, scanning electron microscopy
(SEM) can acquire high-quality 2D images but fails to obtain 3D
microstructures.Although several methods, such as microtomography (Micro-CT)
and Focused Ion Beam Scanning Electron Microscopy (FIB-SEM), can acquire 3D
microstructures, these fail to obtain high-quality 3D images or consume
considerable cost. To address these issues, a method based on solid texture
synthesis is proposed, synthesizing high-quality 3D microstructural image of
hardened cement paste. This method includes 2D backscattered electron (BSE)
image acquisition and 3D microstructure synthesis phases. In the approach, the
synthesis model is based on solid texture synthesis, capturing microstructure
information of the acquired 2D BSE image and generating high-quality 3D
microstructures. In experiments, the method is verified on actual 3D Micro-CT
images and 2D BSE images. Finally, qualitative experiments demonstrate that the
3D microstructures generated by our method have similar visual characteristics
to the given 2D example. Furthermore, quantitative experiments prove that the
synthetic 3D results are consistent with the actual instance in terms of
porosity, particle size distribution, and grey scale co-occurrence matrix.</p>
  </details>
</details>
<details>
  <summary>118. <b>标题：Optimize Deep Learning Models for Prediction of Gene Mutations Using  Unsupervised Clustering</b></summary>
  <p><b>编号</b>：[415]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01593</p>
  <p><b>作者</b>：Zihan Chen,  Xingyu Li,  Miaomiao Yang,  Hong Zhang,  Xu Steven Xu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：proposed algorithm outperformed two recently published baseline algorithms leveraging unsupervised clustering, image patches could help identify predictive patches, environment may provide better prediction ability, wsi based method without selection, slide digital pathology images</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning has become the mainstream methodological choice for analyzing
and interpreting whole-slide digital pathology images (WSIs). It is commonly
assumed that tumor regions carry most predictive information. In this paper, we
proposed an unsupervised clustering-based multiple-instance learning, and apply
our method to develop deep-learning models for prediction of gene mutations
using WSIs from three cancer types in The Cancer Genome Atlas (TCGA) studies
(CRC, LUAD, and HNSCC). We showed that unsupervised clustering of image patches
could help identify predictive patches, exclude patches lack of predictive
information, and therefore improve prediction on gene mutations in all three
different cancer types, compared with the WSI based method without selection of
image patches and models based on only tumor regions. Additionally, our
proposed algorithm outperformed two recently published baseline algorithms
leveraging unsupervised clustering to assist model prediction. The
unsupervised-clustering-based approach for mutation prediction allows
identification of the spatial regions related to mutation of a specific gene
via the resolved probability scores, highlighting the heterogeneity of a
predicted genotype in the tumor microenvironment. Finally, our study also
demonstrated that selection of tumor regions of WSIs is not always the best way
to identify patches for prediction of gene mutations, and other tissue types in
the tumor micro-environment may provide better prediction ability for gene
mutations than tumor tissues.</p>
  </details>
</details>
<details>
  <summary>119. <b>标题：Computer-Aided Extraction of Select MRI Markers of Cerebral Small Vessel  Disease: A Systematic Review</b></summary>
  <p><b>编号</b>：[423]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01411</p>
  <p><b>作者</b>：Jiyang Jiang,  Dadong Wang,  Yang Song,  Perminder S. Sachdev,  Wei Wen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：future studies could consider pooling data, cmb ), dilated perivascular spaces, laborious visual rating approaches, cerebral small vessel disease, one classical image processing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cerebral small vessel disease (CSVD) is a major vascular contributor to
cognitive impairment in ageing, including dementias. Imaging remains the most
promising method for in vivo studies of CSVD. To replace the subjective and
laborious visual rating approaches, emerging studies have applied
state-of-the-art artificial intelligence to extract imaging biomarkers of CSVD
from MRI scans. We aimed to summarise published computer-aided methods to
examine three imaging biomarkers of CSVD, namely cerebral microbleeds (CMB),
dilated perivascular spaces (PVS), and lacunes of presumed vascular origin.
Seventy-one classical image processing, classical machine learning, and deep
learning studies were identified. CMB and PVS have been better studied,
compared to lacunes. While good performance metrics have been achieved in local
test datasets, there have not been generalisable pipelines validated in
different research or clinical cohorts. Transfer learning and weak supervision
techniques have been applied to accommodate the limitations in training data.
Future studies could consider pooling data from multiple sources to increase
diversity, and validating the performance of the methods using both image
processing metrics and associations with clinical measures.</p>
  </details>
</details>
<details>
  <summary>120. <b>标题：Differentiable Rendering for Synthetic Aperture Radar Imagery</b></summary>
  <p><b>编号</b>：[433]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01248</p>
  <p><b>作者</b>：Michael Wilmanski,  Jonathan Tamir</p>
  <p><b>备注</b>：A substantially similar version of this manuscript was submitted to ECCV 2022 and is under review</p>
  <p><b>关键词</b>：allows explicitly modeling geometric priors, limited sar imagery using high, optimization pipeline using first, fidelity simulated sar data, synthetic aperture radar</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>There is rising interest in integrating signal and image processing pipelines
into deep learning training to incorporate more domain knowledge. This can lead
to deep neural networks that are trained more robustly and with limited data,
as well as the capability to solve ill-posed inverse problems. In particular,
there is rising interest in differentiable rendering, which allows explicitly
modeling geometric priors and constraints in the optimization pipeline using
first-order methods such as backpropagation. Existing efforts in differentiable
rendering have focused on imagery from electro-optical sensors, particularly
conventional RGB-imagery. In this work, we propose an approach for
differentiable rendering of Synthetic Aperture Radar (SAR) imagery, which
combines methods from 3D computer graphics with neural rendering. We
demonstrate the approach on the inverse graphics problem of 3D Object
Reconstruction from limited SAR imagery using high-fidelity simulated SAR data.</p>
  </details>
</details>
<details>
  <summary>121. <b>标题：A Novel Mask R-CNN Model to Segment Heterogeneous Brain Tumors through  Image Subtraction</b></summary>
  <p><b>编号</b>：[436]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01201</p>
  <p><b>作者</b>：Sanskriti Singh</p>
  <p><b>备注</b>：8 pages, 7 figures</p>
  <p><b>关键词</b>：tumors usually take 4 mri scans, rsna pneumonia detection challenge dataset, f1 score ), recall, analytics provides mri data, brats2020 brain tumor dataset</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The segmentation of diseases is a popular topic explored by researchers in
the field of machine learning. Brain tumors are extremely dangerous and require
the utmost precision to segment for a successful surgery. Patients with tumors
usually take 4 MRI scans, T1, T1gd, T2, and FLAIR, which are then sent to
radiologists to segment and analyze for possible future surgery. To create a
second segmentation, it would be beneficial to both radiologists and patients
in being more confident in their conclusions. We propose using a method
performed by radiologists called image segmentation and applying it to machine
learning models to prove a better segmentation. Using Mask R-CNN, its ResNet
backbone being pre-trained on the RSNA pneumonia detection challenge dataset,
we can train a model on the Brats2020 Brain Tumor dataset. Center for
Biomedical Image Computing & Analytics provides MRI data on patients with and
without brain tumors and the corresponding segmentations. We can see how well
the method of image subtraction works by comparing it to models without image
subtraction through DICE coefficient (F1 score), recall, and precision on the
untouched test set. Our model performed with a DICE coefficient of 0.75 in
comparison to 0.69 without image subtraction. To further emphasize the
usefulness of image subtraction, we compare our final model to current
state-of-the-art models to segment tumors from MRI scans.</p>
  </details>
</details>
<details>
  <summary>122. <b>标题：Gastrointestinal Polyps and Tumors Detection Based on Multi-scale  Feature-fusion with WCE Sequences</b></summary>
  <p><b>编号</b>：[448]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01012</p>
  <p><b>作者</b>：Zhuo Falin,  Liu Haihua,  Pan Ning</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：small intestinal lesion recognition network based, automatically detect small intestinal polyps, real wce image dataset provided, fusion learning network (\ textbf, latter combines residual structure</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Wireless Capsule Endoscopy(WCE) has been widely used for the screening of
gastrointestinal(GI) diseases, especially the small intestine, due to its
advantages of non-invasive and painless imaging of the entire digestive
tract.However, the huge amount of image data captured by WCE makes manual
reading a process that requires a huge amount of tasks and can easily lead to
missed detection and false detection of lesions.Therefore, In this paper, we
propose a \textbf{T}wo-stage \textbf{M}ulti-scale \textbf{F}eature-fusion
learning network(\textbf{TMFNet}) to automatically detect small intestinal
polyps and tumors in WCE image sequences. Specifically, TMFNet consists of
lesion detection network and lesion identification network. Among them, the
former improves the feature extraction module and detection module based on the
traditional Faster R-CNN network, and readjusts the parameters of the anchor in
the region proposal network(RPN) module;the latter combines residual structure
and feature pyramid structure are used to build a small intestinal lesion
recognition network based on feature fusion, for reducing the false positive
rate of the former and improve the overall accuracy.We used 22,335 WCE images
in the experiment, with a total of 123,092 lesion regions used to train the
detection framework of this paper. In the experiment, the detection framework
is trained and tested on the real WCE image dataset provided by the hospital
gastroenterology department. The sensitivity, false positive and accuracy of
the final model on the RPM are 98.81$\%$, 7.43$\%$ and 92.57$\%$,
respectively.Meanwhile,the corresponding results on the lesion images were
98.75$\%$, 5.62$\%$ and 94.39$\%$. The algorithm model proposed in this paper
is obviously superior to other detection algorithms in detection effect and
performance</p>
  </details>
</details>
<details>
  <summary>123. <b>标题：Automatic Registration of Images with Inconsistent Content Through  Line-Support Region Segmentation and Geometrical Outlier Removal</b></summary>
  <p><b>编号</b>：[463]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00832</p>
  <p><b>作者</b>：Ming Zhao,  Yongpeng Wu,  Shengda Pan,  Fan Zhou,  Bowen An,  André Kaup</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：straight region whose points share roughly, provide reliable feature point matching, synthetic aperture radar images taken, automatic image registration approach, support region segmentation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The implementation of automatic image registration is still difficult in
various applications. In this paper, an automatic image registration approach
through line-support region segmentation and geometrical outlier removal
(ALRS-GOR) is proposed. This new approach is designed to address the problems
associated with the registration of images with affine deformations and
inconsistent content, such as remote sensing images with different spectral
content or noise interference, or map images with inconsistent annotations. To
begin with, line-support regions, namely a straight region whose points share
roughly the same image gradient angle, are extracted to address the issues of
inconsistent content existing in images. To alleviate the incompleteness of
line segments, an iterative strategy with multi-resolution is employed to
preserve global structures that are masked at full resolution by image details
or noise. Then, Geometrical Outlier Removal (GOR) is developed to provide
reliable feature point matching, which is based on affineinvariant geometrical
classifications for corresponding matches initialized by SIFT. The candidate
outliers are selected by comparing the disparity of accumulated classifications
among all matches, instead of conventional methods which only rely on local
geometrical relations. Various image sets have been considered in this paper
for the evaluation of the proposed approach, including aerial images with
simulated affine deformations, remote sensing optical and synthetic aperture
radar images taken at different situations (multispectral, multisensor, and
multitemporal), and map images with inconsistent annotations. Experimental
results demonstrate the superior performance of the proposed method over the
existing approaches for the whole data set.</p>
  </details>
</details>
<details>
  <summary>124. <b>标题：RFVTM: A Recovery and Filtering Vertex Trichotomy Matching for Remote  Sensing Image Registration</b></summary>
  <p><b>编号</b>：[464]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00818</p>
  <p><b>作者</b>：Ming Zhao,  Bowen An,  Yongpeng Wu,  Huynh Van Luong,  André Kaup</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：novel affine invariant descriptor called vertex trichotomy descriptor, robust feature point matching algorithm called recovery, reliable feature point matching, corresponding vertex trichotomy descriptors, identical vertex trichotomy descriptors</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reliable feature point matching is a vital yet challenging process in
feature-based image registration. In this paper,a robust feature point matching
algorithm called Recovery and Filtering Vertex Trichotomy Matching (RFVTM) is
proposed to remove outliers and retain sufficient inliers for remote sensing
images. A novel affine invariant descriptor called vertex trichotomy descriptor
is proposed on the basis of that geometrical relations between any of vertices
and lines are preserved after affine transformations, which is constructed by
mapping each vertex into trichotomy sets. The outlier removals in Vertex
Trichotomy Matching (VTM) are implemented by iteratively comparing the
disparity of corresponding vertex trichotomy descriptors. Some inliers
mistakenly validated by a large amount of outliers are removed in VTM
iterations, and several residual outliers close to correct locations cannot be
excluded with the same graph structures. Therefore, a recovery and filtering
strategy is designed to recover some inliers based on identical vertex
trichotomy descriptors and restricted transformation errors. Assisted with the
additional recovered inliers, residual outliers can also be filtered out during
the process of reaching identical graph for the expanded vertex sets.
Experimental results demonstrate the superior performance on precision and
stability of this algorithm under various conditions, such as remote sensing
images with large transformations, duplicated patterns, or inconsistent
spectral content.</p>
  </details>
</details>
<details>
  <summary>125. <b>标题：UNetFormer: A Unified Vision Transformer Model and Pre-Training  Framework for 3D Medical Image Segmentation</b></summary>
  <p><b>编号</b>：[475]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00631</p>
  <p><b>作者</b>：Ali Hatamizadeh,  Ziyue Xu,  Dong Yang,  Wenqi Li,  Holger Roth,  Daguang Xu</p>
  <p><b>备注</b>：Tech. report, 12 pages, 3 figures</p>
  <p><b>关键词</b>：predict randomly masked volumetric tokens using contextual information, brain tumor segmentation using mri images, liver tumor segmentation task using, recently become popular due, decoder via skip connections</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision Transformers (ViT)s have recently become popular due to their
outstanding modeling capabilities, in particular for capturing long-range
information, and scalability to dataset and model sizes which has led to
state-of-the-art performance in various computer vision and medical image
analysis tasks. In this work, we introduce a unified framework consisting of
two architectures, dubbed UNetFormer, with a 3D Swin Transformer-based encoder
and Convolutional Neural Network (CNN) and transformer-based decoders. In the
proposed model, the encoder is linked to the decoder via skip connections at
five different resolutions with deep supervision. The design of proposed
architecture allows for meeting a wide range of trade-off requirements between
accuracy and computational cost. In addition, we present a methodology for
self-supervised pre-training of the encoder backbone via learning to predict
randomly masked volumetric tokens using contextual information of visible
tokens. We pre-train our framework on a cohort of $5050$ CT images, gathered
from publicly available CT datasets, and present a systematic investigation of
various components such as masking ratio and patch size that affect the
representation learning capability and performance of downstream tasks. We
validate the effectiveness of our pre-training approach by fine-tuning and
testing our model on liver and liver tumor segmentation task using the Medical
Segmentation Decathlon (MSD) dataset and achieve state-of-the-art performance
in terms of various segmentation metrics. To demonstrate its generalizability,
we train and test the model on BraTS 21 dataset for brain tumor segmentation
using MRI images and outperform other methods in terms of Dice score. Code:
this https URL</p>
  </details>
</details>
<details>
  <summary>126. <b>标题：Extremely Low-light Image Enhancement with Scene Text Restoration</b></summary>
  <p><b>编号</b>：[476]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00630</p>
  <p><b>作者</b>：Pohao Hsu,  Che-Tsung Lin,  Chun Chet Ng,  Jie-Long Kew,  Mei Yih Tan,  Shang-Hong Lai,  Chee Seng Chan,  Christopher Zach</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：proposed model outperforms state, novel image enhancement framework, novel text detection loss, qualitative experimental results, made impressive progress</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning-based methods have made impressive progress in enhancing
extremely low-light images - the image quality of the reconstructed images has
generally improved. However, we found out that most of these methods could not
sufficiently recover the image details, for instance, the texts in the scene.
In this paper, a novel image enhancement framework is proposed to precisely
restore the scene texts, as well as the overall quality of the image
simultaneously under extremely low-light images conditions. Mainly, we employed
a self-regularised attention map, an edge map, and a novel text detection loss.
In addition, leveraging synthetic low-light images is beneficial for image
enhancement on the genuine ones in terms of text detection. The quantitative
and qualitative experimental results have shown that the proposed model
outperforms state-of-the-art methods in image restoration, text detection, and
text spotting on See In the Dark and ICDAR15 datasets.</p>
  </details>
</details>
<details>
  <summary>127. <b>标题：TopTemp: Parsing Precipitate Structure from Temper Topology</b></summary>
  <p><b>编号</b>：[477]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00629</p>
  <p><b>作者</b>：Lara Kassab,  Scott Howland,  Henry Kvinge,  Keerti Sahithi Kappagantula,  Tegan Emerson</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：presented work outperforms conventional deep learning baselines, first step towards improving understanding, labor -, time -,, captures domain interpretable features, advanced manufacturing process parameters</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Technological advances are in part enabled by the development of novel
manufacturing processes that give rise to new materials or material property
improvements. Development and evaluation of new manufacturing methodologies is
labor-, time-, and resource-intensive expensive due to complex, poorly defined
relationships between advanced manufacturing process parameters and the
resulting microstructures. In this work, we present a topological
representation of temper (heat-treatment) dependent material micro-structure,
as captured by scanning electron microscopy, called TopTemp. We show that this
topological representation is able to support temper classification of
microstructures in a data limited setting, generalizes well to previously
unseen samples, is robust to image perturbations, and captures domain
interpretable features. The presented work outperforms conventional deep
learning baselines and is a first step towards improving understanding of
process parameters and resulting material properties.</p>
  </details>
</details>
<details>
  <summary>128. <b>标题：Bayesian Image Super-Resolution with Deep Modeling of Image Statistics</b></summary>
  <p><b>编号</b>：[478]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00623</p>
  <p><b>作者</b>：Shangqi Gao,  Xiahai Zhuang</p>
  <p><b>备注</b>：45 pages</p>
  <p><b>关键词</b>：model real image degradation including blurring, three image restoration tasks, using deep neural networks, bayesian image restoration framework, e .,} ideal sisr</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Modeling statistics of image priors is useful for image super-resolution, but
little attention has been paid from the massive works of deep learning-based
methods. In this work, we propose a Bayesian image restoration framework, where
natural image statistics are modeled with the combination of smoothness and
sparsity priors. Concretely, firstly we consider an ideal image as the sum of a
smoothness component and a sparsity residual, and model real image degradation
including blurring, downscaling, and noise corruption. Then, we develop a
variational Bayesian approach to infer their posteriors. Finally, we implement
the variational approach for single image super-resolution (SISR) using deep
neural networks, and propose an unsupervised training strategy. The experiments
on three image restoration tasks, \textit{i.e.,} ideal SISR, realistic SISR,
and real-world SISR, demonstrate that our method has superior model
generalizability against varying noise levels and degradation kernels and is
effective in unsupervised SISR. The code and resulting models are released via
\url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>129. <b>标题：Universal Lymph Node Detection in T2 MRI using Neural Networks</b></summary>
  <p><b>编号</b>：[479]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00622</p>
  <p><b>作者</b>：Tejas Sudharshan Mathai,  Sungwon Lee,  Thomas C. Shen,  Zhiyong Lu,  Ronald M. Summers</p>
  <p><b>备注</b>：Accepted at CARS 2022 (CAR track)</p>
  <p><b>关键词</b>：volumetric t2 mri using neural networks, 122 test t2 mri volumes revealed, without hard negative example mining, trained various neural network models, 5 fp per volume ).</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Purpose: Identification of abdominal Lymph Nodes (LN) that are suspicious for
metastasis in T2 Magnetic Resonance Imaging (MRI) scans is critical for staging
of lymphoproliferative diseases. Prior work on LN detection has been limited to
specific anatomical regions of the body (pelvis, rectum) in single MR slices.
Therefore, the development of a universal approach to detect LN in full T2 MRI
volumes is highly desirable.
Methods: In this study, a Computer Aided Detection (CAD) pipeline to
universally identify abdominal LN in volumetric T2 MRI using neural networks is
proposed. First, we trained various neural network models for detecting LN:
Faster RCNN with and without Hard Negative Example Mining (HNEM), FCOS,
FoveaBox, VFNet, and Detection Transformer (DETR). Next, we show that the
state-of-the-art (SOTA) VFNet model with Adaptive Training Sample Selection
(ATSS) outperforms Faster RCNN with HNEM. Finally, we ensembled models that
surpassed a 45% mAP threshold. We found that the VFNet model and one-stage
model ensemble can be interchangeably used in the CAD pipeline.
Results: Experiments on 122 test T2 MRI volumes revealed that VFNet achieved
a 51.1% mAP and 78.7% recall at 4 false positives (FP) per volume, while the
one-stage model ensemble achieved a mAP of 52.3% and sensitivity of 78.7% at
4FP.
Conclusion: Our contribution is a CAD pipeline that detects LN in T2 MRI
volumes, resulting in a sensitivity improvement of $\sim$14 points over the
current SOTA method for LN detection (sensitivity of 78.7% at 4 FP vs. 64.6% at
5 FP per volume).</p>
  </details>
</details>
<details>
  <summary>130. <b>标题：Visual explanations for polyp detection: How medical doctors assess  intrinsic versus extrinsic explanations</b></summary>
  <p><b>编号</b>：[482]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00617</p>
  <p><b>作者</b>：Steven Hicks,  Andrea Storås,  Michael Riegler,  Cise Midoglu,  Malek Hammou,  Thomas de Lange,  Sravanthi Parasa,  Pål Halvorsen,  Inga Strümke</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：recent years achieved immense success, making medical professionals highly skeptical, gastrointestinal disease detection use case, art explainable artificial intelligence methods, compare two different categories</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning has in recent years achieved immense success in all areas of
computer vision and has the potential of assisting medical doctors in analyzing
visual content for disease and other abnormalities. However, the current state
of deep learning is very much a black box, making medical professionals highly
skeptical about integrating these methods into clinical practice. Several
methods have been proposed in order to shine some light onto these black boxes,
but there is no consensus on the opinion of the medical doctors that will
consume these explanations. This paper presents a study asking medical doctors
about their opinion of current state-of-the-art explainable artificial
intelligence methods when applied to a gastrointestinal disease detection use
case. We compare two different categories of explanation methods, intrinsic and
extrinsic, and gauge their opinion of the current value of these explanations.
The results indicate that intrinsic explanations are preferred and that
explanation.</p>
  </details>
</details>
<h1>自然语言处理</h1>
<details>
  <summary>1. <b>标题：Do As I Can, Not As I Say: Grounding Language in Robotic Affordances</b></summary>
  <p><b>编号</b>：[7]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01691</p>
  <p><b>作者</b>：Michael Ahn,  Anthony Brohan,  Noah Brown,  Yevgen Chebotar,  Omar Cortes,  Byron David,  Chelsea Finn,  Keerthana Gopalakrishnan,  Karol Hausman,  Alex Herzog,  Daniel Ho,  Jasmine Hsu,  Julian Ibarz,  Brian Ichter,  Alex Irpan,  Eric Jang,  Rosario Jauregui Ruano,  Kyle Jeffrey,  Sally Jesmonth,  Nikhil J Joshi,  Ryan Julian,  Dmitry Kalashnikov,  Yuheng Kuang,  Kuang-Huei Lee,  Sergey Levine,  Yao Lu,  Linda Luu,  Carolina Parada,  Peter Pastor,  Jornell Quiambao,  Kanishka Rao,  Jarek Rettinghouse,  Diego Reyes,  Pierre Sermanet,  Nicolas Sievers,  Clayton Tan,  Alexander Toshev,  Vincent Vanhoucke,  Fei Xia,  Ted Xiao,  Peng Xu,  Sichun Xu,  Mengyuan Yan</p>
  <p><b>备注</b>：See website at this https URL</p>
  <p><b>关键词</b>：language model supplies high, language model provides high, temporally extended instructions expressed, propose natural language actions, value functions associated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models can encode a wealth of semantic knowledge about the
world. Such knowledge could be extremely useful to robots aiming to act upon
high-level, temporally extended instructions expressed in natural language.
However, a significant weakness of language models is that they lack real-world
experience, which makes it difficult to leverage them for decision making
within a given embodiment. For example, asking a language model to describe how
to clean a spill might result in a reasonable narrative, but it may not be
applicable to a particular agent, such as a robot, that needs to perform this
task in a particular environment. We propose to provide real-world grounding by
means of pretrained skills, which are used to constrain the model to propose
natural language actions that are both feasible and contextually appropriate.
The robot can act as the language model's "hands and eyes," while the language
model supplies high-level semantic knowledge about the task. We show how
low-level skills can be combined with large language models so that the
language model provides high-level knowledge about the procedures for
performing complex and temporally-extended instructions, while value functions
associated with these skills provide the grounding necessary to connect this
knowledge to a particular physical environment. We evaluate our method on a
number of real-world robotic tasks, where we show the need for real-world
grounding and that this approach is capable of completing long-horizon,
abstract, natural language instructions on a mobile manipulator. The project's
website and the video can be found at this https URL</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Self-Supervised Speech Representations Preserve Speech Characteristics  while Anonymizing Voices</b></summary>
  <p><b>编号</b>：[13]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01677</p>
  <p><b>作者</b>：Abner Hernandez,  Paula Andrea Pérez-Toro,  Juan Camilo Vásquez-Correa,  Juan Rafael Orozco-Arroyave,  Andreas Maier,  Seung Hee Yang</p>
  <p><b>备注</b>：Submitted for review at Interspeech 2022</p>
  <p><b>关键词</b>：train several voice conversion models using self, low word error rate within 1, supervised speech representations including wav2vec2, equal error rate increases, based machine learning models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Collecting speech data is an important step in training speech recognition
systems and other speech-based machine learning models. However, the issue of
privacy protection is an increasing concern that must be addressed. The current
study investigates the use of voice conversion as a method for anonymizing
voices. In particular, we train several voice conversion models using
self-supervised speech representations including Wav2Vec2.0, Hubert and
UniSpeech. Converted voices retain a low word error rate within 1% of the
original voice. Equal error rate increases from 1.52% to 46.24% on the
LibriSpeech test set and from 3.75% to 45.84% on speakers from the VCTK corpus
which signifies degraded performance on speaker verification. Lastly, we
conduct experiments on dysarthric speech data to show that speech features
relevant to articulation, prosody, phonation and phonology can be extracted
from anonymized voices for discriminating between healthy and pathological
speech.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Cross-lingual Self-Supervised Speech Representations for Improved  Dysarthric Speech Recognition</b></summary>
  <p><b>编号</b>：[17]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01670</p>
  <p><b>作者</b>：Abner Hernandez,  Paula Andrea Pérez-Toro,  Elmar Nöth,  Juan Rafael Orozco-Arroyave,  Andreas Maier,  Seung Hee Yang</p>
  <p><b>备注</b>：Submitted for review at Interspeech 2022</p>
  <p><b>关键词</b>：uaspeech corpus ), spanish speakers, improve word error rate, cerebral palsy caused dysarthria, art automatic speech recognition, impaired speech still remains</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>State-of-the-art automatic speech recognition (ASR) systems perform well on
healthy speech. However, the performance on impaired speech still remains an
issue. The current study explores the usefulness of using Wav2Vec
self-supervised speech representations as features for training an ASR system
for dysarthric speech. Dysarthric speech recognition is particularly difficult
as several aspects of speech such as articulation, prosody and phonation can be
impaired. Specifically, we train an acoustic model with features extracted from
Wav2Vec, Hubert, and the cross-lingual XLSR model. Results suggest that speech
representations pretrained on large unlabelled data can improve word error rate
(WER) performance. In particular, features from the multilingual model led to
lower WERs than filterbanks (Fbank) or models trained on a single language.
Improvements were observed in English speakers with cerebral palsy caused
dysarthria (UASpeech corpus), Spanish speakers with Parkinsonian dysarthria
(PC-GITA corpus) and Italian speakers with paralysis-based dysarthria (EasyCall
corpus). Compared to using Fbank features, XLSR-based features reduced WERs by
6.8%, 22.0%, and 7.0% for the UASpeech, PC-GITA, and EasyCall corpus,
respectively.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：MetaAID: A Flexible Framework for Developing Metaverse Applications via  AI Technology and Human Editing</b></summary>
  <p><b>编号</b>：[34]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01614</p>
  <p><b>作者</b>：Hongyin Zhu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：flexible metaverse ai technology framework metaaid, economic internal circulation requires balanced, metaverse application development inevitably requires, existing metaverse application development lacks, framework summarizes common ai technologies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Achieving the expansion of domestic demand and the economic internal
circulation requires balanced and coordinated support from multiple industries
(domains) such as consumption, education, entertainment, engineering
infrastructure, etc., which is indispensable for maintaining economic
development. Metaverse applications may help with this task and can make many
industries more interesting, more efficient, and provide a better user
experience. The first challenge is that metaverse application development
inevitably requires the support of various artificial intelligence (AI)
technologies such as natural language processing (NLP), knowledge graph (KG),
computer vision (CV), and machine learning (ML), etc. However, existing
metaverse application development lacks a lightweight AI technology framework.
This paper proposes a flexible metaverse AI technology framework metaAID that
aims to support language and semantic technologies in the development of
digital twins and virtual humans. The second challenge is that the development
process of metaverse applications involves both technical development tasks and
manual editing work, and often becomes a heavyweight multi-team collaboration
project, not to mention the development of metaverse applications in multiple
industries. Our framework summarizes common AI technologies and application
development templates with common functional modules and interfaces. Based on
this framework, we have designed 5 applications for 3 industries around the
expansion of domestic demand and economic internal circulation. Experimental
results show that our framework can support AI technologies when developing
metaverse applications in different industries.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：LPAttack: A Feasible Annotation Scheme for Capturing Logic Pattern of  Attacks in Arguments</b></summary>
  <p><b>编号</b>：[72]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01512</p>
  <p><b>作者</b>：Farjana Sultana Mim,  Naoya Inoue,  Shoichi Naito,  Keshav Singh,  Kentaro Inui</p>
  <p><b>备注</b>：14 pages, 8 figures</p>
  <p><b>关键词</b>：annotation study shows moderate inter, often comprise complex rhetorical moves, arguer might neither deny, complex rhetorical moves, complex rhetorical moves</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In argumentative discourse, persuasion is often achieved by refuting or
attacking others arguments. Attacking is not always straightforward and often
comprise complex rhetorical moves such that arguers might agree with a logic of
an argument while attacking another logic. Moreover, arguer might neither deny
nor agree with any logics of an argument, instead ignore them and attack the
main stance of the argument by providing new logics and presupposing that the
new logics have more value or importance than the logics present in the
attacked argument. However, no existing studies in the computational
argumentation capture such complex rhetorical moves in attacks or the
presuppositions or value judgements in them. In order to address this gap, we
introduce LPAttack, a novel annotation scheme that captures the common modes
and complex rhetorical moves in attacks along with the implicit presuppositions
and value judgements in them. Our annotation study shows moderate
inter-annotator agreement, indicating that human annotation for the proposed
scheme is feasible. We publicly release our annotated corpus and the annotation
guidelines.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Estimating the Entropy of Linguistic Distributions</b></summary>
  <p><b>编号</b>：[91]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01469</p>
  <p><b>作者</b>：Aryaman Arora,  Clara Meister,  Ryan Cotterell</p>
  <p><b>备注</b>：21 pages (5 pages main text). 4 figures. Accepted to ACL 2022</p>
  <p><b>关键词</b>：two recent information, reported effect size, entropy must typically, underlying probability distribution, theoretic linguistic studies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Shannon entropy is often a quantity of interest to linguists studying the
communicative capacity of human language. However, entropy must typically be
estimated from observed data because researchers do not have access to the
underlying probability distribution that gives rise to these data. While
entropy estimation is a well-studied problem in other fields, there is not yet
a comprehensive exploration of the efficacy of entropy estimators for use with
linguistic data. In this work, we fill this void, studying the empirical
effectiveness of different entropy estimators for linguistic distributions. In
a replication of two recent information-theoretic linguistic studies, we find
evidence that the reported effect size is over-estimated due to over-reliance
on poor entropy estimators. Finally, we end our paper with concrete
recommendations for entropy estimation depending on distribution type and data
availability.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Criação e aplicação de ferramenta para auxiliar no ensino de  algoritmos e programação de computadores</b></summary>
  <p><b>编号</b>：[92]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01468</p>
  <p><b>作者</b>：Afonso Henriques Fontes Neto Segundo,  Joel Sotero da Cunha Neto,  Maria Daniela Santabaia Cavalcanti,  Paulo Cirillo Souza Barbosa,  Raul Fontenele Santana</p>
  <p><b>备注</b>：in Portuguese language</p>
  <p><b>关键词</b>：using video lessons, teaching tool developed, computer programming discipline, tool combines, work aims</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Knowledge about programming is part of the knowledge matrix that will be
required of the professionals of the future. Based on this, this work aims to
report the development of a teaching tool developed during the monitoring
program of the Algorithm and Computer Programming discipline of the University
of Fortaleza. The tool combines the knowledge acquired in the books, with a
language closer to the students, using video lessons and exercises proposed,
with all the content available on the internet. The preliminary results were
positive, with the students approving this new approach and believing that it
could contribute to a better performance in the discipline.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Learning Commonsense-aware Moment-Text Alignment for Fast Video Temporal  Grounding</b></summary>
  <p><b>编号</b>：[99]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01450</p>
  <p><b>作者</b>：Ziyue Wu,  Junyu Gao,  Shucheng Huang,  Changsheng Xu</p>
  <p><b>备注</b>：Code is available at this https URL</p>
  <p><b>关键词</b>：existing approaches adopt elaborately designed cross, grounding temporal video segments described, fast video temporal grounding, fast video temporal grounding, two challenging benchmarks show</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Grounding temporal video segments described in natural language queries
effectively and efficiently is a crucial capability needed in
vision-and-language fields. In this paper, we deal with the fast video temporal
grounding (FVTG) task, aiming at localizing the target segment with high speed
and favorable accuracy. Most existing approaches adopt elaborately designed
cross-modal interaction modules to improve the grounding performance, which
suffer from the test-time bottleneck. Although several common space-based
methods enjoy the high-speed merit during inference, they can hardly capture
the comprehensive and explicit relations between visual and textual modalities.
In this paper, to tackle the dilemma of speed-accuracy tradeoff, we propose a
commonsense-aware cross-modal alignment (CCA) framework, which incorporates
commonsense-guided visual and text representations into a complementary common
space for fast video temporal grounding. Specifically, the commonsense concepts
are explored and exploited by extracting the structural semantic information
from a language corpus. Then, a commonsense-aware interaction module is
designed to obtain bridged visual and text features by utilizing the learned
commonsense concepts. Finally, to maintain the original semantic information of
textual queries, a cross-modal complementary common space is optimized to
obtain matching scores for performing FVTG. Extensive results on two
challenging benchmarks show that our CCA method performs favorably against
state-of-the-arts while running at high speed. Our code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Using Pre-Trained Language Models for Producing Counter Narratives  Against Hate Speech: a Comparative Study</b></summary>
  <p><b>编号</b>：[103]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01440</p>
  <p><b>作者</b>：Serra Sinem Tekiroglu,  Helena Bonaldi,  Margherita Fanton,  Marco Guerini</p>
  <p><b>备注</b>：To appear in "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL): Findings"</p>
  <p><b>关键词</b>：fight online hate speech, trained language models, particular language model, particular decoding mechanism, autoregressive models combined</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we present an extensive study on the use of pre-trained
language models for the task of automatic Counter Narrative (CN) generation to
fight online hate speech in English. We first present a comparative study to
determine whether there is a particular Language Model (or class of LMs) and a
particular decoding mechanism that are the most appropriate to generate CNs.
Findings show that autoregressive models combined with stochastic decodings are
the most promising. We then investigate how an LM performs in generating a CN
with regard to an unseen target of hate. We find out that a key element for
successful `out of target' experiments is not an overall similarity with the
training data but the presence of a specific subset of training data, i.e. a
target that shares some commonalities with the test target that can be defined
a-priori. We finally introduce the idea of a pipeline based on the addition of
an automatic post-editing step to refine generated CNs.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：A Study of Gender Impact in Self-supervised Models for Speech-to-Text  Systems</b></summary>
  <p><b>编号</b>：[117]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01397</p>
  <p><b>作者</b>：Marcely Zanon Boito,  Laurent Besacier,  Natalia Tomashenko,  Yannick Estève</p>
  <p><b>备注</b>：submitted to INTERSPEECH 2022</p>
  <p><b>关键词</b>：observe lower overall performance using gender, relative performance difference measured, speech processing emerged recently, models containing different degrees, speech processing downstream tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Self-supervised models for speech processing emerged recently as popular
foundation blocks in speech processing pipelines. These models are pre-trained
on unlabeled audio data and then used in speech processing downstream tasks
such as automatic speech recognition (ASR) or speech translation (ST). Since
these models are now used in research and industrial systems alike, it becomes
necessary to understand the impact caused by some features such as gender
distribution within pre-training data. Using French as our investigation
language, we train and compare gender-specific wav2vec 2.0 models against
models containing different degrees of gender balance in their pre-training
data. The comparison is performed by applying these models to two
speech-to-text downstream tasks: ASR and ST. Our results show that the type of
downstream integration matters. We observe lower overall performance using
gender-specific pre-training before fine-tuning an end-to-end ASR system.
However, when self-supervised models are used as feature extractors, the
overall ASR and ST results follow more complex patterns, in which the balanced
pre-trained model is not necessarily the best option. Lastly, our crude
'fairness' metric, the relative performance difference measured between female
and male test sets, does not display a strong variation from balanced to
gender-specific pre-trained wav2vec 2.0 models.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Aligned Weight Regularizers for Pruning Pretrained Neural Networks</b></summary>
  <p><b>编号</b>：[122]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01385</p>
  <p><b>作者</b>：James O' Neill,  Sourav Dutta,  Haytham Assem</p>
  <p><b>备注</b>：Accepted to ACL Findings 2022</p>
  <p><b>关键词</b>：propose two weight regularizers, comparing standard supervised learning, shot setting using xlm, lingual language model compression, representational degradation depending</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While various avenues of research have been explored for iterative pruning,
little is known what effect pruning has on zero-shot test performance and its
potential implications on the choice of pruning criteria. This pruning setup is
particularly important for cross-lingual models that implicitly learn alignment
between language representations during pretraining, which if distorted via
pruning, not only leads to poorer performance on language data used for
retraining but also on zero-shot languages that are evaluated.
In this work, we show that there is a clear performance discrepancy in
magnitude-based pruning when comparing standard supervised learning to the
zero-shot setting. From this finding, we propose two weight regularizers that
aim to maximize the alignment between units of pruned and unpruned networks to
mitigate alignment distortion in pruned cross-lingual models and perform well
for both non zero-shot and zero-shot settings.
We provide experimental results on cross-lingual tasks for the zero-shot
setting using XLM-RoBERTa$_{\mathrm{Base}}$, where we also find that pruning
has varying degrees of representational degradation depending on the language
corresponding to the zero-shot test set. This is also the first study that
focuses on cross-lingual language model compression.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Analysis of Joint Speech-Text Embeddings for Semantic Matching</b></summary>
  <p><b>编号</b>：[176]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01235</p>
  <p><b>作者</b>：Muhammad Huzaifah,  Ivan Kukanov</p>
  <p><b>备注</b>：Submitted to INTERSPEECH 2022 for review</p>
  <p><b>关键词</b>：quantitative retrieval accuracy metric, language processing problems involving, pretrained language model acting, incorporate automatic speech recognition, text embedding space trained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Embeddings play an important role in many recent end-to-end solutions for
language processing problems involving more than one data modality. Although
there has been some effort to understand the properties of single-modality
embedding spaces, particularly that of text, their cross-modal counterparts are
less understood. In this work, we study a joint speech-text embedding space
trained for semantic matching by minimizing the distance between paired
utterance and transcription inputs. This was done through dual encoders in a
teacher-student model setup, with a pretrained language model acting as the
teacher and a transformer-based speech encoder as the student. We extend our
method to incorporate automatic speech recognition through both pretraining and
multitask scenarios and found that both approaches improve semantic matching.
Multiple techniques were utilized to analyze and evaluate cross-modal semantic
alignment of the embeddings: a quantitative retrieval accuracy metric,
zero-shot classification to investigate generalizability, and probing of the
encoders to observe the extent of knowledge transfer from one modality to
another.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Diverse Text Generation via Variational Encoder-Decoder Models with  Gaussian Process Priors</b></summary>
  <p><b>编号</b>：[180]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01227</p>
  <p><b>作者</b>：Wanyu Du,  Jianqiao Zhao,  Liwei Wang,  Yangfeng Ji</p>
  <p><b>备注</b>：Accepted by 6th Workshop on Structured Prediction for NLP at ACL2022</p>
  <p><b>关键词</b>：novel latent structured variable model, two typical text generation tasks, map deterministic encoder hidden states, efficient variational inference approach, current methods mostly focus</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generating high quality texts with high diversity is important for many NLG
applications, but current methods mostly focus on building deterministic models
to generate higher quality texts and do not provide many options for promoting
diversity. In this work, we present a novel latent structured variable model to
generate high quality texts by enriching contextual representation learning of
encoder-decoder models. Specifically, we introduce a stochastic function to map
deterministic encoder hidden states into random context variables. The proposed
stochastic function is sampled from a Gaussian process prior to (1) provide
infinite number of joint Gaussian distributions of random context variables
(diversity-promoting) and (2) explicitly model dependency between context
variables (accurate-encoding). To address the learning challenge of Gaussian
processes, we propose an efficient variational inference approach to
approximate the posterior distribution of random context variables. We evaluate
our method in two typical text generation tasks: paraphrase generation and text
style transfer. Experimental results on benchmark datasets demonstrate that our
method improves the generation quality and diversity compared with other
baselines.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：A Part-of-Speech Tagger for Yiddish: First Steps in Tagging the Yiddish  Book Center Corpus</b></summary>
  <p><b>编号</b>：[197]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01175</p>
  <p><b>作者</b>：Seth Kulick,  Neville Ryant,  Beatrice Santorini,  Joel Wallenberg</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：relationships among spelling variants without, embeddings improve tagger performance, many spelling inconsistencies, even simple non, combine two resources</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We describe the construction and evaluation of a part-of-speech tagger for
Yiddish (the first one, to the best of our knowledge). This is the first step
in a larger project of automatically assigning part-of-speech tags and
syntactic structure to Yiddish text for purposes of linguistic research. We
combine two resources for the current work - an 80K word subset of the Penn
Parsed Corpus of Historical Yiddish (PPCHY) (Santorini, 2021) and 650 million
words of OCR'd Yiddish text from the Yiddish Book Center (YBC). We compute word
embeddings on the YBC corpus, and these embeddings are used with a tagger model
trained and evaluated on the PPCHY. Yiddish orthography in the YBC corpus has
many spelling inconsistencies, and we present some evidence that even simple
non-contextualized embeddings are able to capture the relationships among
spelling variants without the need to first "standardize" the corpus. We
evaluate the tagger performance on a 10-fold cross-validation split, with and
without the embeddings, showing that the embeddings improve tagger performance.
However, a great deal of work remains to be done, and we conclude by discussing
some next steps, including the need for additional annotated training and test
data.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：PERFECT: Prompt-free and Efficient Few-shot Learning with Language  Models</b></summary>
  <p><b>编号</b>：[198]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01172</p>
  <p><b>作者</b>：Rabeeh Karimi Mahabadi,  Luke Zettlemoyer,  James Henderson,  Marzieh Saeidi,  Lambert Mathias,  Veselin Stoyanov,  Majid Yazdani</p>
  <p><b>备注</b>：ACL, 2022</p>
  <p><b>关键词</b>：perfect makes two key design choices, also enable nearly 100x faster training, also outperforms existing state, require carefully engineered prompts, pretrained masked language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Current methods for few-shot fine-tuning of pretrained masked language models
(PLMs) require carefully engineered prompts and verbalizers for each new task
to convert examples into a cloze-format that the PLM can score. In this work,
we propose PERFECT, a simple and efficient method for few-shot fine-tuning of
PLMs without relying on any such handcrafting, which is highly effective given
as few as 32 data points. PERFECT makes two key design choices: First, we show
that manually engineered task prompts can be replaced with task-specific
adapters that enable sample-efficient fine-tuning and reduce memory and storage
costs by roughly factors of 5 and 100, respectively. Second, instead of using
handcrafted verbalizers, we learn new multi-token label embeddings during
fine-tuning, which are not tied to the model vocabulary and which allow us to
avoid complex auto-regressive decoding. These embeddings are not only learnable
from limited data but also enable nearly 100x faster training and inference.
Experiments on a wide range of few-shot NLP tasks demonstrate that PERFECT,
while being simple and efficient, also outperforms existing state-of-the-art
few-shot learning methods. Our code is publicly available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Why Exposure Bias Matters: An Imitation Learning Perspective of Error  Accumulation in Language Generation</b></summary>
  <p><b>编号</b>：[199]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01171</p>
  <p><b>作者</b>：Kushal Arora,  Layla El Asri,  Hareesh Bahuleyan,  Jackie Chi Kit Cheung</p>
  <p><b>备注</b>：Accepted in Findings of ACL 2022</p>
  <p><b>关键词</b>：current language generation models suffer, poor generation quality, generation procedure mismatch, imitation learning perspective, exposure bias leads</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Current language generation models suffer from issues such as repetition,
incoherence, and hallucinations. An often-repeated hypothesis is that this
brittleness of generation models is caused by the training and the generation
procedure mismatch, also referred to as exposure bias. In this paper, we verify
this hypothesis by analyzing exposure bias from an imitation learning
perspective. We show that exposure bias leads to an accumulation of errors,
analyze why perplexity fails to capture this accumulation, and empirically show
that this accumulation results in poor generation quality. Source code to
reproduce these experiments is available at
this https URL</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Pragmatic constraints and pronoun reference disambiguation: the possible  and the impossible</b></summary>
  <p><b>编号</b>：[200]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01166</p>
  <p><b>作者</b>：Ernest Davis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：natural text often refer, discourse often requires, parallel syntactic structures, implicitly mentioned previously, extended literary texts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pronoun disambiguation in understanding text and discourse often requires the
application of both general pragmatic knowledge and context-specific
information. In AI and linguistics research, this has mostly been studied in
cases where the referent is explicitly stated in the preceding text nearby.
However, pronouns in natural text often refer to entities, collections, or
events that are only implicitly mentioned previously; in those cases the need
to use pragmatic knowledge to disambiguate becomes much more acute and the
characterization of the knowledge becomes much more difficult. Extended
literary texts at times employ both extremely complex patterns of reference and
extremely rich and subtle forms of knowledge. Indeed, it is occasionally
possible to have a pronoun that is far separated from its referent in a text.
In the opposite direction, pronoun use is affected by considerations of focus
of attention and by formal constraints such as a preference for parallel
syntactic structures; these can be so strong that no pragmatic knowledge
suffices to overrule them.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：A sequence-to-sequence approach for document-level relation extraction</b></summary>
  <p><b>编号</b>：[227]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01098</p>
  <p><b>作者</b>：John Giorgi,  Gary D. Bader,  Bo Wang</p>
  <p><b>备注</b>：Accepted to BioNLP 2022 @ ACL 2022</p>
  <p><b>关键词</b>：docre requires integrating information within, {\ small {\ url, {\ small {\ url, several popular biomedical datasets, https url }}}.</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Motivated by the fact that many relations cross the sentence boundary, there
has been increasing interest in document-level relation extraction (DocRE).
DocRE requires integrating information within and across sentences, capturing
complex interactions between mentions of entities. Most existing methods are
pipeline-based, requiring entities as input. However, jointly learning to
extract entities and relations can improve performance and be more efficient
due to shared parameters and training steps. In this paper, we develop a
sequence-to-sequence approach, seq2rel, that can learn the subtasks of DocRE
(entity extraction, coreference resolution and relation extraction) end-to-end,
replacing a pipeline of task-specific components. Using a simple strategy we
call entity hinting, we compare our approach to existing pipeline-based methods
on several popular biomedical datasets, in some cases exceeding their
performance. We also report the first end-to-end results on these datasets for
future comparison. Finally, we demonstrate that, under our model, an end-to-end
approach outperforms a pipeline-based approach. Our code, data and trained
models are available at {\small{\url{this https URL}}}.
An online demo is available at
{\small{\url{this https URL}}}.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Task2Dial: A Novel Task and Dataset for Commonsense enhanced Task-based  Dialogue Grounded in Documents</b></summary>
  <p><b>编号</b>：[242]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01061</p>
  <p><b>作者</b>：Carl Strathearn,  Dimitra Gkatzia</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：task2dial dataset poses new challenges, human reference texts show, 79 tokens per turn, task2dial dataset contains dialogues, generating requires planning based</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper proposes a novel task on commonsense-enhanced task-based dialogue
grounded in documents and describes the Task2Dial dataset, a novel dataset of
document-grounded task-based dialogues, where an Information Giver (IG)
provides instructions (by consulting a document) to an Information Follower
(IF), so that the latter can successfully complete the task. In this unique
setting, the IF can ask clarification questions which may not be grounded in
the underlying document and require commonsense knowledge to be answered. The
Task2Dial dataset poses new challenges: (1) its human reference texts show more
lexical richness and variation than other document-grounded dialogue datasets;
(2) generating from this set requires paraphrasing as instructional responses
might have been modified from the underlying document; (3) requires commonsense
knowledge, since questions might not necessarily be grounded in the document;
(4) generating requires planning based on context, as task steps need to be
provided in order. The Task2Dial dataset contains dialogues with an average
$18.15$ number of turns and 19.79 tokens per turn, as compared to 12.94 and 12
respectively in existing datasets. As such, learning from this dataset promises
more natural, varied and less template-like system utterances.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：On Efficiently Acquiring Annotations for Multilingual Models</b></summary>
  <p><b>编号</b>：[252]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01016</p>
  <p><b>作者</b>：Joel Ruben Antony Moniz,  Barun Patra,  Matthew R. Gormley</p>
  <p><b>备注</b>：ACL 2022 (Short Paper)</p>
  <p><b>关键词</b>：joint learning across multiple languages using, annotation budget divided equally among, single model performs substantially better, active learning provides additional, supporting multiple languages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>When tasked with supporting multiple languages for a given problem, two
approaches have arisen: training a model for each language with the annotation
budget divided equally among them, and training on a high-resource language
followed by zero-shot transfer to the remaining languages. In this work, we
show that the strategy of joint learning across multiple languages using a
single model performs substantially better than the aforementioned
alternatives. We also demonstrate that active learning provides additional,
complementary benefits. We show that this simple approach enables the model to
be data efficient by allowing it to arbitrate its annotation budget to query
languages it is less certain on. We illustrate the effectiveness of our
proposed method on a diverse set of tasks: a classification task with 4
languages, a sequence tagging task with 4 languages and a dependency parsing
task with 5 languages. Our proposed method, whilst simple, substantially
outperforms the other viable alternatives for building a model in a
multilingual setting under constrained budgets.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Learning Disentangled Semantic Representations for Zero-Shot  Cross-Lingual Transfer in Multilingual Machine Reading Comprehension</b></summary>
  <p><b>编号</b>：[259]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00996</p>
  <p><b>作者</b>：injuan Wu,  Shaojuan Wu,  Xiaowang Zhang,  Deyi Xiong,  Shizhan Chen,  Zhiqiang Zhuang,  Zhiyong Feng</p>
  <p><b>备注</b>：Accepted to ACL 2022 (main conference)</p>
  <p><b>关键词</b>：different languages could make answer spans predicted, novel multilingual mrc framework equipped, shot transfer violate syntactic constraints, three multilingual mrc datasets, siamese semantic disentanglement model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multilingual pre-trained models are able to zero-shot transfer knowledge from
rich-resource to low-resource languages in machine reading comprehension (MRC).
However, inherent linguistic discrepancies in different languages could make
answer spans predicted by zero-shot transfer violate syntactic constraints of
the target language. In this paper, we propose a novel multilingual MRC
framework equipped with a Siamese Semantic Disentanglement Model (SSDM) to
disassociate semantics from syntax in representations learned by multilingual
pre-trained models. To explicitly transfer only semantic knowledge to the
target language, we propose two groups of losses tailored for semantic and
syntactic encoding and disentanglement. Experimental results on three
multilingual MRC datasets (i.e., XQuAD, MLQA, and TyDi QA) demonstrate the
effectiveness of our proposed approach over models based on mBERT and XLM-100.
Code is available at:this https URL.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Deep Speech Based End-to-End Automated Speech Recognition (ASR) for  Indian-English Accents</b></summary>
  <p><b>编号</b>：[269]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00977</p>
  <p><b>作者</b>：Priyank Dubey,  Bilal Shah</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：training system utilizing multiple graphical processing units, used transfer learning approach using, optimized recurrent neural network, mostly done using american, art speech recognition system</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automated Speech Recognition (ASR) is an interdisciplinary application of
computer science and linguistics that enable us to derive the transcription
from the uttered speech waveform. It finds several applications in Military
like High-performance fighter aircraft, helicopters, air-traffic controller.
Other than military speech recognition is used in healthcare, persons with
disabilities and many more. ASR has been an active research area. Several
models and algorithms for speech to text (STT) have been proposed. One of the
most recent is Mozilla Deep Speech, it is based on the Deep Speech research
paper by Baidu. Deep Speech is a state-of-art speech recognition system is
developed using end-to-end deep learning, it is trained using well-optimized
Recurrent Neural Network (RNN) training system utilizing multiple Graphical
Processing Units (GPUs). This training is mostly done using American-English
accent datasets, which results in poor generalizability to other English
accents. India is a land of vast diversity. This can even be seen in the
speech, there are several English accents which vary from state to state. In
this work, we have used transfer learning approach using most recent Deep
Speech model i.e., deepspeech-0.9.3 to develop an end-to-end speech recognition
system for Indian-English accents. This work utilizes fine-tuning and data
argumentation to further optimize and improve the Deep Speech ASR system. Indic
TTS data of Indian-English accents is used for transfer learning and
fine-tuning the pre-trained Deep Speech model. A general comparison is made
among the untrained model, our trained model and other available speech
recognition services for Indian-English Accents.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Question-Driven Graph Fusion Network For Visual Question Answering</b></summary>
  <p><b>编号</b>：[271]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00975</p>
  <p><b>作者</b>：Yuxi Qian,  Yuncong Hu,  Ruonan Wang,  Fangxiang Feng,  Xiaojie Wang</p>
  <p><b>备注</b>：Accepted by ICME 2022</p>
  <p><b>关键词</b>：inevitably introduces irrelevant information brought, driven graph fusion network, explored various visual relationships, three graph attention networks, novel graph aggregation method</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing Visual Question Answering (VQA) models have explored various visual
relationships between objects in the image to answer complex questions, which
inevitably introduces irrelevant information brought by inaccurate object
detection and text grounding. To address the problem, we propose a
Question-Driven Graph Fusion Network (QD-GFN). It first models semantic,
spatial, and implicit visual relations in images by three graph attention
networks, then question information is utilized to guide the aggregation
process of the three graphs, further, our QD-GFN adopts an object filtering
mechanism to remove question-irrelevant objects contained in the image.
Experiment results demonstrate that our QD-GFN outperforms the prior
state-of-the-art on both VQA 2.0 and VQA-CP v2 datasets. Further analysis shows
that both the novel graph aggregation method and object filtering mechanism
play a significant role in improving the performance of the model.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Exploiting Local and Global Features in Transformer-based Extreme  Multi-label Text Classification</b></summary>
  <p><b>编号</b>：[289]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00933</p>
  <p><b>作者</b>：Ruohong Zhang,  Yau-Shian Wang,  Yiming Yang,  Tom Vu,  Likun Lei</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：level features could bring additional gains, proposed model either outperforms, made significant performance improvements, represent different granularity levels, global feature vector may</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Extreme multi-label text classification (XMTC) is the task of tagging each
document with the relevant labels from a very large space of predefined
categories. Recently, large pre-trained Transformer models have made
significant performance improvements in XMTC, which typically use the embedding
of the special CLS token to represent the entire document semantics as a global
feature vector, and match it against candidate labels. However, we argue that
such a global feature vector may not be sufficient to represent different
granularity levels of semantics in the document, and that complementing it with
the local word-level features could bring additional gains. Based on this
insight, we propose an approach that combines both the local and global
features produced by Transformer models to improve the prediction power of the
classifier. Our experiments show that the proposed model either outperforms or
is comparable to the state-of-the-art methods on benchmark datasets.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：BERT-Assisted Semantic Annotation Correction for Emotion-Related  Questions</b></summary>
  <p><b>编号</b>：[298]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00916</p>
  <p><b>作者</b>：Abe Kazemzadeh</p>
  <p><b>备注</b>：Presented at 6th International Workshop on Annotation of useR Data for UbiquitOUs Systems (ARDUOUS) Workshop of IEEE Pervasive Computing (Percom) conference, Mar. 25, 2022. To replicate this experiment please see this https URL and this https URL</p>
  <p><b>关键词</b>：asking game called emotion twenty questions, contain embedded linguistic information, bert neural language model, natural language processing, feed information back</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Annotated data have traditionally been used to provide the input for training
a supervised machine learning (ML) model. However, current pre-trained ML
models for natural language processing (NLP) contain embedded linguistic
information that can be used to inform the annotation process. We use the BERT
neural language model to feed information back into an annotation task that
involves semantic labelling of dialog behavior in a question-asking game called
Emotion Twenty Questions (EMO20Q). First we describe the background of BERT,
the EMO20Q data, and assisted annotation tasks. Then we describe the methods
for fine-tuning BERT for the purpose of checking the annotated labels. To do
this, we use the paraphrase task as a way to check that all utterances with the
same annotation label are classified as paraphrases of each other. We show this
method to be an effective way to assess and revise annotations of textual user
data with complex, utterance-level semantic labels.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Moment-based Adversarial Training for Embodied Language Comprehension</b></summary>
  <p><b>编号</b>：[309]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00889</p>
  <p><b>作者</b>：Shintaro Ishikawa,  Komei Sugiura</p>
  <p><b>备注</b>：Accepted for presentation at ICPR2022</p>
  <p><b>关键词</b>：existing methods sometimes fail, uses two types, still far lower, execute household tasks, coffee maker ,"</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we focus on a vision-and-language task in which a robot is
instructed to execute household tasks. Given an instruction such as "Rinse off
a mug and place it in the coffee maker," the robot is required to locate the
mug, wash it, and put it in the coffee maker. This is challenging because the
robot needs to break down the instruction sentences into subgoals and execute
them in the correct order. On the ALFRED benchmark, the performance of
state-of-the-art methods is still far lower than that of humans. This is
partially because existing methods sometimes fail to infer subgoals that are
not explicitly specified in the instruction sentences. We propose Moment-based
Adversarial Training (MAT), which uses two types of moments for perturbation
updates in adversarial training. We introduce MAT to the embedding spaces of
the instruction, subgoals, and state representations to handle their varieties.
We validated our method on the ALFRED benchmark, and the results demonstrated
that our method outperformed the baseline method for all the metrics on the
benchmark.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Inverse is Better! Fast and Accurate Prompt for Few-shot Slot Tagging</b></summary>
  <p><b>编号</b>：[311]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00885</p>
  <p><b>作者</b>：Yutai Hou,  Cheng Chen,  Xianzhen Luo,  Bohan Li,  Wanxiang Che</p>
  <p><b>备注</b>：Accepted by ACL-findings 2022</p>
  <p><b>关键词</b>：reversely predict slot values given slot types, prompting methods recently achieve impressive success, classic prompts mapping tokens, methods modify input samples, since slot tagging samples</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Prompting methods recently achieve impressive success in few-shot learning.
These methods modify input samples with prompt sentence pieces, and decode
label tokens to map samples to corresponding labels. However, such a paradigm
is very inefficient for the task of slot tagging. Since slot tagging samples
are multiple consecutive words in a sentence, the prompting methods have to
enumerate all n-grams token spans to find all the possible slots, which greatly
slows down the prediction. To tackle this, we introduce an inverse paradigm for
prompting. Different from the classic prompts mapping tokens to labels, we
reversely predict slot values given slot types. Such inverse prompting only
requires a one-turn prediction for each slot type and greatly speeds up the
prediction. Besides, we propose a novel Iterative Prediction Strategy, from
which the model learns to refine predictions by considering the relations
between different slot types. We find, somewhat surprisingly, the proposed
method not only predicts faster but also significantly improves the effect
(improve over 6.1 F1-scores on 10-shot setting) and achieves new
state-of-the-art performance.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Co-VQA : Answering by Interactive Sub Question Sequence</b></summary>
  <p><b>编号</b>：[313]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00879</p>
  <p><b>作者</b>：Ruonan Wang,  Yuxi Qian,  Fangxiang Feng,  Xiaojie Wang,  Huixing Jiang</p>
  <p><b>备注</b>：Accepted by Findings of ACL 2022</p>
  <p><b>关键词</b>：sqss help build direct semantic connections, adaptive chain visual reasoning model, length reasoning chains, visual representation sequentially, perform supervised learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Most existing approaches to Visual Question Answering (VQA) answer questions
directly, however, people usually decompose a complex question into a sequence
of simple sub questions and finally obtain the answer to the original question
after answering the sub question sequence(SQS). By simulating the process, this
paper proposes a conversation-based VQA (Co-VQA) framework, which consists of
three components: Questioner, Oracle, and Answerer. Questioner raises the sub
questions using an extending HRED model, and Oracle answers them one-by-one. An
Adaptive Chain Visual Reasoning Model (ACVRM) for Answerer is also proposed,
where the question-answer pair is used to update the visual representation
sequentially. To perform supervised learning for each model, we introduce a
well-designed method to build a SQS for each question on VQA 2.0 and VQA-CP v2
datasets. Experimental results show that our method achieves state-of-the-art
on VQA-CP v2. Further analyses show that SQSs help build direct semantic
connections between questions and images, provide question-adaptive
variable-length reasoning chains, and with explicit interpretability as well as
error traceability.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Accurate Online Posterior Alignments for Principled  Lexically-Constrained Decoding</b></summary>
  <p><b>编号</b>：[316]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00871</p>
  <p><b>作者</b>：Soumya Chatterjee,  Sunita Sarawagi,  Preethi Jyothi</p>
  <p><b>备注</b>：15 pages, 2 figures. ACL 2022</p>
  <p><b>关键词</b>：good online alignments facilitate important applications, proposed inference technique jointly considers alignment, seamlessly integrated within existing constrained beam, including two distant language pairs, seven lexically constrained translation tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Online alignment in machine translation refers to the task of aligning a
target word to a source word when the target sequence has only been partially
decoded. Good online alignments facilitate important applications such as
lexically constrained translation where user-defined dictionaries are used to
inject lexical constraints into the translation model. We propose a novel
posterior alignment technique that is truly online in its execution and
superior in terms of alignment error rates compared to existing methods. Our
proposed inference technique jointly considers alignment and token
probabilities in a principled manner and can be seamlessly integrated within
existing constrained beam-search decoding algorithms. On five language pairs,
including two distant language pairs, we achieve consistent drop in alignment
error rates. When deployed on seven lexically constrained translation tasks, we
achieve significant improvements in BLEU specifically around the constrained
positions.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：CTRLEval: An Unsupervised Reference-Free Metric for Evaluating  Controlled Text Generation</b></summary>
  <p><b>编号</b>：[319]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00862</p>
  <p><b>作者</b>：Pei Ke,  Hao Zhou,  Yankai Lin,  Peng Li,  Jie Zhou,  Xiaoyan Zhu,  Minlie Huang</p>
  <p><b>备注</b>：Accepted by ACL 2022 (Main Conference)</p>
  <p><b>关键词</b>：whereas supervised ones may overfit task, evaluating controlled text generation models, evaluates controlled text generation, trained language model without, multiple text infilling tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing reference-free metrics have obvious limitations for evaluating
controlled text generation models. Unsupervised metrics can only provide a
task-agnostic evaluation result which correlates weakly with human judgments,
whereas supervised ones may overfit task-specific data with poor generalization
ability to other datasets. In this paper, we propose an unsupervised
reference-free metric called CTRLEval, which evaluates controlled text
generation from different aspects by formulating each aspect into multiple text
infilling tasks. On top of these tasks, the metric assembles the generation
probabilities from a pre-trained language model without any model training.
Experimental results show that our metric has higher correlations with human
judgments than other baselines, while obtaining better generalization of
evaluating generated texts from different models and with different qualities.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Efficient comparison of sentence embeddings</b></summary>
  <p><b>编号</b>：[337]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00820</p>
  <p><b>作者</b>：Spyros Zoupanos,  Stratis Kolovos,  Athanasios Kanavos,  Orestis Papadimitriou,  Manolis Maragoudakis</p>
  <p><b>备注</b>：8 pages, 2 figures</p>
  <p><b>关键词</b>：problem transformation raises new challenges like, two vector comparison approaches, like semantic similarity, perform vector comparisons, natural language processing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The domain of natural language processing (NLP), which has greatly evolved
over the last years, has highly benefited from the recent developments in word
and sentence embeddings. Such embeddings enable the transformation of complex
NLP tasks, like semantic similarity or Question and Answering (Q\&A), into much
simpler to perform vector comparisons. However, such a problem transformation
raises new challenges like the efficient comparison of embeddings and their
manipulation. In this work, we will discuss about various word and sentence
embeddings algorithms, we will select a sentence embedding algorithm, BERT, as
our algorithm of choice and we will evaluate the performance of two vector
comparison approaches, FAISS and Elasticsearch, in the specific problem of
sentence embeddings. According to the results, FAISS outperforms Elasticsearch
when used in a centralized environment with only one node, especially when big
datasets are included.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Constrained Sequence-to-Tree Generation for Hierarchical Text  Classification</b></summary>
  <p><b>编号</b>：[340]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00811</p>
  <p><b>作者</b>：Chao Yu,  Yi Shen,  Yue Mao,  Longjun Cai</p>
  <p><b>备注</b>：Accepted by SIGIR-2022</p>
  <p><b>关键词</b>：multiple hierarchically structured categories within, proposed approach achieves significant, prior studies consider htc, three benchmark datasets, hierarchical text classification</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Hierarchical Text Classification (HTC) is a challenging task where a document
can be assigned to multiple hierarchically structured categories within a
taxonomy. The majority of prior studies consider HTC as a flat multi-label
classification problem, which inevitably leads to "label inconsistency"
problem. In this paper, we formulate HTC as a sequence generation task and
introduce a sequence-to-tree framework (Seq2Tree) for modeling the hierarchical
label structure. Moreover, we design a constrained decoding strategy with
dynamic vocabulary to secure the label consistency of the results. Compared
with previous works, the proposed approach achieves significant and consistent
improvements on three benchmark datasets.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：HLDC: Hindi Legal Documents Corpus</b></summary>
  <p><b>编号</b>：[341]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00806</p>
  <p><b>作者</b>：Arnav Kapoor,  Mudit Dhawan,  Anmol Goel,  T.H. Arjun,  Akshala Bhatnagar,  Vibhu Agrawal,  Amul Agrawal,  Arnab Bhattacharya,  Ponnurangam Kumaraguru,  Ashutosh Modi</p>
  <p><b>备注</b>：16 Pages, Accepted at ACL 2022 Findings</p>
  <p><b>关键词</b>：many populous countries including india, could process legal documents, mtl models use summarization, hindi legal documents corpus, augment legal practitioners</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Many populous countries including India are burdened with a considerable
backlog of legal cases. Development of automated systems that could process
legal documents and augment legal practitioners can mitigate this. However,
there is a dearth of high-quality corpora that is needed to develop such
data-driven systems. The problem gets even more pronounced in the case of low
resource languages such as Hindi. In this resource paper, we introduce the
Hindi Legal Documents Corpus (HLDC), a corpus of more than 900K legal documents
in Hindi. Documents are cleaned and structured to enable the development of
downstream applications. Further, as a use-case for the corpus, we introduce
the task of bail prediction. We experiment with a battery of models and propose
a Multi-Task Learning (MTL) based model for the same. MTL models use
summarization as an auxiliary task along with bail prediction as the main task.
Experiments with different models are indicative of the need for further
research in this area. We release the corpus and model implementation code with
this paper: this https URL</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：End-to-end model for named entity recognition from speech without paired  training data</b></summary>
  <p><b>编号</b>：[342]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00803</p>
  <p><b>作者</b>：Salima Mdhaffar,  Jarod Duret,  Titouan Parcollet,  Yannick Estève</p>
  <p><b>备注</b>：Submitted to INTERSPEECH 2022</p>
  <p><b>关键词</b>：extract semantic information directly, end neural approaches tend, zero paired audio data, end automatic speech recognition, extract semantic information</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent works showed that end-to-end neural approaches tend to become very
popular for spoken language understanding (SLU). Through the term end-to-end,
one considers the use of a single model optimized to extract semantic
information directly from the speech signal. A major issue for such models is
the lack of paired audio and textual data with semantic annotation. In this
paper, we propose an approach to build an end-to-end neural model to extract
semantic information in a scenario in which zero paired audio data is
available. Our approach is based on the use of an external model trained to
generate a sequence of vectorial representations from text. These
representations mimic the hidden representations that could be generated inside
an end-to-end automatic speech recognition (ASR) model by processing a speech
signal. An SLU neural module is then trained using these representations as
input and the annotated text as output. Last, the SLU module replaces the top
layers of the ASR model to achieve the construction of the end-to-end model.
Our experiments on named entity recognition, carried out on the QUAERO corpus,
show that this approach is very promising, getting better results than a
comparable cascade approach or than the use of synthetic voices.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Improving the Factual Accuracy of Abstractive Clinical Text  Summarization using Multi-Objective Optimization</b></summary>
  <p><b>编号</b>：[344]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00797</p>
  <p><b>作者</b>：Amanuel Alambo,  Tanvi Banerjee,  Krishnaprasad Thirunarayan,  Mia Cajita</p>
  <p><b>备注</b>：Accepted to EMBC 2022</p>
  <p><b>关键词</b>：jointly optimize three cost functions, optimizing different loss functions leads, different domains including news articles, indiana university chest x, massive training data come</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While there has been recent progress in abstractive summarization as applied
to different domains including news articles, scientific articles, and blog
posts, the application of these techniques to clinical text summarization has
been limited. This is primarily due to the lack of large-scale training data
and the messy/unstructured nature of clinical notes as opposed to other domains
where massive training data come in structured or semi-structured form.
Further, one of the least explored and critical components of clinical text
summarization is factual accuracy of clinical summaries. This is specifically
crucial in the healthcare domain, cardiology in particular, where an accurate
summary generation that preserves the facts in the source notes is critical to
the well-being of a patient. In this study, we propose a framework for
improving the factual accuracy of abstractive summarization of clinical text
using knowledge-guided multi-objective optimization. We propose to jointly
optimize three cost functions in our proposed architecture during training:
generative loss, entity loss and knowledge loss and evaluate the proposed
architecture on 1) clinical notes of patients with heart failure (HF), which we
collect for this study; and 2) two benchmark datasets, Indiana University Chest
X-ray collection (IU X-Ray), and MIMIC-CXR, that are publicly available. We
experiment with three transformer encoder-decoder architectures and demonstrate
that optimizing different loss functions leads to improved performance in terms
of entity-level factual accuracy.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：A Dual-Contrastive Framework for Low-Resource Cross-Lingual Named Entity  Recognition</b></summary>
  <p><b>编号</b>：[345]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00796</p>
  <p><b>作者</b>：Yingwen Fu,  Nankai Lin,  Ziyu Yang,  Shengyi Jiang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：corresponding translation cannot fully exploit, utilize knowledge distillation method, outperform multiple baseline methods, design two contrastive objectives, lingual named entity recognition</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cross-lingual Named Entity Recognition (NER) has recently become a research
hotspot because it can alleviate the data-hungry problem for low-resource
languages. However, few researches have focused on the scenario where the
source-language labeled data is also limited in some specific domains. A common
approach for this scenario is to generate more training data through
translation or generation-based data augmentation method. Unfortunately, we
find that simply combining source-language data and the corresponding
translation cannot fully exploit the translated data and the improvements
obtained are somewhat limited. In this paper, we describe our novel
dual-contrastive framework ConCNER for cross-lingual NER under the scenario of
limited source-language labeled data. Specifically, based on the
source-language samples and their translations, we design two contrastive
objectives for cross-language NER at different grammatical levels, namely
Translation Contrastive Learning (TCL) to close sentence representations
between translated sentence pairs and Label Contrastive Learning (LCL) to close
token representations within the same labels. Furthermore, we utilize knowledge
distillation method where the NER model trained above is used as the teacher to
train a student model on unlabeled target-language data to better fit the
target language. We conduct extensive experiments on a wide variety of target
languages, and the results demonstrate that ConCNER tends to outperform
multiple baseline methods. For reproducibility, our code for this paper is
available at this https URL.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：CL-XABSA: Contrastive Learning for Cross-lingual Aspect-based Sentiment  Analysis</b></summary>
  <p><b>编号</b>：[349]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00791</p>
  <p><b>作者</b>：Nankai Lin,  Yingwen Fu,  Xiaotian Lin,  Aimin Yang,  Shengyi Jiang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：perform knowledge distillation technology leveraging data, design two contrastive strategies, sentiment level contrastive learning, lingual data alignment instead, token level contrastive learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As an extensive research in the field of Natural language processing (NLP),
aspect-based sentiment analysis (ABSA) is the task of predicting the sentiment
expressed in a text relative to the corresponding aspect. Unfortunately, most
languages lack of sufficient annotation resources, thus more and more recent
researchers focus on cross-lingual aspect-based sentiment analysis (XABSA).
However, most recent researches only concentrate on cross-lingual data
alignment instead of model alignment. To this end, we propose a novel
framework, CL-XABSA: Contrastive Learning for Cross-lingual Aspect-Based
Sentiment Analysis. Specifically, we design two contrastive strategies, token
level contrastive learning of token embeddings (TL-CTE) and sentiment level
contrastive learning of token embeddings (SL-CTE), to regularize the semantic
space of source and target language to be more uniform. Since our framework can
receive datasets in multiple languages during training, our framework can be
adapted not only for XABSA task, but also for multilingual aspect-based
sentiment analysis (MABSA). To further improve the performance of our model, we
perform knowledge distillation technology leveraging data from unlabeled target
language. In the distillation XABSA task, we further explore the comparative
effectiveness of different data (source dataset, translated dataset, and
code-switched dataset). The results demonstrate that the proposed method has a
certain improvement in the three tasks of XABSA, distillation XABSA and MABSA.
For reproducibility, our code for this paper is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Metaphorical User Simulators for Evaluating Task-oriented Dialogue  Systems</b></summary>
  <p><b>编号</b>：[358]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00763</p>
  <p><b>作者</b>：Weiwei Sun,  Shuyu Guo,  Shuo Zhang,  Pengjie Ren,  Zhumin Chen,  Maarten de Rijke,  Zhaochun Ren</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：metaphorical user simulator demonstrates better consistency, mimic user behavior allow us, conducted using three tds datasets, approach demonstrates better generalization, e ., dialogue systems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Task-oriented dialogue systems (TDSs) are assessed mainly in an offline
setting or through human evaluation. The evaluation is often limited to
single-turn or very time-intensive. As an alternative, user simulators that
mimic user behavior allow us to consider a broad set of user goals to generate
human-like conversations for simulated evaluation. Employing existing user
simulators to evaluate TDSs is challenging as user simulators are primarily
designed to optimize dialogue policies for TDSs and have limited evaluation
capability. Moreover, the evaluation of user simulators is an open challenge.
In this work, we proposes a metaphorical user simulator for endto-end TDS
evaluation. We also propose a tester-based evaluation framework to generate
variants, i.e., dialogue systems with different capabilities. Our user
simulator constructs a metaphorical user model that assists the simulator in
reasoning by referring to prior knowledge when encountering new items. We
estimate the quality of simulators by checking the simulated interactions
between simulators and variants. Our experiments are conducted using three TDS
datasets. The metaphorical user simulator demonstrates better consistency with
manual evaluation than Agenda-based simulator and Seq2seq model on three
datasets; our tester framework demonstrates efficiency, and our approach
demonstrates better generalization and scalability.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：Generating recommendations for entity-oriented exploratory search</b></summary>
  <p><b>编号</b>：[367]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00743</p>
  <p><b>作者</b>：David Wadden,  Nikita Gupta,  Kenton Lee,  Kristina Toutanova</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：traditional query recommendation systems select recommendations, generate recommendation sets, input search query, thorough evaluations performed, recommendation set generation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce the task of recommendation set generation for entity-oriented
exploratory search. Given an input search query which is open-ended or
under-specified, the task is to present the user with an easily-understandable
collection of query recommendations, with the goal of facilitating domain
exploration or clarifying user intent. Traditional query recommendation systems
select recommendations by identifying salient keywords in retrieved documents,
or by querying an existing taxonomy or knowledge base for related concepts. In
this work, we build a text-to-text model capable of generating a collection of
recommendations directly, using the language model as a "soft" knowledge base
capable of proposing new concepts not found in an existing taxonomy or set of
retrieved documents. We train the model to generate recommendation sets which
optimize a cost function designed to encourage comprehensiveness,
interestingness, and non-redundancy. In thorough evaluations performed by crowd
workers, we confirm the generalizability of our approach and the high quality
of the generated recommendations.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Learning to Simplify with Data Hopelessly Out of Alignment</b></summary>
  <p><b>编号</b>：[368]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00741</p>
  <p><b>作者</b>：Tadashi Nomoto</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：exist among simplified sentences generated, ground truth simple sentences, text simplification without relying, current best performing system, call conjoined twin networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider whether it is possible to do text simplification without relying
on a "parallel" corpus, one that is made up of sentence-by-sentence alignments
of complex and ground truth simple sentences. To this end, we introduce a
number of concepts, some new and some not, including what we call Conjoined
Twin Networks, Flip-Flop Auto-Encoders (FFA) and Adversarial Networks (GAN). A
comparison is made between Jensen-Shannon (JS-GAN) and Wasserstein GAN, to see
how they impact performance, with stronger results for the former. An
experiment we conducted with a large dataset derived from Wikipedia found the
solid superiority of Twin Networks equipped with FFA and JS-GAN, over the
current best performing system. Furthermore, we discuss where we stand in a
relation to fully supervised methods in the past literature, and highlight with
examples qualitative differences that exist among simplified sentences
generated by supervision-free systems.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：CharacterBERT and Self-Teaching for Improving the Robustness of Dense  Retrievers on Queries with Typos</b></summary>
  <p><b>编号</b>：[377]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00716</p>
  <p><b>作者</b>：Shengyao Zhuang,  Guido Zuccon</p>
  <p><b>备注</b>：9 pages full paper, accepted at SIGIR2022</p>
  <p><b>关键词</b>：new passage retrieval dataset consisting, efficient yet effective training method, st achieves significantly higher effectiveness, small character level perturbation, input tokenization strategy employed</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Previous work has shown that dense retrievers are not robust to out-of-domain
and outlier queries, i.e. their effectiveness on these queries is much poorer
than what expected. In this paper, we consider a specific instance of such
queries: queries that contain typos. We show that a small character level
perturbation in queries (as caused by typos) highly impacts the effectiveness
of dense retrievers. We then demonstrate that the root cause of this resides in
the input tokenization strategy employed by BERT. In BERT, tokenization is
performed using the BERT's WordPiece tokenizer and we show that a token with a
typo will significantly change the token distributions obtained after
tokenization. This distribution change translates to changes in the input
embeddings passed to the BERT-based query encoder of dense retrievers. We then
turn our attention to devising dense retriever methods that are robust to such
typo queries, while still being as performant as previous methods on queries
without typos. For this, we use CharacterBERT as the backbone encoder and an
efficient yet effective training method, called Self-Teaching (ST), that
distills knowledge from queries without typos into the queries with typos.
Experimental results show that CharacterBERT in combination with ST achieves
significantly higher effectiveness on queries with typos compared to previous
methods. Along with these results and the open-sourced implementation of the
methods, we also provide a new passage retrieval dataset consisting of
real-world queries with typos and associated relevance assessments on the MS
MARCO corpus, thus supporting the research community in the investigation of
effective and robust dense retrievers.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Efficient Argument Structure Extraction with Transfer Learning and  Active Learning</b></summary>
  <p><b>编号</b>：[380]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00707</p>
  <p><b>作者</b>：Xinyu Hua,  Lu Wang</p>
  <p><b>备注</b>：Findings of ACL 2022, long paper</p>
  <p><b>关键词</b>：improving data efficiency since constructing high, effective acquisition strategies yield competitive results, based argument structure prediction model, independent sample acquisition strategies, leverage existing annotated data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The automation of extracting argument structures faces a pair of challenges
on (1) encoding long-term contexts to facilitate comprehensive understanding,
and (2) improving data efficiency since constructing high-quality argument
structures is time-consuming. In this work, we propose a novel context-aware
Transformer-based argument structure prediction model which, on five different
domains, significantly outperforms models that rely on features or only encode
limited contexts. To tackle the difficulty of data annotation, we examine two
complementary methods: (i) transfer learning to leverage existing annotated
data to boost model performance in a new target domain, and (ii) active
learning to strategically identify a small amount of samples for annotation. We
further propose model-independent sample acquisition strategies, which can be
generalized to diverse domains. With extensive experiments, we show that our
simple-yet-effective acquisition strategies yield competitive results against
three strong comparisons. Combined with transfer learning, substantial F1 score
boost (5-25) can be further achieved during the early iterations of active
learning across domains.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：CipherDAug: Ciphertext based Data Augmentation for Neural Machine  Translation</b></summary>
  <p><b>编号</b>：[394]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00665</p>
  <p><b>作者</b>：Nishant Kambhatla,  Logan Born,  Anoop Sarkar</p>
  <p><b>备注</b>：ACL 2022 Main Conf. camera ready version</p>
  <p><b>关键词</b>：first generate multiple rot -$ k, outperform strong data augmentation techniques, original parallel data via multi, yields particularly strong results, neural machine translation based</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a novel data-augmentation technique for neural machine translation
based on ROT-$k$ ciphertexts. ROT-$k$ is a simple letter substitution cipher
that replaces a letter in the plaintext with the $k$th letter after it in the
alphabet. We first generate multiple ROT-$k$ ciphertexts using different values
of $k$ for the plaintext which is the source side of the parallel data. We then
leverage this enciphered training data along with the original parallel data
via multi-source training to improve neural machine translation. Our method,
CipherDAug, uses a co-regularization-inspired training procedure, requires no
external data sources other than the original training data, and uses a
standard Transformer to outperform strong data augmentation techniques on
several datasets by a significant margin. This technique combines easily with
existing approaches to data augmentation, and yields particularly strong
results in low-resource settings.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：End-to-end multi-talker audio-visual ASR using an active speaker  attention module</b></summary>
  <p><b>编号</b>：[400]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00652</p>
  <p><b>作者</b>：Richard Rose,  Olivier Siohan</p>
  <p><b>备注</b>：5 pages, 3 figures, 3 tables, 28 citations</p>
  <p><b>关键词</b>：visual overlapping speech dataset created, label ambiguity issue associated, visual context attention model, vcam model improves performance, decode multiple label strings</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a new approach for end-to-end audio-visual multi-talker
speech recognition. The approach, referred to here as the visual context
attention model (VCAM), is important because it uses the available video
information to assign decoded text to one of multiple visible faces. This
essentially resolves the label ambiguity issue associated with most
multi-talker modeling approaches which can decode multiple label strings but
cannot assign the label strings to the correct speakers. This is implemented as
a transformer-transducer based end-to-end model and evaluated using a two
speaker audio-visual overlapping speech dataset created from YouTube videos. It
is shown in the paper that the VCAM model improves performance with respect to
previously reported audio-only and audio-visual multi-talker ASR systems.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Automatic Dialect Density Estimation for African American English</b></summary>
  <p><b>编号</b>：[452]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00967</p>
  <p><b>作者</b>：Alexander Johnson,  Kevin Everson,  Vijay Ravi,  Anissa Gladney,  Mari Ostendorf,  Abeer Alwan</p>
  <p><b>备注</b>：5 pages, 2 figures</p>
  <p><b>关键词</b>：ground truth dialect density measures, investigate several acoustic, explore automatic prediction, compare feature set, african american english</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we explore automatic prediction of dialect density of the
African American English (AAE) dialect, where dialect density is defined as the
percentage of words in an utterance that contain characteristics of the
non-standard dialect. We investigate several acoustic and language modeling
features, including the commonly used X-vector representation and ComParE
feature set, in addition to information extracted from ASR transcripts of the
audio files and prosodic information. To address issues of limited labeled
data, we use a weakly supervised model to project prosodic and X-vector
features into low-dimensional task-relevant representations. An XGBoost model
is then used to predict the speaker's dialect density from these features and
show which are most significant during inference. We evaluate the utility of
these features both alone and in combination for the given task. This work,
which does not rely on hand-labeled transcripts, is performed on audio segments
from the CORAAL database. We show a significant correlation between our
predicted and ground truth dialect density measures for AAE speech in this
database and propose this work as a tool for explaining and mitigating bias in
speech technology.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：A single speaker is almost all you need for automatic speech recognition</b></summary>
  <p><b>编号</b>：[481]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00618</p>
  <p><b>作者</b>：Edresson Casanova,  Christopher Shulby,  Alexander Korolev,  Arnaldo Candido Junior,  Anderson da Silva Soares,  Sandra Aluísio,  Moacir Antonelli Ponti</p>
  <p><b>备注</b>：Submitted to INTERSPEECH 2022</p>
  <p><b>关键词</b>：approach achieves results compared, voice conversion model training, voice conversion applied, obtain promising results, data augmentation method</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We explore the use of speech synthesis and voice conversion applied to
augment datasets for automatic speech recognition (ASR) systems, in scenarios
with only one speaker available for the target language. Through extensive
experiments, we show that our approach achieves results compared to the
state-of-the-art (SOTA) and requires only one speaker in the target language
during speech synthesis/voice conversion model training. Finally, we show that
it is possible to obtain promising results in the training of an ASR model with
our data augmentation method and only a single real speaker in different target
languages.</p>
  </details>
</details>
<h1>机器学习</h1>
<details>
  <summary>1. <b>标题：MaxViT: Multi-Axis Vision Transformer</b></summary>
  <p><b>编号</b>：[1]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01697</p>
  <p><b>作者</b>：Zhengzhong Tu,  Hossein Talebi,  Han Zhang,  Feng Yang,  Peyman Milanfar,  Alan Bovik,  Yinxiao Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：proposed model expresses strong generative modeling capability, design choices allow global, simple hierarchical vision backbone, backbone delivers favorable performance, recently gained significant attention</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transformers have recently gained significant attention in the computer
vision community. However, the lack of scalability of self-attention mechanisms
with respect to image size has limited their wide adoption in state-of-the-art
vision backbones. In this paper we introduce an efficient and scalable
attention model we call multi-axis attention, which consists of two aspects:
blocked local and dilated global attention. These design choices allow
global-local spatial interactions on arbitrary input resolutions with only
linear complexity. We also present a new architectural element by effectively
blending our proposed attention model with convolutions, and accordingly
propose a simple hierarchical vision backbone, dubbed MaxViT, by simply
repeating the basic building block over multiple stages. Notably, MaxViT is
able to "see" globally throughout the entire network, even in earlier,
high-resolution stages. We demonstrate the effectiveness of our model on a
broad spectrum of vision tasks. On image classification, MaxViT achieves
state-of-the-art performance under various settings: without extra data, MaxViT
attains 86.5\% ImageNet-1K top-1 accuracy; with ImageNet-21K pre-training, our
model achieves 88.7\% top-1 accuracy. For downstream tasks, MaxViT as a
backbone delivers favorable performance on object detection as well as visual
aesthetic assessment. We also show that our proposed model expresses strong
generative modeling capability on ImageNet, demonstrating the superior
potential of MaxViT blocks as a universal vision module. We will make the code
and models publicly available.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Joint Hand Motion and Interaction Hotspots Prediction from Egocentric  Videos</b></summary>
  <p><b>编号</b>：[2]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01696</p>
  <p><b>作者</b>：Shaowei Liu,  Subarna Tripathi,  Somdeb Majumdar,  Xiaolong Wang</p>
  <p><b>备注</b>：CVPR 2022, Project page: this https URL</p>
  <p><b>关键词</b>：e ., interaction hotspots )., object interaction reasoning via, oct significantly outperforms state, dimensional representation provides, next active object</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose to forecast future hand-object interactions given an egocentric
video. Instead of predicting action labels or pixels, we directly predict the
hand motion trajectory and the future contact points on the next active object
(i.e., interaction hotspots). This relatively low-dimensional representation
provides a concrete description of future interactions. To tackle this task, we
first provide an automatic way to collect trajectory and hotspots labels on
large-scale data. We then use this data to train an Object-Centric Transformer
(OCT) model for prediction. Our model performs hand and object interaction
reasoning via the self-attention mechanism in Transformers. OCT also provides a
probabilistic framework to sample the future trajectory and hotspots to handle
uncertainty in prediction. We perform experiments on the Epic-Kitchens-55,
Epic-Kitchens-100, and EGTEA Gaze+ datasets, and show that OCT significantly
outperforms state-of-the-art approaches by a large margin. Project page is
available at this https URL .</p>
  </details>
</details>
<details>
  <summary>3. <b>标题："This is my unicorn, Fluffy": Personalizing frozen vision-language  representations</b></summary>
  <p><b>编号</b>：[4]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01694</p>
  <p><b>作者</b>：Niv Cohen,  Rinon Gal,  Eli A. Meirom,  Gal Chechik,  Yuval Atzmon</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：semantic segmentation using rich textual queries, new learning setup called personalized vision, approach learns personalized visual concepts, two new benchmark datasets, scale data provide representations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Vision & Language models pretrained on web-scale data provide
representations that are invaluable for numerous V&L problems. However, it is
unclear how they can be used for reasoning about user-specific visual concepts
in unstructured language. This problem arises in multiple domains, from
personalized image retrieval to personalized interaction with smart devices. We
introduce a new learning setup called Personalized Vision & Language (PerVL)
with two new benchmark datasets for retrieving and segmenting user-specific
"personalized" concepts "in the wild". In PerVL, one should learn personalized
concepts (1) independently of the downstream task (2) allowing a pretrained
model to reason about them with free language, and (3) does not require
personalized negative examples. We propose an architecture for solving PerVL
that operates by extending the input vocabulary of a pretrained model with new
word embeddings for the new personalized concepts. The model can then reason
about them by simply using them in a sentence. We demonstrate that our approach
learns personalized visual concepts from a few examples and can effectively
apply them in image retrieval and semantic segmentation using rich textual
queries.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Do As I Can, Not As I Say: Grounding Language in Robotic Affordances</b></summary>
  <p><b>编号</b>：[7]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01691</p>
  <p><b>作者</b>：Michael Ahn,  Anthony Brohan,  Noah Brown,  Yevgen Chebotar,  Omar Cortes,  Byron David,  Chelsea Finn,  Keerthana Gopalakrishnan,  Karol Hausman,  Alex Herzog,  Daniel Ho,  Jasmine Hsu,  Julian Ibarz,  Brian Ichter,  Alex Irpan,  Eric Jang,  Rosario Jauregui Ruano,  Kyle Jeffrey,  Sally Jesmonth,  Nikhil J Joshi,  Ryan Julian,  Dmitry Kalashnikov,  Yuheng Kuang,  Kuang-Huei Lee,  Sergey Levine,  Yao Lu,  Linda Luu,  Carolina Parada,  Peter Pastor,  Jornell Quiambao,  Kanishka Rao,  Jarek Rettinghouse,  Diego Reyes,  Pierre Sermanet,  Nicolas Sievers,  Clayton Tan,  Alexander Toshev,  Vincent Vanhoucke,  Fei Xia,  Ted Xiao,  Peng Xu,  Sichun Xu,  Mengyuan Yan</p>
  <p><b>备注</b>：See website at this https URL</p>
  <p><b>关键词</b>：language model supplies high, language model provides high, temporally extended instructions expressed, propose natural language actions, value functions associated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models can encode a wealth of semantic knowledge about the
world. Such knowledge could be extremely useful to robots aiming to act upon
high-level, temporally extended instructions expressed in natural language.
However, a significant weakness of language models is that they lack real-world
experience, which makes it difficult to leverage them for decision making
within a given embodiment. For example, asking a language model to describe how
to clean a spill might result in a reasonable narrative, but it may not be
applicable to a particular agent, such as a robot, that needs to perform this
task in a particular environment. We propose to provide real-world grounding by
means of pretrained skills, which are used to constrain the model to propose
natural language actions that are both feasible and contextually appropriate.
The robot can act as the language model's "hands and eyes," while the language
model supplies high-level semantic knowledge about the task. We show how
low-level skills can be combined with large language models so that the
language model provides high-level knowledge about the procedures for
performing complex and temporally-extended instructions, while value functions
associated with these skills provide the grounding necessary to connect this
knowledge to a particular physical environment. We evaluate our method on a
number of real-world robotic tasks, where we show the need for real-world
grounding and that this approach is capable of completing long-horizon,
abstract, natural language instructions on a mobile manipulator. The project's
website and the video can be found at this https URL</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Deep Image: A precious image based deep learning method for online  malware detection in IoT Environment</b></summary>
  <p><b>编号</b>：[8]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01690</p>
  <p><b>作者</b>：Meysam Ghahramani,  Rahim Taheri,  Mohammad Shojafar,  Reza Javidan,  Shaohua Wan</p>
  <p><b>备注</b>：10 pages, 17 figures, SUBMITTED TO IEEE INTERNET OF THINGS JOURNAL, MARCH 2022</p>
  <p><b>关键词</b>：usual machine learning criteria namely accuracy, security experts confront considerable challenges, deep learning approach performed better, three malware detection methods based, developing efficient malware detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The volume of malware and the number of attacks in IoT devices are rising
everyday, which encourages security professionals to continually enhance their
malware analysis tools. Researchers in the field of cyber security have
extensively explored the usage of sophisticated analytics and the efficiency of
malware detection. With the introduction of new malware kinds and attack
routes, security experts confront considerable challenges in developing
efficient malware detection and analysis solutions. In this paper, a different
view of malware analysis is considered and the risk level of each sample
feature is computed, and based on that the risk level of that sample is
calculated. In this way, a criterion is introduced that is used together with
accuracy and FPR criteria for malware analysis in IoT environment. In this
paper, three malware detection methods based on visualization techniques called
the clustering approach, the probabilistic approach, and the deep learning
approach are proposed. Then, in addition to the usual machine learning criteria
namely accuracy and FPR, a proposed criterion based on the risk of samples has
also been used for comparison, with the results showing that the deep learning
approach performed better in detecting malware</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Using Explainable Boosting Machine to Compare Idiographic and Nomothetic  Approaches for Ecological Momentary Assessment Data</b></summary>
  <p><b>编号</b>：[9]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01689</p>
  <p><b>作者</b>：Mandani Ntekouli,  Gerasimos Spanakis,  Lourens Waldorp,  Anne Roefs</p>
  <p><b>备注</b>：13 pages, 2 figures, accepted on the symposium 'Intelligent Data Analysis' (2022)</p>
  <p><b>关键词</b>：knowledge distillation method achieves improved auc scores, linear models using imbalanced synthetic, linear interpretable machine learning, two different nomothetic approaches, benefit ema data classification</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Previous research on EMA data of mental disorders was mainly focused on
multivariate regression-based approaches modeling each individual separately.
This paper goes a step further towards exploring the use of non-linear
interpretable machine learning (ML) models in classification problems. ML
models can enhance the ability to accurately predict the occurrence of
different behaviors by recognizing complicated patterns between variables in
data. To evaluate this, the performance of various ensembles of trees are
compared to linear models using imbalanced synthetic and real-world datasets.
After examining the distributions of AUC scores in all cases, non-linear models
appear to be superior to baseline linear models. Moreover, apart from
personalized approaches, group-level prediction models are also likely to offer
an enhanced performance. According to this, two different nomothetic approaches
to integrate data of more than one individuals are examined, one using directly
all data during training and one based on knowledge distillation.
Interestingly, it is observed that in one of the two real-world datasets,
knowledge distillation method achieves improved AUC scores (mean relative
change of +17\% compared to personalized) showing how it can benefit EMA data
classification and performance.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：MultiMAE: Multi-modal Multi-task Masked Autoencoders</b></summary>
  <p><b>编号</b>：[12]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01678</p>
  <p><b>作者</b>：Roman Bachmann,  David Mizrahi,  Andrei Atanov,  Amir Zamir</p>
  <p><b>备注</b>：Project page at this https URL</p>
  <p><b>关键词</b>：training objective accordingly includes predicting multiple outputs besides, train multimae entirely using pseudo labeling, additional information besides rgb images, make training multimae tractable, optionally accept additional modalities</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a pre-training strategy called Multi-modal Multi-task Masked
Autoencoders (MultiMAE). It differs from standard Masked Autoencoding in two
key aspects: I) it can optionally accept additional modalities of information
in the input besides the RGB image (hence "multi-modal"), and II) its training
objective accordingly includes predicting multiple outputs besides the RGB
image (hence "multi-task").
We make use of masking (across image patches and input modalities) to make
training MultiMAE tractable as well as to ensure cross-modality predictive
coding is indeed learned by the network. We show this pre-training strategy
leads to a flexible, simple, and efficient framework with improved transfer
results to downstream tasks. In particular, the same exact pre-trained network
can be flexibly used when additional information besides RGB images is
available or when no information other than RGB is available - in all
configurations yielding competitive to or significantly better results than the
baselines. To avoid needing training datasets with multiple modalities and
tasks, we train MultiMAE entirely using pseudo labeling, which makes the
framework widely applicable to any RGB dataset.
The experiments are performed on multiple transfer tasks (image
classification, semantic segmentation, depth estimation) and datasets
(ImageNet, ADE20K, Taskonomy, Hypersim, NYUv2). The results show an
intriguingly impressive capability by the model in cross-modal/task predictive
coding and transfer.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Characterizing Parametric and Convergence Stability in Nonconvex and  Nonsmooth Optimizations: A Geometric Approach</b></summary>
  <p><b>编号</b>：[22]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01643</p>
  <p><b>作者</b>：Xiaotie Deng,  Hanyu Li,  Ningyuan Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：parametric stability asks whether minor perturbations, slightly weaker function requirement goes, small enough step sizes, optimization algorithm cannot escape, prove quite tight conditions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider stability issues in minimizing a continuous (probably
parameterized, nonconvex and nonsmooth) real-valued function $f$. We call a
point stationary if all its possible directional derivatives are nonnegative.
In this work, we focus on two notions of stability on stationary points of $f$:
parametric stability and convergence stability. Parametric considerations are
widely studied in various fields, including smoothed analysis, numerical
stability, condition numbers and sensitivity analysis for linear programming.
Parametric stability asks whether minor perturbations on parameters lead to
dramatic changes in the position and $f$ value of a stationary point.
Meanwhile, convergence stability indicates a non-escapable solution: Any point
sequence iteratively produced by an optimization algorithm cannot escape from a
neighborhood of a stationary point but gets close to it in the sense that such
stationary points are stable to the precision parameter and algorithmic
numerical errors. It turns out that these notions have deep connections to
geometry theory. We show that parametric stability is linked to deformations of
graphs of functions. On the other hand, convergence stability is concerned with
area partitioning of the function domain. Utilizing these connections, we prove
quite tight conditions of these two stability notions for a wide range of
functions and optimization algorithms with small enough step sizes and
precision parameters. These conditions are subtle in the sense that a slightly
weaker function requirement goes to the opposite of primitive intuitions and
leads to wrong conclusions. We present three applications of this theory. These
applications reveal some understanding on Nash equilibrium computation,
nonconvex and nonsmooth optimization, as well as the new optimization
methodology of deep neural networks.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：APP: Anytime Progressive Pruning</b></summary>
  <p><b>编号</b>：[23]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01640</p>
  <p><b>作者</b>：Diganta Misra,  Bharat Runwal,  Tianlong Chen,  Zhangyang Wang,  Irina Rish</p>
  <p><b>备注</b>：21 pages including 4 pages of references. Preprint version</p>
  <p><b>关键词</b>：anytime osp models across multiple architectures, proposed approach significantly outperforms, observe interesting nonmonotonic transitions, $\ approx 7 \%$, $\ approx 22 \%$,</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the latest advances in deep learning, there has been a lot of focus on
the online learning paradigm due to its relevance in practical settings.
Although many methods have been investigated for optimal learning settings in
scenarios where the data stream is continuous over time, sparse networks
training in such settings have often been overlooked. In this paper, we explore
the problem of training a neural network with a target sparsity in a particular
case of online learning: the anytime learning at macroscale paradigm (ALMA). We
propose a novel way of progressive pruning, referred to as \textit{Anytime
Progressive Pruning} (APP); the proposed approach significantly outperforms the
baseline dense and Anytime OSP models across multiple architectures and
datasets under short, moderate, and long-sequence training. Our method, for
example, shows an improvement in accuracy of $\approx 7\%$ and a reduction in
the generalization gap by $\approx 22\%$, while being $\approx 1/3$ rd the size
of the dense baseline model in few-shot restricted imagenet training. We
further observe interesting nonmonotonic transitions in the generalization gap
in the high number of megabatches-based ALMA. The code and experiment
dashboards can be accessed at
\url{this https URL} and
\url{this https URL}, respectively.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Do Deep Neural Networks Contribute to Multivariate Time Series Anomaly  Detection?</b></summary>
  <p><b>编号</b>：[25]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01637</p>
  <p><b>作者</b>：Julien Audibert,  Pietro Michiardi,  Frédéric Guyard,  Sébastien Marti,  Maria A. Zuluaga</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：deep neural network approaches, deep neural network methods, multivariate time series benchmarks, propose increasingly complex learning, unsupervised anomaly detection algorithms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Anomaly detection in time series is a complex task that has been widely
studied. In recent years, the ability of unsupervised anomaly detection
algorithms has received much attention. This trend has led researchers to
compare only learning-based methods in their articles, abandoning some more
conventional approaches. As a result, the community in this field has been
encouraged to propose increasingly complex learning-based models mainly based
on deep neural networks. To our knowledge, there are no comparative studies
between conventional, machine learning-based and, deep neural network methods
for the detection of anomalies in multivariate time series. In this work, we
study the anomaly detection performance of sixteen conventional, machine
learning-based and, deep neural network approaches on five real-world open
datasets. By analyzing and comparing the performance of each of the sixteen
methods, we show that no family of methods outperforms the others. Therefore,
we encourage the community to reincorporate the three categories of methods in
the anomaly detection in multivariate time series benchmarks.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Estimating Social Influence from Observational Data</b></summary>
  <p><b>编号</b>：[26]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01633</p>
  <p><b>作者</b>：Dhanya Sridhar,  Caterina De Bacco,  David Blei</p>
  <p><b>备注</b>：To appear at CLeaR 2022 (1st Conference on Causal Learning and Reasoning)</p>
  <p><b>关键词</b>：pif fits probabilistic factor models, develop poisson influence factorization, pif estimates social influence, pif recovers estimates, empirically study pif</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider the problem of estimating social influence, the effect that a
person's behavior has on the future behavior of their peers. The key challenge
is that shared behavior between friends could be equally explained by influence
or by two other confounding factors: 1) latent traits that caused people to
both become friends and engage in the behavior, and 2) latent preferences for
the behavior. This paper addresses the challenges of estimating social
influence with three contributions. First, we formalize social influence as a
causal effect, one which requires inferences about hypothetical interventions.
Second, we develop Poisson Influence Factorization (PIF), a method for
estimating social influence from observational data. PIF fits probabilistic
factor models to networks and behavior data to infer variables that serve as
substitutes for the confounding latent traits. Third, we develop assumptions
under which PIF recovers estimates of social influence. We empirically study
PIF with semi-synthetic and real data from this http URL, and conduct a sensitivity
analysis. We find that PIF estimates social influence most accurately compared
to related methods and remains robust under some violations of its assumptions.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Stuttgart Open Relay Degradation Dataset (SOReDD)</b></summary>
  <p><b>编号</b>：[29]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01626</p>
  <p><b>作者</b>：Benjamin Maschler,  Angel Iliev,  Thi Thu Huong Pham,  Michael Weyrich</p>
  <p><b>备注</b>：Dataset description (8 pages, 4 figures, 8 tables)</p>
  <p><b>关键词</b>：industrial transfer learning algorithms naturally requires appropriate datasets, machine learning oftentimes involve heterogeneous, industrial transfer learning offers, stuttgart open relay degradation dataset, life industrial use cases</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Real-life industrial use cases for machine learning oftentimes involve
heterogeneous and dynamic assets, processes and data, resulting in a need to
continuously adapt the learning algorithm accordingly. Industrial transfer
learning offers to lower the effort of such adaptation by allowing the
utilization of previously acquired knowledge in solving new (variants of)
tasks. Being data-driven methods, the development of industrial transfer
learning algorithms naturally requires appropriate datasets for training.
However, open-source datasets suitable for transfer learning training, i.e.
spanning different assets, processes and data (variants), are rare. With the
Stuttgart Open Relay Degradation Dataset (SOReDD) we want to offer such a
dataset. It provides data on the degradation of different electromechanical
relays under different operating conditions, allowing for a large number of
different transfer scenarios. Although such relays themselves are usually
inexpensive standard components, their failure often leads to the failure of a
machine as a whole due to their role as the central power switching element of
a machine. The main cost factor in the event of a relay defect is therefore not
the relay itself, but the reduced machine availability. It is therefore
desirable to predict relay degradation as accurately as possible for specific
applications in order to be able to replace relays in good time and avoid
unplanned machine downtimes. Nevertheless, data-driven failure prediction for
electromechanical relays faces the challenge that relay degradation behavior is
highly dependent on the operating conditions, high-resolution measurement data
on relay degradation behavior is only collected in rare cases, and such data
can then only cover a fraction of the possible operating environments. Relays
are thus representative of many other central standard components in automation
technology.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：More Efficient Identifiability Verification in ODE Models by Reducing  Non-Identifiability</b></summary>
  <p><b>编号</b>：[30]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01623</p>
  <p><b>作者</b>：Ilia Ilmer,  Alexey Ovchinnikov,  Gleb Pogudin,  Pedro Soto</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：proposed approach significantly improves performance across different computer algebra frameworks, structural global parameter identifiability indicates whether one, eliminating algebraically independent non, global identifiability query, infinitely many values</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Structural global parameter identifiability indicates whether one can
determine a parameter's value from given inputs and outputs in the absence of
noise. If a given model has parameters for which there may be infinitely many
values, such parameters are called non-identifiable. We present a procedure for
accelerating a global identifiability query by eliminating algebraically
independent non-identifiable parameters. Our proposed approach significantly
improves performance across different computer algebra frameworks.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Towards Deep Industrial Transfer Learning: Clustering for Transfer Case  Selection</b></summary>
  <p><b>编号</b>：[31]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01620</p>
  <p><b>作者</b>：Benjamin Maschler,  Tim Knodel,  Michael Weyrich</p>
  <p><b>备注</b>：7 pages, 5 figurs, 2 tables. Submitted to IEEE ETFA 2022</p>
  <p><b>关键词</b>：dynamic industrial use cases without high manual efforts, transfer case selection based upon clustering, deep learning algorithms towards heterogenous, industrial time series dataset, industrial transfer learning increases</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Industrial transfer learning increases the adaptability of deep learning
algorithms towards heterogenous and dynamic industrial use cases without high
manual efforts. The appropriate selection of what to transfer can vastly
improve a transfer's results. In this paper, a transfer case selection based
upon clustering is presented. Founded on a survey of clustering algorithms, the
BIRCH algorithm is selected for this purpose. It is evaluated on an industrial
time series dataset from a discrete manufacturing scenario. Results underline
the approaches' applicability caused by its results' reproducibility and
practical indifference to sequence, size and dimensionality of (sub-)datasets
to be clustered sequentially.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Deep-Ensemble-Based Uncertainty Quantification in Spatiotemporal Graph  Neural Networks for Traffic Forecasting</b></summary>
  <p><b>编号</b>：[33]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01618</p>
  <p><b>作者</b>：Tanwi Mallick,  Prasanna Balaprakash,  Jane Macfarlane</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：diffusion convolutional recurrent neural network, provide forecasts without estimates, commonly used frequentist techniques, scalable bayesian optimization method, scalable deep ensemble approach</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep-learning-based data-driven forecasting methods have produced impressive
results for traffic forecasting. A major limitation of these methods, however,
is that they provide forecasts without estimates of uncertainty, which are
critical for real-time deployments. We focus on a diffusion convolutional
recurrent neural network (DCRNN), a state-of-the-art method for short-term
traffic forecasting. We develop a scalable deep ensemble approach to quantify
uncertainties for DCRNN. Our approach uses a scalable Bayesian optimization
method to perform hyperparameter optimization, selects a set of high-performing
configurations, fits a generative model to capture the joint distributions of
the hyperparameter configurations, and trains an ensemble of models by sampling
a new set of hyperparameter configurations from the generative model. We
demonstrate the efficacy of the proposed methods by comparing them with other
uncertainty estimation techniques. We show that our generic and scalable
approach outperforms the current state-of-the-art Bayesian and a number of
other commonly used frequentist techniques.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：SPECTRE : Spectral Conditioning Helps to Overcome the Expressivity  Limits of One-shot Graph Generators</b></summary>
  <p><b>编号</b>：[35]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01613</p>
  <p><b>作者</b>：Karolis Martinkus,  Andreas Loukas,  Nathanaël Perraudin,  Roger Wattenhofer</p>
  <p><b>备注</b>：20 pages, 10 figures</p>
  <p><b>关键词</b>：also avoiding expensive sequential generation, world graphs spectre achieves, art deep autoregressive generators, much larger graphs, graph generation problem</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We approach the graph generation problem from a spectral perspective by first
generating the dominant parts of the graph Laplacian spectrum and then building
a graph matching these eigenvalues and eigenvectors. Spectral conditioning
allows for direct modeling of the global and local graph structure and helps to
overcome the expressivity and mode collapse issues of one-shot graph
generators. Our novel GAN, called SPECTRE, enables the one-shot generation of
much larger graphs than previously possible with one-shot models. SPECTRE
outperforms state-of-the-art deep autoregressive generators in terms of
modeling fidelity, while also avoiding expensive sequential generation and
dependence on node ordering. A case in point, in sizable synthetic and
real-world graphs SPECTRE achieves a 4-to-170 fold improvement over the best
competitor that does not overfit and is 23-to-30 times faster than
autoregressive generators.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Neural Estimation of the Rate-Distortion Function With Applications to  Operational Source Coding</b></summary>
  <p><b>编号</b>：[36]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01612</p>
  <p><b>作者</b>：Eric Lei,  Hamed Hassani,  Shirin Saeedi Bidokhti</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：resulting functional optimization problem using neural networks, experimental results demonstrate competitive performance, approach presents several computational challenges, designing lossy data compression schemes, shot lossy compression scheme</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A fundamental question in designing lossy data compression schemes is how
well one can do in comparison with the rate-distortion function, which
describes the known theoretical limits of lossy compression. Motivated by the
empirical success of deep neural network (DNN) compressors on large, real-world
data, we investigate methods to estimate the rate-distortion function on such
data, which would allow comparison of DNN compressors with optimality. While
one could use the empirical distribution of the data and apply the
Blahut-Arimoto algorithm, this approach presents several computational
challenges and inaccuracies when the datasets are large and high-dimensional,
such as the case of modern image datasets. Instead, we re-formulate the
rate-distortion objective, and solve the resulting functional optimization
problem using neural networks. We apply the resulting rate-distortion
estimator, called NERD, on popular image datasets, and provide evidence that
NERD can accurately estimate the rate-distortion function. Using our estimate,
we show that the rate-distortion achievable by DNN compressors are within
several bits of the rate-distortion function for real-world datasets.
Additionally, NERD provides access to the rate-distortion achieving channel, as
well as samples from its output marginal. Therefore, using recent results in
reverse channel coding, we describe how NERD can be used to construct an
operational one-shot lossy compression scheme with guarantees on the achievable
rate and distortion. Experimental results demonstrate competitive performance
with DNN compressors.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Modern Views of Machine Learning for Precision Psychiatry</b></summary>
  <p><b>编号</b>：[39]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01607</p>
  <p><b>作者</b>：Zhe Sage Chen,  Prathamesh (Param) Kulkarni,  Isaac R. Galatzer-Levy,  Benedetta Bigio,  Carla Nasca,  Yu Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：potentially provide explainable solutions, mobile technologies also call, methods provide new opportunities, species biomarker identification, multimodal data fusion</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In light of the NIMH's Research Domain Criteria (RDoC), the advent of
functional neuroimaging, novel technologies and methods provide new
opportunities to develop precise and personalized prognosis and diagnosis of
mental disorders. Machine learning (ML) and artificial intelligence (AI)
technologies are playing an increasingly critical role in the new era of
precision psychiatry. Combining ML/AI with neuromodulation technologies can
potentially provide explainable solutions in clinical practice and effective
therapeutic treatment. Advanced wearable and mobile technologies also call for
the new role of ML/AI for digital phenotyping in mobile mental health. In this
review, we provide a comprehensive review of the ML methodologies and
applications by combining neuroimaging, neuromodulation, and advanced mobile
technologies in psychiatry practice. Additionally, we review the role of ML in
molecular phenotyping and cross-species biomarker identification in precision
psychiatry. We further discuss explainable AI (XAI) and causality testing in a
closed-human-in-the-loop manner, and highlight the ML potential in multimedia
information extraction and multimodal data fusion. Finally, we discuss
conceptual and practical challenges in precision psychiatry and highlight ML
opportunities in future research.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：DODA: Data-oriented Sim-to-Real Domain Adaptation for 3D Indoor Semantic  Segmentation</b></summary>
  <p><b>编号</b>：[42]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01599</p>
  <p><b>作者</b>：Runyu Ding,  Jihan Yang,  Li Jiang,  Xiaojuan Qi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：deep learning approaches achieve prominent success, doda encompasses virtual scan simulation, doda surpasses existing uda approaches, 7 popular unsupervised domain adaptation, layout placements across domains</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning approaches achieve prominent success in 3D semantic
segmentation. However, collecting densely annotated real-world 3D datasets is
extremely time-consuming and expensive. Training models on synthetic data and
generalizing on real-world scenarios becomes an appealing alternative, but
unfortunately suffers from notorious domain shifts. In this work, we propose a
Data-Oriented Domain Adaptation (DODA) framework to mitigate pattern and
context gaps caused by different sensing mechanisms and layout placements
across domains. Our DODA encompasses virtual scan simulation to imitate
real-world point cloud patterns and tail-aware cuboid mixing to alleviate the
interior context gap with a cuboid-based intermediate domain. The first
unsupervised sim-to-real adaptation benchmark on 3D indoor semantic
segmentation is also built on 3D-FRONT, ScanNet and S3DIS along with 7 popular
Unsupervised Domain Adaptation (UDA) methods. Our DODA surpasses existing UDA
approaches by over 13% on both 3D-FRONT $\rightarrow$ ScanNet and 3D-FRONT
$\rightarrow$ S3DIS. Code will be available.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Optimising Energy Efficiency in UAV-Assisted Networks using Deep  Reinforcement Learning</b></summary>
  <p><b>编号</b>：[43]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01597</p>
  <p><b>作者</b>：Babatunji Omoniwa,  Boris Galkin,  Ivana Dusparic</p>
  <p><b>备注</b>：5 pages, Submitted to for publication in the IEEE Wireless Communication Letters</p>
  <p><b>关键词</b>：agent reinforcement learning approaches optimise, agent decentralised double deep q, 55 -- 80 %., approach outperforms existing baselines, unmanned aerial vehicles</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this letter, we study the energy efficiency (EE) optimisation of unmanned
aerial vehicles (UAVs) providing wireless coverage to static and mobile ground
users. Recent multi-agent reinforcement learning approaches optimise the
system's EE using a 2D trajectory design, neglecting interference from nearby
UAV cells. We aim to maximise the system's EE by jointly optimising each UAV's
3D trajectory, number of connected users, and the energy consumed, while
accounting for interference. Thus, we propose a cooperative Multi-Agent
Decentralised Double Deep Q-Network (MAD-DDQN) approach. Our approach
outperforms existing baselines in terms of EE by as much as 55 -- 80%.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Langevin Diffusion: An Almost Universal Algorithm for Private Euclidean  (Convex) Optimization</b></summary>
  <p><b>编号</b>：[49]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01585</p>
  <p><b>作者</b>：Arun Ganesh,  Abhradeep Thakurta,  Jalaj Upadhyay</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：$\ ell_2 $- lipschitz convex losses, $(\ epsilon ,\ delta )$- dp, $(\ epsilon ,\ delta )$- dp, ld ), simultaneously provides optimal privacy, discrete time dp optimization algorithms analogous</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper we revisit the problem of differentially private empirical risk
minimization (DP-ERM) and differentially private stochastic convex optimization
(DP-SCO). We show that a well-studied continuous time algorithm from
statistical physics, called Langevin diffusion (LD), simultaneously provides
optimal privacy/utility trade-offs for both DP-ERM and DP-SCO, under
$\epsilon$-DP, and $(\epsilon,\delta)$-DP. Using the uniform stability
properties of LD, we provide the optimal excess population risk guarantee for
$\ell_2$-Lipschitz convex losses under $\epsilon$-DP, which was an open problem
since [BST14].
Along the way, we provide various technical tools, which can be of
independent interest: i) A new Rényi divergence bound for LD, when run on
loss functions over two neighboring data sets, ii) Excess empirical risk bounds
for last-iterate LD, analogous to that of Shamir and Zhang for noisy stochastic
gradient descent (SGD), and iii) A two phase excess risk analysis of LD, where
the first phase is when the diffusion has not converged in any reasonable sense
to a stationary distribution, and in the second phase when the diffusion has
converged to a variant of Gibbs distribution. Our universality results
crucially rely on the dynamics of LD. When it has converged to a stationary
distribution, we obtain the optimal bounds under $\epsilon$-DP. When it is run
only for a very short time $\propto 1/p$, we obtain the optimal bounds under
$(\epsilon,\delta)$-DP. Here, $p$ is the dimensionality of the model space.
Our work initiates a systematic study of DP continuous time optimization. We
believe this may have ramifications in the design of discrete time DP
optimization algorithms analogous to that in the non-private setting, where
continuous time dynamical viewpoints have helped in designing new algorithms,
including the celebrated mirror-descent and Polyak's momentum method.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Coarse-to-Fine Q-attention with Learned Path Ranking</b></summary>
  <p><b>编号</b>：[50]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01571</p>
  <p><b>作者</b>：Stephen James,  Pieter Abbeel</p>
  <p><b>备注</b>：Project page and code: this https URL</p>
  <p><b>关键词</b>：approach across 16 rlbench tasks, propose learned path ranking, reaching paths generated, path generation modules, path generating methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose Learned Path Ranking (LPR), a method that accepts an end-effector
goal pose, and learns to rank a set of goal-reaching paths generated from an
array of path generating methods, including: path planning, Bezier curve
sampling, and a learned policy. The core idea being that each of the path
generation modules will be useful in different tasks, or at different stages in
a task. When LPR is added as an extension to C2F-ARM, our new system,
C2F-ARM+LPR, retains the sample efficiency of its predecessor, while also being
able to accomplish a larger set of tasks; in particular, tasks that require
very specific motions (e.g. opening toilet seat) that need to be inferred from
both demonstrations and exploration data. In addition to benchmarking our
approach across 16 RLBench tasks, we also learn real-world tasks, tabula rasa,
in 10-15 minutes, with only 3 demonstrations.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：DAD: Data-free Adversarial Defense at Test Time</b></summary>
  <p><b>编号</b>：[51]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01568</p>
  <p><b>作者</b>：Gaurav Kumar Nayak,  Ruchit Rawal,  Anirban Chakraborty</p>
  <p><b>备注</b>：WACV 2022. Project page: this https URL</p>
  <p><b>关键词</b>：proposed technique via extensive experiments, detection method correctly identifies 91, carefully crafted imperceptible noises, adversarial sample detection framework, model requires training data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep models are highly susceptible to adversarial attacks. Such attacks are
carefully crafted imperceptible noises that can fool the network and can cause
severe consequences when deployed. To encounter them, the model requires
training data for adversarial training or explicit regularization-based
techniques. However, privacy has become an important concern, restricting
access to only trained models but not the training data (e.g. biometric data).
Also, data curation is expensive and companies may have proprietary rights over
it. To handle such situations, we propose a completely novel problem of
'test-time adversarial defense in absence of training data and even their
statistics'. We solve it in two stages: a) detection and b) correction of
adversarial samples. Our adversarial sample detection framework is initially
trained on arbitrary data and is subsequently adapted to the unlabelled test
data through unsupervised domain adaptation. We further correct the predictions
on detected adversarial samples by transforming them in Fourier domain and
obtaining their low frequency component at our proposed suitable radius for
model prediction. We demonstrate the efficacy of our proposed technique via
extensive experiments against several adversarial attacks and for different
model architectures and datasets. For a non-robust Resnet-18 model pre-trained
on CIFAR-10, our detection method correctly identifies 91.42% adversaries.
Also, we significantly improve the adversarial accuracy from 0% to 37.37% with
a minimal drop of 0.02% in clean accuracy on state-of-the-art 'Auto Attack'
without having to retrain the model.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Introducing ECAPA-TDNN and Wav2Vec2.0 Embeddings to Stuttering Detection</b></summary>
  <p><b>编号</b>：[53]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01564</p>
  <p><b>作者</b>：Shakeel Ahmad Sheikh,  Md Sahidullah,  Fabrice Hirsch,  Slim Ouni</p>
  <p><b>备注</b>：Submitted to Interspeech 2022</p>
  <p><b>关键词</b>：explore audio representations obtained using emphasized channel attention, trained deep models trained, standard sd system trained, massive audio datasets, advanced deep learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The adoption of advanced deep learning (DL) architecture in stuttering
detection (SD) tasks is challenging due to the limited size of the available
datasets. To this end, this work introduces the application of speech
embeddings extracted with pre-trained deep models trained on massive audio
datasets for different tasks. In particular, we explore audio representations
obtained using emphasized channel attention, propagation, and
aggregation-time-delay neural network (ECAPA-TDNN) and Wav2Vec2.0 model trained
on VoxCeleb and LibriSpeech datasets respectively. After extracting the
embeddings, we benchmark with several traditional classifiers, such as a
k-nearest neighbor, Gaussian naive Bayes, and neural network, for the
stuttering detection tasks. In comparison to the standard SD system trained
only on the limited SEP-28k dataset, we obtain a relative improvement of 16.74%
in terms of overall accuracy over baseline. Finally, we have shown that
combining two embeddings and concatenating multiple layers of Wav2Vec2.0 can
further improve SD performance up to 1% and 2.64% respectively.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Con$^{2}$DA: Simplifying Semi-supervised Domain Adaptation by Learning  Consistent and Contrastive Feature Representations</b></summary>
  <p><b>编号</b>：[55]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01558</p>
  <p><b>作者</b>：Manuel Pérez-Carrasco,  Pavlos Protopapas,  Guillermo Cabrera-Vives</p>
  <p><b>备注</b>：11 pages, 3 figures, 4 tables</p>
  <p><b>关键词</b>：extract good discriminative features across different domains, present con $^{ 2 }$ da, use different loss functions, performing stochastic data transformations, feature representation space using</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we present Con$^{2}$DA, a simple framework that extends recent
advances in semi-supervised learning to the semi-supervised domain adaptation
(SSDA) problem. Our framework generates pairs of associated samples by
performing stochastic data transformations to a given input. Associated data
pairs are mapped to a feature representation space using a feature extractor.
We use different loss functions to enforce consistency between the feature
representations of associated data pairs of samples. We show that these learned
representations are useful to deal with differences in data distributions in
the domain adaptation problem. We performed experiments to study the main
components of our model and we show that (i) learning of the consistent and
contrastive feature representations is crucial to extract good discriminative
features across different domains, and ii) our model benefits from the use of
strong augmentation policies. With these findings, our method achieves
state-of-the-art performances in three benchmark datasets for SSDA.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Causality, Causal Discovery, and Causal Inference in Structural  Engineering</b></summary>
  <p><b>编号</b>：[60]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01543</p>
  <p><b>作者</b>：M.Z. Naser</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：traditional machine learning approaches, structural engineering perspective, empirical mean )., data generating mechanism, commonly used algorithms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Much of our experiments are designed to uncover the cause(s) and effect(s)
behind a data generating mechanism (i.e., phenomenon) we happen to be
interested in. Uncovering such relationships allows us to identify the true
working of a phenomenon and, most importantly, articulate a model that may
enable us to further explore the phenomenon on hand and/or allow us to predict
it accurately. Fundamentally, such models are likely to be derived via a causal
approach (as opposed to an observational or empirical mean). In this approach,
causal discovery is required to create a causal model, which can then be
applied to infer the influence of interventions, and answer any hypothetical
questions (i.e., in the form of What ifs? Etc.) that we might have. This paper
builds a case for causal discovery and causal inference and contrasts that
against traditional machine learning approaches; all from a civil and
structural engineering perspective. More specifically, this paper outlines the
key principles of causality and the most commonly used algorithms and packages
for causal discovery and causal inference. Finally, this paper also presents a
series of examples and case studies of how causal concepts can be adopted for
our domain.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：CDKT-FL: Cross-Device Knowledge Transfer using Proxy Dataset in  Federated Learning</b></summary>
  <p><b>编号</b>：[61]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01542</p>
  <p><b>作者</b>：Minh N. H. Nguyen,  Huy Q. Le,  Shashi Raj Pandey,  Choong Seon Hong</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：practical setting towards better generalization abilities, device knowledge transfer following general formulations, proposed method achieves significant speedups, conventional fl methods need redesigning, realizing robust personalized federated learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In a practical setting towards better generalization abilities of client
models for realizing robust personalized Federated Learning (FL) systems,
efficient model aggregation methods have been considered as a critical research
objective. It is a challenging issue due to the consequences of non-i.i.d.
properties of client's data, often referred to as statistical heterogeneity and
small local data samples from the various data distributions. Therefore, to
develop robust generalized global and personalized models, conventional FL
methods need redesigning the knowledge aggregation from biased local models
while considering huge divergence of learning parameters due to skewed client
data. In this work, we demonstrate that the knowledge transfer mechanism is a
de facto technique to achieve these objectives and develop a novel knowledge
distillation-based approach to study the extent of knowledge transfer between
the global model and local models. Henceforth, our method considers the
suitability of transferring the outcome distribution and (or) the embedding
vector of representation from trained models during cross-device knowledge
transfer using a small proxy dataset in heterogeneous FL. In doing so, we
alternatively perform cross-device knowledge transfer following general
formulations as 1) global knowledge transfer and 2) on-device knowledge
transfer. Through simulations on four federated datasets, we show the proposed
method achieves significant speedups and high personalized performance of local
models. Furthermore, the proposed approach offers a more stable algorithm than
FedAvg during the training, with minimal communication data load when
exchanging the trained model's outcomes and representation.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Survey of Matrix Completion Algorithms</b></summary>
  <p><b>编号</b>：[64]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01532</p>
  <p><b>作者</b>：Jafar Jafarov</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：many different conditions since netflix announced, adaptive sensing algorithms many times performs, traditionally many machine learning problems, many real life dataset could, second active matrix completion techniques</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Matrix completion problem has been investigated under many different
conditions since Netflix announced the Netflix Prize problem. Many research
work has been done in the field once it has been discovered that many real life
dataset could be estimated with a low-rank matrix. Since then compressed
sensing, adaptive signal detection has gained the attention of many
researchers. In this survey paper we are going to visit some of the matrix
completion methods, mainly in the direction of passive and adaptive directions.
First, we discuss passive matrix completion methods with convex optimization,
and the second active matrix completion techniques with adaptive signal
detection methods. Traditionally many machine learning problems are solved in
passive environment. However, later it has been observed that adaptive sensing
algorithms many times performs more efficiently than former algorithms. Hence
algorithms in this setting has been extensively studied. Therefore, we are
going to present some of the latest adaptive matrix completion algorithms in
this paper meanwhile providing passive methods.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Matrix Completion with Sparse Noisy Rows</b></summary>
  <p><b>编号</b>：[66]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01530</p>
  <p><b>作者</b>：Jafar Jafarov</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：low rank matrix estimation problems, receive random noise instead, degenerate random noise model, study exact low, degenerate noise model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Exact matrix completion and low rank matrix estimation problems has been
studied in different underlying conditions. In this work we study exact
low-rank completion under non-degenerate noise model. Non-degenerate random
noise model has been previously studied by many researchers under given
condition that the noise is sparse and existing in some of the columns. In this
paper, we assume that each row can receive random noise instead of columns and
propose an interactive algorithm that is robust to this noise. We show that we
use a parametrization technique to give a condition when the underlying matrix
could be recoverable and suggest an algorithm which recovers the underlying
matrix.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：The Group Loss++: A deeper look into group loss for deep metric learning</b></summary>
  <p><b>编号</b>：[73]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01509</p>
  <p><b>作者</b>：Ismail Elezi,  Jenny Seidenschwarz,  Laurin Wagner,  Sebastiano Vascon,  Alessandro Torcinovich,  Marcello Pelillo,  Laura Leal-Taixe</p>
  <p><b>备注</b>：Accepted to IEEE Transactions on Pattern Analysis and Machine Intelligence (tPAMI), 2022. Includes supplementary material</p>
  <p><b>关键词</b>：density regions amongst data points belonging, obtain highly discriminative feature embeddings, consistent labelling amongst samples within, enforces embedding similarity across, inference strategies tailored towards</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep metric learning has yielded impressive results in tasks such as
clustering and image retrieval by leveraging neural networks to obtain highly
discriminative feature embeddings, which can be used to group samples into
different classes. Much research has been devoted to the design of smart loss
functions or data mining strategies for training such networks. Most methods
consider only pairs or triplets of samples within a mini-batch to compute the
loss function, which is commonly based on the distance between embeddings. We
propose Group Loss, a loss function based on a differentiable label-propagation
method that enforces embedding similarity across all samples of a group while
promoting, at the same time, low-density regions amongst data points belonging
to different groups. Guided by the smoothness assumption that "similar objects
should belong to the same group", the proposed loss trains the neural network
for a classification task, enforcing a consistent labelling amongst samples
within a class. We design a set of inference strategies tailored towards our
algorithm, named Group Loss++ that further improve the results of our model. We
show state-of-the-art results on clustering and image retrieval on four
retrieval datasets, and present competitive results on two person
re-identification datasets, providing a unified framework for retrieval and
re-identification.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Which Tricks are Important for Learning to Rank?</b></summary>
  <p><b>编号</b>：[77]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01500</p>
  <p><b>作者</b>：Ivan Lyzhin,  Aleksei Ustimenko,  Andrey Gulin,  Liudmila Prokhorenkova</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：optimizing specific ranking loss functions, smoothed ranking loss preferable, smooth surrogate ranking losses, boosted decision trees, based ranking algorithms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Nowadays, state-of-the-art learning-to-rank (LTR) methods are based on
gradient-boosted decision trees (GBDT). The most well-known algorithm is
LambdaMART that was proposed more than a decade ago. Recently, several other
GBDT-based ranking algorithms were proposed. In this paper, we conduct a
thorough analysis of these methods in a unified setup. In particular, we
address the following questions. Is direct optimization of a smoothed ranking
loss preferable over optimizing a convex surrogate? How to properly construct
and smooth surrogate ranking losses? To address these questions, we compare
LambdaMART with YetiRank and StochasticRank methods and their modifications. We
also improve the YetiRank approach to allow for optimizing specific ranking
loss functions. As a result, we gain insights into learning-to-rank approaches
and obtain a new state-of-the-art algorithm.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：FedRecAttack: Model Poisoning Attack to Federated Recommendation</b></summary>
  <p><b>编号</b>：[78]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01499</p>
  <p><b>作者</b>：Dazhong Rong,  Shuai Ye,  Ruoyan Zhao,  Hon Ning Yuen,  Jianhai Chen,  Qinming He</p>
  <p><b>备注</b>：This paper has been accepted by IEEE International Conference on Data Engineering 2022 (Second Research Round)</p>
  <p><b>关键词</b>：necessary security improvement could, federated learning lose validity, commonly considered fairly secured, fedrecattack remains highly effective, two completely different scenarios</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Federated Recommendation (FR) has received considerable popularity and
attention in the past few years. In FR, for each user, its feature vector and
interaction data are kept locally on its own client thus are private to others.
Without the access to above information, most existing poisoning attacks
against recommender systems or federated learning lose validity. Benifiting
from this characteristic, FR is commonly considered fairly secured. However, we
argue that there is still possible and necessary security improvement could be
made in FR. To prove our opinion, in this paper we present FedRecAttack, a
model poisoning attack to FR aiming to raise the exposure ratio of target
items. In most recommendation scenarios, apart from private user-item
interactions (e.g., clicks, watches and purchases), some interactions are
public (e.g., likes, follows and comments). Motivated by this point, in
FedRecAttack we make use of the public interactions to approximate users'
feature vectors, thereby attacker can generate poisoned gradients accordingly
and control malicious users to upload the poisoned gradients in a well-designed
way. To evaluate the effectiveness and side effects of FedRecAttack, we conduct
extensive experiments on three real-world datasets of different sizes from two
completely different scenarios. Experimental results demonstrate that our
proposed FedRecAttack achieves the state-of-the-art effectiveness while its
side effects are negligible. Moreover, even with small proportion (3%) of
malicious users and small proportion (1%) of public interactions, FedRecAttack
remains highly effective, which reveals that FR is more vulnerable to attack
than people commonly considered.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Satellite Monitoring of Terrestrial Plastic Waste</b></summary>
  <p><b>编号</b>：[83]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01485</p>
  <p><b>作者</b>：Caleb Kruse,  Edward Boyda,  Sully Chen,  Krishna Karra,  Tristan Bou-Nahra,  Dan Hammer,  Jennifer Mathis,  Taylor Maddalene,  Jenna Jambeck,  Fabien Laurier</p>
  <p><b>备注</b>：14 pages, 14 figures</p>
  <p><b>关键词</b>：southeast asia identifies 996 subsequently confirmed waste sites, system deployed across twelve countries, algorithmically monitor waste site footprints, numerous sites sit directly, detected 374 waste aggregations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Plastic waste is a significant environmental pollutant that is difficult to
monitor. We created a system of neural networks to analyze spectral, spatial,
and temporal components of Sentinel-2 satellite data to identify terrestrial
aggregations of waste. The system works at continental scale. We evaluated
performance in Indonesia and detected 374 waste aggregations, more than double
the number of sites found in public databases. The same system deployed across
twelve countries in Southeast Asia identifies 996 subsequently confirmed waste
sites. For each detected site, we algorithmically monitor waste site footprints
through time and cross-reference other datasets to generate physical and social
metadata. 19% of detected waste sites are located within 200 m of a waterway.
Numerous sites sit directly on riverbanks, with high risk of ocean leakage.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Assessing dengue fever risk in Costa Rica by using climate variables and  machine learning techniques</b></summary>
  <p><b>编号</b>：[84]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01483</p>
  <p><b>作者</b>：Luis A. Barboza,  Shu-Wei Chou,  Paola Vásquez,  Yury E. García,  Juan G. Calvo,  Hugo C. Hidalgo,  Fabio Sanchez</p>
  <p><b>备注</b>：13 pages, 4 figures</p>
  <p><b>关键词</b>：borne disease mostly endemic, affect millions every year, machine learning algorithms, geographic distribution makes, generalized additive model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Dengue fever is a vector-borne disease mostly endemic to tropical and
subtropical countries that affect millions every year and is considered a
significant burden for public health. Its geographic distribution makes it
highly sensitive to climate conditions. Here, we explore the effect of climate
variables using the Generalized Additive Model for location, scale, and shape
(GAMLSS) and Random Forest (RF) machine learning algorithms. Using the reported
number of dengue cases, we obtained reliable predictions. The uncertainty of
the predictions was also measured. These predictions will serve as input to
health officials to further improve and optimize the allocation of resources
prior to dengue outbreaks.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Feasibility of nowcasting SDG indicators: a comprehensive survey</b></summary>
  <p><b>编号</b>：[85]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01482</p>
  <p><b>作者</b>：Daniel Hopp,  Emily Fu,  Anu Peltola</p>
  <p><b>备注</b>：2 tables, 3 figures, 74 pages</p>
  <p><b>关键词</b>：accompanying sustainable development goals, comprehensive nowcasting feasibility survey, exist 231 sdg indicators, examining tier 1 indicators, paper provides resources</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The 2030 Agenda and accompanying Sustainable Development Goals (SDGs) are
vital in guiding national and global policy. However, many of the SDG
indicators used to measure progress toward those goals suffer from long
publication lags. Nowcasting has the potential to address this problem and
generate more timely estimates of those indicators. This paper provides
resources for achieving that potential by 1) carrying out a comprehensive
nowcasting feasibility survey of all SDG indicators to assess their potential
to be nowcast, and 2) performing a case study of indicator 9.4.1 to illustrate
and shed light on the process of performing a nowcasting exercise. There exist
231 SDG indicators, but due to only examining Tier 1 indicators and the fact
that many indicators have multiple sub-indicators, 362 indicators and
sub-indicators were eventually surveyed. Of those 362, 150 were found highly
likely to be suitable candidates for nowcasting, 87 were found to be likely,
and 125 were found to be unsuitable.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Event Log Sampling for Predictive Monitoring</b></summary>
  <p><b>编号</b>：[90]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01470</p>
  <p><b>作者</b>：Mohammadreza Fani Sani,  Mozhgan Vazifehdoostirani,  Gyunam Park,  Marco Pegoraro,  Sebastiaan J. van Zelst,  Wil M.P. van der Aalst</p>
  <p><b>备注</b>：7 pages, 1 figure, 4 tables, 34 references</p>
  <p><b>关键词</b>：allows sampling training process instances, complex machine learning models, next activity prediction methods, sampling method allows, running process instances</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Predictive process monitoring is a subfield of process mining that aims to
estimate case or event features for running process instances. Such predictions
are of significant interest to the process stakeholders. However,
state-of-the-art methods for predictive monitoring require the training of
complex machine learning models, which is often inefficient. This paper
proposes an instance selection procedure that allows sampling training process
instances for prediction models. We show that our sampling method allows for a
significant increase of training speed for next activity prediction methods
while maintaining reliable levels of prediction accuracy.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：On scientific understanding with artificial intelligence</b></summary>
  <p><b>编号</b>：[93]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01467</p>
  <p><b>作者</b>：Mario Krenn,  Robert Pollice,  Si Yue Guo,  Matteo Aldeghi,  Alba Cervera-Lierta,  Pascal Friederich,  Gabriel dos Passos Gomes,  Florian Häse,  Adrian Jinich,  AkshatKumar Nigam,  Zhenpeng Yao,  Alán Aspuru-Guzik</p>
  <p><b>备注</b>：13 pages, 3 figures, comments welcome!</p>
  <p><b>关键词</b>：artificial intelligence poses one ultimate question, ultimately bring us closer, focuses research towards androids, advanced artificial systems contribute, every particle physics experiment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Imagine an oracle that correctly predicts the outcome of every particle
physics experiment, the products of every chemical reaction, or the function of
every protein. Such an oracle would revolutionize science and technology as we
know them. However, as scientists, we would not be satisfied with the oracle
itself. We want more. We want to comprehend how the oracle conceived these
predictions. This feat, denoted as scientific understanding, has frequently
been recognized as the essential aim of science. Now, the ever-growing power of
computers and artificial intelligence poses one ultimate question: How can
advanced artificial systems contribute to scientific understanding or achieve
it autonomously?
We are convinced that this is not a mere technical question but lies at the
core of science. Therefore, here we set out to answer where we are and where we
can go from here. We first seek advice from the philosophy of science to
understand scientific understanding. Then we review the current state of the
art, both from literature and by collecting dozens of anecdotes from scientists
about how they acquired new conceptual understanding with the help of
computers. Those combined insights help us to define three dimensions of
android-assisted scientific understanding: The android as a I) computational
microscope, II) resource of inspiration and the ultimate, not yet existent III)
agent of understanding. For each dimension, we explain new avenues to push
beyond the status quo and unleash the full power of artificial intelligence's
contribution to the central aim of science. We hope our perspective inspires
and focuses research towards androids that get new scientific understanding and
ultimately bring us closer to true artificial scientists.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Value Gradient weighted Model-Based Reinforcement Learning</b></summary>
  <p><b>编号</b>：[94]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01464</p>
  <p><b>作者</b>：Claas Voelcker,  Victor Liao,  Animesh Garg,  Amir-massoud Farahmand</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：yet unavoidable modeling errors often lead performance deterioration, aware model learning would fix, naive intuition would suggest, commonly used maximum likelihood, gradient weighted model learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Model-based reinforcement learning (MBRL) is a sample efficient technique to
obtain control policies, yet unavoidable modeling errors often lead performance
deterioration. The model in MBRL is often solely fitted to reconstruct
dynamics, state observations in particular, while the impact of model error on
the policy is not captured by the training objective. This leads to a mismatch
between the intended goal of MBRL, enabling good policy and value learning, and
the target of the loss function employed in practice, future state prediction.
Naive intuition would suggest that value-aware model learning would fix this
problem and, indeed, several solutions to this objective mismatch problem have
been proposed based on theoretical analysis. However, they tend to be inferior
in practice to commonly used maximum likelihood (MLE) based approaches. In this
paper we propose the Value-gradient weighted Model Learning (VaGraM), a novel
method for value-aware model learning which improves the performance of MBRL in
challenging settings, such as small model capacity and the presence of
distracting state dimensions. We analyze both MLE and value-aware approaches
and demonstrate how they fail to account for exploration and the behavior of
function approximation when learning value-aware models and highlight the
additional goals that must be met to stabilize optimization in the deep
learning setting. We verify our analysis by showing that our loss function is
able to achieve high returns on the Mujoco benchmark suite while being more
robust than maximum likelihood based approaches.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：SHiFT: An Efficient, Flexible Search Engine for Transfer Learning</b></summary>
  <p><b>编号</b>：[97]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01457</p>
  <p><b>作者</b>：Cedric Renggli,  Xiaozhe Yao,  Luka Kolar,  Luka Rimanic,  Ana Klimovic,  Ce Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：carefully comparing various selection, repositories keep growing exponentially, support efficient incremental executions, custom query language shift, efficient model search engine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transfer learning can be seen as a data- and compute-efficient alternative to
training models from scratch. The emergence of rich model repositories, such as
TensorFlow Hub, enables practitioners and researchers to unleash the potential
of these models across a wide range of downstream tasks. As these repositories
keep growing exponentially, efficiently selecting a good model for the task at
hand becomes paramount. By carefully comparing various selection and search
strategies, we realize that no single method outperforms the others, and hybrid
or mixed strategies can be beneficial. Therefore, we propose SHiFT, the first
downstream task-aware, flexible, and efficient model search engine for transfer
learning. These properties are enabled by a custom query language SHiFT-QL
together with a cost-based decision maker, which we empirically validate.
Motivated by the iterative nature of machine learning development, we further
support efficient incremental executions of our queries, which requires a
careful implementation when jointly used with our optimizations.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Learning Commonsense-aware Moment-Text Alignment for Fast Video Temporal  Grounding</b></summary>
  <p><b>编号</b>：[99]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01450</p>
  <p><b>作者</b>：Ziyue Wu,  Junyu Gao,  Shucheng Huang,  Changsheng Xu</p>
  <p><b>备注</b>：Code is available at this https URL</p>
  <p><b>关键词</b>：existing approaches adopt elaborately designed cross, grounding temporal video segments described, fast video temporal grounding, fast video temporal grounding, two challenging benchmarks show</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Grounding temporal video segments described in natural language queries
effectively and efficiently is a crucial capability needed in
vision-and-language fields. In this paper, we deal with the fast video temporal
grounding (FVTG) task, aiming at localizing the target segment with high speed
and favorable accuracy. Most existing approaches adopt elaborately designed
cross-modal interaction modules to improve the grounding performance, which
suffer from the test-time bottleneck. Although several common space-based
methods enjoy the high-speed merit during inference, they can hardly capture
the comprehensive and explicit relations between visual and textual modalities.
In this paper, to tackle the dilemma of speed-accuracy tradeoff, we propose a
commonsense-aware cross-modal alignment (CCA) framework, which incorporates
commonsense-guided visual and text representations into a complementary common
space for fast video temporal grounding. Specifically, the commonsense concepts
are explored and exploited by extracting the structural semantic information
from a language corpus. Then, a commonsense-aware interaction module is
designed to obtain bridged visual and text features by utilizing the learned
commonsense concepts. Finally, to maintain the original semantic information of
textual queries, a cross-modal complementary common space is optimized to
obtain matching scores for performing FVTG. Extensive results on two
challenging benchmarks show that our CCA method performs favorably against
state-of-the-arts while running at high speed. Our code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：SAM-kNN Regressor for Online Learning in Water Distribution Networks</b></summary>
  <p><b>编号</b>：[107]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01436</p>
  <p><b>作者</b>：Jonathan Jakob,  André Artelt,  Martina Hasenjäger,  Barbara Hammer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：distribute water via widely branched networks, residual based anomaly detection system, water supply company continuously monitors, residual based anomaly detection systems, since real world networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Water distribution networks are a key component of modern infrastructure for
housing and industry. They transport and distribute water via widely branched
networks from sources to consumers. In order to guarantee a working network at
all times, the water supply company continuously monitors the network and takes
actions when necessary -- e.g. reacting to leakages, sensor faults and drops in
water quality. Since real world networks are too large and complex to be
monitored by a human, algorithmic monitoring systems have been developed. A
popular type of such systems are residual based anomaly detection systems that
can detect events such as leakages and sensor faults. For a continuous high
quality monitoring, it is necessary for these systems to adapt to changed
demands and presence of various anomalies.
In this work, we propose an adaption of the incremental SAM-kNN classifier
for regression to build a residual based anomaly detection system for water
distribution networks that is able to adapt to any kind of change.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Re-examining Distillation For Continual Object Detection</b></summary>
  <p><b>编号</b>：[115]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01407</p>
  <p><b>作者</b>：Eli Verwimp,  Kuo Yang,  Sarah Parisot,  Hong Lanqing,  Steven McDonagh,  Eduardo Pérez-Pellitero,  Matthias De Lange,  Tinne Tuytelaars</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：yet overly confident teacher predictions prevent student models, object detection models forget catastrophically, contemporary continual object detection work, detecting incorrect teacher predictions, training models continually</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Training models continually to detect and classify objects, from new classes
and new domains, remains an open problem. In this work, we conduct a thorough
analysis of why and how object detection models forget catastrophically. We
focus on distillation-based approaches in two-stage networks; the most-common
strategy employed in contemporary continual object detection work.Distillation
aims to transfer the knowledge of a model trained on previous tasks -- the
teacher -- to a new model -- the student -- while it learns the new task. We
show that this works well for the region proposal network, but that wrong, yet
overly confident teacher predictions prevent student models from effective
learning of the classification head. Our analysis provides a foundation that
allows us to propose improvements for existing techniques by detecting
incorrect teacher predictions, based on current ground-truth labels, and by
employing an adaptive Huber loss as opposed to the mean squared error for the
distillation loss in the classification heads. We evidence that our strategy
works not only in a class incremental setting, but also in domain incremental
settings, which constitute a realistic context, likely to be the setting of
representative real-world problems.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：Aligned Weight Regularizers for Pruning Pretrained Neural Networks</b></summary>
  <p><b>编号</b>：[122]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01385</p>
  <p><b>作者</b>：James O' Neill,  Sourav Dutta,  Haytham Assem</p>
  <p><b>备注</b>：Accepted to ACL Findings 2022</p>
  <p><b>关键词</b>：propose two weight regularizers, comparing standard supervised learning, shot setting using xlm, lingual language model compression, representational degradation depending</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While various avenues of research have been explored for iterative pruning,
little is known what effect pruning has on zero-shot test performance and its
potential implications on the choice of pruning criteria. This pruning setup is
particularly important for cross-lingual models that implicitly learn alignment
between language representations during pretraining, which if distorted via
pruning, not only leads to poorer performance on language data used for
retraining but also on zero-shot languages that are evaluated.
In this work, we show that there is a clear performance discrepancy in
magnitude-based pruning when comparing standard supervised learning to the
zero-shot setting. From this finding, we propose two weight regularizers that
aim to maximize the alignment between units of pruned and unpruned networks to
mitigate alignment distortion in pruned cross-lingual models and perform well
for both non zero-shot and zero-shot settings.
We provide experimental results on cross-lingual tasks for the zero-shot
setting using XLM-RoBERTa$_{\mathrm{Base}}$, where we also find that pruning
has varying degrees of representational degradation depending on the language
corresponding to the zero-shot test set. This is also the first study that
focuses on cross-lingual language model compression.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Taking ROCKET on an Efficiency Mission: Multivariate Time Series  Classification with LightWaveS</b></summary>
  <p><b>编号</b>：[124]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01379</p>
  <p><b>作者</b>：Leonardos Pantiskas,  Kees Verstoep,  Mark Hoogendoorn,  Henri Bal</p>
  <p><b>备注</b>：This work has been accepted as a short paper at DCOSS 2022</p>
  <p><b>关键词</b>：lightwaves also scales well across multiple compute nodes, utilizing wavelet scattering transformation, complex models towards practical, recent deep learning models, multivariate time series classification</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Nowadays, with the rising number of sensors in sectors such as healthcare and
industry, the problem of multivariate time series classification (MTSC) is
getting increasingly relevant and is a prime target for machine and deep
learning approaches. Their expanding adoption in real-world environments is
causing a shift in focus from the pursuit of ever higher prediction accuracy
with complex models towards practical, deployable solutions that balance
accuracy and parameters such as prediction speed. An MTSC model that has
attracted attention recently is ROCKET, based on random convolutional kernels,
both because of its very fast training process and its state-of-the-art
accuracy. However, the large number of features it utilizes may be detrimental
to inference time. Examining its theoretical background and limitations enables
us to address potential drawbacks and present LightWaveS: a framework for
accurate MTSC, which is fast both during training and inference. Specifically,
utilizing wavelet scattering transformation and distributed feature selection,
we manage to create a solution which employs just 2.5% of the ROCKET features,
while achieving accuracy comparable to recent deep learning models. LightWaveS
also scales well across multiple compute nodes and with the number of input
channels during training. In addition, it can significantly reduce the input
size and provide insight to an MTSC problem by keeping only the most useful
channels. We present three versions of our algorithm and their results on
distributed training time and scalability, accuracy and inference speedup. We
show that we achieve speedup ranging from 9x to 65x compared to ROCKET during
inference on an edge device, on datasets with comparable accuracy.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Synthetic Graph Generation to Benchmark Graph Learning</b></summary>
  <p><b>编号</b>：[125]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01376</p>
  <p><b>作者</b>：Anton Tsitsulin,  Benedek Rozemberczki,  John Palowitch,  Bryan Perozzi</p>
  <p><b>备注</b>：4 pages. Appeared at the GLB'21 workshop</p>
  <p><b>关键词</b>：shockingly small sample size (~ 10, supervised graph neural network models, many graph analysis tasks, featured synthetic graph generator, synthetic graph generations allows</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Graph learning algorithms have attained state-of-the-art performance on many
graph analysis tasks such as node classification, link prediction, and
clustering. It has, however, become hard to track the field's burgeoning
progress. One reason is due to the very small number of datasets used in
practice to benchmark the performance of graph learning algorithms. This
shockingly small sample size (~10) allows for only limited scientific insight
into the problem.
In this work, we aim to address this deficiency. We propose to generate
synthetic graphs, and study the behaviour of graph learning algorithms in a
controlled scenario. We develop a fully-featured synthetic graph generator that
allows deep inspection of different models. We argue that synthetic graph
generations allows for thorough investigation of algorithms and provides more
insights than overfitting on three citation datasets. In the case study, we
show how our framework provides insight into unsupervised and supervised graph
neural network models.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Training Fully Connected Neural Networks is $\exists\mathbb{R}$-Complete</b></summary>
  <p><b>编号</b>：[126]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01368</p>
  <p><b>作者</b>：Daniel Bertschinger,  Christoph Hertrich,  Paul Jungeblut,  Tillmann Miltzow,  Simon Weber</p>
  <p><b>备注</b>：38 pages, 18 figures</p>
  <p><b>关键词</b>：stoc 2018 ], geometric packing, layer fully connected neural network, focs 2020 ], covering polygons, continuous constraint satisfaction problems, exactly two output neurons</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider the algorithmic problem of finding the optimal weights and biases
for a two-layer fully connected neural network to fit a given set of data
points. This problem is known as empirical risk minimization in the machine
learning community. We show that the problem is $\exists\mathbb{R}$-complete.
This complexity class can be defined as the set of algorithmic problems that
are polynomial-time equivalent to finding real roots of a polynomial with
integer coefficients. Our results hold even if the following restrictions are
all added simultaneously.
$\bullet$ There are exactly two output neurons.
$\bullet$ There are exactly two input neurons.
$\bullet$ The data has only 13 different labels.
$\bullet$ The number of hidden neurons is a constant fraction of the number
of data points.
$\bullet$ The target training error is zero.
$\bullet$ The ReLU activation function is used.
This shows that even very simple networks are difficult to train. The result
offers an explanation (though far from a complete understanding) on why only
gradient descent is widely successful in training neural networks in practice.
We generalize a recent result by Abrahamsen, Kleist and Miltzow [NeurIPS 2021].
This result falls into a recent line of research that tries to unveil that a
series of central algorithmic problems from widely different areas of computer
science and mathematics are $\exists\mathbb{R}$-complete: This includes the art
gallery problem [JACM/STOC 2018], geometric packing [FOCS 2020], covering
polygons with convex polygons [FOCS 2021], and continuous constraint
satisfaction problems [FOCS 2021].</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Learning to solve Minimum Cost Multicuts efficiently using Edge-Weighted  Graph Convolutional Neural Networks</b></summary>
  <p><b>编号</b>：[127]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01366</p>
  <p><b>作者</b>：Steffen Jung,  Margret Keuper</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：therefore adapt various gnn architectures including graph convolutional networks, signed graph convolutional networks, graph convolutional neural networks, providing lower computation times, largely improved scalability compared</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The minimum cost multicut problem is the NP-hard/APX-hard combinatorial
optimization problem of partitioning a real-valued edge-weighted graph such as
to minimize the total cost of the partition. While graph convolutional neural
networks (GNN) have proven to be promising in the context of combinatorial
optimization, most of them are only tailored to or tested on positive-valued
edge weights, i.e. they do not comply to the nature of the multicut problem. We
therefore adapt various GNN architectures including Graph Convolutional
Networks, Signed Graph Convolutional Networks and Graph Isomorphic Networks to
facilitate the efficient encoding of real-valued edge costs. Moreover, we
employ a reformulation of the multicut ILP constraints to a polynomial program
as loss function that allows to learn feasible multicut solutions in a scalable
way. Thus, we provide the first approach towards end-to-end trainable
multicuts. Our findings support that GNN approaches can produce good solutions
in practice while providing lower computation times and largely improved
scalability compared to LP solvers and optimized heuristics, especially when
considering large instances.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Efficient, Uncertainty-based Moderation of Neural Networks Text  Classifiers</b></summary>
  <p><b>编号</b>：[138]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01334</p>
  <p><b>作者</b>：Jakob Smedegaard Andersen,  Walid Maalej</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ca .~ 90 %), modern neural networks classifiers, approx .~ 98, uses prediction uncertainties, probably incorrect classifications</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To maximize the accuracy and increase the overall acceptance of text
classifiers, we propose a framework for the efficient, in-operation moderation
of classifiers' output. Our framework focuses on use cases in which F1-scores
of modern Neural Networks classifiers (ca.~90%) are still inapplicable in
practice. We suggest a semi-automated approach that uses prediction
uncertainties to pass unconfident, probably incorrect classifications to human
moderators. To minimize the workload, we limit the human moderated data to the
point where the accuracy gains saturate and further human effort does not lead
to substantial improvements. A series of benchmarking experiments based on
three different datasets and three state-of-the-art classifiers show that our
framework can improve the classification F1-scores by 5.1 to 11.2% (up to
approx.~98 to 99%), while reducing the moderation load up to 73.3% compared to
a random moderation.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Algorithms for Bayesian network modeling and reliability inference of  complex multistate systems: Part II-Dependent systems</b></summary>
  <p><b>编号</b>：[141]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01327</p>
  <p><b>作者</b>：Xiaohu Zheng,  Wen Yao,  Xiaoqian Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：multistate joint probability inference algorithm, satellite attitude control system, complex multistate system feasible, dependent multistate inference algorithm, complex multistate dependent system</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In using the Bayesian network (BN) to construct the complex multistate
system's reliability model as described in Part I, the memory storage
requirements of the node probability table (NPT) will exceed the random access
memory (RAM) of the computer. However, the proposed inference algorithm of Part
I is not suitable for the dependent system. This Part II proposes a novel
method for BN reliability modeling and analysis to apply the compression idea
to the complex multistate dependent system. In this Part II, the dependent
nodes and their parent nodes are equivalent to a block, based on which the
multistate joint probability inference algorithm is proposed to calculate the
joint probability distribution of a block's all nodes. Then, based on the
proposed multistate compression algorithm of Part I, the dependent multistate
inference algorithm is proposed for the complex multistate dependent system.
The use and accuracy of the proposed algorithms are demonstrated in case 1.
Finally, the proposed algorithms are applied to the reliability modeling and
analysis of the satellite attitude control system. The results show that both
Part I and Part II's proposed algorithms make the reliability modeling and
analysis of the complex multistate system feasible.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：GraFN: Semi-Supervised Node Classification on Graph with Few Labels via  Non-Parametric Distribution Assignment</b></summary>
  <p><b>编号</b>：[149]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01303</p>
  <p><b>作者</b>：Junseok Lee,  Yunhak Oh,  Yeonjun In,  Namkyeong Lee,  Dongmin Hyun,  Chanyoung Park</p>
  <p><b>备注</b>：SIGIR 2022(Short Paper)</p>
  <p><b>关键词</b>：learning class discriminative node representations since, gnns encounter significant performance degradation, grafn randomly samples support nodes, two predicted class distributions, supervised learning paradigm aims</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>GNNs encounter significant performance degradation when the amount of
supervision signals, i.e., number of labeled nodes, is limited, which is
expected as GNNs are trained solely based on the supervision obtained from the
labeled nodes. On the other hand,recent self-supervised learning paradigm aims
to train GNNs by solving pretext tasks that do not require any labeled nodes,
and it has shown to even outperform GNNs trained with few labeled nodes.
However, a major drawback of self-supervised methods is that they fall short of
learning class discriminative node representations since no labeled information
is utilized during training. To this end, we propose a novel semi-supervised
method for graphs, GraFN, that leverages few labeled nodes to ensure nodes that
belong to the same class to be grouped together, thereby achieving the best of
both worlds of semi-supervised and self-supervised methods. Specifically, GraFN
randomly samples support nodes from labeled nodes and anchor nodes from the
entire graph. Then, it minimizes the difference between two predicted class
distributions that are non-parametrically assigned by anchor-supports
similarity from two differently augmented graphs. We experimentally show that
GraFN surpasses both the semi-supervised and self-supervised methods in terms
of node classification on real-world graphs. The source code for GraFN is
available at this https URL.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Explainable Online Lane Change Predictions on a Digital Twin with a  Layer Normalized LSTM and Layer-wise Relevance Propagation</b></summary>
  <p><b>编号</b>：[154]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01292</p>
  <p><b>作者</b>：Christoph Wehner,  Francis Powlesland,  Bashar Altakrouri,  Ute Schmid</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：hand without negatively affecting predictive effectiveness, core implementation includes consuming live data, layer normalized lstms using layer, manoeuvre anticipation go hand, layer normalized lstms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Artificial Intelligence and Digital Twins play an integral role in driving
innovation in the domain of intelligent driving. Long short-term memory (LSTM)
is a leading driver in the field of lane change prediction for manoeuvre
anticipation. However, the decision-making process of such models is complex
and non-transparent, hence reducing the trustworthiness of the smart solution.
This work presents an innovative approach and a technical implementation for
explaining lane change predictions of layer normalized LSTMs using Layer-wise
Relevance Propagation (LRP). The core implementation includes consuming live
data from a digital twin on a German highway, live predictions and explanations
of lane changes by extending LRP to layer normalized LSTMs, and an interface
for communicating and explaining the predictions to a human user. We aim to
demonstrate faithful, understandable, and adaptable explanations of lane change
prediction to increase the adoption and trustworthiness of AI systems that
involve humans. Our research also emphases that explainability and
state-of-the-art performance of ML models for manoeuvre anticipation go hand in
hand without negatively affecting predictive effectiveness.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Analysis of lifelog data using optimal feature selection based  unsupervised logistic regression (OFS-ULR) for chronic disease classification</b></summary>
  <p><b>编号</b>：[159]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01281</p>
  <p><b>作者</b>：Sadhana Tiwari,  Sonali Agarwal</p>
  <p><b>备注</b>：Data analytics and Machine learning in healthcare</p>
  <p><b>关键词</b>：performance evaluation using spark streaming environment, newly constructed model achieved highest accuracy, conventional classification models show limited performance, based unsupervised logistic regression model, pervasive healthcare monitoring systems causes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advancement in the field of pervasive healthcare monitoring systems
causes the generation of a huge amount of lifelog data in real-time. Chronic
diseases are one of the most serious health challenges in developing and
developed countries. According to WHO, this accounts for 73% of all deaths and
60% of the global burden of diseases. Chronic disease classification models are
now harnessing the potential of lifelog data to explore better healthcare
practices. This paper is to construct an optimal feature selection-based
unsupervised logistic regression model (OFS-ULR) to classify chronic diseases.
Since lifelog data analysis is crucial due to its sensitive nature; thus the
conventional classification models show limited performance. Therefore,
designing new classifiers for the classification of chronic diseases using
lifelog data is the need of the age. The vital part of building a good model
depends on pre-processing of the dataset, identifying important features, and
then training a learning algorithm with suitable hyper parameters for better
performance. The proposed approach improves the performance of existing methods
using a series of steps such as (i) removing redundant or invalid instances,
(ii) making the data labelled using clustering and partitioning the data into
classes, (iii) identifying the suitable subset of features by applying either
some domain knowledge or selection algorithm, (iv) hyper parameter tuning for
models to get best results, and (v) performance evaluation using Spark
streaming environment. For this purpose, two-time series datasets are used in
the experiment to compute the accuracy, recall, precision, and f1-score. The
experimental analysis proves the suitability of the proposed approach as
compared to the conventional classifiers and our newly constructed model
achieved highest accuracy and reduced training complexity among all among all.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：FedSynth: Gradient Compression via Synthetic Data in Federated Learning</b></summary>
  <p><b>编号</b>：[162]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01273</p>
  <p><b>作者</b>：Shengyuan Hu,  Jack Goetz,  Kshitiz Malik,  Hongyuan Zhan,  Zhe Liu,  Yue Liu</p>
  <p><b>备注</b>：8 pages</p>
  <p><b>关键词</b>：three common federated learning benchmark datasets, model performs similarly well, local model update via, random masking baselines, global model accuracy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Model compression is important in federated learning (FL) with large models
to reduce communication cost. Prior works have been focusing on sparsification
based compression that could desparately affect the global model accuracy. In
this work, we propose a new scheme for upstream communication where instead of
transmitting the model update, each client learns and transmits a light-weight
synthetic dataset such that using it as the training data, the model performs
similarly well on the real training data. The server will recover the local
model update via the synthetic data and apply standard aggregation. We then
provide a new algorithm FedSynth to learn the synthetic data locally.
Empirically, we find our method is comparable/better than random masking
baselines in all three common federated learning benchmark datasets.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Analysis of Joint Speech-Text Embeddings for Semantic Matching</b></summary>
  <p><b>编号</b>：[176]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01235</p>
  <p><b>作者</b>：Muhammad Huzaifah,  Ivan Kukanov</p>
  <p><b>备注</b>：Submitted to INTERSPEECH 2022 for review</p>
  <p><b>关键词</b>：quantitative retrieval accuracy metric, language processing problems involving, pretrained language model acting, incorporate automatic speech recognition, text embedding space trained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Embeddings play an important role in many recent end-to-end solutions for
language processing problems involving more than one data modality. Although
there has been some effort to understand the properties of single-modality
embedding spaces, particularly that of text, their cross-modal counterparts are
less understood. In this work, we study a joint speech-text embedding space
trained for semantic matching by minimizing the distance between paired
utterance and transcription inputs. This was done through dual encoders in a
teacher-student model setup, with a pretrained language model acting as the
teacher and a transformer-based speech encoder as the student. We extend our
method to incorporate automatic speech recognition through both pretraining and
multitask scenarios and found that both approaches improve semantic matching.
Multiple techniques were utilized to analyze and evaluate cross-modal semantic
alignment of the embeddings: a quantitative retrieval accuracy metric,
zero-shot classification to investigate generalizability, and probing of the
encoders to observe the extent of knowledge transfer from one modality to
another.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：MLPro: A System for Hosting Crowdsourced Machine Learning Challenges for  Open-Ended Research Problems</b></summary>
  <p><b>编号</b>：[183]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01216</p>
  <p><b>作者</b>：Peter Washington,  Aayush Nandkeolyar,  Sam Yang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：typical ml team could viably investigate, automatic online code judging platform, many experts submit similar solutions, creativity applied towards inventing, automated expert crowdsourcing systems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The task of developing a machine learning (ML) model for a particular problem
is inherently open-ended, and there is an unbounded set of possible solutions.
Steps of the ML development pipeline, such as feature engineering, loss
function specification, data imputation, and dimensionality reduction, require
the engineer to consider an extensive and often infinite array of
possibilities. Successfully identifying high-performing solutions for an
unfamiliar dataset or problem requires a mix of mathematical prowess and
creativity applied towards inventing and repurposing novel ML methods. Here, we
explore the feasibility of hosting crowdsourced ML challenges to facilitate a
breadth-first exploration of open-ended research problems, thereby expanding
the search space of problem solutions beyond what a typical ML team could
viably investigate. We develop MLPro, a system which combines the notion of
open-ended ML coding problems with the concept of an automatic online code
judging platform. To conduct a pilot evaluation of this paradigm, we
crowdsource several open-ended ML challenges to ML and data science
practitioners. We describe results from two separate challenges. We find that
for sufficiently unconstrained and complex problems, many experts submit
similar solutions, but some experts provide unique solutions which outperform
the "typical" solution class. We suggest that automated expert crowdsourcing
systems such as MLPro have the potential to accelerate ML engineering
creativity.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：Learning Linear Symmetries in Data Using Moment Matching</b></summary>
  <p><b>编号</b>：[184]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01213</p>
  <p><b>作者</b>：Colin Hagemeyer</p>
  <p><b>备注</b>：25 pages, 10 figures</p>
  <p><b>关键词</b>：using methods like data augmentation, use symmetries derived, graph automorphism problem, finding orthogonal symmetries, different methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>It is common in machine learning and statistics to use symmetries derived
from expert knowledge to simplify problems or improve performance, using
methods like data augmentation or penalties. In this paper we consider the
unsupervised and semi-supervised problems of learning such symmetries in a
distribution directly from data in a model-free fashion. We show that in the
worst case this problem is as difficult as the graph automorphism problem.
However, if we restrict to the case where the covariance matrix has unique
eigenvalues, then the eigenvectors will also be eigenvectors of the symmetry
transformation. If we further restrict to finding orthogonal symmetries, then
the eigenvalues will be either be 1 or -1, and the problem reduces to
determining which eigenvectors are which. We develop and compare theoretically
and empirically the effectiveness of different methods of selecting which
eigenvectors should have eigenvalue -1 in the symmetry transformation, and
discuss how to extend this approach to non-orthogonal cases where we have
labels</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Towards Large-Scale Learned Solvers for Parametric PDEs with  Model-Parallel Fourier Neural Operators</b></summary>
  <p><b>编号</b>：[188]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01205</p>
  <p><b>作者</b>：Thomas J. Grady II,  Rishi Khan,  Mathias Louboutin,  Ziyi Yin,  Philipp A. Witte,  Ranveer Chandra,  Russell J. Hewett,  Felix J. Herrmann</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：recently introduced neural network architecture, conventional numerical pde solvers, fourier neural operators, varying pde solutions, simulating multiphase co</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Fourier neural operators (FNOs) are a recently introduced neural network
architecture for learning solution operators of partial differential equations
(PDEs), which have been shown to perform significantly better than comparable
approaches based on convolutional networks. Once trained, FNOs can achieve
speed-ups of multiple orders of magnitude over conventional numerical PDE
solvers. However, due to the high dimensionality of their input data and
network weights, FNOs have so far only been applied to two-dimensional or small
three-dimensional problems. To remove this limited problem-size barrier, we
propose a model-parallel version of FNOs based on domain-decomposition of both
the input data and network weights. We demonstrate that our model-parallel FNO
is able to predict time-varying PDE solutions of over 3.2 billions variables on
Summit using up to 768 GPUs and show an example of training a distributed FNO
on the Azure cloud for simulating multiphase CO$_2$ dynamics in the Earth's
subsurface.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Revisiting Sliced Wasserstein on Images: From Vectorization to  Convolution</b></summary>
  <p><b>编号</b>：[193]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01188</p>
  <p><b>作者</b>：Khai Nguyen,  Nhat Ho</p>
  <p><b>备注</b>：34 pages, 12 figures, 10 tables</p>
  <p><b>关键词</b>：later slicing process becomes harder, variants via incorporating stride, training deep generative modeling, propose novel slicing methods, dimensional projected probability measures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The conventional sliced Wasserstein is defined between two probability
measures that have realizations as vectors. When comparing two probability
measures over images, practitioners first need to vectorize images and then
project them to one-dimensional space by using matrix multiplication between
the sample matrix and the projection matrix. After that, the sliced Wasserstein
is evaluated by averaging the two corresponding one-dimensional projected
probability measures. However, this approach has two limitations. The first
limitation is that the spatial structure of images is not captured efficiently
by the vectorization step; therefore, the later slicing process becomes harder
to gather the discrepancy information. The second limitation is memory
inefficiency since each slicing direction is a vector that has the same
dimension as the images. To address these limitations, we propose novel slicing
methods for sliced Wasserstein between probability measures over images that
are based on the convolution operators. We derive convolution sliced
Wasserstein (CSW) and its variants via incorporating stride, dilation, and
non-linear activation function into the convolution operators. We investigate
the metricity of CSW as well as its sample complexity, its computational
complexity, and its connection to conventional sliced Wasserstein distances.
Finally, we demonstrate the favorable performance of CSW over the conventional
sliced Wasserstein in comparing probability measures over images and in
training deep generative modeling on images.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Why Exposure Bias Matters: An Imitation Learning Perspective of Error  Accumulation in Language Generation</b></summary>
  <p><b>编号</b>：[199]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01171</p>
  <p><b>作者</b>：Kushal Arora,  Layla El Asri,  Hareesh Bahuleyan,  Jackie Chi Kit Cheung</p>
  <p><b>备注</b>：Accepted in Findings of ACL 2022</p>
  <p><b>关键词</b>：current language generation models suffer, poor generation quality, generation procedure mismatch, imitation learning perspective, exposure bias leads</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Current language generation models suffer from issues such as repetition,
incoherence, and hallucinations. An often-repeated hypothesis is that this
brittleness of generation models is caused by the training and the generation
procedure mismatch, also referred to as exposure bias. In this paper, we verify
this hypothesis by analyzing exposure bias from an imitation learning
perspective. We show that exposure bias leads to an accumulation of errors,
analyze why perplexity fails to capture this accumulation, and empirically show
that this accumulation results in poor generation quality. Source code to
reproduce these experiments is available at
this https URL</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Seemo: A new tool for early design window view satisfaction evaluation  in residential buildings</b></summary>
  <p><b>编号</b>：[201]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01164</p>
  <p><b>作者</b>：Jaeha Kim,  Michael Kent,  Katharina Kral,  Timur Dogan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：better understand occupant view satisfaction, 181 participant view satisfaction survey, existing view satisfaction data must, related research remains challenging, people spend approximately 90</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>People spend approximately 90% of their lives indoors, and thus arguably, the
indoor space design can significantly influence occupant well-being. Adequate
views to the outside are one of the most cited indoor qualities related to
occupant well-being. However, due to urbanization and densification trends,
designers may have difficulties in providing vistas and views to the outside
with an assortment of content, which can support the needs of their occupants.
To better understand occupant view satisfaction and provide reliable design
feedback to architects, existing view satisfaction data must be expanded to
capture a wider variety of view scenarios and occupants. Most related research
remains challenging in architectural practice due to a lack of easy-to-use
early-design analysis tools. However, early assessment of view can be
advantageous as design decisions in early design, such as building orientation,
plan layout, and facade design, can improve the view quality. This paper,
hence, presents results from a 181 participant view satisfaction survey with
590 window views. The survey data is used to train a tree-regression model to
predict view satisfaction. The prediction performance was compared to an
existing view assessment framework through case studies. The result showed that
the new prediction is more accurate to the surveyed result than the framework.
Further, the prediction performance was generally high for most responses,
verifying the reliability. To facilitate view analysis in early design, this
paper describes integrating the satisfaction prediction model and a ray-casting
tool to compute view parameters in the CAD environment.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：Best-Response Bayesian Reinforcement Learning with Bayes-adaptive POMDPs  for Centaurs</b></summary>
  <p><b>编号</b>：[202]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01160</p>
  <p><b>作者</b>：Mustafa Mert Çelikok,  Frans A. Oliehoek,  Samuel Kaski</p>
  <p><b>备注</b>：This paper is presented in part at the International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS) 2022</p>
  <p><b>关键词</b>：rational humans make better decisions reduces, modelled using bayesian best, machine must make sure, towards better decisions, preliminary theoretical analysis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Centaurs are half-human, half-AI decision-makers where the AI's goal is to
complement the human. To do so, the AI must be able to recognize the goals and
constraints of the human and have the means to help them. We present a novel
formulation of the interaction between the human and the AI as a sequential
game where the agents are modelled using Bayesian best-response models. We show
that in this case the AI's problem of helping bounded-rational humans make
better decisions reduces to a Bayes-adaptive POMDP. In our simulated
experiments, we consider an instantiation of our framework for humans who are
subjectively optimistic about the AI's future behaviour. Our results show that
when equipped with a model of the human, the AI can infer the human's bounds
and nudge them towards better decisions. We discuss ways in which the machine
can learn to improve upon its own limitations as well with the help of the
human. We identify a novel trade-off for centaurs in partially observable
tasks: for the AI's actions to be acceptable to the human, the machine must
make sure their beliefs are sufficiently aligned, but aligning beliefs might be
costly. We present a preliminary theoretical analysis of this trade-off and its
dependence on task structure.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：Byzantine-Robust Federated Linear Bandits</b></summary>
  <p><b>编号</b>：[205]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01155</p>
  <p><b>作者</b>：Ali Jadbabaie,  Haochuan Li,  Jian Qian,  Yi Tian</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：standard federated learning algorithms applied, sublinear $\ tilde {\ mathcal, linear bandit optimization problem, common linear bandit model, algorithm differentially private via</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we study a linear bandit optimization problem in a federated
setting where a large collection of distributed agents collaboratively learn a
common linear bandit model. Standard federated learning algorithms applied to
this setting are vulnerable to Byzantine attacks on even a small fraction of
agents. We propose a novel algorithm with a robust aggregation oracle that
utilizes the geometric median. We prove that our proposed algorithm is robust
to Byzantine attacks on fewer than half of agents and achieves a sublinear
$\tilde{\mathcal{O}}({T^{3/4}})$ regret with $\mathcal{O}(\sqrt{T})$ steps of
communication in $T$ steps. Moreover, we make our algorithm differentially
private via a tree-based mechanism. Finally, if the level of corruption is
known to be small, we show that using the geometric median of mean oracle for
robust aggregation further improves the regret bound.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Proactive Anomaly Detection for Robot Navigation with Multi-Sensor  Fusion</b></summary>
  <p><b>编号</b>：[209]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01146</p>
  <p><b>作者</b>：Tianchen Ji,  Arun Narenthiran Sivakumar,  Girish Chowdhary,  Katherine Driggs-Campbell</p>
  <p><b>备注</b>：Accepted by RA-L with ICRA 2022 option</p>
  <p><b>关键词</b>：reactive anomaly detection methods identify anomalous task executions based, field robot data demonstrates superior failure identification performance, mobile robots often produce anomalous behaviors, provide robust anomaly detection, proactive anomaly detection network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite the rapid advancement of navigation algorithms, mobile robots often
produce anomalous behaviors that can lead to navigation failures. The ability
to detect such anomalous behaviors is a key component in modern robots to
achieve high-levels of autonomy. Reactive anomaly detection methods identify
anomalous task executions based on the current robot state and thus lack the
ability to alert the robot before an actual failure occurs. Such an alert delay
is undesirable due to the potential damage to both the robot and the
surrounding objects. We propose a proactive anomaly detection network (PAAD)
for robot navigation in unstructured and uncertain environments. PAAD predicts
the probability of future failure based on the planned motions from the
predictive controller and the current observation from the perception module.
Multi-sensor signals are fused effectively to provide robust anomaly detection
in the presence of sensor occlusion as seen in field environments. Our
experiments on field robot data demonstrates superior failure identification
performance than previous methods, and that our model can capture anomalous
behaviors in real-time while maintaining a low false detection rate in
cluttered fields. Code, dataset, and video are available at
this https URL</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：A System for Interactive Examination of Learned Security Policies</b></summary>
  <p><b>编号</b>：[214]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01126</p>
  <p><b>作者</b>：Kim Hammar,  Rolf Stadler</p>
  <p><b>备注</b>：Preprint, original submission to NOMS22 Demo track. Copyright IEEE, may be transferred without notice. arXiv admin note: text overlap with arXiv:2111.00289</p>
  <p><b>关键词</b>：network intrusion use case, reinforcement learning approach, markov decision processes, system enables insight, learned security policies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a system for interactive examination of learned security policies.
It allows a user to traverse episodes of Markov decision processes in a
controlled manner and to track the actions triggered by security policies.
Similar to a software debugger, a user can continue or or halt an episode at
any time step and inspect parameters and probability distributions of interest.
The system enables insight into the structure of a given policy and in the
behavior of a policy in edge cases. We demonstrate the system with a network
intrusion use case. We examine the evolution of an IT infrastructure's state
and the actions prescribed by security policies while an attack occurs. The
policies for the demonstration have been obtained through a reinforcement
learning approach that includes a simulation system where policies are
incrementally learned and an emulation system that produces statistics that
drive the simulation runs.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：Fitting an immersed submanifold to data via Sussmann's orbit theorem</b></summary>
  <p><b>编号</b>：[216]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01119</p>
  <p><b>作者</b>：Joshua Hanson,  Maxim Raginsky</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：flows along finitely many vector fields starting, proposed approach makes fundamental use, minimum expected reconstruction error, excess risk relative, empirical risk minimization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper describes an approach for fitting an immersed submanifold of a
finite-dimensional Euclidean space to random samples. The reconstruction
mapping from the ambient space to the desired submanifold is implemented as a
composition of an encoder that maps each point to a tuple of (positive or
negative) times and a decoder given by a composition of flows along finitely
many vector fields starting from a fixed initial point. The encoder supplies
the times for the flows. The encoder-decoder map is obtained by empirical risk
minimization, and a high-probability bound is given on the excess risk relative
to the minimum expected reconstruction error over a given class of
encoder-decoder maps. The proposed approach makes fundamental use of Sussmann's
orbit theorem, which guarantees that the image of the reconstruction map is
indeed contained in an immersed submanifold.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：Adversarially robust segmentation models learn perceptually-aligned  gradients</b></summary>
  <p><b>编号</b>：[226]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01099</p>
  <p><b>作者</b>：Pedro Sandoval-Segura</p>
  <p><b>备注</b>：12 pages, 3 figures</p>
  <p><b>关键词</b>：adversarially robust models exhibit gradients, place additional weight behind, producing plausible image inpaintings, trained semantic segmentation networks, semantic segmentation networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The effects of adversarial training on semantic segmentation networks has not
been thoroughly explored. While previous work has shown that
adversarially-trained image classifiers can be used to perform image synthesis,
we have yet to understand how best to leverage an adversarially-trained
segmentation network to do the same. Using a simple optimizer, we demonstrate
that adversarially-trained semantic segmentation networks can be used to
perform image inpainting and generation. Our experiments demonstrate that
adversarially-trained segmentation networks are more robust and indeed exhibit
perceptually-aligned gradients which help in producing plausible image
inpaintings. We seek to place additional weight behind the hypothesis that
adversarially robust models exhibit gradients that are more
perceptually-aligned with human vision. Through image synthesis, we argue that
perceptually-aligned gradients promote a better understanding of a neural
network's learned representations and aid in making neural networks more
interpretable.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：pmuBAGE: The Benchmarking Assortment of Generated PMU Data for Power  System Events -- Part I: Overview and Results</b></summary>
  <p><b>编号</b>：[228]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01095</p>
  <p><b>作者</b>：Brandon Foggo,  Koji Yamashita,  Nanpeng Yu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：create highly realistic event data without compromising, highly accessible standard benchmarking dataset would enable, recently seen phenomenal advancements via data, successful machine learning techniques, novel learning method based</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present pmuGE (phasor measurement unit Generator of Events), one of the
first data-driven generative model for power system event data. We have trained
this model on thousands of actual events and created a dataset denoted pmuBAGE
(the Benchmarking Assortment of Generated PMU Events). The dataset consists of
almost 1000 instances of labeled event data to encourage benchmark evaluations
on phasor measurement unit (PMU) data analytics. The dataset is available
online for use by any researcher or practitioner in the field. PMU data are
challenging to obtain, especially those covering event periods. Nevertheless,
power system problems have recently seen phenomenal advancements via
data-driven machine learning solutions - solutions created by researchers who
were fortunate enough to obtain such PMU data. A highly accessible standard
benchmarking dataset would enable a drastic acceleration of the development of
successful machine learning techniques in this field. We propose a novel
learning method based on the Event Participation Decomposition of Power System
Events, which makes it possible to learn a generative model of PMU data during
system anomalies. The model can create highly realistic event data without
compromising the differential privacy of the PMUs used to train it. The dataset
is available online for any researcher to use at the pmuBAGE Github Repository
- this https URL.
Part I - This is part I of a two part paper. In part I, we describe a high
level overview of pmuBAGE, its creation, and the experiments used to test it.
Part II will discuss the exact models used in its generation in far more
detail.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：Breaking the De-Pois Poisoning Defense</b></summary>
  <p><b>编号</b>：[230]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01090</p>
  <p><b>作者</b>：Alaa Anani,  Mohamed Ghanem,  Lotfy Abdel Khaliq</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：target models simultaneously -- allowing us, proposed effective defense models, machine learning models, one major variant, evasive issue resembling</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Attacks on machine learning models have been, since their conception, a very
persistent and evasive issue resembling an endless cat-and-mouse game. One
major variant of such attacks is poisoning attacks which can indirectly
manipulate an ML model. It has been observed over the years that the majority
of proposed effective defense models are only effective when an attacker is not
aware of them being employed. In this paper, we show that the attack-agnostic
De-Pois defense is hardly an exception to that rule. In fact, we demonstrate
its vulnerability to the simplest White-Box and Black-Box attacks by an
attacker that knows the structure of the De-Pois defense model. In essence, the
De-Pois defense relies on a critic model that can be used to detect poisoned
data before passing it to the target model. In our work, we break this
poison-protection layer by replicating the critic model and then performing a
composed gradient-sign attack on both the critic and target models
simultaneously -- allowing us to bypass the critic firewall to poison the
target model.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：Faces: AI Blitz XIII Solutions</b></summary>
  <p><b>编号</b>：[234]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01081</p>
  <p><b>作者</b>：Andrew Melnik,  Eren Akbulut,  Jannik Sheikh,  Kira Loos,  Michael Buettner,  Tobias Lenze</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ai blitz xiii faces challenge hosted, team glados took second place, http url platform consisted, https url, sentiment classification</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>AI Blitz XIII Faces challenge hosted on this http URL platform consisted of
five problems: Sentiment Classification, Age Prediction, Mask Prediction, Face
Recognition, and Face De-Blurring. Our team GLaDOS took second place. Here we
present our solutions and results. Code implementation:
this https URL</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：Data Cards: Purposeful and Transparent Dataset Documentation for  Responsible AI</b></summary>
  <p><b>编号</b>：[237]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01075</p>
  <p><b>作者</b>：Mahima Pushkarna (1),  Andrew Zaldivar (1),  Oddur Kjartansson (1) ((1) Google Research)</p>
  <p><b>备注</b>：Submitted to ACM Conference on Fairness, Accountability, and Transparency 2022 (ACM FAccT 2022) 17 pages (including references) , 2 figures, 3 tables. Appendix A: 1 pages, 1 table; Appendix B: 1 page, 1 table; Appendix C: 5 Pages, 5 figures; Appendix D: 3 pages, 3 figures; Appendix E: 24 pages, 24 figures</p>
  <p><b>关键词</b>：using two case studies, decisions affecting model performance, industry moves towards large, support adoption across domains, present lessons learned</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As research and industry moves towards large-scale models capable of numerous
downstream tasks, the complexity of understanding multi-modal datasets that
give nuance to models rapidly increases. A clear and thorough understanding of
a dataset's origins, development, intent, ethical considerations and evolution
becomes a necessary step for the responsible and informed deployment of models,
especially those in people-facing contexts and high-risk domains. However, the
burden of this understanding often falls on the intelligibility, conciseness,
and comprehensiveness of the documentation. It requires consistency and
comparability across the documentation of all datasets involved, and as such
documentation must be treated as a user-centric product in and of itself. In
this paper, we propose Data Cards for fostering transparent, purposeful and
human-centered documentation of datasets within the practical contexts of
industry and research. Data Cards are structured summaries of essential facts
about various aspects of ML datasets needed by stakeholders across a dataset's
lifecycle for responsible AI development. These summaries provide explanations
of processes and rationales that shape the data and consequently the models,
such as upstream sources, data collection and annotation methods; training and
evaluation methods, intended use; or decisions affecting model performance. We
also present frameworks that ground Data Cards in real-world utility and
human-centricity. Using two case studies, we report on desirable
characteristics that support adoption across domains, organizational
structures, and audience groups. Finally, we present lessons learned from
deploying over 20 Data Cards.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：A Differentially Private Framework for Deep Learning with Convexified  Loss Functions</b></summary>
  <p><b>编号</b>：[243]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01049</p>
  <p><b>作者</b>：Zhigang Lu,  Hassan Jameel Asghar,  Mohamed Ali Kaafar,  Darren Webb,  Peter Dickinson</p>
  <p><b>备注</b>：This paper has been accepted by the IEEE Transactions on Information Forensics & Security. Early access of IEEE Explore will be available soon</p>
  <p><b>关键词</b>：source differentially private stochastic gradient descent, overall privacy budget $\ epsilon, objective functions limit objective perturbation, six commonly used real, private neural network trained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Differential privacy (DP) has been applied in deep learning for preserving
privacy of the underlying training sets. Existing DP practice falls into three
categories - objective perturbation, gradient perturbation and output
perturbation. They suffer from three main problems. First, conditions on
objective functions limit objective perturbation in general deep learning
tasks. Second, gradient perturbation does not achieve a satisfactory
privacy-utility trade-off due to over-injected noise in each epoch. Third, high
utility of the output perturbation method is not guaranteed because of the
loose upper bound on the global sensitivity of the trained model parameters as
the noise scale parameter. To address these problems, we analyse a tighter
upper bound on the global sensitivity of the model parameters. Under a
black-box setting, based on this global sensitivity, to control the overall
noise injection, we propose a novel output perturbation framework by injecting
DP noise into a randomly sampled neuron (via the exponential mechanism) at the
output layer of a baseline non-private neural network trained with a
convexified loss function. We empirically compare the privacy-utility
trade-off, measured by accuracy loss to baseline non-private models and the
privacy leakage against black-box membership inference (MI) attacks, between
our framework and the open-source differentially private stochastic gradient
descent (DP-SGD) approaches on six commonly used real-world datasets. The
experimental evaluations show that, when the baseline models have observable
privacy leakage under MI attacks, our framework achieves a better
privacy-utility trade-off than existing DP-SGD implementations, given an
overall privacy budget $\epsilon \leq 1$ for a large number of queries.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：On Efficiently Acquiring Annotations for Multilingual Models</b></summary>
  <p><b>编号</b>：[252]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01016</p>
  <p><b>作者</b>：Joel Ruben Antony Moniz,  Barun Patra,  Matthew R. Gormley</p>
  <p><b>备注</b>：ACL 2022 (Short Paper)</p>
  <p><b>关键词</b>：joint learning across multiple languages using, annotation budget divided equally among, single model performs substantially better, active learning provides additional, supporting multiple languages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>When tasked with supporting multiple languages for a given problem, two
approaches have arisen: training a model for each language with the annotation
budget divided equally among them, and training on a high-resource language
followed by zero-shot transfer to the remaining languages. In this work, we
show that the strategy of joint learning across multiple languages using a
single model performs substantially better than the aforementioned
alternatives. We also demonstrate that active learning provides additional,
complementary benefits. We show that this simple approach enables the model to
be data efficient by allowing it to arbitrate its annotation budget to query
languages it is less certain on. We illustrate the effectiveness of our
proposed method on a diverse set of tasks: a classification task with 4
languages, a sequence tagging task with 4 languages and a dependency parsing
task with 5 languages. Our proposed method, whilst simple, substantially
outperforms the other viable alternatives for building a model in a
multilingual setting under constrained budgets.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：A Computational Analysis of Pitch Drift in Unaccompanied Solo Singing  using DBSCAN Clustering</b></summary>
  <p><b>编号</b>：[254]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01009</p>
  <p><b>作者</b>：Sepideh Shafiei,  S. Hakam</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：unaccompanied vocalists usually change, using pitch histogram, measuring pitch drift, called pitch drift, unaccompanied vocal performance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Unaccompanied vocalists usually change the tuning unintentionally and end up
with a higher or lower pitch than the starting point during a long performance.
This phenomenon is called pitch drift, which is dependent on various elements,
such as the skill of the performer, and the length and difficulty of the
performance. In this paper, we propose a computational method for measuring
pitch drift in the course of an unaccompanied vocal performance, using pitch
histogram and DBSCAN clustering.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：Towards Web Phishing Detection Limitations and Mitigation</b></summary>
  <p><b>编号</b>：[266]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00985</p>
  <p><b>作者</b>：Alsharif Abuadbba,  Shuo Wang,  Mahathir Almashor,  Muhammed Ejaz Ahmed,  Raj Gaire,  Seyit Camtepe,  Surya Nepal</p>
  <p><b>备注</b>：12 pages</p>
  <p><b>关键词</b>：10k phishing links reported per hour, 13k phishing pages targeting major brands, ultimate phishing content within javascript, benign sites show promising accuracy, results show successful evasion</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Web phishing remains a serious cyber threat responsible for most data
breaches. Machine Learning (ML)-based anti-phishing detectors are seen as an
effective countermeasure, and are increasingly adopted by web-browsers and
software products. However, with an average of 10K phishing links reported per
hour to platforms such as PhishTank and VirusTotal (VT), the deficiencies of
such ML-based solutions are laid bare. We first explore how phishing sites
bypass ML-based detection with a deep dive into 13K phishing pages targeting
major brands such as Facebook. Results show successful evasion is caused by:
(1) use of benign services to obscure phishing URLs; (2) high similarity
between the HTML structures of phishing and benign pages; (3) hiding the
ultimate phishing content within Javascript and running such scripts only on
the client; (4) looking beyond typical credentials and credit cards for new
content such as IDs and documents; (5) hiding phishing content until after
human interaction. We attribute the root cause to the dependency of ML-based
models on the vertical feature space (webpage content). These solutions rely
only on what phishers present within the page itself. Thus, we propose
Anti-SubtlePhish, a more resilient model based on logistic regression. The key
augmentation is the inclusion of a horizontal feature space, which examines
correlation variables between the final render of suspicious pages against what
trusted services have recorded (e.g., PageRank). To defeat (1) and (2), we
correlate information between WHOIS, PageRank, and page analytics. To combat
(3), (4) and (5), we correlate features after rendering the page. Experiments
with 100K phishing/benign sites show promising accuracy (98.8%). We also
obtained 100% accuracy against 0-day phishing pages that were manually crafted,
comparing well to the 0% recorded by VT vendors over the first four days.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：FedGBF: An efficient vertical federated learning framework via gradient  boosting and bagging</b></summary>
  <p><b>编号</b>：[270]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00976</p>
  <p><b>作者</b>：Yujin Han,  Pan Du,  Kai Yang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：existing federated boosting model sequentially builds, federated bagging model saves time, vertically federated setting termed, high interactive communication costs, attracted increasing attention recently</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Federated learning, conducive to solving data privacy and security problems,
has attracted increasing attention recently. However, the existing federated
boosting model sequentially builds a decision tree model with the weak base
learner, resulting in redundant boosting steps and high interactive
communication costs. In contrast, the federated bagging model saves time by
building multi-decision trees in parallel, but it suffers from performance
loss. With the aim of obtaining an outstanding performance with less time cost,
we propose a novel model in a vertically federated setting termed as Federated
Gradient Boosting Forest (FedGBF). FedGBF simultaneously integrates the
boosting and bagging's preponderance by building the decision trees in parallel
as a base learner for boosting. Subsequent to FedGBF, the problem of
hyperparameters tuning is rising. Then we propose the Dynamic FedGBF, which
dynamically changes each forest's parameters and thus reduces the complexity.
Finally, the experiments based on the benchmark datasets demonstrate the
superiority of our method.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：Kernel Extreme Learning Machine Optimized by the Sparrow Search  Algorithm for Hyperspectral Image Classification</b></summary>
  <p><b>编号</b>：[273]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00973</p>
  <p><b>作者</b>：Zhixin Yan,  Jiawei Huang,  Kehua Xiang</p>
  <p><b>备注</b>：17 pages</p>
  <p><b>关键词</b>：multiscale fusion feature hyperspectral image classification method, new swarm intelligence optimization method, hyperspectral image classification algorithm, kernel extreme learning machine, strong global search capability</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To improve the classification performance and generalization ability of the
hyperspectral image classification algorithm, this paper uses Multi-Scale Total
Variation (MSTV) to extract the spectral features, local binary pattern (LBP)
to extract spatial features, and feature superposition to obtain the fused
features of hyperspectral images. A new swarm intelligence optimization method
with high convergence and strong global search capability, the Sparrow Search
Algorithm (SSA), is used to optimize the kernel parameters and regularization
coefficients of the Kernel Extreme Learning Machine (KELM). In summary, a
multiscale fusion feature hyperspectral image classification method (MLS-KELM)
is proposed in this paper. The Indian Pines, Pavia University and Houston 2013
datasets were selected to validate the classification performance of MLS-KELM,
and the method was applied to ZY1-02D hyperspectral data. The experimental
results show that MLS-KELM has better classification performance and
generalization ability compared with other popular classification methods, and
MLS-KELM shows its strong robustness in the small sample case.</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：A Dynamic Meta-Learning Model for Time-Sensitive Cold-Start  Recommendations</b></summary>
  <p><b>编号</b>：[275]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00970</p>
  <p><b>作者</b>：Krishna Prasad Neupane,  Ervine Zheng,  Yu Kong,  Qi Yu</p>
  <p><b>备注</b>：7 pages, conference</p>
  <p><b>关键词</b>：historical interactions may also lead, world data help demonstrate, turn relatively inactive recently, proposed model leverages historical, novel dynamic recommendation model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a novel dynamic recommendation model that focuses on users who
have interactions in the past but turn relatively inactive recently. Making
effective recommendations to these time-sensitive cold-start users is critical
to maintain the user base of a recommender system. Due to the sparse recent
interactions, it is challenging to capture these users' current preferences
precisely. Solely relying on their historical interactions may also lead to
outdated recommendations misaligned with their recent interests. The proposed
model leverages historical and current user-item interactions and dynamically
factorizes a user's (latent) preference into time-specific and time-evolving
representations that jointly affect user behaviors. These latent factors
further interact with an optimized item embedding to achieve accurate and
timely recommendations. Experiments over real-world data help demonstrate the
effectiveness of the proposed time-sensitive cold-start recommendation model.</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：Dynamic physical activity recommendation on personalised mobile health  information service: A deep reinforcement learning approach</b></summary>
  <p><b>编号</b>：[278]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00961</p>
  <p><b>作者</b>：Ji Fang,  Vincent CS Lee,  Haiyan Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：current health service system usually provides recommendations based, information service makes healthcare management easier, mhealth information service system comprising, future health outcomes may reduce, make physical activity recommendation decisions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Mobile health (mHealth) information service makes healthcare management
easier for users, who want to increase physical activity and improve health.
However, the differences in activity preference among the individual, adherence
problems, and uncertainty of future health outcomes may reduce the effect of
the mHealth information service. The current health service system usually
provides recommendations based on fixed exercise plans that do not satisfy the
user specific needs. This paper seeks an efficient way to make physical
activity recommendation decisions on physical activity promotion in
personalised mHealth information service by establishing data-driven model. In
this study, we propose a real-time interaction model to select the optimal
exercise plan for the individual considering the time-varying characteristics
in maximising the long-term health utility of the user. We construct a
framework for mHealth information service system comprising a personalised AI
module, which is based on the scientific knowledge about physical activity to
evaluate the individual exercise performance, which may increase the awareness
of the mHealth artificial intelligence system. The proposed deep reinforcement
learning (DRL) methodology combining two classes of approaches to improve the
learning capability for the mHealth information service system. A deep learning
method is introduced to construct the hybrid neural network combing long-short
term memory (LSTM) network and deep neural network (DNN) techniques to infer
the individual exercise behavior from the time series data. A reinforcement
learning method is applied based on the asynchronous advantage actor-critic
algorithm to find the optimal policy through exploration and exploitation.</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：Long-tailed Extreme Multi-label Text Classification with Generated  Pseudo Label Descriptions</b></summary>
  <p><b>编号</b>：[279]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00958</p>
  <p><b>作者</b>：Ruohong Zhang,  Yau-Shian Wang,  Yiming Yang,  Donghan Yu,  Tom Vu,  Likun Lei</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：severe data scarce problem associated, neural embedding based retrieval models, severe data scarce conditions, proposed approach achieves state, generating informative label descriptions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Extreme Multi-label Text Classification (XMTC) has been a tough challenge in
machine learning research and applications due to the sheer sizes of the label
spaces and the severe data scarce problem associated with the long tail of rare
labels in highly skewed distributions. This paper addresses the challenge of
tail label prediction by proposing a novel approach, which combines the
effectiveness of a trained bag-of-words (BoW) classifier in generating
informative label descriptions under severe data scarce conditions, and the
power of neural embedding based retrieval models in mapping input documents (as
queries) to relevant label descriptions. The proposed approach achieves
state-of-the-art performance on XMTC benchmark datasets and significantly
outperforms the best methods so far in the tail label prediction. We also
provide a theoretical analysis for relating the BoW and neural models w.r.t.
performance lower bound.</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：Model-Free and Model-Based Policy Evaluation when Causality is Uncertain</b></summary>
  <p><b>编号</b>：[280]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00956</p>
  <p><b>作者</b>：David Bruns-Smith</p>
  <p><b>备注</b>：International Conference on Machine Learning. PMLR, 2021</p>
  <p><b>关键词</b>：policy evaluation algorithms give valid causal estimates, robust mdps gives sharper lower bounds, existing techniques produce extremely conservative bounds, may exist unobserved variables, unknown behavior policy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>When decision-makers can directly intervene, policy evaluation algorithms
give valid causal estimates. In off-policy evaluation (OPE), there may exist
unobserved variables that both impact the dynamics and are used by the unknown
behavior policy. These "confounders" will introduce spurious correlations and
naive estimates for a new policy will be biased. We develop worst-case bounds
to assess sensitivity to these unobserved confounders in finite horizons when
confounders are drawn iid each period. We demonstrate that a model-based
approach with robust MDPs gives sharper lower bounds by exploiting domain
knowledge about the dynamics. Finally, we show that when unobserved confounders
are persistent over time, OPE is far more difficult and existing techniques
produce extremely conservative bounds.</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：Exploiting Local and Global Features in Transformer-based Extreme  Multi-label Text Classification</b></summary>
  <p><b>编号</b>：[289]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00933</p>
  <p><b>作者</b>：Ruohong Zhang,  Yau-Shian Wang,  Yiming Yang,  Tom Vu,  Likun Lei</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：level features could bring additional gains, proposed model either outperforms, made significant performance improvements, represent different granularity levels, global feature vector may</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Extreme multi-label text classification (XMTC) is the task of tagging each
document with the relevant labels from a very large space of predefined
categories. Recently, large pre-trained Transformer models have made
significant performance improvements in XMTC, which typically use the embedding
of the special CLS token to represent the entire document semantics as a global
feature vector, and match it against candidate labels. However, we argue that
such a global feature vector may not be sufficient to represent different
granularity levels of semantics in the document, and that complementing it with
the local word-level features could bring additional gains. Based on this
insight, we propose an approach that combines both the local and global
features produced by Transformer models to improve the prediction power of the
classifier. Our experiments show that the proposed model either outperforms or
is comparable to the state-of-the-art methods on benchmark datasets.</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：AutoProtoNet: Interpretability for Prototypical Networks</b></summary>
  <p><b>编号</b>：[291]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00929</p>
  <p><b>作者</b>：Pedro Sandoval-Segura,  Wallace Lawson</p>
  <p><b>备注</b>：11 pages, 4 figures</p>
  <p><b>关键词</b>：debug inadequate classification parameters, custom classification task, validation set consisting, prototype refinement method, make meaningful corrections</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In meta-learning approaches, it is difficult for a practitioner to make sense
of what kind of representations the model employs. Without this ability, it can
be difficult to both understand what the model knows as well as to make
meaningful corrections. To address these challenges, we introduce AutoProtoNet,
which builds interpretability into Prototypical Networks by training an
embedding space suitable for reconstructing inputs, while remaining convenient
for few-shot learning. We demonstrate how points in this embedding space can be
visualized and used to understand class representations. We also devise a
prototype refinement method, which allows a human to debug inadequate
classification parameters. We use this debugging technique on a custom
classification task and find that it leads to accuracy improvements on a
validation set consisting of in-the-wild images. We advocate for
interpretability in meta-learning approaches and show that there are
interactive ways for a human to enhance meta-learning algorithms.</p>
  </details>
</details>
<details>
  <summary>83. <b>标题：Class-Incremental Learning by Knowledge Distillation with Adaptive  Feature Consolidation</b></summary>
  <p><b>编号</b>：[307]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00895</p>
  <p><b>作者</b>：Minsoo Kang,  Jaeyoo Park,  Bohyung Han</p>
  <p><b>备注</b>：CVPR 2022</p>
  <p><b>关键词</b>：experimental results show significant accuracy improvement, novel class incremental learning approach based, notorious catastrophic forgetting problem despite, optimization strategy effectively alleviates, resulting loss increases incurred</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a novel class incremental learning approach based on deep neural
networks, which continually learns new tasks with limited memory for storing
examples in the previous tasks. Our algorithm is based on knowledge
distillation and provides a principled way to maintain the representations of
old models while adjusting to new tasks effectively. The proposed method
estimates the relationship between the representation changes and the resulting
loss increases incurred by model updates. It minimizes the upper bound of the
loss increases using the representations, which exploits the estimated
importance of each feature map within a backbone model. Based on the
importance, the model restricts updates of important features for robustness
while allowing changes in less critical features for flexibility. This
optimization strategy effectively alleviates the notorious catastrophic
forgetting problem despite the limited accessibility of data in the previous
tasks. The experimental results show significant accuracy improvement of the
proposed algorithm over the existing methods on the standard datasets. Code is
available.</p>
  </details>
</details>
<details>
  <summary>84. <b>标题：Learning List-wise Representation in Reinforcement Learning for Ads  Allocation with Multiple Auxiliary Tasks</b></summary>
  <p><b>编号</b>：[310]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00888</p>
  <p><b>作者</b>：Guogang Liao,  Ze Wang,  Xiaowen Shi,  Xiaoxu Wu,  Chuheng Zhang,  Yongkang Wang,  Xingxing Wang,  Dong Wang</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2109.04353, arXiv:2204.00377</p>
  <p><b>关键词</b>：based ads allocation agent makes decisions based, news feed sites )., conduct extensive offline experiments, world food delivery platform, meituan food delivery platform</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the recent prevalence of reinforcement learning (RL), there have been
tremendous interests in utilizing RL for ads allocation in recommendation
platforms (e.g., e-commerce and news feed sites). For better performance,
recent RL-based ads allocation agent makes decisions based on representations
of list-wise item arrangement. This results in a high-dimensional state-action
space, which makes it difficult to learn an efficient and generalizable
list-wise representation. To address this problem, we propose a novel algorithm
to learn a better representation by leveraging task-specific signals on Meituan
food delivery platform. Specifically, we propose three different types of
auxiliary tasks that are based on reconstruction, prediction, and contrastive
learning respectively. We conduct extensive offline experiments on the
effectiveness of these auxiliary tasks and test our method on real-world food
delivery platform. The experimental results show that our method can learn
better list-wise representations and achieve higher revenue for the platform.</p>
  </details>
</details>
<details>
  <summary>85. <b>标题：Accurate Online Posterior Alignments for Principled  Lexically-Constrained Decoding</b></summary>
  <p><b>编号</b>：[316]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00871</p>
  <p><b>作者</b>：Soumya Chatterjee,  Sunita Sarawagi,  Preethi Jyothi</p>
  <p><b>备注</b>：15 pages, 2 figures. ACL 2022</p>
  <p><b>关键词</b>：good online alignments facilitate important applications, proposed inference technique jointly considers alignment, seamlessly integrated within existing constrained beam, including two distant language pairs, seven lexically constrained translation tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Online alignment in machine translation refers to the task of aligning a
target word to a source word when the target sequence has only been partially
decoded. Good online alignments facilitate important applications such as
lexically constrained translation where user-defined dictionaries are used to
inject lexical constraints into the translation model. We propose a novel
posterior alignment technique that is truly online in its execution and
superior in terms of alignment error rates compared to existing methods. Our
proposed inference technique jointly considers alignment and token
probabilities in a principled manner and can be seamlessly integrated within
existing constrained beam-search decoding algorithms. On five language pairs,
including two distant language pairs, we achieve consistent drop in alignment
error rates. When deployed on seven lexically constrained translation tasks, we
achieve significant improvements in BLEU specifically around the constrained
positions.</p>
  </details>
</details>
<details>
  <summary>86. <b>标题：A Differential Evolution-Enhanced Latent Factor Analysis Model for  High-dimensional and Sparse Data</b></summary>
  <p><b>编号</b>：[320]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00861</p>
  <p><b>作者</b>：Jia Chen,  Di Wu,  Xin Luo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：transitional latent factor analysis, specific gradient direction step, stochastic gradient descent, involved latent factors, various big data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>High-dimensional and sparse (HiDS) matrices are frequently adopted to
describe the complex relationships in various big data-related systems and
applications. A Position-transitional Latent Factor Analysis (PLFA) model can
accurately and efficiently represent an HiDS matrix. However, its involved
latent factors are optimized by stochastic gradient descent with the specific
gradient direction step-by-step, which may cause a suboptimal solution. To
address this issue, this paper proposes a Sequential-Group-Differential-
Evolution (SGDE) algorithm to refine the latent factors optimized by a PLFA
model, thereby achieving a highly-accurate SGDE-PLFA model to HiDS matrices. As
demonstrated by the experiments on four HiDS matrices, a SGDE-PLFA model
outperforms the state-of-the-art models.</p>
  </details>
</details>
<details>
  <summary>87. <b>标题：Adversarial Neon Beam: Robust Physical-World Adversarial Attack to DNNs</b></summary>
  <p><b>编号</b>：[323]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00853</p>
  <p><b>作者</b>：Chengyin Hu,  Kalibinuer Tiliwalidi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：attack method called adversarial neon beam, achieve better physical perturbation concealment, adversarial neon beam attack, achieve advanced attack effect, advanced physical attack methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the physical world, light affects the performance of deep neural networks.
Nowadays, many products based on deep neural network have been put into daily
life. There are few researches on the effect of light on the performance of
deep neural network models. However, the adversarial perturbations generated by
light may have extremely dangerous effects on these systems. In this work, we
propose an attack method called adversarial neon beam (AdvNB), which can
execute the physical attack by obtaining the physical parameters of adversarial
neon beams with very few queries. Experiments show that our algorithm can
achieve advanced attack effect in both digital test and physical test. In the
digital environment, 99.3% attack success rate was achieved, and in the
physical environment, 100% attack success rate was achieved. Compared with the
most advanced physical attack methods, our method can achieve better physical
perturbation concealment. In addition, by analyzing the experimental data, we
reveal some new phenomena brought about by the adversarial neon beam attack.</p>
  </details>
</details>
<details>
  <summary>88. <b>标题：Production of Categorical Data Verifying Differential Privacy:  Conception and Applications to Machine Learning</b></summary>
  <p><b>编号</b>：[324]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00850</p>
  <p><b>作者</b>：Héber H. Arcolezi</p>
  <p><b>备注</b>：Ph.D. Thesis defended in January 2022 at the University Bourgogne Franche-Comt\'e. Supervisor: Jean-Fran\c{c}ois Couchot</p>
  <p><b>关键词</b>：differentially private ml models achieve nearly, public organizations regularly collect, multiple collections throughout time, input data perturbation setting, e ., multiple attributes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Private and public organizations regularly collect and analyze digitalized
data about their associates, volunteers, clients, etc. However, because most
personal data are sensitive, there is a key challenge in designing
privacy-preserving systems. To tackle privacy concerns, research communities
have proposed different methods to preserve privacy, with Differential privacy
(DP) standing out as a formal definition that allows quantifying the
privacy-utility trade-off. Besides, with the local DP (LDP) model, users can
sanitize their data locally before transmitting it to the server. The objective
of this thesis is thus two-fold: O$_1$) To improve the utility and privacy in
multiple frequency estimates under LDP guarantees, which is fundamental to
statistical learning. And O$_2$) To assess the privacy-utility trade-off of
machine learning (ML) models trained over differentially private data. For
O$_1$, we first tackled the problem from two "multiple" perspectives, i.e.,
multiple attributes and multiple collections throughout time, while focusing on
utility. Secondly, we focused our attention on the multiple attributes aspect
only, in which we proposed a solution focusing on privacy while preserving
utility. In both cases, we demonstrate through analytical and experimental
validations the advantages of our proposed solutions over state-of-the-art LDP
protocols. For O$_2$, we empirically evaluated ML-based solutions designed to
solve real-world problems while ensuring DP guarantees. Indeed, we mainly used
the input data perturbation setting from the privacy-preserving ML literature.
This is the situation in which the whole dataset is sanitized independently
and, thus, we implemented LDP algorithms from the perspective of the
centralized data owner. In all cases, we concluded that differentially private
ML models achieve nearly the same utility metrics as non-private ones.</p>
  </details>
</details>
<details>
  <summary>89. <b>标题：Chordal Sparsity for Lipschitz Constant Estimation of Deep Neural  Networks</b></summary>
  <p><b>编号</b>：[326]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00846</p>
  <p><b>作者</b>：Anton Xue,  Lars Lindemann,  Alexander Robey,  Hamed Hassani,  George J. Pappas,  Rajeev Alur</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：gain tighter estimates without incurring, estimating lipschitz constants must navigate, semidefinite programming technique known, achieving zero accuracy loss, calculating lipschitz constants</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Lipschitz constants of neural networks allow for guarantees of robustness in
image classification, safety in controller design, and generalizability beyond
the training data. As calculating Lipschitz constants is NP-hard, techniques
for estimating Lipschitz constants must navigate the trade-off between
scalability and accuracy. In this work, we significantly push the scalability
frontier of a semidefinite programming technique known as LipSDP while
achieving zero accuracy loss. We first show that LipSDP has chordal sparsity,
which allows us to derive a chordally sparse formulation that we call
Chordal-LipSDP. The key benefit is that the main computational bottleneck of
LipSDP, a large semidefinite constraint, is now decomposed into an equivalent
collection of smaller ones: allowing Chordal-LipSDP to outperform LipSDP
particularly as the network depth grows. Moreover, our formulation uses a
tunable sparsity parameter that enables one to gain tighter estimates without
incurring a significant computational cost. We illustrate the scalability of
our approach through extensive numerical experiments.</p>
  </details>
</details>
<details>
  <summary>90. <b>标题：Intelligence at the Extreme Edge: A Survey on Reformable TinyML</b></summary>
  <p><b>编号</b>：[330]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00827</p>
  <p><b>作者</b>：Visal Rajapakse,  Ishan Karunanayake,  Nadeem Ahmed</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：upsurging research field proposes, scarcely available benchmarking tools, efficient pervasive devices capable, dubbed tiny machine learning, selected industrial areas</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rapid miniaturization of Machine Learning (ML) for low powered processing
has opened gateways to provide cognition at the extreme edge (E.g., sensors and
actuators). Dubbed Tiny Machine Learning (TinyML), this upsurging research
field proposes to democratize the use of Machine Learning (ML) and Deep
Learning (DL) on frugal Microcontroller Units (MCUs). MCUs are highly
energy-efficient pervasive devices capable of operating with less than a few
Milliwatts of power. Nevertheless, many solutions assume that TinyML can only
run inference. Despite this, growing interest in TinyML has led to work that
makes them reformable, i.e., work that permits TinyML to improve once deployed.
In line with this, roadblocks in MCU based solutions in general, such as
reduced physical access and long deployment periods of MCUs, deem reformable
TinyML to play a significant part in more effective solutions. In this work, we
present a survey on reformable TinyML solutions with the proposal of a novel
taxonomy for ease of separation. Here, we also discuss the suitability of each
hierarchical layer in the taxonomy for allowing reformability. In addition to
these, we explore the workflow of TinyML and analyze the identified deployment
schemes and the scarcely available benchmarking tools. Furthermore, we discuss
how reformable TinyML can impact a few selected industrial areas and discuss
the challenges and future directions.</p>
  </details>
</details>
<details>
  <summary>91. <b>标题：AdaSmooth: An Adaptive Learning Rate Method based on Effective Ratio</b></summary>
  <p><b>编号</b>：[332]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00825</p>
  <p><b>作者</b>：Jun Lu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：show promising results compared, gradient descent called adasmooth, alternative machine learning tasks, dimension learning rate method, different convolutional neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>It is well known that we need to choose the hyper-parameters in Momentum,
AdaGrad, AdaDelta, and other alternative stochastic optimizers. While in many
cases, the hyper-parameters are tuned tediously based on experience becoming
more of an art than science. We present a novel per-dimension learning rate
method for gradient descent called AdaSmooth. The method is insensitive to
hyper-parameters thus it requires no manual tuning of the hyper-parameters like
Momentum, AdaGrad, and AdaDelta methods. We show promising results compared to
other methods on different convolutional neural networks, multi-layer
perceptron, and alternative machine learning tasks. Empirical results
demonstrate that AdaSmooth works well in practice and compares favorably to
other stochastic optimization methods in neural networks.</p>
  </details>
</details>
<details>
  <summary>92. <b>标题：Efficient comparison of sentence embeddings</b></summary>
  <p><b>编号</b>：[337]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00820</p>
  <p><b>作者</b>：Spyros Zoupanos,  Stratis Kolovos,  Athanasios Kanavos,  Orestis Papadimitriou,  Manolis Maragoudakis</p>
  <p><b>备注</b>：8 pages, 2 figures</p>
  <p><b>关键词</b>：problem transformation raises new challenges like, two vector comparison approaches, like semantic similarity, perform vector comparisons, natural language processing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The domain of natural language processing (NLP), which has greatly evolved
over the last years, has highly benefited from the recent developments in word
and sentence embeddings. Such embeddings enable the transformation of complex
NLP tasks, like semantic similarity or Question and Answering (Q\&A), into much
simpler to perform vector comparisons. However, such a problem transformation
raises new challenges like the efficient comparison of embeddings and their
manipulation. In this work, we will discuss about various word and sentence
embeddings algorithms, we will select a sentence embedding algorithm, BERT, as
our algorithm of choice and we will evaluate the performance of two vector
comparison approaches, FAISS and Elasticsearch, in the specific problem of
sentence embeddings. According to the results, FAISS outperforms Elasticsearch
when used in a centralized environment with only one node, especially when big
datasets are included.</p>
  </details>
</details>
<details>
  <summary>93. <b>标题：HLDC: Hindi Legal Documents Corpus</b></summary>
  <p><b>编号</b>：[341]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00806</p>
  <p><b>作者</b>：Arnav Kapoor,  Mudit Dhawan,  Anmol Goel,  T.H. Arjun,  Akshala Bhatnagar,  Vibhu Agrawal,  Amul Agrawal,  Arnab Bhattacharya,  Ponnurangam Kumaraguru,  Ashutosh Modi</p>
  <p><b>备注</b>：16 Pages, Accepted at ACL 2022 Findings</p>
  <p><b>关键词</b>：many populous countries including india, could process legal documents, mtl models use summarization, hindi legal documents corpus, augment legal practitioners</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Many populous countries including India are burdened with a considerable
backlog of legal cases. Development of automated systems that could process
legal documents and augment legal practitioners can mitigate this. However,
there is a dearth of high-quality corpora that is needed to develop such
data-driven systems. The problem gets even more pronounced in the case of low
resource languages such as Hindi. In this resource paper, we introduce the
Hindi Legal Documents Corpus (HLDC), a corpus of more than 900K legal documents
in Hindi. Documents are cleaned and structured to enable the development of
downstream applications. Further, as a use-case for the corpus, we introduce
the task of bail prediction. We experiment with a battery of models and propose
a Multi-Task Learning (MTL) based model for the same. MTL models use
summarization as an auxiliary task along with bail prediction as the main task.
Experiments with different models are indicative of the need for further
research in this area. We release the corpus and model implementation code with
this paper: this https URL</p>
  </details>
</details>
<details>
  <summary>94. <b>标题：Paoding: Supervised Robustness-preserving Data-free Neural Network  Pruning</b></summary>
  <p><b>编号</b>：[352]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00783</p>
  <p><b>作者</b>：Mark Huasong Meng,  Guangdong Bai,  Sin Gee Teo,  Jin Song Dong</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：model consumers often encounter resource, trained neural network models, diverse neural network models, significantly outperforms existing one, less resource consumption</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>When deploying pre-trained neural network models in real-world applications,
model consumers often encounter resource-constraint platforms such as mobile
and smart devices. They typically use the pruning technique to reduce the size
and complexity of the model, generating a lighter one with less resource
consumption. Nonetheless, most existing pruning methods are proposed with a
premise that the model after being pruned has a chance to be fine-tuned or even
retrained based on the original training data. This may be unrealistic in
practice, as the data controllers are often reluctant to provide their model
consumers with the original data. In this work, we study the neural network
pruning in the \emph{data-free} context, aiming to yield lightweight models
that are not only accurate in prediction but also robust against undesired
inputs in open-world deployments. Considering the absence of the fine-tuning
and retraining that can fix the mis-pruned units, we replace the traditional
aggressive one-shot strategy with a conservative one that treats the pruning as
a progressive process. We propose a pruning method based on stochastic
optimization that uses robustness-related metrics to guide the pruning process.
Our method is implemented as a Python package named \textsc{Paoding} and
evaluated with a series of experiments on diverse neural network models. The
experimental results show that it significantly outperforms existing one-shot
data-free pruning approaches in terms of robustness preservation and accuracy.</p>
  </details>
</details>
<details>
  <summary>95. <b>标题：Revealing the real-world CO2 emission reduction of ridesplitting and its  determinants based on machine learning</b></summary>
  <p><b>编号</b>：[353]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00777</p>
  <p><b>作者</b>：Wenxiang Li,  Yuanyuan Li,  Ziyuan Pu,  Long Cheng,  Lei Wang,  Linchuan Yang</p>
  <p><b>备注</b>：33 pages, 12 figures</p>
  <p><b>关键词</b>：shapley additive explanations method, interpretable machine learning models, co2 emission reduction rate, co2 emission reduction rate, co2 emission reduction rate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Ridesplitting, which is a form of pooled ridesourcing service, has great
potential to alleviate the negative impacts of ridesourcing on the environment.
However, most existing studies only explored its theoretical environmental
benefits based on optimization models and simulations. To put into practice,
this study aims to reveal the real-world emission reduction of ridesplitting
and its determinants based on the observed data of ridesourcing in Chengdu,
China. Integrating the trip data with the COPERT model, this study calculates
the CO2 emissions of shared rides (ridesplitting) and their substituted single
rides (regular ridesourcing) to estimate the CO2 emission reduction of each
ridesplitting trip. The results show that not all ridesplitting trips reduce
emissions from ridesourcing in the real world. The CO2 emission reduction rate
of ridesplitting varies from trip to trip, averaging at 43.15g/km. Then, the
interpretable machine learning models, gradient boosting machines, are applied
to explore the relationship between the CO2 emission reduction rate of
ridesplitting and its determinants. Based on the SHapley Additive exPlanations
method, the overlap rate and detour rate of shared rides are identified to be
the most important factors that determine the CO2 emission reduction rate of
ridesplitting. Increasing the overlap rate, the number of shared rides, average
speed, and ride distance ratio and decreasing the detour rate, actual trip
distance, ride distance gap can increase the CO2 emission reduction rate of
ridesplitting. In addition, nonlinear effects and interactions of several key
factors are examined through the partial dependence plots. This study provides
a scientific method for the government and ridesourcing companies to better
assess and optimize the environmental benefits of ridesplitting.</p>
  </details>
</details>
<details>
  <summary>96. <b>标题：Speaker adaptation for Wav2vec2 based dysarthric ASR</b></summary>
  <p><b>编号</b>：[356]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00770</p>
  <p><b>作者</b>：Murali Karthick Baskar,  Tim Herzig,  Diana Nguyen,  Mireia Diez,  Tim Polzehl,  Lukáš Burget,  Jan "Honza'' Černocký</p>
  <p><b>备注</b>：Submitted to INTERSPEECH 2022</p>
  <p><b>关键词</b>：experimental analysis show steady improvements using, proposed approach across diverse domains, tuning wav2vec2 using fmllr features, readily available pretrained models, posed major challenges due</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Dysarthric speech recognition has posed major challenges due to lack of
training data and heavy mismatch in speaker characteristics. Recent ASR systems
have benefited from readily available pretrained models such as wav2vec2 to
improve the recognition performance. Speaker adaptation using fMLLR and
xvectors have provided major gains for dysarthric speech with very little
adaptation data. However, integration of wav2vec2 with fMLLR features or
xvectors during wav2vec2 finetuning is yet to be explored. In this work, we
propose a simple adaptation network for fine-tuning wav2vec2 using fMLLR
features. The adaptation network is also flexible to handle other speaker
adaptive features such as xvectors. Experimental analysis show steady
improvements using our proposed approach across all impairment severity levels
and attains 57.72\% WER for high severity in UASpeech dataset. We also
performed experiments on German dataset to substantiate the consistency of our
proposed approach across diverse domains.</p>
  </details>
</details>
<details>
  <summary>97. <b>标题：Modeling Dynamic User Preference via Dictionary Learning for Sequential  Recommendation</b></summary>
  <p><b>编号</b>：[363]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00752</p>
  <p><b>作者</b>：Chao Chen,  Dongsheng Li,  Junchi Yan,  Xiaokang Yang</p>
  <p><b>备注</b>：13 pages, 15 figures, TKDE 2021</p>
  <p><b>关键词</b>：many existing recommendation algorithms -- including, user dynamic preferences shared across users, deep ones -- often model, better predict user future behaviors, deep autoregressive model integrated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Capturing the dynamics in user preference is crucial to better predict user
future behaviors because user preferences often drift over time. Many existing
recommendation algorithms -- including both shallow and deep ones -- often
model such dynamics independently, i.e., user static and dynamic preferences
are not modeled under the same latent space, which makes it difficult to fuse
them for recommendation. This paper considers the problem of embedding a user's
sequential behavior into the latent space of user preferences, namely
translating sequence to preference. To this end, we formulate the sequential
recommendation task as a dictionary learning problem, which learns: 1) a shared
dictionary matrix, each row of which represents a partial signal of user
dynamic preferences shared across users; and 2) a posterior distribution
estimator using a deep autoregressive model integrated with Gated Recurrent
Unit (GRU), which can select related rows of the dictionary to represent a
user's dynamic preferences conditioned on his/her past behaviors. Qualitative
studies on the Netflix dataset demonstrate that the proposed method can capture
the user preference drifts over time and quantitative studies on multiple
real-world datasets demonstrate that the proposed method can achieve higher
accuracy compared with state-of-the-art factorization and neural sequential
recommendation methods. The code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>98. <b>标题：Path Development Network with Finite-dimensional Lie Group  Representation</b></summary>
  <p><b>编号</b>：[369]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00740</p>
  <p><b>作者</b>：Hang Lou,  Siran Li,  Hao Ni</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：suitable matrix lie group, resulting hybrid model achieves, dimensional matrix lie groups, various sequential data tasks, trainable path development layer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The path signature, a mathematically principled and universal feature of
sequential data, leads to a performance boost of deep learning-based models in
various sequential data tasks as a complimentary feature. However, it suffers
from the curse of dimensionality when the path dimension is high. To tackle
this problem, we propose a novel, trainable path development layer, which
exploits representations of sequential data with the help of finite-dimensional
matrix Lie groups. We also design the backpropagation algorithm of the
development layer via an optimisation method on manifolds known as
trivialisation. Numerical experiments demonstrate that the path development
consistently and significantly outperforms, in terms of accuracy and
dimensionality, signature features on several empirical datasets. Moreover,
stacking the LSTM with the development layer with a suitable matrix Lie group
is empirically proven to alleviate the gradient issues of LSTMs and the
resulting hybrid model achieves the state-of-the-art performance.</p>
  </details>
</details>
<details>
  <summary>99. <b>标题：SkeleVision: Towards Adversarial Resiliency of Person Tracking with  Multi-Task Learning</b></summary>
  <p><b>编号</b>：[370]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00734</p>
  <p><b>作者</b>：Nilaksh Das,  Sheng-Yun Peng,  Duen Horng Chau</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：adversarial attacks raises serious concerns regarding, person tracking using computer vision techniques, widely used siamrpn tracker, powerful adversarial attacks, world datasets reveals</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Person tracking using computer vision techniques has wide ranging
applications such as autonomous driving, home security and sports analytics.
However, the growing threat of adversarial attacks raises serious concerns
regarding the security and reliability of such techniques. In this work, we
study the impact of multi-task learning (MTL) on the adversarial robustness of
the widely used SiamRPN tracker, in the context of person tracking.
Specifically, we investigate the effect of jointly learning with semantically
analogous tasks of person tracking and human keypoint detection. We conduct
extensive experiments with more powerful adversarial attacks that can be
physically realizable, demonstrating the practical value of our approach. Our
empirical study with simulated as well as real-world datasets reveals that
training with MTL consistently makes it harder to attack the SiamRPN tracker,
compared to typically training only on the single task of person tracking.</p>
  </details>
</details>
<details>
  <summary>100. <b>标题：Analysis of Sparse Subspace Clustering: Experiments and Random  Projection</b></summary>
  <p><b>编号</b>：[371]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00723</p>
  <p><b>作者</b>：Mehmet F. Demirel,  Enrico Au-Yeung</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：powerful clustering algorithm called sparse subspace clustering, perform sparse subspace clustering, important unsupervised learning problems, demonstrate several experiments using, groups whose elements</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Clustering can be defined as the process of assembling objects into a number
of groups whose elements are similar to each other in some manner. As a
technique that is used in many domains, such as face clustering, plant
categorization, image segmentation, document classification, clustering is
considered one of the most important unsupervised learning problems. Scientists
have surveyed this problem for years and developed different techniques that
can solve it, such as k-means clustering. We analyze one of these techniques: a
powerful clustering algorithm called Sparse Subspace Clustering. We demonstrate
several experiments using this method and then introduce a new approach that
can reduce the computational time required to perform sparse subspace
clustering.</p>
  </details>
</details>
<details>
  <summary>101. <b>标题：Strategies for Safe Multi-Armed Bandits with Logarithmic Regret and Risk</b></summary>
  <p><b>编号</b>：[381]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00706</p>
  <p><b>作者</b>：Tianrui Chen,  Aditya Gangrade,  Venkatesh Saligrama</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：bayesian indices satisfy tight gap, describe doubly optimistic strategies, dependent logarithmic regret bounds, one must maintain safety, maintain optimistic indices</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We investigate a natural but surprisingly unstudied approach to the
multi-armed bandit problem under safety risk constraints. Each arm is
associated with an unknown law on safety risks and rewards, and the learner's
goal is to maximise reward whilst not playing unsafe arms, as determined by a
given threshold on the mean risk.
We formulate a pseudo-regret for this setting that enforces this safety
constraint in a per-round way by softly penalising any violation, regardless of
the gain in reward due to the same. This has practical relevance to scenarios
such as clinical trials, where one must maintain safety for each round rather
than in an aggregated sense.
We describe doubly optimistic strategies for this scenario, which maintain
optimistic indices for both safety risk and reward. We show that schema based
on both frequentist and Bayesian indices satisfy tight gap-dependent
logarithmic regret bounds, and further that these play unsafe arms only
logarithmically many times in total. This theoretical analysis is complemented
by simulation studies demonstrating the effectiveness of the proposed schema,
and probing the domains in which their use is appropriate.</p>
  </details>
</details>
<details>
  <summary>102. <b>标题：A Reinforcement Learning Approach to Sensing Design in  Resource-Constrained Wireless Networked Control Systems</b></summary>
  <p><b>编号</b>：[382]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00703</p>
  <p><b>作者</b>：Luca Ballotta,  Giovanni Peserico,  Francesco Zanini</p>
  <p><b>备注</b>：8 pages, 4 figures, submitted to CDC 2022; fixed author names</p>
  <p><b>关键词</b>：constrained platforms generates accurate measurements, constrained agent resources raise, either send raw measurements, performs global monitoring, wireless communication might</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we consider a wireless network of smart sensors (agents) that
monitor a dynamical process and send measurements to a base station that
performs global monitoring and decision-making. Smart sensors are equipped with
both sensing and computation, and can either send raw measurements or process
them prior to transmission. Constrained agent resources raise a fundamental
latency-accuracy trade-off. On the one hand, raw measurements are inaccurate
but fast to produce. On the other hand, data processing on resource-constrained
platforms generates accurate measurements at the cost of non-negligible
computation latency. Further, if processed data are also compressed, latency
caused by wireless communication might be higher for raw measurements. Hence,
it is challenging to decide when and where sensors in the network should
transmit raw measurements or leverage time-consuming local processing. To
tackle this design problem, we propose a Reinforcement Learning approach to
learn an efficient policy that dynamically decides when measurements are to be
processed at each sensor. Effectiveness of our proposed approach is validated
through a numerical simulation with case study on smart sensing motivated by
the Internet of Drones.</p>
  </details>
</details>
<details>
  <summary>103. <b>标题：Testing Feedforward Neural Networks Training Programs</b></summary>
  <p><b>编号</b>：[385]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00694</p>
  <p><b>作者</b>：Houssem Ben Braiek,  Foutse Khomh</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：novel problem requires significant engineering work, current automated test data generators search, revealing several coding bugs, propose practical verification routines, world buggy dl programs</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Nowadays, we are witnessing an increasing effort to improve the performance
and trustworthiness of Deep Neural Networks (DNNs), with the aim to enable
their adoption in safety critical systems such as self-driving cars. Multiple
testing techniques are proposed to generate test cases that can expose
inconsistencies in the behavior of DNN models. These techniques assume
implicitly that the training program is bug-free and appropriately configured.
However, satisfying this assumption for a novel problem requires significant
engineering work to prepare the data, design the DNN, implement the training
program, and tune the hyperparameters in order to produce the model for which
current automated test data generators search for corner-case behaviors. All
these model training steps can be error-prone. Therefore, it is crucial to
detect and correct errors throughout all the engineering steps of DNN-based
software systems and not only on the resulting DNN model. In this paper, we
gather a catalog of training issues and based on their symptoms and their
effects on the behavior of the training program, we propose practical
verification routines to detect the aforementioned issues, automatically, by
continuously validating that some important properties of the learning dynamics
hold during the training. Then, we design, TheDeepChecker, an end-to-end
property-based debugging approach for DNN training programs. We assess the
effectiveness of TheDeepChecker on synthetic and real-world buggy DL programs
and compare it with Amazon SageMaker Debugger (SMD). Results show that
TheDeepChecker's on-execution validation of DNN-based program's properties
succeeds in revealing several coding bugs and system misconfigurations, early
on and at a low cost. Moreover, TheDeepChecker outperforms the SMD's offline
rules verification on training logs in terms of detection accuracy and DL bugs
coverage.</p>
  </details>
</details>
<details>
  <summary>104. <b>标题：Assimilation of Satellite Active Fires Data</b></summary>
  <p><b>编号</b>：[388]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00686</p>
  <p><b>作者</b>：James D. Haley</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：data assimilation improves improves fire modeling capabilities, ancillary effects like reduced air quality, knowledge using mathematically sound methods, improving wildfire modeling capabilities, way towards new avenues</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Wildland fires pose an increasingly serious problem in our society. The
number and severity of these fires has been rising for many years. Wildfires
pose direct threats to life and property as well as threats through ancillary
effects like reduced air quality. The aim of this thesis is to develop
techniques to help combat the impacts of wildfires by improving wildfire
modeling capabilities by using satellite fire observations. Already much work
has been done in this direction by other researchers. Our work seeks to expand
the body of knowledge using mathematically sound methods to utilize information
about wildfires that considers the uncertainties inherent in the satellite
data.
In this thesis we explore methods for using satellite data to help initialize
and steer wildfire simulations. In particular, we develop a method for
constructing the history of a fire, a new technique for assimilating wildfire
data, and a method for modifying the behavior of a modeled fire by inferring
information about the fuels in the fire domain. These goals rely on being able
to estimate the time a fire first arrived at every location in a geographic
region of interest. Because detailed knowledge of real wildfires is typically
unavailable, the basic procedure for developing and testing the methods in this
thesis will be to first work with simulated data so that the estimates produced
can be compared with known solutions. The methods thus developed are then
applied to real-world scenarios. Analysis of these scenarios shows that the
work with constructing the history of fires and data assimilation improves
improves fire modeling capabilities. The research is significant because it
gives us a better understanding of the capabilities and limitations of using
satellite data to inform wildfire models and it points the way towards new
avenues for modeling fire behavior.</p>
  </details>
</details>
<details>
  <summary>105. <b>标题：Learnable latent embeddings for joint behavioral and neural analysis</b></summary>
  <p><b>编号</b>：[393]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00673</p>
  <p><b>作者</b>：Steffen Schneider,  Jin Hwa Lee,  Mackenzie Weygandt Mathis</p>
  <p><b>备注</b>：Website: cebra.ai</p>
  <p><b>关键词</b>：uncovering complex kinematic features, flexibly leverage joint behavior, complex behaviors across species, reveal underlying correlates, performance latent spaces</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Mapping behavioral actions to neural activity is a fundamental goal of
neuroscience. As our ability to record large neural and behavioral data
increases, there is growing interest in modeling neural dynamics during
adaptive behaviors to probe neural representations. In particular, neural
latent embeddings can reveal underlying correlates of behavior, yet, we lack
non-linear techniques that can explicitly and flexibly leverage joint behavior
and neural data. Here, we fill this gap with a novel method, CEBRA, that
jointly uses behavioral and neural data in a hypothesis- or discovery-driven
manner to produce consistent, high-performance latent spaces. We validate its
accuracy and demonstrate our tool's utility for both calcium and
electrophysiology datasets, across sensory and motor tasks, and in simple or
complex behaviors across species. It allows for single and multi-session
datasets to be leveraged for hypothesis testing or can be used label-free.
Lastly, we show that CEBRA can be used for the mapping of space, uncovering
complex kinematic features, and rapid, high-accuracy decoding of natural movies
from visual cortex.</p>
  </details>
</details>
<details>
  <summary>106. <b>标题：Hysteresis-Based RL: Robustifying Reinforcement Learning-based Control  Policies via Hybrid Control</b></summary>
  <p><b>编号</b>：[398]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00654</p>
  <p><b>作者</b>：Jan de Priester,  Ricardo G. Sanfelice,  Nathan van de Wouw</p>
  <p><b>备注</b>：This paper has been accepted for publication at the 2022 American Control Conference (ACC)</p>
  <p><b>关键词</b>：algorithms may lack robustness guarantees, proximal policy optimization, new hybrid algorithm, hyrl ), augmenting, deriving control policies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reinforcement learning (RL) is a promising approach for deriving control
policies for complex systems. As we show in two control problems, the derived
policies from using the Proximal Policy Optimization (PPO) and Deep Q-Network
(DQN) algorithms may lack robustness guarantees. Motivated by these issues, we
propose a new hybrid algorithm, which we call Hysteresis-Based RL (HyRL),
augmenting an existing RL algorithm with hysteresis switching and two stages of
learning. We illustrate its properties in two examples for which PPO and DQN
fail.</p>
  </details>
</details>
<details>
  <summary>107. <b>标题：Knowledge distillation with error-correcting transfer learning for wind  power prediction</b></summary>
  <p><b>编号</b>：[402]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00649</p>
  <p><b>作者</b>：Hao Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：hybrid methodologies combining advanced data science, five turbines featuring various terrains, individually modeling massive turbines, scale weather forecasts non, transfer learning parameters tuning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Wind power prediction, especially for turbines, is vital for the operation,
controllability, and economy of electricity companies. Hybrid methodologies
combining advanced data science with weather forecasting have been
incrementally applied to the predictions. Nevertheless, individually modeling
massive turbines from scratch and downscaling weather forecasts to turbine size
are neither easy nor economical. Aiming at it, this paper proposes a novel
framework with mathematical underpinnings for turbine power prediction. This
framework is the first time to incorporate knowledge distillation into energy
forecasting, enabling accurate and economical constructions of turbine models
by learning knowledge from the well-established park model. Besides, park-scale
weather forecasts non-explicitly are mapped to turbines by transfer learning of
predicted power errors, achieving model correction for better performance. The
proposed framework is deployed on five turbines featuring various terrains in
an Arctic wind park, the results are evaluated against the competitors of
ablation investigation. The major findings reveal that the proposed framework,
developed on favorable knowledge distillation and transfer learning parameters
tuning, yields performance boosts from 3.3 % to 23.9 % over its competitors.
This advantage also exists in terms of wind energy physics and computing
efficiency, which are verified by the prediction quality rate and calculation
time.</p>
  </details>
</details>
<details>
  <summary>108. <b>标题：Cluster-based ensemble learning for wind power modeling with  meteorological wind data</b></summary>
  <p><b>编号</b>：[403]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00646</p>
  <p><b>作者</b>：Hao Chen</p>
  <p><b>备注</b>：UNDER REVIEW Renewable & Sustainable Energy Reviews</p>
  <p><b>关键词</b>：proposed modeling framework thus demonstrates promise, thus outperform models without, orderly integrates three types, wind energy generation hinge, best farthest first clustering</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Optimal implementation and monitoring of wind energy generation hinge on
reliable power modeling that is vital for understanding turbine control, farm
operational optimization, and grid load balance. Based on the idea of similar
wind condition leads to similar wind power; this paper constructs a modeling
scheme that orderly integrates three types of ensemble learning algorithms,
bagging, boosting, and stacking, and clustering approaches to achieve optimal
power modeling. It also investigates applications of different clustering
algorithms and methodology for determining cluster numbers in wind power
modeling. The results reveal that all ensemble models with clustering exploit
the intrinsic information of wind data and thus outperform models without it by
approximately 15% on average. The model with the best farthest first clustering
is computationally rapid and performs exceptionally well with an improvement of
around 30%. The modeling is further boosted by about 5% by introducing stacking
that fuses ensembles with varying clusters. The proposed modeling framework
thus demonstrates promise by delivering efficient and robust modeling
performance.</p>
  </details>
</details>
<details>
  <summary>109. <b>标题：SIMBAR: Single Image-Based Scene Relighting For Effective Data  Augmentation For Automated Driving Vision Tasks</b></summary>
  <p><b>编号</b>：[405]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00644</p>
  <p><b>作者</b>：Xianling Zhang,  Nathan Tseng,  Ameerah Syed,  Rohan Bhasin,  Nikita Jaipuria</p>
  <p><b>备注</b>：Accepted to CVPR 2022. Project page: this https URL</p>
  <p><b>关键词</b>：scene relighting leveraging explicit geometric representations, world autonomous driving datasets comprise, view scene relighting baselines, automated driving vision tasks, multiple object tracking accuracy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Real-world autonomous driving datasets comprise of images aggregated from
different drives on the road. The ability to relight captured scenes to unseen
lighting conditions, in a controllable manner, presents an opportunity to
augment datasets with a richer variety of lighting conditions, similar to what
would be encountered in the real-world. This paper presents a novel image-based
relighting pipeline, SIMBAR, that can work with a single image as input. To the
best of our knowledge, there is no prior work on scene relighting leveraging
explicit geometric representations from a single image. We present qualitative
comparisons with prior multi-view scene relighting baselines. To further
validate and effectively quantify the benefit of leveraging SIMBAR for data
augmentation for automated driving vision tasks, object detection and tracking
experiments are conducted with a state-of-the-art method, a Multiple Object
Tracking Accuracy (MOTA) of 93.3% is achieved with CenterTrack on
SIMBAR-augmented KITTI - an impressive 9.0% relative improvement over the
baseline MOTA of 85.6% with CenterTrack on original KITTI, both models trained
from scratch and tested on Virtual KITTI. For more details and SIMBAR relit
datasets, please visit our project website (this https URL).</p>
  </details>
</details>
<details>
  <summary>110. <b>标题：Application of Dimensional Reduction in Artificial Neural Networks to  Improve Emergency Department Triage During Chemical Mass Casualty Incidents</b></summary>
  <p><b>编号</b>：[406]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00642</p>
  <p><b>作者</b>：Nicholas D. Boltin,  Joan M. Culley,  Homayoun Valafar</p>
  <p><b>备注</b>：8 Pages to be submitted to CSCE-HIMS 2022</p>
  <p><b>关键词</b>：nearly 40 ssx without losing significant model accuracy, improve ann model performance accuracy, four statistical dimension reduction techniques, ml models require large volumes, provide efficient decision support</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Chemical Mass Casualty Incidents (MCI) place a heavy burden on hospital staff
and resources. Machine Learning (ML) tools can provide efficient decision
support to caregivers. However, ML models require large volumes of data for the
most accurate results, which is typically not feasible in the chaotic nature of
a chemical MCI. This study examines the application of four statistical
dimension reduction techniques: Random Selection, Covariance/Variance,
Pearson's Linear Correlation, and Principle Component Analysis to reduce a
dataset of 311 hazardous chemicals and 79 related signs and symptoms (SSx). An
Artificial Neural Network pipeline was developed to create comparative models.
Results show that the number of signs and symptoms needed to determine a
chemical culprit can be reduced to nearly 40 SSx without losing significant
model accuracy. Evidence also suggests that the application of dimension
reduction methods can improve ANN model performance accuracy.</p>
  </details>
</details>
<details>
  <summary>111. <b>标题：Learning Neural Acoustic Fields</b></summary>
  <p><b>编号</b>：[407]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00628</p>
  <p><b>作者</b>：Andrew Luo,  Yilun Du,  Michael J. Tarr,  Joshua B. Tenenbaum,  Antonio Torralba,  Chuang Gan</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：neural impulse response function, increasingly higher quality representations, object moves around us, introduce neural acoustic fields, learning spatial auditory representations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Our environment is filled with rich and dynamic acoustic information. When we
walk into a cathedral, the reverberations as much as appearance inform us of
the sanctuary's wide open space. Similarly, as an object moves around us, we
expect the sound emitted to also exhibit this movement. While recent advances
in learned implicit functions have led to increasingly higher quality
representations of the visual world, there have not been commensurate advances
in learning spatial auditory representations. To address this gap, we introduce
Neural Acoustic Fields (NAFs), an implicit representation that captures how
sounds propagate in a physical scene. By modeling acoustic propagation in a
scene as a linear time-invariant system, NAFs learn to continuously map all
emitter and listener location pairs to a neural impulse response function that
can then be applied to arbitrary sounds. We demonstrate that the continuous
nature of NAFs enables us to render spatial acoustics for a listener at an
arbitrary location, and can predict sound propagation at novel locations. We
further show that the representation learned by NAFs can help improve visual
learning with sparse views. Finally, we show that a representation informative
of scene structure emerges during the learning of NAFs.</p>
  </details>
</details>
<details>
  <summary>112. <b>标题：Explainable and Interpretable Diabetic Retinopathy Classification Based  on Neural-Symbolic Learning</b></summary>
  <p><b>编号</b>：[409]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00624</p>
  <p><b>作者</b>：Se-In Jang,  Michael J.A. Girard,  Alexandre H. Thiery</p>
  <p><b>备注</b>：Published in AAAI-22 Workshop</p>
  <p><b>关键词</b>：proposed explaindr method exhibits promising performance, diabetic retinopathy classification dataset show, include humanreadable features obtained, diabetic retinopathy characteristics related, interpretable diabetic retinopathy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we propose an explainable and interpretable diabetic
retinopathy (ExplainDR) classification model based on neural-symbolic learning.
To gain explainability, a highlevel symbolic representation should be
considered in decision making. Specifically, we introduce a human-readable
symbolic representation, which follows a taxonomy style of diabetic retinopathy
characteristics related to eye health conditions to achieve explainability. We
then include humanreadable features obtained from the symbolic representation
in the disease prediction. Experimental results on a diabetic retinopathy
classification dataset show that our proposed ExplainDR method exhibits
promising performance when compared to that from state-of-the-art methods
applied to the IDRiD dataset, while also providing interpretability and
explainability.</p>
  </details>
</details>
<details>
  <summary>113. <b>标题：CogNGen: Constructing the Kernel of a Hyperdimensional Predictive  Processing Cognitive Architecture</b></summary>
  <p><b>编号</b>：[410]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00619</p>
  <p><b>作者</b>：Alexander Ororbia,  M. Alex Kelly</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：future work includes testing cogngen, efficiently scaling hyperdimensional memory models, combines two neurobiologically plausible, modern machine learning techniques, cognitive neural generative system</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a new cognitive architecture that combines two neurobiologically
plausible, computational models: (1) a variant of predictive processing known
as neural generative coding (NGC) and (2) hyperdimensional, vector-symbolic
models of human memory. We draw inspiration from well-known cognitive
architectures such as ACT-R, Soar, Leabra, and Spaun/Nengo. Our cognitive
architecture, the COGnitive Neural GENerative system (CogNGen), is in broad
agreement with these architectures, but provides a level of detail between
ACT-R's high-level, symbolic description of human cognition and Spaun's
low-level neurobiological description. CogNGen creates the groundwork for
developing agents that learn continually from diverse tasks and model human
performance at larger scales than what is possible with existent cognitive
architectures. We aim to develop a cognitive architecture that has the power of
modern machine learning techniques while retaining long-term memory,
single-trial learning, transfer-learning, planning, and other capacities
associated with high-level cognition. We test CogNGen on a set of maze-learning
tasks, including mazes that test short-term memory and planning, and find that
the addition of vector-symbolic models of memory improves the ability of the
NGC reinforcement learning model to master the maze task. Future work includes
testing CogNGen on more tasks and exploring methods for efficiently scaling
hyperdimensional memory models to lifetime learning.</p>
  </details>
</details>
<details>
  <summary>114. <b>标题：Deep Feature Screening: Feature Selection for Ultra High-Dimensional  Data via Deep Neural Networks</b></summary>
  <p><b>编号</b>：[411]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01682</p>
  <p><b>作者</b>：Kexuan Li,  Fangfang Wang,  Lingli Yang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：step nonparametric approach called deep feature screening, multivariate rank distance correlation recently developed, demonstrated via extensive simulation studies, traditional statistical feature selection methods, applies feature screening based</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The applications of traditional statistical feature selection methods to
high-dimension, low sample-size data often struggle and encounter challenging
problems, such as overfitting, curse of dimensionality, computational
infeasibility, and strong model assumption. In this paper, we propose a novel
two-step nonparametric approach called Deep Feature Screening (DeepFS) that can
overcome these problems and identify significant features with high precision
for ultra high-dimensional, low-sample-size data. This approach first extracts
a low-dimensional representation of input data and then applies feature
screening based on multivariate rank distance correlation recently developed by
Deb and Sen (2021). This approach combines the strengths of both deep neural
networks and feature screening, and thereby has the following appealing
features in addition to its ability of handling ultra high-dimensional data
with small number of samples: (1) it is model free and distribution free; (2)
it can be used for both supervised and unsupervised feature selection; and (3)
it is capable of recovering the original input data. The superiority of DeepFS
is demonstrated via extensive simulation studies and real data analyses.</p>
  </details>
</details>
<details>
  <summary>115. <b>标题：End-to-end multi-particle reconstruction in high occupancy imaging  calorimeters with graph neural networks</b></summary>
  <p><b>编号</b>：[412]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01681</p>
  <p><b>作者</b>：Shah Rukh Qasim,  Nadezda Chernyavskaya,  Jan Kieseler,  Kenneth Long,  Oleksandr Viazlo,  Maurizio Pierini,  Raheel Nawaz</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：}( 1000 )$ particles, generation granular calorimeters similar, weighted graph neural network, graph segmentation technique, inference computational cost</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present an end-to-end reconstruction algorithm to build particle
candidates from detector hits in next-generation granular calorimeters similar
to that foreseen for the high-luminosity upgrade of the CMS detector. The
algorithm exploits a distance-weighted graph neural network, trained with
object condensation, a graph segmentation technique. Through a single-shot
approach, the reconstruction task is paired with energy regression. We describe
the reconstruction performance in terms of efficiency as well as in terms of
energy resolution. In addition, we show the jet reconstruction performance of
our method and discuss its inference computational cost. This work is the
first-ever example of single-shot calorimetric reconstruction of ${\cal
O}(1000)$ particles in high-luminosity conditions with 200 pileup to our
knowledge.</p>
  </details>
</details>
<details>
  <summary>116. <b>标题：Scalable Spike-and-Slab</b></summary>
  <p><b>编号</b>：[413]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01668</p>
  <p><b>作者</b>：Niloy Biswas,  Lester Mackey,  Xiao-Li Meng</p>
  <p><b>备注</b>：26 pages, 5 figures</p>
  <p><b>关键词</b>：slab posteriors incur prohibitive computational costs, order $\ max \{ n, np \}$ computational cost, scalable gibbs sampling implementation, favorable statistical properties</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Spike-and-slab priors are commonly used for Bayesian variable selection, due
to their interpretability and favorable statistical properties. However,
existing samplers for spike-and-slab posteriors incur prohibitive computational
costs when the number of variables is large. In this article, we propose
Scalable Spike-and-Slab ($S^3$), a scalable Gibbs sampling implementation for
high-dimensional Bayesian regression with the continuous spike-and-slab prior
of George and McCulloch (1993). For a dataset with $n$ observations and $p$
covariates, $S^3$ has order $\max\{ n^2 p_t, np \}$ computational cost at
iteration $t$ where $p_t$ never exceeds the number of covariates switching
spike-and-slab states between iterations $t$ and $t-1$ of the Markov chain.
This improves upon the order $n^2 p$ per-iteration cost of state-of-the-art
implementations as, typically, $p_t$ is substantially smaller than $p$. We
apply $S^3$ on synthetic and real-world datasets, demonstrating orders of
magnitude speed-ups over existing exact samplers and significant gains in
inferential quality over approximate samplers with comparable cost.</p>
  </details>
</details>
<details>
  <summary>117. <b>标题：Optimize Deep Learning Models for Prediction of Gene Mutations Using  Unsupervised Clustering</b></summary>
  <p><b>编号</b>：[415]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01593</p>
  <p><b>作者</b>：Zihan Chen,  Xingyu Li,  Miaomiao Yang,  Hong Zhang,  Xu Steven Xu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：proposed algorithm outperformed two recently published baseline algorithms leveraging unsupervised clustering, image patches could help identify predictive patches, environment may provide better prediction ability, wsi based method without selection, slide digital pathology images</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning has become the mainstream methodological choice for analyzing
and interpreting whole-slide digital pathology images (WSIs). It is commonly
assumed that tumor regions carry most predictive information. In this paper, we
proposed an unsupervised clustering-based multiple-instance learning, and apply
our method to develop deep-learning models for prediction of gene mutations
using WSIs from three cancer types in The Cancer Genome Atlas (TCGA) studies
(CRC, LUAD, and HNSCC). We showed that unsupervised clustering of image patches
could help identify predictive patches, exclude patches lack of predictive
information, and therefore improve prediction on gene mutations in all three
different cancer types, compared with the WSI based method without selection of
image patches and models based on only tumor regions. Additionally, our
proposed algorithm outperformed two recently published baseline algorithms
leveraging unsupervised clustering to assist model prediction. The
unsupervised-clustering-based approach for mutation prediction allows
identification of the spatial regions related to mutation of a specific gene
via the resolved probability scores, highlighting the heterogeneity of a
predicted genotype in the tumor microenvironment. Finally, our study also
demonstrated that selection of tumor regions of WSIs is not always the best way
to identify patches for prediction of gene mutations, and other tissue types in
the tumor micro-environment may provide better prediction ability for gene
mutations than tumor tissues.</p>
  </details>
</details>
<details>
  <summary>118. <b>标题：Deep Learning for Spectral Filling in Radio Frequency Applications</b></summary>
  <p><b>编号</b>：[419]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01536</p>
  <p><b>作者</b>：Matthew Setzler,  Elizabeth Coda,  Jeremiah Rounds,  Michael Vann,  Michael Girard</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：effectively increase channel capacity without increasing bandwidth, present three computational experiments demonstrating, automatically learn novel modulation schemes, rf channel transmitting digital messages, e ., without interfering</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Due to the Internet of Things (IoT) proliferation, Radio Frequency (RF)
channels are increasingly congested with new kinds of devices, which carry
unique and diverse communication needs. This poses complex challenges in modern
digital communications, and calls for the development of technological
innovations that (i) optimize capacity (bitrate) in limited bandwidth
environments, (ii) integrate cooperatively with already-deployed RF protocols,
and (iii) are adaptive to the ever-changing demands in modern digital
communications. In this paper we present methods for applying deep neural
networks for spectral filling. Given an RF channel transmitting digital
messages with a pre-established modulation scheme, we automatically learn novel
modulation schemes for sending extra information, in the form of additional
messages, "around" the fixed-modulation signals (i.e., without interfering with
them). In so doing, we effectively increase channel capacity without increasing
bandwidth. We further demonstrate the ability to generate signals that closely
resemble the original modulations, such that the presence of extra messages is
undetectable to third-party listeners. We present three computational
experiments demonstrating the efficacy of our methods, and conclude by
discussing the implications of our results for modern RF applications.</p>
  </details>
</details>
<details>
  <summary>119. <b>标题：A single Long Short-Term Memory network for enhancing the prediction of  path-dependent plasticity with material heterogeneity and anisotropy</b></summary>
  <p><b>编号</b>：[422]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01466</p>
  <p><b>作者</b>：Ehsan Motevali Haghighi,  SeonHong Na</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：rnn possesses inductive biases toward information, replicate elastoplastic behaviors considering material heterogeneity, conventional deep recurrent neural networks, dimensional transversely anisotropic material associated, short term memory unit</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This study presents the applicability of conventional deep recurrent neural
networks (RNN) to predict path-dependent plasticity associated with material
heterogeneity and anisotropy. Although the architecture of RNN possesses
inductive biases toward information over time, it is still challenging to learn
the path-dependent material behavior as a function of the loading path
considering the change from elastic to elastoplastic regimes. Our attempt is to
develop a simple machine-learning-based model that can replicate elastoplastic
behaviors considering material heterogeneity and anisotropy. The basic
Long-Short Term Memory Unit (LSTM) is adopted for the modeling of plasticity in
the two-dimensional space by enhancing the inductive bias toward the past
information through manipulating input variables. Our results find that a
single LSTM based model can capture the J2 plasticity responses under both
monotonic and arbitrary loading paths provided the material heterogeneity. The
proposed neural network architecture is then used to model elastoplastic
responses of a two-dimensional transversely anisotropic material associated
with computational homogenization (FE2). It is also found that a single LSTM
model can be used to accurately and effectively capture the path-dependent
responses of heterogeneous and anisotropic microstructures under arbitrary
mechanical loading conditions.</p>
  </details>
</details>
<details>
  <summary>120. <b>标题：Anti-Spoofing Using Transfer Learning with Variational Information  Bottleneck</b></summary>
  <p><b>编号</b>：[424]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01387</p>
  <p><b>作者</b>：Youngsik Eom,  Yeonghyeon Lee,  Ji Sub Um,  Hoirin Kim</p>
  <p><b>备注</b>：Submitted to Interspeech 2022</p>
  <p><b>关键词</b>：transfer learning scheme based, existing automatic speaker verification, asvspoof 2019 logical access, using limited training data, proposed system improves performance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advances in sophisticated synthetic speech generated from
text-to-speech (TTS) or voice conversion (VC) systems cause threats to the
existing automatic speaker verification (ASV) systems. Since such synthetic
speech is generated from diverse algorithms, generalization ability with using
limited training data is indispensable for a robust anti-spoofing system. In
this work, we propose a transfer learning scheme based on the wav2vec 2.0
pretrained model with variational information bottleneck (VIB) for speech
anti-spoofing task. Evaluation on the ASVspoof 2019 logical access (LA)
database shows that our method improves the performance of distinguishing
unseen spoofed and genuine speech, outperforming current state-of-the-art
anti-spoofing systems. Furthermore, we show that the proposed system improves
performance in low-resource and cross-dataset settings of anti-spoofing task
significantly, demonstrating that our system is also robust in terms of data
size and data distribution.</p>
  </details>
</details>
<details>
  <summary>121. <b>标题：Deep learning, stochastic gradient descent and diffusion maps</b></summary>
  <p><b>编号</b>：[425]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01365</p>
  <p><b>作者</b>：Carmina Fjellström,  Kaj Nyström</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：zero eigenvalues indicate zero diffusion along, sgd dynamics may mainly live, use diffusion maps introduced, minima selection mainly happens, truly data driven approach</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Stochastic gradient descent (SGD) is widely used in deep learning due to its
computational efficiency but a complete understanding of why SGD performs so
well remains a major challenge. It has been observed empirically that most
eigenvalues of the Hessian of the loss functions on the loss landscape of
over-parametrized deep networks are close to zero, while only a small number of
eigenvalues are large. Zero eigenvalues indicate zero diffusion along the
corresponding directions. This indicates that the process of minima selection
mainly happens in the relatively low-dimensional subspace corresponding to top
eigenvalues of the Hessian. Although the parameter space is very
high-dimensional, these findings seems to indicate that the SGD dynamics may
mainly live on a low-dimensional manifold. In this paper we pursue a truly data
driven approach to the problem of getting a potentially deeper understanding of
the high-dimensional parameter surface, and in particular of the landscape
traced out by SGD, by analyzing the data generated through SGD, or any other
optimizer for that matter, in order to possibly discovery (local)
low-dimensional representations of the optimization landscape. As our vehicle
for the exploration we use diffusion maps introduced by R. Coifman and
coauthors.</p>
  </details>
</details>
<details>
  <summary>122. <b>标题：Discretely Indexed Flows</b></summary>
  <p><b>编号</b>：[426]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01361</p>
  <p><b>作者</b>：Elouan Argouarc'h,  François Desbouvries,  Eric Barat,  Eiji Kawasaki,  Thomas Dautremer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：underlying additional latent variable, deterministic transport becomes stochastic, solving variational estimation problems, propose discretely indexed flows, precisely discretely indexed</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper we propose Discretely Indexed flows (DIF) as a new tool for
solving variational estimation problems. Roughly speaking, DIF are built as an
extension of Normalizing Flows (NF), in which the deterministic transport
becomes stochastic, and more precisely discretely indexed. Due to the discrete
nature of the underlying additional latent variable, DIF inherit the good
computational behavior of NF: they benefit from both a tractable density as
well as a straightforward sampling scheme, and can thus be used for the dual
problems of Variational Inference (VI) and of Variational density estimation
(VDE). On the other hand, DIF can also be understood as an extension of mixture
density models, in which the constant mixture weights are replaced by flexible
functions. As a consequence, DIF are better suited for capturing distributions
with discontinuities, sharp edges and fine details, which is a main advantage
of this construction. Finally we propose a methodology for constructiong DIF in
practice, and see that DIF can be sequentially cascaded, and cascaded with NF.</p>
  </details>
</details>
<details>
  <summary>123. <b>标题：MOSRA: Joint Mean Opinion Score and Room Acoustics Speech Quality  Assessment</b></summary>
  <p><b>编号</b>：[428]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01345</p>
  <p><b>作者</b>：Karl El Hajal,  Milos Cernak,  Pablo Mainar</p>
  <p><b>备注</b>：Submitted to Interspeech 2022</p>
  <p><b>关键词</b>：overall mean opinion score, outside voice recording ),, g ., video call, joint training method enhances, dimensional speech quality metric</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The acoustic environment can degrade speech quality during communication
(e.g., video call, remote presentation, outside voice recording), and its
impact is often unknown. Objective metrics for speech quality have proven
challenging to develop given the multi-dimensionality of factors that affect
speech quality and the difficulty of collecting labeled data. Hypothesizing the
impact of acoustics on speech quality, this paper presents MOSRA: a
non-intrusive multi-dimensional speech quality metric that can predict room
acoustics parameters (SNR, STI, T60, DRR, and C50) alongside the overall mean
opinion score (MOS) for speech quality. By explicitly optimizing the model to
learn these room acoustics parameters, we can extract more informative features
and improve the generalization for the MOS task when the training data is
limited. Furthermore, we also show that this joint training method enhances the
blind estimation of room acoustics, improving the performance of current
state-of-the-art models. An additional side-effect of this joint prediction is
the improvement in the explainability of the predictions, which is a valuable
feature for many applications.</p>
  </details>
</details>
<details>
  <summary>124. <b>标题：Into-TTS : Intonation Template based Prosody Control System</b></summary>
  <p><b>编号</b>：[431]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01271</p>
  <p><b>作者</b>：Jihwan Lee,  Joun Yeop Lee,  Heejin Choi,  Seongkyu Mun,  Sangjun Park,  Chanwoo Kim</p>
  <p><b>备注</b>：Submitted to INTERSPEECH 2022</p>
  <p><b>关键词</b>：different intonations using predefined intonation templates, use intonation control system covering, end tts systems often fail, model proper intonations, utilize contextual information</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Intonations take an important role in delivering the intention of the
speaker. However, current end-to-end TTS systems often fail to model proper
intonations. To alleviate this problem, we propose a novel, intuitive method to
synthesize speech in different intonations using predefined intonation
templates. Prior to the acoustic model training, speech data are automatically
grouped into intonation templates by k-means clustering, according to their
sentence-final F0 contour. Two proposed modules are added to the end-to-end TTS
framework: intonation classifier and intonation encoder. The intonation
classifier recommends a suitable intonation template to the given text. The
intonation encoder, attached to the text encoder output, synthesizes speech
abiding the requested intonation template. Main contributions of our paper are:
(a) an easy-to-use intonation control system covering a wide range of users;
(b) better performance in wrapping speech in a requested intonation with
improved pitch distance and MOS; and (c) feasibility to future integration
between TTS and NLP, TTS being able to utilize contextual information. Audio
samples are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>125. <b>标题：Differentiable Rendering for Synthetic Aperture Radar Imagery</b></summary>
  <p><b>编号</b>：[433]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01248</p>
  <p><b>作者</b>：Michael Wilmanski,  Jonathan Tamir</p>
  <p><b>备注</b>：A substantially similar version of this manuscript was submitted to ECCV 2022 and is under review</p>
  <p><b>关键词</b>：allows explicitly modeling geometric priors, limited sar imagery using high, optimization pipeline using first, fidelity simulated sar data, synthetic aperture radar</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>There is rising interest in integrating signal and image processing pipelines
into deep learning training to incorporate more domain knowledge. This can lead
to deep neural networks that are trained more robustly and with limited data,
as well as the capability to solve ill-posed inverse problems. In particular,
there is rising interest in differentiable rendering, which allows explicitly
modeling geometric priors and constraints in the optimization pipeline using
first-order methods such as backpropagation. Existing efforts in differentiable
rendering have focused on imagery from electro-optical sensors, particularly
conventional RGB-imagery. In this work, we propose an approach for
differentiable rendering of Synthetic Aperture Radar (SAR) imagery, which
combines methods from 3D computer graphics with neural rendering. We
demonstrate the approach on the inverse graphics problem of 3D Object
Reconstruction from limited SAR imagery using high-fidelity simulated SAR data.</p>
  </details>
</details>
<details>
  <summary>126. <b>标题：Capturing positive utilities during the estimation of recursive logit  models: A prism-based approach</b></summary>
  <p><b>编号</b>：[435]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01215</p>
  <p><b>作者</b>：Yuki Oyama</p>
  <p><b>备注</b>：20 pages, 9 figures</p>
  <p><b>关键词</b>：prism constraint defined based upon, rl model achieved higher goodness, value functions remains unsolved, realistic route choice behavior, updated every iteration</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Although the recursive logit (RL) model has been recently popular and has led
to many applications and extensions, an important numerical issue with respect
to the evaluation of value functions remains unsolved. This issue is
particularly significant for model estimation, during which the parameters are
updated every iteration and may violate the model feasible condition. To solve
this numerical issue, this paper proposes a prism-constrained RL (Prism-RL)
model that implicitly restricts the path set by the prism constraint defined
based upon a state-extended network representation. Providing a set of
numerical experiments, we show that the Prism-RL model succeeds in the stable
estimation regardless of the initial and true parameter values and is able to
capture positive utilities. In the real application to a pedestrian network, we
found the positive effect of street green presence on pedestrians. Moreover,
the Prism-RL model achieved higher goodness of fit than the RL model, implying
that the Prism-RL model can also describe more realistic route choice behavior.</p>
  </details>
</details>
<details>
  <summary>127. <b>标题：Continuous Variable Quantum MNIST Classifiers</b></summary>
  <p><b>编号</b>：[437]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01194</p>
  <p><b>作者</b>：Sophie Choe</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：4 qumode hybrid classifier achieves 100, quantum neural network hybrid multiclassifiers, cv quantum neural network circuit, continuous variable quantum neural networks, binary classifier architecture proposed</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, classical and continuous variable (CV) quantum neural network
hybrid multiclassifiers are presented using the MNIST dataset. The combination
of cutoff dimension and probability measurement method in the CV model allows a
quantum circuit to produce output vectors of size equal to n raised to the
power of n where n represents cutoff dimension and m, the number of qumodes.
They are then translated as one-hot encoded labels, padded with an appropriate
number of zeros. The total of eight different classifiers are built using
2,3,...,8 qumodes, based on the binary classifier architecture proposed in
Continuous variable quantum neural networks. The displacement gate and the Kerr
gate in the CV model allow for the bias addition and nonlinear activation
components of classical neural networks to quantum. The classifiers are
composed of a classical feedforward neural network, a quantum data encoding
circuit, and a CV quantum neural network circuit. On a truncated MNIST dataset
of 600 samples, a 4 qumode hybrid classifier achieves 100% training accuracy.</p>
  </details>
</details>
<details>
  <summary>128. <b>标题：Proceedings of TDA: Applications of Topological Data Analysis to Data  Science, Artificial Intelligence, and Machine Learning Workshop at SDM 2022</b></summary>
  <p><b>编号</b>：[441]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01142</p>
  <p><b>作者</b>：R. W. R. Darling,  John A. Emanuello,  Emilie Purvine,  Ahmad Ridley</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：last several years bringing, workshop bringing together experts, natural language processing, mutual comprehensive awareness, fostering meaningful exchanges</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Topological Data Analysis (TDA) is a rigorous framework that borrows
techniques from geometric and algebraic topology, category theory, and
combinatorics in order to study the "shape" of such complex high-dimensional
data. Research in this area has grown significantly over the last several years
bringing a deeply rooted theory to bear on practical applications in areas such
as genomics, natural language processing, medicine, cybersecurity, energy, and
climate change. Within some of these areas, TDA has also been used to augment
AI and ML techniques.
We believe there is further utility to be gained in this space that can be
facilitated by a workshop bringing together experts (both theorists and
practitioners) and non-experts. Currently there is an active community of pure
mathematicians with research interests in developing and exploring the
theoretical and computational aspects of TDA. Applied mathematicians and other
practitioners are also present in community but do not represent a majority.
This speaks to the primary aim of this workshop which is to grow a wider
community of interest in TDA. By fostering meaningful exchanges between these
groups, from across the government, academia, and industry, we hope to create
new synergies that can only come through building a mutual comprehensive
awareness of the problem and solution spaces.</p>
  </details>
</details>
<details>
  <summary>129. <b>标题：Correlation Functions in Random Fully Connected Neural Networks at  Finite Width</b></summary>
  <p><b>编号</b>：[444]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01058</p>
  <p><b>作者</b>：Boris Hanin</p>
  <p><b>备注</b>：88p</p>
  <p><b>关键词</b>：article considers fully connected neural networks, obtain exact layerwise recursions, linearities including $\ mathrm, vanishing gradient problem, somewhat simplified version</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This article considers fully connected neural networks with Gaussian random
weights and biases and $L$ hidden layers, each of width proportional to a large
parameter $n$. For polynomially bounded non-linearities we give sharp estimates
in powers of $1/n$ for the joint correlation functions of the network output
and its derivatives. Moreover, we obtain exact layerwise recursions for these
correlation functions and solve a number of special cases for classes of
non-linearities including $\mathrm{ReLU}$ and $\tanh$. We find in both cases
that the depth-to-width ratio $L/n$ plays the role of an effective network
depth, controlling both the scale of fluctuations at individual neurons and the
size of inter-neuron correlations. We use this to study a somewhat simplified
version of the so-called exploding and vanishing gradient problem, proving that
this particular variant occurs if and only if $L/n$ is large. Several of the
key ideas in this article were first developed at a physics level of rigor in a
recent monograph with Roberts and Yaida.</p>
  </details>
</details>
<details>
  <summary>130. <b>标题：Learning-Based Approaches for Graph Problems: A Survey</b></summary>
  <p><b>编号</b>：[445]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01057</p>
  <p><b>作者</b>：Kai Siong Yow,  Siqiang Luo</p>
  <p><b>备注</b>：41 pages</p>
  <p><b>关键词</b>：famous examples include graph colouring, many graph problems specifically, travelling salesman problem, systematic review mainly, represented using graphs</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Over the years, many graph problems specifically those in NP-complete are
studied by a wide range of researchers. Some famous examples include graph
colouring, travelling salesman problem and subgraph isomorphism. Most of these
problems are typically addressed by exact algorithms, approximate algorithms
and heuristics. There are however some drawback for each of these methods.
Recent studies have employed learning-based frameworks such as machine learning
techniques in solving these problems, given that they are useful in discovering
new patterns in structured data that can be represented using graphs. This
research direction has successfully attracted a considerable amount of
attention. In this survey, we provide a systematic review mainly on classic
graph problems in which learning-based approaches have been proposed in
addressing the problems. We discuss the overview of each framework, and provide
analyses based on the design and performance of the framework. Some potential
research questions are also suggested. Ultimately, this survey gives a clearer
insight and can be used as a stepping stone to the research community in
studying problems in this field.</p>
  </details>
</details>
<details>
  <summary>131. <b>标题：Understanding the unstable convergence of gradient descent</b></summary>
  <p><b>编号</b>：[447]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01050</p>
  <p><b>作者</b>：Kwangjun Ahn,  Jingzhao Zhang,  Suvrit Sra</p>
  <p><b>备注</b>：21 pages; Comments would be appreciated!</p>
  <p><b>关键词</b>：machine learning applications step sizes often, elucidate key causes behind, l $- smooth cost, transparent view backed, gradient descent rely</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Most existing analyses of (stochastic) gradient descent rely on the condition
that for $L$-smooth cost, the step size is less than $2/L$. However, many works
have observed that in machine learning applications step sizes often do not
fulfill this condition, yet (stochastic) gradient descent converges, albeit in
an unstable manner. We investigate this unstable convergence phenomenon from
first principles, and elucidate key causes behind it. We also identify its main
characteristics, and how they interrelate, offering a transparent view backed
by both theory and experiments.</p>
  </details>
</details>
<details>
  <summary>132. <b>标题：Bi-fidelity Modeling of Uncertain and Partially Unknown Systems using  DeepONets</b></summary>
  <p><b>编号</b>：[450]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00997</p>
  <p><b>作者</b>：Subhayan De,  Malik Hassanaly,  Matthew Reynolds,  Ryan N. King,  Alireza Doostan</p>
  <p><b>备注</b>：21 pages, 12 figures</p>
  <p><b>关键词</b>：shifted research focuses towards data, partially unknown complex physical systems, require significant computational resources, neural network architecture suitable, scale complex physical systems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advances in modeling large-scale complex physical systems have shifted
research focuses towards data-driven techniques. However, generating datasets
by simulating complex systems can require significant computational resources.
Similarly, acquiring experimental datasets can prove difficult as well. For
these systems, often computationally inexpensive, but in general inaccurate,
models, known as the low-fidelity models, are available. In this paper, we
propose a bi-fidelity modeling approach for complex physical systems, where we
model the discrepancy between the true system's response and low-fidelity
response in the presence of a small training dataset from the true system's
response using a deep operator network (DeepONet), a neural network
architecture suitable for approximating nonlinear operators. We apply the
approach to model systems that have parametric uncertainty and are partially
unknown. Three numerical examples are used to show the efficacy of the proposed
approach to model uncertain and partially unknown complex physical systems.</p>
  </details>
</details>
<details>
  <summary>133. <b>标题：Risk-Aware Control and Optimization for High-Renewable Power Grids</b></summary>
  <p><b>编号</b>：[454]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00950</p>
  <p><b>作者</b>：Neil Barry,  Minas Chatzos,  Wenbo Chen,  Dahye Han,  Chaofan Huang,  Roshan Joseph,  Michael Klamkin,  Seonho Park,  Mathieu Tanneau,  Pascal Van Hentenryck,  Shangkun Wang,  Hanyu Zhang,  Haoruo Zhao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：energy raises fundamental challenges, existing deterministic optimization models, clearing raises challenges, electrical power grid, renewable energy sources</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The transition of the electrical power grid from fossil fuels to renewable
sources of energy raises fundamental challenges to the market-clearing
algorithms that drive its operations. Indeed, the increased stochasticity in
load and the volatility of renewable energy sources have led to significant
increases in prediction errors, affecting the reliability and efficiency of
existing deterministic optimization models. The RAMC project was initiated to
investigate how to move from this deterministic setting into a risk-aware
framework where uncertainty is quantified explicitly and incorporated in the
market-clearing optimizations. Risk-aware market-clearing raises challenges on
its own, primarily from a computational standpoint. This paper reviews how RAMC
approaches risk-aware market clearing and presents some of its innovations in
uncertainty quantification, optimization, and machine learning. Experimental
results on real networks are presented.</p>
  </details>
</details>
<details>
  <summary>134. <b>标题：Dimensionless machine learning: Imposing exact units equivariance</b></summary>
  <p><b>编号</b>：[459]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00887</p>
  <p><b>作者</b>：Soledad Villar,  Weichi Yao,  David W. Hogg,  Ben Blum-Smith,  Bianca Dumitrascu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：simple numerical examples involving dynamical systems, sample prediction accuracy gains one, physics relevance must obey self, relationships among measured quantities, inputs using classic results</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Units equivariance is the exact symmetry that follows from the requirement
that relationships among measured quantities of physics relevance must obey
self-consistent dimensional scalings. Here, we employ dimensional analysis and
ideas from equivariant machine learning to provide a two stage learning
procedure for units-equivariant machine learning. For a given learning task, we
first construct a dimensionless version of its inputs using classic results
from dimensional analysis, and then perform inference in the dimensionless
space. Our approach can be used to impose units equivariance across a broad
range of machine learning methods which are equivariant to rotations and other
groups. We discuss the in-sample and out-of-sample prediction accuracy gains
one can obtain in contexts like symbolic regression and emulation, where
symmetry is important. We illustrate our approach with simple numerical
examples involving dynamical systems in physics and ecology.</p>
  </details>
</details>
<details>
  <summary>135. <b>标题：Distributional Gradient Boosting Machines</b></summary>
  <p><b>编号</b>：[467]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00778</p>
  <p><b>作者</b>：Alexander März,  Thomas Kneib</p>
  <p><b>备注</b>：Distributional Regression, LightGBM, Normalizing Flow, Probabilistic Forecasting, XGBoost</p>
  <p><b>关键词</b>：entire conditional distribution greatly enhances existing tree, conditional cumulative distribution function via normalizing flows, unified probabilistic gradient boosting framework, based gradient boosting implementations, entire conditional distribution</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a unified probabilistic gradient boosting framework for regression
tasks that models and predicts the entire conditional distribution of a
univariate response variable as a function of covariates. Our likelihood-based
approach allows us to either model all conditional moments of a parametric
distribution, or to approximate the conditional cumulative distribution
function via Normalizing Flows. As underlying computational backbones, our
framework is based on XGBoost and LightGBM. Modelling and predicting the entire
conditional distribution greatly enhances existing tree-based gradient boosting
implementations, as it allows to create probabilistic forecasts from which
prediction intervals and quantiles of interest can be derived. Empirical
results show that our framework achieves state-of-the-art forecast accuracy.</p>
  </details>
</details>
<details>
  <summary>136. <b>标题：Variational message passing for online polynomial NARMAX identification</b></summary>
  <p><b>编号</b>：[469]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00769</p>
  <p><b>作者</b>：Wouter Kouw,  Albert Podusenko,  Magnus Koudahl,  Maarten Schoukens</p>
  <p><b>备注</b>：6 pages, 4 figures. Accepted to the American Control Conference 2022</p>
  <p><b>关键词</b>：variational message passing algorithm, small sample size settings, online nonlinear system identification, variational bayesian estimator outperforms, variational bayesian inference procedure</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a variational Bayesian inference procedure for online nonlinear
system identification. For each output observation, a set of parameter
posterior distributions is updated, which is then used to form a posterior
predictive distribution for future outputs. We focus on the class of polynomial
NARMAX models, which we cast into probabilistic form and represent in terms of
a Forney-style factor graph. Inference in this graph is efficiently performed
by a variational message passing algorithm. We show empirically that our
variational Bayesian estimator outperforms an online recursive least-squares
estimator, most notably in small sample size settings and low noise regimes,
and performs on par with an iterative least-squares estimator trained offline.</p>
  </details>
</details>
<details>
  <summary>137. <b>标题：Identifying Exoplanets with Machine Learning Methods: A Preliminary  Study</b></summary>
  <p><b>编号</b>：[473]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00721</p>
  <p><b>作者</b>：Yucheng Jin,  Lanyi Yang,  Chia-En Chiang</p>
  <p><b>备注</b>：12 pages with 9 figures and 2 tables</p>
  <p><b>关键词</b>：used another nasa dataset consisted, successfully obtained reasonable clusters, using machine learning methods, kepler dataset collected, unsupervised learning task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The discovery of habitable exoplanets has long been a heated topic in
astronomy. Traditional methods for exoplanet identification include the wobble
method, direct imaging, gravitational microlensing, etc., which not only
require a considerable investment of manpower, time, and money, but also are
limited by the performance of astronomical telescopes. In this study, we
proposed the idea of using machine learning methods to identify exoplanets. We
used the Kepler dataset collected by NASA from the Kepler Space Observatory to
conduct supervised learning, which predicts the existence of exoplanet
candidates as a three-categorical classification task, using decision tree,
random forest, naïve Bayes, and neural network; we used another NASA dataset
consisted of the confirmed exoplanets data to conduct unsupervised learning,
which divides the confirmed exoplanets into different clusters, using k-means
clustering. As a result, our models achieved accuracies of 99.06%, 92.11%,
88.50%, and 99.79%, respectively, in the supervised learning task and
successfully obtained reasonable clusters in the unsupervised learning task.</p>
  </details>
</details>
<details>
  <summary>138. <b>标题：UNetFormer: A Unified Vision Transformer Model and Pre-Training  Framework for 3D Medical Image Segmentation</b></summary>
  <p><b>编号</b>：[475]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00631</p>
  <p><b>作者</b>：Ali Hatamizadeh,  Ziyue Xu,  Dong Yang,  Wenqi Li,  Holger Roth,  Daguang Xu</p>
  <p><b>备注</b>：Tech. report, 12 pages, 3 figures</p>
  <p><b>关键词</b>：predict randomly masked volumetric tokens using contextual information, brain tumor segmentation using mri images, liver tumor segmentation task using, recently become popular due, decoder via skip connections</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision Transformers (ViT)s have recently become popular due to their
outstanding modeling capabilities, in particular for capturing long-range
information, and scalability to dataset and model sizes which has led to
state-of-the-art performance in various computer vision and medical image
analysis tasks. In this work, we introduce a unified framework consisting of
two architectures, dubbed UNetFormer, with a 3D Swin Transformer-based encoder
and Convolutional Neural Network (CNN) and transformer-based decoders. In the
proposed model, the encoder is linked to the decoder via skip connections at
five different resolutions with deep supervision. The design of proposed
architecture allows for meeting a wide range of trade-off requirements between
accuracy and computational cost. In addition, we present a methodology for
self-supervised pre-training of the encoder backbone via learning to predict
randomly masked volumetric tokens using contextual information of visible
tokens. We pre-train our framework on a cohort of $5050$ CT images, gathered
from publicly available CT datasets, and present a systematic investigation of
various components such as masking ratio and patch size that affect the
representation learning capability and performance of downstream tasks. We
validate the effectiveness of our pre-training approach by fine-tuning and
testing our model on liver and liver tumor segmentation task using the Medical
Segmentation Decathlon (MSD) dataset and achieve state-of-the-art performance
in terms of various segmentation metrics. To demonstrate its generalizability,
we train and test the model on BraTS 21 dataset for brain tumor segmentation
using MRI images and outperform other methods in terms of Dice score. Code:
this https URL</p>
  </details>
</details>
<details>
  <summary>139. <b>标题：TopTemp: Parsing Precipitate Structure from Temper Topology</b></summary>
  <p><b>编号</b>：[477]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00629</p>
  <p><b>作者</b>：Lara Kassab,  Scott Howland,  Henry Kvinge,  Keerti Sahithi Kappagantula,  Tegan Emerson</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：presented work outperforms conventional deep learning baselines, first step towards improving understanding, labor -, time -,, captures domain interpretable features, advanced manufacturing process parameters</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Technological advances are in part enabled by the development of novel
manufacturing processes that give rise to new materials or material property
improvements. Development and evaluation of new manufacturing methodologies is
labor-, time-, and resource-intensive expensive due to complex, poorly defined
relationships between advanced manufacturing process parameters and the
resulting microstructures. In this work, we present a topological
representation of temper (heat-treatment) dependent material micro-structure,
as captured by scanning electron microscopy, called TopTemp. We show that this
topological representation is able to support temper classification of
microstructures in a data limited setting, generalizes well to previously
unseen samples, is robust to image perturbations, and captures domain
interpretable features. The presented work outperforms conventional deep
learning baselines and is a first step towards improving understanding of
process parameters and resulting material properties.</p>
  </details>
</details>
<details>
  <summary>140. <b>标题：Bayesian Image Super-Resolution with Deep Modeling of Image Statistics</b></summary>
  <p><b>编号</b>：[478]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00623</p>
  <p><b>作者</b>：Shangqi Gao,  Xiahai Zhuang</p>
  <p><b>备注</b>：45 pages</p>
  <p><b>关键词</b>：model real image degradation including blurring, three image restoration tasks, using deep neural networks, bayesian image restoration framework, e .,} ideal sisr</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Modeling statistics of image priors is useful for image super-resolution, but
little attention has been paid from the massive works of deep learning-based
methods. In this work, we propose a Bayesian image restoration framework, where
natural image statistics are modeled with the combination of smoothness and
sparsity priors. Concretely, firstly we consider an ideal image as the sum of a
smoothness component and a sparsity residual, and model real image degradation
including blurring, downscaling, and noise corruption. Then, we develop a
variational Bayesian approach to infer their posteriors. Finally, we implement
the variational approach for single image super-resolution (SISR) using deep
neural networks, and propose an unsupervised training strategy. The experiments
on three image restoration tasks, \textit{i.e.,} ideal SISR, realistic SISR,
and real-world SISR, demonstrate that our method has superior model
generalizability against varying noise levels and degradation kernels and is
effective in unsupervised SISR. The code and resulting models are released via
\url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>141. <b>标题：Universal Lymph Node Detection in T2 MRI using Neural Networks</b></summary>
  <p><b>编号</b>：[479]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00622</p>
  <p><b>作者</b>：Tejas Sudharshan Mathai,  Sungwon Lee,  Thomas C. Shen,  Zhiyong Lu,  Ronald M. Summers</p>
  <p><b>备注</b>：Accepted at CARS 2022 (CAR track)</p>
  <p><b>关键词</b>：volumetric t2 mri using neural networks, 122 test t2 mri volumes revealed, without hard negative example mining, trained various neural network models, 5 fp per volume ).</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Purpose: Identification of abdominal Lymph Nodes (LN) that are suspicious for
metastasis in T2 Magnetic Resonance Imaging (MRI) scans is critical for staging
of lymphoproliferative diseases. Prior work on LN detection has been limited to
specific anatomical regions of the body (pelvis, rectum) in single MR slices.
Therefore, the development of a universal approach to detect LN in full T2 MRI
volumes is highly desirable.
Methods: In this study, a Computer Aided Detection (CAD) pipeline to
universally identify abdominal LN in volumetric T2 MRI using neural networks is
proposed. First, we trained various neural network models for detecting LN:
Faster RCNN with and without Hard Negative Example Mining (HNEM), FCOS,
FoveaBox, VFNet, and Detection Transformer (DETR). Next, we show that the
state-of-the-art (SOTA) VFNet model with Adaptive Training Sample Selection
(ATSS) outperforms Faster RCNN with HNEM. Finally, we ensembled models that
surpassed a 45% mAP threshold. We found that the VFNet model and one-stage
model ensemble can be interchangeably used in the CAD pipeline.
Results: Experiments on 122 test T2 MRI volumes revealed that VFNet achieved
a 51.1% mAP and 78.7% recall at 4 false positives (FP) per volume, while the
one-stage model ensemble achieved a mAP of 52.3% and sensitivity of 78.7% at
4FP.
Conclusion: Our contribution is a CAD pipeline that detects LN in T2 MRI
volumes, resulting in a sensitivity improvement of $\sim$14 points over the
current SOTA method for LN detection (sensitivity of 78.7% at 4 FP vs. 64.6% at
5 FP per volume).</p>
  </details>
</details>
<details>
  <summary>142. <b>标题：Visual explanations for polyp detection: How medical doctors assess  intrinsic versus extrinsic explanations</b></summary>
  <p><b>编号</b>：[482]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00617</p>
  <p><b>作者</b>：Steven Hicks,  Andrea Storås,  Michael Riegler,  Cise Midoglu,  Malek Hammou,  Thomas de Lange,  Sravanthi Parasa,  Pål Halvorsen,  Inga Strümke</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：recent years achieved immense success, making medical professionals highly skeptical, gastrointestinal disease detection use case, art explainable artificial intelligence methods, compare two different categories</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning has in recent years achieved immense success in all areas of
computer vision and has the potential of assisting medical doctors in analyzing
visual content for disease and other abnormalities. However, the current state
of deep learning is very much a black box, making medical professionals highly
skeptical about integrating these methods into clinical practice. Several
methods have been proposed in order to shine some light onto these black boxes,
but there is no consensus on the opinion of the medical doctors that will
consume these explanations. This paper presents a study asking medical doctors
about their opinion of current state-of-the-art explainable artificial
intelligence methods when applied to a gastrointestinal disease detection use
case. We compare two different categories of explanation methods, intrinsic and
extrinsic, and gauge their opinion of the current value of these explanations.
The results indicate that intrinsic explanations are preferred and that
explanation.</p>
  </details>
</details>
<h1>人工智能</h1>
<details>
  <summary>1. <b>标题：MaxViT: Multi-Axis Vision Transformer</b></summary>
  <p><b>编号</b>：[1]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01697</p>
  <p><b>作者</b>：Zhengzhong Tu,  Hossein Talebi,  Han Zhang,  Feng Yang,  Peyman Milanfar,  Alan Bovik,  Yinxiao Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：proposed model expresses strong generative modeling capability, design choices allow global, simple hierarchical vision backbone, backbone delivers favorable performance, recently gained significant attention</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transformers have recently gained significant attention in the computer
vision community. However, the lack of scalability of self-attention mechanisms
with respect to image size has limited their wide adoption in state-of-the-art
vision backbones. In this paper we introduce an efficient and scalable
attention model we call multi-axis attention, which consists of two aspects:
blocked local and dilated global attention. These design choices allow
global-local spatial interactions on arbitrary input resolutions with only
linear complexity. We also present a new architectural element by effectively
blending our proposed attention model with convolutions, and accordingly
propose a simple hierarchical vision backbone, dubbed MaxViT, by simply
repeating the basic building block over multiple stages. Notably, MaxViT is
able to "see" globally throughout the entire network, even in earlier,
high-resolution stages. We demonstrate the effectiveness of our model on a
broad spectrum of vision tasks. On image classification, MaxViT achieves
state-of-the-art performance under various settings: without extra data, MaxViT
attains 86.5\% ImageNet-1K top-1 accuracy; with ImageNet-21K pre-training, our
model achieves 88.7\% top-1 accuracy. For downstream tasks, MaxViT as a
backbone delivers favorable performance on object detection as well as visual
aesthetic assessment. We also show that our proposed model expresses strong
generative modeling capability on ImageNet, demonstrating the superior
potential of MaxViT blocks as a universal vision module. We will make the code
and models publicly available.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：T*$\varepsilon$ -- Bounded-Suboptimal Efficient Motion Planning for  Minimum-Time Planar Curvature-Constrained Systems</b></summary>
  <p><b>编号</b>：[14]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01673</p>
  <p><b>作者</b>：Doron Pinsky,  Petr Váňa,  Jan Faigl,  Oren Salzman</p>
  <p><b>备注</b>：8 pages, 6 figures</p>
  <p><b>关键词</b>：systems might require evaluating many, optimal transitions connecting two close, existing methods either pre, demonstrate using empirical evaluation, provided $\ varepsilon $)</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider the problem of finding collision-free paths for
curvature-constrained systems in the presence of obstacles while minimizing
execution time. Specifically, we focus on the setting where a planar system can
travel at some range of speeds with unbounded acceleration. This setting can
model many systems, such as fixed-wing drones. Unfortunately, planning for such
systems might require evaluating many (local) time-optimal transitions
connecting two close-by configurations, which is computationally expensive.
Existing methods either pre-compute all such transitions in a preprocessing
stage or use heuristics to speed up the search, thus foregoing any guarantees
on solution quality. Our key insight is that computing all the time-optimal
transitions is both~(i)~computationally expensive and~(ii)~unnecessary for many
problem instances. We show that by finding bounded-suboptimal solutions
(solutions whose cost is bounded by $1+\varepsilon$ times the cost of the
optimal solution for any user-provided $\varepsilon$) and not time-optimal
solutions, one can dramatically reduce the number of time-optimal transitions
used. We demonstrate using empirical evaluation that our planning framework can
reduce the runtime by several orders of magnitude compared to the
state-of-the-art while still providing guarantees on the quality of the
solution.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Exemplar-bsaed Pattern Synthesis with Implicit Periodic Field Network</b></summary>
  <p><b>编号</b>：[16]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01671</p>
  <p><b>作者</b>：Haiwei Chen,  Jiayi Liu,  Weikai Chen,  Shichen Liu,  Yajie Zhao</p>
  <p><b>备注</b>：8 pages, CVPR 2022</p>
  <p><b>关键词</b>：continuously designed gan training procedures, periodic encoding scheme encourages diversity, based visual pattern synthesis framework, implicit formulation directly maps, present novel experimental results</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Synthesis of ergodic, stationary visual patterns is widely applicable in
texturing, shape modeling, and digital content creation. The wide applicability
of this technique thus requires the pattern synthesis approaches to be
scalable, diverse, and authentic. In this paper, we propose an exemplar-based
visual pattern synthesis framework that aims to model the inner statistics of
visual patterns and generate new, versatile patterns that meet the
aforementioned requirements. To this end, we propose an implicit network based
on generative adversarial network (GAN) and periodic encoding, thus calling our
network the Implicit Periodic Field Network (IPFN). The design of IPFN ensures
scalability: the implicit formulation directly maps the input coordinates to
features, which enables synthesis of arbitrary size and is computationally
efficient for 3D shape synthesis. Learning with a periodic encoding scheme
encourages diversity: the network is constrained to model the inner statistics
of the exemplar based on spatial latent codes in a periodic field. Coupled with
continuously designed GAN training procedures, IPFN is shown to synthesize
tileable patterns with smooth transitions and local variations. Last but not
least, thanks to both the adversarial training technique and the encoded
Fourier features, IPFN learns high-frequency functions that produce authentic,
high-quality results. To validate our approach, we present novel experimental
results on various applications in 2D texture synthesis and 3D shape synthesis.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Characterizing Parametric and Convergence Stability in Nonconvex and  Nonsmooth Optimizations: A Geometric Approach</b></summary>
  <p><b>编号</b>：[22]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01643</p>
  <p><b>作者</b>：Xiaotie Deng,  Hanyu Li,  Ningyuan Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：parametric stability asks whether minor perturbations, slightly weaker function requirement goes, small enough step sizes, optimization algorithm cannot escape, prove quite tight conditions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider stability issues in minimizing a continuous (probably
parameterized, nonconvex and nonsmooth) real-valued function $f$. We call a
point stationary if all its possible directional derivatives are nonnegative.
In this work, we focus on two notions of stability on stationary points of $f$:
parametric stability and convergence stability. Parametric considerations are
widely studied in various fields, including smoothed analysis, numerical
stability, condition numbers and sensitivity analysis for linear programming.
Parametric stability asks whether minor perturbations on parameters lead to
dramatic changes in the position and $f$ value of a stationary point.
Meanwhile, convergence stability indicates a non-escapable solution: Any point
sequence iteratively produced by an optimization algorithm cannot escape from a
neighborhood of a stationary point but gets close to it in the sense that such
stationary points are stable to the precision parameter and algorithmic
numerical errors. It turns out that these notions have deep connections to
geometry theory. We show that parametric stability is linked to deformations of
graphs of functions. On the other hand, convergence stability is concerned with
area partitioning of the function domain. Utilizing these connections, we prove
quite tight conditions of these two stability notions for a wide range of
functions and optimization algorithms with small enough step sizes and
precision parameters. These conditions are subtle in the sense that a slightly
weaker function requirement goes to the opposite of primitive intuitions and
leads to wrong conclusions. We present three applications of this theory. These
applications reveal some understanding on Nash equilibrium computation,
nonconvex and nonsmooth optimization, as well as the new optimization
methodology of deep neural networks.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Estimating Social Influence from Observational Data</b></summary>
  <p><b>编号</b>：[26]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01633</p>
  <p><b>作者</b>：Dhanya Sridhar,  Caterina De Bacco,  David Blei</p>
  <p><b>备注</b>：To appear at CLeaR 2022 (1st Conference on Causal Learning and Reasoning)</p>
  <p><b>关键词</b>：pif fits probabilistic factor models, develop poisson influence factorization, pif estimates social influence, pif recovers estimates, empirically study pif</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider the problem of estimating social influence, the effect that a
person's behavior has on the future behavior of their peers. The key challenge
is that shared behavior between friends could be equally explained by influence
or by two other confounding factors: 1) latent traits that caused people to
both become friends and engage in the behavior, and 2) latent preferences for
the behavior. This paper addresses the challenges of estimating social
influence with three contributions. First, we formalize social influence as a
causal effect, one which requires inferences about hypothetical interventions.
Second, we develop Poisson Influence Factorization (PIF), a method for
estimating social influence from observational data. PIF fits probabilistic
factor models to networks and behavior data to infer variables that serve as
substitutes for the confounding latent traits. Third, we develop assumptions
under which PIF recovers estimates of social influence. We empirically study
PIF with semi-synthetic and real data from this http URL, and conduct a sensitivity
analysis. We find that PIF estimates social influence most accurately compared
to related methods and remains robust under some violations of its assumptions.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Towards Deep Industrial Transfer Learning: Clustering for Transfer Case  Selection</b></summary>
  <p><b>编号</b>：[31]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01620</p>
  <p><b>作者</b>：Benjamin Maschler,  Tim Knodel,  Michael Weyrich</p>
  <p><b>备注</b>：7 pages, 5 figurs, 2 tables. Submitted to IEEE ETFA 2022</p>
  <p><b>关键词</b>：dynamic industrial use cases without high manual efforts, transfer case selection based upon clustering, deep learning algorithms towards heterogenous, industrial time series dataset, industrial transfer learning increases</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Industrial transfer learning increases the adaptability of deep learning
algorithms towards heterogenous and dynamic industrial use cases without high
manual efforts. The appropriate selection of what to transfer can vastly
improve a transfer's results. In this paper, a transfer case selection based
upon clustering is presented. Founded on a survey of clustering algorithms, the
BIRCH algorithm is selected for this purpose. It is evaluated on an industrial
time series dataset from a discrete manufacturing scenario. Results underline
the approaches' applicability caused by its results' reproducibility and
practical indifference to sequence, size and dimensionality of (sub-)datasets
to be clustered sequentially.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：SPECTRE : Spectral Conditioning Helps to Overcome the Expressivity  Limits of One-shot Graph Generators</b></summary>
  <p><b>编号</b>：[35]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01613</p>
  <p><b>作者</b>：Karolis Martinkus,  Andreas Loukas,  Nathanaël Perraudin,  Roger Wattenhofer</p>
  <p><b>备注</b>：20 pages, 10 figures</p>
  <p><b>关键词</b>：also avoiding expensive sequential generation, world graphs spectre achieves, art deep autoregressive generators, much larger graphs, graph generation problem</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We approach the graph generation problem from a spectral perspective by first
generating the dominant parts of the graph Laplacian spectrum and then building
a graph matching these eigenvalues and eigenvectors. Spectral conditioning
allows for direct modeling of the global and local graph structure and helps to
overcome the expressivity and mode collapse issues of one-shot graph
generators. Our novel GAN, called SPECTRE, enables the one-shot generation of
much larger graphs than previously possible with one-shot models. SPECTRE
outperforms state-of-the-art deep autoregressive generators in terms of
modeling fidelity, while also avoiding expensive sequential generation and
dependence on node ordering. A case in point, in sizable synthetic and
real-world graphs SPECTRE achieves a 4-to-170 fold improvement over the best
competitor that does not overfit and is 23-to-30 times faster than
autoregressive generators.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：A Machine With Human-Like Memory Systems</b></summary>
  <p><b>编号</b>：[37]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01611</p>
  <p><b>作者</b>：Taewoon Kim,  Michael Cochez,  Vincent Francois-Lavet,  Mark Neerincx,  Piek Vossen</p>
  <p><b>备注</b>：Submitted to Human-Centered Design of Symbiotic Hybrid Intelligence 2022 (this https URL)</p>
  <p><b>关键词</b>：one agent acting alone, two memory systems, two agents collaborating, room ", compatible, hybrid intelligence setup</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Inspired by the cognitive science theory, we explicitly model an agent with
both semantic and episodic memory systems, and show that it is better than
having just one of the two memory systems. In order to show this, we have
designed and released our own challenging environment, "the Room", compatible
with OpenAI Gym, where an agent has to properly learn how to encode, store, and
retrieve memories to maximize its rewards. The Room environment allows for a
hybrid intelligence setup where machines and humans can collaborate. We show
that two agents collaborating with each other results in better performance
than one agent acting alone. We have open-sourced our code and models at
this https URL.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Modern Views of Machine Learning for Precision Psychiatry</b></summary>
  <p><b>编号</b>：[39]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01607</p>
  <p><b>作者</b>：Zhe Sage Chen,  Prathamesh (Param) Kulkarni,  Isaac R. Galatzer-Levy,  Benedetta Bigio,  Carla Nasca,  Yu Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：potentially provide explainable solutions, mobile technologies also call, methods provide new opportunities, species biomarker identification, multimodal data fusion</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In light of the NIMH's Research Domain Criteria (RDoC), the advent of
functional neuroimaging, novel technologies and methods provide new
opportunities to develop precise and personalized prognosis and diagnosis of
mental disorders. Machine learning (ML) and artificial intelligence (AI)
technologies are playing an increasingly critical role in the new era of
precision psychiatry. Combining ML/AI with neuromodulation technologies can
potentially provide explainable solutions in clinical practice and effective
therapeutic treatment. Advanced wearable and mobile technologies also call for
the new role of ML/AI for digital phenotyping in mobile mental health. In this
review, we provide a comprehensive review of the ML methodologies and
applications by combining neuroimaging, neuromodulation, and advanced mobile
technologies in psychiatry practice. Additionally, we review the role of ML in
molecular phenotyping and cross-species biomarker identification in precision
psychiatry. We further discuss explainable AI (XAI) and causality testing in a
closed-human-in-the-loop manner, and highlight the ML potential in multimedia
information extraction and multimodal data fusion. Finally, we discuss
conceptual and practical challenges in precision psychiatry and highlight ML
opportunities in future research.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Exosoul: ethical profiling in the digital world</b></summary>
  <p><b>编号</b>：[46]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01588</p>
  <p><b>作者</b>：Costanza Alfieri,  Paola Inverardi,  Patrizio Migliarini,  Massimiliano Palmiero</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：society pose new ethical challenges beyond data protection, digital behaviors concerning privacy violation, multidisciplinary project exosoul aims, discuss two clustering solutions, predicting general moral preferences</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The development and the spread of increasingly autonomous digital
technologies in our society pose new ethical challenges beyond data protection
and privacy violation. Users are unprotected in their interactions with digital
technologies and at the same time autonomous systems are free to occupy the
space of decisions that is prerogative of each human being. In this context the
multidisciplinary project Exosoul aims at developing a personalized software
exoskeleton which mediates actions in the digital world according to the moral
preferences of the user. The exoskeleton relies on the ethical profiling of a
user, similar in purpose to the privacy profiling proposed in the literature,
but aiming at reflecting and predicting general moral preferences. Our approach
is hybrid, first based on the identification of profiles in a top-down manner,
and then on the refinement of profiles by a personalized data-driven approach.
In this work we report our initial experiment on building such top-down
profiles. We consider the correlations between ethics positions (idealism and
relativism) personality traits (honesty/humility, conscientiousness,
Machiavellianism and narcissism) and worldview (normativism), and then we use a
clustering approach to create ethical profiles predictive of user's digital
behaviors concerning privacy violation, copy-right infringements, caution and
protection. Data were collected by administering a questionnaire to 317 young
individuals. In the paper we discuss two clustering solutions, one data-driven
and one model-driven, in terms of validity and predictive power of digital
behavior.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Coarse-to-Fine Q-attention with Learned Path Ranking</b></summary>
  <p><b>编号</b>：[50]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01571</p>
  <p><b>作者</b>：Stephen James,  Pieter Abbeel</p>
  <p><b>备注</b>：Project page and code: this https URL</p>
  <p><b>关键词</b>：approach across 16 rlbench tasks, propose learned path ranking, reaching paths generated, path generation modules, path generating methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose Learned Path Ranking (LPR), a method that accepts an end-effector
goal pose, and learns to rank a set of goal-reaching paths generated from an
array of path generating methods, including: path planning, Bezier curve
sampling, and a learned policy. The core idea being that each of the path
generation modules will be useful in different tasks, or at different stages in
a task. When LPR is added as an extension to C2F-ARM, our new system,
C2F-ARM+LPR, retains the sample efficiency of its predecessor, while also being
able to accomplish a larger set of tasks; in particular, tasks that require
very specific motions (e.g. opening toilet seat) that need to be inferred from
both demonstrations and exploration data. In addition to benchmarking our
approach across 16 RLBench tasks, we also learn real-world tasks, tabula rasa,
in 10-15 minutes, with only 3 demonstrations.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：RobustSense: Defending Adversarial Attack for Secure Device-Free Human  Activity Recognition</b></summary>
  <p><b>编号</b>：[54]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01560</p>
  <p><b>作者</b>：Jianfei Yang,  Han Zou,  Lihua Xie</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：wireless human activity recognition, wireless human activity recognition, bring severe safety hazards, achieve consistent predictions regardless, free human activity recognition</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep neural networks have empowered accurate device-free human activity
recognition, which has wide applications. Deep models can extract robust
features from various sensors and generalize well even in challenging
situations such as data-insufficient cases. However, these systems could be
vulnerable to input perturbations, i.e. adversarial attacks. We empirically
demonstrate that both black-box Gaussian attacks and modern adversarial
white-box attacks can render their accuracies to plummet. In this paper, we
firstly point out that such phenomenon can bring severe safety hazards to
device-free sensing systems, and then propose a novel learning framework,
RobustSense, to defend common attacks. RobustSense aims to achieve consistent
predictions regardless of whether there exists an attack on its input or not,
alleviating the negative effect of distribution perturbation caused by
adversarial attacks. Extensive experiments demonstrate that our proposed method
can significantly enhance the model robustness of existing deep models,
overcoming possible attacks. The results validate that our method works well on
wireless human activity recognition and person identification systems. To the
best of our knowledge, this is the first work to investigate adversarial
attacks and further develop a novel defense framework for wireless human
activity recognition in mobile computing research.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：CDKT-FL: Cross-Device Knowledge Transfer using Proxy Dataset in  Federated Learning</b></summary>
  <p><b>编号</b>：[61]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01542</p>
  <p><b>作者</b>：Minh N. H. Nguyen,  Huy Q. Le,  Shashi Raj Pandey,  Choong Seon Hong</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：practical setting towards better generalization abilities, device knowledge transfer following general formulations, proposed method achieves significant speedups, conventional fl methods need redesigning, realizing robust personalized federated learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In a practical setting towards better generalization abilities of client
models for realizing robust personalized Federated Learning (FL) systems,
efficient model aggregation methods have been considered as a critical research
objective. It is a challenging issue due to the consequences of non-i.i.d.
properties of client's data, often referred to as statistical heterogeneity and
small local data samples from the various data distributions. Therefore, to
develop robust generalized global and personalized models, conventional FL
methods need redesigning the knowledge aggregation from biased local models
while considering huge divergence of learning parameters due to skewed client
data. In this work, we demonstrate that the knowledge transfer mechanism is a
de facto technique to achieve these objectives and develop a novel knowledge
distillation-based approach to study the extent of knowledge transfer between
the global model and local models. Henceforth, our method considers the
suitability of transferring the outcome distribution and (or) the embedding
vector of representation from trained models during cross-device knowledge
transfer using a small proxy dataset in heterogeneous FL. In doing so, we
alternatively perform cross-device knowledge transfer following general
formulations as 1) global knowledge transfer and 2) on-device knowledge
transfer. Through simulations on four federated datasets, we show the proposed
method achieves significant speedups and high personalized performance of local
models. Furthermore, the proposed approach offers a more stable algorithm than
FedAvg during the training, with minimal communication data load when
exchanging the trained model's outcomes and representation.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Artificial Intelligence: Framework of driving triggers to past, present  and future applications and influencers of industry sector adoption</b></summary>
  <p><b>编号</b>：[69]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01518</p>
  <p><b>作者</b>：Richard Fulton,  Diane Fulton,  Susan Kaplan</p>
  <p><b>备注</b>：19 pages, 2 figures, AIFU Conference</p>
  <p><b>关键词</b>：several key industry sectors along, next several decades, essential transformative technology, affect adoption speed, research examines</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To gain a sense of the development of Artificial Intelligence (AI), this
research analyzes what has been done in the past, presently in the last decade
and what is predicted for the next several decades. The paper will highlight
the biggest changes in AI and give examples of how these technologies are
applied in several key industry sectors along with influencers that can affect
adoption speed. Lastly, the research examines the driving triggers such as
cost, speed, accuracy, diversity/inclusion and interdisciplinary
research/collaboration that propel AI into an essential transformative
technology.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Context-aware Visual Tracking with Joint Meta-updating</b></summary>
  <p><b>编号</b>：[71]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01513</p>
  <p><b>作者</b>：Qiuhong Shen,  Xin Li,  Fanyang Meng,  Yongsheng Liang</p>
  <p><b>备注</b>：9 pages, 8 figures</p>
  <p><b>关键词</b>：various emerging video applications, updater optimizes trackers directly, proposed tracking method achieves, visual object tracking acts, existing deep trackers</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Visual object tracking acts as a pivotal component in various emerging video
applications. Despite the numerous developments in visual tracking, existing
deep trackers are still likely to fail when tracking against objects with
dramatic variation. These deep trackers usually do not perform online update or
update single sub-branch of the tracking model, for which they cannot adapt to
the appearance variation of objects. Efficient updating methods are therefore
crucial for tracking while previous meta-updater optimizes trackers directly
over parameter space, which is prone to over-fit even collapse on longer
sequences. To address these issues, we propose a context-aware tracking model
to optimize the tracker over the representation space, which jointly
meta-update both branches by exploiting information along the whole sequence,
such that it can avoid the over-fitting problem. First, we note that the
embedded features of the localization branch and the box-estimation branch,
focusing on the local and global information of the target, are effective
complements to each other. Based on this insight, we devise a
context-aggregation module to fuse information in historical frames, followed
by a context-aware module to learn affinity vectors for both branches of the
tracker. Besides, we develop a dedicated meta-learning scheme, on account of
fast and stable updating with limited training samples. The proposed tracking
method achieves an EAO score of 0.514 on VOT2018 with the speed of 40FPS,
demonstrating its capability of improving the accuracy and robustness of the
underlying tracker with little speed drop.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Towards a New Science of Disinformation</b></summary>
  <p><b>编号</b>：[80]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01489</p>
  <p><b>作者</b>：Claudio S. Pinhanez,  German H. Flores,  Marisa A. Vasconcelos,  Mu Qiao,  Nick Linck,  Rogério de Paula,  Yuya J. Ong</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：upcoming cybersecurity problem, technological challenges facing, cheap deepfake technology, generated fake audios, effectively engage users</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>How can we best address the dangerous impact that deep learning-generated
fake audios, photographs, and videos (a.k.a. deepfakes) may have in personal
and societal life? We foresee that the availability of cheap deepfake
technology will create a second wave of disinformation where people will
receive specific, personalized disinformation through different channels,
making the current approaches to fight disinformation obsolete. We argue that
fake media has to be seen as an upcoming cybersecurity problem, and we have to
shift from combating its spread to a prevention and cure framework where users
have available ways to verify, challenge, and argue against the veracity of
each piece of media they are exposed to. To create the technologies behind this
framework, we propose that a new Science of Disinformation is needed, one which
creates a theoretical framework both for the processes of communication and
consumption of false content. Key scientific and technological challenges
facing this research agenda are listed and discussed in the light of
state-of-art technologies for fake media generation and detection, argument
finding and construction, and how to effectively engage users in the prevention
and cure processes.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Unsupervised Learning of Accurate Siamese Tracking</b></summary>
  <p><b>编号</b>：[88]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01475</p>
  <p><b>作者</b>：Qiuhong Shen,  Lei Qiao,  Jinyang Guo,  Peixia Li,  Xin Li,  Bo Li,  Weitao Feng,  Weihao Gan,  Wei Wu,  Wanli Ouyang</p>
  <p><b>备注</b>：13 pages, 7 figures, to appear in CVPR 2022</p>
  <p><b>关键词</b>：since noisy labels may degrade training, prior unsupervised tracking approaches rely heavily, tracker outperforms preceding unsupervised methods, various computer vision tasks, guided loss reweighting strategy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Unsupervised learning has been popular in various computer vision tasks,
including visual object tracking. However, prior unsupervised tracking
approaches rely heavily on spatial supervision from template-search pairs and
are still unable to track objects with strong variation over a long time span.
As unlimited self-supervision signals can be obtained by tracking a video along
a cycle in time, we investigate evolving a Siamese tracker by tracking videos
forward-backward. We present a novel unsupervised tracking framework, in which
we can learn temporal correspondence both on the classification branch and
regression branch. Specifically, to propagate reliable template feature in the
forward propagation process so that the tracker can be trained in the cycle, we
first propose a consistency propagation transformation. We then identify an
ill-posed penalty problem in conventional cycle training in backward
propagation process. Thus, a differentiable region mask is proposed to select
features as well as to implicitly penalize tracking errors on intermediate
frames. Moreover, since noisy labels may degrade training, we propose a
mask-guided loss reweighting strategy to assign dynamic weights based on the
quality of pseudo labels. In extensive experiments, our tracker outperforms
preceding unsupervised methods by a substantial margin, performing on par with
supervised methods on large-scale datasets such as TrackingNet and LaSOT. Code
is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Event Log Sampling for Predictive Monitoring</b></summary>
  <p><b>编号</b>：[90]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01470</p>
  <p><b>作者</b>：Mohammadreza Fani Sani,  Mozhgan Vazifehdoostirani,  Gyunam Park,  Marco Pegoraro,  Sebastiaan J. van Zelst,  Wil M.P. van der Aalst</p>
  <p><b>备注</b>：7 pages, 1 figure, 4 tables, 34 references</p>
  <p><b>关键词</b>：allows sampling training process instances, complex machine learning models, next activity prediction methods, sampling method allows, running process instances</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Predictive process monitoring is a subfield of process mining that aims to
estimate case or event features for running process instances. Such predictions
are of significant interest to the process stakeholders. However,
state-of-the-art methods for predictive monitoring require the training of
complex machine learning models, which is often inefficient. This paper
proposes an instance selection procedure that allows sampling training process
instances for prediction models. We show that our sampling method allows for a
significant increase of training speed for next activity prediction methods
while maintaining reliable levels of prediction accuracy.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Value Gradient weighted Model-Based Reinforcement Learning</b></summary>
  <p><b>编号</b>：[94]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01464</p>
  <p><b>作者</b>：Claas Voelcker,  Victor Liao,  Animesh Garg,  Amir-massoud Farahmand</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：yet unavoidable modeling errors often lead performance deterioration, aware model learning would fix, naive intuition would suggest, commonly used maximum likelihood, gradient weighted model learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Model-based reinforcement learning (MBRL) is a sample efficient technique to
obtain control policies, yet unavoidable modeling errors often lead performance
deterioration. The model in MBRL is often solely fitted to reconstruct
dynamics, state observations in particular, while the impact of model error on
the policy is not captured by the training objective. This leads to a mismatch
between the intended goal of MBRL, enabling good policy and value learning, and
the target of the loss function employed in practice, future state prediction.
Naive intuition would suggest that value-aware model learning would fix this
problem and, indeed, several solutions to this objective mismatch problem have
been proposed based on theoretical analysis. However, they tend to be inferior
in practice to commonly used maximum likelihood (MLE) based approaches. In this
paper we propose the Value-gradient weighted Model Learning (VaGraM), a novel
method for value-aware model learning which improves the performance of MBRL in
challenging settings, such as small model capacity and the presence of
distracting state dimensions. We analyze both MLE and value-aware approaches
and demonstrate how they fail to account for exploration and the behavior of
function approximation when learning value-aware models and highlight the
additional goals that must be met to stabilize optimization in the deep
learning setting. We verify our analysis by showing that our loss function is
able to achieve high returns on the Mujoco benchmark suite while being more
robust than maximum likelihood based approaches.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Disentangling Abstraction from Statistical Pattern Matching in Human and  Machine Learning</b></summary>
  <p><b>编号</b>：[106]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01437</p>
  <p><b>作者</b>：Sreejan Kumar,  Ishita Dasgupta,  Raja Marjieh,  Nathaniel D. Daw,  Jonathan D. Cohen,  Thomas L. Griffiths</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：future work towards developing machines, reinforcement learning agent performs worse, different underlying generative process, inductive bias towards abstraction, empirically identified human priors</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The ability to acquire abstract knowledge is a hallmark of human intelligence
and is believed by many to be one of the core differences between humans and
neural network models. Agents can be endowed with an inductive bias towards
abstraction through meta-learning, where they are trained on a distribution of
tasks that share some abstract structure that can be learned and applied.
However, because neural networks are hard to interpret, it can be difficult to
tell whether agents have learned the underlying abstraction, or alternatively
statistical patterns that are characteristic of that abstraction. In this work,
we compare the performance of humans and agents in a meta-reinforcement
learning paradigm in which tasks are generated from abstract rules. We define a
novel methodology for building "task metamers" that closely match the
statistics of the abstract tasks but use a different underlying generative
process, and evaluate performance on both abstract and metamer tasks. In our
first set of experiments, we found that humans perform better at abstract tasks
than metamer tasks whereas a widely-used meta-reinforcement learning agent
performs worse on the abstract tasks than the matched metamers. In a second set
of experiments, we base the tasks on abstractions derived directly from
empirically identified human priors. We utilize the same procedure to generate
corresponding metamer tasks, and see the same double dissociation between
humans and agents. This work provides a foundation for characterizing
differences between humans and machine learning that can be used in future work
towards developing machines with human-like behavior.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：SAM-kNN Regressor for Online Learning in Water Distribution Networks</b></summary>
  <p><b>编号</b>：[107]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01436</p>
  <p><b>作者</b>：Jonathan Jakob,  André Artelt,  Martina Hasenjäger,  Barbara Hammer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：distribute water via widely branched networks, residual based anomaly detection system, water supply company continuously monitors, residual based anomaly detection systems, since real world networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Water distribution networks are a key component of modern infrastructure for
housing and industry. They transport and distribute water via widely branched
networks from sources to consumers. In order to guarantee a working network at
all times, the water supply company continuously monitors the network and takes
actions when necessary -- e.g. reacting to leakages, sensor faults and drops in
water quality. Since real world networks are too large and complex to be
monitored by a human, algorithmic monitoring systems have been developed. A
popular type of such systems are residual based anomaly detection systems that
can detect events such as leakages and sensor faults. For a continuous high
quality monitoring, it is necessary for these systems to adapt to changed
demands and presence of various anomalies.
In this work, we propose an adaption of the incremental SAM-kNN classifier
for regression to build a residual based anomaly detection system for water
distribution networks that is able to adapt to any kind of change.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Automated Machine Learning for Deep Recommender Systems: A Survey</b></summary>
  <p><b>编号</b>：[120]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01390</p>
  <p><b>作者</b>：Bo Chen,  Xiangyu Zhao,  Yejing Wang,  Wenqi Fan,  Huifeng Guo,  Ruiming Tang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：employ sophisticated neural network architectures, current commercial online service providers, unprecedented feature representations effectiveness, discuss appealing research directions, deep recommender systems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep recommender systems (DRS) are critical for current commercial online
service providers, which address the issue of information overload by
recommending items that are tailored to the user's interests and preferences.
They have unprecedented feature representations effectiveness and the capacity
of modeling the non-linear relationships between users and items. Despite their
advancements, DRS models, like other deep learning models, employ sophisticated
neural network architectures and other vital components that are typically
designed and tuned by human experts. This article will give a comprehensive
summary of automated machine learning (AutoML) for developing DRS models. We
first provide an overview of AutoML for DRS models and the related techniques.
Then we discuss the state-of-the-art AutoML approaches that automate the
feature selection, feature embeddings, feature interactions, and system design
in DRS. Finally, we discuss appealing research directions and summarize the
survey.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Detection of Dangerous Events on Social Media: A Perspective Review</b></summary>
  <p><b>编号</b>：[129]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01351</p>
  <p><b>作者</b>：M. Luqman Jamil,  Sebastião Pais,  João Cordeiro</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：definitive cause cannot survive without, three main types based, detecting events happening, based dangerous events, users every hour</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Social media is an essential gateway of information and communication for
people worldwide. The amount of time spent and reliance of people on social
media makes it a vital resource for detecting events happening in real life.
Thousands of significant events are posted by users every hour in the form of
multimedia. Some individuals and groups target the audience to promote their
agenda among these users. Their cause can threaten other groups and individuals
who do not share the same views or have specific differences. Any group with a
definitive cause cannot survive without the support which acts as a catalyst
for their agenda. A phenomenon occurs where people are fed information that
motivates them to act on their behalf and carry out their agenda. One is
benefit results in the loss of the others by putting their lives, assets,
physical and emotional health in danger. This paper introduces a concept of
dangerous events to approach this problem and their three main types based on
their characteristics: action, scenarios, and sentiment-based dangerous events.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：An application of Pixel Interval Down-sampling (PID) for dense tiny  microorganism counting on environmental microorganism images</b></summary>
  <p><b>编号</b>：[134]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01341</p>
  <p><b>作者</b>：Jiawei Zhang,  Ning Xu,  Chen Li,  Md Mamunur Rahaman,  Yu-Dong Yao,  Yu-Hao Lin,  Jinghua Zhang,  Tao Jiang,  Wenjun Qin,  Marcin Grzegorzek</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：done using classical segmentation metrics, art approaches like attention u, dense tiny objects counting tasks, 2448 yeast cell images, dense tiny objects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper proposes a novel pixel interval down-sampling network (PID-Net)
for dense tiny objects (yeast cells) counting tasks with higher accuracy. The
PID-Net is an end-to-end CNN model with encoder to decoder architecture. The
pixel interval down-sampling operations are concatenated with max-pooling
operations to combine the sparse and dense features. It addresses the
limitation of contour conglutination of dense objects while counting.
Evaluation was done using classical segmentation metrics (Dice, Jaccard,
Hausdorff distance) as well as counting metrics. Experimental result shows that
the proposed PID-Net has the best performance and potential for dense tiny
objects counting tasks, which achieves 96.97% counting accuracy on the dataset
with 2448 yeast cell images. By comparing with the state-of-the-art approaches
like Attention U-Net, Swin U-Net and Trans U-Net, the proposed PID-Net can
segment the dense tiny objects with clearer boundaries and fewer incorrect
debris, which shows the great potential of PID-Net in the task of accurate
counting tasks.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Flexible Portrait Image Editing with Fine-Grained Control</b></summary>
  <p><b>编号</b>：[145]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01318</p>
  <p><b>作者</b>：Linlin Liu,  Qian Fu,  Fei Hou,  Ying He</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：novel asymmetric conditional gan architecture, single neural network model, guide controllable image generation, also present ablation studies, contain positional information</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We develop a new method for portrait image editing, which supports
fine-grained editing of geometries, colors, lights and shadows using a single
neural network model. We adopt a novel asymmetric conditional GAN architecture:
the generators take the transformed conditional inputs, such as edge maps,
color palette, sliders and masks, that can be directly edited by the user; the
discriminators take the conditional inputs in the way that can guide
controllable image generation more effectively. Taking color editing as an
example, we feed color palettes (which can be edited easily) into the
generator, and color maps (which contain positional information of colors) into
the discriminator. We also design a region-weighted discriminator so that
higher weights are assigned to more important regions, like eyes and skin.
Using a color palette, the user can directly specify the desired colors of
hair, skin, eyes, lip and background. Color sliders allow the user to blend
colors in an intuitive manner. The user can also edit lights and shadows by
modifying the corresponding masks. We demonstrate the effectiveness of our
method by evaluating it on the CelebAMask-HQ dataset with a wide range of
tasks, including geometry/color/shadow/light editing, hand-drawn sketch to
image translation, and color transfer. We also present ablation studies to
justify our design.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：GraFN: Semi-Supervised Node Classification on Graph with Few Labels via  Non-Parametric Distribution Assignment</b></summary>
  <p><b>编号</b>：[149]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01303</p>
  <p><b>作者</b>：Junseok Lee,  Yunhak Oh,  Yeonjun In,  Namkyeong Lee,  Dongmin Hyun,  Chanyoung Park</p>
  <p><b>备注</b>：SIGIR 2022(Short Paper)</p>
  <p><b>关键词</b>：learning class discriminative node representations since, gnns encounter significant performance degradation, grafn randomly samples support nodes, two predicted class distributions, supervised learning paradigm aims</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>GNNs encounter significant performance degradation when the amount of
supervision signals, i.e., number of labeled nodes, is limited, which is
expected as GNNs are trained solely based on the supervision obtained from the
labeled nodes. On the other hand,recent self-supervised learning paradigm aims
to train GNNs by solving pretext tasks that do not require any labeled nodes,
and it has shown to even outperform GNNs trained with few labeled nodes.
However, a major drawback of self-supervised methods is that they fall short of
learning class discriminative node representations since no labeled information
is utilized during training. To this end, we propose a novel semi-supervised
method for graphs, GraFN, that leverages few labeled nodes to ensure nodes that
belong to the same class to be grouped together, thereby achieving the best of
both worlds of semi-supervised and self-supervised methods. Specifically, GraFN
randomly samples support nodes from labeled nodes and anchor nodes from the
entire graph. Then, it minimizes the difference between two predicted class
distributions that are non-parametrically assigned by anchor-supports
similarity from two differently augmented graphs. We experimentally show that
GraFN surpasses both the semi-supervised and self-supervised methods in terms
of node classification on real-world graphs. The source code for GraFN is
available at this https URL.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：REM: Routing Entropy Minimization for Capsule Networks</b></summary>
  <p><b>编号</b>：[150]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01298</p>
  <p><b>作者</b>：Riccardo Renzulli,  Enzo Tartaglione,  Marco Grangetto</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：model parameters distribution towards low entropy configurations, also generate static parse trees, capsule networks build stronger relationships, inspired neural network model, capsule networks ambition</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Capsule Networks ambition is to build an explainable and
biologically-inspired neural network model. One of their main innovations
relies on the routing mechanism which extracts a parse tree: its main purpose
is to explicitly build relationships between capsules. However, their true
potential in terms of explainability has not surfaced yet: these relationships
are extremely heterogeneous and difficult to understand. This paper proposes
REM, a technique which minimizes the entropy of the parse tree-like structure,
improving its explainability. We accomplish this by driving the model
parameters distribution towards low entropy configurations, using a pruning
mechanism as a proxy. We also generate static parse trees with no performance
loss, showing that, with REM, Capsule Networks build stronger relationships
between capsules.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Explainable Online Lane Change Predictions on a Digital Twin with a  Layer Normalized LSTM and Layer-wise Relevance Propagation</b></summary>
  <p><b>编号</b>：[154]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01292</p>
  <p><b>作者</b>：Christoph Wehner,  Francis Powlesland,  Bashar Altakrouri,  Ute Schmid</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：hand without negatively affecting predictive effectiveness, core implementation includes consuming live data, layer normalized lstms using layer, manoeuvre anticipation go hand, layer normalized lstms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Artificial Intelligence and Digital Twins play an integral role in driving
innovation in the domain of intelligent driving. Long short-term memory (LSTM)
is a leading driver in the field of lane change prediction for manoeuvre
anticipation. However, the decision-making process of such models is complex
and non-transparent, hence reducing the trustworthiness of the smart solution.
This work presents an innovative approach and a technical implementation for
explaining lane change predictions of layer normalized LSTMs using Layer-wise
Relevance Propagation (LRP). The core implementation includes consuming live
data from a digital twin on a German highway, live predictions and explanations
of lane changes by extending LRP to layer normalized LSTMs, and an interface
for communicating and explaining the predictions to a human user. We aim to
demonstrate faithful, understandable, and adaptable explanations of lane change
prediction to increase the adoption and trustworthiness of AI systems that
involve humans. Our research also emphases that explainability and
state-of-the-art performance of ML models for manoeuvre anticipation go hand in
hand without negatively affecting predictive effectiveness.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：SPFNet:Subspace Pyramid Fusion Network for Semantic Segmentation</b></summary>
  <p><b>编号</b>：[160]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01278</p>
  <p><b>作者</b>：Mohammed A. M. Elhassan,  Chenhui Yang,  Chenxi Huang,  Tewodros Legesse Munea</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：help select category localization details, enhance communication across different sub, egca adopts shuffle attention mechanism, hardly extract sufficient context information, propose subspace pyramid fusion network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The encoder-decoder structure has significantly improved performance in many
vision tasks by fusing low-level and high-level feature maps. However, this
approach can hardly extract sufficient context information for pixel-wise
segmentation. In addition, extracting similar low-level features at multiple
scales could lead to redundant information. To tackle these issues, we propose
Subspace Pyramid Fusion Network (SPFNet). Specifically, we combine pyramidal
module and context aggregation module to exploit the impact of
multi-scale/global context information. At first, we construct a Subspace
Pyramid Fusion Module (SPFM) based on Reduced Pyramid Pooling (RPP). Then, we
propose the Efficient Global Context Aggregation (EGCA) module to capture
discriminative features by fusing multi-level global context features. Finally,
we add decoder-based subpixel convolution to retrieve the high-resolution
feature maps, which can help select category localization details. SPFM learns
separate RPP for each feature subspace to capture multi-scale feature
representations, which is more useful for semantic segmentation. EGCA adopts
shuffle attention mechanism to enhance communication across different
sub-features. Experimental results on two well-known semantic segmentation
datasets, including Camvid and Cityscapes, show that our proposed method is
competitive with other state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Aligning Silhouette Topology for Self-Adaptive 3D Human Pose Recovery</b></summary>
  <p><b>编号</b>：[161]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01276</p>
  <p><b>作者</b>：Mugalodi Rakesh,  Jogendra Nath Kundu,  Varun Jampani,  R. Venkatesh Babu</p>
  <p><b>备注</b>：NeurIPS 2021</p>
  <p><b>关键词</b>：alignment loss via distance field computation, existing 3d human pose estimation techniques, standard foreground silhouette estimation techniques, novel target adaptation framework, 3d pose supervision forms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Articulation-centric 2D/3D pose supervision forms the core training objective
in most existing 3D human pose estimation techniques. Except for synthetic
source environments, acquiring such rich supervision for each real target
domain at deployment is highly inconvenient. However, we realize that standard
foreground silhouette estimation techniques (on static camera feeds) remain
unaffected by domain-shifts. Motivated by this, we propose a novel target
adaptation framework that relies only on silhouette supervision to adapt a
source-trained model-based regressor. However, in the absence of any auxiliary
cue (multi-view, depth, or 2D pose), an isolated silhouette loss fails to
provide a reliable pose-specific gradient and requires to be employed in tandem
with a topology-centric loss. To this end, we develop a series of
convolution-friendly spatial transformations in order to disentangle a
topological-skeleton representation from the raw silhouette. Such a design
paves the way to devise a Chamfer-inspired spatial topological-alignment loss
via distance field computation, while effectively avoiding any gradient
hindering spatial-to-pointset mapping. Experimental results demonstrate our
superiority against prior-arts in self-adapting a source trained model to
diverse unlabeled target domains, such as a) in-the-wild datasets, b)
low-resolution image domains, and c) adversarially perturbed image domains (via
UAP).</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Multi-modality Associative Bridging through Memory: Speech Sound  Recollected from Face Video</b></summary>
  <p><b>编号</b>：[167]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01265</p>
  <p><b>作者</b>：Minsu Kim,  Joanna Hong,  Se Jin Park,  Yong Man Ro</p>
  <p><b>备注</b>：Published at ICCV 2021</p>
  <p><b>关键词</b>：associative bridge properly relates, target modal representations inside, modal bridging framework, provides rich information, e ., visual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we introduce a novel audio-visual multi-modal bridging
framework that can utilize both audio and visual information, even with
uni-modal inputs. We exploit a memory network that stores source (i.e., visual)
and target (i.e., audio) modal representations, where source modal
representation is what we are given, and target modal representations are what
we want to obtain from the memory network. We then construct an associative
bridge between source and target memories that considers the interrelationship
between the two memories. By learning the interrelationship through the
associative bridge, the proposed bridging framework is able to obtain the
target modal representations inside the memory network, even with the source
modal input only, and it provides rich information for its downstream tasks. We
apply the proposed framework to two tasks: lip reading and speech
reconstruction from silent video. Through the proposed associative bridge and
modality-specific memories, each task knowledge is enriched with the recalled
audio context, achieving state-of-the-art performance. We also verify that the
associative bridge properly relates the source and target memories.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：BatchFormerV2: Exploring Sample Relationships for Dense Representation  Learning</b></summary>
  <p><b>编号</b>：[173]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01254</p>
  <p><b>作者</b>：Zhi Hou,  Baosheng Yu,  Chaoyue Wang,  Yibing Zhan,  Dacheng Tao</p>
  <p><b>备注</b>：Tech report</p>
  <p><b>关键词</b>：two important dense prediction tasks, batchformerv2 consistently improves current detr, overcoming data scarcity challenges, also visual recognition applications, popular visual recognition tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Attention mechanisms have been very popular in deep neural networks, where
the Transformer architecture has achieved great success in not only natural
language processing but also visual recognition applications. Recently, a new
Transformer module, applying on batch dimension rather than spatial/channel
dimension, i.e., BatchFormer [18], has been introduced to explore sample
relationships for overcoming data scarcity challenges. However, it only works
with image-level representations for classification. In this paper, we devise a
more general batch Transformer module, BatchFormerV2, which further enables
exploring sample relationships for dense representation learning. Specifically,
when applying the proposed module, it employs a two-stream pipeline during
training, i.e., either with or without a BatchFormerV2 module, where the
batchformer stream can be removed for testing. Therefore, the proposed method
is a plug-and-play module and can be easily integrated into different vision
Transformers without any extra inference cost. Without bells and whistles, we
show the effectiveness of the proposed method for a variety of popular visual
recognition tasks, including image classification and two important dense
prediction tasks: object detection and panoptic segmentation. Particularly,
BatchFormerV2 consistently improves current DETR-based detection methods (e.g.,
DETR, Deformable-DETR, Conditional DETR, and SMCA) by over 1.3%. Code will be
made publicly available.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Why Exposure Bias Matters: An Imitation Learning Perspective of Error  Accumulation in Language Generation</b></summary>
  <p><b>编号</b>：[199]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01171</p>
  <p><b>作者</b>：Kushal Arora,  Layla El Asri,  Hareesh Bahuleyan,  Jackie Chi Kit Cheung</p>
  <p><b>备注</b>：Accepted in Findings of ACL 2022</p>
  <p><b>关键词</b>：current language generation models suffer, poor generation quality, generation procedure mismatch, imitation learning perspective, exposure bias leads</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Current language generation models suffer from issues such as repetition,
incoherence, and hallucinations. An often-repeated hypothesis is that this
brittleness of generation models is caused by the training and the generation
procedure mismatch, also referred to as exposure bias. In this paper, we verify
this hypothesis by analyzing exposure bias from an imitation learning
perspective. We show that exposure bias leads to an accumulation of errors,
analyze why perplexity fails to capture this accumulation, and empirically show
that this accumulation results in poor generation quality. Source code to
reproduce these experiments is available at
this https URL</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Pragmatic constraints and pronoun reference disambiguation: the possible  and the impossible</b></summary>
  <p><b>编号</b>：[200]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01166</p>
  <p><b>作者</b>：Ernest Davis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：natural text often refer, discourse often requires, parallel syntactic structures, implicitly mentioned previously, extended literary texts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pronoun disambiguation in understanding text and discourse often requires the
application of both general pragmatic knowledge and context-specific
information. In AI and linguistics research, this has mostly been studied in
cases where the referent is explicitly stated in the preceding text nearby.
However, pronouns in natural text often refer to entities, collections, or
events that are only implicitly mentioned previously; in those cases the need
to use pragmatic knowledge to disambiguate becomes much more acute and the
characterization of the knowledge becomes much more difficult. Extended
literary texts at times employ both extremely complex patterns of reference and
extremely rich and subtle forms of knowledge. Indeed, it is occasionally
possible to have a pronoun that is far separated from its referent in a text.
In the opposite direction, pronoun use is affected by considerations of focus
of attention and by formal constraints such as a preference for parallel
syntactic structures; these can be so strong that no pragmatic knowledge
suffices to overrule them.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Best-Response Bayesian Reinforcement Learning with Bayes-adaptive POMDPs  for Centaurs</b></summary>
  <p><b>编号</b>：[202]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01160</p>
  <p><b>作者</b>：Mustafa Mert Çelikok,  Frans A. Oliehoek,  Samuel Kaski</p>
  <p><b>备注</b>：This paper is presented in part at the International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS) 2022</p>
  <p><b>关键词</b>：rational humans make better decisions reduces, modelled using bayesian best, machine must make sure, towards better decisions, preliminary theoretical analysis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Centaurs are half-human, half-AI decision-makers where the AI's goal is to
complement the human. To do so, the AI must be able to recognize the goals and
constraints of the human and have the means to help them. We present a novel
formulation of the interaction between the human and the AI as a sequential
game where the agents are modelled using Bayesian best-response models. We show
that in this case the AI's problem of helping bounded-rational humans make
better decisions reduces to a Bayes-adaptive POMDP. In our simulated
experiments, we consider an instantiation of our framework for humans who are
subjectively optimistic about the AI's future behaviour. Our results show that
when equipped with a model of the human, the AI can infer the human's bounds
and nudge them towards better decisions. We discuss ways in which the machine
can learn to improve upon its own limitations as well with the help of the
human. We identify a novel trade-off for centaurs in partially observable
tasks: for the AI's actions to be acceptable to the human, the machine must
make sure their beliefs are sufficiently aligned, but aligning beliefs might be
costly. We present a preliminary theoretical analysis of this trade-off and its
dependence on task structure.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Proactive Anomaly Detection for Robot Navigation with Multi-Sensor  Fusion</b></summary>
  <p><b>编号</b>：[209]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01146</p>
  <p><b>作者</b>：Tianchen Ji,  Arun Narenthiran Sivakumar,  Girish Chowdhary,  Katherine Driggs-Campbell</p>
  <p><b>备注</b>：Accepted by RA-L with ICRA 2022 option</p>
  <p><b>关键词</b>：reactive anomaly detection methods identify anomalous task executions based, field robot data demonstrates superior failure identification performance, mobile robots often produce anomalous behaviors, provide robust anomaly detection, proactive anomaly detection network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite the rapid advancement of navigation algorithms, mobile robots often
produce anomalous behaviors that can lead to navigation failures. The ability
to detect such anomalous behaviors is a key component in modern robots to
achieve high-levels of autonomy. Reactive anomaly detection methods identify
anomalous task executions based on the current robot state and thus lack the
ability to alert the robot before an actual failure occurs. Such an alert delay
is undesirable due to the potential damage to both the robot and the
surrounding objects. We propose a proactive anomaly detection network (PAAD)
for robot navigation in unstructured and uncertain environments. PAAD predicts
the probability of future failure based on the planned motions from the
predictive controller and the current observation from the perception module.
Multi-sensor signals are fused effectively to provide robust anomaly detection
in the presence of sensor occlusion as seen in field environments. Our
experiments on field robot data demonstrates superior failure identification
performance than previous methods, and that our model can capture anomalous
behaviors in real-time while maintaining a low false detection rate in
cluttered fields. Code, dataset, and video are available at
this https URL</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：A sequence-to-sequence approach for document-level relation extraction</b></summary>
  <p><b>编号</b>：[227]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01098</p>
  <p><b>作者</b>：John Giorgi,  Gary D. Bader,  Bo Wang</p>
  <p><b>备注</b>：Accepted to BioNLP 2022 @ ACL 2022</p>
  <p><b>关键词</b>：docre requires integrating information within, {\ small {\ url, {\ small {\ url, several popular biomedical datasets, https url }}}.</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Motivated by the fact that many relations cross the sentence boundary, there
has been increasing interest in document-level relation extraction (DocRE).
DocRE requires integrating information within and across sentences, capturing
complex interactions between mentions of entities. Most existing methods are
pipeline-based, requiring entities as input. However, jointly learning to
extract entities and relations can improve performance and be more efficient
due to shared parameters and training steps. In this paper, we develop a
sequence-to-sequence approach, seq2rel, that can learn the subtasks of DocRE
(entity extraction, coreference resolution and relation extraction) end-to-end,
replacing a pipeline of task-specific components. Using a simple strategy we
call entity hinting, we compare our approach to existing pipeline-based methods
on several popular biomedical datasets, in some cases exceeding their
performance. We also report the first end-to-end results on these datasets for
future comparison. Finally, we demonstrate that, under our model, an end-to-end
approach outperforms a pipeline-based approach. Our code, data and trained
models are available at {\small{\url{this https URL}}}.
An online demo is available at
{\small{\url{this https URL}}}.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Virtual Relational Knowledge Graphs for Recommendation</b></summary>
  <p><b>编号</b>：[231]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01089</p>
  <p><b>作者</b>：Lingyun Lu,  Bang Wang,  Zizhuo Zhang,  Shenghao Liu,  Han Xu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：first construct virtual relational graphs, sometimes one relation type involves, two public datasets validate, use every relation type, virtual relational knowledge graphs</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Incorporating knowledge graph as side information has become a new trend in
recommendation systems. Recent studies regard items as entities of a knowledge
graph and leverage graph neural networks to assist item encoding, yet by
considering each relation type individually. However, relation types are often
too many and sometimes one relation type involves too few entities. We argue
that it is not efficient nor effective to use every relation type for item
encoding. In this paper, we propose a VRKG4Rec model (Virtual Relational
Knowledge Graphs for Recommendation), which explicitly distinguish the
influence of different relations for item representation learning. We first
construct virtual relational graphs (VRKGs) by an unsupervised learning scheme.
We also design a local weighted smoothing (LWS) mechanism for encoding nodes,
which iteratively updates a node embedding only depending on the embedding of
its own and its neighbors, but involve no additional training parameters. We
also employ the LWS mechanism on a user-item bipartite graph for user
representation learning, which utilizes encodings of items with relational
knowledge to help training representations of users. Experiment results on two
public datasets validate that our VRKG4Rec model outperforms the
state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：Data Cards: Purposeful and Transparent Dataset Documentation for  Responsible AI</b></summary>
  <p><b>编号</b>：[237]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01075</p>
  <p><b>作者</b>：Mahima Pushkarna (1),  Andrew Zaldivar (1),  Oddur Kjartansson (1) ((1) Google Research)</p>
  <p><b>备注</b>：Submitted to ACM Conference on Fairness, Accountability, and Transparency 2022 (ACM FAccT 2022) 17 pages (including references) , 2 figures, 3 tables. Appendix A: 1 pages, 1 table; Appendix B: 1 page, 1 table; Appendix C: 5 Pages, 5 figures; Appendix D: 3 pages, 3 figures; Appendix E: 24 pages, 24 figures</p>
  <p><b>关键词</b>：using two case studies, decisions affecting model performance, industry moves towards large, support adoption across domains, present lessons learned</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As research and industry moves towards large-scale models capable of numerous
downstream tasks, the complexity of understanding multi-modal datasets that
give nuance to models rapidly increases. A clear and thorough understanding of
a dataset's origins, development, intent, ethical considerations and evolution
becomes a necessary step for the responsible and informed deployment of models,
especially those in people-facing contexts and high-risk domains. However, the
burden of this understanding often falls on the intelligibility, conciseness,
and comprehensiveness of the documentation. It requires consistency and
comparability across the documentation of all datasets involved, and as such
documentation must be treated as a user-centric product in and of itself. In
this paper, we propose Data Cards for fostering transparent, purposeful and
human-centered documentation of datasets within the practical contexts of
industry and research. Data Cards are structured summaries of essential facts
about various aspects of ML datasets needed by stakeholders across a dataset's
lifecycle for responsible AI development. These summaries provide explanations
of processes and rationales that shape the data and consequently the models,
such as upstream sources, data collection and annotation methods; training and
evaluation methods, intended use; or decisions affecting model performance. We
also present frameworks that ground Data Cards in real-world utility and
human-centricity. Using two case studies, we report on desirable
characteristics that support adoption across domains, organizational
structures, and audience groups. Finally, we present lessons learned from
deploying over 20 Data Cards.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Task2Dial: A Novel Task and Dataset for Commonsense enhanced Task-based  Dialogue Grounded in Documents</b></summary>
  <p><b>编号</b>：[242]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01061</p>
  <p><b>作者</b>：Carl Strathearn,  Dimitra Gkatzia</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：task2dial dataset poses new challenges, human reference texts show, 79 tokens per turn, task2dial dataset contains dialogues, generating requires planning based</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper proposes a novel task on commonsense-enhanced task-based dialogue
grounded in documents and describes the Task2Dial dataset, a novel dataset of
document-grounded task-based dialogues, where an Information Giver (IG)
provides instructions (by consulting a document) to an Information Follower
(IF), so that the latter can successfully complete the task. In this unique
setting, the IF can ask clarification questions which may not be grounded in
the underlying document and require commonsense knowledge to be answered. The
Task2Dial dataset poses new challenges: (1) its human reference texts show more
lexical richness and variation than other document-grounded dialogue datasets;
(2) generating from this set requires paraphrasing as instructional responses
might have been modified from the underlying document; (3) requires commonsense
knowledge, since questions might not necessarily be grounded in the document;
(4) generating requires planning based on context, as task steps need to be
provided in order. The Task2Dial dataset contains dialogues with an average
$18.15$ number of turns and 19.79 tokens per turn, as compared to 12.94 and 12
respectively in existing datasets. As such, learning from this dataset promises
more natural, varied and less template-like system utterances.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：AutoOpt: A Methodological Framework of Automatically Designing  Metaheuristics for Optimization Problems</b></summary>
  <p><b>编号</b>：[258]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00998</p>
  <p><b>作者</b>：Qi Zhao,  Bai Yan,  Yuhui Shi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：world case study demonstrates autoopt, autoopt benefits academic researchers, solving various optimization problems, various optimization problems, practical users struggling</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Metaheuristics are gradient-free and problem-independent search algorithms.
They have gained huge success in solving various optimization problems in
academia and industry. Automated metaheuristic design is a promising
alternative to human-made design. This paper proposes a general and
comprehensive methodological framework, AutoOpt, for automatically designing
metaheuristics for various optimization problems. AutoOpt consists of: 1) a
bi-level criterion to evaluate the designed algorithms' performance; 2) a
general schema of the decision space from where the algorithms will be
designed; 3) a mixed graph- and real number-based representation to represent
the designed algorithms; and 4) a model-free method to conduct the design
process. AutoOpt benefits academic researchers and practical users struggling
to design metaheuristics for optimization problems. A real-world case study
demonstrates AutoOpt's effectiveness and efficiency.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：FedGBF: An efficient vertical federated learning framework via gradient  boosting and bagging</b></summary>
  <p><b>编号</b>：[270]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00976</p>
  <p><b>作者</b>：Yujin Han,  Pan Du,  Kai Yang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：existing federated boosting model sequentially builds, federated bagging model saves time, vertically federated setting termed, high interactive communication costs, attracted increasing attention recently</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Federated learning, conducive to solving data privacy and security problems,
has attracted increasing attention recently. However, the existing federated
boosting model sequentially builds a decision tree model with the weak base
learner, resulting in redundant boosting steps and high interactive
communication costs. In contrast, the federated bagging model saves time by
building multi-decision trees in parallel, but it suffers from performance
loss. With the aim of obtaining an outstanding performance with less time cost,
we propose a novel model in a vertically federated setting termed as Federated
Gradient Boosting Forest (FedGBF). FedGBF simultaneously integrates the
boosting and bagging's preponderance by building the decision trees in parallel
as a base learner for boosting. Subsequent to FedGBF, the problem of
hyperparameters tuning is rising. Then we propose the Dynamic FedGBF, which
dynamically changes each forest's parameters and thus reduces the complexity.
Finally, the experiments based on the benchmark datasets demonstrate the
superiority of our method.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：Inverse is Better! Fast and Accurate Prompt for Few-shot Slot Tagging</b></summary>
  <p><b>编号</b>：[311]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00885</p>
  <p><b>作者</b>：Yutai Hou,  Cheng Chen,  Xianzhen Luo,  Bohan Li,  Wanxiang Che</p>
  <p><b>备注</b>：Accepted by ACL-findings 2022</p>
  <p><b>关键词</b>：reversely predict slot values given slot types, prompting methods recently achieve impressive success, classic prompts mapping tokens, methods modify input samples, since slot tagging samples</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Prompting methods recently achieve impressive success in few-shot learning.
These methods modify input samples with prompt sentence pieces, and decode
label tokens to map samples to corresponding labels. However, such a paradigm
is very inefficient for the task of slot tagging. Since slot tagging samples
are multiple consecutive words in a sentence, the prompting methods have to
enumerate all n-grams token spans to find all the possible slots, which greatly
slows down the prediction. To tackle this, we introduce an inverse paradigm for
prompting. Different from the classic prompts mapping tokens to labels, we
reversely predict slot values given slot types. Such inverse prompting only
requires a one-turn prediction for each slot type and greatly speeds up the
prediction. Besides, we propose a novel Iterative Prediction Strategy, from
which the model learns to refine predictions by considering the relations
between different slot types. We find, somewhat surprisingly, the proposed
method not only predicts faster but also significantly improves the effect
(improve over 6.1 F1-scores on 10-shot setting) and achieves new
state-of-the-art performance.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：SciNoBo : A Hierarchical Multi-Label Classifier of Scientific  Publications</b></summary>
  <p><b>编号</b>：[312]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00880</p>
  <p><b>作者</b>：Nikolaos Gialitsis,  Sotiris Kotitsas,  Haris Papageorgiou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：existing works address classification either, common multilayer network structure made, classifying scientific publications according, novel classification system, organize scientific literature</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Classifying scientific publications according to Field-of-Science (FoS)
taxonomies is of crucial importance, allowing funders, publishers, scholars,
companies and other stakeholders to organize scientific literature more
effectively. Most existing works address classification either at venue level
or solely based on the textual content of a research publication. We present
SciNoBo, a novel classification system of publications to predefined FoS
taxonomies, leveraging the structural properties of a publication and its
citations and references organised in a multilayer network. In contrast to
other works, our system supports assignments of publications to multiple fields
by considering their multidisciplinarity potential. By unifying publications
and venues under a common multilayer network structure made up of citing and
publishing relationships, classifications at the venue-level can be augmented
with publication-level classifications. We evaluate SciNoBo on a publications'
dataset extracted from Microsoft Academic Graph and we perform a comparative
analysis against a state-of-the-art neural-network baseline. The results reveal
that our proposed system is capable of producing high-quality classifications
of publications.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：CTRLEval: An Unsupervised Reference-Free Metric for Evaluating  Controlled Text Generation</b></summary>
  <p><b>编号</b>：[319]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00862</p>
  <p><b>作者</b>：Pei Ke,  Hao Zhou,  Yankai Lin,  Peng Li,  Jie Zhou,  Xiaoyan Zhu,  Minlie Huang</p>
  <p><b>备注</b>：Accepted by ACL 2022 (Main Conference)</p>
  <p><b>关键词</b>：whereas supervised ones may overfit task, evaluating controlled text generation models, evaluates controlled text generation, trained language model without, multiple text infilling tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing reference-free metrics have obvious limitations for evaluating
controlled text generation models. Unsupervised metrics can only provide a
task-agnostic evaluation result which correlates weakly with human judgments,
whereas supervised ones may overfit task-specific data with poor generalization
ability to other datasets. In this paper, we propose an unsupervised
reference-free metric called CTRLEval, which evaluates controlled text
generation from different aspects by formulating each aspect into multiple text
infilling tasks. On top of these tasks, the metric assembles the generation
probabilities from a pre-trained language model without any model training.
Experimental results show that our metric has higher correlations with human
judgments than other baselines, while obtaining better generalization of
evaluating generated texts from different models and with different qualities.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Rotated Object Detection via Scale-invariant Mahalanobis Distance in  Aerial Images</b></summary>
  <p><b>编号</b>：[328]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00840</p>
  <p><b>作者</b>：Siyang Wen,  Wei Guo,  Ruijie Wu,  Yi Liu</p>
  <p><b>备注</b>：5 pages, 6 figures</p>
  <p><b>关键词</b>：new loss function called mahalanobis distance loss, rotated object detection usually use ln, meaningful yet challenging task, detection metric rotational intersection, parameter rotated object detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Rotated object detection in aerial images is a meaningful yet challenging
task as objects are densely arranged and have arbitrary orientations. The
eight-parameter (coordinates of box vectors) methods in rotated object
detection usually use ln-norm losses (L1 loss, L2 loss, and smooth L1 loss) as
loss functions. As ln-norm losses are mainly based on non-scale-invariant
Minkowski distance, using ln-norm losses will lead to inconsistency with the
detection metric rotational Intersection-over-Union (IoU) and training
instability. To address the problems, we use Mahalanobis distance to calculate
loss between the predicted and the target box vertices' vectors, proposing a
new loss function called Mahalanobis Distance Loss (MDL) for eight-parameter
rotated object detection. As Mahalanobis distance is scale-invariant, MDL is
more consistent with detection metric than ln-norm losses and more stable
during training. To alleviate the problem of boundary discontinuity like all
other eight-parameter methods, we further take the minimum loss value to make
MDL continuous at boundary cases. We achieve state-of-art performance on
DOTA-v1.0 with the proposed method MDL. Furthermore, with the comparative
experiment of smooth L1 loss under the same condi-tion, we find that MDL
performs better in rotated object detection.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：HLDC: Hindi Legal Documents Corpus</b></summary>
  <p><b>编号</b>：[341]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00806</p>
  <p><b>作者</b>：Arnav Kapoor,  Mudit Dhawan,  Anmol Goel,  T.H. Arjun,  Akshala Bhatnagar,  Vibhu Agrawal,  Amul Agrawal,  Arnab Bhattacharya,  Ponnurangam Kumaraguru,  Ashutosh Modi</p>
  <p><b>备注</b>：16 Pages, Accepted at ACL 2022 Findings</p>
  <p><b>关键词</b>：many populous countries including india, could process legal documents, mtl models use summarization, hindi legal documents corpus, augment legal practitioners</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Many populous countries including India are burdened with a considerable
backlog of legal cases. Development of automated systems that could process
legal documents and augment legal practitioners can mitigate this. However,
there is a dearth of high-quality corpora that is needed to develop such
data-driven systems. The problem gets even more pronounced in the case of low
resource languages such as Hindi. In this resource paper, we introduce the
Hindi Legal Documents Corpus (HLDC), a corpus of more than 900K legal documents
in Hindi. Documents are cleaned and structured to enable the development of
downstream applications. Further, as a use-case for the corpus, we introduce
the task of bail prediction. We experiment with a battery of models and propose
a Multi-Task Learning (MTL) based model for the same. MTL models use
summarization as an auxiliary task along with bail prediction as the main task.
Experiments with different models are indicative of the need for further
research in this area. We release the corpus and model implementation code with
this paper: this https URL</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Introduction to the Artificial Intelligence that can be applied to the  Network Automation Journey</b></summary>
  <p><b>编号</b>：[343]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00800</p>
  <p><b>作者</b>：Gilbert Moisio,  Alexandre Gonzalvez,  Noam Zeitoun</p>
  <p><b>备注</b>：20 pages</p>
  <p><b>关键词</b>：refine features need, computer network world, artificial intelligence comes, new way, implement algorithms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The computer network world is changing and the NetDevOps approach has brought
the dynamics of applications and systems into the field of communication
infrastructure. Businesses are changing and businesses are faced with
difficulties related to the diversity of hardware and software that make up
those infrastructures. The "Intent-Based Networking - Concepts and Definitions"
document describes the different parts of the ecosystem that could be involved
in NetDevOps. The recognize, generate intent, translate and refine features
need a new way to implement algorithms. This is where artificial intelligence
comes in.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：SAD: A Large-scale Dataset towards Airport Detection in Synthetic  Aperture Radar Images</b></summary>
  <p><b>编号</b>：[350]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00790</p>
  <p><b>作者</b>：Fan Zhang,  Daochang Wang,  Fei Ma,  Qiang Yin,  Deliang Xiang,  Yongsheng Zhou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：art airport area detection algorithms, covers 104 airfield instances, multiple deep learning approach, publicly available sar dataset, contains 624 sar images</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Airports have an important role in both military and civilian domains. The
synthetic aperture radar (SAR) based airport detection has received increasing
attention in recent years. However, due to the high cost of SAR imaging and
annotation process, there is no publicly available SAR dataset for airport
detection. As a result, deep learning methods have not been fully used in
airport detection tasks. To provide a benchmark for airport detection research
in SAR images, this paper introduces a large-scale SAR Airport Dataset (SAD).
In order to adequately reflect the demands of real world applications, it
contains 624 SAR images from Sentinel 1B and covers 104 airfield instances with
different scales, orientations and shapes. The experiments of multiple deep
learning approach on this dataset proves its effectiveness. It developing
state-of-the-art airport area detection algorithms or other relevant tasks.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Speaker adaptation for Wav2vec2 based dysarthric ASR</b></summary>
  <p><b>编号</b>：[356]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00770</p>
  <p><b>作者</b>：Murali Karthick Baskar,  Tim Herzig,  Diana Nguyen,  Mireia Diez,  Tim Polzehl,  Lukáš Burget,  Jan "Honza'' Černocký</p>
  <p><b>备注</b>：Submitted to INTERSPEECH 2022</p>
  <p><b>关键词</b>：experimental analysis show steady improvements using, proposed approach across diverse domains, tuning wav2vec2 using fmllr features, readily available pretrained models, posed major challenges due</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Dysarthric speech recognition has posed major challenges due to lack of
training data and heavy mismatch in speaker characteristics. Recent ASR systems
have benefited from readily available pretrained models such as wav2vec2 to
improve the recognition performance. Speaker adaptation using fMLLR and
xvectors have provided major gains for dysarthric speech with very little
adaptation data. However, integration of wav2vec2 with fMLLR features or
xvectors during wav2vec2 finetuning is yet to be explored. In this work, we
propose a simple adaptation network for fine-tuning wav2vec2 using fMLLR
features. The adaptation network is also flexible to handle other speaker
adaptive features such as xvectors. Experimental analysis show steady
improvements using our proposed approach across all impairment severity levels
and attains 57.72\% WER for high severity in UASpeech dataset. We also
performed experiments on German dataset to substantiate the consistency of our
proposed approach across diverse domains.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Safe Reinforcement Learning via Shielding for POMDPs</b></summary>
  <p><b>编号</b>：[361]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00755</p>
  <p><b>作者</b>：Steven Carr,  Nils Jansen,  Sebastian Junges,  Ufuk Topcu</p>
  <p><b>备注</b>：15 pages, 15 Figures, 6 Tables</p>
  <p><b>关键词</b>：art generally assumes perfect sensing capabilities, called shields provide formal safety guarantees, partially observable markov decision processes, magnitude fewer training episodes, art deep rl algorithms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reinforcement learning (RL) in safety-critical environments requires an agent
to avoid decisions with catastrophic consequences. Various approaches
addressing the safety of RL exist to mitigate this problem. In particular,
so-called shields provide formal safety guarantees on the behavior of RL agents
based on (partial) models of the agents' environment. Yet, the state-of-the-art
generally assumes perfect sensing capabilities of the agents, which is
unrealistic in real-life applications. The standard models to capture scenarios
with limited sensing are partially observable Markov decision processes
(POMDPs). Safe RL for these models remains an open problem so far. We propose
and thoroughly evaluate a tight integration of formally-verified shields for
POMDPs with state-of-the-art deep RL algorithms and create an efficacious
method that safely learns policies under partial observability. We empirically
demonstrate that an RL agent using a shield, beyond being safe, converges to
higher values of expected reward. Moreover, shielded agents need an order of
magnitude fewer training episodes than unshielded agents, especially in
challenging sparse-reward settings.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：RFID-Based Indoor Spatial Query Evaluation with Bayesian Filtering  Techniques</b></summary>
  <p><b>编号</b>：[365]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00747</p>
  <p><b>作者</b>：Bo Hui,  Wenlu Wang,  Jiao Yu,  Zhitao Gong,  Wei-Shinn Ku,  Min-Te Sun,  Hua Lu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：cannot apply existing spatial query evaluation techniques devised, develop efficient indoor spatial query algorithms, evaluate indoor spatial queries effectively, evaluating indoor spatial queries, develop innovative indoor range</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>People spend a significant amount of time in indoor spaces (e.g., office
buildings, subway systems, etc.) in their daily lives. Therefore, it is
important to develop efficient indoor spatial query algorithms for supporting
various location-based applications. However, indoor spaces differ from outdoor
spaces because users have to follow the indoor floor plan for their movements.
In addition, positioning in indoor environments is mainly based on sensing
devices (e.g., RFID readers) rather than GPS devices. Consequently, we cannot
apply existing spatial query evaluation techniques devised for outdoor
environments for this new challenge. Because Bayesian filtering techniques can
be employed to estimate the state of a system that changes over time using a
sequence of noisy measurements made on the system, in this research, we propose
the Bayesian filtering-based location inference methods as the basis for
evaluating indoor spatial queries with noisy RFID raw data. Furthermore, two
novel models, indoor walking graph model and anchor point indexing model, are
created for tracking object locations in indoor environments. Based on the
inference method and tracking models, we develop innovative indoor range and k
nearest neighbor (kNN) query algorithms. We validate our solution through use
of both synthetic data and real-world data. Our experimental results show that
the proposed algorithms can evaluate indoor spatial queries effectively and
efficiently. We open-source the code, data, and floor plan at
this https URL.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Testing Feedforward Neural Networks Training Programs</b></summary>
  <p><b>编号</b>：[385]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00694</p>
  <p><b>作者</b>：Houssem Ben Braiek,  Foutse Khomh</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：novel problem requires significant engineering work, current automated test data generators search, revealing several coding bugs, propose practical verification routines, world buggy dl programs</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Nowadays, we are witnessing an increasing effort to improve the performance
and trustworthiness of Deep Neural Networks (DNNs), with the aim to enable
their adoption in safety critical systems such as self-driving cars. Multiple
testing techniques are proposed to generate test cases that can expose
inconsistencies in the behavior of DNN models. These techniques assume
implicitly that the training program is bug-free and appropriately configured.
However, satisfying this assumption for a novel problem requires significant
engineering work to prepare the data, design the DNN, implement the training
program, and tune the hyperparameters in order to produce the model for which
current automated test data generators search for corner-case behaviors. All
these model training steps can be error-prone. Therefore, it is crucial to
detect and correct errors throughout all the engineering steps of DNN-based
software systems and not only on the resulting DNN model. In this paper, we
gather a catalog of training issues and based on their symptoms and their
effects on the behavior of the training program, we propose practical
verification routines to detect the aforementioned issues, automatically, by
continuously validating that some important properties of the learning dynamics
hold during the training. Then, we design, TheDeepChecker, an end-to-end
property-based debugging approach for DNN training programs. We assess the
effectiveness of TheDeepChecker on synthetic and real-world buggy DL programs
and compare it with Amazon SageMaker Debugger (SMD). Results show that
TheDeepChecker's on-execution validation of DNN-based program's properties
succeeds in revealing several coding bugs and system misconfigurations, early
on and at a low cost. Moreover, TheDeepChecker outperforms the SMD's offline
rules verification on training logs in terms of detection accuracy and DL bugs
coverage.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Hysteresis-Based RL: Robustifying Reinforcement Learning-based Control  Policies via Hybrid Control</b></summary>
  <p><b>编号</b>：[398]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00654</p>
  <p><b>作者</b>：Jan de Priester,  Ricardo G. Sanfelice,  Nathan van de Wouw</p>
  <p><b>备注</b>：This paper has been accepted for publication at the 2022 American Control Conference (ACC)</p>
  <p><b>关键词</b>：algorithms may lack robustness guarantees, proximal policy optimization, new hybrid algorithm, hyrl ), augmenting, deriving control policies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reinforcement learning (RL) is a promising approach for deriving control
policies for complex systems. As we show in two control problems, the derived
policies from using the Proximal Policy Optimization (PPO) and Deep Q-Network
(DQN) algorithms may lack robustness guarantees. Motivated by these issues, we
propose a new hybrid algorithm, which we call Hysteresis-Based RL (HyRL),
augmenting an existing RL algorithm with hysteresis switching and two stages of
learning. We illustrate its properties in two examples for which PPO and DQN
fail.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Explainable and Interpretable Diabetic Retinopathy Classification Based  on Neural-Symbolic Learning</b></summary>
  <p><b>编号</b>：[409]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00624</p>
  <p><b>作者</b>：Se-In Jang,  Michael J.A. Girard,  Alexandre H. Thiery</p>
  <p><b>备注</b>：Published in AAAI-22 Workshop</p>
  <p><b>关键词</b>：proposed explaindr method exhibits promising performance, diabetic retinopathy classification dataset show, include humanreadable features obtained, diabetic retinopathy characteristics related, interpretable diabetic retinopathy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we propose an explainable and interpretable diabetic
retinopathy (ExplainDR) classification model based on neural-symbolic learning.
To gain explainability, a highlevel symbolic representation should be
considered in decision making. Specifically, we introduce a human-readable
symbolic representation, which follows a taxonomy style of diabetic retinopathy
characteristics related to eye health conditions to achieve explainability. We
then include humanreadable features obtained from the symbolic representation
in the disease prediction. Experimental results on a diabetic retinopathy
classification dataset show that our proposed ExplainDR method exhibits
promising performance when compared to that from state-of-the-art methods
applied to the IDRiD dataset, while also providing interpretability and
explainability.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：CogNGen: Constructing the Kernel of a Hyperdimensional Predictive  Processing Cognitive Architecture</b></summary>
  <p><b>编号</b>：[410]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00619</p>
  <p><b>作者</b>：Alexander Ororbia,  M. Alex Kelly</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：future work includes testing cogngen, efficiently scaling hyperdimensional memory models, combines two neurobiologically plausible, modern machine learning techniques, cognitive neural generative system</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a new cognitive architecture that combines two neurobiologically
plausible, computational models: (1) a variant of predictive processing known
as neural generative coding (NGC) and (2) hyperdimensional, vector-symbolic
models of human memory. We draw inspiration from well-known cognitive
architectures such as ACT-R, Soar, Leabra, and Spaun/Nengo. Our cognitive
architecture, the COGnitive Neural GENerative system (CogNGen), is in broad
agreement with these architectures, but provides a level of detail between
ACT-R's high-level, symbolic description of human cognition and Spaun's
low-level neurobiological description. CogNGen creates the groundwork for
developing agents that learn continually from diverse tasks and model human
performance at larger scales than what is possible with existent cognitive
architectures. We aim to develop a cognitive architecture that has the power of
modern machine learning techniques while retaining long-term memory,
single-trial learning, transfer-learning, planning, and other capacities
associated with high-level cognition. We test CogNGen on a set of maze-learning
tasks, including mazes that test short-term memory and planning, and find that
the addition of vector-symbolic models of memory improves the ability of the
NGC reinforcement learning model to master the maze task. Future work includes
testing CogNGen on more tasks and exploring methods for efficiently scaling
hyperdimensional memory models to lifetime learning.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Optimize Deep Learning Models for Prediction of Gene Mutations Using  Unsupervised Clustering</b></summary>
  <p><b>编号</b>：[415]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01593</p>
  <p><b>作者</b>：Zihan Chen,  Xingyu Li,  Miaomiao Yang,  Hong Zhang,  Xu Steven Xu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：proposed algorithm outperformed two recently published baseline algorithms leveraging unsupervised clustering, image patches could help identify predictive patches, environment may provide better prediction ability, wsi based method without selection, slide digital pathology images</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning has become the mainstream methodological choice for analyzing
and interpreting whole-slide digital pathology images (WSIs). It is commonly
assumed that tumor regions carry most predictive information. In this paper, we
proposed an unsupervised clustering-based multiple-instance learning, and apply
our method to develop deep-learning models for prediction of gene mutations
using WSIs from three cancer types in The Cancer Genome Atlas (TCGA) studies
(CRC, LUAD, and HNSCC). We showed that unsupervised clustering of image patches
could help identify predictive patches, exclude patches lack of predictive
information, and therefore improve prediction on gene mutations in all three
different cancer types, compared with the WSI based method without selection of
image patches and models based on only tumor regions. Additionally, our
proposed algorithm outperformed two recently published baseline algorithms
leveraging unsupervised clustering to assist model prediction. The
unsupervised-clustering-based approach for mutation prediction allows
identification of the spatial regions related to mutation of a specific gene
via the resolved probability scores, highlighting the heterogeneity of a
predicted genotype in the tumor microenvironment. Finally, our study also
demonstrated that selection of tumor regions of WSIs is not always the best way
to identify patches for prediction of gene mutations, and other tissue types in
the tumor micro-environment may provide better prediction ability for gene
mutations than tumor tissues.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：MOSRA: Joint Mean Opinion Score and Room Acoustics Speech Quality  Assessment</b></summary>
  <p><b>编号</b>：[428]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01345</p>
  <p><b>作者</b>：Karl El Hajal,  Milos Cernak,  Pablo Mainar</p>
  <p><b>备注</b>：Submitted to Interspeech 2022</p>
  <p><b>关键词</b>：overall mean opinion score, outside voice recording ),, g ., video call, joint training method enhances, dimensional speech quality metric</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The acoustic environment can degrade speech quality during communication
(e.g., video call, remote presentation, outside voice recording), and its
impact is often unknown. Objective metrics for speech quality have proven
challenging to develop given the multi-dimensionality of factors that affect
speech quality and the difficulty of collecting labeled data. Hypothesizing the
impact of acoustics on speech quality, this paper presents MOSRA: a
non-intrusive multi-dimensional speech quality metric that can predict room
acoustics parameters (SNR, STI, T60, DRR, and C50) alongside the overall mean
opinion score (MOS) for speech quality. By explicitly optimizing the model to
learn these room acoustics parameters, we can extract more informative features
and improve the generalization for the MOS task when the training data is
limited. Furthermore, we also show that this joint training method enhances the
blind estimation of room acoustics, improving the performance of current
state-of-the-art models. An additional side-effect of this joint prediction is
the improvement in the explainability of the predictions, which is a valuable
feature for many applications.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Monte Carlo Physarum Machine: Characteristics of Pattern Formation in  Continuous Stochastic Transport Networks</b></summary>
  <p><b>编号</b>：[432]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01256</p>
  <p><b>作者</b>：Oskar Elek,  Joseph N. Burchett,  J. Xavier Prochaska,  Angus G. Forbes</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：present monte carlo physarum machine, produce consistent 3d density maps, physarum polycephalum slime mold, like morphologies -- called, reconstructing continuous transport networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present Monte Carlo Physarum Machine: a computational model suitable for
reconstructing continuous transport networks from sparse 2D and 3D data. MCPM
is a probabilistic generalization of Jones's 2010 agent-based model for
simulating the growth of Physarum polycephalum slime mold. We compare MCPM to
Jones's work on theoretical grounds, and describe a task-specific variant
designed for reconstructing the large-scale distribution of gas and dark matter
in the Universe known as the Cosmic web. To analyze the new model, we first
explore MCPM's self-patterning behavior, showing a wide range of continuous
network-like morphologies -- called "polyphorms" -- that the model produces
from geometrically intuitive parameters. Applying MCPM to both simulated and
observational cosmological datasets, we then evaluate its ability to produce
consistent 3D density maps of the Cosmic web. Finally, we examine other
possible tasks where MCPM could be useful, along with several examples of
fitting to domain-specific data as proofs of concept.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Few Shot Protein Generation</b></summary>
  <p><b>编号</b>：[438]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01168</p>
  <p><b>作者</b>：Soumya Ram,  Tristan Bepler</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：protein transformer conditions sequence generation directly, generative approach accurately models epistasis, outperforms conventional family modeling approaches, fitting dedicated family models, alternative sequence modeling approaches</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present the MSA-to-protein transformer, a generative model of protein
sequences conditioned on protein families represented by multiple sequence
alignments (MSAs). Unlike existing approaches to learning generative models of
protein families, the MSA-to-protein transformer conditions sequence generation
directly on a learned encoding of the multiple sequence alignment,
circumventing the need for fitting dedicated family models. By training on a
large set of well-curated multiple sequence alignments in Pfam, our
MSA-to-protein transformer generalizes well to protein families not observed
during training and outperforms conventional family modeling approaches,
especially when MSAs are small. Our generative approach accurately models
epistasis and indels and allows for exact inference and efficient sampling
unlike other approaches. We demonstrate the protein sequence modeling
capabilities of our MSA-to-protein transformer and compare it with alternative
sequence modeling approaches in comprehensive benchmark experiments.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：Selective Kernel Attention for Robust Speaker Verification</b></summary>
  <p><b>编号</b>：[449]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.01005</p>
  <p><b>作者</b>：Sung Hwan Mun,  Jee-weon Jung,  Nam Soo Kim</p>
  <p><b>备注</b>：Submitted to INTERSPEECH 2022. 5 pages, 3 figures, 1 table</p>
  <p><b>关键词</b>：art speaker verification architectures adopt multi, propose three module variants using, ska mechanism whereby two modules, three different evaluation protocols, minimum detection cost function</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent state-of-the-art speaker verification architectures adopt multi-scale
processing and frequency-channel attention techniques. However, their full
potential may not have been exploited because these techniques' receptive
fields are fixed where most convolutional layers operate with specified kernel
sizes such as 1, 3 or 5. We aim to further improve this line of research by
introducing a selective kernel attention (SKA) mechanism. The SKA mechanism
allows each convolutional layer to adaptively select the kernel size in a
data-driven fashion based on an attention mechanism that exploits both
frequency and channel domain using the previous layer's output. We propose
three module variants using the SKA mechanism whereby two modules are applied
in front of an ECAPA-TDNN model, and the other is combined with the Res2Net
backbone block. Experimental results demonstrate that our proposed model
consistently outperforms the conventional counterpart on the three different
evaluation protocols in terms of both equal error rate and minimum detection
cost function. In addition, we present a detailed analysis that helps
understand how the SKA module works.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：UNetFormer: A Unified Vision Transformer Model and Pre-Training  Framework for 3D Medical Image Segmentation</b></summary>
  <p><b>编号</b>：[475]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00631</p>
  <p><b>作者</b>：Ali Hatamizadeh,  Ziyue Xu,  Dong Yang,  Wenqi Li,  Holger Roth,  Daguang Xu</p>
  <p><b>备注</b>：Tech. report, 12 pages, 3 figures</p>
  <p><b>关键词</b>：predict randomly masked volumetric tokens using contextual information, brain tumor segmentation using mri images, liver tumor segmentation task using, recently become popular due, decoder via skip connections</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision Transformers (ViT)s have recently become popular due to their
outstanding modeling capabilities, in particular for capturing long-range
information, and scalability to dataset and model sizes which has led to
state-of-the-art performance in various computer vision and medical image
analysis tasks. In this work, we introduce a unified framework consisting of
two architectures, dubbed UNetFormer, with a 3D Swin Transformer-based encoder
and Convolutional Neural Network (CNN) and transformer-based decoders. In the
proposed model, the encoder is linked to the decoder via skip connections at
five different resolutions with deep supervision. The design of proposed
architecture allows for meeting a wide range of trade-off requirements between
accuracy and computational cost. In addition, we present a methodology for
self-supervised pre-training of the encoder backbone via learning to predict
randomly masked volumetric tokens using contextual information of visible
tokens. We pre-train our framework on a cohort of $5050$ CT images, gathered
from publicly available CT datasets, and present a systematic investigation of
various components such as masking ratio and patch size that affect the
representation learning capability and performance of downstream tasks. We
validate the effectiveness of our pre-training approach by fine-tuning and
testing our model on liver and liver tumor segmentation task using the Medical
Segmentation Decathlon (MSD) dataset and achieve state-of-the-art performance
in terms of various segmentation metrics. To demonstrate its generalizability,
we train and test the model on BraTS 21 dataset for brain tumor segmentation
using MRI images and outperform other methods in terms of Dice score. Code:
this https URL</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Universal Lymph Node Detection in T2 MRI using Neural Networks</b></summary>
  <p><b>编号</b>：[479]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00622</p>
  <p><b>作者</b>：Tejas Sudharshan Mathai,  Sungwon Lee,  Thomas C. Shen,  Zhiyong Lu,  Ronald M. Summers</p>
  <p><b>备注</b>：Accepted at CARS 2022 (CAR track)</p>
  <p><b>关键词</b>：volumetric t2 mri using neural networks, 122 test t2 mri volumes revealed, without hard negative example mining, trained various neural network models, 5 fp per volume ).</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Purpose: Identification of abdominal Lymph Nodes (LN) that are suspicious for
metastasis in T2 Magnetic Resonance Imaging (MRI) scans is critical for staging
of lymphoproliferative diseases. Prior work on LN detection has been limited to
specific anatomical regions of the body (pelvis, rectum) in single MR slices.
Therefore, the development of a universal approach to detect LN in full T2 MRI
volumes is highly desirable.
Methods: In this study, a Computer Aided Detection (CAD) pipeline to
universally identify abdominal LN in volumetric T2 MRI using neural networks is
proposed. First, we trained various neural network models for detecting LN:
Faster RCNN with and without Hard Negative Example Mining (HNEM), FCOS,
FoveaBox, VFNet, and Detection Transformer (DETR). Next, we show that the
state-of-the-art (SOTA) VFNet model with Adaptive Training Sample Selection
(ATSS) outperforms Faster RCNN with HNEM. Finally, we ensembled models that
surpassed a 45% mAP threshold. We found that the VFNet model and one-stage
model ensemble can be interchangeably used in the CAD pipeline.
Results: Experiments on 122 test T2 MRI volumes revealed that VFNet achieved
a 51.1% mAP and 78.7% recall at 4 false positives (FP) per volume, while the
one-stage model ensemble achieved a mAP of 52.3% and sensitivity of 78.7% at
4FP.
Conclusion: Our contribution is a CAD pipeline that detects LN in T2 MRI
volumes, resulting in a sensitivity improvement of $\sim$14 points over the
current SOTA method for LN detection (sensitivity of 78.7% at 4 FP vs. 64.6% at
5 FP per volume).</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：Visual explanations for polyp detection: How medical doctors assess  intrinsic versus extrinsic explanations</b></summary>
  <p><b>编号</b>：[482]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2204.00617</p>
  <p><b>作者</b>：Steven Hicks,  Andrea Storås,  Michael Riegler,  Cise Midoglu,  Malek Hammou,  Thomas de Lange,  Sravanthi Parasa,  Pål Halvorsen,  Inga Strümke</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：recent years achieved immense success, making medical professionals highly skeptical, gastrointestinal disease detection use case, art explainable artificial intelligence methods, compare two different categories</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning has in recent years achieved immense success in all areas of
computer vision and has the potential of assisting medical doctors in analyzing
visual content for disease and other abnormalities. However, the current state
of deep learning is very much a black box, making medical professionals highly
skeptical about integrating these methods into clinical practice. Several
methods have been proposed in order to shine some light onto these black boxes,
but there is no consensus on the opinion of the medical doctors that will
consume these explanations. This paper presents a study asking medical doctors
about their opinion of current state-of-the-art explainable artificial
intelligence methods when applied to a gastrointestinal disease detection use
case. We compare two different categories of explanation methods, intrinsic and
extrinsic, and gauge their opinion of the current value of these explanations.
The results indicate that intrinsic explanations are preferred and that
explanation.</p>
  </details>
</details>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">徐耀彬</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://louishsu.xyz/2022/04/06/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">http://louishsu.xyz/2022/04/06/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://louishsu.xyz" target="_blank">LOUIS' BLOG</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2021/10/22/%E4%B8%AD%E5%9B%BD%E6%B3%95%E5%BE%8B%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E8%AF%84%E6%B5%8B(CAIL2021)%EF%BC%9A%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96(Rank2).html"><img class="next-cover" src="http://cail.cipsc.org.cn/img/index_mainpic.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">徐耀彬</div><div class="author-info__description">做知识的原创者！</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">11</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/isLouisHsu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/isLouisHsu" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:is.louishsu@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">记录和分享一些学习和开源内容，若有问题可通过邮箱is.louishsu@foxmail.com联系，欢迎交流！！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">统计</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">计算机视觉</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">自然语言处理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">机器学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">人工智能</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/04/06/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2022-04-06)"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv每日速递(2022-04-06)"/></a><div class="content"><a class="title" href="/2022/04/06/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2022-04-06)">Arxiv每日速递(2022-04-06)</a><time datetime="2022-04-06T00:41:11.786Z" title="发表于 2022-04-06 08:41:11">2022-04-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/10/22/%E4%B8%AD%E5%9B%BD%E6%B3%95%E5%BE%8B%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E8%AF%84%E6%B5%8B(CAIL2021)%EF%BC%9A%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96(Rank2).html" title="中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)"><img src="http://cail.cipsc.org.cn/img/index_mainpic.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)"/></a><div class="content"><a class="title" href="/2021/10/22/%E4%B8%AD%E5%9B%BD%E6%B3%95%E5%BE%8B%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E8%AF%84%E6%B5%8B(CAIL2021)%EF%BC%9A%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96(Rank2).html" title="中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)">中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)</a><time datetime="2021-10-22T14:29:06.000Z" title="发表于 2021-10-22 22:29:06">2021-10-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/05/19/%E5%85%A8%E7%90%83%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0%E5%A4%A7%E8%B5%9B%E3%80%90%E8%B5%9B%E9%81%93%E4%B8%80%E3%80%91%EF%BC%9A%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E6%8A%A5%E5%91%8A%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B.html" title="全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测"><img src="https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161037709574435991610377095138.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测"/></a><div class="content"><a class="title" href="/2021/05/19/%E5%85%A8%E7%90%83%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0%E5%A4%A7%E8%B5%9B%E3%80%90%E8%B5%9B%E9%81%93%E4%B8%80%E3%80%91%EF%BC%9A%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E6%8A%A5%E5%91%8A%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B.html" title="全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测">全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测</a><time datetime="2021-05-19T09:57:06.000Z" title="发表于 2021-05-19 17:57:06">2021-05-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/09/16/%E8%AF%A6%E8%A7%A3%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%EF%BC%9ALSTM-CRF.html" title="详解命名实体识别模型：LSTM-CRF"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fpic1.zhimg.com%2Fv2-1ed6a10dbb239a148e42df088c5870a6_1440w.jpg%3Fsource%3D172ae18b&amp;refer=http%3A%2F%2Fpic1.zhimg.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1638183974&amp;t=4632f13fd487ed1cfcb12d67a6b71e52" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="详解命名实体识别模型：LSTM-CRF"/></a><div class="content"><a class="title" href="/2020/09/16/%E8%AF%A6%E8%A7%A3%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%EF%BC%9ALSTM-CRF.html" title="详解命名实体识别模型：LSTM-CRF">详解命名实体识别模型：LSTM-CRF</a><time datetime="2020-09-16T09:26:44.000Z" title="发表于 2020-09-16 17:26:44">2020-09-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/06/14/%E8%AE%B0%E4%B8%80%E6%AC%A1MySQL%E8%A7%A3%E5%86%B3%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E6%97%B6%E7%9A%84%E5%88%86%E8%A1%A8%E9%97%AE%E9%A2%98.html" title="记一次MySQL解决大数据存储时的分表问题"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="记一次MySQL解决大数据存储时的分表问题"/></a><div class="content"><a class="title" href="/2020/06/14/%E8%AE%B0%E4%B8%80%E6%AC%A1MySQL%E8%A7%A3%E5%86%B3%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E6%97%B6%E7%9A%84%E5%88%86%E8%A1%A8%E9%97%AE%E9%A2%98.html" title="记一次MySQL解决大数据存储时的分表问题">记一次MySQL解决大数据存储时的分表问题</a><time datetime="2020-06-14T15:24:27.000Z" title="发表于 2020-06-14 23:24:27">2020-06-14</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2022 By 徐耀彬</div><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender"title="主题采用butterfly"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr"title="本站使用JsDelivr为静态资源提供CDN加速"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub"title="本站项目由Gtihub托管"alt="img"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"alt="img"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></br></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    let initData = {
      el: '#twikoo-wrap',
      envId: 'blog-',
      region: ''
    }

    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    twikoo.init(initData)
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'blog-',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/机器学习/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐱 机器学习 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/自然语言处理/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 自然语言处理 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/竞赛相关/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 竞赛相关 (2)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/阅读笔记/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 阅读笔记 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="http://louishsu.xyz/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style>
  <script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/hexo-github-calendar@1.21/hexo_githubcalendar.js"></script>
  <script data-pjax>
        function GithubCalendarConfig(){
            var git_githubapiurl ="https://python-github-calendar-api.vercel.app/api?isLouisHsu";
            var git_color =['#ebedf0', '#fdcdec', '#fc9bd9', '#fa6ac5', '#f838b2', '#f5089f', '#c4067e', '#92055e', '#540336', '#48022f', '#30021f'];
            var git_user ="isLouisHsu";
            var parent_div_git = document.getElementById('recent-posts');
            var git_div_html = '<div class="recent-post-item" style="width:100%;height:auto;padding:10px;"><div id="github_loading" style="width:10%;height:100%;margin:0 auto;display: block"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"  viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animateTransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animateTransform></path></svg></div><div id="github_container"></div></div>';
            if(parent_div_git && location.pathname =='/'){
                console.log('已挂载github calendar')
                // parent_div_git.innerHTML=git_div_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",git_div_html) // 有报错，但不影响使用(支持pjax跳转)
            };
            GithubCalendar(git_githubapiurl,git_color,git_user)
        }
        if(document.getElementById('recent-posts')){
            GithubCalendarConfig()
        }
    </script>
    <style>#github_container{min-height:280px}@media screen and (max-width:650px) {#github_container{background-image:;min-height:0px}}</style>
    <style></style><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>