<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Arxiv每日速递(2022-06-21) | LOUIS' BLOG</title><meta name="author" content="徐耀彬"><meta name="copyright" content="徐耀彬"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。 统计 今日共更新290篇论文，其中：  77篇计算机视觉（cs.CV） 25篇自然语言处理（cs.CL） 126篇机器学习（cs.LG） 55篇人工智能（cs.AI）  计算机视觉    1. 标题：TAVA: Template-free Animatable Volume">
<meta property="og:type" content="article">
<meta property="og:title" content="Arxiv每日速递(2022-06-21)">
<meta property="og:url" content="http://louishsu.xyz/2022/06/21/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">
<meta property="og:site_name" content="LOUIS&#39; BLOG">
<meta property="og:description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。 统计 今日共更新290篇论文，其中：  77篇计算机视觉（cs.CV） 25篇自然语言处理（cs.CL） 126篇机器学习（cs.LG） 55篇人工智能（cs.AI）  计算机视觉    1. 标题：TAVA: Template-free Animatable Volume">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png">
<meta property="article:published_time" content="2022-06-21T00:42:49.088Z">
<meta property="article:modified_time" content="2022-06-21T00:44:17.910Z">
<meta property="article:author" content="徐耀彬">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://louishsu.xyz/2022/06/21/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-06-21 08:44:17'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/background.css"><script src="https://cdn.jsdelivr.net/npm/echarts@4.7.0/dist/echarts.min.js"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">11</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">LOUIS' BLOG</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Arxiv每日速递(2022-06-21)<a class="post-edit-link" href="https://github.com/isLouisHsu/blog/tree/master/source_posts/Arxiv每日速递.md" title="编辑" target="_blank"><i class="fas fa-pencil-square"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-06-21T00:42:49.088Z" title="发表于 2022-06-21 08:42:49">2022-06-21</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-06-21T00:44:17.910Z" title="更新于 2022-06-21 08:44:17">2022-06-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">阅读笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">71.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>428分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2022/06/21/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。</p>
<h1>统计</h1>
<p>今日共更新290篇论文，其中：</p>
<ul>
<li>77篇计算机视觉（<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>）</li>
<li>25篇自然语言处理（<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>）</li>
<li>126篇机器学习（cs.LG）</li>
<li>55篇人工智能（<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>）</li>
</ul>
<h1>计算机视觉</h1>
<details>
  <summary>1. <b>标题：TAVA: Template-free Animatable Volumetric Actors</b></summary>
  <p><b>编号</b>：[1]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08929</p>
  <p><b>作者</b>：Ruilong Li,  Julian Tanke,  Minh Vo,  Michael Zollhofer,  Jurgen Gall,  Angjoo Kanazawa,  Christoph Lassner</p>
  <p><b>备注</b>：Code: this https URL; Project Website: this https URL</p>
  <p><b>关键词</b>：generate photo-realistic virtual, photo-realistic virtual avatars, virtual avatars, potential to generate, generate photo-realistic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Coordinate-based volumetric representations have the potential to generate
photo-realistic virtual avatars from images. However, virtual avatars also need
to be controllable even to a novel pose that may not have been observed.
Traditional techniques, such as LBS, provide such a function; yet it usually
requires a hand-designed body template, 3D scan data, and limited appearance
models. On the other hand, neural representation has been shown to be powerful
in representing visual details, but are under explored on deforming dynamic
articulated actors. In this paper, we propose TAVA, a method to create T
emplate-free Animatable Volumetric Actors, based on neural representations. We
rely solely on multi-view data and a tracked skeleton to create a volumetric
model of an actor, which can be animated at the test time given novel pose.
Since TAVA does not require a body template, it is applicable to humans as well
as other creatures such as animals. Furthermore, TAVA is designed such that it
can recover accurate dense correspondences, making it amenable to
content-creation and editing tasks. Through extensive experiments, we
demonstrate that the proposed method generalizes well to novel poses as well as
unseen views and showcase basic editing capabilities.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Cross-task Attention Mechanism for Dense Multi-task Learning</b></summary>
  <p><b>编号</b>：[2]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08927</p>
  <p><b>作者</b>：Ivan Lopes,  Tuan-Hung Vu,  Raoul de Charette</p>
  <p><b>备注</b>：10 figures, 6 tables, 23 pages</p>
  <p><b>关键词</b>：complex scenes, promising solution, comprehensive understanding, understanding of complex, Multi-task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multi-task learning has recently become a promising solution for a
comprehensive understanding of complex scenes. Not only being memory-efficient,
multi-task models with an appropriate design can favor exchange of
complementary signals across tasks. In this work, we jointly address 2D
semantic segmentation, and two geometry-related tasks, namely dense depth,
surface normal estimation as well as edge estimation showing their benefit on
indoor and outdoor datasets. We propose a novel multi-task learning
architecture that exploits pair-wise cross-task exchange through
correlation-guided attention and self-attention to enhance the average
representation learning for all tasks. We conduct extensive experiments
considering three multi-task setups, showing the benefit of our proposal in
comparison to competitive baselines in both synthetic and real benchmarks. We
also extend our method to the novel multi-task unsupervised domain adaptation
setting. Our code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：VectorMapNet: End-to-end Vectorized HD Map Learning</b></summary>
  <p><b>编号</b>：[4]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08920</p>
  <p><b>作者</b>：Yicheng Liu,  Yue Wang,  Yilun Wang,  Hang Zhao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：including moving obstacles, surrounding environments, including moving, static High-Definition, good understanding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Autonomous driving systems require a good understanding of surrounding
environments, including moving obstacles and static High-Definition (HD)
semantic maps. Existing methods approach the semantic map problem by offline
manual annotations, which suffer from serious scalability issues. More recent
learning-based methods produce dense rasterized segmentation predictions which
do not include instance information of individual map elements and require
heuristic post-processing that involves many hand-designed components, to
obtain vectorized maps. To that end, we introduce an end-to-end vectorized HD
map learning pipeline, termed VectorMapNet. VectorMapNet takes onboard sensor
observations and predicts a sparse set of polylines primitives in the
bird's-eye view to model the geometry of HD maps. Based on this pipeline, our
method can explicitly model the spatial relation between map elements and
generate vectorized maps that are friendly for downstream autonomous driving
tasks without the need for post-processing. In our experiments, VectorMapNet
achieves strong HD map learning performance on nuScenes dataset, surpassing
previous state-of-the-art methods by 14.2 mAP. Qualitatively, we also show that
VectorMapNet is capable of generating comprehensive maps and capturing more
fine-grained details of road geometry. To the best of our knowledge,
VectorMapNet is the first work designed toward end-to-end vectorized HD map
learning problems.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：VLMixer: Unpaired Vision-Language Pre-training via Cross-Modal CutMix</b></summary>
  <p><b>编号</b>：[5]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08919</p>
  <p><b>作者</b>：Teng Wang,  Wenhao Jiang,  Zhichao Lu,  Feng Zheng,  Ran Cheng,  Chengguo Yin,  Ping Luo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Existing vision-language pre-training, enormous human labors, paired image-text datasets, data cleaning techniques, elaborate data cleaning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing vision-language pre-training (VLP) methods primarily rely on paired
image-text datasets, which are either annotated by enormous human labors, or
crawled from the internet followed by elaborate data cleaning techniques. To
reduce the dependency on well-aligned image-text pairs, it is promising to
directly leverage the large-scale text-only and image-only corpora. This paper
proposes a data augmentation method, namely cross-modal CutMix (CMC), for
implicit cross-modal alignment learning in unpaired VLP. Specifically, CMC
transforms natural sentences from the textual view into a multi-modal view,
where visually-grounded words in a sentence are randomly replaced by diverse
image patches with similar semantics. There are several appealing proprieties
of the proposed CMC. First, it enhances the data diversity while keeping the
semantic meaning intact for tackling problems where the aligned data are
scarce; Second, by attaching cross-modal noise on uni-modal data, it guides
models to learn token-level interactions across modalities for better
denoising. Furthermore, we present a new unpaired VLP method, dubbed as
VLMixer, that integrates CMC with contrastive learning to pull together the
uni-modal and multi-modal views for better instance-level alignments among
different modalities. Extensive experiments on five downstream tasks show that
VLMixer could surpass previous state-of-the-art unpaired VLP methods.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks</b></summary>
  <p><b>编号</b>：[7]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08916</p>
  <p><b>作者</b>：Jiasen Lu,  Christopher Clark,  Rowan Zellers,  Roozbeh Mottaghi,  Aniruddha Kembhavi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：referring expression comprehension, spanning classical computer, including pose estimation, natural language processing, including RGB images</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose Unified-IO, a model that performs a large variety of AI tasks
spanning classical computer vision tasks, including pose estimation, object
detection, depth estimation and image generation, vision-and-language tasks
such as region captioning and referring expression comprehension, to natural
language processing tasks such as question answering and paraphrasing.
Developing a single unified model for such a large variety of tasks poses
unique challenges due to the heterogeneous inputs and outputs pertaining to
each task, including RGB images, per-pixel maps, binary masks, bounding boxes,
and language. We achieve this unification by homogenizing every supported input
and output into a sequence of discrete vocabulary tokens. This common
representation across all tasks allows us to train a single transformer-based
architecture, jointly on over 80 diverse datasets in the vision and language
fields. Unified-IO is the first model capable of performing all 7 tasks on the
GRIT benchmark and produces strong results across 16 diverse benchmarks like
NYUv2-Depth, ImageNet, VQA2.0, OK-VQA, Swig, VizWizGround, BoolQ, and SciTail,
with no task or benchmark specific fine-tuning. Demos for Unified-IO are
available at this https URL.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Colonoscopy 3D Video Dataset with Paired Depth from 2D-3D Registration</b></summary>
  <p><b>编号</b>：[10]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08903</p>
  <p><b>作者</b>：Taylor L. Bobrow,  Mayank Golhar,  Rohan Vijayan,  Venkata S. Akshintala,  Juan R. Garcia,  Nicholas J. Durr</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：missing region detection, important clinical application, including depth estimation, region detection, ground truth</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Screening colonoscopy is an important clinical application for several 3D
computer vision techniques, including depth estimation, surface reconstruction,
and missing region detection. However, the development, evaluation, and
comparison of these techniques in real colonoscopy videos remain largely
qualitative due to the difficulty of acquiring ground truth data. In this work,
we present a Colonoscopy 3D Video Dataset (C3VD) acquired with a high
definition clinical colonoscope and high-fidelity colon models for benchmarking
computer vision methods in colonoscopy. We introduce a novel multimodal 2D-3D
registration technique to register optical video sequences with ground truth
rendered views of a known 3D model. The different modalities are registered by
transforming optical images to depth maps with a Generative Adversarial Network
and aligning edge features with an evolutionary optimizer. This registration
method achieves an average translation error of 0.321 millimeters and an
average rotation error of 0.159 degrees in simulation experiments where
error-free ground truth is available. The method also leverages video
information, improving registration accuracy by 55.6% for translation and 60.4%
for rotation compared to single frame registration. 22 short video sequences
were registered to generate 10,015 total frames with paired ground truth depth,
surface normals, optical flow, occlusion, six degree-of-freedom pose, coverage
maps, and 3D models. The dataset also includes screening videos acquired by a
gastroenterologist with paired ground truth pose and 3D surface models. The
dataset and registration source code are available at this http URL.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：SimA: Simple Softmax-free Attention for Vision Transformers</b></summary>
  <p><b>编号</b>：[12]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08898</p>
  <p><b>作者</b>：Soroush Abbasi Koohpayegani,  Hamed Pirsiavash</p>
  <p><b>备注</b>：Code is available here: $\href{this https URL}{\text{This https URL}}$</p>
  <p><b>关键词</b>：attention block, Softmax layer, attention, vision transformers, Softmax</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, vision transformers have become very popular. However, deploying
them in many applications is computationally expensive partly due to the
Softmax layer in the attention block. We introduce a simple but effective,
Softmax-free attention block, SimA, which normalizes query and key matrices
with simple $\ell_1$-norm instead of using Softmax layer. Then, the attention
block in SimA is a simple multiplication of three matrices, so SimA can
dynamically change the ordering of the computation at the test time to achieve
linear computation on the number of tokens or the number of channels. We
empirically show that SimA applied to three SOTA variations of transformers,
DeiT, XCiT, and CvT, results in on-par accuracy compared to the SOTA models,
without any need for Softmax layer. Interestingly, changing SimA from
multi-head to single-head has only a small effect on the accuracy, which
simplifies the attention block further. The code is available here:
$\href{this https URL}{\text{This https URL}}$</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Representational Multiplicity Should Be Exposed, Not Eliminated</b></summary>
  <p><b>编号</b>：[14]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08890</p>
  <p><b>作者</b>：Ari Heljakka,  Martin Trapp,  Juho Kannala,  Arno Solin</p>
  <p><b>备注</b>：15 pages, 5 figures</p>
  <p><b>关键词</b>：real-world performance characteristics, machine learning models, performance characteristics, prevalent and well-observed, poorly understood</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>It is prevalent and well-observed, but poorly understood, that two machine
learning models with similar performance during training can have very
different real-world performance characteristics. This implies elusive
differences in the internals of the models, manifesting as representational
multiplicity (RM). We introduce a conceptual and experimental setup for
analyzing RM and show that certain training methods systematically result in
greater RM than others, measured by activation similarity via singular vector
canonical correlation analysis (SVCCA). We further correlate it with predictive
multiplicity measured by the variance in i.i.d. and out-of-distribution test
set predictions, in four common image data sets. We call for systematic
measurement and maximal exposure, not elimination, of RM in models. Qualitative
tools such as our confabulator analysis can facilitate understanding and
communication of RM effects to stakeholders.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：CtrlFormer: Learning Transferable State Representation for Visual  Control via Transformer</b></summary>
  <p><b>编号</b>：[17]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08883</p>
  <p><b>作者</b>：Yao Mu,  Shoufa Chen,  Mingyu Ding,  Jianyu Chen,  Runjian Chen,  Ping Luo</p>
  <p><b>备注</b>：ICML 2022</p>
  <p><b>关键词</b>：achieved great successes, achieved great, great successes, vision and language, control</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transformer has achieved great successes in learning vision and language
representation, which is general across various downstream tasks. In visual
control, learning transferable state representation that can transfer between
different control tasks is important to reduce the training sample size.
However, porting Transformer to sample-efficient visual control remains a
challenging and unsolved problem. To this end, we propose a novel Control
Transformer (CtrlFormer), possessing many appealing benefits that prior arts do
not have. Firstly, CtrlFormer jointly learns self-attention mechanisms between
visual tokens and policy tokens among different control tasks, where multitask
representation can be learned and transferred without catastrophic forgetting.
Secondly, we carefully design a contrastive reinforcement learning paradigm to
train CtrlFormer, enabling it to achieve high sample efficiency, which is
important in control problems. For example, in the DMControl benchmark, unlike
recent advanced methods that failed by producing a zero score in the "Cartpole"
task after transfer learning with 100k samples, CtrlFormer can achieve a
state-of-the-art score with only 100k samples while maintaining the performance
of previous tasks. The code and models are released in our project homepage.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Edge-Aided Sensor Data Sharing in Vehicular Communication Networks</b></summary>
  <p><b>编号</b>：[18]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08882</p>
  <p><b>作者</b>：Rui Song,  Anupama Hegde,  Numan Senel,  Alois Knoll,  Andreas Festag</p>
  <p><b>备注</b>：Accepted for IEEE 95th Vehicular Technology Conference (VTC2022-Spring)</p>
  <p><b>关键词</b>：connected automated vehicles, Sensor data sharing, Sensor data, significantly improve, improve the range</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sensor data sharing in vehicular networks can significantly improve the range
and accuracy of environmental perception for connected automated vehicles.
Different concepts and schemes for dissemination and fusion of sensor data have
been developed. It is common to these schemes that measurement errors of the
sensors impair the perception quality and can result in road traffic accidents.
Specifically, when the measurement error from the sensors (also referred as
measurement noise) is unknown and time varying, the performance of the data
fusion process is restricted, which represents a major challenge in the
calibration of sensors. In this paper, we consider sensor data sharing and
fusion in a vehicular network with both, vehicle-to-infrastructure and
vehicle-to-vehicle communication. We propose a method, named Bidirectional
Feedback Noise Estimation (BiFNoE), in which an edge server collects and caches
sensor measurement data from vehicles. The edge estimates the noise and the
targets alternately in double dynamic sliding time windows and enhances the
distributed cooperative environment sensing at each vehicle with low
communication costs. We evaluate the proposed algorithm and data dissemination
strategy in an application scenario by simulation and show that the perception
accuracy is on average improved by around 80 % with only 12 kbps uplink and 28
kbps downlink bandwidth.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Improving Generalization of Metric Learning via Listwise  Self-distillation</b></summary>
  <p><b>编号</b>：[20]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08880</p>
  <p><b>作者</b>：Zelong Zeng,  Fan Yang,  Zheng Wang,  Shin'ichi Satoh</p>
  <p><b>备注</b>：11 pages, 7 figures</p>
  <p><b>关键词</b>：space while keeping, employ a strategy, deep metric learning, LSD, samples</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Most deep metric learning (DML) methods employ a strategy that forces all
positive samples to be close in the embedding space while keeping them away
from negative ones. However, such a strategy ignores the internal relationships
of positive (negative) samples and often leads to overfitting, especially in
the presence of hard samples and mislabeled samples. In this work, we propose a
simple yet effective regularization, namely Listwise Self-Distillation (LSD),
which progressively distills a model's own knowledge to adaptively assign a
more appropriate distance target to each sample pair in a batch. LSD encourages
smoother embeddings and information mining within positive (negative) samples
as a way to mitigate overfitting and thus improve generalization. Our LSD can
be directly integrated into general DML frameworks. Extensive experiments show
that LSD consistently boosts the performance of various metric learning methods
on multiple datasets.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Fast Lossless Neural Compression with Integer-Only Discrete Flows</b></summary>
  <p><b>编号</b>：[23]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08869</p>
  <p><b>作者</b>：Siyu Wang,  Jianfei Chen,  Chongxuan Li,  Jun Zhu,  Bo Zhang</p>
  <p><b>备注</b>：Accepted as a conference paper at International Conference on Machine Learning (ICML) 2022</p>
  <p><b>关键词</b>：learned data distributions, applying entropy codecs, outperformed traditional codecs, significantly outperformed traditional, data distributions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>By applying entropy codecs with learned data distributions, neural
compressors have significantly outperformed traditional codecs in terms of
compression ratio. However, the high inference latency of neural networks
hinders the deployment of neural compressors in practical applications. In this
work, we propose Integer-only Discrete Flows (IODF), an efficient neural
compressor with integer-only arithmetic. Our work is built upon integer
discrete flows, which consists of invertible transformations between discrete
random variables. We propose efficient invertible transformations with
integer-only arithmetic based on 8-bit quantization. Our invertible
transformation is equipped with learnable binary gates to remove redundant
filters during inference. We deploy IODF with TensorRT on GPUs, achieving 10x
inference speedup compared to the fastest existing neural compressors, while
retaining the high compression rates on ImageNet32 and ImageNet64.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：DGMIL: Distribution Guided Multiple Instance Learning for Whole Slide  Image Classification</b></summary>
  <p><b>编号</b>：[26]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08861</p>
  <p><b>作者</b>：Linhao Qu,  Xiaoyuan Luo,  Shaolei Liu,  Manning Wang,  Zhijian Song</p>
  <p><b>备注</b>：accepted by MICCAI 2022</p>
  <p><b>关键词</b>：Multiple Instance Learning, Slide Images, Instance Learning, Learning, MIL</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multiple Instance Learning (MIL) is widely used in analyzing
histopathological Whole Slide Images (WSIs). However, existing MIL methods do
not explicitly model the data distribution, and instead they only learn a
bag-level or instance-level decision boundary discriminatively by training a
classifier. In this paper, we propose DGMIL: a feature distribution guided deep
MIL framework for WSI classification and positive patch localization. Instead
of designing complex discriminative network architectures, we reveal that the
inherent feature distribution of histopathological image data can serve as a
very effective guide for instance classification. We propose a
cluster-conditioned feature distribution modeling method and a pseudo
label-based iterative feature space refinement strategy so that in the final
feature space the positive and negative instances can be easily separated.
Experiments on the CAMELYON16 dataset and the TCGA Lung Cancer dataset show
that our method achieves new SOTA for both global classification and positive
patch localization tasks.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：MineDojo: Building Open-Ended Embodied Agents with Internet-Scale  Knowledge</b></summary>
  <p><b>编号</b>：[29]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08853</p>
  <p><b>作者</b>：Linxi Fan,  Guanzhi Wang,  Yunfan Jiang,  Ajay Mandlekar,  Yuncong Yang,  Haoyi Zhu,  Andrew Tang,  De-An Huang,  Yuke Zhu,  Anima Anandkumar</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：made great strides, domains like Atari, Atari games, made great, great strides</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Autonomous agents have made great strides in specialist domains like Atari
games and Go. However, they typically learn tabula rasa in isolated
environments with limited and manually conceived objectives, thus failing to
generalize across a wide spectrum of tasks and capabilities. Inspired by how
humans continually learn and adapt in the open world, we advocate a trinity of
ingredients for building generalist agents: 1) an environment that supports a
multitude of tasks and goals, 2) a large-scale database of multimodal
knowledge, and 3) a flexible and scalable agent architecture. We introduce
MineDojo, a new framework built on the popular Minecraft game that features a
simulation suite with thousands of diverse open-ended tasks and an
internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and
forum discussions. Using MineDojo's data, we propose a novel agent learning
algorithm that leverages large pre-trained video-language models as a learned
reward function. Our agent is able to solve a variety of open-ended tasks
specified in free-form language without any manually designed dense shaping
reward. We open-source the simulation suite and knowledge bases
(this https URL) to promote research towards the goal of generally
capable embodied agents.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Entity-Graph Enhanced Cross-Modal Pretraining for Instance-level Product  Retrieval</b></summary>
  <p><b>编号</b>：[34]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08842</p>
  <p><b>作者</b>：Xiao Dong,  Xunlin Zhan,  Yunchao Wei,  Xiaoyong Wei,  Yaowei Wang,  Minlong Lu,  Xiaochun Cao,  Xiaodan Liang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：fine-grained product categories, conduct weakly-supervised multi-modal, weakly-supervised multi-modal instance-level, instance-level product retrieval, realistic environment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Our goal in this research is to study a more realistic environment in which
we can conduct weakly-supervised multi-modal instance-level product retrieval
for fine-grained product categories. We first contribute the Product1M
datasets, and define two real practical instance-level retrieval tasks to
enable the evaluations on the price comparison and personalized
recommendations. For both instance-level tasks, how to accurately pinpoint the
product target mentioned in the visual-linguistic data and effectively decrease
the influence of irrelevant contents is quite challenging. To address this, we
exploit to train a more effective cross-modal pertaining model which is
adaptively capable of incorporating key concept information from the
multi-modal data, by using an entity graph whose node and edge respectively
denote the entity and the similarity relation between entities. Specifically, a
novel Entity-Graph Enhanced Cross-Modal Pretraining (EGE-CMP) model is proposed
for instance-level commodity retrieval, that explicitly injects entity
knowledge in both node-based and subgraph-based ways into the multi-modal
networks via a self-supervised hybrid-stream transformer, which could reduce
the confusion between different object contents, thereby effectively guiding
the network to focus on entities with real semantic. Experimental results well
verify the efficacy and generalizability of our EGE-CMP, outperforming several
SOTA cross-modal baselines like CLIP, UNITER and CAPTURE.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：A Comparative Study of Confidence Calibration in Deep Learning: From  Computer Vision to Medical Imaging</b></summary>
  <p><b>编号</b>：[38]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08833</p>
  <p><b>作者</b>：Riqiang Gao,  Thomas Li,  Yucheng Tang,  Zhoubing Xu,  Michael Kammer,  Sanja L. Antic,  Kim Sandler,  Fabien Moldonado,  Thomas A. Lasko,  Bennett Landman</p>
  <p><b>备注</b>：17 pages, 6 figures</p>
  <p><b>关键词</b>：computer vision, medical imaging, general computer vision, computer, vision</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Although deep learning prediction models have been successful in the
discrimination of different classes, they can often suffer from poor
calibration across challenging domains including healthcare. Moreover, the
long-tail distribution poses great challenges in deep learning classification
problems including clinical disease prediction. There are approaches proposed
recently to calibrate deep prediction in computer vision, but there are no
studies found to demonstrate how the representative models work in different
challenging contexts. In this paper, we bridge the confidence calibration from
computer vision to medical imaging with a comparative study of four high-impact
calibration models. Our studies are conducted in different contexts (natural
image classification and lung cancer risk estimation) including in balanced vs.
imbalanced training sets and in computer vision vs. medical imaging. Our
results support key findings: (1) We achieve new conclusions which are not
studied under different learning contexts, e.g., combining two calibration
models that both mitigate the overconfident prediction can lead to
under-confident prediction, and simpler calibration models from the computer
vision domain tend to be more generalizable to medical imaging. (2) We
highlight the gap between general computer vision tasks and medical imaging
prediction, e.g., calibration methods ideal for general computer vision tasks
may in fact damage the calibration of medical imaging prediction. (3) We also
reinforce previous conclusions in natural image classification settings. We
believe that this study has merits to guide readers to choose calibration
models and understand gaps between general computer vision and medical imaging
domains.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Multimodal Attention-based Deep Learning for Alzheimer's Disease  Diagnosis</b></summary>
  <p><b>编号</b>：[41]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08826</p>
  <p><b>作者</b>：Michal Golovanevsky,  Carsten Eickhoff,  Ritambhara Singh</p>
  <p><b>备注</b>：11 pages, 5 figures</p>
  <p><b>关键词</b>：common neurodegenerative disorder, Alzheimer Disease Diagnosis, Multimodal Alzheimer Disease, Alzheimer Disease, Disease Diagnosis framework</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Alzheimer's Disease (AD) is the most common neurodegenerative disorder with
one of the most complex pathogeneses, making effective and clinically
actionable decision support difficult. The objective of this study was to
develop a novel multimodal deep learning framework to aid medical professionals
in AD diagnosis. We present a Multimodal Alzheimer's Disease Diagnosis
framework (MADDi) to accurately detect the presence of AD and mild cognitive
impairment (MCI) from imaging, genetic, and clinical data. MADDi is novel in
that we use cross-modal attention, which captures interactions between
modalities - a method not previously explored in this domain. We perform
multi-class classification, a challenging task considering the strong
similarities between MCI and AD. We compare with previous state-of-the-art
models, evaluate the importance of attention, and examine the contribution of
each modality to the model's performance. MADDi classifies MCI, AD, and
controls with 96.88% accuracy on a held-out test set. When examining the
contribution of different attention schemes, we found that the combination of
cross-modal attention with self-attention performed the best, and no attention
layers in the model performed the worst, with a 7.9% difference in F1-Scores.
Our experiments underlined the importance of structured clinical data to help
machine learning models contextualize and interpret the remaining modalities.
Extensive ablation studies showed that any multimodal mixture of input features
without access to structured clinical information suffered marked performance
losses. This study demonstrates the merit of combining multiple input
modalities via cross-modal attention to deliver highly accurate AD diagnostic
decision support.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Open-Sampling: Exploring Out-of-Distribution data for Re-balancing  Long-tailed datasets</b></summary>
  <p><b>编号</b>：[49]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08802</p>
  <p><b>作者</b>：Hongxin Wei,  Lue Tao,  Renchunzi Xie,  Lei Feng,  Bo An</p>
  <p><b>备注</b>：Accepted by ICML 2022</p>
  <p><b>关键词</b>：extreme class imbalance, training dataset suffers, perform poorly, suffers from extreme, Deep neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep neural networks usually perform poorly when the training dataset suffers
from extreme class imbalance. Recent studies found that directly training with
out-of-distribution data (i.e., open-set samples) in a semi-supervised manner
would harm the generalization performance. In this work, we theoretically show
that out-of-distribution data can still be leveraged to augment the minority
classes from a Bayesian perspective. Based on this motivation, we propose a
novel method called Open-sampling, which utilizes open-set noisy labels to
re-balance the class priors of the training dataset. For each open-set
instance, the label is sampled from our pre-defined distribution that is
complementary to the distribution of original class priors. We empirically show
that Open-sampling not only re-balances the class priors but also encourages
the neural network to learn separable representations. Extensive experiments
demonstrate that our proposed method significantly outperforms existing data
re-balancing methods and can boost the performance of existing state-of-the-art
methods.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Video Shadow Detection via Spatio-Temporal Interpolation Consistency  Training</b></summary>
  <p><b>编号</b>：[50]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08801</p>
  <p><b>作者</b>：Xiao Lu,  Yihong Cao,  Sheng Liu,  Chengjiang Long,  Zipei Chen,  Xuanyu Zhou,  Yimin Yang,  Chunxia Xiao</p>
  <p><b>备注</b>：Accepted in CVPR2022</p>
  <p><b>关键词</b>：annotate large-scale datasets, challenging to annotate, annotate large-scale, video shadow detection, shadow detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>It is challenging to annotate large-scale datasets for supervised video
shadow detection methods. Using a model trained on labeled images to the video
frames directly may lead to high generalization error and temporal inconsistent
results. In this paper, we address these challenges by proposing a
Spatio-Temporal Interpolation Consistency Training (STICT) framework to
rationally feed the unlabeled video frames together with the labeled images
into an image shadow detection network training. Specifically, we propose the
Spatial and Temporal ICT, in which we define two new interpolation schemes,
\textit{i.e.}, the spatial interpolation and the temporal interpolation. We
then derive the spatial and temporal interpolation consistency constraints
accordingly for enhancing generalization in the pixel-wise classification task
and for encouraging temporal consistent predictions, respectively. In addition,
we design a Scale-Aware Network for multi-scale shadow knowledge learning in
images, and propose a scale-consistency constraint to minimize the discrepancy
among the predictions at different scales. Our proposed approach is extensively
validated on the ViSha dataset and a self-annotated dataset. Experimental
results show that, even without video labels, our approach is better than most
state of the art supervised, semi-supervised or unsupervised image/video shadow
detection methods and other methods in related tasks. Code and dataset are
available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：The Importance of Background Information for Out of Distribution  Generalization</b></summary>
  <p><b>编号</b>：[52]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08794</p>
  <p><b>作者</b>：Jupinder Parmar,  Khaled Saab,  Brian Pogatchnik,  Daniel Rubin,  Christopher Ré</p>
  <p><b>备注</b>：6 pages, 2 figures</p>
  <p><b>关键词</b>：trustworthy machine learning, medical image classification, deployed in healthcare, problem for trustworthy, trustworthy machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Domain generalization in medical image classification is an important problem
for trustworthy machine learning to be deployed in healthcare. We find that
existing approaches for domain generalization which utilize ground-truth
abnormality segmentations to control feature attributions have poor
out-of-distribution (OOD) performance relative to the standard baseline of
empirical risk minimization (ERM). We investigate what regions of an image are
important for medical image classification and show that parts of the
background, that which is not contained in the abnormality segmentation,
provides helpful signal. We then develop a new task-specific mask which covers
all relevant regions. Utilizing this new segmentation mask significantly
improves the performance of the existing methods on the OOD test sets. To
obtain better generalization results than ERM, we find it necessary to scale up
the training data size in addition to the usage of these task-specific masks.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：FD-CAM: Improving Faithfulness and Discriminability of Visual  Explanation for CNNs</b></summary>
  <p><b>编号</b>：[54]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08792</p>
  <p><b>作者</b>：Hui Li,  Zihao Li,  Rui Ma,  Tieru Wu</p>
  <p><b>备注</b>：Accepted by ICPR 2022 and also accepted by CVPR 2022 Explainable Artificial Intelligence for Computer Vision (XAI4CV) Workshop</p>
  <p><b>关键词</b>：convolutional neural networks, internal working mechanism, Class activation map, neural networks, combine activation maps</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Class activation map (CAM) has been widely studied for visual explanation of
the internal working mechanism of convolutional neural networks. The key of
existing CAM-based methods is to compute effective weights to combine
activation maps in the target convolution layer. Existing gradient and score
based weighting schemes have shown superiority in ensuring either the
discriminability or faithfulness of the CAM, but they normally cannot excel in
both properties. In this paper, we propose a novel CAM weighting scheme, named
FD-CAM, to improve both the faithfulness and discriminability of the CAM-based
CNN visual explanation. First, we improve the faithfulness and discriminability
of the score-based weights by performing a grouped channel switching operation.
Specifically, for each channel, we compute its similarity group and switch the
group of channels on or off simultaneously to compute changes in the class
prediction score as the weights. Then, we combine the improved score-based
weights with the conventional gradient-based weights so that the
discriminability of the final CAM can be further improved. We perform extensive
comparisons with the state-of-the-art CAM algorithms. The quantitative and
qualitative results show our FD-CAM can produce more faithful and more
discriminative visual explanations of the CNNs. We also conduct experiments to
verify the effectiveness of the proposed grouped channel switching and weight
combination scheme on improving the results. Our code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：DU-Net based Unsupervised Contrastive Learning for Cancer Segmentation  in Histology Images</b></summary>
  <p><b>编号</b>：[55]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08791</p>
  <p><b>作者</b>：Yilong Li,  Yaqi Wang,  Huiyu Zhou,  Huaqiong Wang,  Gangyong Jia,  Qianni Zhang</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2002.05709 by other authors</p>
  <p><b>关键词</b>：unsupervised cancer segmentation, cancer segmentation framework, histology images, introduce an unsupervised, unsupervised cancer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we introduce an unsupervised cancer segmentation framework for
histology images. The framework involves an effective contrastive learning
scheme for extracting distinctive visual representations for segmentation. The
encoder is a Deep U-Net (DU-Net) structure that contains an extra fully
convolution layer compared to the normal U-Net. A contrastive learning scheme
is developed to solve the problem of lacking training sets with high-quality
annotations on tumour boundaries. A specific set of data augmentation
techniques are employed to improve the discriminability of the learned colour
features from contrastive learning. Smoothing and noise elimination are
conducted using convolutional Conditional Random Fields. The experiments
demonstrate competitive performance in segmentation even better than some
popular supervised networks.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Reconstructing vehicles from orthographic drawings using deep neural  networks</b></summary>
  <p><b>编号</b>：[57]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08789</p>
  <p><b>作者</b>：Robin Klippert</p>
  <p><b>备注</b>：9 Pages</p>
  <p><b>关键词</b>：multiple orthographic drawings, explores the current, orthographic drawings, drawings using deep, deep neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper explores the current state-of-the-art of object reconstruction
from multiple orthographic drawings using deep neural networks. It proposes two
algorithms to extract multiple views from a single image. The paper proposes a
system based on pixel-aligned implicit functions (PIFu) and develops an
advanced sampling strategy to generate signed distance samples. It also
compares this approach to depth map regression from multiple views.
Additionally, the paper uses a novel dataset for vehicle reconstruction from
the racing game Assetto Corsa, which features higher quality models than the
commonly used ShapeNET dataset. The trained neural network generalizes well to
real-world inputs and creates plausible and detailed reconstructions.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：CTooth: A Fully Annotated 3D Dataset and Benchmark for Tooth Volume  Segmentation on Cone Beam Computed Tomography Images</b></summary>
  <p><b>编号</b>：[62]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08778</p>
  <p><b>作者</b>：Weiwei Cui,  Yaqi Wang,  Qianni Zhang,  Huiyu Zhou,  Dan Song,  Xingyong Zuo,  Gangyong Jia,  Liaoyuan Zeng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：computer-aided dental diagnosis, diagnosis and treatment, prerequisite for computer-aided, tooth, segmentation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>3D tooth segmentation is a prerequisite for computer-aided dental diagnosis
and treatment. However, segmenting all tooth regions manually is subjective and
time-consuming. Recently, deep learning-based segmentation methods produce
convincing results and reduce manual annotation efforts, but it requires a
large quantity of ground truth for training. To our knowledge, there are few
tooth data available for the 3D segmentation study. In this paper, we establish
a fully annotated cone beam computed tomography dataset CTooth with tooth gold
standard. This dataset contains 22 volumes (7363 slices) with fine tooth labels
annotated by experienced radiographic interpreters. To ensure a relative even
data sampling distribution, data variance is included in the CTooth including
missing teeth and dental restoration. Several state-of-the-art segmentation
methods are evaluated on this dataset. Afterwards, we further summarise and
apply a series of 3D attention-based Unet variants for segmenting tooth
volumes. This work provides a new benchmark for the tooth volume segmentation
task. Experimental evidence proves that attention modules of the 3D UNet
structure boost responses in tooth areas and inhibit the influence of
background and noise. The best performance is achieved by 3D Unet with SKNet
attention module, of 88.04 \% Dice and 78.71 \% IOU, respectively. The
attention-based Unet framework outperforms other state-of-the-art methods on
the CTooth dataset. The codebase and dataset are released.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：A Database for Perceived Quality Assessment of User-Generated VR Videos</b></summary>
  <p><b>编号</b>：[73]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08751</p>
  <p><b>作者</b>：Yuming Fang,  Yiru Yao,  Xiangjie Sui,  Kede Ma</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：gained increasing attention, increasing attention due, Virtual reality, cameras and displays, popularization of consumer-grade</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Virtual reality (VR) videos (typically in the form of 360$^\circ$ videos)
have gained increasing attention due to the fast development of VR technologies
and the remarkable popularization of consumer-grade 360$^\circ$ cameras and
displays. Thus it is pivotal to understand how people perceive user-generated
VR videos, which may suffer from commingled authentic distortions, often
localized in space and time. In this paper, we establish one of the largest
360$^\circ$ video databases, containing 502 user-generated videos with rich
content and distortion diversities. We capture viewing behaviors (i.e.,
scanpaths) of 139 users, and collect their opinion scores of perceived quality
under four different viewing conditions (two starting points $\times$ two
exploration times). We provide a thorough statistical analysis of recorded
data, resulting in several interesting observations, such as the significant
impact of viewing conditions on viewing behaviors and perceived quality.
Besides, we explore other usage of our data and analysis, including evaluation
of computational models for quality assessment and saliency detection of
360$^\circ$ videos. We have made the dataset and code available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：From a few Accurate 2D Correspondences to 3D Point Clouds</b></summary>
  <p><b>编号</b>：[75]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08749</p>
  <p><b>作者</b>：Trung-Kien Le,  Ping Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：world points, point clouds, initial world points, points, world</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Key points, correspondences, projection matrices, point clouds and dense
clouds are the skeletons in image-based 3D reconstruction, of which point
clouds have the important role in generating a realistic and natural model for
a 3D reconstructed object. To achieve a good 3D reconstruction, the point
clouds must be almost everywhere in the surface of the object. In this article,
with a main purpose to build the point clouds covering the entire surface of
the object, we propose a new feature named a geodesic feature or geo-feature.
Based on the new geo-feature, if there are several (given) initial world points
on the object's surface along with all accurately estimated projection
matrices, some new world points on the geodesics connecting any two of these
given world points will be reconstructed. Then the regions on the surface
bordering by these initial world points will be covered by the point clouds.
Thus, if the initial world points are around the surface, the point clouds will
cover the entire surface.
This article proposes a new method to estimate the world points and
projection matrices from their correspondences. This method derives the
closed-form and iterative solutions for the world points and projection
matrices and proves that when the number of world points is less than seven and
the number of images is at least five, the proposed solutions are global
optimal. We propose an algorithm named World points from their Correspondences
(WPfC) to estimate the world points and projection matrices from their
correspondences, and another algorithm named Creating Point Clouds (CrPC) to
create the point clouds from the world points and projection matrices given by
the first algorithm.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：ReViSe: Remote Vital Signs Measurement Using Smartphone Camera</b></summary>
  <p><b>编号</b>：[76]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08748</p>
  <p><b>作者</b>：Donghao Qiao,  Amtul Haq Ayesha,  Farhana Zulkernine,  Raihan Masroor,  Nauman Jaffar</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：collecting biometric data, Remote Photoplethysmography, Heart Rate Variability, enables vital signs, MAE</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Remote Photoplethysmography (rPPG) is a fast, effective, inexpensive and
convenient method for collecting biometric data as it enables vital signs
estimation using face videos. Remote contactless medical service provisioning
has proven to be a dire necessity during the COVID-19 pandemic. We propose an
end-to-end framework to measure people's vital signs including Heart Rate (HR),
Heart Rate Variability (HRV), Oxygen Saturation (SpO2) and Blood Pressure (BP)
based on the rPPG methodology from the video of a user's face captured with a
smartphone camera. We extract face landmarks with a deep learning-based neural
network model in real-time. Multiple face patches also called
Region-of-Interests (RoIs) are extracted by using the predicted face landmarks.
Several filters are applied to reduce the noise from the RoIs in the extracted
cardiac signals called Blood Volume Pulse (BVP) signal. We trained and
validated machine learning models using two public rPPG datasets namely the
TokyoTech rPPG and the Pulse Rate Detection (PURE) datasets, on which our
models achieved the following Mean Absolute Errors (MAE): a) for HR, 1.73 and
3.95 Beats-Per-Minute (bpm) respectively, b) for HRV, 18.55 and 25.03 ms
respectively, and c) for SpO2, a MAE of 1.64 on the PURE dataset. We validated
our end-to-end rPPG framework, ReViSe, in real life environment, and thereby
created the Video-HR dataset. Our HR estimation model achieved a MAE of 2.49
bpm on this dataset. Since no publicly available rPPG datasets existed for BP
measurement with face videos, we used a dataset with signals from fingertip
sensor to train our model and also created our own video dataset, Video-BP. On
our Video-BP dataset, our BP estimation model achieved a MAE of 6.7 mmHg for
Systolic Blood Pressure (SBP), and a MAE of 9.6 mmHg for Diastolic Blood
Pressure (DBP).</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：An Algorithm for the SE(3)-Transformation on Neural Implicit Maps for  Remapping Functions</b></summary>
  <p><b>编号</b>：[94]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08712</p>
  <p><b>作者</b>：Yijun Yuan,  Andreas Nuechter</p>
  <p><b>备注</b>：Accepted to RAL2022, code at this https URL</p>
  <p><b>关键词</b>：neural implicit map, neural implicit, implicit map, Implicit, object reconstruction due</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Implicit representations are widely used for object reconstruction due to
their efficiency and flexibility. In 2021, a novel structure named neural
implicit map has been invented for incremental reconstruction. A neural
implicit map alleviates the problem of inefficient memory cost of previous
online 3D dense reconstruction while producing better quality. % However, the
neural implicit map suffers the limitation that it does not support remapping
as the frames of scans are encoded into a deep prior after generating the
neural implicit map. This means, that neither this generation process is
invertible, nor a deep prior is transformable. The non-remappable property
makes it not possible to apply loop-closure techniques. % We present a neural
implicit map based transformation algorithm to fill this gap. As our neural
implicit map is transformable, our model supports remapping for this special
map of latent features. % Experiments show that our remapping module is capable
to well-transform neural implicit maps to new poses. Embedded into a SLAM
framework, our mapping model is able to tackle the remapping of loop closures
and demonstrates high-quality surface reconstruction. % Our implementation is
available at github\footnote{\url{this https URL}} for
the research community.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Maximum Class Separation as Inductive Bias in One Matrix</b></summary>
  <p><b>编号</b>：[98]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08704</p>
  <p><b>作者</b>：Tejaswi Kasarla,  Gertjan J. Burghouts,  Max van Spengler,  Elise van der Pol,  Rita Cucchiara,  Pascal Mettes</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：well-known inductive bias, inductive bias, traditional algorithms, constitutes a well-known, machine learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Maximizing the separation between classes constitutes a well-known inductive
bias in machine learning and a pillar of many traditional algorithms. By
default, deep networks are not equipped with this inductive bias and therefore
many alternative solutions have been proposed through differential
optimization. Current approaches tend to optimize classification and separation
jointly: aligning inputs with class vectors and separating class vectors
angularly. This paper proposes a simple alternative: encoding maximum
separation as an inductive bias in the network by adding one fixed matrix
multiplication before computing the softmax activations. The main observation
behind our approach is that separation does not require optimization but can be
solved in closed-form prior to training and plugged into a network. We outline
a recursive approach to obtain the matrix consisting of maximally separable
vectors for any number of classes, which can be added with negligible
engineering effort and computational overhead. Despite its simple nature, this
one matrix multiplication provides real impact. We show that our proposal
directly boosts classification, long-tailed recognition, out-of-distribution
detection, and open-set recognition, from CIFAR to ImageNet. We find
empirically that maximum separation works best as a fixed bias; making the
matrix learnable adds nothing to the performance. The closed-form
implementation and code to reproduce the experiments are on github.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Towards Real-Time Visual Tracking with Graded Color-names Features</b></summary>
  <p><b>编号</b>：[101]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08701</p>
  <p><b>作者</b>：Lin Li,  Guoli Wang,  Xuemei Guo,</p>
  <p><b>备注</b>：12 pages, 5 figures</p>
  <p><b>关键词</b>：traditional MeanShift algorithm, MeanShift algorithm, simplicity and efficiency, target, algorithm</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>MeanShift algorithm has been widely used in tracking tasks because of its
simplicity and efficiency. However, the traditional MeanShift algorithm needs
to label the initial region of the target, which reduces the applicability of
the algorithm. Furthermore, it is only applicable to the scene with a large
overlap rate between the target area and the candidate area. Therefore, when
the target speed is fast, the target scale change, shape deformation or the
target occlusion occurs, the tracking performance will be deteriorated. In this
paper, we address the challenges above-mentioned by developing a tracking
method that combines the background models and the graded features of
color-names under the MeanShift framework. This method significantly improve
performance in the above scenarios. In addition, it facilitates the balance
between detection accuracy and detection speed. Experimental results
demonstrate the validation of the proposed method.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Sparse Double Descent: Where Network Pruning Aggravates Overfitting</b></summary>
  <p><b>编号</b>：[107]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08684</p>
  <p><b>作者</b>：Zheng He,  Zeke Xie,  Quanzhi Zhu,  Zengchang Qin</p>
  <p><b>备注</b>：ICML 2022</p>
  <p><b>关键词</b>：double descent, decreasing model capacity, network pruning, sparse double descent, reduces the computational</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>People usually believe that network pruning not only reduces the
computational cost of deep networks, but also prevents overfitting by
decreasing model capacity. However, our work surprisingly discovers that
network pruning sometimes even aggravates overfitting. We report an unexpected
sparse double descent phenomenon that, as we increase model sparsity via
network pruning, test performance first gets worse (due to overfitting), then
gets better (due to relieved overfitting), and gets worse at last (due to
forgetting useful information). While recent studies focused on the deep double
descent with respect to model overparameterization, they failed to recognize
that sparsity may also cause double descent. In this paper, we have three main
contributions. First, we report the novel sparse double descent phenomenon
through extensive experiments. Second, for this phenomenon, we propose a novel
learning distance interpretation that the curve of $\ell_{2}$ learning distance
of sparse models (from initialized parameters to final parameters) may
correlate with the sparse double descent curve well and reflect generalization
better than minima flatness. Third, in the context of sparse double descent, a
winning ticket in the lottery ticket hypothesis surprisingly may not always
win.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：AggNet: Learning to Aggregate Faces for Group Membership Verification</b></summary>
  <p><b>编号</b>：[108]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08683</p>
  <p><b>作者</b>：Marzieh Gheisari,  Javad Amirian,  Teddy Furon,  Laurent Amsaleg</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：face recognition applications, recognition applications, revealing their identity, interested to verify, face recognition</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In some face recognition applications, we are interested to verify whether an
individual is a member of a group, without revealing their identity. Some
existing methods, propose a mechanism for quantizing precomputed face
descriptors into discrete embeddings and aggregating them into one group
representation. However, this mechanism is only optimized for a given closed
set of individuals and needs to learn the group representations from scratch
every time the groups are changed. In this paper, we propose a deep
architecture that jointly learns face descriptors and the aggregation mechanism
for better end-to-end performances. The system can be applied to new groups
with individuals never seen before and the scheme easily manages new
memberships or membership endings. We show through experiments on multiple
large-scale wild-face datasets, that the proposed method leads to higher
verification performance compared to other baselines.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Bridge-Tower: Building Bridges Between Encoders in Vision-Language  Representation Learning</b></summary>
  <p><b>编号</b>：[118]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08657</p>
  <p><b>作者</b>：Xiao Xu,  Chenfei Wu,  Shachar Rosenman,  Vasudev Lal,  Nan Duan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：dominated visual-language representation, visual-language representation learning, uni-modal encoders, recent years, cross-modal encoder</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision-Language (VL) models with the Two-Tower architecture have dominated
visual-language representation learning in recent years. Current VL models
either use lightweight uni-modal encoders and learn to extract, align and fuse
both modalities simultaneously in a cross-modal encoder, or feed the last-layer
uni-modal features directly into the top cross-modal encoder, ignoring the
semantic information at the different levels in the deep uni-modal encoders.
Both approaches possibly restrict vision-language representation learning and
limit model performance. In this paper, we introduce multiple bridge layers
that build a connection between the top layers of uni-modal encoders and each
layer of the cross-modal encoder. This enables comprehensive bottom-up
interactions between visual and textual representations at different semantic
levels, resulting in more effective cross-modal alignment and fusion. Our
proposed Bridge-Tower, pre-trained with only $4$M images, achieves
state-of-the-art performance on various downstream vision-language tasks. On
the VQAv2 test-std set, Bridge-Tower achieves an accuracy of $78.73\%$,
outperforming the previous state-of-the-art METER model by $1.09\%$ with the
same pre-training data and almost no additional parameters and computational
cost. Notably, when further scaling the model, Bridge-Tower achieves an
accuracy of $81.15\%$, surpassing models that are pre-trained on
orders-of-magnitude larger datasets. Code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Learning Implicit Feature Alignment Function for Semantic Segmentation</b></summary>
  <p><b>编号</b>：[120]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08655</p>
  <p><b>作者</b>：Hanzhe Hu,  Yinbo Chen,  Jiarui Xu,  Shubhankar Borse,  Hong Cai,  Fatih Porikli,  Xiaolong Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Integrating high-level context, Integrating high-level, low-level details, central importance, importance in semantic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Integrating high-level context information with low-level details is of
central importance in semantic segmentation. Towards this end, most existing
segmentation models apply bilinear up-sampling and convolutions to feature maps
of different scales, and then align them at the same resolution. However,
bilinear up-sampling blurs the precise information learned in these feature
maps and convolutions incur extra computation costs. To address these issues,
we propose the Implicit Feature Alignment function (IFA). Our method is
inspired by the rapidly expanding topic of implicit neural representations,
where coordinate-based neural networks are used to designate fields of signals.
In IFA, feature vectors are viewed as representing a 2D field of information.
Given a query coordinate, nearby feature vectors with their relative
coordinates are taken from the multi-level feature maps and then fed into an
MLP to generate the corresponding output. As such, IFA implicitly aligns the
feature maps at different levels and is capable of producing segmentation maps
in arbitrary resolutions. We demonstrate the efficacy of IFA on multiple
datasets, including Cityscapes, PASCAL Context, and ADE20K. Our method can be
combined with improvement on various architectures, and it achieves
state-of-the-art computation-accuracy trade-off on common benchmarks. Code will
be made available at this https URL.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：All Mistakes Are Not Equal: Comprehensive Hierarchy Aware Multi-label  Predictions (CHAMP)</b></summary>
  <p><b>编号</b>：[121]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08653</p>
  <p><b>作者</b>：Ashwin Vaswani,  Gaurav Aggarwal,  Praneeth Netrapalli,  Narayan G Hegde</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：domain-specific hierarchy tree, present Comprehensive Hierarchy, hierarchy tree, Aware Multi-label Predictions, domain-specific hierarchy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper considers the problem of Hierarchical Multi-Label Classification
(HMC), where (i) several labels can be present for each example, and (ii)
labels are related via a domain-specific hierarchy tree. Guided by the
intuition that all mistakes are not equal, we present Comprehensive Hierarchy
Aware Multi-label Predictions (CHAMP), a framework that penalizes a
misprediction depending on its severity as per the hierarchy tree. While there
have been works that apply such an idea to single-label classification, to the
best of our knowledge, there are limited such works for multilabel
classification focusing on the severity of mistakes. The key reason is that
there is no clear way of quantifying the severity of a misprediction a priori
in the multilabel setting. In this work, we propose a simple but effective
metric to quantify the severity of a mistake in HMC, naturally leading to
CHAMP. Extensive experiments on six public HMC datasets across modalities
(image, audio, and text) demonstrate that incorporating hierarchical
information leads to substantial gains as CHAMP improves both AUPRC (2.6%
median percentage improvement) and hierarchical metrics (2.85% median
percentage improvement), over stand-alone hierarchical or multilabel
classification methods. Compared to standard multilabel baselines, CHAMP
provides improved AUPRC in both robustness (8.87% mean percentage improvement )
and less data regimes. Further, our method provides a framework to enhance
existing multilabel classification algorithms with better mistakes (18.1% mean
percentage increment).</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Local Slot Attention for Vision-and-Language Navigation</b></summary>
  <p><b>编号</b>：[124]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08645</p>
  <p><b>作者</b>：Yifeng Zhuang,  Qiang Sun,  Yanwei Fu,  Lifeng Chen,  Xiangyang Sue</p>
  <p><b>备注</b>：ICMR 2022</p>
  <p><b>关键词</b>：frontier study aiming, language processing community, natural language processing, VLN task requires, VLN task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision-and-language navigation (VLN), a frontier study aiming to pave the way
for general-purpose robots, has been a hot topic in the computer vision and
natural language processing community. The VLN task requires an agent to
navigate to a goal location following natural language instructions in
unfamiliar environments.
Recently, transformer-based models have gained significant improvements on
the VLN task. Since the attention mechanism in the transformer architecture can
better integrate inter- and intra-modal information of vision and language.
However, there exist two problems in current transformer-based models.
1) The models process each view independently without taking the integrity of
the objects into account.
2) During the self-attention operation in the visual modality, the views that
are spatially distant can be inter-weaved with each other without explicit
restriction. This kind of mixing may introduce extra noise instead of useful
information.
To address these issues, we propose 1) A slot-attention based module to
incorporate information from segmentation of the same object. 2) A local
attention mask mechanism to limit the visual attention span. The proposed
modules can be easily plugged into any VLN architecture and we use the
Recurrent VLN-Bert as our base model. Experiments on the R2R dataset show that
our model has achieved the state-of-the-art results.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Improving Diversity of Multiple Trajectory Prediction based on  Map-adaptive Lane Loss</b></summary>
  <p><b>编号</b>：[127]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08641</p>
  <p><b>作者</b>：Sanmin Kim,  Hyeongseok Jeon,  Junwon Choi,  Dongsuk Kum</p>
  <p><b>备注</b>：11pages 5figures</p>
  <p><b>关键词</b>：autonomous driving tend, Prior arts, trajectory, Trajectory Prediction Attention, field of motion</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Prior arts in the field of motion predictions for autonomous driving tend to
focus on finding a trajectory that is close to the ground truth trajectory.
Such problem formulations and approaches, however, frequently lead to loss of
diversity and biased trajectory predictions. Therefore, they are unsuitable for
real-world autonomous driving where diverse and road-dependent multimodal
trajectory predictions are critical for safety. To this end, this study
proposes a novel loss function, \textit{Lane Loss}, that ensures map-adaptive
diversity and accommodates geometric constraints. A two-stage trajectory
prediction architecture with a novel trajectory candidate proposal module,
\textit{Trajectory Prediction Attention (TPA)}, is trained with Lane Loss
encourages multiple trajectories to be diversely distributed, covering feasible
maneuvers in a map-aware manner. Furthermore, considering that the existing
trajectory performance metrics are focusing on evaluating the accuracy based on
the ground truth future trajectory, a quantitative evaluation metric is also
suggested to evaluate the diversity of predicted multiple trajectories. The
experiments performed on the Argoverse dataset show that the proposed method
significantly improves the diversity of the predicted trajectories without
sacrificing the prediction accuracy.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Uncertainty-aware Evaluation of Time-Series Classification for Online  Handwriting Recognition with Domain Shift</b></summary>
  <p><b>编号</b>：[128]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08640</p>
  <p><b>作者</b>：Andreas Klaß,  Sven M. Lorenz,  Martin W. Lauer-Schmaltz,  David Rügamer,  Bernd Bischl,  Christopher Mutschler,  Felix Ott</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：machine learning model, machine learning, computer vision applications, data, applications</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>For many applications, analyzing the uncertainty of a machine learning model
is indispensable. While research of uncertainty quantification (UQ) techniques
is very advanced for computer vision applications, UQ methods for
spatio-temporal data are less studied. In this paper, we focus on models for
online handwriting recognition, one particular type of spatio-temporal data.
The data is observed from a sensor-enhanced pen with the goal to classify
written characters. We conduct a broad evaluation of aleatoric (data) and
epistemic (model) UQ based on two prominent techniques for Bayesian inference,
Stochastic Weight Averaging-Gaussian (SWAG) and Deep Ensembles. Next to a
better understanding of the model, UQ techniques can detect out-of-distribution
data and domain shifts when combining right-handed and left-handed writers (an
underrepresented group).</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：Minimum Noticeable Difference based Adversarial Privacy Preserving Image  Generation</b></summary>
  <p><b>编号</b>：[130]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08638</p>
  <p><b>作者</b>：Wen Sun,  Jian Jin,  Weisi Lin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Deep learning models, adversarial, adversarial image generation, Deep learning, learning models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning models are found to be vulnerable to adversarial examples, as
wrong predictions can be caused by small perturbation in input for deep
learning models. Most of the existing works of adversarial image generation try
to achieve attacks for most models, while few of them make efforts on
guaranteeing the perceptual quality of the adversarial examples. High quality
adversarial examples matter for many applications, especially for the privacy
preserving. In this work, we develop a framework based on the Minimum
Noticeable Difference (MND) concept to generate adversarial privacy preserving
images that have minimum perceptual difference from the clean ones but are able
to attack deep learning models. To achieve this, an adversarial loss is firstly
proposed to make the deep learning models attacked by the adversarial images
successfully. Then, a perceptual quality-preserving loss is developed by taking
the magnitude of perturbation and perturbation-caused structural and gradient
changes into account, which aims to preserve high perceptual quality for
adversarial image generation. To the best of our knowledge, this is the first
work on exploring quality-preserving adversarial image generation based on the
MND concept for privacy preserving. To evaluate its performance in terms of
perceptual quality, the deep models on image classification and face
recognition are tested with the proposed method and several anchor methods in
this work. Extensive experimental results demonstrate that the proposed MND
framework is capable of generating adversarial images with remarkably improved
performance metrics (e.g., PSNR, SSIM, and MOS) than that generated with the
anchor methods.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Learning Using Privileged Information for Zero-Shot Action Recognition</b></summary>
  <p><b>编号</b>：[131]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08632</p>
  <p><b>作者</b>：Zhiyi Gao,  Wanqing Li,  Zihui Guo,  Bin Yu,  Yonghong Hou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Zero-Shot Action Recognition, recognize video actions, Action Recognition, aims to recognize, Zero-Shot Action</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Zero-Shot Action Recognition (ZSAR) aims to recognize video actions that have
never been seen during training. Most existing methods assume a shared semantic
space between seen and unseen actions and intend to directly learn a mapping
from a visual space to the semantic space. This approach has been challenged by
the semantic gap between the visual space and semantic space. This paper
presents a novel method that uses object semantics as privileged information to
narrow the semantic gap and, hence, effectively, assist the learning. In
particular, a simple hallucination network is proposed to implicitly extract
object semantics during testing without explicitly extracting objects and a
cross-attention module is developed to augment visual feature with the object
semantics. Experiments on the Olympic Sports, HMDB51 and UCF101 datasets have
shown that the proposed method outperforms the state-of-the-art methods by a
large margin.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Understanding Aesthetics with Language: A Photo Critique Dataset for  Aesthetic Assessment</b></summary>
  <p><b>编号</b>：[138]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08614</p>
  <p><b>作者</b>：Daniel Vera Nieto,  Luigi Celona,  Clara Fernandez-Labrador</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ill-defined task due, Computational inference, subjective nature, ill-defined task, task due</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Computational inference of aesthetics is an ill-defined task due to its
subjective nature. Many datasets have been proposed to tackle the problem by
providing pairs of images and aesthetic scores based on human ratings. However,
humans are better at expressing their opinion, taste, and emotions by means of
language rather than summarizing them in a single number. In fact, photo
critiques provide much richer information as they reveal how and why users rate
the aesthetics of visual stimuli. In this regard, we propose the Reddit Photo
Critique Dataset (RPCD), which contains tuples of image and photo critiques.
RPCD consists of 74K images and 220K comments and is collected from a Reddit
community used by hobbyists and professional photographers to improve their
photography skills by leveraging constructive community feedback. The proposed
dataset differs from previous aesthetics datasets mainly in three aspects,
namely (i) the large scale of the dataset and the extension of the comments
criticizing different aspects of the image, (ii) it contains mostly UltraHD
images, and (iii) it can easily be extended to new data as it is collected
through an automatic pipeline. To the best of our knowledge, in this work, we
propose the first attempt to estimate the aesthetic quality of visual stimuli
from the critiques. To this end, we exploit the polarity of the sentiment of
criticism as an indicator of aesthetic judgment. We demonstrate how sentiment
polarity correlates positively with the aesthetic judgment available for two
aesthetic assessment benchmarks. Finally, we experiment with several models by
using the sentiment scores as a target for ranking images. Dataset and
baselines are available (this https URL).</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Masked Autoencoders for Generic Event Boundary Detection CVPR'2022  Kinetics-GEBD Challenge</b></summary>
  <p><b>编号</b>：[140]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08610</p>
  <p><b>作者</b>：Rui He,  Yuanxi Sun,  Youzeng Li,  Zuwei Huang,  Feng Hu,  Xu Cheng,  Jie Tang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Event Boundary Detection, Generic Event Boundary, Boundary Detection, Event Boundary, taxonomy-free event boundaries</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generic Event Boundary Detection (GEBD) tasks aim at detecting generic,
taxonomy-free event boundaries that segment a whole video into chunks. In this
paper, we apply Masked Autoencoders to improve algorithm performance on the
GEBD tasks. Our approach mainly adopted the ensemble of Masked Autoencoders
fine-tuned on the GEBD task as a self-supervised learner with other base
models. Moreover, we also use a semi-supervised pseudo-label method to take
full advantage of the abundant unlabeled Kinetics-400 data while training. In
addition, we propose a soft-label method to partially balance the positive and
negative samples and alleviate the problem of ambiguous labeling in this task.
Lastly, a tricky segmentation alignment policy is implemented to refine
boundaries predicted by our models to more accurate locations. With our
approach, we achieved 85.94% on the F1-score on the Kinetics-GEBD test set,
which improved the F1-score by 2.31% compared to the winner of the 2021
Kinetics-GEBD Challenge. Our code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：On Efficient Real-Time Semantic Segmentation: A Survey</b></summary>
  <p><b>编号</b>：[143]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08605</p>
  <p><b>作者</b>：Christopher J. Holder,  Muhammad Shafique</p>
  <p><b>备注</b>：18 pages, 13 figures, 4 tables This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible</p>
  <p><b>关键词</b>：facilitating scene understanding, vehicle vision stack, autonomous vehicle vision, semantic segmentation models, performing semantic segmentation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Semantic segmentation is the problem of assigning a class label to every
pixel in an image, and is an important component of an autonomous vehicle
vision stack for facilitating scene understanding and object detection.
However, many of the top performing semantic segmentation models are extremely
complex and cumbersome, and as such are not suited to deployment onboard
autonomous vehicle platforms where computational resources are limited and
low-latency operation is a vital requirement. In this survey, we take a
thorough look at the works that aim to address this misalignment with more
compact and efficient models capable of deployment on low-memory embedded
systems while meeting the constraint of real-time inference. We discuss several
of the most prominent works in the field, placing them within a taxonomy based
on their major contributions, and finally we evaluate the inference speed of
the discussed models under consistent hardware and software setups that
represent a typical research environment with high-end GPU and a realistic
deployed scenario using low-memory embedded GPU hardware. Our experimental
results demonstrate that many works are capable of real-time performance on
resource-constrained hardware, while illustrating the consistent trade-off
between latency and accuracy.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：HairFIT: Pose-Invariant Hairstyle Transfer via Flow-based Hair Alignment  and Semantic-Region-Aware Inpainting</b></summary>
  <p><b>编号</b>：[151]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08585</p>
  <p><b>作者</b>：Chaeyeon Chung,  Taewoo Kim,  Hyelin Nam,  Seunghwan Choi,  Gyojung Gu,  Sunghyun Park,  Jaegul Choo</p>
  <p><b>备注</b>：BMVC 2021 Oral Presentation</p>
  <p><b>关键词</b>：hair, Hairstyle transfer, source, transfer, Hairstyle</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Hairstyle transfer is the task of modifying a source hairstyle to a target
one. Although recent hairstyle transfer models can reflect the delicate
features of hairstyles, they still have two major limitations. First, the
existing methods fail to transfer hairstyles when a source and a target image
have different poses (e.g., viewing direction or face size), which is prevalent
in the real world. Also, the previous models generate unrealistic images when
there is a non-trivial amount of regions in the source image occluded by its
original hair. When modifying long hair to short hair, shoulders or backgrounds
occluded by the long hair need to be inpainted. To address these issues, we
propose a novel framework for pose-invariant hairstyle transfer, HairFIT. Our
model consists of two stages: 1) flow-based hair alignment and 2) hair
synthesis. In the hair alignment stage, we leverage a keypoint-based optical
flow estimator to align a target hairstyle with a source pose. Then, we
generate a final hairstyle-transferred image in the hair synthesis stage based
on Semantic-region-aware Inpainting Mask (SIM) estimator. Our SIM estimator
divides the occluded regions in the source image into different semantic
regions to reflect their distinct features during the inpainting. To
demonstrate the effectiveness of our model, we conduct quantitative and
qualitative evaluations using multi-view datasets, K-hairstyle and VoxCeleb.
The results indicate that HairFIT achieves a state-of-the-art performance by
successfully transferring hairstyles between images of different poses, which
has never been achieved before.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Enhanced Bi-directional Motion Estimation for Video Frame Interpolation</b></summary>
  <p><b>编号</b>：[157]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08572</p>
  <p><b>作者</b>：Jin Xin,  Wu Longhai,  Shen Guotao,  Chen Youxin,  Chen Jie,  Koo Jayoon,  Hahm Cheul-hee</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：simple yet effective, effective algorithm, video frame interpolation, Existing motion-based interpolation, frame interpolation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a novel simple yet effective algorithm for motion-based video
frame interpolation. Existing motion-based interpolation methods typically rely
on a pre-trained optical flow model or a U-Net based pyramid network for motion
estimation, which either suffer from large model size or limited capacity in
handling complex and large motion cases. In this work, by carefully integrating
intermediateoriented forward-warping, lightweight feature encoder, and
correlation volume into a pyramid recurrent framework, we derive a compact
model to simultaneously estimate the bidirectional motion between input frames.
It is 15 times smaller in size than PWC-Net, yet enables more reliable and
flexible handling of challenging motion cases. Based on estimated
bi-directional motion, we forward-warp input frames and their context features
to intermediate frame, and employ a synthesis network to estimate the
intermediate frame from warped representations. Our method achieves excellent
performance on a broad range of video frame interpolation benchmarks. Code will
be available soon.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Multi-Contextual Predictions with Vision Transformer for Video Anomaly  Detection</b></summary>
  <p><b>编号</b>：[160]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08568</p>
  <p><b>作者</b>：Joo-Yeon Lee,  Woo-Jeoung Nam,  Seong-Whan Lee</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Video Anomaly Detection, main methodologies, traditionally tackled, Video, Anomaly Detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Video Anomaly Detection(VAD) has been traditionally tackled in two main
methodologies: the reconstruction-based approach and the prediction-based one.
As the reconstruction-based methods learn to generalize the input image, the
model merely learns an identity function and strongly causes the problem called
generalizing issue. On the other hand, since the prediction-based ones learn to
predict a future frame given several previous frames, they are less sensitive
to the generalizing issue. However, it is still uncertain if the model can
learn the spatio-temporal context of a video. Our intuition is that the
understanding of the spatio-temporal context of a video plays a vital role in
VAD as it provides precise information on how the appearance of an event in a
video clip changes. Hence, to fully exploit the context information for anomaly
detection in video circumstances, we designed the transformer model with three
different contextual prediction streams: masked, whole and partial. By learning
to predict the missing frames of consecutive normal frames, our model can
effectively learn various normality patterns in the video, which leads to a
high reconstruction error at the abnormal cases that are unsuitable to the
learned context. To verify the effectiveness of our approach, we assess our
model on the public benchmark datasets: USCD Pedestrian 2, CUHK Avenue and
ShanghaiTech and evaluate the performance with the anomaly score metric of
reconstruction error. The results demonstrate that our proposed approach
achieves a competitive performance compared to the existing video anomaly
detection methods.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Rectify ViT Shortcut Learning by Visual Saliency</b></summary>
  <p><b>编号</b>：[161]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08567</p>
  <p><b>作者</b>：Chong Ma,  Lin Zhao,  Yuzhong Chen,  David Weizhong Liu,  Xi Jiang,  Tuo Zhang,  Xintao Hu,  Dinggang Shen,  Dajiang Zhu,  Tianming Liu</p>
  <p><b>备注</b>：NeurIPS2022 Under Review</p>
  <p><b>关键词</b>：degenerated feature representations, Shortcut learning, deep learning models, learning, leading to degenerated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Shortcut learning is common but harmful to deep learning models, leading to
degenerated feature representations and consequently jeopardizing the model's
generalizability and interpretability. However, shortcut learning in the widely
used Vision Transformer framework is largely unknown. Meanwhile, introducing
domain-specific knowledge is a major approach to rectifying the shortcuts,
which are predominated by background related factors. For example, in the
medical imaging field, eye-gaze data from radiologists is an effective human
visual prior knowledge that has the great potential to guide the deep learning
models to focus on meaningful foreground regions of interest. However,
obtaining eye-gaze data is time-consuming, labor-intensive and sometimes even
not practical. In this work, we propose a novel and effective saliency-guided
vision transformer (SGT) model to rectify shortcut learning in ViT with the
absence of eye-gaze data. Specifically, a computational visual saliency model
is adopted to predict saliency maps for input image samples. Then, the saliency
maps are used to distil the most informative image patches. In the proposed
SGT, the self-attention among image patches focus only on the distilled
informative ones. Considering this distill operation may lead to global
information lost, we further introduce, in the last encoder layer, a residual
connection that captures the self-attention across all the image patches. The
experiment results on four independent public datasets show that our SGT
framework can effectively learn and leverage human prior knowledge without eye
gaze data and achieves much better performance than baselines. Meanwhile, it
successfully rectifies the harmful shortcut learning and significantly improves
the interpretability of the ViT model, demonstrating the promise of
transferring human prior knowledge derived visual saliency in rectifying
shortcut learning</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Active Data Discovery: Mining Unknown Data using Submodular Information  Measures</b></summary>
  <p><b>编号</b>：[162]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08566</p>
  <p><b>作者</b>：Suraj Kothawade,  Shivang Chopra,  Saikat Ghosh,  Rishabh Iyer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：adaptively sampling subsets, rare data instances, rare data, Active Learning, rare</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Active Learning is a very common yet powerful framework for iteratively and
adaptively sampling subsets of the unlabeled sets with a human in the loop with
the goal of achieving labeling efficiency. Most real world datasets have
imbalance either in classes and slices, and correspondingly, parts of the
dataset are rare. As a result, there has been a lot of work in designing active
learning approaches for mining these rare data instances. Most approaches
assume access to a seed set of instances which contain these rare data
instances. However, in the event of more extreme rareness, it is reasonable to
assume that these rare data instances (either classes or slices) may not even
be present in the seed labeled set, and a critical need for the active learning
paradigm is to efficiently discover these rare data instances. In this work, we
provide an active data discovery framework which can mine unknown data slices
and classes efficiently using the submodular conditional gain and submodular
conditional mutual information functions. We provide a general algorithmic
framework which works in a number of scenarios including image classification
and object detection and works with both rare classes and rare slices present
in the unlabeled set. We show significant accuracy and labeling efficiency
gains with our approach compared to existing state-of-the-art active learning
approaches for actively discovering these rare classes and slices.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Rarity Score : A New Metric to Evaluate the Uncommonness of Synthesized  Images</b></summary>
  <p><b>编号</b>：[169]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08549</p>
  <p><b>作者</b>：Jiyeon Han,  Hwanil Choi,  Yunjey Choi,  Junho Kim,  Jung-Woo Ha,  Jaesik Choi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：image synthesis play, synthesis play, play a key, key role, generative models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Evaluation metrics in image synthesis play a key role to measure performances
of generative models. However, most metrics mainly focus on image fidelity.
Existing diversity metrics are derived by comparing distributions, and thus
they cannot quantify the diversity or rarity degree of each generated image. In
this work, we propose a new evaluation metric, called `rarity score', to
measure the individual rarity of each image synthesized by generative models.
We first show empirical observation that common samples are close to each other
and rare samples are far from each other in nearest-neighbor distances of
feature space. We then use our metric to demonstrate that the extent to which
different generative models produce rare images can be effectively compared. We
also propose a method to compare rarities between datasets that share the same
concept such as CelebA-HQ and FFHQ. Finally, we analyze the use of metrics in
different designs of feature spaces to better understand the relationship
between feature spaces and resulting sparse images. Code will be publicly
available online for the research community.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Texture Generation Using Graph Generative Adversarial Network And  Differentiable Rendering</b></summary>
  <p><b>编号</b>：[170]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08547</p>
  <p><b>作者</b>：Dharma KC,  Clayton T. Morrison,  Bradley Walls</p>
  <p><b>备注</b>：17 pages</p>
  <p><b>关键词</b>：photo realistic asset, realistic asset generation, important step, step towards photo, photo realistic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Novel texture synthesis for existing 3D mesh models is an important step
towards photo realistic asset generation for existing simulators. But existing
methods inherently work in the 2D image space which is the projection of the 3D
space from a given camera perspective. These methods take camera angle, 3D
model information, lighting information and generate photorealistic 2D image.
To generate a photorealistic image from another perspective or lighting, we
need to make a computationally expensive forward pass each time we change the
parameters. Also, it is hard to generate such images for a simulator that can
satisfy the temporal constraints the sequences of images should be similar but
only need to change the viewpoint of lighting as desired. The solution can not
be directly integrated with existing tools like Blender and Unreal Engine.
Manual solution is expensive and time consuming. We thus present a new system
called a graph generative adversarial network (GGAN) that can generate textures
which can be directly integrated into a given 3D mesh models with tools like
Blender and Unreal Engine and can be simulated from any perspective and
lighting condition easily.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Large-Margin Representation Learning for Texture Classification</b></summary>
  <p><b>编号</b>：[173]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08537</p>
  <p><b>作者</b>：Jonathan de Matos,  Luiz Eduardo Soares de Oliveira,  Alceu de Souza Britto Junior,  Alessandro Lameiras Koerich</p>
  <p><b>备注</b>：7 pages</p>
  <p><b>关键词</b>：combining convolutional layers, paper presents, approach combining convolutional, large-margin metric learning, training supervised models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a novel approach combining convolutional layers (CLs) and
large-margin metric learning for training supervised models on small datasets
for texture classification. The core of such an approach is a loss function
that computes the distances between instances of interest and support vectors.
The objective is to update the weights of CLs iteratively to learn a
representation with a large margin between classes. Each iteration results in a
large-margin discriminant model represented by support vectors based on such a
representation. The advantage of the proposed approach w.r.t. convolutional
neural networks (CNNs) is two-fold. First, it allows representation learning
with a small amount of data due to the reduced number of parameters compared to
an equivalent CNN. Second, it has a low training cost since the backpropagation
considers only support vectors. The experimental results on texture and
histopathologic image datasets have shown that the proposed approach achieves
competitive accuracy with lower computational cost and faster convergence when
compared to equivalent CNNs.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：CDNet: Contrastive Disentangled Network for Fine-Grained Image  Categorization of Ocular B-Scan Ultrasound</b></summary>
  <p><b>编号</b>：[179]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08524</p>
  <p><b>作者</b>：Ruilong Dan,  Yunxiang Li,  Yijie Wang,  Gangyong Jia,  Ruiquan Ge,  Juan Ye,  Qun Jin,  Yaqi Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：B-scan ultrasound modality, B-scan ultrasound, diagnosing ocular diseases, Precise and rapid, modality is vital</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Precise and rapid categorization of images in the B-scan ultrasound modality
is vital for diagnosing ocular diseases. Nevertheless, distinguishing various
diseases in ultrasound still challenges experienced ophthalmologists. Thus a
novel contrastive disentangled network (CDNet) is developed in this work,
aiming to tackle the fine-grained image categorization (FGIC) challenges of
ocular abnormalities in ultrasound images, including intraocular tumor (IOT),
retinal detachment (RD), posterior scleral staphyloma (PSS), and vitreous
hemorrhage (VH). Three essential components of CDNet are the weakly-supervised
lesion localization module (WSLL), contrastive multi-zoom (CMZ) strategy, and
hyperspherical contrastive disentangled loss (HCD-Loss), respectively. These
components facilitate feature disentanglement for fine-grained recognition in
both the input and output aspects. The proposed CDNet is validated on our ZJU
Ocular Ultrasound Dataset (ZJUOUSD), consisting of 5213 samples. Furthermore,
the generalization ability of CDNet is validated on two public and widely-used
chest X-ray FGIC benchmarks. Quantitative and qualitative results demonstrate
the efficacy of our proposed CDNet, which achieves state-of-the-art performance
in the FGIC task. Code is available at:
this https URL .</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：VLMbench: A Compositional Benchmark for Vision-and-Language Manipulation</b></summary>
  <p><b>编号</b>：[181]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08522</p>
  <p><b>作者</b>：Kaizhi Zheng,  Xiaotong Chen,  Odest Chadwicke Jenkins,  Xin Eric Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：humans naturally intend, Automatic Manipulation Solver, flexibility and compositionality, naturally intend, manipulation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Benefiting from language flexibility and compositionality, humans naturally
intend to use language to command an embodied agent for complex tasks such as
navigation and object manipulation. In this work, we aim to fill the blank of
the last mile of embodied agents -- object manipulation by following human
guidance, e.g., "move the red mug next to the box while keeping it upright." To
this end, we introduce an Automatic Manipulation Solver (AMSolver) simulator
and build a Vision-and-Language Manipulation benchmark (VLMbench) based on it,
containing various language instructions on categorized robotic manipulation
tasks. Specifically, modular rule-based task templates are created to
automatically generate robot demonstrations with language instructions,
consisting of diverse object shapes and appearances, action types, and motion
constraints. We also develop a keypoint-based model 6D-CLIPort to deal with
multi-view observations and language input and output a sequence of 6 degrees
of freedom (DoF) actions. We hope the new simulator and benchmark will
facilitate future research on language-guided robotic manipulation.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Effective Solid State LiDAR Odometry Using Continuous-time Filter  Registration</b></summary>
  <p><b>编号</b>：[184]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08517</p>
  <p><b>作者</b>：Xin Zheng,  Jianke Zhu</p>
  <p><b>备注</b>：8 pages, 6 figures</p>
  <p><b>关键词</b>：autonomous driving recently, conventional mechanical multi-line, mechanical multi-line spinning, multi-line spinning LiDARs, driving recently</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Solid-state LiDARs are more compact and cheaper than the conventional
mechanical multi-line spinning LiDARs, which have become increasingly popular
in autonomous driving recently. However, there are several challenges for these
new LiDAR sensors, including severe motion distortions, small field of view and
sparse point cloud, which hinder them from being widely used in LiDAR odometry.
To tackle these problems, we present an effective continuous-time LiDAR
odometry (ECTLO) method for the Risley prism-based LiDARs with non-repetitive
scanning patterns. To account for the noisy data, a filter-based point-to-plane
Gaussian Mixture Model is used for robust registration. Moreover, a LiDAR-only
continuous-time motion model is employed to relieve the inevitable distortions.
To facilitate the implicit data association in parallel, we maintain all map
points within a single range image. Extensive experiments have been conducted
on various testbeds using the solid-state LiDARs with different scanning
patterns, whose promising results demonstrate the efficacy of our proposed
approach.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Neural Architecture Adaptation for Object Detection by Searching Channel  Dimensions and Mapping Pre-trained Parameters</b></summary>
  <p><b>编号</b>：[189]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08509</p>
  <p><b>作者</b>：Harim Jung,  Myeong-Seok Oh,  Cheoljong Yang,  Seong-Whan Lee</p>
  <p><b>备注</b>：Accepted to ICPR 2022</p>
  <p><b>关键词</b>：object detection, object detection frameworks, image classification, backbone architectures originally, object</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Most object detection frameworks use backbone architectures originally
designed for image classification, conventionally with pre-trained parameters
on ImageNet. However, image classification and object detection are essentially
different tasks and there is no guarantee that the optimal backbone for
classification is also optimal for object detection. Recent neural architecture
search (NAS) research has demonstrated that automatically designing a backbone
specifically for object detection helps improve the overall accuracy. In this
paper, we introduce a neural architecture adaptation method that can optimize
the given backbone for detection purposes, while still allowing the use of
pre-trained parameters. We propose to adapt both the micro- and
macro-architecture by searching for specific operations and the number of
layers, in addition to the output channel dimensions of each block. It is
important to find the optimal channel depth, as it greatly affects the feature
representation capability and computation cost. We conduct experiments with our
searched backbone for object detection and demonstrate that our backbone
outperforms both manually designed and searched state-of-the-art backbones on
the COCO dataset.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：What do navigation agents learn about their environment?</b></summary>
  <p><b>编号</b>：[192]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08500</p>
  <p><b>作者</b>：Kshitij Dwivedi,  Gemma Roig,  Aniruddha Kembhavi,  Roozbeh Mottaghi</p>
  <p><b>备注</b>：CVPR 2022</p>
  <p><b>关键词</b>：models trained end, agents typically consist, deep learning models, art visual navigation, learning models trained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Today's state of the art visual navigation agents typically consist of large
deep learning models trained end to end. Such models offer little to no
interpretability about the learned skills or the actions of the agent taken in
response to its environment. While past works have explored interpreting deep
learning models, little attention has been devoted to interpreting embodied AI
systems, which often involve reasoning about the structure of the environment,
target characteristics and the outcome of one's actions. In this paper, we
introduce the Interpretability System for Embodied agEnts (iSEE) for Point Goal
and Object Goal navigation agents. We use iSEE to probe the dynamic
representations produced by these agents for the presence of information about
the agent as well as the environment. We demonstrate interesting insights about
navigation agents using iSEE, including the ability to encode reachable
locations (to avoid obstacles), visibility of the target, progress from the
initial spawn location as well as the dramatic effect on the behaviors of
agents when we mask out critical individual neurons. The code is available at:
this https URL</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Unsupervised Kinematic Motion Detection for Part-segmented 3D Shape  Collections</b></summary>
  <p><b>编号</b>：[194]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08497</p>
  <p><b>作者</b>：Xianghao Xu,  Yifan Ruan,  Srinath Sridhar,  Daniel Ritchie</p>
  <p><b>备注</b>：SIGGRAPH 2022</p>
  <p><b>关键词</b>：populating virtual worlds, synthetic data generation, models of manufactured, vision and robotics, important for populating</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>3D models of manufactured objects are important for populating virtual worlds
and for synthetic data generation for vision and robotics. To be most useful,
such objects should be articulated: their parts should move when interacted
with. While articulated object datasets exist, creating them is
labor-intensive. Learning-based prediction of part motions can help, but all
existing methods require annotated training data. In this paper, we present an
unsupervised approach for discovering articulated motions in a part-segmented
3D shape collection. Our approach is based on a concept we call category
closure: any valid articulation of an object's parts should keep the object in
the same semantic category (e.g. a chair stays a chair). We operationalize this
concept with an algorithm that optimizes a shape's part motion parameters such
that it can transform into other shapes in the collection. We evaluate our
approach by using it to re-discover part motions from the PartNet-Mobility
dataset. For almost all shape categories, our method's predicted motion
parameters have low error with respect to ground truth annotations,
outperforming two supervised motion prediction methods.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Controllable Image Enhancement</b></summary>
  <p><b>编号</b>：[202]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08488</p>
  <p><b>作者</b>：Heewon Kim,  Kyoung Mu Lee</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Editing flat-looking images, stunning photographs requires, Editing flat-looking, photographs requires skill, stunning photographs</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Editing flat-looking images into stunning photographs requires skill and
time. Automated image enhancement algorithms have attracted increased interest
by generating high-quality images without user interaction. However, the
quality assessment of a photograph is subjective. Even in tone and color
adjustments, a single photograph of auto-enhancement is challenging to fit user
preferences which are subtle and even changeable. To address this problem, we
present a semiautomatic image enhancement algorithm that can generate
high-quality images with multiple styles by controlling a few parameters. We
first disentangle photo retouching skills from high-quality images and build an
efficient enhancement system for each skill. Specifically, an encoder-decoder
framework encodes the retouching skills into latent codes and decodes them into
the parameters of image signal processing (ISP) functions. The ISP functions
are computationally efficient and consist of only 19 parameters. Despite our
approach requiring multiple inferences to obtain the desired result,
experimental results present that the proposed method achieves state-of-the-art
performances on the benchmark dataset for image quality and model efficiency.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Backdoor Attacks on Vision Transformers</b></summary>
  <p><b>编号</b>：[209]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08477</p>
  <p><b>作者</b>：Akshayvarun Subramanya,  Aniruddha Saha,  Soroush Abbasi Koohpayegani,  Ajinkya Tejankar,  Hamed Pirsiavash</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：recently demonstrated exemplary, Vision Transformers, demonstrated exemplary performance, vision tasks, recently demonstrated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision Transformers (ViT) have recently demonstrated exemplary performance on
a variety of vision tasks and are being used as an alternative to CNNs. Their
design is based on a self-attention mechanism that processes images as a
sequence of patches, which is quite different compared to CNNs. Hence it is
interesting to study if ViTs are vulnerable to backdoor attacks. Backdoor
attacks happen when an attacker poisons a small part of the training data for
malicious purposes. The model performance is good on clean test images, but the
attacker can manipulate the decision of the model by showing the trigger at
test time. To the best of our knowledge, we are the first to show that ViTs are
vulnerable to backdoor attacks. We also find an intriguing difference between
ViTs and CNNs - interpretation algorithms effectively highlight the trigger on
test images for ViTs but not for CNNs. Based on this observation, we propose a
test-time image blocking defense for ViTs which reduces the attack success rate
by a large margin. Code is available here:
this https URL</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Zero-Shot AutoML with Pretrained Models</b></summary>
  <p><b>编号</b>：[210]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08476</p>
  <p><b>作者</b>：Ekrem Öztürk,  Fabio Ferreira,  Hadi S. Jomaa,  Lars Schmidt-Thieme,  Josif Grabocka,  Frank Hutter</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：low compute budget, compute budget, risking overfitting, low compute, fine-tuning hyperparameters</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Given a new dataset D and a low compute budget, how should we choose a
pre-trained model to fine-tune to D, and set the fine-tuning hyperparameters
without risking overfitting, particularly if D is small? Here, we extend
automated machine learning (AutoML) to best make these choices. Our
domain-independent meta-learning approach learns a zero-shot surrogate model
which, at test time, allows to select the right deep learning (DL) pipeline
(including the pre-trained model and fine-tuning hyperparameters) for a new
dataset D given only trivial meta-features describing D such as image
resolution or the number of classes. To train this zero-shot model, we collect
performance data for many DL pipelines on a large collection of datasets and
meta-train on this data to minimize a pairwise ranking objective. We evaluate
our approach under the strict time limit of the vision track of the ChaLearn
AutoDL challenge benchmark, clearly outperforming all challenge contenders.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：Recursive Neural Programs: Variational Learning of Image Grammars and  Part-Whole Hierarchies</b></summary>
  <p><b>编号</b>：[216]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08462</p>
  <p><b>作者</b>：Ares Fisher,  Rajesh P.N. Rao</p>
  <p><b>备注</b>：Code repository up soon :)</p>
  <p><b>关键词</b>：Human vision involves, structured representations based, vision involves parsing, representations based, Human vision</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Human vision involves parsing and representing objects and scenes using
structured representations based on part-whole hierarchies. Computer vision and
machine learning researchers have recently sought to emulate this capability
using capsule networks, reference frames and active predictive coding, but a
generative model formulation has been lacking. We introduce Recursive Neural
Programs (RNPs), which, to our knowledge, is the first neural generative model
to address the part-whole hierarchy learning problem. RNPs model images as
hierarchical trees of probabilistic sensory-motor programs that recursively
reuse learned sensory-motor primitives to model an image within different
reference frames, forming recursive image grammars. We express RNPs as
structured variational autoencoders (sVAEs) for inference and sampling, and
demonstrate parts-based parsing, sampling and one-shot transfer learning for
MNIST, Omniglot and Fashion-MNIST datasets, demonstrating the model's
expressive power. Our results show that RNPs provide an intuitive and
explainable way of composing objects and scenes, allowing rich compositionality
and intuitive interpretations of objects in terms of part-whole hierarchies.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：TUSK: Task-Agnostic Unsupervised Keypoints</b></summary>
  <p><b>编号</b>：[217]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08460</p>
  <p><b>作者</b>：Yuhe Jin,  Weiwei Sun,  Jan Hosang,  Eduard Trulls,  Kwang Moo Yi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：abstract geometric shape, learning rely heavily, abstract geometric, geometric shape, rely heavily</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing unsupervised methods for keypoint learning rely heavily on the
assumption that a specific keypoint type (e.g. elbow, digit, abstract geometric
shape) appears only once in an image. This greatly limits their applicability,
as each instance must be isolated before applying the method-an issue that is
never discussed or evaluated. We thus propose a novel method to learn
Task-agnostic, UnSupervised Keypoints (TUSK) which can deal with multiple
instances. To achieve this, instead of the commonly-used strategy of detecting
multiple heatmaps, each dedicated to a specific keypoint type, we use a single
heatmap for detection, and enable unsupervised learning of keypoint types
through clustering. Specifically, we encode semantics into the keypoints by
teaching them to reconstruct images from a sparse set of keypoints and their
descriptors, where the descriptors are forced to form distinct clusters in
feature space around learned prototypes. This makes our approach amenable to a
wider range of tasks than any previous unsupervised keypoint method: we show
experiments on multiple-instance detection and classification, object
discovery, and landmark detection-all unsupervised-with performance on par with
the state of the art, while also being able to deal with multiple instances.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Scalable Temporal Localization of Sensitive Activities in Movies and TV  Episodes</b></summary>
  <p><b>编号</b>：[231]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08429</p>
  <p><b>作者</b>：Xiang Hao,  Jingxiang Chen,  Shixing Chen,  Ahmed Saad,  Raffay Hamid</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：better-informed viewing choices, customers make better-informed, make better-informed viewing, viewing choices, video-streaming services</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To help customers make better-informed viewing choices, video-streaming
services try to moderate their content and provide more visibility into which
portions of their movies and TV episodes contain age-appropriate material
(e.g., nudity, sex, violence, or drug-use). Supervised models to localize these
sensitive activities require large amounts of clip-level labeled data which is
hard to obtain, while weakly-supervised models to this end usually do not offer
competitive accuracy. To address this challenge, we propose a novel Coarse2Fine
network designed to make use of readily obtainable video-level weak labels in
conjunction with sparse clip-level labels of age-appropriate activities. Our
model aggregates frame-level predictions to make video-level classifications
and is therefore able to leverage sparse clip-level labels along with
video-level labels. Furthermore, by performing frame-level predictions in a
hierarchical manner, our approach is able to overcome the label-imbalance
problem caused due to the rare-occurrence nature of age-appropriate content. We
present comparative results of our approach using 41,234 movies and TV episodes
(~3 years of video-content) from 521 sub-genres and 250 countries making it by
far the largest-scale empirical analysis of age-appropriate activity
localization in long-form videos ever published. Our approach offers 107.2%
relative mAP improvement (from 5.5% to 11.4%) over existing state-of-the-art
activity-localization approaches.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：EyeNeRF: A Hybrid Representation for Photorealistic Synthesis, Animation  and Relighting of Human Eyes</b></summary>
  <p><b>编号</b>：[232]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08428</p>
  <p><b>作者</b>：Gengyan Li (1 and 2),  Abhimitra Meka (1),  Franziska Müller (1),  Marcel C. Bühler (2),  Otmar Hilliges (2) ((1) Google Inc., (2) ETH Zürich)</p>
  <p><b>备注</b>：16 pages, 16 figures, 1 table, to be published in ACM Transactions on Graphics (TOG) (Volume: 41, Issue: 4), 2022</p>
  <p><b>关键词</b>：creating high-quality animatable, modeling human eyes, animatable and relightable, avatars of people, eye region</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A unique challenge in creating high-quality animatable and relightable 3D
avatars of people is modeling human eyes. The challenge of synthesizing eyes is
multifold as it requires 1) appropriate representations for the various
components of the eye and the periocular region for coherent viewpoint
synthesis, capable of representing diffuse, refractive and highly reflective
surfaces, 2) disentangling skin and eye appearance from environmental
illumination such that it may be rendered under novel lighting conditions, and
3) capturing eyeball motion and the deformation of the surrounding skin to
enable re-gazing. These challenges have traditionally necessitated the use of
expensive and cumbersome capture setups to obtain high-quality results, and
even then, modeling of the eye region holistically has remained elusive. We
present a novel geometry and appearance representation that enables
high-fidelity capture and photorealistic animation, view synthesis and
relighting of the eye region using only a sparse set of lights and cameras. Our
hybrid representation combines an explicit parametric surface model for the
eyeball with implicit deformable volumetric representations for the periocular
region and the interior of the eye. This novel hybrid model has been designed
to address the various parts of that challenging facial area - the explicit
eyeball surface allows modeling refraction and high-frequency specular
reflection at the cornea, whereas the implicit representation is well suited to
model lower-frequency skin reflection via spherical harmonics and can represent
non-surface structures such as hair or diffuse volumetric bodies, both of which
are a challenge for explicit surface models. We show that for high-resolution
close-ups of the eye, our model can synthesize high-fidelity animated gaze from
novel views under unseen illumination conditions.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：SATBench: Benchmarking the speed-accuracy tradeoff in object recognition  by humans and dynamic neural networks</b></summary>
  <p><b>编号</b>：[233]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08427</p>
  <p><b>作者</b>：Ajay Subramanian,  Sara Price,  Omkar Kumbhar,  Elena Sizikova,  Najib J. Majaj,  Denis G. Pelli</p>
  <p><b>备注</b>：19 pages, 12 figures. Under Review at NeurIPS Datasets and Benchmarks Track 2022</p>
  <p><b>关键词</b>：core of everyday, reading and driving, driving is active, active object recognition, neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The core of everyday tasks like reading and driving is active object
recognition. Attempts to model such tasks are currently stymied by the
inability to incorporate time. People show a flexible tradeoff between speed
and accuracy and this tradeoff is a crucial human skill. Deep neural networks
have emerged as promising candidates for predicting peak human object
recognition performance and neural activity. However, modeling the temporal
dimension i.e., the speed-accuracy tradeoff (SAT), is essential for them to
serve as useful computational models for how humans recognize objects. To this
end, we here present the first large-scale (148 observers, 4 neural networks, 8
tasks) dataset of the speed-accuracy tradeoff (SAT) in recognizing ImageNet
images. In each human trial, a beep, indicating the desired reaction time,
sounds at a fixed delay after the image is presented, and observer's response
counts only if it occurs near the time of the beep. In a series of blocks, we
test many beep latencies, i.e., reaction times. We observe that human accuracy
increases with reaction time and proceed to compare its characteristics with
the behavior of several dynamic neural networks that are capable of
inference-time adaptive computation. Using FLOPs as an analog for reaction
time, we compare networks with humans on curve-fit error, category-wise
correlation, and curve steepness, and conclude that cascaded dynamic neural
networks are a promising model of human reaction time in object recognition
tasks.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：IRISformer: Dense Vision Transformers for Single-Image Inverse Rendering  in Indoor Scenes</b></summary>
  <p><b>编号</b>：[235]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08423</p>
  <p><b>作者</b>：Rui Zhu,  Zhengqin Li,  Janarbek Matai,  Fatih Porikli,  Manmohan Chandraker</p>
  <p><b>备注</b>：CVPR 22 camera ready version with supplementary</p>
  <p><b>关键词</b>：exhibit significant appearance, significant appearance variations, appearance variations due, scenes exhibit significant, arbitrarily diverse object</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Indoor scenes exhibit significant appearance variations due to myriad
interactions between arbitrarily diverse object shapes, spatially-changing
materials, and complex lighting. Shadows, highlights, and inter-reflections
caused by visible and invisible light sources require reasoning about
long-range interactions for inverse rendering, which seeks to recover the
components of image formation, namely, shape, material, and lighting. In this
work, our intuition is that the long-range attention learned by transformer
architectures is ideally suited to solve longstanding challenges in
single-image inverse rendering. We demonstrate with a specific instantiation of
a dense vision transformer, IRISformer, that excels at both single-task and
multi-task reasoning required for inverse rendering. Specifically, we propose a
transformer architecture to simultaneously estimate depths, normals,
spatially-varying albedo, roughness and lighting from a single image of an
indoor scene. Our extensive evaluations on benchmark datasets demonstrate
state-of-the-art results on each of the above tasks, enabling applications like
object insertion and material editing in a single unconstrained real image,
with greater photorealism than prior works. Code and data are publicly released
at this https URL.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：Real-time motion amplification on mobile devices</b></summary>
  <p><b>编号</b>：[236]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08422</p>
  <p><b>作者</b>：Henning U. Voss</p>
  <p><b>备注</b>：Supplemental data at this https URL</p>
  <p><b>关键词</b>：amplification algorithm suitable, motion amplification algorithm, devices is presented, amplification algorithm, algorithm suitable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A simple motion amplification algorithm suitable for real-time applications
on mobile devices is presented. It is based on motion enhancement by moving
average differencing (MEMAD), a temporal high-pass filter for video streams.
MEMAD can amplify small moving objects or subtle motion in larger objects. It
is computationally sufficiently simple to be implemented in real time on
smartphones. In the specific implementation as an Android phone app, MEMAD is
demonstrated on examples chosen such as to motivate applications in the
engineering, biological, and medical sciences.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：Going Deeper than Tracking: a Survey of Computer-Vision Based  Recognition of Animal Pain and Affective States</b></summary>
  <p><b>编号</b>：[245]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08405</p>
  <p><b>作者</b>：Sofia Broomé,  Marcelo Feighelstein,  Anna Zamansky,  Gabriel Carreira Lencioni,  Pia Haubro Andersen,  Francisca Pessanha,  Marwa Mahmoud,  Hedvig Kjellström,  Albert Ali Salah</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：animal motion tracking, game changer, motion tracking, pose recognition, animal motion</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Advances in animal motion tracking and pose recognition have been a game
changer in the study of animal behavior. Recently, an increasing number of
works go 'deeper' than tracking, and address automated recognition of animals'
internal states such as emotions and pain with the aim of improving animal
welfare, making this a timely moment for a systematization of the field. This
paper provides a comprehensive survey of computer vision-based research on
recognition of affective states and pain in animals, addressing both facial and
bodily behavior analysis. We summarize the efforts that have been presented so
far within this topic -- classifying them across different dimensions,
highlight challenges and research gaps, and provide best practice
recommendations for advancing the field, and some future directions for
research.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：Incorporating intratumoral heterogeneity into weakly-supervised deep  learning models via variance pooling</b></summary>
  <p><b>编号</b>：[258]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08885</p>
  <p><b>作者</b>：Iain Carmichael,  Andrew H. Song,  Richard J. Chen,  Drew F.K. Williamson,  Tiffany Y. Chen,  Faisal Mahmood</p>
  <p><b>备注</b>：To appear in MICCAI 2022</p>
  <p><b>关键词</b>：requires modeling complex, modeling complex features, Supervised learning tasks, slide images, tumor microenvironment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Supervised learning tasks such as cancer survival prediction from gigapixel
whole slide images (WSIs) are a critical challenge in computational pathology
that requires modeling complex features of the tumor microenvironment. These
learning tasks are often solved with deep multi-instance learning (MIL) models
that do not explicitly capture intratumoral heterogeneity. We develop a novel
variance pooling architecture that enables a MIL model to incorporate
intratumoral heterogeneity into its predictions. Two interpretability tools
based on representative patches are illustrated to probe the biological signals
captured by these models. An empirical study with 4,479 gigapixel WSIs from the
Cancer Genome Atlas shows that adding variance pooling onto MIL frameworks
improves survival prediction performance for five cancer types.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：Leveraging Uncertainty in Deep Learning for Pancreatic Adenocarcinoma  Grading</b></summary>
  <p><b>编号</b>：[265]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08787</p>
  <p><b>作者</b>：Biraja Ghoshal,  Bhargab Ghoshal,  Allan Tucker</p>
  <p><b>备注</b>：26th UK Conference on Medical Image Understanding and Analysis; 27 - 29 July 2022; University of Cambridge, UK. arXiv admin note: text overlap with arXiv:2003.10769</p>
  <p><b>关键词</b>：worst prognoses compared, worst prognoses, prognoses compared, diagnosing pancreatic adenocarcinomas, Pancreatic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pancreatic cancers have one of the worst prognoses compared to other cancers,
as they are diagnosed when cancer has progressed towards its latter stages. The
current manual histological grading for diagnosing pancreatic adenocarcinomas
is time-consuming and often results in misdiagnosis. In digital pathology,
AI-based cancer grading must be extremely accurate in prediction and
uncertainty quantification to improve reliability and explainability and are
essential for gaining clinicians trust in the technology. We present Bayesian
Convolutional Neural Networks for automated pancreatic cancer grading from MGG
and HE stained images to estimate uncertainty in model prediction. We show that
the estimated uncertainty correlates with prediction error. Specifically, it is
useful in setting the acceptance threshold using a metric that weighs
classification accuracy-reject trade-off and misclassification cost controlled
by hyperparameters and can be employed in clinical settings.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：FiT: Parameter Efficient Few-shot Transfer Learning for Personalized and  Federated Image Classification</b></summary>
  <p><b>编号</b>：[271]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08671</p>
  <p><b>作者</b>：Aliaksandra Shysheya,  John Bronskill,  Massimiliano Patacchiola,  Sebastian Nowozin,  Richard E Turner</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Modern deep learning, deep learning systems, Modern deep, amounts of data, systems are increasingly</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Modern deep learning systems are increasingly deployed in situations such as
personalization and federated learning where it is necessary to support i)
learning on small amounts of data, and ii) communication efficient distributed
training protocols. In this work we develop FiLM Transfer (FiT) which fulfills
these requirements in the image classification setting. FiT uses an
automatically configured Naive Bayes classifier on top of a fixed backbone that
has been pretrained on large image datasets. Parameter efficient FiLM layers
are used to modulate the backbone, shaping the representation for the
downstream task. The network is trained via an episodic fine-tuning protocol.
The approach is parameter efficient which is key for enabling few-shot
learning, inexpensive model updates for personalization, and communication
efficient federated learning. We experiment with FiT on a wide range of
downstream datasets and show that it achieves better classification accuracy
than the state-of-the-art Big Transfer (BiT) algorithm at low-shot and on the
challenging VTAB-1k benchmark, with fewer than 1% of the updateable parameters.
Finally, we demonstrate the parameter efficiency of FiT in distributed low-shot
applications including model personalization and federated learning where model
update size is an important performance metric.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：OADAT: Experimental and Synthetic Clinical Optoacoustic Data for  Standardized Image Processing</b></summary>
  <p><b>编号</b>：[276]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08612</p>
  <p><b>作者</b>：Berkan Lafci,  Firat Ozdemir,  Xosé Luís Deán-Ben,  Daniel Razansky,  Fernando Perez-Cruz</p>
  <p><b>备注</b>：25 pages, 18 figures, 8 tables</p>
  <p><b>关键词</b>：nanosecond-duration laser pulses, ultrasound waves generated, thermoelastic expansion, based on excitation, excitation of biological</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Optoacoustic (OA) imaging is based on excitation of biological tissues with
nanosecond-duration laser pulses followed by subsequent detection of ultrasound
waves generated via light-absorption-mediated thermoelastic expansion. OA
imaging features a powerful combination between rich optical contrast and high
resolution in deep tissues. This enabled the exploration of a number of
attractive new applications both in clinical and laboratory settings. However,
no standardized datasets generated with different types of experimental set-up
and associated processing methods are available to facilitate advances in
broader applications of OA in clinical settings. This complicates an objective
comparison between new and established data processing methods, often leading
to qualitative results and arbitrary interpretations of the data. In this
paper, we provide both experimental and synthetic OA raw signals and
reconstructed image domain datasets rendered with different experimental
parameters and tomographic acquisition geometries. We further provide trained
neural networks to tackle three important challenges related to OA image
processing, namely accurate reconstruction under limited view tomographic
conditions, removal of spatial undersampling artifacts and anatomical
segmentation for improved image reconstruction. Specifically, we define 18
experiments corresponding to the aforementioned challenges as benchmarks to be
used as a reference for the development of more advanced processing methods.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：COVID-19 Detection using Transfer Learning with Convolutional Neural  Network</b></summary>
  <p><b>编号</b>：[280]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08557</p>
  <p><b>作者</b>：Pramit Dutta,  Tanny Roy,  Nafisa Anjum</p>
  <p><b>备注</b>：4 pages, 4 figures, 2nd International Conference on Robotics, Electrical and Signal Processing Techniques (ICREST), DHAKA, Bangladesh</p>
  <p><b>关键词</b>：recognized in December, fatal infectious disease, epidemic situation, Coronavirus disease, fatal infectious</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The Novel Coronavirus disease 2019 (COVID-19) is a fatal infectious disease,
first recognized in December 2019 in Wuhan, Hubei, China, and has gone on an
epidemic situation. Under these circumstances, it became more important to
detect COVID-19 in infected people. Nowadays, the testing kits are gradually
lessening in number compared to the number of infected population. Under recent
prevailing conditions, the diagnosis of lung disease by analyzing chest CT
(Computed Tomography) images has become an important tool for both diagnosis
and prophecy of COVID-19 patients. In this study, a Transfer learning strategy
(CNN) for detecting COVID-19 infection from CT images has been proposed. In the
proposed model, a multilayer Convolutional neural network (CNN) with Transfer
learning model Inception V3 has been designed. Similar to CNN, it uses
convolution and pooling to extract features, but this transfer learning model
contains weights of dataset Imagenet. Thus it can detect features very
effectively which gives it an upper hand for achieving better accuracy.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：Multi-Classification of Brain Tumor Images Using Transfer Learning Based  Deep Neural Network</b></summary>
  <p><b>编号</b>：[282]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08543</p>
  <p><b>作者</b>：Pramit Dutta,  Khaleda Akhter Sathi,  Md. Saiful Islam</p>
  <p><b>备注</b>：7 pages, 4 figures, 2 tables, International Virtual Conference on ARTIFICIAL INTELLIGENCE FOR SMART COMMUNITY, Malaysia</p>
  <p><b>关键词</b>：brain tumor images, based diagnostics system, computer based diagnostics, brain tumor, tumor images</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent advancement towards computer based diagnostics system, the
classification of brain tumor images is a challenging task. This paper mainly
focuses on elevating the classification accuracy of brain tumor images with
transfer learning based deep neural network. The classification approach is
started with the image augmentation operation including rotation, zoom,
hori-zontal flip, width shift, height shift, and shear to increase the
diversity in image datasets. Then the general features of the input brain tumor
images are extracted based on a pre-trained transfer learning method comprised
of Inception-v3. Fi-nally, the deep neural network with 4 customized layers is
employed for classi-fying the brain tumors in most frequent brain tumor types
as meningioma, glioma, and pituitary. The proposed model acquires an effective
performance with an overall accuracy of 96.25% which is much improved than some
existing multi-classification methods. Whereas, the fine-tuning of
hyper-parameters and inclusion of customized DNN with the Inception-v3 model
results in an im-provement of the classification accuracy.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：Orientation-guided Graph Convolutional Network for Bone Surface  Segmentation</b></summary>
  <p><b>编号</b>：[285]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08481</p>
  <p><b>作者</b>：Aimon Rahman,  Wele Gedara Chaminda Bandara,  Jeya Maria Jose Valanarasu,  Ilker Hacihaliloglu,  Vishal M Patel</p>
  <p><b>备注</b>：Accepted at MICCAI 2022</p>
  <p><b>关键词</b>：computer-assisted surgical procedures, ultrasound-guided computer-assisted surgical, produce fragmented predictions, automatic bone surface, artifacts and low</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Due to imaging artifacts and low signal-to-noise ratio in ultrasound images,
automatic bone surface segmentation networks often produce fragmented
predictions that can hinder the success of ultrasound-guided computer-assisted
surgical procedures. Existing pixel-wise predictions often fail to capture the
accurate topology of bone tissues due to a lack of supervision to enforce
connectivity. In this work, we propose an orientation-guided graph
convolutional network to improve connectivity while segmenting the bone
surface. We also propose an additional supervision on the orientation of the
bone surface to further impose connectivity. We validated our approach on 1042
vivo US scans of femur, knee, spine, and distal radius. Our approach improves
over the state-of-the-art methods by 5.01% in connectivity metric.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：OpenSRH: optimizing brain tumor surgery using intraoperative stimulated  Raman histology</b></summary>
  <p><b>编号</b>：[288]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08439</p>
  <p><b>作者</b>：Cheng Jiang,  Asadur Chowdury,  Xinhai Hou,  Akhil Kondepudi,  Christian W. Freudiger,  Kyle Conway,  Sandra Camelo-Piragua,  Daniel A. Orringer,  Honglak Lee,  Todd C. Hollon</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Accurate intraoperative diagnosis, essential for providing, providing safe, safe and effective, effective care</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Accurate intraoperative diagnosis is essential for providing safe and
effective care during brain tumor surgery. Our standard-of-care diagnostic
methods are time, resource, and labor intensive, which restricts access to
optimal surgical treatments. To address these limitations, we propose an
alternative workflow that combines stimulated Raman histology (SRH), a rapid
optical imaging method, with deep learning-based automated interpretation of
SRH images for intraoperative brain tumor diagnosis and real-time surgical
decision support. Here, we present OpenSRH, the first public dataset of
clinical SRH images from 300+ brain tumors patients and 1300+ unique whole
slide optical images. OpenSRH contains data from the most common brain tumors
diagnoses, full pathologic annotations, whole slide tumor segmentations, raw
and processed optical imaging data for end-to-end model development and
validation. We provide a framework for patch-based whole slide SRH
classification and inference using weak (i.e. patient-level) diagnostic labels.
Finally, we benchmark two computer vision tasks: multiclass histologic brain
tumor classification and patch-based contrastive representation learning. We
hope OpenSRH will facilitate the clinical translation of rapid optical imaging
and real-time ML-based surgical decision support in order to improve the
access, safety, and efficacy of cancer surgery in the era of precision
medicine. Dataset access, code, and benchmarks are available at
this http URL.</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：Learning Generic Lung Ultrasound Biomarkers for Decoupling Feature  Extraction from Downstream Tasks</b></summary>
  <p><b>编号</b>：[290]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08398</p>
  <p><b>作者</b>：Gautam Rajendrakumar Gare,  Tom Fox,  Pete Lowery,  Kevin Zamora,  Hai V. Tran,  Laura Hutchins,  David Montgomery,  Amita Krishnan,  Deva Kannan Ramanan,  Ricardo Luis Rodriguez,  Bennett P deBoisblanc,  John Michael Galeotti</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Contemporary artificial neural, artificial neural networks, Contemporary artificial, ANN, artificial neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Contemporary artificial neural networks (ANN) are trained end-to-end, jointly
learning both features and classifiers for the task of interest. Though
enormously effective, this paradigm imposes significant costs in assembling
annotated task-specific datasets and training large-scale networks. We propose
to decouple feature learning from downstream lung ultrasound tasks by
introducing an auxiliary pre-task of visual biomarker classification. We
demonstrate that one can learn an informative, concise, and interpretable
feature space from ultrasound videos by training models for predicting
biomarker labels. Notably, biomarker feature extractors can be trained from
data annotated with weak video-scale supervision. These features can be used by
a variety of downstream Expert models targeted for diverse clinical tasks
(Diagnosis, lung severity, S/F ratio). Crucially, task-specific expert models
are comparable in accuracy to end-to-end models directly trained for such
target tasks, while being significantly lower cost to train.</p>
  </details>
</details>
<h1>自然语言处理</h1>
<details>
  <summary>1. <b>标题：niksss at HinglishEval: Language-agnostic BERT-based Contextual  Embeddings with Catboost for Quality Evaluation of the Low-Resource  Synthetically Generated Code-Mixed Hinglish Text</b></summary>
  <p><b>编号</b>：[8]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08910</p>
  <p><b>作者</b>：Nikhil Singh</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：paper describes, HinglishEval challenge, system description, system, text generation system</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper describes the system description for the HinglishEval challenge at
INLG 2022. The goal of this task was to investigate the factors influencing the
quality of the code-mixed text generation system. The task was divided into two
subtasks, quality rating prediction and annotators disagreement prediction of
the synthetic Hinglish dataset. We attempted to solve these tasks using
sentence-level embeddings, which are obtained from mean pooling the
contextualized word embeddings for all input tokens in our text. We
experimented with various classifiers on top of the embeddings produced for
respective tasks. Our best-performing system ranked 1st on subtask B and 3rd on
subtask A.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：MineDojo: Building Open-Ended Embodied Agents with Internet-Scale  Knowledge</b></summary>
  <p><b>编号</b>：[29]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08853</p>
  <p><b>作者</b>：Linxi Fan,  Guanzhi Wang,  Yunfan Jiang,  Ajay Mandlekar,  Yuncong Yang,  Haoyi Zhu,  Andrew Tang,  De-An Huang,  Yuke Zhu,  Anima Anandkumar</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：made great strides, domains like Atari, Atari games, made great, great strides</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Autonomous agents have made great strides in specialist domains like Atari
games and Go. However, they typically learn tabula rasa in isolated
environments with limited and manually conceived objectives, thus failing to
generalize across a wide spectrum of tasks and capabilities. Inspired by how
humans continually learn and adapt in the open world, we advocate a trinity of
ingredients for building generalist agents: 1) an environment that supports a
multitude of tasks and goals, 2) a large-scale database of multimodal
knowledge, and 3) a flexible and scalable agent architecture. We introduce
MineDojo, a new framework built on the popular Minecraft game that features a
simulation suite with thousands of diverse open-ended tasks and an
internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and
forum discussions. Using MineDojo's data, we propose a novel agent learning
algorithm that leverages large pre-trained video-language models as a learned
reward function. Our agent is able to solve a variety of open-ended tasks
specified in free-form language without any manually designed dense shaping
reward. We open-source the simulation suite and knowledge bases
(this https URL) to promote research towards the goal of generally
capable embodied agents.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：What can Speech and Language Tell us About the Working Alliance in  Psychotherapy</b></summary>
  <p><b>编号</b>：[37]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08835</p>
  <p><b>作者</b>：Sebastian P. Bayerl,  Gabriel Roccabruna,  Shammur Absar Chowdhury,  Tommaso Ciulli,  Morena Danieli,  Korbinian Riedhammer,  Giuseppe Riccardi</p>
  <p><b>备注</b>：Accepted at Interspeech 2022</p>
  <p><b>关键词</b>：Cognitive Behavioral Therapy, health domain, Behavioral Therapy, Working Alliance, Working Alliance Inventory</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We are interested in the problem of conversational analysis and its
application to the health domain. Cognitive Behavioral Therapy is a structured
approach in psychotherapy, allowing the therapist to help the patient to
identify and modify the malicious thoughts, behavior, or actions. This
cooperative effort can be evaluated using the Working Alliance Inventory
Observer-rated Shortened - a 12 items inventory covering task, goal, and
relationship - which has a relevant influence on therapeutic outcomes. In this
work, we investigate the relation between this alliance inventory and the
spoken conversations (sessions) between the patient and the psychotherapist. We
have delivered eight weeks of e-therapy, collected their audio and video call
sessions, and manually transcribed them. The spoken conversations have been
annotated and evaluated with WAI ratings by professional therapists. We have
investigated speech and language features and their association with WAI items.
The feature types include turn dynamics, lexical entrainment, and
conversational descriptors extracted from the speech and language signals. Our
findings provide strong evidence that a subset of these features are strong
indicators of working alliance. To the best of our knowledge, this is the first
and a novel study to exploit speech and language for characterising working
alliance.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Language with Vision: a Study on Grounded Word and Sentence Embeddings</b></summary>
  <p><b>编号</b>：[43]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08823</p>
  <p><b>作者</b>：Hassan Shahmohammadi,  Maria Heitmeier,  Elnaz Shafaei-Bajestan,  Hendrik P. A. Lensch,  Harald Baayen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：enrich text-based representations, leveraging perceptual knowledge, active field, field of research, research aiming</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Language grounding to vision is an active field of research aiming to enrich
text-based representations of word meanings by leveraging perceptual knowledge
from vision. Despite many attempts at language grounding, it is still unclear
how to effectively inject visual knowledge into the word embeddings of a
language in such a way that a proper balance of textual and visual knowledge is
maintained. Some common concerns are the following. Is visual grounding
beneficial for abstract words or is its contribution only limited to concrete
words? What is the optimal way of bridging the gap between text and vision? How
much do we gain by visually grounding textual embeddings? The present study
addresses these questions by proposing a simple yet very effective grounding
approach for pre-trained word embeddings. Our model aligns textual embeddings
with vision while largely preserving the distributional statistics that
characterize word use in text corpora. By applying a learned alignment, we are
able to generate visually grounded embeddings for unseen words, including
abstract words. A series of evaluations on word similarity benchmarks shows
that visual grounding is beneficial not only for concrete words, but also for
abstract words. We also show that our method for visual grounding offers
advantages for contextualized embeddings, but only when these are trained on
corpora of relatively modest size. Code and grounded embeddings for English are
available at this https URL.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Self-supervised speech unit discovery from articulatory and acoustic  features using VQ-VAE</b></summary>
  <p><b>编号</b>：[56]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08790</p>
  <p><b>作者</b>：Marc-Antoine Georges,  Jean-Luc Schwartz,  Thomas Hueber</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：human perception system, recruit motor knowledge, auditory speech inputs, processing auditory speech, human perception</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The human perception system is often assumed to recruit motor knowledge when
processing auditory speech inputs. Using articulatory modeling and deep
learning, this study examines how this articulatory information can be used for
discovering speech units in a self-supervised setting. We used vector-quantized
variational autoencoders (VQ-VAE) to learn discrete representations from
articulatory and acoustic speech data. In line with the zero-resource paradigm,
an ABX test was then used to investigate how the extracted representations
encode phonetically relevant properties. Experiments were conducted on three
different corpora in English and French. We found that articulatory information
rather organises the latent representations in terms of place of articulation
whereas the speech acoustics mainly structure the latent space in terms of
manner of articulation. We show that an optimal fusion of the two modalities
can lead to a joint representation of these phonetic dimensions more accurate
than each modality considered individually. Since articulatory information is
usually not available in a practical situation, we finally investigate the
benefit it provides when inferred from the speech acoustics in a
self-supervised manner.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：The ITU Faroese Pairs Dataset</b></summary>
  <p><b>编号</b>：[84]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08727</p>
  <p><b>作者</b>：Leon Derczynski,  Annika Solveig Hedegaard Isfeldt,  Signhild Djurhuus</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ITU Copenhagen, article documents, documents a dataset, dataset of sentence, Faroese</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This article documents a dataset of sentence pairs between Faroese and
Danish, produced at ITU Copenhagen. The data covers tranlsation from both
source languages, and is intended for use as training data for machine
translation systems in this language pair.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Crowdsourcing Relative Rankings of Multi-Word Expressions: Experts  versus Non-Experts</b></summary>
  <p><b>编号</b>：[87]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08724</p>
  <p><b>作者</b>：David Alfter,  Therese Lindström Tiedemann,  Elena Volodina</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：crowdsourcing experiment, study we investigate, agree on questions, questions of difficulty, non-experts agree</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this study we investigate to which degree experts and non-experts agree on
questions of difficulty in a crowdsourcing experiment. We ask non-experts
(second language learners of Swedish) and two groups of experts (teachers of
Swedish as a second/foreign language and CEFR experts) to rank multi-word
expressions in a crowdsourcing experiment. We find that the resulting rankings
by all the three tested groups correlate to a very high degree, which suggests
that judgments produced in a comparative setting are not influenced by
professional insights into Swedish as a second language.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：CookDial: A dataset for task-oriented dialogs grounded in procedural  documents</b></summary>
  <p><b>编号</b>：[88]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08723</p>
  <p><b>作者</b>：Yiwei Jiang,  Klim Zaporojets,  Johannes Deleu,  Thomas Demeester,  Chris Develder</p>
  <p><b>备注</b>：The dataset and codes are available at this https URL</p>
  <p><b>关键词</b>：work presents, procedural knowledge understanding, Agent Action Frame, Action Frame Prediction, dialog</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This work presents a new dialog dataset, CookDial, that facilitates research
on task-oriented dialog systems with procedural knowledge understanding. The
corpus contains 260 human-to-human task-oriented dialogs in which an agent,
given a recipe document, guides the user to cook a dish. Dialogs in CookDial
exhibit two unique features: (i) procedural alignment between the dialog flow
and supporting document; (ii) complex agent decision-making that involves
segmenting long sentences, paraphrasing hard instructions and resolving
coreference in the dialog context. In addition, we identify three challenging
(sub)tasks in the assumed task-oriented dialog system: (1) User Question
Understanding, (2) Agent Action Frame Prediction, and (3) Agent Response
Generation. For each of these tasks, we develop a neural baseline model, which
we evaluate on the CookDial dataset. We publicly release the CookDial dataset,
comprising rich annotations of both dialogs and recipe documents, to stimulate
further research on domain-specific document-grounded dialog systems.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Statistical and Neural Methods for Cross-lingual Entity Label Mapping in  Knowledge Graphs</b></summary>
  <p><b>编号</b>：[95]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08709</p>
  <p><b>作者</b>：Gabriel Amaral,  Mārcis Pinnis,  Inguna Skadiņa,  Odinaldo Rodrigues,  Elena Simperl</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：amass vast amounts, Wikidata amass vast, named entity information, amass vast, vast amounts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Knowledge bases such as Wikidata amass vast amounts of named entity
information, such as multilingual labels, which can be extremely useful for
various multilingual and cross-lingual applications. However, such labels are
not guaranteed to match across languages from an information consistency
standpoint, greatly compromising their usefulness for fields such as machine
translation. In this work, we investigate the application of word and sentence
alignment techniques coupled with a matching algorithm to align cross-lingual
entity labels extracted from Wikidata in 10 languages. Our results indicate
that mapping between Wikidata's main labels stands to be considerably improved
(up to $20$ points in F1-score) by any of the employed methods. We show how
methods relying on sentence embeddings outperform all others, even across
different scripts. We believe the application of such techniques to measure the
similarity of label pairs, coupled with a knowledge base rich in high-quality
entity labels, to be an excellent asset to machine translation.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：BITS Pilani at HinglishEval: Quality Evaluation for Code-Mixed Hinglish  Text Using Transformers</b></summary>
  <p><b>编号</b>：[109]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08680</p>
  <p><b>作者</b>：Shaz Furniturewala,  Vijay Kumari,  Amulya Ratna Dash,  Hriday Kedia,  Yashvardhan Sharma</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：text data consists, Code-Mixed text data, words or phrases, Code-Mixed text, text data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Code-Mixed text data consists of sentences having words or phrases from more
than one language. Most multi-lingual communities worldwide communicate using
multiple languages, with English usually one of them. Hinglish is a Code-Mixed
text composed of Hindi and English but written in Roman script. This paper aims
to determine the factors influencing the quality of Code-Mixed text data
generated by the system. For the HinglishEval task, the proposed model uses
multi-lingual BERT to find the similarity between synthetically generated and
human-generated sentences to predict the quality of synthetically generated
Hinglish sentences.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：A Quantitative and Qualitative Analysis of Suicide Ideation Detection  using Deep Learning</b></summary>
  <p><b>编号</b>：[111]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08673</p>
  <p><b>作者</b>：Siqu Long,  Rina Cabral,  Josiah Poon,  Soyeon Caren Han</p>
  <p><b>备注</b>：Accepted in HealTAC 2022</p>
  <p><b>关键词</b>：preventing youth suicide, youth suicide, attention from researchers, social media platforms, preventing youth</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>For preventing youth suicide, social media platforms have received much
attention from researchers. A few researches apply machine learning, or deep
learning-based text classification approaches to classify social media posts
containing suicidality risk. This paper replicated competitive social
media-based suicidality detection/prediction models. We evaluated the
feasibility of detecting suicidal ideation using multiple datasets and
different state-of-the-art deep learning models, RNN-, CNN-, and
Attention-based models. Using two suicidality evaluation datasets, we evaluated
28 combinations of 7 input embeddings with 4 commonly used deep learning models
and 5 pretrained language models in quantitative and qualitative ways. Our
replication study confirms that deep learning works well for social media-based
suicidality detection in general, but it highly depends on the dataset's
quality.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Bridge-Tower: Building Bridges Between Encoders in Vision-Language  Representation Learning</b></summary>
  <p><b>编号</b>：[118]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08657</p>
  <p><b>作者</b>：Xiao Xu,  Chenfei Wu,  Shachar Rosenman,  Vasudev Lal,  Nan Duan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：dominated visual-language representation, visual-language representation learning, uni-modal encoders, recent years, cross-modal encoder</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision-Language (VL) models with the Two-Tower architecture have dominated
visual-language representation learning in recent years. Current VL models
either use lightweight uni-modal encoders and learn to extract, align and fuse
both modalities simultaneously in a cross-modal encoder, or feed the last-layer
uni-modal features directly into the top cross-modal encoder, ignoring the
semantic information at the different levels in the deep uni-modal encoders.
Both approaches possibly restrict vision-language representation learning and
limit model performance. In this paper, we introduce multiple bridge layers
that build a connection between the top layers of uni-modal encoders and each
layer of the cross-modal encoder. This enables comprehensive bottom-up
interactions between visual and textual representations at different semantic
levels, resulting in more effective cross-modal alignment and fusion. Our
proposed Bridge-Tower, pre-trained with only $4$M images, achieves
state-of-the-art performance on various downstream vision-language tasks. On
the VQAv2 test-std set, Bridge-Tower achieves an accuracy of $78.73\%$,
outperforming the previous state-of-the-art METER model by $1.09\%$ with the
same pre-training data and almost no additional parameters and computational
cost. Notably, when further scaling the model, Bridge-Tower achieves an
accuracy of $81.15\%$, surpassing models that are pre-trained on
orders-of-magnitude larger datasets. Code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Understanding Aesthetics with Language: A Photo Critique Dataset for  Aesthetic Assessment</b></summary>
  <p><b>编号</b>：[138]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08614</p>
  <p><b>作者</b>：Daniel Vera Nieto,  Luigi Celona,  Clara Fernandez-Labrador</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ill-defined task due, Computational inference, subjective nature, ill-defined task, task due</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Computational inference of aesthetics is an ill-defined task due to its
subjective nature. Many datasets have been proposed to tackle the problem by
providing pairs of images and aesthetic scores based on human ratings. However,
humans are better at expressing their opinion, taste, and emotions by means of
language rather than summarizing them in a single number. In fact, photo
critiques provide much richer information as they reveal how and why users rate
the aesthetics of visual stimuli. In this regard, we propose the Reddit Photo
Critique Dataset (RPCD), which contains tuples of image and photo critiques.
RPCD consists of 74K images and 220K comments and is collected from a Reddit
community used by hobbyists and professional photographers to improve their
photography skills by leveraging constructive community feedback. The proposed
dataset differs from previous aesthetics datasets mainly in three aspects,
namely (i) the large scale of the dataset and the extension of the comments
criticizing different aspects of the image, (ii) it contains mostly UltraHD
images, and (iii) it can easily be extended to new data as it is collected
through an automatic pipeline. To the best of our knowledge, in this work, we
propose the first attempt to estimate the aesthetic quality of visual stimuli
from the critiques. To this end, we exploit the polarity of the sentiment of
criticism as an indicator of aesthetic judgment. We demonstrate how sentiment
polarity correlates positively with the aesthetic judgment available for two
aesthetic assessment benchmarks. Finally, we experiment with several models by
using the sentiment scores as a target for ranking images. Dataset and
baselines are available (this https URL).</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Automatic Correction of Human Translations</b></summary>
  <p><b>编号</b>：[148]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08593</p>
  <p><b>作者</b>：Jessy Lin,  Geza Kovacs,  Aditya Shastry,  Joern Wuebker,  John DeNero</p>
  <p><b>备注</b>：NAACL 2022. Dataset available at: this https URL</p>
  <p><b>关键词</b>：automatically correcting human-generated, TEC, errors, correcting human-generated translations, automatically correcting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce translation error correction (TEC), the task of automatically
correcting human-generated translations. Imperfections in machine translations
(MT) have long motivated systems for improving translations post-hoc with
automatic post-editing. In contrast, little attention has been devoted to the
problem of automatically correcting human translations, despite the intuition
that humans make distinct errors that machines would be well-suited to assist
with, from typos to inconsistencies in translation conventions. To investigate
this, we build and release the Aced corpus with three TEC datasets. We show
that human errors in TEC exhibit a more diverse range of errors and far fewer
translation fluency errors than the MT errors in automatic post-editing
datasets, suggesting the need for dedicated TEC models that are specialized to
correct human errors. We show that pre-training instead on synthetic errors
based on human errors improves TEC F-score by as much as 5.1 points. We
conducted a human-in-the-loop user study with nine professional translation
editors and found that the assistance of our TEC system led them to produce
significantly higher quality revised translations.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：VLMbench: A Compositional Benchmark for Vision-and-Language Manipulation</b></summary>
  <p><b>编号</b>：[181]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08522</p>
  <p><b>作者</b>：Kaizhi Zheng,  Xiaotong Chen,  Odest Chadwicke Jenkins,  Xin Eric Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：humans naturally intend, Automatic Manipulation Solver, flexibility and compositionality, naturally intend, manipulation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Benefiting from language flexibility and compositionality, humans naturally
intend to use language to command an embodied agent for complex tasks such as
navigation and object manipulation. In this work, we aim to fill the blank of
the last mile of embodied agents -- object manipulation by following human
guidance, e.g., "move the red mug next to the box while keeping it upright." To
this end, we introduce an Automatic Manipulation Solver (AMSolver) simulator
and build a Vision-and-Language Manipulation benchmark (VLMbench) based on it,
containing various language instructions on categorized robotic manipulation
tasks. Specifically, modular rule-based task templates are created to
automatically generate robot demonstrations with language instructions,
consisting of diverse object shapes and appearances, action types, and motion
constraints. We also develop a keypoint-based model 6D-CLIPort to deal with
multi-view observations and language input and output a sequence of 6 degrees
of freedom (DoF) actions. We hope the new simulator and benchmark will
facilitate future research on language-guided robotic manipulation.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：A Unified Evaluation of Textual Backdoor Learning: Frameworks and  Benchmarks</b></summary>
  <p><b>编号</b>：[187]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08514</p>
  <p><b>作者</b>：Ganqu Cui,  Lifan Yuan,  Bingxiang He,  Yangyi Chen,  Zhiyuan Liu,  Maosong Sun</p>
  <p><b>备注</b>：19 pages</p>
  <p><b>关键词</b>：NLP systems, threat to NLP, NLP, backdoor, poisoned</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Textual backdoor attacks are a kind of practical threat to NLP systems. By
injecting a backdoor in the training phase, the adversary could control model
predictions via predefined triggers. As various attack and defense models have
been proposed, it is of great significance to perform rigorous evaluations.
However, we highlight two issues in previous backdoor learning evaluations: (1)
The differences between real-world scenarios (e.g. releasing poisoned datasets
or models) are neglected, and we argue that each scenario has its own
constraints and concerns, thus requires specific evaluation protocols; (2) The
evaluation metrics only consider whether the attacks could flip the models'
predictions on poisoned samples and retain performances on benign samples, but
ignore that poisoned samples should also be stealthy and semantic-preserving.
To address these issues, we categorize existing works into three practical
scenarios in which attackers release datasets, pre-trained models, and
fine-tuned models respectively, then discuss their unique evaluation
methodologies. On metrics, to completely evaluate poisoned samples, we use
grammar error increase and perplexity difference for stealthiness, along with
text similarity for validity. After formalizing the frameworks, we develop an
open-source toolkit OpenBackdoor to foster the implementations and evaluations
of textual backdoor learning. With this toolkit, we perform extensive
experiments to benchmark attack and defense models under the suggested
paradigm. To facilitate the underexplored defenses against poisoned datasets,
we further propose CUBE, a simple yet strong clustering-based defense baseline.
We hope that our frameworks and benchmarks could serve as the cornerstones for
future model development and evaluations.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：A Numerical Reasoning Question Answering System with Fine-grained  Retriever and the Ensemble of Multiple Generators for FinQA</b></summary>
  <p><b>编号</b>：[191]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08506</p>
  <p><b>作者</b>：Bin Wang,  Jiangzhou Ju,  Yunlin Mao,  Xin-Yu Dai,  Shujian Huang,  Jiajun Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：performing quantitative analysis, greatly increase business, increase business efficiency, numerical reasoning, numerical reasoning question</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The numerical reasoning in the financial domain -- performing quantitative
analysis and summarizing the information from financial reports -- can greatly
increase business efficiency and reduce costs of billions of dollars. Here, we
propose a numerical reasoning question answering system to answer numerical
reasoning questions among financial text and table data sources, consisting of
a retriever module, a generator module, and an ensemble module. Specifically,
in the retriever module, in addition to retrieving the whole row data, we
innovatively design a cell retriever that retrieves the gold cells to avoid
bringing unrelated and similar cells in the same row to the inputs of the
generator module. In the generator module, we utilize multiple generators to
produce programs, which are operation steps to answer the question. Finally, in
the ensemble module, we integrate multiple programs to choose the best program
as the output of our system. In the final private test set in FinQA
Competition, our system obtains 69.79 execution accuracy.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Interpretable AMR-Based Question Decomposition for Multi-hop Question  Answering</b></summary>
  <p><b>编号</b>：[204]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08486</p>
  <p><b>作者</b>：Zhenyun Deng,  Yonghua Zhu,  Yang Chen,  Michael Witbrock,  Patricia Riddle</p>
  <p><b>备注</b>：Accepted by IJCAI 2022</p>
  <p><b>关键词</b>：multiple scattered paragraphs, Abstract Meaning Representation, Effective multi-hop question, multiple scattered, scattered paragraphs</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Effective multi-hop question answering (QA) requires reasoning over multiple
scattered paragraphs and providing explanations for answers. Most existing
approaches cannot provide an interpretable reasoning process to illustrate how
these models arrive at an answer. In this paper, we propose a Question
Decomposition method based on Abstract Meaning Representation (QDAMR) for
multi-hop QA, which achieves interpretable reasoning by decomposing a multi-hop
question into simpler sub-questions and answering them in order. Since
annotating the decomposition is expensive, we first delegate the complexity of
understanding the multi-hop question to an AMR parser. We then achieve the
decomposition of a multi-hop question via segmentation of the corresponding AMR
graph based on the required reasoning type. Finally, we generate sub-questions
using an AMR-to-Text generation model and answer them with an off-the-shelf QA
model. Experimental results on HotpotQA demonstrate that our approach is
competitive for interpretable reasoning and that the sub-questions generated by
QDAMR are well-formed, outperforming existing question-decomposition-based
multi-hop QA approaches.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Methods for Estimating and Improving Robustness of Language Models</b></summary>
  <p><b>编号</b>：[224]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08446</p>
  <p><b>作者</b>：Michal Štefánik</p>
  <p><b>备注</b>：Thesis proposal, accepted & to appear in NAACL SRW 2022</p>
  <p><b>关键词</b>：suffer notorious flaws, surface-level textual relations, notorious flaws related, full semantic complexity, large language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite their outstanding performance, large language models (LLMs) suffer
notorious flaws related to their preference for simple, surface-level textual
relations over full semantic complexity of the problem. This proposal
investigates a common denominator of this problem in their weak ability to
generalise outside of the training domain. We survey diverse research
directions providing estimations of model generalisation ability and find that
incorporating some of these measures in the training objectives leads to
enhanced distributional robustness of neural models. Based on these findings,
we present future research directions towards enhancing the robustness of LLMs.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Enriching Abusive Language Detection with Community Context</b></summary>
  <p><b>编号</b>：[225]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08445</p>
  <p><b>作者</b>：Jana Kurrek,  Haji Mohammad Saleem,  Derek Ruths</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：actively empowering, benign or actively, pejorative expressions, abusive language, abusive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Uses of pejorative expressions can be benign or actively empowering. When
models for abuse detection misclassify these expressions as derogatory, they
inadvertently censor productive conversations held by marginalized groups. One
way to engage with non-dominant perspectives is to add context around
conversations. Previous research has leveraged user- and thread-level features,
but it often neglects the spaces within which productive conversations take
place. Our paper highlights how community context can improve classification
outcomes in abusive language detection. We make two main contributions to this
end. First, we demonstrate that online communities cluster by the nature of
their support towards victims of abuse. Second, we establish how community
context improves accuracy and reduces the false positive rates of
state-of-the-art abusive language classifiers. These findings suggest a
promising direction for context-aware models in abusive language research.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：GAAMA 2.0: An Integrated System that Answers Boolean and Extractive  Question</b></summary>
  <p><b>编号</b>：[227]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08441</p>
  <p><b>作者</b>：Scott McCarley,  Mihaela Bornea,  Sara Rosenthal,  Anthony Ferritto,  Md Arafat Sultan,  Avirup Sil,  Radu Florian</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：offer integrated support, machine reading comprehension, Recent machine reading, reading comprehension datasets, machine reading</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent machine reading comprehension datasets include extractive and boolean
questions but current approaches do not offer integrated support for answering
both question types. We present a multilingual machine reading comprehension
system and front-end demo that handles boolean questions by providing both a
YES/NO answer and highlighting supporting evidence, and handles extractive
questions by highlighting the answer in the passage. Our system, GAAMA 2.0, is
ranked first on the Tydi QA leaderboard at the time of this writing. We
contrast two different implementations of our approach. The first includes
several independent stacks of transformers allowing easy deployment of each
component. The second is a single stack of transformers utilizing adapters to
reduce GPU memory footprint in a resource-constrained environment.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：DialogueScript: Using Dialogue Agents to Produce a Script</b></summary>
  <p><b>编号</b>：[234]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08425</p>
  <p><b>作者</b>：Patrícia Schmidtová,  Dávid Javorský,  Christián Mikláš,  Tomáš Musil,  Rudolf Rosa,  Ondřej Dušek</p>
  <p><b>备注</b>：Non-archival paper at the 4th Workshop on Narrative Understanding (WNU 2022)</p>
  <p><b>关键词</b>：personality types, generating scripts, simulated dramatic networks, types, manage character interaction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a novel approach to generating scripts by using agents with
different personality types. To manage character interaction in the script, we
employ simulated dramatic networks. Automatic and human evaluation on multiple
criteria shows that our approach outperforms a vanilla-GPT2-based baseline. We
further introduce a new metric to evaluate dialogue consistency based on
natural language inference and demonstrate its validity.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：CS-UM6P at SemEval-2022 Task 6: Transformer-based Models for Intended  Sarcasm Detection in English and Arabic</b></summary>
  <p><b>编号</b>：[239]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08415</p>
  <p><b>作者</b>：Abdelkader El Mahdaouy,  Abdellah El Mekki,  Kabil Essefar,  Abderrahman Skiredj,  Ismail Berrada</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Natural Language Processing, literal meaning, form of figurative, sentence differs, Opinion Mining</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sarcasm is a form of figurative language where the intended meaning of a
sentence differs from its literal meaning. This poses a serious challenge to
several Natural Language Processing (NLP) applications such as Sentiment
Analysis, Opinion Mining, and Author Profiling. In this paper, we present our
participating system to the intended sarcasm detection task in English and
Arabic languages. Our system\footnote{The source code of our system is
available at \url{this https URL}} consists of
three deep learning-based models leveraging two existing pre-trained language
models for Arabic and English. We have participated in all sub-tasks. Our
official submissions achieve the best performance on sub-task A for Arabic
language and rank second in sub-task B. For sub-task C, our system is ranked
7th and 11th on Arabic and English datasets, respectively.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Deep Multi-Task Models for Misogyny Identification and Categorization on  Arabic Social Media</b></summary>
  <p><b>编号</b>：[243]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08407</p>
  <p><b>作者</b>：Abdelkader El Mahdaouy,  Abdellah El Mekki,  Ahmed Oumar,  Hajar Mousannif,  Ismail Berrada</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：social media platforms, Natural Language Processing, media platforms, hate speech, interconnected society</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The prevalence of toxic content on social media platforms, such as hate
speech, offensive language, and misogyny, presents serious challenges to our
interconnected society. These challenging issues have attracted widespread
attention in Natural Language Processing (NLP) community. In this paper, we
present the submitted systems to the first Arabic Misogyny Identification
shared task. We investigate three multi-task learning models as well as their
single-task counterparts. In order to encode the input text, our models rely on
the pre-trained MARBERT language model. The overall obtained results show that
all our submitted models have achieved the best performances (top three ranked
submissions) in both misogyny identification and categorization tasks.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Predicting Hate Intensity of Twitter Conversation Threads</b></summary>
  <p><b>编号</b>：[244]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08406</p>
  <p><b>作者</b>：Qing Meng,  Tharun Suresh,  Roy Ka-Wei Lee,  Tanmoy Chakraborty</p>
  <p><b>备注</b>：22 pages, 10 figures, 3 tables</p>
  <p><b>关键词</b>：online social media, social media, hate speech, Online hate speech, social media companies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Tweets are the most concise form of communication in online social media,
wherein a single tweet has the potential to make or break the discourse of the
conversation. Online hate speech is more accessible than ever, and stifling its
propagation is of utmost importance for social media companies and users for
congenial communication. Most of the research barring a recent few has focused
on classifying an individual tweet regardless of the tweet thread/context
leading up to that point. One of the classical approaches to curb hate speech
is to adopt a reactive strategy after the hate speech postage. The ex-post
facto strategy results in neglecting subtle posts that do not show the
potential to instigate hate speech on their own but may portend in the
subsequent discussion ensuing in the post's replies. In this paper, we propose
DRAGNET++, which aims to predict the intensity of hatred that a tweet can bring
in through its reply chain in the future. It uses the semantic and propagating
structure of the tweet threads to maximize the contextual information leading
up to and the fall of hate intensity at each subsequent tweet. We explore three
publicly available Twitter datasets -- Anti-Racism contains the reply tweets of
a collection of social media discourse on racist remarks during US political
and Covid-19 background; Anti-Social presents a dataset of 40 million tweets
amidst the COVID-19 pandemic on anti-social behaviours; and Anti-Asian presents
Twitter datasets collated based on anti-Asian behaviours during COVID-19
pandemic. All the curated datasets consist of structural graph information of
the Tweet threads. We show that DRAGNET++ outperforms all the state-of-the-art
baselines significantly. It beats the best baseline by an 11\% margin on the
Person correlation coefficient and a decrease of 25\% on RMSE for the
Anti-Racism dataset with a similar performance on the other two datasets.</p>
  </details>
</details>
<h1>机器学习</h1>
<details>
  <summary>1. <b>标题：Learning a Single Neuron with Adversarial Label Noise via Gradient  Descent</b></summary>
  <p><b>编号</b>：[6]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08918</p>
  <p><b>作者</b>：Ilias Diakonikolas,  Vasilis Kontonis,  Christos Tzamos,  Nikos Zarifis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：mathbf, adversarial label noise, mathbb, single neuron, label noise</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the fundamental problem of learning a single neuron, i.e., a
function of the form $\mathbf{x}\mapsto\sigma(\mathbf{w}\cdot\mathbf{x})$ for
monotone activations $\sigma:\mathbb{R}\mapsto\mathbb{R}$, with respect to the
$L_2^2$-loss in the presence of adversarial label noise. Specifically, we are
given labeled examples from a distribution $D$ on $(\mathbf{x},
y)\in\mathbb{R}^d \times \mathbb{R}$ such that there exists
$\mathbf{w}^\ast\in\mathbb{R}^d$ achieving $F(\mathbf{w}^\ast)=\epsilon$, where
$F(\mathbf{w})=\mathbf{E}_{(\mathbf{x},y)\sim D}[(\sigma(\mathbf{w}\cdot
\mathbf{x})-y)^2]$. The goal of the learner is to output a hypothesis vector
$\mathbf{w}$ such that $F(\mathbb{w})=C\, \epsilon$ with high probability,
where $C>1$ is a universal constant. As our main contribution, we give
efficient constant-factor approximate learners for a broad class of
distributions (including log-concave distributions) and activation functions.
Concretely, for the class of isotropic log-concave distributions, we obtain the
following important corollaries:
For the logistic activation, we obtain the first polynomial-time constant
factor approximation (even under the Gaussian distribution). Our algorithm has
sample complexity $\widetilde{O}(d/\epsilon)$, which is tight within
polylogarithmic factors.
For the ReLU activation, we give an efficient algorithm with sample
complexity $\tilde{O}(d\, \polylog(1/\epsilon))$. Prior to our work, the best
known constant-factor approximate learner had sample complexity
$\tilde{\Omega}(d/\epsilon)$.
In both of these settings, our algorithms are simple, performing
gradient-descent on the (regularized) $L_2^2$-loss. The correctness of our
algorithms relies on novel structural results that we establish, showing that
(essentially all) stationary points of the underlying non-convex loss are
approximately optimal.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Popular decision tree algorithms are provably noise tolerant</b></summary>
  <p><b>编号</b>：[11]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08899</p>
  <p><b>作者</b>：Guy Blanc,  Jane Lange,  Ali Malik,  Li-Yang Tan</p>
  <p><b>备注</b>：Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022</p>
  <p><b>关键词</b>：highly noise tolerant, including the classic, impurity-based decision tree, decision tree learning, framework of boosting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Using the framework of boosting, we prove that all impurity-based decision
tree learning algorithms, including the classic ID3, C4.5, and CART, are highly
noise tolerant. Our guarantees hold under the strongest noise model of nasty
noise, and we provide near-matching upper and lower bounds on the allowable
noise rate. We further show that these algorithms, which are simple and have
long been central to everyday machine learning, enjoy provable guarantees in
the noisy setting that are unmatched by existing algorithms in the theoretical
literature on decision tree learning. Taken together, our results add to an
ongoing line of research that seeks to place the empirical success of these
practical decision tree algorithms on firm theoretical footing.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Representational Multiplicity Should Be Exposed, Not Eliminated</b></summary>
  <p><b>编号</b>：[14]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08890</p>
  <p><b>作者</b>：Ari Heljakka,  Martin Trapp,  Juho Kannala,  Arno Solin</p>
  <p><b>备注</b>：15 pages, 5 figures</p>
  <p><b>关键词</b>：real-world performance characteristics, machine learning models, performance characteristics, prevalent and well-observed, poorly understood</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>It is prevalent and well-observed, but poorly understood, that two machine
learning models with similar performance during training can have very
different real-world performance characteristics. This implies elusive
differences in the internals of the models, manifesting as representational
multiplicity (RM). We introduce a conceptual and experimental setup for
analyzing RM and show that certain training methods systematically result in
greater RM than others, measured by activation similarity via singular vector
canonical correlation analysis (SVCCA). We further correlate it with predictive
multiplicity measured by the variance in i.i.d. and out-of-distribution test
set predictions, in four common image data sets. We call for systematic
measurement and maximal exposure, not elimination, of RM in models. Qualitative
tools such as our confabulator analysis can facilitate understanding and
communication of RM effects to stakeholders.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Fast Population-Based Reinforcement Learning on a Single Machine</b></summary>
  <p><b>编号</b>：[15]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08888</p>
  <p><b>作者</b>：Arthur Flajolet,  Claire Bizon Monroc,  Karim Beguir,  Thomas Pierrot</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：demonstrated great promise, demonstrated great, great promise, Training, population-based training</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Training populations of agents has demonstrated great promise in
Reinforcement Learning for stabilizing training, improving exploration and
asymptotic performance, and generating a diverse set of solutions. However,
population-based training is often not considered by practitioners as it is
perceived to be either prohibitively slow (when implemented sequentially), or
computationally expensive (if agents are trained in parallel on independent
accelerators). In this work, we compare implementations and revisit previous
studies to show that the judicious use of compilation and vectorization allows
population-based training to be performed on a single machine with one
accelerator with minimal overhead compared to training a single agent. We also
show that, when provided with a few accelerators, our protocols extend to large
population sizes for applications such as hyperparameter tuning. We hope that
this work and the public release of our code will encourage practitioners to
use population-based learning more frequently for their research and
applications.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Resolution Limits of Non-Adaptive 20 Questions Search for a Moving  Target</b></summary>
  <p><b>编号</b>：[16]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08884</p>
  <p><b>作者</b>：Lin Zhou,  Alfred Hero</p>
  <p><b>备注</b>：extended version of arXiv:2103.08097, under review in IEEE Transactions on Information Theory</p>
  <p><b>关键词</b>：questions estimation framework, unknown initial location, non-adaptive search strategies, questions estimation, estimation framework</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Using the 20 questions estimation framework with query-dependent noise, we
study non-adaptive search strategies for a moving target over the unit cube
with unknown initial location and velocities under a piecewise constant
velocity model. In this search problem, there is an oracle who knows the
instantaneous location of the target at any time. Our task is to query the
oracle as few times as possible to accurately estimate the location of the
target at any specified time. We first study the case where the oracle's answer
to each query is corrupted by discrete noise and then generalize our results to
the case of additive white Gaussian noise. In our formulation, the performance
criterion is the resolution, which is defined as the maximal $L_\infty$
distance between the true locations and estimated locations. We characterize
the minimal resolution of an optimal non-adaptive query procedure with a finite
number of queries by deriving non-asymptotic and asymptotic bounds. Our bounds
are tight in the first-order asymptotic sense when the number of queries
satisfies a certain condition and our bounds are tight in the stronger
second-order asymptotic sense when the target moves with a constant velocity.
To prove our results, we relate the current problem to channel coding, borrow
ideas from finite blocklength information theory and construct bounds on the
number of possible quantized target trajectories.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：CtrlFormer: Learning Transferable State Representation for Visual  Control via Transformer</b></summary>
  <p><b>编号</b>：[17]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08883</p>
  <p><b>作者</b>：Yao Mu,  Shoufa Chen,  Mingyu Ding,  Jianyu Chen,  Runjian Chen,  Ping Luo</p>
  <p><b>备注</b>：ICML 2022</p>
  <p><b>关键词</b>：achieved great successes, achieved great, great successes, vision and language, control</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transformer has achieved great successes in learning vision and language
representation, which is general across various downstream tasks. In visual
control, learning transferable state representation that can transfer between
different control tasks is important to reduce the training sample size.
However, porting Transformer to sample-efficient visual control remains a
challenging and unsolved problem. To this end, we propose a novel Control
Transformer (CtrlFormer), possessing many appealing benefits that prior arts do
not have. Firstly, CtrlFormer jointly learns self-attention mechanisms between
visual tokens and policy tokens among different control tasks, where multitask
representation can be learned and transferred without catastrophic forgetting.
Secondly, we carefully design a contrastive reinforcement learning paradigm to
train CtrlFormer, enabling it to achieve high sample efficiency, which is
important in control problems. For example, in the DMControl benchmark, unlike
recent advanced methods that failed by producing a zero score in the "Cartpole"
task after transfer learning with 100k samples, CtrlFormer can achieve a
state-of-the-art score with only 100k samples while maintaining the performance
of previous tasks. The code and models are released in our project homepage.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Improving Generalization of Metric Learning via Listwise  Self-distillation</b></summary>
  <p><b>编号</b>：[20]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08880</p>
  <p><b>作者</b>：Zelong Zeng,  Fan Yang,  Zheng Wang,  Shin'ichi Satoh</p>
  <p><b>备注</b>：11 pages, 7 figures</p>
  <p><b>关键词</b>：space while keeping, employ a strategy, deep metric learning, LSD, samples</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Most deep metric learning (DML) methods employ a strategy that forces all
positive samples to be close in the embedding space while keeping them away
from negative ones. However, such a strategy ignores the internal relationships
of positive (negative) samples and often leads to overfitting, especially in
the presence of hard samples and mislabeled samples. In this work, we propose a
simple yet effective regularization, namely Listwise Self-Distillation (LSD),
which progressively distills a model's own knowledge to adaptively assign a
more appropriate distance target to each sample pair in a batch. LSD encourages
smoother embeddings and information mining within positive (negative) samples
as a way to mitigate overfitting and thus improve generalization. Our LSD can
be directly integrated into general DML frameworks. Extensive experiments show
that LSD consistently boosts the performance of various metric learning methods
on multiple datasets.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：How robust are pre-trained models to distribution shift?</b></summary>
  <p><b>编号</b>：[22]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08871</p>
  <p><b>作者</b>：Yuge Shi,  Imant Daunhawer,  Julia E. Vogt,  Philip H.S. Torr,  Amartya Sanyal</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：spurious correlations, machine learning models, vulnerability of machine, context of supervised, spurious correlations affect</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The vulnerability of machine learning models to spurious correlations has
mostly been discussed in the context of supervised learning (SL). However,
there is a lack of insight on how spurious correlations affect the performance
of popular self-supervised learning (SSL) and auto-encoder based models (AE).
In this work, we shed light on this by evaluating the performance of these
models on both real world and synthetic distribution shift datasets. Following
observations that the linear head itself can be susceptible to spurious
correlations, we develop a novel evaluation scheme with the linear head trained
on out-of-distribution (OOD) data, to isolate the performance of the
pre-trained models from a potential bias of the linear head used for
evaluation. With this new methodology, we show that SSL models are consistently
more robust to distribution shifts and thus better at OOD generalisation than
AE and SL models.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Fast Lossless Neural Compression with Integer-Only Discrete Flows</b></summary>
  <p><b>编号</b>：[23]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08869</p>
  <p><b>作者</b>：Siyu Wang,  Jianfei Chen,  Chongxuan Li,  Jun Zhu,  Bo Zhang</p>
  <p><b>备注</b>：Accepted as a conference paper at International Conference on Machine Learning (ICML) 2022</p>
  <p><b>关键词</b>：learned data distributions, applying entropy codecs, outperformed traditional codecs, significantly outperformed traditional, data distributions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>By applying entropy codecs with learned data distributions, neural
compressors have significantly outperformed traditional codecs in terms of
compression ratio. However, the high inference latency of neural networks
hinders the deployment of neural compressors in practical applications. In this
work, we propose Integer-only Discrete Flows (IODF), an efficient neural
compressor with integer-only arithmetic. Our work is built upon integer
discrete flows, which consists of invertible transformations between discrete
random variables. We propose efficient invertible transformations with
integer-only arithmetic based on 8-bit quantization. Our invertible
transformation is equipped with learnable binary gates to remove redundant
filters during inference. We deploy IODF with TensorRT on GPUs, achieving 10x
inference speedup compared to the fastest existing neural compressors, while
retaining the high compression rates on ImageNet32 and ImageNet64.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Avoid Overfitting User Specific Information in Federated Keyword  Spotting</b></summary>
  <p><b>编号</b>：[24]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08864</p>
  <p><b>作者</b>：Xin-Chun Li,  Jin-Lin Tang,  Shaoming Song,  Bingshuai Li,  Yinchuan Li,  Yunfeng Shao,  Le Gan,  De-Chuan Zhan</p>
  <p><b>备注</b>：Accepted by Interspeech 2022</p>
  <p><b>关键词</b>：specific wake-up word, Keyword spotting, aims to discriminate, discriminate a specific, specific wake-up</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Keyword spotting (KWS) aims to discriminate a specific wake-up word from
other signals precisely and efficiently for different users. Recent works
utilize various deep networks to train KWS models with all users' speech data
centralized without considering data privacy. Federated KWS (FedKWS) could
serve as a solution without directly sharing users' data. However, the small
amount of data, different user habits, and various accents could lead to fatal
problems, e.g., overfitting or weight divergence. Hence, we propose several
strategies to encourage the model not to overfit user-specific information in
FedKWS. Specifically, we first propose an adversarial learning strategy, which
updates the downloaded global model against an overfitted local model and
explicitly encourages the global model to capture user-invariant information.
Furthermore, we propose an adaptive local training strategy, letting clients
with more training data and more uniform class distributions undertake more
local update steps. Equivalently, this strategy could weaken the negative
impacts of those users whose data is less qualified. Our proposed FedKWS-UI
could explicitly and implicitly learn user-invariant information in FedKWS.
Abundant experimental results on federated Google Speech Commands verify the
effectiveness of FedKWS-UI.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：MineDojo: Building Open-Ended Embodied Agents with Internet-Scale  Knowledge</b></summary>
  <p><b>编号</b>：[29]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08853</p>
  <p><b>作者</b>：Linxi Fan,  Guanzhi Wang,  Yunfan Jiang,  Ajay Mandlekar,  Yuncong Yang,  Haoyi Zhu,  Andrew Tang,  De-An Huang,  Yuke Zhu,  Anima Anandkumar</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：made great strides, domains like Atari, Atari games, made great, great strides</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Autonomous agents have made great strides in specialist domains like Atari
games and Go. However, they typically learn tabula rasa in isolated
environments with limited and manually conceived objectives, thus failing to
generalize across a wide spectrum of tasks and capabilities. Inspired by how
humans continually learn and adapt in the open world, we advocate a trinity of
ingredients for building generalist agents: 1) an environment that supports a
multitude of tasks and goals, 2) a large-scale database of multimodal
knowledge, and 3) a flexible and scalable agent architecture. We introduce
MineDojo, a new framework built on the popular Minecraft game that features a
simulation suite with thousands of diverse open-ended tasks and an
internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and
forum discussions. Using MineDojo's data, we propose a novel agent learning
algorithm that leverages large pre-trained video-language models as a learned
reward function. Our agent is able to solve a variety of open-ended tasks
specified in free-form language without any manually designed dense shaping
reward. We open-source the simulation suite and knowledge bases
(this https URL) to promote research towards the goal of generally
capable embodied agents.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Channel-wise Mixed-precision Assignment for DNN Inference on Constrained  Edge Nodes</b></summary>
  <p><b>编号</b>：[30]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08852</p>
  <p><b>作者</b>：Matteo Risso,  Alessio Burrello,  Luca Benini,  Enrico Macii,  Massimo Poncino,  Daniele Jahier Pagliari</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：deep neural networks, Neural Architecture Search, automated Neural Architecture, deep neural, widely employed</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Quantization is widely employed in both cloud and edge systems to reduce the
memory occupation, latency, and energy consumption of deep neural networks. In
particular, mixed-precision quantization, i.e., the use of different bit-widths
for different portions of the network, has been shown to provide excellent
efficiency gains with limited accuracy drops, especially with optimized
bit-width assignments determined by automated Neural Architecture Search (NAS)
tools. State-of-the-art mixed-precision works layer-wise, i.e., it uses
different bit-widths for the weights and activations tensors of each network
layer. In this work, we widen the search space, proposing a novel NAS that
selects the bit-width of each weight tensor channel independently. This gives
the tool the additional flexibility of assigning a higher precision only to the
weights associated with the most informative features. Testing on the MLPerf
Tiny benchmark suite, we obtain a rich collection of Pareto-optimal models in
the accuracy vs model size and accuracy vs energy spaces. When deployed on the
MPIC RISC-V edge processor, our networks reduce the memory and energy for
inference by up to 63% and 27% respectively compared to a layer-wise approach,
for the same accuracy.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：SMPL: Simulated Industrial Manufacturing and Process Control Learning  Environments</b></summary>
  <p><b>编号</b>：[31]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08851</p>
  <p><b>作者</b>：Mohan Zhang,  Xiaozhou Wang,  Benjamin Decardi-Nelson,  Bo Song,  An Zhang,  Jinfeng Liu,  Sile Tao,  Jiayi Cheng,  Xiaohong Liu,  DengDeng Yu,  Matthew Poon,  Animesh Garg</p>
  <p><b>备注</b>：Neurips 2022 Preprint. Under review</p>
  <p><b>关键词</b>：Traditional biological, pre-defined thresholds, biological and pharmaceutical, controlled by human, human workers</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Traditional biological and pharmaceutical manufacturing plants are controlled
by human workers or pre-defined thresholds. Modernized factories have advanced
process control algorithms such as model predictive control (MPC). However,
there is little exploration of applying deep reinforcement learning to control
manufacturing plants. One of the reasons is the lack of high fidelity
simulations and standard APIs for benchmarking. To bridge this gap, we develop
an easy-to-use library that includes five high-fidelity simulation
environments: BeerFMTEnv, ReactorEnv, AtropineEnv, PenSimEnv and mAbEnv, which
cover a wide range of manufacturing processes. We build these environments on
published dynamics models. Furthermore, we benchmark online and offline,
model-based and model-free reinforcement learning algorithms for comparisons of
follow-up research.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：AutoML Two-Sample Test</b></summary>
  <p><b>编号</b>：[33]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08843</p>
  <p><b>作者</b>：Jonas M. Kübler,  Vincent Stimper,  Simon Buchholz,  Krikamol Muandet,  Bernhard Schölkopf</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：tools for scientific, scientific discovery, detect distribution shifts, AutoML two-sample test, machine learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Two-sample tests are important in statistics and machine learning, both as
tools for scientific discovery as well as to detect distribution shifts. This
led to the development of many sophisticated test procedures going beyond the
standard supervised learning frameworks, whose usage can require specialized
knowledge about two-sample testing. We use a simple test that takes the mean
discrepancy of a witness function as the test statistic and prove that
minimizing a squared loss leads to a witness with optimal testing power. This
allows us to leverage recent advancements in AutoML. Without any user input
about the problems at hand, and using the same method for all our experiments,
our AutoML two-sample test achieves competitive performance on a diverse
distribution shift benchmark as well as on challenging two-sample testing
problems.
We provide an implementation of the AutoML two-sample test in the Python
package autotst.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Random projections and Kernelised Leave One Cluster Out  Cross-Validation: Universal baselines and evaluation tools for supervised  machine learning for materials properties</b></summary>
  <p><b>编号</b>：[35]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08841</p>
  <p><b>作者</b>：Samantha Durdy,  Michael Gaultois,  Vladimir Gusev,  Danushka Bollegala,  Matthew J. Rosseinsky</p>
  <p><b>备注</b>：16 pages including references, 9 figures</p>
  <p><b>关键词</b>：materials science literature, current computational materials, computational materials science, science literature, common place</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With machine learning being a popular topic in current computational
materials science literature, creating representations for compounds has become
common place. These representations are rarely compared, as evaluating their
performance - and the performance of the algorithms that they are used with -
is non-trivial. With many materials datasets containing bias and skew caused by
the research process, leave one cluster out cross validation (LOCO-CV) has been
introduced as a way of measuring the performance of an algorithm in predicting
previously unseen groups of materials. This raises the question of the impact,
and control, of the range of cluster sizes on the LOCO-CV measurement outcomes.
We present a thorough comparison between composition-based representations, and
investigate how kernel approximation functions can be used to better separate
data to enhance LOCO-CV applications.
We find that domain knowledge does not improve machine learning performance
in most tasks tested, with band gap prediction being the notable exception. We
also find that the radial basis function improves the linear separability of
chemical datasets in all 10 datasets tested and provide a framework for the
application of this function in the LOCO-CV process to improve the outcome of
LOCO-CV measurements regardless of machine learning algorithm, choice of
metric, and choice of compound representation. We recommend kernelised LOCO-CV
as a training paradigm for those looking to measure the extrapolatory power of
an algorithm on materials data.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Decentralized adaptive clustering of deep nets is beneficial for client  collaboration</b></summary>
  <p><b>编号</b>：[36]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08839</p>
  <p><b>作者</b>：Edvin Listo Zec,  Ebba Ekblom,  Martin Willbo,  Olof Mogren,  Sarunas Girdzijauskas</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：personalized deep learning, deep learning models, training personalized deep, local learning tasks, data distributions differ</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the problem of training personalized deep learning models in a
decentralized peer-to-peer setting, focusing on the setting where data
distributions differ between the clients and where different clients have
different local learning tasks. We study both covariate and label shift, and
our contribution is an algorithm which for each client finds beneficial
collaborations based on a similarity estimate for the local task. Our method
does not rely on hyperparameters which are hard to estimate, such as the number
of client clusters, but rather continuously adapts to the network topology
using soft cluster assignment based on a novel adaptive gossip algorithm. We
test the proposed method in various settings where data is not independent and
identically distributed among the clients. The experimental evaluation shows
that the proposed method performs better than previous state-of-the-art
algorithms for this problem setting, and handles situations well where previous
methods fail.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Prediction of Solar Radiation Based on Spatial and Temporal Embeddings  for Solar Generation Forecast</b></summary>
  <p><b>编号</b>：[39]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08832</p>
  <p><b>作者</b>：Mohammad Alqudah,  Tatjana Dokic,  Mladen Kezunovic,  Zoran Obradovic</p>
  <p><b>备注</b>：Proceedings of the 53rd IEEE Hawaii International Conference on System Sciences (HICSS 2020)</p>
  <p><b>关键词</b>：real-time solar generation, solar generation forecast, temporal structural dependencies, Solar Radiation Database, dependencies is proposed</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A novel method for real-time solar generation forecast using weather data,
while exploiting both spatial and temporal structural dependencies is proposed.
The network observed over time is projected to a lower-dimensional
representation where a variety of weather measurements are used to train a
structured regression model while weather forecast is used at the inference
stage. Experiments were conducted at 288 locations in the San Antonio, TX area
on obtained from the National Solar Radiation Database. The model predicts
solar irradiance with a good accuracy (R2 0.91 for the summer, 0.85 for the
winter, and 0.89 for the global model). The best accuracy was obtained by the
Random Forest Regressor. Multiple experiments were conducted to characterize
influence of missing data and different time horizons providing evidence that
the new algorithm is robust for data missing not only completely at random but
also when the mechanism is spatial, and temporal.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：FedNew: A Communication-Efficient and Privacy-Preserving Newton-Type  Method for Federated Learning</b></summary>
  <p><b>编号</b>：[40]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08829</p>
  <p><b>作者</b>：Anis Elgabli,  Chaouki Ben Issaid,  Amrit S. Bedi,  Ketan Rajawat,  Mehdi Bennis,  Vaneet Aggarwal</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：federated learning due, fast convergence, popular in federated, federated learning, Hessian information</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Newton-type methods are popular in federated learning due to their fast
convergence. Still, they suffer from two main issues, namely: low communication
efficiency and low privacy due to the requirement of sending Hessian
information from clients to parameter server (PS). In this work, we introduced
a novel framework called FedNew in which there is no need to transmit Hessian
information from clients to PS, hence resolving the bottleneck to improve
communication efficiency. In addition, FedNew hides the gradient information
and results in a privacy-preserving approach compared to the existing
state-of-the-art. The core novel idea in FedNew is to introduce a two level
framework, and alternate between updating the inverse Hessian-gradient product
using only one alternating direction method of multipliers (ADMM) step and then
performing the global model update using Newton's method. Though only one ADMM
pass is used to approximate the inverse Hessian-gradient product at each
iteration, we develop a novel theoretical approach to show the converging
behavior of FedNew for convex problems. Additionally, a significant reduction
in communication overhead is achieved by utilizing stochastic quantization.
Numerical results using real datasets show the superiority of FedNew compared
to existing methods in terms of communication costs.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Multimodal Attention-based Deep Learning for Alzheimer's Disease  Diagnosis</b></summary>
  <p><b>编号</b>：[41]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08826</p>
  <p><b>作者</b>：Michal Golovanevsky,  Carsten Eickhoff,  Ritambhara Singh</p>
  <p><b>备注</b>：11 pages, 5 figures</p>
  <p><b>关键词</b>：common neurodegenerative disorder, Alzheimer Disease Diagnosis, Multimodal Alzheimer Disease, Alzheimer Disease, Disease Diagnosis framework</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Alzheimer's Disease (AD) is the most common neurodegenerative disorder with
one of the most complex pathogeneses, making effective and clinically
actionable decision support difficult. The objective of this study was to
develop a novel multimodal deep learning framework to aid medical professionals
in AD diagnosis. We present a Multimodal Alzheimer's Disease Diagnosis
framework (MADDi) to accurately detect the presence of AD and mild cognitive
impairment (MCI) from imaging, genetic, and clinical data. MADDi is novel in
that we use cross-modal attention, which captures interactions between
modalities - a method not previously explored in this domain. We perform
multi-class classification, a challenging task considering the strong
similarities between MCI and AD. We compare with previous state-of-the-art
models, evaluate the importance of attention, and examine the contribution of
each modality to the model's performance. MADDi classifies MCI, AD, and
controls with 96.88% accuracy on a held-out test set. When examining the
contribution of different attention schemes, we found that the combination of
cross-modal attention with self-attention performed the best, and no attention
layers in the model performed the worst, with a 7.9% difference in F1-Scores.
Our experiments underlined the importance of structured clinical data to help
machine learning models contextualize and interpret the remaining modalities.
Extensive ablation studies showed that any multimodal mixture of input features
without access to structured clinical information suffered marked performance
losses. This study demonstrates the merit of combining multiple input
modalities via cross-modal attention to deliver highly accurate AD diagnostic
decision support.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Holistic Transformer: A Joint Neural Network for Trajectory Prediction  and Decision-Making of Autonomous Vehicles</b></summary>
  <p><b>编号</b>：[45]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08809</p>
  <p><b>作者</b>：Hongyu Hu,  Qi Wang,  Zhengguang Zhang,  Zhengyi Li,  Zhenhai Gao</p>
  <p><b>备注</b>：26 pages, 6 figures</p>
  <p><b>关键词</b>：require good understanding, environmental context, autonomous vehicles, vehicles that require, made by referring</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Trajectory prediction and behavioral decision-making are two important tasks
for autonomous vehicles that require good understanding of the environmental
context; behavioral decisions are better made by referring to the outputs of
trajectory predictions. However, most current solutions perform these two tasks
separately. Therefore, a joint neural network that combines multiple cues is
proposed and named as the holistic transformer to predict trajectories and make
behavioral decisions simultaneously. To better explore the intrinsic
relationships between cues, the network uses existing knowledge and adopts
three kinds of attention mechanisms: the sparse multi-head type for reducing
noise impact, feature selection sparse type for optimally using partial prior
knowledge, and multi-head with sigmoid activation type for optimally using
posteriori knowledge. Compared with other trajectory prediction models, the
proposed model has better comprehensive performance and good interpretability.
Perceptual noise robustness experiments demonstrate that the proposed model has
good noise robustness. Thus, simultaneous trajectory prediction and behavioral
decision-making combining multiple cues can reduce computational costs and
enhance semantic relationships between scenes and agents.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Truly Unordered Probabilistic Rule Sets for Multi-class Classification</b></summary>
  <p><b>编号</b>：[47]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08804</p>
  <p><b>作者</b>：Lincen Yang,  Matthijs van Leeuwen</p>
  <p><b>备注</b>：Accepted for publication at ECMLPKDD 2022</p>
  <p><b>关键词</b>：frequently revisited due, rule sets, Unordered Rule Sets, Rule, learns rule sets</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Rule set learning has long been studied and has recently been frequently
revisited due to the need for interpretable models. Still, existing methods
have several shortcomings: 1) most recent methods require a binary feature
matrix as input, learning rules directly from numeric variables is
understudied; 2) existing methods impose orders among rules, either explicitly
or implicitly, which harms interpretability; and 3) currently no method exists
for learning probabilistic rule sets for multi-class target variables (there is
only a method for probabilistic rule lists).
We propose TURS, for Truly Unordered Rule Sets, which addresses these
shortcomings. We first formalise the problem of learning truly unordered rule
sets. To resolve conflicts caused by overlapping rules, i.e., instances covered
by multiple rules, we propose a novel approach that exploits the probabilistic
properties of our rule sets. We next develop a two-phase heuristic algorithm
that learns rule sets by carefully growing rules. An important innovation is
that we use a surrogate score to take the global potential of the rule set into
account when learning a local rule.
Finally, we empirically demonstrate that, compared to non-probabilistic and
(explicitly or implicitly) ordered state-of-the-art methods, our method learns
rule sets that not only have better interpretability (i.e., they are smaller
and truly unordered), but also better predictive performance.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Open-Sampling: Exploring Out-of-Distribution data for Re-balancing  Long-tailed datasets</b></summary>
  <p><b>编号</b>：[49]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08802</p>
  <p><b>作者</b>：Hongxin Wei,  Lue Tao,  Renchunzi Xie,  Lei Feng,  Bo An</p>
  <p><b>备注</b>：Accepted by ICML 2022</p>
  <p><b>关键词</b>：extreme class imbalance, training dataset suffers, perform poorly, suffers from extreme, Deep neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep neural networks usually perform poorly when the training dataset suffers
from extreme class imbalance. Recent studies found that directly training with
out-of-distribution data (i.e., open-set samples) in a semi-supervised manner
would harm the generalization performance. In this work, we theoretically show
that out-of-distribution data can still be leveraged to augment the minority
classes from a Bayesian perspective. Based on this motivation, we propose a
novel method called Open-sampling, which utilizes open-set noisy labels to
re-balance the class priors of the training dataset. For each open-set
instance, the label is sampled from our pre-defined distribution that is
complementary to the distribution of original class priors. We empirically show
that Open-sampling not only re-balances the class priors but also encourages
the neural network to learn separable representations. Extensive experiments
demonstrate that our proposed method significantly outperforms existing data
re-balancing methods and can boost the performance of existing state-of-the-art
methods.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Reconstructing vehicles from orthographic drawings using deep neural  networks</b></summary>
  <p><b>编号</b>：[57]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08789</p>
  <p><b>作者</b>：Robin Klippert</p>
  <p><b>备注</b>：9 Pages</p>
  <p><b>关键词</b>：multiple orthographic drawings, explores the current, orthographic drawings, drawings using deep, deep neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper explores the current state-of-the-art of object reconstruction
from multiple orthographic drawings using deep neural networks. It proposes two
algorithms to extract multiple views from a single image. The paper proposes a
system based on pixel-aligned implicit functions (PIFu) and develops an
advanced sampling strategy to generate signed distance samples. It also
compares this approach to depth map regression from multiple views.
Additionally, the paper uses a novel dataset for vehicle reconstruction from
the racing game Assetto Corsa, which features higher quality models than the
commonly used ShapeNET dataset. The trained neural network generalizes well to
real-world inputs and creates plausible and detailed reconstructions.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Discovery of the Content and Engagement with the Content</b></summary>
  <p><b>编号</b>：[59]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08786</p>
  <p><b>作者</b>：Pushkal Agarwal,  Nishanth Sastry,  Edward Wood</p>
  <p><b>备注</b>：In APEN workshop, AAAI ICWSM 2018</p>
  <p><b>关键词</b>：Parliament allowed broadcasters, eventually television coverage, Parliament allowed, select committees, Parliament started publishing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the second half of the 20th century, Parliament allowed broadcasters to
transmit radio and eventually television coverage of debates and meetings of
select committees. More recently, in an effort to further improve transparency
and citizen engagement, the UK Parliament started publishing videos of these
debates and meetings itself, and tweeting details of debates as they happened.
In this paper, we attempt to characterise how people engage with video data of
Parliamentary debates by using more than two years of Google Analytics data
around these videos. We analyse the patterns of engagement - how do they land
on a particular video? How do they hear about this video, i.e., what is the
(HTTP) referrer website that led to the user clicking on the video? Once a user
lands on a video, how do they engage with it? For how long is the video played?
What is the next destination? etc. Answering these questions is an important
first step towards understanding why and how people use Parliamentary videos,
and therefore, how the video delivery platform should be adapted and
personalised for the needs of the citizens of the country. Taking inspiration
from An, Kwak, and Jansen (2017), we employ Non-Negative Matrix Factorization
(NMF) (Lee and Seung, 1999) on the video views matrix to identify different
archetypes of users, and identify archetypes. A deeper examination of the
archetypes we find reveals that they are primarily distinguished by how they
land on the video page: Search (i.e., through a search engine), Referral (i.e.,
from other Parliamentary websites), Direct (i.e., through a direct link, which
is embedded on another website), Social (i.e., through a social platform such
as Facebook or Twitter) and Others.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Reinforcement Learning in Macroeconomic Policy Design: A New Frontier?</b></summary>
  <p><b>编号</b>：[61]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08781</p>
  <p><b>作者</b>：Callum Tilbury</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：policy design toolboxes, enter mainstream policy, mainstream policy design, rich academic history, design toolboxes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Agent-based computational macroeconomics is a field with a rich academic
history, yet one which has struggled to enter mainstream policy design
toolboxes, plagued by the challenges associated with representing a complex and
dynamic reality. The field of Reinforcement Learning (RL), too, has a rich
history, and has recently been at the centre of several exponential
developments. Modern RL implementations have been able to achieve unprecedented
levels of sophistication, handling previously-unthinkable degrees of
complexity. This review surveys the historical barriers of classical
agent-based techniques in macroeconomic modelling, and contemplates whether
recent developments in RL can overcome any of them.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Multiple-Play Stochastic Bandits with Shareable Finite-Capacity Arms</b></summary>
  <p><b>编号</b>：[63]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08776</p>
  <p><b>作者</b>：Xuchuang Wang,  Hong Xie,  John C.S. Lui</p>
  <p><b>备注</b>：to appear in ICML 2022</p>
  <p><b>关键词</b>：multiple-play multi-armed bandits, shareable arm setting, shareable arm, lower bound, bound</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We generalize the multiple-play multi-armed bandits (MP-MAB) problem with a
shareable arm setting, in which several plays can share the same arm.
Furthermore, each shareable arm has a finite reward capacity and a ''per-load''
reward distribution, both of which are unknown to the learner. The reward from
a shareable arm is load-dependent, which is the "per-load" reward multiplying
either the number of plays pulling the arm, or its reward capacity when the
number of plays exceeds the capacity limit. When the "per-load" reward follows
a Gaussian distribution, we prove a sample complexity lower bound of learning
the capacity from load-dependent rewards and also a regret lower bound of this
new MP-MAB problem. We devise a capacity estimator whose sample complexity
upper bound matches the lower bound in terms of reward means and capacities. We
also propose an online learning algorithm to address the problem and prove its
regret upper bound. This regret upper bound's first term is the same as regret
lower bound's, and its second and third terms also evidently correspond to
lower bound's. Extensive experiments validate our algorithm's performance and
also its gain in 5G & 4G base station selection.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Beyond Ridge Regression for Distribution-Free Data</b></summary>
  <p><b>编号</b>：[70]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08757</p>
  <p><b>作者</b>：Koby Bibas,  Meir Feder</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：supervised batch learning, normalized maximum likelihood, predictive normalized maximum, maximum likelihood, min-max regret solution</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In supervised batch learning, the predictive normalized maximum likelihood
(pNML) has been proposed as the min-max regret solution for the
distribution-free setting, where no distributional assumptions are made on the
data. However, the pNML is not defined for a large capacity hypothesis class as
over-parameterized linear regression. For a large class, a common approach is
to use regularization or a model prior. In the context of online prediction
where the min-max solution is the Normalized Maximum Likelihood (NML), it has
been suggested to use NML with ``luckiness'': A prior-like function is applied
to the hypothesis class, which reduces its effective size. Motivated by the
luckiness concept, for linear regression we incorporate a luckiness function
that penalizes the hypothesis proportionally to its l2 norm. This leads to the
ridge regression solution. The associated pNML with luckiness (LpNML)
prediction deviates from the ridge regression empirical risk minimizer (Ridge
ERM): When the test data reside in the subspace corresponding to the small
eigenvalues of the empirical correlation matrix of the training data, the
prediction is shifted toward 0. Our LpNML reduces the Ridge ERM error by up to
20% for the PMLB sets, and is up to 4.9% more robust in the presence of
distribution shift compared to recent leading methods for UCI sets.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Federated learning with incremental clustering for heterogeneous data</b></summary>
  <p><b>编号</b>：[72]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08752</p>
  <p><b>作者</b>：Fabiola Espinoza Castellon,  Aurelien Mayoue,  Jacques-Henri Sublemontier,  Cedric Gouy-Pailler</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：clients' devices, Federated learning enables, Federated learning, enables different parties, parties to collaboratively</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Federated learning enables different parties to collaboratively build a
global model under the orchestration of a server while keeping the training
data on clients' devices. However, performance is affected when clients have
heterogeneous data. To cope with this problem, we assume that despite data
heterogeneity, there are groups of clients who have similar data distributions
that can be clustered. In previous approaches, in order to cluster clients the
server requires clients to send their parameters simultaneously. However, this
can be problematic in a context where there is a significant number of
participants that may have limited availability. To prevent such a bottleneck,
we propose FLIC (Federated Learning with Incremental Clustering), in which the
server exploits the updates sent by clients during federated training instead
of asking them to send their parameters simultaneously. Hence no additional
communications between the server and the clients are necessary other than what
classical federated learning requires. We empirically demonstrate for various
non-IID cases that our approach successfully splits clients into groups
following the same data distributions. We also identify the limitations of FLIC
by studying its capability to partition clients at the early stages of the
federated learning process efficiently. We further address attacks on models as
a form of data heterogeneity and empirically show that FLIC is a robust defense
against poisoning attacks even when the proportion of malicious clients is
higher than 50\%.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：ReViSe: Remote Vital Signs Measurement Using Smartphone Camera</b></summary>
  <p><b>编号</b>：[76]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08748</p>
  <p><b>作者</b>：Donghao Qiao,  Amtul Haq Ayesha,  Farhana Zulkernine,  Raihan Masroor,  Nauman Jaffar</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：collecting biometric data, Remote Photoplethysmography, Heart Rate Variability, enables vital signs, MAE</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Remote Photoplethysmography (rPPG) is a fast, effective, inexpensive and
convenient method for collecting biometric data as it enables vital signs
estimation using face videos. Remote contactless medical service provisioning
has proven to be a dire necessity during the COVID-19 pandemic. We propose an
end-to-end framework to measure people's vital signs including Heart Rate (HR),
Heart Rate Variability (HRV), Oxygen Saturation (SpO2) and Blood Pressure (BP)
based on the rPPG methodology from the video of a user's face captured with a
smartphone camera. We extract face landmarks with a deep learning-based neural
network model in real-time. Multiple face patches also called
Region-of-Interests (RoIs) are extracted by using the predicted face landmarks.
Several filters are applied to reduce the noise from the RoIs in the extracted
cardiac signals called Blood Volume Pulse (BVP) signal. We trained and
validated machine learning models using two public rPPG datasets namely the
TokyoTech rPPG and the Pulse Rate Detection (PURE) datasets, on which our
models achieved the following Mean Absolute Errors (MAE): a) for HR, 1.73 and
3.95 Beats-Per-Minute (bpm) respectively, b) for HRV, 18.55 and 25.03 ms
respectively, and c) for SpO2, a MAE of 1.64 on the PURE dataset. We validated
our end-to-end rPPG framework, ReViSe, in real life environment, and thereby
created the Video-HR dataset. Our HR estimation model achieved a MAE of 2.49
bpm on this dataset. Since no publicly available rPPG datasets existed for BP
measurement with face videos, we used a dataset with signals from fingertip
sensor to train our model and also created our own video dataset, Video-BP. On
our Video-BP dataset, our BP estimation model achieved a MAE of 6.7 mmHg for
Systolic Blood Pressure (SBP), and a MAE of 9.6 mmHg for Diastolic Blood
Pressure (DBP).</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Machine Learning-Driven Process of Alumina Ceramics Laser Machining</b></summary>
  <p><b>编号</b>：[77]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08747</p>
  <p><b>作者</b>：Razyeh Behbahani,  Hamidreza Yazdani Sarvestani,  Erfan Fatehi,  Elham Kiyani,  Behnam Ashrafi,  Mikko Karttunen,  Meysam Rahmat</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：highly flexible non-contact, flexible non-contact manufacturing, non-contact manufacturing technique, Laser machining, laser machining parameters</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Laser machining is a highly flexible non-contact manufacturing technique that
has been employed widely across academia and industry. Due to nonlinear
interactions between light and matter, simulation methods are extremely
crucial, as they help enhance the machining quality by offering comprehension
of the inter-relationships between the laser processing parameters. On the
other hand, experimental processing parameter optimization recommends a
systematic, and consequently time-consuming, investigation over the available
processing parameter space. An intelligent strategy is to employ machine
learning (ML) techniques to capture the relationship between picosecond laser
machining parameters for finding proper parameter combinations to create the
desired cuts on industrial-grade alumina ceramic with deep, smooth and
defect-free patterns. Laser parameters such as beam amplitude and frequency,
scanner passing speed and the number of passes over the surface, as well as the
vertical distance of the scanner from the sample surface, are used for
predicting the depth, top width, and bottom width of the engraved channels
using ML models. Owing to the complex correlation between laser parameters, it
is shown that Neural Networks (NN) are the most efficient in predicting the
outputs. Equipped with an ML model that captures the interconnection between
laser parameters and the engraved channel dimensions, one can predict the
required input parameters to achieve a target channel geometry. This strategy
significantly reduces the cost and effort of experimental laser machining
during the development phase, without compromising accuracy or performance. The
developed techniques can be applied to a wide range of ceramic laser machining
processes.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Learning Fair Representation via Distributional Contrastive  Disentanglement</b></summary>
  <p><b>编号</b>：[78]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08743</p>
  <p><b>作者</b>：Changdae Oh,  Heeji Won,  Junhyuk So,  Taero Kim,  Yewon Kim,  Hosik Choi,  Kyungwoo Song</p>
  <p><b>备注</b>：Accepted by KDD 2022 (Research Track)</p>
  <p><b>关键词</b>：crucial for achieving, Learning fair representation, representation, fair representation, Learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Learning fair representation is crucial for achieving fairness or debiasing
sensitive information. Most existing works rely on adversarial representation
learning to inject some invariance into representation. However, adversarial
learning methods are known to suffer from relatively unstable training, and
this might harm the balance between fairness and predictiveness of
representation. We propose a new approach, learning FAir Representation via
distributional CONtrastive Variational AutoEncoder (FarconVAE), which induces
the latent space to be disentangled into sensitive and nonsensitive parts. We
first construct the pair of observations with different sensitive attributes
but with the same labels. Then, FarconVAE enforces each non-sensitive latent to
be closer, while sensitive latents to be far from each other and also far from
the non-sensitive latent by contrasting their distributions. We provide a new
type of contrastive loss motivated by Gaussian and Student-t kernels for
distributional contrastive learning with theoretical analysis. Besides, we
adopt a new swap-reconstruction loss to boost the disentanglement further.
FarconVAE shows superior performance on fairness, pretrained model debiasing,
and domain generalization tasks from various modalities, including tabular,
image, and text.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Near-Optimal No-Regret Learning for General Convex Games</b></summary>
  <p><b>编号</b>：[79]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08742</p>
  <p><b>作者</b>：Gabriele Farina,  Ioannis Anagnostides,  Haipeng Luo,  Chung-Wei Lee,  Christian Kroer,  Tuomas Sandholm</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：repetitions grows polylogarithmically, compact strategy sets, established uncoupled learning, repetitions grows, no-regret framework</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A recent line of work has established uncoupled learning dynamics such that,
when employed by all players in a game, each player's \emph{regret} after $T$
repetitions grows polylogarithmically in $T$, an exponential improvement over
the traditional guarantees within the no-regret framework. However, so far
these results have only been limited to certain classes of games with
structured strategy spaces -- such as normal-form and extensive-form games. The
question as to whether $O(\text{polylog} T)$ regret bounds can be obtained for
general convex and compact strategy sets -- which occur in many fundamental
models in economics and multiagent systems -- while retaining efficient
strategy updates is an important question. In this paper, we answer this in the
positive by establishing the first uncoupled learning algorithm with $O(\log
T)$ per-player regret in general \emph{convex games}, that is, games with
concave utility functions supported on arbitrary convex and compact strategy
sets. Our learning dynamics are based on an instantiation of optimistic
follow-the-regularized-leader over an appropriately \emph{lifted} space using a
\emph{self-concordant regularizer} that is, peculiarly, not a barrier for the
feasible region. Further, our learning dynamics are efficiently implementable
given access to a proximal oracle for the convex strategy set, leading to
$O(\log\log T)$ per-iteration complexity; we also give extensions when access
to only a \emph{linear} optimization oracle is assumed. Finally, we adapt our
dynamics to guarantee $O(\sqrt{T})$ regret in the adversarial regime. Even in
those special cases where prior results apply, our algorithm improves over the
state-of-the-art regret bounds either in terms of the dependence on the number
of iterations or on the dimension of the strategy sets.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Detecting Adversarial Examples in Batches -- a geometrical approach</b></summary>
  <p><b>编号</b>：[80]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08738</p>
  <p><b>作者</b>：Danush Kumar Venkatesh,  Peter Steinbach</p>
  <p><b>备注</b>：Submitted to AdvML workshop at ICML2022</p>
  <p><b>关键词</b>：speech recognition applications, successfully solved complex, solved complex tasks, deep learning methods, recognition applications</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Many deep learning methods have successfully solved complex tasks in computer
vision and speech recognition applications. Nonetheless, the robustness of
these models has been found to be vulnerable to perturbed inputs or adversarial
examples, which are imperceptible to the human eye, but lead the model to
erroneous output decisions. In this study, we adapt and introduce two geometric
metrics, density and coverage, and evaluate their use in detecting adversarial
samples in batches of unseen data. We empirically study these metrics using
MNIST and two real-world biomedical datasets from MedMNIST, subjected to two
different adversarial attacks. Our experiments show promising results for both
metrics to detect adversarial examples. We believe that his work can lay the
ground for further study on these metrics' use in deployed machine learning
systems to monitor for possible attacks by adversarial examples or related
pathologies such as dataset shift.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Evaluation of Contrastive Learning with Various Code Representations for  Code Clone Detection</b></summary>
  <p><b>编号</b>：[85]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08726</p>
  <p><b>作者</b>：Maksim Zubkov,  Egor Spirin,  Egor Bogomolov,  Timofey Bryksin</p>
  <p><b>备注</b>：12 pages, 7 figures</p>
  <p><b>关键词</b>：implement similar functionality, Code, Clone detection, implement similar, detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Code clones are pairs of code snippets that implement similar functionality.
Clone detection is a fundamental branch of automatic source code comprehension,
having many applications in refactoring recommendation, plagiarism detection,
and code summarization. A particularly interesting case of clone detection is
the detection of semantic clones, i.e., code snippets that have the same
functionality but significantly differ in implementation. A promising approach
to detecting semantic clones is contrastive learning (CL), a machine learning
paradigm popular in computer vision but not yet commonly adopted for code
processing.
Our work aims to evaluate the most popular CL algorithms combined with three
source code representations on two tasks. The first task is code clone
detection, which we evaluate on the POJ-104 dataset containing implementations
of 104 algorithms. The second task is plagiarism detection. To evaluate the
models on this task, we introduce CodeTransformator, a tool for transforming
source code. We use it to create a dataset that mimics plagiarised code based
on competitive programming solutions. We trained nine models for both tasks and
compared them with six existing approaches, including traditional tools and
modern pre-trained neural models. The results of our evaluation show that
proposed models perform diversely in each task, however the performance of the
graph-based models is generally above the others. Among CL algorithms, SimCLR
and SwAV lead to better results, while Moco is the most robust approach. Our
code and trained models are available at
this https URL, this https URL.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Fast Finite Width Neural Tangent Kernel</b></summary>
  <p><b>编号</b>：[90]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08720</p>
  <p><b>作者</b>：Roman Novak,  Jascha Sohl-Dickstein,  Samuel S. Schoenholz</p>
  <p><b>备注</b>：Published as a conference paper at ICML 2022</p>
  <p><b>关键词</b>：Theta, Neural Tangent Kernel, partial, finite width NTK, Tangent Kernel</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The Neural Tangent Kernel (NTK), defined as $\Theta_\theta^f(x_1, x_2) =
\left[\partial f(\theta, x_1)\big/\partial \theta\right] \left[\partial
f(\theta, x_2)\big/\partial \theta\right]^T$ where $\left[\partial f(\theta,
\cdot)\big/\partial \theta\right]$ is a neural network (NN) Jacobian, has
emerged as a central object of study in deep learning. In the infinite width
limit, the NTK can sometimes be computed analytically and is useful for
understanding training and generalization of NN architectures. At finite
widths, the NTK is also used to better initialize NNs, compare the conditioning
across models, perform architecture search, and do meta-learning.
Unfortunately, the finite width NTK is notoriously expensive to compute, which
severely limits its practical utility. We perform the first in-depth analysis
of the compute and memory requirements for NTK computation in finite width
networks. Leveraging the structure of neural networks, we further propose two
novel algorithms that change the exponent of the compute and memory
requirements of the finite width NTK, dramatically improving efficiency. Our
algorithms can be applied in a black box fashion to any differentiable
function, including those implementing neural networks. We open-source our
implementations within the Neural Tangents package (arXiv:1912.02803) at
this https URL.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Evaluating the Impact of Source Code Parsers on ML4SE Models</b></summary>
  <p><b>编号</b>：[93]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08713</p>
  <p><b>作者</b>：Ilya Utkin,  Egor Spirin,  Egor Bogomolov,  Timofey Bryksin</p>
  <p><b>备注</b>：12 pages, 3 figures</p>
  <p><b>关键词</b>：apply Machine Learning, Machine Learning, software engineering problems, practitioners apply Machine, apply Machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As researchers and practitioners apply Machine Learning to increasingly more
software engineering problems, the approaches they use become more
sophisticated. A lot of modern approaches utilize internal code structure in
the form of an abstract syntax tree (AST) or its extensions: path-based
representation, complex graph combining AST with additional edges. Even though
the process of extracting ASTs from code can be done with different parsers,
the impact of choosing a parser on the final model quality remains unstudied.
Moreover, researchers often omit the exact details of extracting particular
code representations.
In this work, we evaluate two models, namely Code2Seq and TreeLSTM, in the
method name prediction task backed by eight different parsers for the Java
language. To unify the process of data preparation with different parsers, we
develop SuperParser, a multi-language parser-agnostic library based on
PathMiner. SuperParser facilitates the end-to-end creation of datasets suitable
for training and evaluation of ML models that work with structural information
from source code. Our results demonstrate that trees built by different parsers
vary in their structure and content. We then analyze how this diversity affects
the models' quality and show that the quality gap between the most and least
suitable parsers for both models turns out to be significant. Finally, we
discuss other features of the parsers that researchers and practitioners should
take into account when selecting a parser along with the impact on the models'
quality.
The code of SuperParser is publicly available at
this https URL. We also publish Java-norm, the dataset
we use to evaluate the models: this https URL.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Statistical and Neural Methods for Cross-lingual Entity Label Mapping in  Knowledge Graphs</b></summary>
  <p><b>编号</b>：[95]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08709</p>
  <p><b>作者</b>：Gabriel Amaral,  Mārcis Pinnis,  Inguna Skadiņa,  Odinaldo Rodrigues,  Elena Simperl</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：amass vast amounts, Wikidata amass vast, named entity information, amass vast, vast amounts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Knowledge bases such as Wikidata amass vast amounts of named entity
information, such as multilingual labels, which can be extremely useful for
various multilingual and cross-lingual applications. However, such labels are
not guaranteed to match across languages from an information consistency
standpoint, greatly compromising their usefulness for fields such as machine
translation. In this work, we investigate the application of word and sentence
alignment techniques coupled with a matching algorithm to align cross-lingual
entity labels extracted from Wikidata in 10 languages. Our results indicate
that mapping between Wikidata's main labels stands to be considerably improved
(up to $20$ points in F1-score) by any of the employed methods. We show how
methods relying on sentence embeddings outperform all others, even across
different scripts. We believe the application of such techniques to measure the
similarity of label pairs, coupled with a knowledge base rich in high-quality
entity labels, to be an excellent asset to machine translation.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Explainability's Gain is Optimality's Loss? -- How Explanations Bias  Decision-making</b></summary>
  <p><b>编号</b>：[97]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08705</p>
  <p><b>作者</b>：Charles Wan,  Rodrigo Belo,  Leid Zejnilović</p>
  <p><b>备注</b>：To appear in the Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society (AIES '22)</p>
  <p><b>关键词</b>：serve organizational goals, organizational goals, serve organizational, evaluating alternatives, machine learning algorithms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Decisions in organizations are about evaluating alternatives and choosing the
one that would best serve organizational goals. To the extent that the
evaluation of alternatives could be formulated as a predictive task with
appropriate metrics, machine learning algorithms are increasingly being used to
improve the efficiency of the process. Explanations help to facilitate
communication between the algorithm and the human decision-maker, making it
easier for the latter to interpret and make decisions on the basis of
predictions by the former. Feature-based explanations' semantics of causal
models, however, induce leakage from the decision-maker's prior beliefs. Our
findings from a field experiment demonstrate empirically how this leads to
confirmation bias and disparate impact on the decision-maker's confidence in
the predictions. Such differences can lead to sub-optimal and biased decision
outcomes.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：Maximum Class Separation as Inductive Bias in One Matrix</b></summary>
  <p><b>编号</b>：[98]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08704</p>
  <p><b>作者</b>：Tejaswi Kasarla,  Gertjan J. Burghouts,  Max van Spengler,  Elise van der Pol,  Rita Cucchiara,  Pascal Mettes</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：well-known inductive bias, inductive bias, traditional algorithms, constitutes a well-known, machine learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Maximizing the separation between classes constitutes a well-known inductive
bias in machine learning and a pillar of many traditional algorithms. By
default, deep networks are not equipped with this inductive bias and therefore
many alternative solutions have been proposed through differential
optimization. Current approaches tend to optimize classification and separation
jointly: aligning inputs with class vectors and separating class vectors
angularly. This paper proposes a simple alternative: encoding maximum
separation as an inductive bias in the network by adding one fixed matrix
multiplication before computing the softmax activations. The main observation
behind our approach is that separation does not require optimization but can be
solved in closed-form prior to training and plugged into a network. We outline
a recursive approach to obtain the matrix consisting of maximally separable
vectors for any number of classes, which can be added with negligible
engineering effort and computational overhead. Despite its simple nature, this
one matrix multiplication provides real impact. We show that our proposal
directly boosts classification, long-tailed recognition, out-of-distribution
detection, and open-set recognition, from CIFAR to ImageNet. We find
empirically that maximum separation works best as a fixed bias; making the
matrix learnable adds nothing to the performance. The closed-form
implementation and code to reproduce the experiments are on github.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Plotly-Resampler: Effective Visual Analytics for Large Time Series</b></summary>
  <p><b>编号</b>：[99]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08703</p>
  <p><b>作者</b>：Jonas Van Der Donckt,  Jeroen Van Der Donckt,  Emiel Deprost,  Sofie Van Hoecke</p>
  <p><b>备注</b>：The first two authors contributed equally. Submitted to IEEE VIS 2022</p>
  <p><b>关键词</b>：time series, effective time series, time series visualization, important step, realize effective time</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Visual analytics is arguably the most important step in getting acquainted
with your data. This is especially the case for time series, as this data type
is hard to describe and cannot be fully understood when using for example
summary statistics. To realize effective time series visualization, four
requirements have to be met; a tool should be (1) interactive, (2) scalable to
millions of data points, (3) integrable in conventional data science
environments, and (4) highly configurable. We observe that open source Python
visualization toolkits empower data scientists in most visual analytics tasks,
but lack the combination of scalability and interactivity to realize effective
time series visualization. As a means to facilitate these requirements, we
created Plotly-Resampler, an open source Python library. Plotly-Resampler is an
add-on for Plotly's Python bindings, enhancing line chart scalability on top of
an interactive toolkit by aggregating the underlying data depending on the
current graph view. Plotly-Resampler is built to be snappy, as the reactivity
of a tool qualitatively affects how analysts visually explore and analyze data.
A benchmark task highlights how our toolkit scales better than alternatives in
terms of number of samples and time series. Additionally, Plotly-Resampler's
flexible data aggregation functionality paves the path towards researching
novel aggregation techniques. Plotly-Resampler's integrability, together with
its configurability, convenience, and high scalability, allows to effectively
analyze high-frequency data in your day-to-day Python environment.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Sheaf Neural Networks with Connection Laplacians</b></summary>
  <p><b>编号</b>：[100]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08702</p>
  <p><b>作者</b>：Federico Barbero,  Cristian Bodnar,  Haitz Sáez de Ocáriz Borde,  Michael Bronstein,  Petar Veličković,  Pietro Liò</p>
  <p><b>备注</b>：Presented at the ICML 2022 Workshop on Topology, Algebra, and Geometry in Machine Learning</p>
  <p><b>关键词</b>：Graph Neural Network, Sheaf Neural Network, Neural Network, Graph Neural, Network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A Sheaf Neural Network (SNN) is a type of Graph Neural Network (GNN) that
operates on a sheaf, an object that equips a graph with vector spaces over its
nodes and edges and linear maps between these spaces. SNNs have been shown to
have useful theoretical properties that help tackle issues arising from
heterophily and over-smoothing. One complication intrinsic to these models is
finding a good sheaf for the task to be solved. Previous works proposed two
diametrically opposed approaches: manually constructing the sheaf based on
domain knowledge and learning the sheaf end-to-end using gradient-based
methods. However, domain knowledge is often insufficient, while learning a
sheaf could lead to overfitting and significant computational overhead. In this
work, we propose a novel way of computing sheaves drawing inspiration from
Riemannian geometry: we leverage the manifold assumption to compute
manifold-and-graph-aware orthogonal maps, which optimally align the tangent
spaces of neighbouring data points. We show that this approach achieves
promising results with less computational overhead when compared to previous
SNN models. Overall, this work provides an interesting connection between
algebraic topology and differential geometry, and we hope that it will spark
future research in this direction.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Towards Human-Level Bimanual Dexterous Manipulation with Reinforcement  Learning</b></summary>
  <p><b>编号</b>：[106]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08686</p>
  <p><b>作者</b>：Yuanpei Chen,  Yaodong Yang,  Tianhao Wu,  Shengjie Wang,  Xidong Feng,  Jiechuang Jiang,  Stephen Marcus McAleer,  Hao Dong,  Zongqing Lu,  Song-Chun Zhu</p>
  <p><b>备注</b>：36 pages, 7 figures</p>
  <p><b>关键词</b>：Achieving human-level dexterity, important open problem, Achieving human-level, problem in robotics, human-level dexterity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Achieving human-level dexterity is an important open problem in robotics.
However, tasks of dexterous hand manipulation, even at the baby level, are
challenging to solve through reinforcement learning (RL). The difficulty lies
in the high degrees of freedom and the required cooperation among heterogeneous
agents (e.g., joints of fingers). In this study, we propose the Bimanual
Dexterous Hands Benchmark (Bi-DexHands), a simulator that involves two
dexterous hands with tens of bimanual manipulation tasks and thousands of
target objects. Specifically, tasks in Bi-DexHands are designed to match
different levels of human motor skills according to cognitive science
literature. We built Bi-DexHands in the Issac Gym; this enables highly
efficient RL training, reaching 30,000+ FPS by only one single NVIDIA RTX 3090.
We provide a comprehensive benchmark for popular RL algorithms under different
settings; this includes Single-agent/Multi-agent RL, Offline RL, Multi-task RL,
and Meta RL. Our results show that the PPO type of on-policy algorithms can
master simple manipulation tasks that are equivalent up to 48-month human
babies (e.g., catching a flying object, opening a bottle), while multi-agent RL
can further help to master manipulations that require skilled bimanual
cooperation (e.g., lifting a pot, stacking blocks). Despite the success on each
single task, when it comes to acquiring multiple manipulation skills, existing
RL algorithms fail to work in most of the multi-task and the few-shot learning
settings, which calls for more substantial development from the RL community.
Our project is open sourced at this https URL.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：Sparse Double Descent: Where Network Pruning Aggravates Overfitting</b></summary>
  <p><b>编号</b>：[107]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08684</p>
  <p><b>作者</b>：Zheng He,  Zeke Xie,  Quanzhi Zhu,  Zengchang Qin</p>
  <p><b>备注</b>：ICML 2022</p>
  <p><b>关键词</b>：double descent, decreasing model capacity, network pruning, sparse double descent, reduces the computational</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>People usually believe that network pruning not only reduces the
computational cost of deep networks, but also prevents overfitting by
decreasing model capacity. However, our work surprisingly discovers that
network pruning sometimes even aggravates overfitting. We report an unexpected
sparse double descent phenomenon that, as we increase model sparsity via
network pruning, test performance first gets worse (due to overfitting), then
gets better (due to relieved overfitting), and gets worse at last (due to
forgetting useful information). While recent studies focused on the deep double
descent with respect to model overparameterization, they failed to recognize
that sparsity may also cause double descent. In this paper, we have three main
contributions. First, we report the novel sparse double descent phenomenon
through extensive experiments. Second, for this phenomenon, we propose a novel
learning distance interpretation that the curve of $\ell_{2}$ learning distance
of sparse models (from initialized parameters to final parameters) may
correlate with the sparse double descent curve well and reflect generalization
better than minima flatness. Third, in the context of sparse double descent, a
winning ticket in the lottery ticket hypothesis surprisingly may not always
win.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：BITS Pilani at HinglishEval: Quality Evaluation for Code-Mixed Hinglish  Text Using Transformers</b></summary>
  <p><b>编号</b>：[109]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08680</p>
  <p><b>作者</b>：Shaz Furniturewala,  Vijay Kumari,  Amulya Ratna Dash,  Hriday Kedia,  Yashvardhan Sharma</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：text data consists, Code-Mixed text data, words or phrases, Code-Mixed text, text data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Code-Mixed text data consists of sentences having words or phrases from more
than one language. Most multi-lingual communities worldwide communicate using
multiple languages, with English usually one of them. Hinglish is a Code-Mixed
text composed of Hindi and English but written in Roman script. This paper aims
to determine the factors influencing the quality of Code-Mixed text data
generated by the system. For the HinglishEval task, the proposed model uses
multi-lingual BERT to find the similarity between synthetically generated and
human-generated sentences to predict the quality of synthetically generated
Hinglish sentences.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Understanding Robust Overfitting of Adversarial Training and Beyond</b></summary>
  <p><b>编号</b>：[110]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08675</p>
  <p><b>作者</b>：Chaojian Yu,  Bo Han,  Li Shen,  Jun Yu,  Chen Gong,  Mingming Gong,  Tongliang Liu</p>
  <p><b>备注</b>：ICML2022</p>
  <p><b>关键词</b>：overfitting widely exists, data, Robust overfitting, adversarial training, adversarial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Robust overfitting widely exists in adversarial training of deep networks.
The exact underlying reasons for this are still not completely understood.
Here, we explore the causes of robust overfitting by comparing the data
distribution of \emph{non-overfit} (weak adversary) and \emph{overfitted}
(strong adversary) adversarial training, and observe that the distribution of
the adversarial data generated by weak adversary mainly contain small-loss
data. However, the adversarial data generated by strong adversary is more
diversely distributed on the large-loss data and the small-loss data. Given
these observations, we further designed data ablation adversarial training and
identify that some small-loss data which are not worthy of the adversary
strength cause robust overfitting in the strong adversary mode. To relieve this
issue, we propose \emph{minimum loss constrained adversarial training} (MLCAT):
in a minibatch, we learn large-loss data as usual, and adopt additional
measures to increase the loss of the small-loss data. Technically, MLCAT
hinders data fitting when they become easy to learn to prevent robust
overfitting; philosophically, MLCAT reflects the spirit of turning waste into
treasure and making the best use of each adversarial data; algorithmically, we
designed two realizations of MLCAT, and extensive experiments demonstrate that
MLCAT can eliminate robust overfitting and further boost adversarial
robustness.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：A Deep Learning Approach for the Segmentation of Electroencephalography  Data in Eye Tracking Applications</b></summary>
  <p><b>编号</b>：[112]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08672</p>
  <p><b>作者</b>：Lukas Wolf,  Ard Kastrati,  Martyna Beata Płomecka,  Jie-Ming Li,  Dustin Klebe,  Alexander Veicht,  Roger Wattenhofer,  Nicolas Langer</p>
  <p><b>备注</b>：21 pages, Published at the Proceedings of the 39th International Conference on Machine Learning (ICML) 2022</p>
  <p><b>关键词</b>：eye gaze information, health and behaviour, human cognition, critical aspects, aspects of human</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The collection of eye gaze information provides a window into many critical
aspects of human cognition, health and behaviour. Additionally, many
neuroscientific studies complement the behavioural information gained from eye
tracking with the high temporal resolution and neurophysiological markers
provided by electroencephalography (EEG). One of the essential eye-tracking
software processing steps is the segmentation of the continuous data stream
into events relevant to eye-tracking applications, such as saccades, fixations,
and blinks.
Here, we introduce DETRtime, a novel framework for time-series segmentation
that creates ocular event detectors that do not require additionally recorded
eye-tracking modality and rely solely on EEG data. Our end-to-end deep
learning-based framework brings recent advances in Computer Vision to the
forefront of the times series segmentation of EEG data. DETRtime achieves
state-of-the-art performance in ocular event detection across diverse
eye-tracking experiment paradigms. In addition to that, we provide evidence
that our model generalizes well in the task of EEG sleep stage segmentation.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Boosting Factorization Machines via Saliency-Guided Mixup</b></summary>
  <p><b>编号</b>：[115]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08661</p>
  <p><b>作者</b>：Chenwang Wu,  Defu Lian,  Yong Ge,  Min Zhou,  Enhong Chen,  Dacheng Tao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：recommender systems due, recommender systems, systems due, adaptability and ability, sparse data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Factorization machines (FMs) are widely used in recommender systems due to
their adaptability and ability to learn from sparse data. However, for the
ubiquitous non-interactive features in sparse data, existing FMs can only
estimate the parameters corresponding to these features via the inner product
of their embeddings. Undeniably, they cannot learn the direct interactions of
these features, which limits the model's expressive power. To this end, we
first present MixFM, inspired by Mixup, to generate auxiliary training data to
boost FMs. Unlike existing augmentation strategies that require labor costs and
expertise to collect additional information such as position and fields, these
extra data generated by MixFM only by the convex combination of the raw ones
without any professional knowledge support. More importantly, if the parent
samples to be mixed have non-interactive features, MixFM will establish their
direct interactions. Second, considering that MixFM may generate redundant or
even detrimental instances, we further put forward a novel Factorization
Machine powered by Saliency-guided Mixup (denoted as SMFM). Guided by the
customized saliency, SMFM can generate more informative neighbor data. Through
theoretical analysis, we prove that the proposed methods minimize the upper
bound of the generalization error, which hold a beneficial effect on enhancing
FMs. Significantly, we give the first generalization bound of FM, implying the
generalization requires more data and a smaller embedding size under the
sufficient representation capability. Finally, extensive experiments on five
datasets confirm that our approaches are superior to baselines. Besides, the
results show that "poisoning" mixed data is likewise beneficial to the FM
variants.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Digital Twin Data Modelling by Randomized Orthogonal Decomposition and  Deep Learning</b></summary>
  <p><b>编号</b>：[117]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08659</p>
  <p><b>作者</b>：Diana Alina Bistrian,  Omer San,  Ionel Michael Navon</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：digital twin model, digital twin, DIGITAL TWIN DATA, TWIN DATA MODEL, twin model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A digital twin is a surrogate model that has the main feature to mirror the
original process behavior. Associating the dynamical process with a digital
twin model of reduced complexity has the significant advantage to map the
dynamics with high accuracy and reduced costs in CPU time and hardware to
timescales over which that suffers significantly changes and so it is difficult
to explore. This paper introduces a new framework for creating efficient
digital twin models of fluid flows. We introduce a novel algorithm that
combines the advantages of Krylov based dynamic mode decomposition with proper
orthogonal decomposition and outperforms the selection of the most influential
modes. We prove that randomized orthogonal decomposition algorithm provides
several advantages over SVD empirical orthogonal decomposition methods and
mitigates the projection error formulating a multiobjective optimization
problem.We involve the state-of-the-art artificial intelligence Deep Learning
(DL) to perform a real-time adaptive calibration of the digital twin model,
with increasing fidelity. The output is a high-fidelity DIGITAL TWIN DATA MODEL
of the fluid flow dynamics, with the advantage of a reduced complexity. The new
modelling tools are investigated in the numerical simulation of three wave
phenomena with increasing complexity. We show that the outputs are consistent
with the original source data.We perform a thorough assessment of the
performance of the new digital twin data models, in terms of numerical accuracy
and computational efficiency, including a time simulation response feature
study.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Bridge-Tower: Building Bridges Between Encoders in Vision-Language  Representation Learning</b></summary>
  <p><b>编号</b>：[118]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08657</p>
  <p><b>作者</b>：Xiao Xu,  Chenfei Wu,  Shachar Rosenman,  Vasudev Lal,  Nan Duan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：dominated visual-language representation, visual-language representation learning, uni-modal encoders, recent years, cross-modal encoder</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision-Language (VL) models with the Two-Tower architecture have dominated
visual-language representation learning in recent years. Current VL models
either use lightweight uni-modal encoders and learn to extract, align and fuse
both modalities simultaneously in a cross-modal encoder, or feed the last-layer
uni-modal features directly into the top cross-modal encoder, ignoring the
semantic information at the different levels in the deep uni-modal encoders.
Both approaches possibly restrict vision-language representation learning and
limit model performance. In this paper, we introduce multiple bridge layers
that build a connection between the top layers of uni-modal encoders and each
layer of the cross-modal encoder. This enables comprehensive bottom-up
interactions between visual and textual representations at different semantic
levels, resulting in more effective cross-modal alignment and fusion. Our
proposed Bridge-Tower, pre-trained with only $4$M images, achieves
state-of-the-art performance on various downstream vision-language tasks. On
the VQAv2 test-std set, Bridge-Tower achieves an accuracy of $78.73\%$,
outperforming the previous state-of-the-art METER model by $1.09\%$ with the
same pre-training data and almost no additional parameters and computational
cost. Notably, when further scaling the model, Bridge-Tower achieves an
accuracy of $81.15\%$, surpassing models that are pre-trained on
orders-of-magnitude larger datasets. Code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：tinySNN: Towards Memory- and Energy-Efficient Spiking Neural Networks</b></summary>
  <p><b>编号</b>：[119]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08656</p>
  <p><b>作者</b>：Rachmad Vidya Wicaksana Putra,  Muhammad Shafique</p>
  <p><b>备注</b>：9 figures</p>
  <p><b>关键词</b>：Larger Spiking Neural, Spiking Neural Network, Spiking Neural, Larger Spiking, offer higher accuracy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Larger Spiking Neural Network (SNN) models are typically favorable as they
can offer higher accuracy. However, employing such models on the resource- and
energy-constrained embedded platforms is inefficient. Towards this, we present
a tinySNN framework that optimizes the memory and energy requirements of SNN
processing in both the training and inference phases, while keeping the
accuracy high. It is achieved by reducing the SNN operations, improving the
learning quality, quantizing the SNN parameters, and selecting the appropriate
SNN model. Furthermore, our tinySNN quantizes different SNN parameters (i.e.,
weights and neuron parameters) to maximize the compression while exploring
different combinations of quantization schemes, precision levels, and rounding
schemes to find the model that provides acceptable accuracy. The experimental
results demonstrate that our tinySNN significantly reduces the memory footprint
and the energy consumption of SNNs without accuracy loss as compared to the
baseline network. Therefore, our tinySNN effectively compresses the given SNN
model to achieve high accuracy in a memory- and energy-efficient manner, hence
enabling the employment of SNNs for the resource- and energy-constrained
embedded applications.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：All Mistakes Are Not Equal: Comprehensive Hierarchy Aware Multi-label  Predictions (CHAMP)</b></summary>
  <p><b>编号</b>：[121]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08653</p>
  <p><b>作者</b>：Ashwin Vaswani,  Gaurav Aggarwal,  Praneeth Netrapalli,  Narayan G Hegde</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：domain-specific hierarchy tree, present Comprehensive Hierarchy, hierarchy tree, Aware Multi-label Predictions, domain-specific hierarchy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper considers the problem of Hierarchical Multi-Label Classification
(HMC), where (i) several labels can be present for each example, and (ii)
labels are related via a domain-specific hierarchy tree. Guided by the
intuition that all mistakes are not equal, we present Comprehensive Hierarchy
Aware Multi-label Predictions (CHAMP), a framework that penalizes a
misprediction depending on its severity as per the hierarchy tree. While there
have been works that apply such an idea to single-label classification, to the
best of our knowledge, there are limited such works for multilabel
classification focusing on the severity of mistakes. The key reason is that
there is no clear way of quantifying the severity of a misprediction a priori
in the multilabel setting. In this work, we propose a simple but effective
metric to quantify the severity of a mistake in HMC, naturally leading to
CHAMP. Extensive experiments on six public HMC datasets across modalities
(image, audio, and text) demonstrate that incorporating hierarchical
information leads to substantial gains as CHAMP improves both AUPRC (2.6%
median percentage improvement) and hierarchical metrics (2.85% median
percentage improvement), over stand-alone hierarchical or multilabel
classification methods. Compared to standard multilabel baselines, CHAMP
provides improved AUPRC in both robustness (8.87% mean percentage improvement )
and less data regimes. Further, our method provides a framework to enhance
existing multilabel classification algorithms with better mistakes (18.1% mean
percentage increment).</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Scalable Differentially Private Clustering via Hierarchically Separated  Trees</b></summary>
  <p><b>编号</b>：[123]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08646</p>
  <p><b>作者</b>：Vincent Cohen-Addad,  Alessandro Epasto,  Silvio Lattanzi,  Vahab Mirrokni,  Andres Munoz,  David Saulpic,  Chris Schwiegelshohn,  Sergei Vassilvitskii</p>
  <p><b>备注</b>：To appear at KDD'22</p>
  <p><b>关键词</b>：dimensional Euclidean space, dimensional Euclidean, Euclidean space, Euclidean, clustering problem</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the private $k$-median and $k$-means clustering problem in $d$
dimensional Euclidean space. By leveraging tree embeddings, we give an
efficient and easy to implement algorithm, that is empirically competitive with
state of the art non private methods. We prove that our method computes a
solution with cost at most $O(d^{3/2}\log n)\cdot OPT + O(k d^2 \log^2 n /
\epsilon^2)$, where $\epsilon$ is the privacy guarantee. (The dimension term,
$d$, can be replaced with $O(\log k)$ using standard dimension reduction
techniques.) Although the worst-case guarantee is worse than that of state of
the art private clustering methods, the algorithm we propose is practical, runs
in near-linear, $\tilde{O}(nkd)$, time and scales to tens of millions of
points. We also show that our method is amenable to parallelization in
large-scale distributed computing environments. In particular we show that our
private algorithms can be implemented in logarithmic number of MPC rounds in
the sublinear memory regime. Finally, we complement our theoretical analysis
with an empirical evaluation demonstrating the algorithm's efficiency and
accuracy in comparison to other privacy clustering baselines.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Minimum Noticeable Difference based Adversarial Privacy Preserving Image  Generation</b></summary>
  <p><b>编号</b>：[130]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08638</p>
  <p><b>作者</b>：Wen Sun,  Jian Jin,  Weisi Lin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Deep learning models, adversarial, adversarial image generation, Deep learning, learning models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning models are found to be vulnerable to adversarial examples, as
wrong predictions can be caused by small perturbation in input for deep
learning models. Most of the existing works of adversarial image generation try
to achieve attacks for most models, while few of them make efforts on
guaranteeing the perceptual quality of the adversarial examples. High quality
adversarial examples matter for many applications, especially for the privacy
preserving. In this work, we develop a framework based on the Minimum
Noticeable Difference (MND) concept to generate adversarial privacy preserving
images that have minimum perceptual difference from the clean ones but are able
to attack deep learning models. To achieve this, an adversarial loss is firstly
proposed to make the deep learning models attacked by the adversarial images
successfully. Then, a perceptual quality-preserving loss is developed by taking
the magnitude of perturbation and perturbation-caused structural and gradient
changes into account, which aims to preserve high perceptual quality for
adversarial image generation. To the best of our knowledge, this is the first
work on exploring quality-preserving adversarial image generation based on the
MND concept for privacy preserving. To evaluate its performance in terms of
perceptual quality, the deep models on image classification and face
recognition are tested with the proposed method and several anchor methods in
this work. Extensive experimental results demonstrate that the proposed MND
framework is capable of generating adversarial images with remarkably improved
performance metrics (e.g., PSNR, SSIM, and MOS) than that generated with the
anchor methods.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：The Role of Depth, Width, and Activation Complexity in the Number of  Linear Regions of Neural Networks</b></summary>
  <p><b>编号</b>：[137]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08615</p>
  <p><b>作者</b>：Alexis Goujon,  Arian Etemadi,  Michael Unser</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：feedforward neural networks, neural networks generate, networks generate continuous, continuous and piecewise-linear, linear regions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Many feedforward neural networks generate continuous and piecewise-linear
(CPWL) mappings. Specifically, they partition the input domain into regions on
which the mapping is an affine function. The number of these so-called linear
regions offers a natural metric to characterize the expressiveness of CPWL
mappings. Although the precise determination of this quantity is often out of
reach, bounds have been proposed for specific architectures, including the
well-known ReLU and Maxout networks. In this work, we propose a more general
perspective and provide precise bounds on the maximal number of linear regions
of CPWL networks based on three sources of expressiveness: depth, width, and
activation complexity. Our estimates rely on the combinatorial structure of
convex partitions and highlight the distinctive role of depth which, on its
own, is able to exponentially increase the number of regions. We then introduce
a complementary stochastic framework to estimate the average number of linear
regions produced by a CPWL network architecture. Under reasonable assumptions,
the expected density of linear regions along any 1D path is bounded by the
product of depth, width, and a measure of activation complexity (up to a
scaling factor). This yields an identical role to the three sources of
expressiveness: no exponential growth with depth is observed anymore.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：On Efficient Real-Time Semantic Segmentation: A Survey</b></summary>
  <p><b>编号</b>：[143]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08605</p>
  <p><b>作者</b>：Christopher J. Holder,  Muhammad Shafique</p>
  <p><b>备注</b>：18 pages, 13 figures, 4 tables This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible</p>
  <p><b>关键词</b>：facilitating scene understanding, vehicle vision stack, autonomous vehicle vision, semantic segmentation models, performing semantic segmentation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Semantic segmentation is the problem of assigning a class label to every
pixel in an image, and is an important component of an autonomous vehicle
vision stack for facilitating scene understanding and object detection.
However, many of the top performing semantic segmentation models are extremely
complex and cumbersome, and as such are not suited to deployment onboard
autonomous vehicle platforms where computational resources are limited and
low-latency operation is a vital requirement. In this survey, we take a
thorough look at the works that aim to address this misalignment with more
compact and efficient models capable of deployment on low-memory embedded
systems while meeting the constraint of real-time inference. We discuss several
of the most prominent works in the field, placing them within a taxonomy based
on their major contributions, and finally we evaluate the inference speed of
the discussed models under consistent hardware and software setups that
represent a typical research environment with high-end GPU and a realistic
deployed scenario using low-memory embedded GPU hardware. Our experimental
results demonstrate that many works are capable of real-time performance on
resource-constrained hardware, while illustrating the consistent trade-off
between latency and accuracy.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：On the Influence of Enforcing Model Identifiability on Learning dynamics  of Gaussian Mixture Models</b></summary>
  <p><b>编号</b>：[146]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08598</p>
  <p><b>作者</b>：Pascal Mattia Esser,  Frank Nielsen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：parameter space, model parameter space, statistical model space, space, parameter</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A common way to learn and analyze statistical models is to consider
operations in the model parameter space. But what happens if we optimize in the
parameter space and there is no one-to-one mapping between the parameter space
and the underlying statistical model space? Such cases frequently occur for
hierarchical models which include statistical mixtures or stochastic neural
networks, and these models are said to be singular. Singular models reveal
several important and well-studied problems in machine learning like the
decrease in convergence speed of learning trajectories due to attractor
behaviors. In this work, we propose a relative reparameterization technique of
the parameter space, which yields a general method for extracting regular
submodels from singular models. Our method enforces model identifiability
during training and we study the learning dynamics for gradient descent and
expectation maximization for Gaussian Mixture Models (GMMs) under relative
parameterization, showing faster experimental convergence and a improved
manifold shape of the dynamics around the singularity. Extending the analysis
beyond GMMs, we furthermore analyze the Fisher information matrix under
relative reparameterization and its influence on the generalization error, and
show how the method can be applied to more complex models like deep neural
networks.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Accelerating numerical methods by gradient-based meta-solving</b></summary>
  <p><b>编号</b>：[147]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08594</p>
  <p><b>作者</b>：Sohei Arisaka,  Qianxiao Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：computational problems repeatedly, solve similar computational, similar computational problems, science and engineering, similar computational</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In science and engineering applications, it is often required to solve
similar computational problems repeatedly. In such cases, we can utilize the
data from previously solved problem instances to improve the efficiency of
finding subsequent solutions. This offers a unique opportunity to combine
machine learning (in particular, meta-learning) and scientific computing. To
date, a variety of such domain-specific methods have been proposed in the
literature, but a generic approach for designing these methods remains
under-explored. In this paper, we tackle this issue by formulating a general
framework to describe these problems, and propose a gradient-based algorithm to
solve them in a unified way. As an illustration of this approach, we study the
adaptive generation of parameters for iterative solvers to accelerate the
solution of differential equations. We demonstrate the performance and
versatility of our method through theoretical analysis and numerical
experiments, including applications to incompressible flow simulations and an
inverse problem of parameter estimation.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Automatic Correction of Human Translations</b></summary>
  <p><b>编号</b>：[148]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08593</p>
  <p><b>作者</b>：Jessy Lin,  Geza Kovacs,  Aditya Shastry,  Joern Wuebker,  John DeNero</p>
  <p><b>备注</b>：NAACL 2022. Dataset available at: this https URL</p>
  <p><b>关键词</b>：automatically correcting human-generated, TEC, errors, correcting human-generated translations, automatically correcting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce translation error correction (TEC), the task of automatically
correcting human-generated translations. Imperfections in machine translations
(MT) have long motivated systems for improving translations post-hoc with
automatic post-editing. In contrast, little attention has been devoted to the
problem of automatically correcting human translations, despite the intuition
that humans make distinct errors that machines would be well-suited to assist
with, from typos to inconsistencies in translation conventions. To investigate
this, we build and release the Aced corpus with three TEC datasets. We show
that human errors in TEC exhibit a more diverse range of errors and far fewer
translation fluency errors than the MT errors in automatic post-editing
datasets, suggesting the need for dedicated TEC models that are specialized to
correct human errors. We show that pre-training instead on synthetic errors
based on human errors improves TEC F-score by as much as 5.1 points. We
conducted a human-in-the-loop user study with nine professional translation
editors and found that the assistance of our TEC system led them to produce
significantly higher quality revised translations.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：NAFS: A Simple yet Tough-to-beat Baseline for Graph Representation  Learning</b></summary>
  <p><b>编号</b>：[152]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08583</p>
  <p><b>作者</b>：Wentao Zhang,  Zeang Sheng,  Mingyu Yang,  Yang Li,  Yu Shen,  Zhi Yang,  Bin Cui</p>
  <p><b>备注</b>：17 pages, 8 figures</p>
  <p><b>关键词</b>：shown prominent performance, graph neural networks, neural networks, shown prominent, prominent performance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, graph neural networks (GNNs) have shown prominent performance in
graph representation learning by leveraging knowledge from both graph structure
and node features. However, most of them have two major limitations. First,
GNNs can learn higher-order structural information by stacking more layers but
can not deal with large depth due to the over-smoothing issue. Second, it is
not easy to apply these methods on large graphs due to the expensive
computation cost and high memory usage. In this paper, we present node-adaptive
feature smoothing (NAFS), a simple non-parametric method that constructs node
representations without parameter learning. NAFS first extracts the features of
each node with its neighbors of different hops by feature smoothing, and then
adaptively combines the smoothed features. Besides, the constructed node
representation can further be enhanced by the ensemble of smoothed features
extracted via different smoothing strategies. We conduct experiments on four
benchmark datasets on two different application scenarios: node clustering and
link prediction. Remarkably, NAFS with feature ensemble outperforms the
state-of-the-art GNNs on these tasks and mitigates the aforementioned two
limitations of most learning-based GNN counterparts.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：DFG-NAS: Deep and Flexible Graph Neural Architecture Search</b></summary>
  <p><b>编号</b>：[153]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08582</p>
  <p><b>作者</b>：Wentao Zhang,  Zheyu Lin,  Yu Shen,  Yang Li,  Zhi Yang,  Bin Cui</p>
  <p><b>备注</b>：13 pages, 7 figures</p>
  <p><b>关键词</b>：Graph neural networks, graph-based applications, intensively applied, Graph neural, neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Graph neural networks (GNNs) have been intensively applied to various
graph-based applications. Despite their success, manually designing the
well-behaved GNNs requires immense human expertise. And thus it is inefficient
to discover the potentially optimal data-specific GNN architecture. This paper
proposes DFG-NAS, a new neural architecture search (NAS) method that enables
the automatic search of very deep and flexible GNN architectures. Unlike most
existing methods that focus on micro-architectures, DFG-NAS highlights another
level of design: the search for macro-architectures on how atomic propagation
(\textbf{\texttt{P}}) and transformation (\textbf{\texttt{T}}) operations are
integrated and organized into a GNN. To this end, DFG-NAS proposes a novel
search space for \textbf{\texttt{P-T}} permutations and combinations based on
message-passing dis-aggregation, defines four custom-designed
macro-architecture mutations, and employs the evolutionary algorithm to conduct
an efficient and effective search. Empirical studies on four node
classification tasks demonstrate that DFG-NAS outperforms state-of-the-art
manual designs and NAS methods of GNNs.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：Query-Efficient and Scalable Black-Box Adversarial Attacks on Discrete  Sequential Data via Bayesian Optimization</b></summary>
  <p><b>编号</b>：[155]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08575</p>
  <p><b>作者</b>：Deokjae Lee,  Seungyong Moon,  Junhyeok Lee,  Hyun Oh Song</p>
  <p><b>备注</b>：ICML 2022; Codes at this https URL</p>
  <p><b>关键词</b>：discrete sequential data, limited query access, victim model, discrete sequential, sequential data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We focus on the problem of adversarial attacks against models on discrete
sequential data in the black-box setting where the attacker aims to craft
adversarial examples with limited query access to the victim model. Existing
black-box attacks, mostly based on greedy algorithms, find adversarial examples
using pre-computed key positions to perturb, which severely limits the search
space and might result in suboptimal solutions. To this end, we propose a
query-efficient black-box attack using Bayesian optimization, which dynamically
computes important positions using an automatic relevance determination (ARD)
categorical kernel. We introduce block decomposition and history subsampling
techniques to improve the scalability of Bayesian optimization when an input
sequence becomes long. Moreover, we develop a post-optimization algorithm that
finds adversarial examples with smaller perturbation size. Experiments on
natural language and protein classification tasks demonstrate that our method
consistently achieves higher attack success rate with significant reduction in
query count and modification rate compared to the previous state-of-the-art
methods.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：Bootstrapped Transformer for Offline Reinforcement Learning</b></summary>
  <p><b>编号</b>：[159]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08569</p>
  <p><b>作者</b>：Kerong Wang,  Hanye Zhao,  Xufang Luo,  Kan Ren,  Weinan Zhang,  Dongsheng Li</p>
  <p><b>备注</b>：A complete manuscript under review</p>
  <p><b>关键词</b>：previously collected static, collected static trajectory, Offline reinforcement learning, static trajectory data, reinforcement learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Offline reinforcement learning (RL) aims at learning policies from previously
collected static trajectory data without interacting with the real environment.
Recent works provide a novel perspective by viewing offline RL as a generic
sequence generation problem, adopting sequence models such as Transformer
architecture to model distributions over trajectories, and repurposing beam
search as a planning algorithm. However, the training datasets utilized in
general offline RL tasks are quite limited and often suffer from insufficient
distribution coverage, which could be harmful to training sequence generation
models yet has not drawn enough attention in the previous works. In this paper,
we propose a novel algorithm named Bootstrapped Transformer, which incorporates
the idea of bootstrapping and leverages the learned model to self-generate more
offline data to further boost the sequence model training. We conduct extensive
experiments on two offline RL benchmarks and demonstrate that our model can
largely remedy the existing offline RL training limitations and beat other
strong baseline methods. We also analyze the generated pseudo data and the
revealed characteristics may shed some light on offline RL training. The codes
are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：MET: Masked Encoding for Tabular Data</b></summary>
  <p><b>编号</b>：[164]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08564</p>
  <p><b>作者</b>：Kushal Majmundar,  Sachin Goyal,  Praneeth Netrapalli,  Prateek Jain</p>
  <p><b>备注</b>：Under Review, 18 pages, 6 Tables, 4 Figures</p>
  <p><b>关键词</b>：learning based SSL, task of self-supervised, tabular data, data, self-supervised representation learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider the task of self-supervised representation learning (SSL) for
tabular data: tabular-SSL. Typical contrastive learning based SSL methods
require instance-wise data augmentations which are difficult to design for
unstructured tabular data. Existing tabular-SSL methods design such
augmentations in a relatively ad-hoc fashion and can fail to capture the
underlying data manifold. Instead of augmentations based approaches for
tabular-SSL, we propose a new reconstruction based method, called Masked
Encoding for Tabular Data (MET), that does not require augmentations. MET is
based on the popular MAE approach for vision-SSL [He et al., 2021] and uses two
key ideas: (i) since each coordinate in a tabular dataset has a distinct
meaning, we need to use separate representations for all coordinates, and (ii)
using an adversarial reconstruction loss in addition to the standard one.
Empirical results on five diverse tabular datasets show that MET achieves a new
state of the art (SOTA) on all of these datasets and improves up to 9% over
current SOTA methods. We shed more light on the working of MET via experiments
on carefully designed simple datasets.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：Boosting Graph Structure Learning with Dummy Nodes</b></summary>
  <p><b>编号</b>：[165]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08561</p>
  <p><b>作者</b>：Xin Liu,  Jiayang Cheng,  Yangqiu Song,  Xin Jiang</p>
  <p><b>备注</b>：Accepted by ICML 2022</p>
  <p><b>关键词</b>：superior methods, proposed to handle, handle scalability, scalability and oversmoothing, oversmoothing issues</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the development of graph kernels and graph representation learning, many
superior methods have been proposed to handle scalability and oversmoothing
issues on graph structure learning. However, most of those strategies are
designed based on practical experience rather than theoretical analysis. In
this paper, we use a particular dummy node connecting to all existing vertices
without affecting original vertex and edge properties. We further prove that
such the dummy node can help build an efficient monomorphic edge-to-vertex
transform and an epimorphic inverse to recover the original graph back. It also
indicates that adding dummy nodes can preserve local and global structures for
better graph representation learning. We extend graph kernels and graph neural
networks with dummy nodes and conduct experiments on graph classification and
subgraph isomorphism matching tasks. Empirical results demonstrate that taking
graphs with dummy nodes as input significantly boosts graph structure learning,
and using their edge-to-vertex graphs can also achieve similar results. We also
discuss the gain of expressive power from the dummy in neural networks.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：How You Start Matters for Generalization</b></summary>
  <p><b>编号</b>：[166]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08558</p>
  <p><b>作者</b>：Sameera Ramasinghe,  Lachlan MacDonald,  Moshiur Farazi,  Hemanth Sartachandran,  Simon Lucey</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：remarkable generalization properties, Characterizing the remarkable, open problem, over-parameterized neural networks, neural networks remains</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Characterizing the remarkable generalization properties of over-parameterized
neural networks remains an open problem. In this paper, we promote a shift of
focus towards initialization rather than neural architecture or (stochastic)
gradient descent to explain this implicit regularization. Through a Fourier
lens, we derive a general result for the spectral bias of neural networks and
show that the generalization of neural networks is heavily tied to their
initialization. Further, we empirically solidify the developed theoretical
insights using practical, deep networks. Finally, we make a case against the
controversial flat-minima conjecture and show that Fourier analysis grants a
more reliable framework for understanding the generalization of neural
networks.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：Thompson Sampling for Robust Transfer in Multi-Task Bandits</b></summary>
  <p><b>编号</b>：[167]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08556</p>
  <p><b>作者</b>：Zhi Wang,  Chicheng Zhang,  Kamalika Chaudhuri</p>
  <p><b>备注</b>：To appear in Proceedings of the 39th International Conference on Machine Learning (ICML-2022)</p>
  <p><b>关键词</b>：multi-armed bandit environments, necessarily identical multi-armed, identical multi-armed bandit, bandit environments, necessarily identical</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the problem of online multi-task learning where the tasks are
performed within similar but not necessarily identical multi-armed bandit
environments. In particular, we study how a learner can improve its overall
performance across multiple related tasks through robust transfer of knowledge.
While an upper confidence bound (UCB)-based algorithm has recently been shown
to achieve nearly-optimal performance guarantees in a setting where all tasks
are solved concurrently, it remains unclear whether Thompson sampling (TS)
algorithms, which have superior empirical performance in general, share similar
theoretical properties. In this work, we present a TS-type algorithm for a more
general online multi-task learning protocol, which extends the concurrent
setting. We provide its frequentist analysis and prove that it is also
nearly-optimal using a novel concentration inequality for multi-task data
aggregation at random stopping times. Finally, we evaluate the algorithm on
synthetic data and show that the TS-type algorithm enjoys superior empirical
performance in comparison with the UCB-based algorithm and a baseline algorithm
that performs TS for each individual task without transfer.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：SOS: Score-based Oversampling for Tabular Data</b></summary>
  <p><b>编号</b>：[168]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08555</p>
  <p><b>作者</b>：Jayoung Kim,  Chaejeong Lee,  Yehjin Shin,  Sewon Park,  Minjung Kim,  Noseong Park,  Jihoon Cho</p>
  <p><b>备注</b>：Accepted by KDD 2022</p>
  <p><b>关键词</b>：generating fake images, generative models, recent breakthrough, Score-based generative models, fake images</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Score-based generative models (SGMs) are a recent breakthrough in generating
fake images. SGMs are known to surpass other generative models, e.g.,
generative adversarial networks (GANs) and variational autoencoders (VAEs).
Being inspired by their big success, in this work, we fully customize them for
generating fake tabular data. In particular, we are interested in oversampling
minor classes since imbalanced classes frequently lead to sub-optimal training
outcomes. To our knowledge, we are the first presenting a score-based tabular
data oversampling method. Firstly, we re-design our own score network since we
have to process tabular data. Secondly, we propose two options for our
generation method: the former is equivalent to a style transfer for tabular
data and the latter uses the standard generative policy of SGMs. Lastly, we
define a fine-tuning method, which further enhances the oversampling quality.
In our experiments with 6 datasets and 10 baselines, our method outperforms
other oversampling methods in all cases.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：Strategic Representation</b></summary>
  <p><b>编号</b>：[172]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08542</p>
  <p><b>作者</b>：Vineet Nair,  Ganesh Ghalme,  Inbal Talgam-Cohen,  Nir Rosenfeld</p>
  <p><b>备注</b>：ICML 2022</p>
  <p><b>关键词</b>：reducing excessive information, reducing excessive, excessive information, information to manageable, representations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Humans have come to rely on machines for reducing excessive information to
manageable representations. But this reliance can be abused -- strategic
machines might craft representations that manipulate their users. How can a
user make good choices based on strategic representations? We formalize this as
a learning problem, and pursue algorithms for decision-making that are robust
to manipulation. In our main setting of interest, the system represents
attributes of an item to the user, who then decides whether or not to consume.
We model this interaction through the lens of strategic classification (Hardt
et al. 2016), reversed: the user, who learns, plays first; and the system,
which responds, plays second. The system must respond with representations that
reveal `nothing but the truth' but need not reveal the entire truth. Thus, the
user faces the problem of learning set functions under strategic subset
selection, which presents distinct algorithmic and statistical challenges. Our
main result is a learning algorithm that minimizes error despite strategic
representations, and our theoretical analysis sheds light on the trade-off
between learning effort and susceptibility to manipulation.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：Large-Margin Representation Learning for Texture Classification</b></summary>
  <p><b>编号</b>：[173]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08537</p>
  <p><b>作者</b>：Jonathan de Matos,  Luiz Eduardo Soares de Oliveira,  Alceu de Souza Britto Junior,  Alessandro Lameiras Koerich</p>
  <p><b>备注</b>：7 pages</p>
  <p><b>关键词</b>：combining convolutional layers, paper presents, approach combining convolutional, large-margin metric learning, training supervised models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a novel approach combining convolutional layers (CLs) and
large-margin metric learning for training supervised models on small datasets
for texture classification. The core of such an approach is a loss function
that computes the distances between instances of interest and support vectors.
The objective is to update the weights of CLs iteratively to learn a
representation with a large margin between classes. Each iteration results in a
large-margin discriminant model represented by support vectors based on such a
representation. The advantage of the proposed approach w.r.t. convolutional
neural networks (CNNs) is two-fold. First, it allows representation learning
with a small amount of data due to the reduced number of parameters compared to
an equivalent CNN. Second, it has a low training cost since the backpropagation
considers only support vectors. The experimental results on texture and
histopathologic image datasets have shown that the proposed approach achieves
competitive accuracy with lower computational cost and faster convergence when
compared to equivalent CNNs.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：Accelerating Shapley Explanation via Contributive Cooperator Selection</b></summary>
  <p><b>编号</b>：[176]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08529</p>
  <p><b>作者</b>：Guanchu Wang,  Yu-Neng Chuang,  Mengnan Du,  Fan Yang,  Quan Zhou,  Pushkar Tripathi,  Xuanting Cai,  Xia Hu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：DNN model prediction, exponentially growing complexity, input feature coalitions, DNN model, DNN models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Even though Shapley value provides an effective explanation for a DNN model
prediction, the computation relies on the enumeration of all possible input
feature coalitions, which leads to the exponentially growing complexity. To
address this problem, we propose a novel method SHEAR to significantly
accelerate the Shapley explanation for DNN models, where only a few coalitions
of input features are involved in the computation. The selection of the feature
coalitions follows our proposed Shapley chain rule to minimize the absolute
error from the ground-truth Shapley values, such that the computation can be
both efficient and accurate. To demonstrate the effectiveness, we
comprehensively evaluate SHEAR across multiple metrics including the absolute
error from the ground-truth Shapley value, the faithfulness of the
explanations, and running speed. The experimental results indicate SHEAR
consistently outperforms state-of-the-art baseline methods across different
evaluation metrics, which demonstrates its potentials in real-world
applications where the computational resource is limited.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：SafeRL-Kit: Evaluating Efficient Reinforcement Learning Methods for Safe  Autonomous Driving</b></summary>
  <p><b>编号</b>：[177]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08528</p>
  <p><b>作者</b>：Linrui Zhang,  Qin Zhang,  Li Shen,  Bo Yuan,  Xueqian Wang</p>
  <p><b>备注</b>：The 1st Workshop on Safe Learning for Autonomous Driving (SL4AD) with ICML 2022</p>
  <p><b>关键词</b>：achieved significant success, achieved significant, significant success, success on risk-sensitive, shown promise</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Safe reinforcement learning (RL) has achieved significant success on
risk-sensitive tasks and shown promise in autonomous driving (AD) as well.
Considering the distinctiveness of this community, efficient and reproducible
baselines are still lacking for safe AD. In this paper, we release SafeRL-Kit
to benchmark safe RL methods for AD-oriented tasks. Concretely, SafeRL-Kit
contains several latest algorithms specific to zero-constraint-violation tasks,
including Safety Layer, Recovery RL, off-policy Lagrangian method, and Feasible
Actor-Critic. In addition to existing approaches, we propose a novel
first-order method named Exact Penalty Optimization (EPO) and sufficiently
demonstrate its capability in safe AD. All algorithms in SafeRL-Kit are
implemented (i) under the off-policy setting, which improves sample efficiency
and can better leverage past logs; (ii) with a unified learning framework,
providing off-the-shelf interfaces for researchers to incorporate their
domain-specific knowledge into fundamental safe RL methods. Conclusively, we
conduct a comparative evaluation of the above algorithms in SafeRL-Kit and shed
light on their efficacy for safe autonomous driving. The source code is
available at \href{ this https URL}{this https URL}.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：A Spatio-Temporal Neural Network Forecasting Approach for Emulation of  Firefront Models</b></summary>
  <p><b>编号</b>：[180]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08523</p>
  <p><b>作者</b>：Andrew Bolt,  Carolyn Huston,  Petra Kuhnert,  Joel Janek Dabrowski,  James Hilton,  Conrad Sanderson</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：wildfire spread typically, spread typically employ, fuel type, typically employ empirical, typically employ</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Computational simulations of wildfire spread typically employ empirical
rate-of-spread calculations under various conditions (such as terrain, fuel
type, weather). Small perturbations in conditions can often lead to significant
changes in fire spread (such as speed and direction), necessitating a
computationally expensive large set of simulations to quantify uncertainty.
Model emulation seeks alternative representations of physical models using
machine learning, aiming to provide more efficient and/or simplified surrogate
models. We propose a dedicated spatio-temporal neural network based framework
for model emulation, able to capture the complex behaviour of fire spread
models. The proposed approach can approximate forecasts at fine spatial and
temporal resolutions that are often challenging for neural network based
approaches. Furthermore, the proposed approach is robust even with small
training sets, due to novel data augmentation methods. Empirical experiments
show good agreement between simulated and emulated firefronts, with an average
Jaccard score of 0.76.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：Thompson Sampling Achieves $\tilde O(\sqrt{T})$ Regret in Linear  Quadratic Control</b></summary>
  <p><b>编号</b>：[183]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08520</p>
  <p><b>作者</b>：Taylan Kargin,  Sahin Lale,  Kamyar Azizzadenesheli,  Anima Anandkumar,  Babak Hassibi</p>
  <p><b>备注</b>：Accepted for presentation at the Conference on Learning Theory (COLT) 2022</p>
  <p><b>关键词</b>：Thompson Sampling, adaptive control, carefully prescribed distribution, decision-making under uncertainty, observed data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Thompson Sampling (TS) is an efficient method for decision-making under
uncertainty, where an action is sampled from a carefully prescribed
distribution which is updated based on the observed data. In this work, we
study the problem of adaptive control of stabilizable linear-quadratic
regulators (LQRs) using TS, where the system dynamics are unknown. Previous
works have established that $\tilde O(\sqrt{T})$ frequentist regret is optimal
for the adaptive control of LQRs. However, the existing methods either work
only in restrictive settings, require a priori known stabilizing controllers,
or utilize computationally intractable approaches. We propose an efficient TS
algorithm for the adaptive control of LQRs, TS-based Adaptive Control, TSAC,
that attains $\tilde O(\sqrt{T})$ regret, even for multidimensional systems,
thereby solving the open problem posed in Abeille and Lazaric (2018). TSAC does
not require a priori known stabilizing controller and achieves fast
stabilization of the underlying system by effectively exploring the environment
in the early stages. Our result hinges on developing a novel lower bound on the
probability that the TS provides an optimistic sample. By carefully prescribing
an early exploration strategy and a policy update rule, we show that TS
achieves order-optimal regret in adaptive control of multidimensional
stabilizable LQRs. We empirically demonstrate the performance and the
efficiency of TSAC in several adaptive control tasks.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：MetaFed: Federated Learning among Federations with Cyclic Knowledge  Distillation for Personalized Healthcare</b></summary>
  <p><b>编号</b>：[185]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08516</p>
  <p><b>作者</b>：Yiqiang Chen,  Wang Lu,  Xin Qin,  Jindong Wang,  Xing Xie</p>
  <p><b>备注</b>：Accepted by IJCAI'22 federated learning workshop; code at this https URL</p>
  <p><b>关键词</b>：attracted increasing attention, raw user data, Federated learning, learning has attracted, attracted increasing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Federated learning has attracted increasing attention to building models
without accessing the raw user data, especially in healthcare. In real
applications, different federations can seldom work together due to possible
reasons such as data heterogeneity and distrust/inexistence of the central
server. In this paper, we propose a novel framework called MetaFed to
facilitate trustworthy FL between different federations. MetaFed obtains a
personalized model for each federation without a central server via the
proposed Cyclic Knowledge Distillation. Specifically, MetaFed treats each
federation as a meta distribution and aggregates knowledge of each federation
in a cyclic manner. The training is split into two parts: common knowledge
accumulation and personalization. Comprehensive experiments on three benchmarks
demonstrate that MetaFed without a server achieves better accuracy compared to
state-of-the-art methods (e.g., 10%+ accuracy improvement compared to the
baseline for PAMAP2) with fewer communication costs.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：ComENet: Towards Complete and Efficient Message Passing for 3D Molecular  Graphs</b></summary>
  <p><b>编号</b>：[186]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08515</p>
  <p><b>作者</b>：Limei Wang,  Yi Liu,  Yuchao Lin,  Haoran Liu,  Shuiwang Ji</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：information completely, learning representations, information, completely and efficiently, real-world data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Many real-world data can be modeled as 3D graphs, but learning
representations that incorporates 3D information completely and efficiently is
challenging. Existing methods either use partial 3D information, or suffer from
excessive computational cost. To incorporate 3D information completely and
efficiently, we propose a novel message passing scheme that operates within
1-hop neighborhood. Our method guarantees full completeness of 3D information
on 3D graphs by achieving global and local completeness. Notably, we propose
the important rotation angles to fulfill global completeness. Additionally, we
show that our method is orders of magnitude faster than prior methods. We
provide rigorous proof of completeness and analysis of time complexity for our
methods. As molecules are in essence quantum systems, we build the
\underline{com}plete and \underline{e}fficient graph neural network (ComENet)
by combing quantum inspired basis functions and the proposed message passing
scheme. Experimental results demonstrate the capability and efficiency of
ComENet, especially on real-world datasets that are large in both numbers and
sizes of graphs. Our code is publicly available as part of the DIG library
(\url{this https URL}).</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：A Unified Evaluation of Textual Backdoor Learning: Frameworks and  Benchmarks</b></summary>
  <p><b>编号</b>：[187]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08514</p>
  <p><b>作者</b>：Ganqu Cui,  Lifan Yuan,  Bingxiang He,  Yangyi Chen,  Zhiyuan Liu,  Maosong Sun</p>
  <p><b>备注</b>：19 pages</p>
  <p><b>关键词</b>：NLP systems, threat to NLP, NLP, backdoor, poisoned</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Textual backdoor attacks are a kind of practical threat to NLP systems. By
injecting a backdoor in the training phase, the adversary could control model
predictions via predefined triggers. As various attack and defense models have
been proposed, it is of great significance to perform rigorous evaluations.
However, we highlight two issues in previous backdoor learning evaluations: (1)
The differences between real-world scenarios (e.g. releasing poisoned datasets
or models) are neglected, and we argue that each scenario has its own
constraints and concerns, thus requires specific evaluation protocols; (2) The
evaluation metrics only consider whether the attacks could flip the models'
predictions on poisoned samples and retain performances on benign samples, but
ignore that poisoned samples should also be stealthy and semantic-preserving.
To address these issues, we categorize existing works into three practical
scenarios in which attackers release datasets, pre-trained models, and
fine-tuned models respectively, then discuss their unique evaluation
methodologies. On metrics, to completely evaluate poisoned samples, we use
grammar error increase and perplexity difference for stealthiness, along with
text similarity for validity. After formalizing the frameworks, we develop an
open-source toolkit OpenBackdoor to foster the implementations and evaluations
of textual backdoor learning. With this toolkit, we perform extensive
experiments to benchmark attack and defense models under the suggested
paradigm. To facilitate the underexplored defenses against poisoned datasets,
we further propose CUBE, a simple yet strong clustering-based defense baseline.
We hope that our frameworks and benchmarks could serve as the cornerstones for
future model development and evaluations.</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：TLETA: Deep Transfer Learning and Integrated Cellular Knowledge for  Estimated Time of Arrival Prediction</b></summary>
  <p><b>编号</b>：[188]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08513</p>
  <p><b>作者</b>：Hieu Tran,  Son Nguyen,  I-Ling Yen,  Farokh Bastani</p>
  <p><b>备注</b>：8 pages, 3 figures, 3 tables. The 25th IEEE International Conference on Intelligent Transportation Systems (IEEE ITSC 2022)</p>
  <p><b>关键词</b>：studied widely, ETA, Vehicle arrival time, arrival time prediction, special vehicles</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vehicle arrival time prediction has been studied widely. With the emergence
of IoT devices and deep learning techniques, estimated time of arrival (ETA)
has become a critical component in intelligent transportation systems. Though
many tools exist for ETA, ETA for special vehicles, such as ambulances, fire
engines, etc., is still challenging due to the limited amount of traffic data
for special vehicles. Existing works use one model for all types of vehicles,
which can lead to low accuracy. To tackle this, as the first in the field, we
propose a deep transfer learning framework TLETA for the driving time
prediction. TLETA constructs cellular spatial-temporal knowledge grids for
extracting driving patterns, combined with the road network structure embedding
to build a deep neural network for ETA. TLETA contains transferable layers to
support knowledge transfer between different categories of vehicles.
Importantly, our transfer models only train the last layers to map the
transferred knowledge, that reduces the training time significantly. The
experimental studies show that our model predicts travel time with high
accuracy and outperforms many state-of-the-art approaches.</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：What do navigation agents learn about their environment?</b></summary>
  <p><b>编号</b>：[192]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08500</p>
  <p><b>作者</b>：Kshitij Dwivedi,  Gemma Roig,  Aniruddha Kembhavi,  Roozbeh Mottaghi</p>
  <p><b>备注</b>：CVPR 2022</p>
  <p><b>关键词</b>：models trained end, agents typically consist, deep learning models, art visual navigation, learning models trained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Today's state of the art visual navigation agents typically consist of large
deep learning models trained end to end. Such models offer little to no
interpretability about the learned skills or the actions of the agent taken in
response to its environment. While past works have explored interpreting deep
learning models, little attention has been devoted to interpreting embodied AI
systems, which often involve reasoning about the structure of the environment,
target characteristics and the outcome of one's actions. In this paper, we
introduce the Interpretability System for Embodied agEnts (iSEE) for Point Goal
and Object Goal navigation agents. We use iSEE to probe the dynamic
representations produced by these agents for the presence of information about
the agent as well as the environment. We demonstrate interesting insights about
navigation agents using iSEE, including the ability to encode reachable
locations (to avoid obstacles), visibility of the target, progress from the
initial spawn location as well as the dramatic effect on the behaviors of
agents when we mask out critical individual neurons. The code is available at:
this https URL</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：A Parametric Class of Approximate Gradient Updates for Policy  Optimization</b></summary>
  <p><b>编号</b>：[193]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08499</p>
  <p><b>作者</b>：Ramki Gummadi,  Saurabh Kumar,  Junfeng Wen,  Dale Schuurmans</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：maximizing expected return, versus policy representation, diverse principles, model is interpreted, objective is formulated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Approaches to policy optimization have been motivated from diverse
principles, based on how the parametric model is interpreted (e.g. value versus
policy representation) or how the learning objective is formulated, yet they
share a common goal of maximizing expected return. To better capture the
commonalities and identify key differences between policy optimization methods,
we develop a unified perspective that re-expresses the underlying updates in
terms of a limited choice of gradient form and scaling function. In particular,
we identify a parameterized space of approximate gradient updates for policy
optimization that is highly structured, yet covers both classical and recent
examples, including PPO. As a result, we obtain novel yet well motivated
updates that generalize existing algorithms in a way that can deliver benefits
both in terms of convergence speed and final result quality. An experimental
investigation demonstrates that the additional degrees of freedom provided in
the parameterized family of updates can be leveraged to obtain non-trivial
improvements both in synthetic domains and on popular deep RL benchmarks.</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：Self-Supervised Contrastive Pre-Training For Time Series via  Time-Frequency Consistency</b></summary>
  <p><b>编号</b>：[195]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08496</p>
  <p><b>作者</b>：Xiang Zhang,  Ziyuan Zhao,  Theodoros Tsiligkaridis,  Marinka Zitnik</p>
  <p><b>备注</b>：Under review; the anonymouse code repo link will be made non-anonymous after acceptance; 21 pages (13 pages main paper + 8 pages supplementary materials)</p>
  <p><b>关键词</b>：short cyclic effects, poor downstream performance, unique challenge due, fast-evolving trends, cyclic effects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pre-training on time series poses a unique challenge due to the potential
mismatch between pre-training and target domains, such as shifts in temporal
dynamics, fast-evolving trends, and long-range and short cyclic effects, which
can lead to poor downstream performance. While domain adaptation methods can
mitigate these shifts, most methods need examples directly from the target
domain, making them suboptimal for pre-training. To address this challenge,
methods need to accommodate target domains with different temporal dynamics and
be capable of doing so without seeing any target examples during pre-training.
Relative to other modalities, in time series, we expect that time-based and
frequency-based representations of the same example are located close together
in the time-frequency space. To this end, we posit that time-frequency
consistency (TF-C) -- embedding a time-based neighborhood of a particular
example close to its frequency-based neighborhood and back -- is desirable for
pre-training. Motivated by TF-C, we define a decomposable pre-training model,
where the self-supervised signal is provided by the distance between time and
frequency components, each individually trained by contrastive estimation. We
evaluate the new method on eight datasets, including electrodiagnostic testing,
human activity recognition, mechanical fault detection, and physical status
monitoring. Experiments against eight state-of-the-art methods show that TF-C
outperforms baselines by 15.4% (F1 score) on average in one-to-one settings
(e.g., fine-tuning an EEG-pretrained model on EMG data) and by up to 8.4% (F1
score) in challenging one-to-many settings, reflecting the breadth of scenarios
that arise in real-world applications. The source code and datasets are
available at https: //anonymous.4open.science/r/TFC-pretraining-6B07.</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：TKIL: Tangent Kernel Approach for Class Balanced Incremental Learning</b></summary>
  <p><b>编号</b>：[199]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08492</p>
  <p><b>作者</b>：Jinlin Xiang,  Eli Shlizerman</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：called catastrophic forgetting, phenomenon called catastrophic, previously learned tasks, incremental learning, Tangent Kernel</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>When learning new tasks in a sequential manner, deep neural networks tend to
forget tasks that they previously learned, a phenomenon called catastrophic
forgetting. Class incremental learning methods aim to address this problem by
keeping a memory of a few exemplars from previously learned tasks, and
distilling knowledge from them. However, existing methods struggle to balance
the performance across classes since they typically overfit the model to the
latest task. In our work, we propose to address these challenges with the
introduction of a novel methodology of Tangent Kernel for Incremental Learning
(TKIL) that achieves class-balanced performance. The approach preserves the
representations across classes and balances the accuracy for each class, and as
such achieves better overall accuracy and variance. TKIL approach is based on
Neural Tangent Kernel (NTK), which describes the convergence behavior of neural
networks as a kernel function in the limit of infinite width. In TKIL, the
gradients between feature layers are treated as the distance between the
representations of these layers and can be defined as Gradients Tangent Kernel
loss (GTK loss) such that it is minimized along with averaging weights. This
allows TKIL to automatically identify the task and to quickly adapt to it
during inference. Experiments on CIFAR-100 and ImageNet datasets with various
incremental learning settings show that these strategies allow TKIL to
outperform existing state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：Revisiting Self-Distillation</b></summary>
  <p><b>编号</b>：[200]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08491</p>
  <p><b>作者</b>：Minh Pham,  Minsu Cho,  Ameya Joshi,  Chinmay Hegde</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：model compression, large model, Knowledge, Knowledge distillation, teacher</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Knowledge distillation is the procedure of transferring "knowledge" from a
large model (the teacher) to a more compact one (the student), often being used
in the context of model compression. When both models have the same
architecture, this procedure is called self-distillation. Several works have
anecdotally shown that a self-distilled student can outperform the teacher on
held-out data. In this work, we systematically study self-distillation in a
number of settings. We first show that even with a highly accurate teacher,
self-distillation allows a student to surpass the teacher in all cases.
Secondly, we revisit existing theoretical explanations of (self) distillation
and identify contradicting examples, revealing possible drawbacks of these
explanations. Finally, we provide an alternative explanation for the dynamics
of self-distillation through the lens of loss landscape geometry. We conduct
extensive experiments to show that self-distillation leads to flatter minima,
thereby resulting in better generalization.</p>
  </details>
</details>
<details>
  <summary>83. <b>标题：Debugging using Orthogonal Gradient Descent</b></summary>
  <p><b>编号</b>：[201]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08489</p>
  <p><b>作者</b>：Narsimha Chilkuri,  Chris Eliasmith</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：partially faulty, Orthogonal Gradient Descent, trained model, continual learning problem, model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this report we consider the following problem: Given a trained model that
is partially faulty, can we correct its behaviour without having to train the
model from scratch? In other words, can we ``debug" neural networks similar to
how we address bugs in our mathematical models and standard computer code. We
base our approach on the hypothesis that debugging can be treated as a two-task
continual learning problem. In particular, we employ a modified version of a
continual learning algorithm called Orthogonal Gradient Descent (OGD) to
demonstrate, via two simple experiments on the MNIST dataset, that we can
in-fact \textit{unlearn} the undesirable behaviour while retaining the general
performance of the model, and we can additionally \textit{relearn} the
appropriate behaviour, both without having to train the model from scratch.</p>
  </details>
</details>
<details>
  <summary>84. <b>标题：High-Speed Accurate Robot Control using Learned Forward Kinodynamics and  Non-linear Least Squares Optimization</b></summary>
  <p><b>编号</b>：[203]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08487</p>
  <p><b>作者</b>：Pranav Atreya,  Haresh Karnan,  Kavan Singh Sikand,  Xuesu Xiao,  Garrett Warnell,  Sadegh Rabiee,  Peter Stone,  Joydeep Biswas</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：real world requires, high-speed robot control, control, robot control, robot</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Accurate control of robots in the real world requires a control system that
is capable of taking into account the kinodynamic interactions of the robot
with its environment. At high speeds, the dependence of the movement of the
robot on these kinodynamic interactions becomes more pronounced, making
high-speed, accurate robot control a challenging problem. Previous work has
shown that learning the inverse kinodynamics (IKD) of the robot can be helpful
for high-speed robot control. However a learned inverse kinodynamic model can
only be applied to a limited class of control problems, and different control
problems require the learning of a new IKD model. In this work we present a new
formulation for accurate, high-speed robot control that makes use of a learned
forward kinodynamic (FKD) model and non-linear least squares optimization. By
nature of the formulation, this approach is extensible to a wide array of
control problems without requiring the retraining of a new model. We
demonstrate the ability of this approach to accurately control a scale
one-tenth robot car at high speeds, and show improved results over baselines.</p>
  </details>
</details>
<details>
  <summary>85. <b>标题：Classification of datasets with imputed missing values: does imputation  quality matter?</b></summary>
  <p><b>编号</b>：[208]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08478</p>
  <p><b>作者</b>：Tolou Shadbahr,  Michael Roberts,  Jan Stanczuk,  Julian Gilbey,  Philip Teare,  Sören Dittmer,  Matthew Thorpe,  Ramon Vinas Torne,  Evis Sala,  Pietro Lio,  Mishal Patel,  AIX-COVNET Collaboration,  James H.F. Rudd,  Tuomas Mirtti,  Antti Rannikko,  John A.D. Aston,  Jing Tang,  Carola-Bibiane Schönlieb</p>
  <p><b>备注</b>：17 pages, 10 figures, 30 supplementary pages</p>
  <p><b>关键词</b>：machine learning practitioners, common aim, Classifying samples, incomplete datasets, learning practitioners</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Classifying samples in incomplete datasets is a common aim for machine
learning practitioners, but is non-trivial. Missing data is found in most
real-world datasets and these missing values are typically imputed using
established methods, followed by classification of the now complete, imputed,
samples. The focus of the machine learning researcher is then to optimise the
downstream classification performance. In this study, we highlight that it is
imperative to consider the quality of the imputation. We demonstrate how the
commonly used measures for assessing quality are flawed and propose a new class
of discrepancy scores which focus on how well the method recreates the overall
distribution of the data. To conclude, we highlight the compromised
interpretability of classifier models trained using poorly imputed data.</p>
  </details>
</details>
<details>
  <summary>86. <b>标题：Backdoor Attacks on Vision Transformers</b></summary>
  <p><b>编号</b>：[209]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08477</p>
  <p><b>作者</b>：Akshayvarun Subramanya,  Aniruddha Saha,  Soroush Abbasi Koohpayegani,  Ajinkya Tejankar,  Hamed Pirsiavash</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：recently demonstrated exemplary, Vision Transformers, demonstrated exemplary performance, vision tasks, recently demonstrated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision Transformers (ViT) have recently demonstrated exemplary performance on
a variety of vision tasks and are being used as an alternative to CNNs. Their
design is based on a self-attention mechanism that processes images as a
sequence of patches, which is quite different compared to CNNs. Hence it is
interesting to study if ViTs are vulnerable to backdoor attacks. Backdoor
attacks happen when an attacker poisons a small part of the training data for
malicious purposes. The model performance is good on clean test images, but the
attacker can manipulate the decision of the model by showing the trigger at
test time. To the best of our knowledge, we are the first to show that ViTs are
vulnerable to backdoor attacks. We also find an intriguing difference between
ViTs and CNNs - interpretation algorithms effectively highlight the trigger on
test images for ViTs but not for CNNs. Based on this observation, we propose a
test-time image blocking defense for ViTs which reduces the attack success rate
by a large margin. Code is available here:
this https URL</p>
  </details>
</details>
<details>
  <summary>87. <b>标题：Zero-Shot AutoML with Pretrained Models</b></summary>
  <p><b>编号</b>：[210]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08476</p>
  <p><b>作者</b>：Ekrem Öztürk,  Fabio Ferreira,  Hadi S. Jomaa,  Lars Schmidt-Thieme,  Josif Grabocka,  Frank Hutter</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：low compute budget, compute budget, risking overfitting, low compute, fine-tuning hyperparameters</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Given a new dataset D and a low compute budget, how should we choose a
pre-trained model to fine-tune to D, and set the fine-tuning hyperparameters
without risking overfitting, particularly if D is small? Here, we extend
automated machine learning (AutoML) to best make these choices. Our
domain-independent meta-learning approach learns a zero-shot surrogate model
which, at test time, allows to select the right deep learning (DL) pipeline
(including the pre-trained model and fine-tuning hyperparameters) for a new
dataset D given only trivial meta-features describing D such as image
resolution or the number of classes. To train this zero-shot model, we collect
performance data for many DL pipelines on a large collection of datasets and
meta-train on this data to minimize a pairwise ranking objective. We evaluate
our approach under the strict time limit of the vision track of the ChaLearn
AutoDL challenge benchmark, clearly outperforming all challenge contenders.</p>
  </details>
</details>
<details>
  <summary>88. <b>标题：XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence</b></summary>
  <p><b>编号</b>：[211]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08474</p>
  <p><b>作者</b>：Ming Zhu,  Aneesh Jain,  Karthik Suresh,  Roshan Ravindran,  Sindhu Tipirneni,  Chandan K. Reddy</p>
  <p><b>备注</b>：20 pages, 11 tables, 2 figures</p>
  <p><b>关键词</b>：achieved good performance, Recent advances, code, advances in machine, machine learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advances in machine learning have significantly improved the
understanding of source code data and achieved good performance on a number of
downstream tasks. Open source repositories like GitHub enable this process with
rich unlabeled code data. However, the lack of high quality labeled data has
largely hindered the progress of several code related tasks, such as program
translation, summarization, synthesis, and code search. This paper introduces
XLCoST, Cross-Lingual Code SnippeT dataset, a new benchmark dataset for
cross-lingual code intelligence. Our dataset contains fine-grained parallel
data from 8 languages (7 commonly used programming languages and English), and
supports 10 cross-lingual code tasks. To the best of our knowledge, it is the
largest parallel dataset for source code both in terms of size and the number
of languages. We also provide the performance of several state-of-the-art
baseline models for each task. We believe this new dataset can be a valuable
asset for the research community and facilitate the development and validation
of new methods for cross-lingual code intelligence.</p>
  </details>
</details>
<details>
  <summary>89. <b>标题：A Robust Stacking Framework for Training Deep Graph Models with  Multifaceted Node Features</b></summary>
  <p><b>编号</b>：[212]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08473</p>
  <p><b>作者</b>：Jiuhai Chen,  Jonas Mueller,  Vassilis N. Ioannidis,  Tom Goldstein,  David Wipf</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：numerical node features, node features, supervised learning tasks, numerical node, structure as inputs</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Graph Neural Networks (GNNs) with numerical node features and graph structure
as inputs have demonstrated superior performance on various supervised learning
tasks with graph data. However the numerical node features utilized by GNNs are
commonly extracted from raw data which is of text or tabular
(numeric/categorical) type in most real-world applications. The best models for
such data types in most standard supervised learning settings with IID
(non-graph) data are not simple neural network layers and thus are not easily
incorporated into a GNN. Here we propose a robust stacking framework that fuses
graph-aware propagation with arbitrary models intended for IID data, which are
ensembled and stacked in multiple layers. Our layer-wise framework leverages
bagging and stacking strategies to enjoy strong generalization, in a manner
which effectively mitigates label leakage and overfitting. Across a variety of
graph datasets with tabular/text node features, our method achieves comparable
or superior performance relative to both tabular/text and graph neural network
models, as well as existing state-of-the-art hybrid strategies that combine the
two.</p>
  </details>
</details>
<details>
  <summary>90. <b>标题：PRANC: Pseudo RAndom Networks for Compacting deep models</b></summary>
  <p><b>编号</b>：[215]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08464</p>
  <p><b>作者</b>：Parsa Nooralinejad,  Ali Abbasi,  Soheil Kolouri,  Hamed Pirsiavash</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Machine Learning settings, distributed Machine Learning, Machine Learning, Learning settings, distributed Machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Communication becomes a bottleneck in various distributed Machine Learning
settings. Here, we propose a novel training framework that leads to highly
efficient communication of models between agents. In short, we train our
network to be a linear combination of many pseudo-randomly generated frozen
models. For communication, the source agent transmits only the `seed' scalar
used to generate the pseudo-random `basis' networks along with the learned
linear mixture coefficients. Our method, denoted as PRANC, learns almost
$100\times$ fewer parameters than a deep model and still performs well on
several datasets and architectures. PRANC enables 1) efficient communication of
models between agents, 2) efficient model storage, and 3) accelerated inference
by generating layer-wise weights on the fly. We test PRANC on CIFAR-10,
CIFAR-100, tinyImageNet, and ImageNet-100 with various architectures like
AlexNet, LeNet, ResNet18, ResNet20, and ResNet56 and demonstrate a massive
reduction in the number of parameters while providing satisfactory performance
on these benchmark datasets. The code is available
\href{this https URL}{this https URL}</p>
  </details>
</details>
<details>
  <summary>91. <b>标题：Recursive Neural Programs: Variational Learning of Image Grammars and  Part-Whole Hierarchies</b></summary>
  <p><b>编号</b>：[216]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08462</p>
  <p><b>作者</b>：Ares Fisher,  Rajesh P.N. Rao</p>
  <p><b>备注</b>：Code repository up soon :)</p>
  <p><b>关键词</b>：Human vision involves, structured representations based, vision involves parsing, representations based, Human vision</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Human vision involves parsing and representing objects and scenes using
structured representations based on part-whole hierarchies. Computer vision and
machine learning researchers have recently sought to emulate this capability
using capsule networks, reference frames and active predictive coding, but a
generative model formulation has been lacking. We introduce Recursive Neural
Programs (RNPs), which, to our knowledge, is the first neural generative model
to address the part-whole hierarchy learning problem. RNPs model images as
hierarchical trees of probabilistic sensory-motor programs that recursively
reuse learned sensory-motor primitives to model an image within different
reference frames, forming recursive image grammars. We express RNPs as
structured variational autoencoders (sVAEs) for inference and sampling, and
demonstrate parts-based parsing, sampling and one-shot transfer learning for
MNIST, Omniglot and Fashion-MNIST datasets, demonstrating the model's
expressive power. Our results show that RNPs provide an intuitive and
explainable way of composing objects and scenes, allowing rich compositionality
and intuitive interpretations of objects in terms of part-whole hierarchies.</p>
  </details>
</details>
<details>
  <summary>92. <b>标题：TUSK: Task-Agnostic Unsupervised Keypoints</b></summary>
  <p><b>编号</b>：[217]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08460</p>
  <p><b>作者</b>：Yuhe Jin,  Weiwei Sun,  Jan Hosang,  Eduard Trulls,  Kwang Moo Yi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：abstract geometric shape, learning rely heavily, abstract geometric, geometric shape, rely heavily</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing unsupervised methods for keypoint learning rely heavily on the
assumption that a specific keypoint type (e.g. elbow, digit, abstract geometric
shape) appears only once in an image. This greatly limits their applicability,
as each instance must be isolated before applying the method-an issue that is
never discussed or evaluated. We thus propose a novel method to learn
Task-agnostic, UnSupervised Keypoints (TUSK) which can deal with multiple
instances. To achieve this, instead of the commonly-used strategy of detecting
multiple heatmaps, each dedicated to a specific keypoint type, we use a single
heatmap for detection, and enable unsupervised learning of keypoint types
through clustering. Specifically, we encode semantics into the keypoints by
teaching them to reconstruct images from a sparse set of keypoints and their
descriptors, where the descriptors are forced to form distinct clusters in
feature space around learned prototypes. This makes our approach amenable to a
wider range of tasks than any previous unsupervised keypoint method: we show
experiments on multiple-instance detection and classification, object
discovery, and landmark detection-all unsupervised-with performance on par with
the state of the art, while also being able to deal with multiple instances.</p>
  </details>
</details>
<details>
  <summary>93. <b>标题：Local overlap reduction procedure for dynamic ensemble selection</b></summary>
  <p><b>编号</b>：[218]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08455</p>
  <p><b>作者</b>：Mariana A. Souza,  Robert Sabourin,  George D. C. Cavalcanti,  Rafael M. O. Cruz</p>
  <p><b>备注</b>：Paper accepted to the 2022 International Joint Conference on Neural Networks</p>
  <p><b>关键词</b>：local class overlap, challenging for classification, classification models, end up biased, Class</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Class imbalance is a characteristic known for making learning more
challenging for classification models as they may end up biased towards the
majority class. A promising approach among the ensemble-based methods in the
context of imbalance learning is Dynamic Selection (DS). DS techniques single
out a subset of the classifiers in the ensemble to label each given unknown
sample according to their estimated competence in the area surrounding the
query. Because only a small region is taken into account in the selection
scheme, the global class disproportion may have less impact over the system's
performance. However, the presence of local class overlap may severely hinder
the DS techniques' performance over imbalanced distributions as it not only
exacerbates the effects of the under-representation but also introduces
ambiguous and possibly unreliable samples to the competence estimation process.
Thus, in this work, we propose a DS technique which attempts to minimize the
effects of the local class overlap during the classifier selection procedure.
The proposed method iteratively removes from the target region the instance
perceived as the hardest to classify until a classifier is deemed competent to
label the query sample. The known samples are characterized using instance
hardness measures that quantify the local class overlap. Experimental results
show that the proposed technique can significantly outperform the baseline as
well as several other DS techniques, suggesting its suitability for dealing
with class under-representation and overlap. Furthermore, the proposed
technique still yielded competitive results when using an under-sampled, less
overlapped version of the labelled sets, specially over the problems with a
high proportion of minority class samples in overlap areas. Code available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>94. <b>标题：Quantifying Feature Contributions to Overall Disparity Using Information  Theory</b></summary>
  <p><b>编号</b>：[219]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08454</p>
  <p><b>作者</b>：Sanghamitra Dutta,  Praveen Venkatesh,  Pulkit Grover</p>
  <p><b>备注</b>：Presented at the AAAI-22 Workshop on Information-Theoretic Methods for Causal Inference and Discovery in March 2022</p>
  <p><b>关键词</b>：machine-learning algorithm makes, algorithm makes biased, makes biased decisions, individual feature, bias exists</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>When a machine-learning algorithm makes biased decisions, it can be helpful
to understand the sources of disparity to explain why the bias exists. Towards
this, we examine the problem of quantifying the contribution of each individual
feature to the observed disparity. If we have access to the decision-making
model, one potential approach (inspired from intervention-based approaches in
explainability literature) is to vary each individual feature (while keeping
the others fixed) and use the resulting change in disparity to quantify its
contribution. However, we may not have access to the model or be able to
test/audit its outputs for individually varying features. Furthermore, the
decision may not always be a deterministic function of the input features
(e.g., with human-in-the-loop). For these situations, we might need to explain
contributions using purely distributional (i.e., observational) techniques,
rather than interventional. We ask the question: what is the "potential"
contribution of each individual feature to the observed disparity in the
decisions when the exact decision-making mechanism is not accessible? We first
provide canonical examples (thought experiments) that help illustrate the
difference between distributional and interventional approaches to explaining
contributions, and when either is better suited. When unable to intervene on
the inputs, we quantify the "redundant" statistical dependency about the
protected attribute that is present in both the final decision and an
individual feature, by leveraging a body of work in information theory called
Partial Information Decomposition. We also perform a simple case study to show
how this technique could be applied to quantify contributions.</p>
  </details>
</details>
<details>
  <summary>95. <b>标题：GOOD: A Graph Out-of-Distribution Benchmark</b></summary>
  <p><b>编号</b>：[220]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08452</p>
  <p><b>作者</b>：Shurui Gui,  Xiner Li,  Limei Wang,  Shuiwang Ji</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：test data follow, OOD, follow different distributions, deals with scenarios, training and test</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Out-of-distribution (OOD) learning deals with scenarios in which training and
test data follow different distributions. Although general OOD problems have
been intensively studied in machine learning, graph OOD is only an emerging
area of research. Currently, there lacks a systematic benchmark tailored to
graph OOD method evaluation. In this work, we aim at developing an OOD
benchmark, known as GOOD, for graphs specifically. We explicitly make
distinctions between covariate and concept shifts and design data splits that
accurately reflect different shifts. We consider both graph and node prediction
tasks as there are key differences when designing shifts. Overall, GOOD
contains 8 datasets with 14 domain selections. When combined with covariate,
concept, and no shifts, we obtain 42 different splits. We provide performance
results on 7 commonly used baseline methods with 10 random runs. This results
in 294 dataset-model combinations in total. Our results show significant
performance gaps between in-distribution and OOD settings. Our results also
shed light on different performance trends between covariate and concept shifts
by different methods. Our GOOD benchmark is a growing project and expects to
expand in both quantity and variety of resources as the area develops. The GOOD
benchmark can be accessed via
$\href{this https URL}{\text{this https URL}}$.</p>
  </details>
</details>
<details>
  <summary>96. <b>标题：I Know What You Trained Last Summer: A Survey on Stealing Machine  Learning Models and Defences</b></summary>
  <p><b>编号</b>：[221]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08451</p>
  <p><b>作者</b>：Daryna Oliynyk,  Rudolf Mayer,  Andreas Rauber</p>
  <p><b>备注</b>：Under review at ACM Computing Surveys</p>
  <p><b>关键词</b>：complex machine learning, machine learning models, complex machine, machine learning, widespread paradigm</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine Learning-as-a-Service (MLaaS) has become a widespread paradigm,
making even the most complex machine learning models available for clients via
e.g. a pay-per-query principle. This allows users to avoid time-consuming
processes of data collection, hyperparameter tuning, and model training.
However, by giving their customers access to the (predictions of their) models,
MLaaS providers endanger their intellectual property, such as sensitive
training data, optimised hyperparameters, or learned model parameters.
Adversaries can create a copy of the model with (almost) identical behavior
using the the prediction labels only. While many variants of this attack have
been described, only scattered defence strategies have been proposed,
addressing isolated threats. This raises the necessity for a thorough
systematisation of the field of model stealing, to arrive at a comprehensive
understanding why these attacks are successful, and how they could be
holistically defended against. We address this by categorising and comparing
model stealing attacks, assessing their performance, and exploring
corresponding defence techniques in different settings. We propose a taxonomy
for attack and defence approaches, and provide guidelines on how to select the
right attack or defence strategy based on the goal and available resources.
Finally, we analyse which defences are rendered less effective by current
attack strategies.</p>
  </details>
</details>
<details>
  <summary>97. <b>标题：Active Fairness Auditing</b></summary>
  <p><b>编号</b>：[222]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08450</p>
  <p><b>作者</b>：Tom Yan,  Chicheng Zhang</p>
  <p><b>备注</b>：34 pages; 2 figures; ICML 2022</p>
  <p><b>关键词</b>：fast spreading adoption, industries poses significant, poses significant regulatory, significant regulatory challenges, machine learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The fast spreading adoption of machine learning (ML) by companies across
industries poses significant regulatory challenges. One such challenge is
scalability: how can regulatory bodies efficiently audit these ML models,
ensuring that they are fair? In this paper, we initiate the study of
query-based auditing algorithms that can estimate the demographic parity of ML
models in a query-efficient manner. We propose an optimal deterministic
algorithm, as well as a practical randomized, oracle-efficient algorithm with
comparable guarantees. Furthermore, we make inroads into understanding the
optimal query complexity of randomized active fairness estimation algorithms.
Our first exploration of active fairness estimation aims to put AI governance
on firmer theoretical foundations.</p>
  </details>
</details>
<details>
  <summary>98. <b>标题：Empirical Bayesian Approaches for Robust Constraint-based Causal  Discovery under Insufficient Data</b></summary>
  <p><b>编号</b>：[223]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08448</p>
  <p><b>作者</b>：Zijun Cui,  Naiyu Yin,  Yuru Wang,  Qiang Ji</p>
  <p><b>备注</b>：Accepted to IJCAI 2022</p>
  <p><b>关键词</b>：learn cause-effect relationships, causal discovery methods, Causal discovery, Existing causal discovery, constraint-based causal discovery</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Causal discovery is to learn cause-effect relationships among variables given
observational data and is important for many applications. Existing causal
discovery methods assume data sufficiency, which may not be the case in many
real world datasets. As a result, many existing causal discovery methods can
fail under limited data. In this work, we propose Bayesian-augmented
frequentist independence tests to improve the performance of constraint-based
causal discovery methods under insufficient data: 1) We firstly introduce a
Bayesian method to estimate mutual information (MI), based on which we propose
a robust MI based independence test; 2) Secondly, we consider the Bayesian
estimation of hypothesis likelihood and incorporate it into a well-defined
statistical test, resulting in a robust statistical testing based independence
test. We apply proposed independence tests to constraint-based causal discovery
methods and evaluate the performance on benchmark datasets with insufficient
samples. Experiments show significant performance improvement in terms of both
accuracy and efficiency over SOTA methods.</p>
  </details>
</details>
<details>
  <summary>99. <b>标题：Understanding Decision-Time vs. Background Planning in Model-Based  Reinforcement Learning</b></summary>
  <p><b>编号</b>：[226]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08442</p>
  <p><b>作者</b>：Safa Alver,  Doina Precup</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：model-based reinforcement learning, planning, model-based reinforcement, agent can leverage, leverage a learned</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In model-based reinforcement learning, an agent can leverage a learned model
to improve its way of behaving in different ways. Two prevalent approaches are
decision-time planning and background planning. In this study, we are
interested in understanding under what conditions and in which settings one of
these two planning styles will perform better than the other in domains that
require fast responses. After viewing them through the lens of dynamic
programming, we first consider the classical instantiations of these planning
styles and provide theoretical results and hypotheses on which one will perform
better in the pure planning, planning & learning, and transfer learning
settings. We then consider the modern instantiations of these planning styles
and provide hypotheses on which one will perform better in the last two of the
considered settings. Lastly, we perform several illustrative experiments to
empirically validate both our theoretical results and hypotheses. Overall, our
findings suggest that even though decision-time planning does not perform as
well as background planning in their classical instantiations, in their modern
instantiations, it can perform on par or better than background planning in
both the planning & learning and transfer learning settings.</p>
  </details>
</details>
<details>
  <summary>100. <b>标题：SATBench: Benchmarking the speed-accuracy tradeoff in object recognition  by humans and dynamic neural networks</b></summary>
  <p><b>编号</b>：[233]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08427</p>
  <p><b>作者</b>：Ajay Subramanian,  Sara Price,  Omkar Kumbhar,  Elena Sizikova,  Najib J. Majaj,  Denis G. Pelli</p>
  <p><b>备注</b>：19 pages, 12 figures. Under Review at NeurIPS Datasets and Benchmarks Track 2022</p>
  <p><b>关键词</b>：core of everyday, reading and driving, driving is active, active object recognition, neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The core of everyday tasks like reading and driving is active object
recognition. Attempts to model such tasks are currently stymied by the
inability to incorporate time. People show a flexible tradeoff between speed
and accuracy and this tradeoff is a crucial human skill. Deep neural networks
have emerged as promising candidates for predicting peak human object
recognition performance and neural activity. However, modeling the temporal
dimension i.e., the speed-accuracy tradeoff (SAT), is essential for them to
serve as useful computational models for how humans recognize objects. To this
end, we here present the first large-scale (148 observers, 4 neural networks, 8
tasks) dataset of the speed-accuracy tradeoff (SAT) in recognizing ImageNet
images. In each human trial, a beep, indicating the desired reaction time,
sounds at a fixed delay after the image is presented, and observer's response
counts only if it occurs near the time of the beep. In a series of blocks, we
test many beep latencies, i.e., reaction times. We observe that human accuracy
increases with reaction time and proceed to compare its characteristics with
the behavior of several dynamic neural networks that are capable of
inference-time adaptive computation. Using FLOPs as an analog for reaction
time, we compare networks with humans on curve-fit error, category-wise
correlation, and curve steepness, and conclude that cascaded dynamic neural
networks are a promising model of human reaction time in object recognition
tasks.</p>
  </details>
</details>
<details>
  <summary>101. <b>标题：Learning to Teach Fairness-aware Deep Multi-task Learning</b></summary>
  <p><b>编号</b>：[246]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08403</p>
  <p><b>作者</b>：Arjun Roy,  Eirini Ntoutsi</p>
  <p><b>备注</b>：Accepted to be published in the Proceedings of the "European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECMLPKDD)", Sept. 19th to 23rd 2022</p>
  <p><b>关键词</b>：single task learning, focuses on single, Fairness-aware learning, STL, MTL</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Fairness-aware learning mainly focuses on single task learning (STL). The
fairness implications of multi-task learning (MTL) have only recently been
considered and a seminal approach has been proposed that considers the
fairness-accuracy trade-off for each task and the performance trade-off among
different tasks. Instead of a rigid fairness-accuracy trade-off formulation, we
propose a flexible approach that learns how to be fair in a MTL setting by
selecting which objective (accuracy or fairness) to optimize at each step. We
introduce the L2T-FMT algorithm that is a teacher-student network trained
collaboratively; the student learns to solve the fair MTL problem while the
teacher instructs the student to learn from either accuracy or fairness,
depending on what is harder to learn for each task. Moreover, this dynamic
selection of which objective to use at each step for each task reduces the
number of trade-off weights from 2T to T, where T is the number of tasks. Our
experiments on three real datasets show that L2T-FMT improves on both fairness
(12-19%) and accuracy (up to 2%) over state-of-the-art approaches.</p>
  </details>
</details>
<details>
  <summary>102. <b>标题：Powershap: A Power-full Shapley Feature Selection Method</b></summary>
  <p><b>编号</b>：[248]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08394</p>
  <p><b>作者</b>：Jarne Verhaeghe,  Jeroen Van Der Donckt,  Femke Ongenae,  Sofie Van Hoecke</p>
  <p><b>备注</b>：Accepted at ECML PKDD 2022</p>
  <p><b>关键词</b>：powerful machine learning, machine learning models, Feature selection, crucial step, step in developing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Feature selection is a crucial step in developing robust and powerful machine
learning models. Feature selection techniques can be divided into two
categories: filter and wrapper methods. While wrapper methods commonly result
in strong predictive performances, they suffer from a large computational
complexity and therefore take a significant amount of time to complete,
especially when dealing with high-dimensional feature sets. Alternatively,
filter methods are considerably faster, but suffer from several other
disadvantages, such as (i) requiring a threshold value, (ii) not taking into
account intercorrelation between features, and (iii) ignoring feature
interactions with the model. To this end, we present powershap, a novel wrapper
feature selection method, which leverages statistical hypothesis testing and
power calculations in combination with Shapley values for quick and intuitive
feature selection. Powershap is built on the core assumption that an
informative feature will have a larger impact on the prediction compared to a
known random feature. Benchmarks and simulations show that powershap
outperforms other filter methods with predictive performances on par with
wrapper methods while being significantly faster, often even reaching half or a
third of the execution time. As such, powershap provides a competitive and
quick algorithm that can be used by various models in different domains.
Furthermore, powershap is implemented as a plug-and-play and open-source
sklearn component, enabling easy integration in conventional data science
pipelines. User experience is even further enhanced by also providing an
automatic mode that automatically tunes the hyper-parameters of the powershap
algorithm, allowing to use the algorithm without any configuration needed.</p>
  </details>
</details>
<details>
  <summary>103. <b>标题：Embarrassingly Parallel Independent Training of Multi-Layer Perceptrons  with Heterogeneous Architectures</b></summary>
  <p><b>编号</b>：[252]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08369</p>
  <p><b>作者</b>：Felipe Costa Farias,  Teresa Bernarda Ludermir,  Carmelo Jose Albanez Bastos-Filho</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Neural Network architecture, Perceptron Neural Networks, tasks to perform, critical and challenging, challenging tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The definition of a Neural Network architecture is one of the most critical
and challenging tasks to perform. In this paper, we propose ParallelMLPs.
ParallelMLPs is a procedure to enable the training of several independent
Multilayer Perceptron Neural Networks with a different number of neurons and
activation functions in parallel by exploring the principle of locality and
parallelization capabilities of modern CPUs and GPUs. The core idea of this
technique is to use a Modified Matrix Multiplication that replaces an ordinal
matrix multiplication by two simple matrix operations that allow separate and
independent paths for gradient flowing, which can be used in other scenarios.
We have assessed our algorithm in simulated datasets varying the number of
samples, features and batches using 10,000 different models. We achieved a
training speedup from 1 to 4 orders of magnitude if compared to the sequential
approach.</p>
  </details>
</details>
<details>
  <summary>104. <b>标题：The Open Catalyst 2022 (OC22) Dataset and Challenges for Oxide  Electrocatalysis</b></summary>
  <p><b>编号</b>：[253]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08917</p>
  <p><b>作者</b>：Richard Tran,  Janice Lan,  Muhammed Shuaibi,  Siddharth Goyal,  Brandon M. Wood,  Abhishek Das,  Javier Heras-Domingo,  Adeesh Kolluru,  Ammar Rizvi,  Nima Shoghi,  Anuroop Sriram,  Zachary Ulissi,  C. Lawrence Zitnick</p>
  <p><b>备注</b>：37 pages, 11 figures</p>
  <p><b>关键词</b>：made considerable progress, machine learning communities, developing machine learning, machine learning, machine learning potential</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Computational catalysis and machine learning communities have made
considerable progress in developing machine learning models for catalyst
discovery and design. Yet, a general machine learning potential that spans the
chemical space of catalysis is still out of reach. A significant hurdle is
obtaining access to training data across a wide range of materials. One
important class of materials where data is lacking are oxides, which inhibits
models from studying the Oxygen Evolution Reaction and oxide electrocatalysis
more generally. To address this we developed the Open Catalyst 2022(OC22)
dataset, consisting of 62,521 Density Functional Theory (DFT) relaxations
(~9,884,504 single point calculations) across a range of oxide materials,
coverages, and adsorbates (*H, *O, *N, *C, *OOH, *OH, *OH2, *O2, *CO). We
define generalized tasks to predict the total system energy that are applicable
across catalysis, develop baseline performance of several graph neural networks
(SchNet, DimeNet++, ForceNet, SpinConv, PaiNN, GemNet-dT, GemNet-OC), and
provide pre-defined dataset splits to establish clear benchmarks for future
efforts. For all tasks, we study whether combining datasets leads to better
results, even if they contain different materials or adsorbates. Specifically,
we jointly train models on Open Catalyst 2020 (OC20) Dataset and OC22, or
fine-tune pretrained OC20 models on OC22. In the most general task, GemNet-OC
sees a ~32% improvement in energy predictions through fine-tuning and a ~9%
improvement in force predictions via joint training. Surprisingly, joint
training on both the OC20 and much smaller OC22 datasets also improves total
energy predictions on OC20 by ~19%. The dataset and baseline models are open
sourced, and a public leaderboard will follow to encourage continued community
developments on the total energy tasks and data.</p>
  </details>
</details>
<details>
  <summary>105. <b>标题：SYMBA: Symbolic Computation of Squared Amplitudes in High Energy Physics  with Machine ALearning</b></summary>
  <p><b>编号</b>：[254]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08901</p>
  <p><b>作者</b>：Abdulhakim Alnuqaydan,  Sergei Gleyzer,  Harrison Prosper</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：important physical quantities, high-energy physics, important physical, physical quantities, time consuming</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The cross section is one of the most important physical quantities in
high-energy physics and the most time consuming to compute. While machine
learning has proven to be highly successful in numerical calculations in
high-energy physics, analytical calculations using machine learning are still
in their infancy. In this work, we use a sequence-to-sequence transformer model
to compute a key element of the cross section calculation, namely, the squared
amplitude of an interaction. We show that a transformer model is able to
predict correctly 89.0% and 99.4% of squared amplitudes of QCD and QED
processes, respectively. We discuss the performance of the current model, its
limitations and possible future directions for this work.</p>
  </details>
</details>
<details>
  <summary>106. <b>标题：Adapting the Linearised Laplace Model Evidence for Modern Deep Learning</b></summary>
  <p><b>编号</b>：[255]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08900</p>
  <p><b>作者</b>：Javier Antorán,  David Janz,  James Urquhart Allingham,  Erik Daxberger,  Riccardo Barbano,  Eric Nalisnick,  José Miguel Hernández-Lobato</p>
  <p><b>备注</b>：Paper appearing at ICML 2022</p>
  <p><b>关键词</b>：linearised Laplace method, received renewed attention, Bayesian deep learning, linearised Laplace, estimating model uncertainty</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The linearised Laplace method for estimating model uncertainty has received
renewed attention in the Bayesian deep learning community. The method provides
reliable error bars and admits a closed-form expression for the model evidence,
allowing for scalable selection of model hyperparameters. In this work, we
examine the assumptions behind this method, particularly in conjunction with
model selection. We show that these interact poorly with some now-standard
tools of deep learning--stochastic approximation methods and normalisation
layers--and make recommendations for how to better adapt this classic method to
the modern setting. We provide theoretical support for our recommendations and
validate them empirically on MLPs, classic CNNs, residual networks with and
without normalisation layers, generative autoencoders and transformers.</p>
  </details>
</details>
<details>
  <summary>107. <b>标题：Scaling multi-species occupancy models to large citizen science datasets</b></summary>
  <p><b>编号</b>：[256]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08894</p>
  <p><b>作者</b>：Martin Ingram,  Damjan Vukcevic,  Nick Golding</p>
  <p><b>备注</b>：39 pages, 6 figures (+ supplementary material)</p>
  <p><b>关键词</b>：multi-species occupancy models, Occupancy models, multi-species occupancy, models, Occupancy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Citizen science datasets can be very large and promise to improve species
distribution modelling, but detection is imperfect, risking bias when fitting
models. In particular, observers may not detect species that are actually
present. Occupancy models can estimate and correct for this observation
process, and multi-species occupancy models exploit similarities in the
observation process, which can improve estimates for rare species. However, the
computational methods currently used to fit these models do not scale to large
datasets. We develop approximate Bayesian inference methods and use graphics
processing units (GPUs) to scale multi-species occupancy models to very large
citizen science data. We fit multi-species occupancy models to one month of
data from the eBird project consisting of 186,811 checklist records comprising
430 bird species. We evaluate the predictions on a spatially separated test set
of 59,338 records, comparing two different inference methods -- Markov chain
Monte Carlo (MCMC) and variational inference (VI) -- to occupancy models fitted
to each species separately using maximum likelihood. We fitted models to the
entire dataset using VI, and up to 32,000 records with MCMC. VI fitted to the
entire dataset performed best, outperforming single-species models on both AUC
(90.4% compared to 88.7%) and on log likelihood (-0.080 compared to -0.085). We
also evaluate how well range maps predicted by the model agree with expert
maps. We find that modelling the detection process greatly improves agreement
and that the resulting maps agree as closely with expert maps as ones estimated
using high quality survey data. Our results demonstrate that multi-species
occupancy models are a compelling approach to model large citizen science
datasets, and that, once the observation process is taken into account, they
can model species distributions accurately.</p>
  </details>
</details>
<details>
  <summary>108. <b>标题：Lossy Compression with Gaussian Diffusion</b></summary>
  <p><b>编号</b>：[257]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08889</p>
  <p><b>作者</b>：Lucas Theis,  Tim Salimans,  Matthew D. Hoffman,  Fabian Mentzer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：unconditional diffusion generative, based on unconditional, unconditional diffusion, lossy compression approach, compression approach called</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We describe a novel lossy compression approach called DiffC which is based on
unconditional diffusion generative models. Unlike modern compression schemes
which rely on transform coding and quantization to restrict the transmitted
information, DiffC relies on the efficient communication of pixels corrupted by
Gaussian noise. We implement a proof of concept and find that it works
surprisingly well despite the lack of an encoder transform, outperforming the
state-of-the-art generative compression method HiFiC on ImageNet 64x64. DiffC
only uses a single model to encode and denoise corrupted pixels at arbitrary
bitrates. The approach further provides support for progressive coding, that
is, decoding from partial bit streams. We perform a rate-distortion analysis to
gain a deeper understanding of its performance, providing analytical results
for multivariate Gaussian data as well as initial results for general
distributions. Furthermore, we show that a flow-based reconstruction achieves a
3 dB gain over ancestral sampling at high bitrates.</p>
  </details>
</details>
<details>
  <summary>109. <b>标题：Incorporating intratumoral heterogeneity into weakly-supervised deep  learning models via variance pooling</b></summary>
  <p><b>编号</b>：[258]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08885</p>
  <p><b>作者</b>：Iain Carmichael,  Andrew H. Song,  Richard J. Chen,  Drew F.K. Williamson,  Tiffany Y. Chen,  Faisal Mahmood</p>
  <p><b>备注</b>：To appear in MICCAI 2022</p>
  <p><b>关键词</b>：requires modeling complex, modeling complex features, Supervised learning tasks, slide images, tumor microenvironment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Supervised learning tasks such as cancer survival prediction from gigapixel
whole slide images (WSIs) are a critical challenge in computational pathology
that requires modeling complex features of the tumor microenvironment. These
learning tasks are often solved with deep multi-instance learning (MIL) models
that do not explicitly capture intratumoral heterogeneity. We develop a novel
variance pooling architecture that enables a MIL model to incorporate
intratumoral heterogeneity into its predictions. Two interpretability tools
based on representative patches are illustrated to probe the biological signals
captured by these models. An empirical study with 4,479 gigapixel WSIs from the
Cancer Genome Atlas shows that adding variance pooling onto MIL frameworks
improves survival prediction performance for five cancer types.</p>
  </details>
</details>
<details>
  <summary>110. <b>标题：Mirror Descent with Relative Smoothness in Measure Spaces, with  application to Sinkhorn and EM</b></summary>
  <p><b>编号</b>：[259]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08873</p>
  <p><b>作者</b>：Pierre-Cyril Aubin-Frankowski,  Anna Korba,  Flavien Léger</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：space of measures, problems in machine, machine learning, mirror descent, mirror descent algorithm</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Many problems in machine learning can be formulated as optimizing a convex
functional over a space of measures. This paper studies the convergence of the
mirror descent algorithm in this infinite-dimensional setting. Defining Bregman
divergences through directional derivatives, we derive the convergence of the
scheme for relatively smooth and strongly convex pairs of functionals. Applying
our result to joint distributions and the Kullback--Leibler (KL) divergence, we
show that Sinkhorn's primal iterations for entropic optimal transport in the
continuous setting correspond to a mirror descent, and we obtain a new proof of
its (sub)linear convergence. We also show that Expectation Maximization (EM)
can always formally be written as a mirror descent, and, when optimizing on the
latent distribution while fixing the mixtures, we derive sublinear rates of
convergence.</p>
  </details>
</details>
<details>
  <summary>111. <b>标题：Generalized Frank-Wolfe Algorithm for Bilevel Optimization</b></summary>
  <p><b>编号</b>：[260]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08868</p>
  <p><b>作者</b>：Ruichen Jiang,  Nazanin Abolfazli,  Aryan Mokhtari,  Erfan Yazdandoost Hamedani</p>
  <p><b>备注</b>：24 pages, 4 figures</p>
  <p><b>关键词</b>：constrained optimization problem, simple bilevel optimization, convex constrained optimization, smooth objective function, epsilon</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we study a class of bilevel optimization problems, also known
as simple bilevel optimization, where we minimize a smooth objective function
over the optimal solution set of another convex constrained optimization
problem. Several iterative methods have been developed for tackling this class
of problems. Alas, their convergence guarantees are not satisfactory as they
are either asymptotic for the upper-level objective, or the convergence rates
are slow and sub-optimal. To address this issue, in this paper, we introduce a
generalization of the Frank-Wolfe (FW) method to solve the considered problem.
The main idea of our method is to locally approximate the solution set of the
lower-level problem via a cutting plane, and then run a FW-type update to
decrease the upper-level objective. When the upper-level objective is convex,
we show that our method requires
${\mathcal{O}}(\max\{1/\epsilon_f,1/\epsilon_g\})$ iterations to find a
solution that is $\epsilon_f$-optimal for the upper-level objective and
$\epsilon_g$-optimal for the lower-level objective. Moreover, when the
upper-level objective is non-convex, our method requires
${\mathcal{O}}(\max\{1/\epsilon_f^2,1/(\epsilon_f\epsilon_g)\})$ iterations to
find an $(\epsilon_f,\epsilon_g)$-optimal solution. We further prove stronger
convergence guarantees under the Hölderian error bound assumption on the
lower-level problem. To the best of our knowledge, our method achieves the
best-known iteration complexity for the considered bilevel problem. We also
present numerical experiments to showcase the superior performance of our
method compared with state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>112. <b>标题：Leveraging Uncertainty in Deep Learning for Pancreatic Adenocarcinoma  Grading</b></summary>
  <p><b>编号</b>：[265]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08787</p>
  <p><b>作者</b>：Biraja Ghoshal,  Bhargab Ghoshal,  Allan Tucker</p>
  <p><b>备注</b>：26th UK Conference on Medical Image Understanding and Analysis; 27 - 29 July 2022; University of Cambridge, UK. arXiv admin note: text overlap with arXiv:2003.10769</p>
  <p><b>关键词</b>：worst prognoses compared, worst prognoses, prognoses compared, diagnosing pancreatic adenocarcinomas, Pancreatic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pancreatic cancers have one of the worst prognoses compared to other cancers,
as they are diagnosed when cancer has progressed towards its latter stages. The
current manual histological grading for diagnosing pancreatic adenocarcinomas
is time-consuming and often results in misdiagnosis. In digital pathology,
AI-based cancer grading must be extremely accurate in prediction and
uncertainty quantification to improve reliability and explainability and are
essential for gaining clinicians trust in the technology. We present Bayesian
Convolutional Neural Networks for automated pancreatic cancer grading from MGG
and HE stained images to estimate uncertainty in model prediction. We show that
the estimated uncertainty correlates with prediction error. Specifically, it is
useful in setting the acceptance threshold using a metric that weighs
classification accuracy-reject trade-off and misclassification cost controlled
by hyperparameters and can be employed in clinical settings.</p>
  </details>
</details>
<details>
  <summary>113. <b>标题：Spherical Sliced-Wasserstein</b></summary>
  <p><b>编号</b>：[266]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08780</p>
  <p><b>作者</b>：Clément Bonet,  Paul Berg,  Nicolas Courty,  François Septier,  Lucas Drumetz,  Minh-Tan Pham</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：original computational burden, Wasserstein distance, computational burden, introduced to reduce, reduce its original</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Many variants of the Wasserstein distance have been introduced to reduce its
original computational burden. In particular the Sliced-Wasserstein distance
(SW), which leverages one-dimensional projections for which a closed-form
solution of the Wasserstein distance is available, has received a lot of
interest. Yet, it is restricted to data living in Euclidean spaces, while the
Wasserstein distance has been studied and used recently on manifolds. We focus
more specifically on the sphere, for which we define a novel SW discrepancy,
which we call spherical Sliced-Wasserstein, making a first step towards
defining SW discrepancies on manifolds. Our construction is notably based on
closed-form solutions of the Wasserstein distance on the circle, together with
a new spherical Radon transform. Along with efficient algorithms and the
corresponding implementations, we illustrate its properties in several machine
learning use cases where spherical representations of data are at stake:
density estimation on the sphere, variational inference or hyperspherical
auto-encoders.</p>
  </details>
</details>
<details>
  <summary>114. <b>标题：Tensor-on-Tensor Regression: Riemannian Optimization,  Over-parameterization, Statistical-computational Gap, and Their Interplay</b></summary>
  <p><b>编号</b>：[267]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08756</p>
  <p><b>作者</b>：Yuetian Luo,  Anru R. Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：low Tucker rank, Tucker rank parameter, low Tucker, connect tensor responses, Tucker rank</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the tensor-on-tensor regression, where the goal is to connect tensor
responses to tensor covariates with a low Tucker rank parameter tensor/matrix
without the prior knowledge of its intrinsic rank. We propose the Riemannian
gradient descent (RGD) and Riemannian Gauss-Newton (RGN) methods and cope with
the challenge of unknown rank by studying the effect of rank
over-parameterization. We provide the first convergence guarantee for the
general tensor-on-tensor regression by showing that RGD and RGN respectively
converge linearly and quadratically to a statistically optimal estimate in both
rank correctly-parameterized and over-parameterized settings. Our theory
reveals an intriguing phenomenon: Riemannian optimization methods naturally
adapt to over-parameterization without modifications to their implementation.
We also give the first rigorous evidence for the statistical-computational gap
in scalar-on-tensor regression under the low-degree polynomials framework. Our
theory demonstrates a ``blessing of statistical-computational gap" phenomenon:
in a wide range of scenarios in tensor-on-tensor regression for tensors of
order three or higher, the computationally required sample size matches what is
needed by moderate rank over-parameterization when considering computationally
feasible estimators, while there are no such benefits in the matrix settings.
This shows moderate rank over-parameterization is essentially ``cost-free" in
terms of sample size in tensor-on-tensor regression of order three or higher.
Finally, we conduct simulation studies to show the advantages of our proposed
methods and to corroborate our theoretical findings.</p>
  </details>
</details>
<details>
  <summary>115. <b>标题：Generalised Policy Improvement with Geometric Policy Composition</b></summary>
  <p><b>编号</b>：[269]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08736</p>
  <p><b>作者</b>：Shantanu Thakoor,  Mark Rowland,  Diana Borsa,  Will Dabney,  Rémi Munos,  André Barreto</p>
  <p><b>备注</b>：ICML 2022</p>
  <p><b>关键词</b>：value-based reinforcement learning, full planning approach, planning approach typical, value-based reinforcement, full planning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce a method for policy improvement that interpolates between the
greedy approach of value-based reinforcement learning (RL) and the full
planning approach typical of model-based RL. The new method builds on the
concept of a geometric horizon model (GHM, also known as a gamma-model), which
models the discounted state-visitation distribution of a given policy. We show
that we can evaluate any non-Markov policy that switches between a set of base
Markov policies with fixed probability by a careful composition of the base
policy GHMs, without any additional learning. We can then apply generalised
policy improvement (GPI) to collections of such non-Markov policies to obtain a
new Markov policy that will in general outperform its precursors. We provide a
thorough theoretical analysis of this approach, develop applications to
transfer and standard RL, and empirically demonstrate its effectiveness over
standard GPI on a challenging deep RL continuous control task. We also provide
an analysis of GHM training methods, proving a novel convergence result
regarding previously proposed methods and showing how to train these models
stably in deep RL settings.</p>
  </details>
</details>
<details>
  <summary>116. <b>标题：FiT: Parameter Efficient Few-shot Transfer Learning for Personalized and  Federated Image Classification</b></summary>
  <p><b>编号</b>：[271]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08671</p>
  <p><b>作者</b>：Aliaksandra Shysheya,  John Bronskill,  Massimiliano Patacchiola,  Sebastian Nowozin,  Richard E Turner</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Modern deep learning, deep learning systems, Modern deep, amounts of data, systems are increasingly</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Modern deep learning systems are increasingly deployed in situations such as
personalization and federated learning where it is necessary to support i)
learning on small amounts of data, and ii) communication efficient distributed
training protocols. In this work we develop FiLM Transfer (FiT) which fulfills
these requirements in the image classification setting. FiT uses an
automatically configured Naive Bayes classifier on top of a fixed backbone that
has been pretrained on large image datasets. Parameter efficient FiLM layers
are used to modulate the backbone, shaping the representation for the
downstream task. The network is trained via an episodic fine-tuning protocol.
The approach is parameter efficient which is key for enabling few-shot
learning, inexpensive model updates for personalization, and communication
efficient federated learning. We experiment with FiT on a wide range of
downstream datasets and show that it achieves better classification accuracy
than the state-of-the-art Big Transfer (BiT) algorithm at low-shot and on the
challenging VTAB-1k benchmark, with fewer than 1% of the updateable parameters.
Finally, we demonstrate the parameter efficiency of FiT in distributed low-shot
applications including model personalization and federated learning where model
update size is an important performance metric.</p>
  </details>
</details>
<details>
  <summary>117. <b>标题：The Sensorium competition on predicting large-scale mouse primary visual  cortex activity</b></summary>
  <p><b>编号</b>：[272]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08666</p>
  <p><b>作者</b>：Konstantin F. Willeke (1 and 2 and 3),  Paul G. Fahey (4 and 5),  Mohammad Bashiri (1 and 2 and 3),  Laura Pede (3),  Max F. Burg (1 and 2 and 3 and 6),  Christoph Blessing (3),  Santiago A. Cadena (1 and 3 and 6),  Zhiwei Ding (4 and 5),  Konstantin-Klemens Lurz (1 and 2 and 3),  Kayla Ponder (4 and 5),  Taliah Muhammad (4 and 5),  Saumil S. Patel (4 and 5),  Alexander S. Ecker (3 and 7),  Andreas S. Tolias (4 and 5 and 8),  Fabian H. Sinz (2 and 3 and 4 and 5) ((1) International Max Planck Research School for Intelligent Systems, University of Tuebingen, Germany, (2) Institute for Bioinformatics and Medical Informatics, University of Tuebingen, Germany (3) Institute of Computer Science and Campus Institute Data Science, University of Goettingen, Germany, (4) Department of Neuroscience, Baylor College of Medicine, Houston, USA, (5) Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine, Houston, USA, (6) Institute for Theoretical Physics, University of Tuebingen, Germany, (7) Max Planck Institute for Dynamics and Self-Organization, Goettingen, Germany, (8) Electrical and Computer Engineering, Rice University, Houston, USA)</p>
  <p><b>备注</b>：NeurIPS 2022 Competition Track</p>
  <p><b>关键词</b>：mouse visual system, activity becomes increasingly, increasingly nonlinear, nonlinear with respect, biological visual system</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The neural underpinning of the biological visual system is challenging to
study experimentally, in particular as the neuronal activity becomes
increasingly nonlinear with respect to visual input. Artificial neural networks
(ANNs) can serve a variety of goals for improving our understanding of this
complex system, not only serving as predictive digital twins of sensory cortex
for novel hypothesis generation in silico, but also incorporating bio-inspired
architectural motifs to progressively bridge the gap between biological and
machine vision. The mouse has recently emerged as a popular model system to
study visual information processing, but no standardized large-scale benchmark
to identify state-of-the-art models of the mouse visual system has been
established. To fill this gap, we propose the Sensorium benchmark competition.
We collected a large-scale dataset from mouse primary visual cortex containing
the responses of more than 28,000 neurons across seven mice stimulated with
thousands of natural images, together with simultaneous behavioral measurements
that include running speed, pupil dilation, and eye movements. The benchmark
challenge will rank models based on predictive performance for neuronal
responses on a held-out test set, and includes two tracks for model input
limited to either stimulus only (Sensorium) or stimulus plus behavior
(Sensorium+). We provide a starting kit to lower the barrier for entry,
including tutorials, pre-trained baseline models, and APIs with one line
commands for data loading and submission. We would like to see this as a
starting point for regular challenges and data releases, and as a standard tool
for measuring progress in large-scale neural system identification models of
the mouse visual system and beyond.</p>
  </details>
</details>
<details>
  <summary>118. <b>标题：Orthonormal Expansions for Translation-Invariant Kernels</b></summary>
  <p><b>编号</b>：[273]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08648</p>
  <p><b>作者</b>：Filip Tronarp,  Toni Karvonen</p>
  <p><b>备注</b>：23 pages, 8 figures</p>
  <p><b>关键词</b>：general Fourier analytic, Fourier analytic technique, constructing orthonormal basis, orthonormal basis expansions, general Fourier</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a general Fourier analytic technique for constructing orthonormal
basis expansions of translation-invariant kernels from orthonormal bases of
$\mathscr{L}_2(\mathbb{R})$. This allows us to derive explicit expansions on
the real line for (i) Matérn kernels of all half-integer orders in terms of
associated Laguerre functions, (ii) the Cauchy kernel in terms of rational
functions, and (iii) the Gaussian kernel in terms of Hermite functions.</p>
  </details>
</details>
<details>
  <summary>119. <b>标题：RECAPP: Crafting a More Efficient Catalyst for Convex Optimization</b></summary>
  <p><b>编号</b>：[275]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08627</p>
  <p><b>作者</b>：Yair Carmon,  Arun Jambulapati,  Yujia Jin,  Aaron Sidford</p>
  <p><b>备注</b>：Accepted at ICML'22</p>
  <p><b>关键词</b>：proximal point computation, accelerated proximal point, proximal point, approximate proximal point, optimization to approximate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The accelerated proximal point algorithm (APPA), also known as "Catalyst", is
a well-established reduction from convex optimization to approximate proximal
point computation (i.e., regularized minimization). This reduction is
conceptually elegant and yields strong convergence rate guarantees. However,
these rates feature an extraneous logarithmic term arising from the need to
compute each proximal point to high accuracy. In this work, we propose a novel
Relaxed Error Criterion for Accelerated Proximal Point (RECAPP) that eliminates
the need for high accuracy subproblem solutions. We apply RECAPP to two
canonical problems: finite-sum and max-structured minimization. For finite-sum
problems, we match the best known complexity, previously obtained by
carefully-designed problem-specific algorithms. For minimizing $\max_y f(x,y)$
where $f$ is convex in $x$ and strongly-concave in $y$, we improve on the best
known (Catalyst-based) bound by a logarithmic factor.</p>
  </details>
</details>
<details>
  <summary>120. <b>标题：On Integrating Prior Knowledge into Gaussian Processes for Prognostic  Health Monitoring</b></summary>
  <p><b>编号</b>：[277]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08600</p>
  <p><b>作者</b>：Simon Pfingstl,  Markus Zimmermann</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Gaussian process regression, predicting states based, covariance functions, Gaussian process, predicting states</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Gaussian process regression is a powerful method for predicting states based
on given data. It has been successfully applied for probabilistic predictions
of structural systems to quantify, for example, the crack growth in mechanical
structures. Typically, predefined mean and covariance functions are employed to
construct the Gaussian process model. Then, the model is updated using current
data during operation while prior information based on previous data is
ignored. However, predefined mean and covariance functions without prior
information reduce the potential of Gaussian processes. This paper proposes a
method to improve the predictive capabilities of Gaussian processes. We
integrate prior knowledge by deriving the mean and covariance functions from
previous data. More specifically, we first approximate previous data by a
weighted sum of basis functions and then derive the mean and covariance
functions directly from the estimated weight coefficients. Basis functions may
be either estimated or derived from problem-specific governing equations to
incorporate physical information. The applicability and effectiveness of this
approach are demonstrated for fatigue crack growth, laser degradation, and
milling machine wear data. We show that well-chosen mean and covariance
functions, like those based on previous data, significantly increase look-ahead
time and accuracy. Using physical basis functions further improves accuracy. In
addition, computation effort for training is significantly reduced.</p>
  </details>
</details>
<details>
  <summary>121. <b>标题：Optimal Extragradient-Based Bilinearly-Coupled Saddle-Point Optimization</b></summary>
  <p><b>编号</b>：[279]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08573</p>
  <p><b>作者</b>：Simon S. Du,  Gauthier Gidel,  Michael I. Jordan,  Chris Junchi Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：smooth convex-concave bilinearly-coupled, mathbf, bilinearly-coupled saddle-point problem, convex-concave bilinearly-coupled saddle-point, smooth convex-concave</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider the smooth convex-concave bilinearly-coupled saddle-point
problem, $\min_{\mathbf{x}}\max_{\mathbf{y}}~F(\mathbf{x}) +
H(\mathbf{x},\mathbf{y}) - G(\mathbf{y})$, where one has access to stochastic
first-order oracles for $F$, $G$ as well as the bilinear coupling function $H$.
Building upon standard stochastic extragradient analysis for variational
inequalities, we present a stochastic \emph{accelerated gradient-extragradient
(AG-EG)} descent-ascent algorithm that combines extragradient and Nesterov's
acceleration in general stochastic settings. This algorithm leverages scheduled
restarting to admit a fine-grained nonasymptotic convergence rate that matches
known lower bounds by both \citet{ibrahim2020linear} and \citet{zhang2021lower}
in their corresponding settings, plus an additional statistical error term for
bounded stochastic noise that is optimal up to a constant prefactor. This is
the first result that achieves such a relatively mature characterization of
optimality in saddle-point optimization.</p>
  </details>
</details>
<details>
  <summary>122. <b>标题：NU-Wave 2: A General Neural Audio Upsampling Model for Various Sampling  Rates</b></summary>
  <p><b>编号</b>：[281]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08545</p>
  <p><b>作者</b>：Seungu Han,  Junhyeok Lee</p>
  <p><b>备注</b>：Accepted to Interspeech 2022</p>
  <p><b>关键词</b>：super-resolution models fixed, target sampling rates, sampling rates, audio super-resolution models, fixed the initial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Conventionally, audio super-resolution models fixed the initial and the
target sampling rates, which necessitate the model to be trained for each pair
of sampling rates. We introduce NU-Wave 2, a diffusion model for neural audio
upsampling that enables the generation of 48 kHz audio signals from inputs of
various sampling rates with a single model. Based on the architecture of
NU-Wave, NU-Wave 2 uses short-time Fourier convolution (STFC) to generate
harmonics to resolve the main failure modes of NU-Wave, and incorporates
bandwidth spectral feature transform (BSFT) to condition the bandwidths of
inputs in the frequency domain. We experimentally demonstrate that NU-Wave 2
produces high-resolution audio regardless of the sampling rate of input while
requiring fewer parameters than other models. The official code and the audio
samples are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>123. <b>标题：Reframed GES with a Neural Conditional Dependence Measure</b></summary>
  <p><b>编号</b>：[283]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08531</p>
  <p><b>作者</b>：Xinwei Shen,  Shengyu Zhu,  Jiji Zhang,  Shoubo Hu,  Zhitang Chen</p>
  <p><b>备注</b>：Accepted to UAI 2022</p>
  <p><b>关键词</b>：Markov equivalence class, Markov equivalence, Greedy Equivalence Search, GES algorithm, Markov</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In a nonparametric setting, the causal structure is often identifiable only
up to Markov equivalence, and for the purpose of causal inference, it is useful
to learn a graphical representation of the Markov equivalence class (MEC). In
this paper, we revisit the Greedy Equivalence Search (GES) algorithm, which is
widely cited as a score-based algorithm for learning the MEC of the underlying
causal structure. We observe that in order to make the GES algorithm consistent
in a nonparametric setting, it is not necessary to design a scoring metric that
evaluates graphs. Instead, it suffices to plug in a consistent estimator of a
measure of conditional dependence to guide the search. We therefore present a
reframing of the GES algorithm, which is more flexible than the standard
score-based version and readily lends itself to the nonparametric setting with
a general measure of conditional dependence. In addition, we propose a neural
conditional dependence (NCD) measure, which utilizes the expressive power of
deep neural networks to characterize conditional independence in a
nonparametric manner. We establish the optimality of the reframed GES algorithm
under standard assumptions and the consistency of using our NCD estimator to
decide conditional independence. Together these results justify the proposed
approach. Experimental results demonstrate the effectiveness of our method in
causal discovery, as well as the advantages of using our NCD measure over
kernel-based measures.</p>
  </details>
</details>
<details>
  <summary>124. <b>标题：Variational Estimators of the Degree-corrected Latent Block Model for  Bipartite Networks</b></summary>
  <p><b>编号</b>：[286]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08465</p>
  <p><b>作者</b>：Yunpeng Zhao,  Ning Hao,  Ji Zhu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：unsupervised learning task, movie review dataset, movie review, latent block model, unsupervised learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Biclustering on bipartite graphs is an unsupervised learning task that
simultaneously clusters the two types of objects in the graph, for example,
users and movies in a movie review dataset. The latent block model (LBM) has
been proposed as a model-based tool for biclustering. Biclustering results by
the LBM are, however, usually dominated by the row and column sums of the data
matrix, i.e., degrees. We propose a degree-corrected latent block model
(DC-LBM) to accommodate degree heterogeneity in row and column clusters, which
greatly outperforms the classical LBM in the MovieLens dataset and simulated
data. We develop an efficient variational expectation-maximization algorithm by
observing that the row and column degrees maximize the objective function in
the M step given any probability assignment on the cluster labels. We prove the
label consistency of the variational estimator under the DC-LBM, which allows
the expected graph density goes to zero as long as the average expected degrees
of rows and columns go to infinity.</p>
  </details>
</details>
<details>
  <summary>125. <b>标题：OpenSRH: optimizing brain tumor surgery using intraoperative stimulated  Raman histology</b></summary>
  <p><b>编号</b>：[288]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08439</p>
  <p><b>作者</b>：Cheng Jiang,  Asadur Chowdury,  Xinhai Hou,  Akhil Kondepudi,  Christian W. Freudiger,  Kyle Conway,  Sandra Camelo-Piragua,  Daniel A. Orringer,  Honglak Lee,  Todd C. Hollon</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Accurate intraoperative diagnosis, essential for providing, providing safe, safe and effective, effective care</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Accurate intraoperative diagnosis is essential for providing safe and
effective care during brain tumor surgery. Our standard-of-care diagnostic
methods are time, resource, and labor intensive, which restricts access to
optimal surgical treatments. To address these limitations, we propose an
alternative workflow that combines stimulated Raman histology (SRH), a rapid
optical imaging method, with deep learning-based automated interpretation of
SRH images for intraoperative brain tumor diagnosis and real-time surgical
decision support. Here, we present OpenSRH, the first public dataset of
clinical SRH images from 300+ brain tumors patients and 1300+ unique whole
slide optical images. OpenSRH contains data from the most common brain tumors
diagnoses, full pathologic annotations, whole slide tumor segmentations, raw
and processed optical imaging data for end-to-end model development and
validation. We provide a framework for patch-based whole slide SRH
classification and inference using weak (i.e. patient-level) diagnostic labels.
Finally, we benchmark two computer vision tasks: multiclass histologic brain
tumor classification and patch-based contrastive representation learning. We
hope OpenSRH will facilitate the clinical translation of rapid optical imaging
and real-time ML-based surgical decision support in order to improve the
access, safety, and efficacy of cancer surgery in the era of precision
medicine. Dataset access, code, and benchmarks are available at
this http URL.</p>
  </details>
</details>
<details>
  <summary>126. <b>标题：Learning Generic Lung Ultrasound Biomarkers for Decoupling Feature  Extraction from Downstream Tasks</b></summary>
  <p><b>编号</b>：[290]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08398</p>
  <p><b>作者</b>：Gautam Rajendrakumar Gare,  Tom Fox,  Pete Lowery,  Kevin Zamora,  Hai V. Tran,  Laura Hutchins,  David Montgomery,  Amita Krishnan,  Deva Kannan Ramanan,  Ricardo Luis Rodriguez,  Bennett P deBoisblanc,  John Michael Galeotti</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Contemporary artificial neural, artificial neural networks, Contemporary artificial, ANN, artificial neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Contemporary artificial neural networks (ANN) are trained end-to-end, jointly
learning both features and classifiers for the task of interest. Though
enormously effective, this paradigm imposes significant costs in assembling
annotated task-specific datasets and training large-scale networks. We propose
to decouple feature learning from downstream lung ultrasound tasks by
introducing an auxiliary pre-task of visual biomarker classification. We
demonstrate that one can learn an informative, concise, and interpretable
feature space from ultrasound videos by training models for predicting
biomarker labels. Notably, biomarker feature extractors can be trained from
data annotated with weak video-scale supervision. These features can be used by
a variety of downstream Expert models targeted for diverse clinical tasks
(Diagnosis, lung severity, S/F ratio). Crucially, task-specific expert models
are comparable in accuracy to end-to-end models directly trained for such
target tasks, while being significantly lower cost to train.</p>
  </details>
</details>
<h1>人工智能</h1>
<details>
  <summary>1. <b>标题：TAVA: Template-free Animatable Volumetric Actors</b></summary>
  <p><b>编号</b>：[1]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08929</p>
  <p><b>作者</b>：Ruilong Li,  Julian Tanke,  Minh Vo,  Michael Zollhofer,  Jurgen Gall,  Angjoo Kanazawa,  Christoph Lassner</p>
  <p><b>备注</b>：Code: this https URL; Project Website: this https URL</p>
  <p><b>关键词</b>：generate photo-realistic virtual, photo-realistic virtual avatars, virtual avatars, potential to generate, generate photo-realistic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Coordinate-based volumetric representations have the potential to generate
photo-realistic virtual avatars from images. However, virtual avatars also need
to be controllable even to a novel pose that may not have been observed.
Traditional techniques, such as LBS, provide such a function; yet it usually
requires a hand-designed body template, 3D scan data, and limited appearance
models. On the other hand, neural representation has been shown to be powerful
in representing visual details, but are under explored on deforming dynamic
articulated actors. In this paper, we propose TAVA, a method to create T
emplate-free Animatable Volumetric Actors, based on neural representations. We
rely solely on multi-view data and a tracked skeleton to create a volumetric
model of an actor, which can be animated at the test time given novel pose.
Since TAVA does not require a body template, it is applicable to humans as well
as other creatures such as animals. Furthermore, TAVA is designed such that it
can recover accurate dense correspondences, making it amenable to
content-creation and editing tasks. Through extensive experiments, we
demonstrate that the proposed method generalizes well to novel poses as well as
unseen views and showcase basic editing capabilities.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Cross-task Attention Mechanism for Dense Multi-task Learning</b></summary>
  <p><b>编号</b>：[2]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08927</p>
  <p><b>作者</b>：Ivan Lopes,  Tuan-Hung Vu,  Raoul de Charette</p>
  <p><b>备注</b>：10 figures, 6 tables, 23 pages</p>
  <p><b>关键词</b>：complex scenes, promising solution, comprehensive understanding, understanding of complex, Multi-task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multi-task learning has recently become a promising solution for a
comprehensive understanding of complex scenes. Not only being memory-efficient,
multi-task models with an appropriate design can favor exchange of
complementary signals across tasks. In this work, we jointly address 2D
semantic segmentation, and two geometry-related tasks, namely dense depth,
surface normal estimation as well as edge estimation showing their benefit on
indoor and outdoor datasets. We propose a novel multi-task learning
architecture that exploits pair-wise cross-task exchange through
correlation-guided attention and self-attention to enhance the average
representation learning for all tasks. We conduct extensive experiments
considering three multi-task setups, showing the benefit of our proposal in
comparison to competitive baselines in both synthetic and real benchmarks. We
also extend our method to the novel multi-task unsupervised domain adaptation
setting. Our code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Fast Population-Based Reinforcement Learning on a Single Machine</b></summary>
  <p><b>编号</b>：[15]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08888</p>
  <p><b>作者</b>：Arthur Flajolet,  Claire Bizon Monroc,  Karim Beguir,  Thomas Pierrot</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：demonstrated great promise, demonstrated great, great promise, Training, population-based training</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Training populations of agents has demonstrated great promise in
Reinforcement Learning for stabilizing training, improving exploration and
asymptotic performance, and generating a diverse set of solutions. However,
population-based training is often not considered by practitioners as it is
perceived to be either prohibitively slow (when implemented sequentially), or
computationally expensive (if agents are trained in parallel on independent
accelerators). In this work, we compare implementations and revisit previous
studies to show that the judicious use of compilation and vectorization allows
population-based training to be performed on a single machine with one
accelerator with minimal overhead compared to training a single agent. We also
show that, when provided with a few accelerators, our protocols extend to large
population sizes for applications such as hyperparameter tuning. We hope that
this work and the public release of our code will encourage practitioners to
use population-based learning more frequently for their research and
applications.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：CtrlFormer: Learning Transferable State Representation for Visual  Control via Transformer</b></summary>
  <p><b>编号</b>：[17]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08883</p>
  <p><b>作者</b>：Yao Mu,  Shoufa Chen,  Mingyu Ding,  Jianyu Chen,  Runjian Chen,  Ping Luo</p>
  <p><b>备注</b>：ICML 2022</p>
  <p><b>关键词</b>：achieved great successes, achieved great, great successes, vision and language, control</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transformer has achieved great successes in learning vision and language
representation, which is general across various downstream tasks. In visual
control, learning transferable state representation that can transfer between
different control tasks is important to reduce the training sample size.
However, porting Transformer to sample-efficient visual control remains a
challenging and unsolved problem. To this end, we propose a novel Control
Transformer (CtrlFormer), possessing many appealing benefits that prior arts do
not have. Firstly, CtrlFormer jointly learns self-attention mechanisms between
visual tokens and policy tokens among different control tasks, where multitask
representation can be learned and transferred without catastrophic forgetting.
Secondly, we carefully design a contrastive reinforcement learning paradigm to
train CtrlFormer, enabling it to achieve high sample efficiency, which is
important in control problems. For example, in the DMControl benchmark, unlike
recent advanced methods that failed by producing a zero score in the "Cartpole"
task after transfer learning with 100k samples, CtrlFormer can achieve a
state-of-the-art score with only 100k samples while maintaining the performance
of previous tasks. The code and models are released in our project homepage.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Logic-based Reward Shaping for Multi-Agent Reinforcement Learning</b></summary>
  <p><b>编号</b>：[19]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08881</p>
  <p><b>作者</b>：Ingy ElSayed-Aly,  Lu Feng</p>
  <p><b>备注</b>：10 pages, technical report</p>
  <p><b>关键词</b>：maximize observed rewards, Reinforcement learning, relies heavily, reward shaping, heavily on exploration</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reinforcement learning (RL) relies heavily on exploration to learn from its
environment and maximize observed rewards. Therefore, it is essential to design
a reward function that guarantees optimal learning from the received
experience. Previous work has combined automata and logic based reward shaping
with environment assumptions to provide an automatic mechanism to synthesize
the reward function based on the task. However, there is limited work on how to
expand logic-based reward shaping to Multi-Agent Reinforcement Learning (MARL).
The environment will need to consider the joint state in order to keep track of
other agents if the task requires cooperation, thus suffering from the curse of
dimensionality with respect to the number of agents. This project explores how
logic-based reward shaping for MARL can be designed for different scenarios and
tasks. We present a novel method for semi-centralized logic-based MARL reward
shaping that is scalable in the number of agents and evaluate it in multiple
scenarios.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：MineDojo: Building Open-Ended Embodied Agents with Internet-Scale  Knowledge</b></summary>
  <p><b>编号</b>：[29]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08853</p>
  <p><b>作者</b>：Linxi Fan,  Guanzhi Wang,  Yunfan Jiang,  Ajay Mandlekar,  Yuncong Yang,  Haoyi Zhu,  Andrew Tang,  De-An Huang,  Yuke Zhu,  Anima Anandkumar</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：made great strides, domains like Atari, Atari games, made great, great strides</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Autonomous agents have made great strides in specialist domains like Atari
games and Go. However, they typically learn tabula rasa in isolated
environments with limited and manually conceived objectives, thus failing to
generalize across a wide spectrum of tasks and capabilities. Inspired by how
humans continually learn and adapt in the open world, we advocate a trinity of
ingredients for building generalist agents: 1) an environment that supports a
multitude of tasks and goals, 2) a large-scale database of multimodal
knowledge, and 3) a flexible and scalable agent architecture. We introduce
MineDojo, a new framework built on the popular Minecraft game that features a
simulation suite with thousands of diverse open-ended tasks and an
internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and
forum discussions. Using MineDojo's data, we propose a novel agent learning
algorithm that leverages large pre-trained video-language models as a learned
reward function. Our agent is able to solve a variety of open-ended tasks
specified in free-form language without any manually designed dense shaping
reward. We open-source the simulation suite and knowledge bases
(this https URL) to promote research towards the goal of generally
capable embodied agents.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Holistic Transformer: A Joint Neural Network for Trajectory Prediction  and Decision-Making of Autonomous Vehicles</b></summary>
  <p><b>编号</b>：[45]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08809</p>
  <p><b>作者</b>：Hongyu Hu,  Qi Wang,  Zhengguang Zhang,  Zhengyi Li,  Zhenhai Gao</p>
  <p><b>备注</b>：26 pages, 6 figures</p>
  <p><b>关键词</b>：require good understanding, environmental context, autonomous vehicles, vehicles that require, made by referring</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Trajectory prediction and behavioral decision-making are two important tasks
for autonomous vehicles that require good understanding of the environmental
context; behavioral decisions are better made by referring to the outputs of
trajectory predictions. However, most current solutions perform these two tasks
separately. Therefore, a joint neural network that combines multiple cues is
proposed and named as the holistic transformer to predict trajectories and make
behavioral decisions simultaneously. To better explore the intrinsic
relationships between cues, the network uses existing knowledge and adopts
three kinds of attention mechanisms: the sparse multi-head type for reducing
noise impact, feature selection sparse type for optimally using partial prior
knowledge, and multi-head with sigmoid activation type for optimally using
posteriori knowledge. Compared with other trajectory prediction models, the
proposed model has better comprehensive performance and good interpretability.
Perceptual noise robustness experiments demonstrate that the proposed model has
good noise robustness. Thus, simultaneous trajectory prediction and behavioral
decision-making combining multiple cues can reduce computational costs and
enhance semantic relationships between scenes and agents.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：DU-Net based Unsupervised Contrastive Learning for Cancer Segmentation  in Histology Images</b></summary>
  <p><b>编号</b>：[55]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08791</p>
  <p><b>作者</b>：Yilong Li,  Yaqi Wang,  Huiyu Zhou,  Huaqiong Wang,  Gangyong Jia,  Qianni Zhang</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2002.05709 by other authors</p>
  <p><b>关键词</b>：unsupervised cancer segmentation, cancer segmentation framework, histology images, introduce an unsupervised, unsupervised cancer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we introduce an unsupervised cancer segmentation framework for
histology images. The framework involves an effective contrastive learning
scheme for extracting distinctive visual representations for segmentation. The
encoder is a Deep U-Net (DU-Net) structure that contains an extra fully
convolution layer compared to the normal U-Net. A contrastive learning scheme
is developed to solve the problem of lacking training sets with high-quality
annotations on tumour boundaries. A specific set of data augmentation
techniques are employed to improve the discriminability of the learned colour
features from contrastive learning. Smoothing and noise elimination are
conducted using convolutional Conditional Random Fields. The experiments
demonstrate competitive performance in segmentation even better than some
popular supervised networks.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Reconstructing vehicles from orthographic drawings using deep neural  networks</b></summary>
  <p><b>编号</b>：[57]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08789</p>
  <p><b>作者</b>：Robin Klippert</p>
  <p><b>备注</b>：9 Pages</p>
  <p><b>关键词</b>：multiple orthographic drawings, explores the current, orthographic drawings, drawings using deep, deep neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper explores the current state-of-the-art of object reconstruction
from multiple orthographic drawings using deep neural networks. It proposes two
algorithms to extract multiple views from a single image. The paper proposes a
system based on pixel-aligned implicit functions (PIFu) and develops an
advanced sampling strategy to generate signed distance samples. It also
compares this approach to depth map regression from multiple views.
Additionally, the paper uses a novel dataset for vehicle reconstruction from
the racing game Assetto Corsa, which features higher quality models than the
commonly used ShapeNET dataset. The trained neural network generalizes well to
real-world inputs and creates plausible and detailed reconstructions.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Is Multi-Modal Necessarily Better? Robustness Evaluation of Multi-modal  Fake News Detection</b></summary>
  <p><b>编号</b>：[58]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08788</p>
  <p><b>作者</b>：Jinyin Chen,  Chengyu Jia,  Haibin Zheng,  Ruoxi Chen,  Chenbo Fu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：negative social influence, social influence push, influence push fake, web managers, detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The proliferation of fake news and its serious negative social influence push
fake news detection methods to become necessary tools for web managers.
Meanwhile, the multi-media nature of social media makes multi-modal fake news
detection popular for its ability to capture more modal features than uni-modal
detection methods. However, current literature on multi-modal detection is more
likely to pursue the detection accuracy but ignore the robustness of the
detector. To address this problem, we propose a comprehensive robustness
evaluation of multi-modal fake news detectors. In this work, we simulate the
attack methods of malicious users and developers, i.e., posting fake news and
injecting backdoors. Specifically, we evaluate multi-modal detectors with five
adversarial and two backdoor attack methods. Experiment results imply that: (1)
The detection performance of the state-of-the-art detectors degrades
significantly under adversarial attacks, even worse than general detectors; (2)
Most multi-modal detectors are more vulnerable when subjected to attacks on
visual modality than textual modality; (3) Popular events' images will cause
significant degradation to the detectors when they are subjected to backdoor
attacks; (4) The performance of these detectors under multi-modal attacks is
worse than under uni-modal attacks; (5) Defensive methods will improve the
robustness of the multi-modal detectors.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Reinforcement Learning in Macroeconomic Policy Design: A New Frontier?</b></summary>
  <p><b>编号</b>：[61]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08781</p>
  <p><b>作者</b>：Callum Tilbury</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：policy design toolboxes, enter mainstream policy, mainstream policy design, rich academic history, design toolboxes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Agent-based computational macroeconomics is a field with a rich academic
history, yet one which has struggled to enter mainstream policy design
toolboxes, plagued by the challenges associated with representing a complex and
dynamic reality. The field of Reinforcement Learning (RL), too, has a rich
history, and has recently been at the centre of several exponential
developments. Modern RL implementations have been able to achieve unprecedented
levels of sophistication, handling previously-unthinkable degrees of
complexity. This review surveys the historical barriers of classical
agent-based techniques in macroeconomic modelling, and contemplates whether
recent developments in RL can overcome any of them.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：CTooth: A Fully Annotated 3D Dataset and Benchmark for Tooth Volume  Segmentation on Cone Beam Computed Tomography Images</b></summary>
  <p><b>编号</b>：[62]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08778</p>
  <p><b>作者</b>：Weiwei Cui,  Yaqi Wang,  Qianni Zhang,  Huiyu Zhou,  Dan Song,  Xingyong Zuo,  Gangyong Jia,  Liaoyuan Zeng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：computer-aided dental diagnosis, diagnosis and treatment, prerequisite for computer-aided, tooth, segmentation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>3D tooth segmentation is a prerequisite for computer-aided dental diagnosis
and treatment. However, segmenting all tooth regions manually is subjective and
time-consuming. Recently, deep learning-based segmentation methods produce
convincing results and reduce manual annotation efforts, but it requires a
large quantity of ground truth for training. To our knowledge, there are few
tooth data available for the 3D segmentation study. In this paper, we establish
a fully annotated cone beam computed tomography dataset CTooth with tooth gold
standard. This dataset contains 22 volumes (7363 slices) with fine tooth labels
annotated by experienced radiographic interpreters. To ensure a relative even
data sampling distribution, data variance is included in the CTooth including
missing teeth and dental restoration. Several state-of-the-art segmentation
methods are evaluated on this dataset. Afterwards, we further summarise and
apply a series of 3D attention-based Unet variants for segmenting tooth
volumes. This work provides a new benchmark for the tooth volume segmentation
task. Experimental evidence proves that attention modules of the 3D UNet
structure boost responses in tooth areas and inhibit the influence of
background and noise. The best performance is achieved by 3D Unet with SKNet
attention module, of 88.04 \% Dice and 78.71 \% IOU, respectively. The
attention-based Unet framework outperforms other state-of-the-art methods on
the CTooth dataset. The codebase and dataset are released.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：C-Pack of IPAs: A C90 Program Benchmark of Introductory Programming  Assignments</b></summary>
  <p><b>编号</b>：[67]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08768</p>
  <p><b>作者</b>：Pedro Orvalho,  Mikoláš Janota,  Vasco Manquinho</p>
  <p><b>备注</b>：3 pages, 3 tables, 1 GitHub url: this https URL</p>
  <p><b>关键词</b>：Massive Open Online, Massive Open, Open Online, introductory programming assignments, enrolled in Massive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Due to the vast number of students enrolled in Massive Open Online Courses
(MOOCs), there has been an increasing number of automated program repair
techniques focused on introductory programming assignments (IPAs). Such
techniques take advantage of previous correct student implementations in order
to provide automated, comprehensive, and personalized feedback to students.
This paper presents C-Pack-IPAs, a publicly available benchmark of students'
programs submitted for 25 different IPAs. C-Pack-IPAs contains semantically
correct, semantically incorrect, and syntactically incorrect programs plus a
test suite for each IPA. Hence, C-Pack-IPAs can be used to help evaluate the
development of novel semantic, as well as syntactic, automated program repair
frameworks, focused on providing feedback to novice programmers.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Rectifying Mono-Label Boolean Classifiers</b></summary>
  <p><b>编号</b>：[69]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08758</p>
  <p><b>作者</b>：Sylvie Coste-Marquis,  Pierre Marquis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Sigma, mono-label Boolean classifiers, Boolean classifier, star, Boolean</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We elaborate on the notion of rectification of a Boolean classifier $\Sigma$.
Given $\Sigma$ and some background knowledge $T$, postulates characterizing the
way $\Sigma$ must be changed into a new classifier $\Sigma \star T$ that
complies with $T$ have already been presented. We focus here on the specific
case of mono-label Boolean classifiers, i.e., there is a single target concept
and any instance is classified either as positive (an element of the concept),
or as negative (an element of the complementary concept). In this specific
case, our main contribution is twofold: (1) we show that there is a unique
rectification operator $\star$ satisfying the postulates, and (2) when $\Sigma$
and $T$ are Boolean circuits, we show how a classification circuit equivalent
to $\Sigma \star T$ can be computed in time linear in the size of $\Sigma$ and
$T$; when $\Sigma$ and $T$ are decision trees, a decision tree equivalent to
$\Sigma \star T$ can be computed in time polynomial in the size of $\Sigma$ and
$T$.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Learning Fair Representation via Distributional Contrastive  Disentanglement</b></summary>
  <p><b>编号</b>：[78]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08743</p>
  <p><b>作者</b>：Changdae Oh,  Heeji Won,  Junhyuk So,  Taero Kim,  Yewon Kim,  Hosik Choi,  Kyungwoo Song</p>
  <p><b>备注</b>：Accepted by KDD 2022 (Research Track)</p>
  <p><b>关键词</b>：crucial for achieving, Learning fair representation, representation, fair representation, Learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Learning fair representation is crucial for achieving fairness or debiasing
sensitive information. Most existing works rely on adversarial representation
learning to inject some invariance into representation. However, adversarial
learning methods are known to suffer from relatively unstable training, and
this might harm the balance between fairness and predictiveness of
representation. We propose a new approach, learning FAir Representation via
distributional CONtrastive Variational AutoEncoder (FarconVAE), which induces
the latent space to be disentangled into sensitive and nonsensitive parts. We
first construct the pair of observations with different sensitive attributes
but with the same labels. Then, FarconVAE enforces each non-sensitive latent to
be closer, while sensitive latents to be far from each other and also far from
the non-sensitive latent by contrasting their distributions. We provide a new
type of contrastive loss motivated by Gaussian and Student-t kernels for
distributional contrastive learning with theoretical analysis. Besides, we
adopt a new swap-reconstruction loss to boost the disentanglement further.
FarconVAE shows superior performance on fairness, pretrained model debiasing,
and domain generalization tasks from various modalities, including tabular,
image, and text.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：N$^2$M$^2$: Learning Navigation for Arbitrary Mobile Manipulation  Motions in Unseen and Dynamic Environments</b></summary>
  <p><b>编号</b>：[81]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08737</p>
  <p><b>作者</b>：Daniel Honerkamp,  Tim Welschehold,  Abhinav Valada</p>
  <p><b>备注</b>：Project website: this http URL</p>
  <p><b>关键词</b>：end-effector trajectory generation, service robotics, mobile manipulation remains, mobile manipulation, industrial and service</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite its importance in both industrial and service robotics, mobile
manipulation remains a significant challenge as it requires a seamless
integration of end-effector trajectory generation with navigation skills as
well as reasoning over long-horizons. Existing methods struggle to control the
large configuration space, and to navigate dynamic and unknown environments. In
previous work, we proposed to decompose mobile manipulation tasks into a
simplified motion generator for the end-effector in task space and a trained
reinforcement learning agent for the mobile base to account for kinematic
feasibility of the motion. In this work, we introduce Neural Navigation for
Mobile Manipulation (N$^2$M$^2$) which extends this decomposition to complex
obstacle environments and enables it to tackle a broad range of tasks in real
world settings. The resulting approach can perform unseen, long-horizon tasks
in unexplored environments while instantly reacting to dynamic obstacles and
environmental changes. At the same time, it provides a simple way to define new
mobile manipulation tasks. We demonstrate the capabilities of our proposed
approach in extensive simulation and real-world experiments on multiple
kinematically diverse mobile manipulators. Code and videos are publicly
available at this http URL.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Fast Finite Width Neural Tangent Kernel</b></summary>
  <p><b>编号</b>：[90]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08720</p>
  <p><b>作者</b>：Roman Novak,  Jascha Sohl-Dickstein,  Samuel S. Schoenholz</p>
  <p><b>备注</b>：Published as a conference paper at ICML 2022</p>
  <p><b>关键词</b>：Theta, Neural Tangent Kernel, partial, finite width NTK, Tangent Kernel</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The Neural Tangent Kernel (NTK), defined as $\Theta_\theta^f(x_1, x_2) =
\left[\partial f(\theta, x_1)\big/\partial \theta\right] \left[\partial
f(\theta, x_2)\big/\partial \theta\right]^T$ where $\left[\partial f(\theta,
\cdot)\big/\partial \theta\right]$ is a neural network (NN) Jacobian, has
emerged as a central object of study in deep learning. In the infinite width
limit, the NTK can sometimes be computed analytically and is useful for
understanding training and generalization of NN architectures. At finite
widths, the NTK is also used to better initialize NNs, compare the conditioning
across models, perform architecture search, and do meta-learning.
Unfortunately, the finite width NTK is notoriously expensive to compute, which
severely limits its practical utility. We perform the first in-depth analysis
of the compute and memory requirements for NTK computation in finite width
networks. Leveraging the structure of neural networks, we further propose two
novel algorithms that change the exponent of the compute and memory
requirements of the finite width NTK, dramatically improving efficiency. Our
algorithms can be applied in a black box fashion to any differentiable
function, including those implementing neural networks. We open-source our
implementations within the Neural Tangents package (arXiv:1912.02803) at
this https URL.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：You Only Derive Once (YODO): Automatic Differentiation for Efficient  Sensitivity Analysis in Bayesian Networks</b></summary>
  <p><b>编号</b>：[105]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08687</p>
  <p><b>作者</b>：Rafael Ballester-Ripoll,  Manuele Leonelli</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Sensitivity analysis measures, quantity of interest, taking a specific, Bayesian network, Bayesian network parameters</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sensitivity analysis measures the influence of a Bayesian network's
parameters on a quantity of interest defined by the network, such as the
probability of a variable taking a specific value. In particular, the so-called
sensitivity value measures the quantity of interest's partial derivative with
respect to the network's conditional probabilities. However, finding such
values in large networks with thousands of parameters can become
computationally very expensive. We propose to use automatic differentiation
combined with exact inference to obtain all sensitivity values in a single
pass. Our method first marginalizes the whole network once using e.g. variable
elimination and then backpropagates this operation to obtain the gradient with
respect to all input parameters. We demonstrate our routines by ranking all
parameters by importance on a Bayesian network modeling humanitarian crises and
disasters, and then show the method's efficiency by scaling it to huge networks
with up to 100'000 parameters. An implementation of the methods using the
popular machine learning library PyTorch is freely available.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Towards Human-Level Bimanual Dexterous Manipulation with Reinforcement  Learning</b></summary>
  <p><b>编号</b>：[106]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08686</p>
  <p><b>作者</b>：Yuanpei Chen,  Yaodong Yang,  Tianhao Wu,  Shengjie Wang,  Xidong Feng,  Jiechuang Jiang,  Stephen Marcus McAleer,  Hao Dong,  Zongqing Lu,  Song-Chun Zhu</p>
  <p><b>备注</b>：36 pages, 7 figures</p>
  <p><b>关键词</b>：Achieving human-level dexterity, important open problem, Achieving human-level, problem in robotics, human-level dexterity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Achieving human-level dexterity is an important open problem in robotics.
However, tasks of dexterous hand manipulation, even at the baby level, are
challenging to solve through reinforcement learning (RL). The difficulty lies
in the high degrees of freedom and the required cooperation among heterogeneous
agents (e.g., joints of fingers). In this study, we propose the Bimanual
Dexterous Hands Benchmark (Bi-DexHands), a simulator that involves two
dexterous hands with tens of bimanual manipulation tasks and thousands of
target objects. Specifically, tasks in Bi-DexHands are designed to match
different levels of human motor skills according to cognitive science
literature. We built Bi-DexHands in the Issac Gym; this enables highly
efficient RL training, reaching 30,000+ FPS by only one single NVIDIA RTX 3090.
We provide a comprehensive benchmark for popular RL algorithms under different
settings; this includes Single-agent/Multi-agent RL, Offline RL, Multi-task RL,
and Meta RL. Our results show that the PPO type of on-policy algorithms can
master simple manipulation tasks that are equivalent up to 48-month human
babies (e.g., catching a flying object, opening a bottle), while multi-agent RL
can further help to master manipulations that require skilled bimanual
cooperation (e.g., lifting a pot, stacking blocks). Despite the success on each
single task, when it comes to acquiring multiple manipulation skills, existing
RL algorithms fail to work in most of the multi-task and the few-shot learning
settings, which calls for more substantial development from the RL community.
Our project is open sourced at this https URL.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Understanding Robust Overfitting of Adversarial Training and Beyond</b></summary>
  <p><b>编号</b>：[110]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08675</p>
  <p><b>作者</b>：Chaojian Yu,  Bo Han,  Li Shen,  Jun Yu,  Chen Gong,  Mingming Gong,  Tongliang Liu</p>
  <p><b>备注</b>：ICML2022</p>
  <p><b>关键词</b>：overfitting widely exists, data, Robust overfitting, adversarial training, adversarial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Robust overfitting widely exists in adversarial training of deep networks.
The exact underlying reasons for this are still not completely understood.
Here, we explore the causes of robust overfitting by comparing the data
distribution of \emph{non-overfit} (weak adversary) and \emph{overfitted}
(strong adversary) adversarial training, and observe that the distribution of
the adversarial data generated by weak adversary mainly contain small-loss
data. However, the adversarial data generated by strong adversary is more
diversely distributed on the large-loss data and the small-loss data. Given
these observations, we further designed data ablation adversarial training and
identify that some small-loss data which are not worthy of the adversary
strength cause robust overfitting in the strong adversary mode. To relieve this
issue, we propose \emph{minimum loss constrained adversarial training} (MLCAT):
in a minibatch, we learn large-loss data as usual, and adopt additional
measures to increase the loss of the small-loss data. Technically, MLCAT
hinders data fitting when they become easy to learn to prevent robust
overfitting; philosophically, MLCAT reflects the spirit of turning waste into
treasure and making the best use of each adversarial data; algorithmically, we
designed two realizations of MLCAT, and extensive experiments demonstrate that
MLCAT can eliminate robust overfitting and further boost adversarial
robustness.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：A Deep Learning Approach for the Segmentation of Electroencephalography  Data in Eye Tracking Applications</b></summary>
  <p><b>编号</b>：[112]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08672</p>
  <p><b>作者</b>：Lukas Wolf,  Ard Kastrati,  Martyna Beata Płomecka,  Jie-Ming Li,  Dustin Klebe,  Alexander Veicht,  Roger Wattenhofer,  Nicolas Langer</p>
  <p><b>备注</b>：21 pages, Published at the Proceedings of the 39th International Conference on Machine Learning (ICML) 2022</p>
  <p><b>关键词</b>：eye gaze information, health and behaviour, human cognition, critical aspects, aspects of human</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The collection of eye gaze information provides a window into many critical
aspects of human cognition, health and behaviour. Additionally, many
neuroscientific studies complement the behavioural information gained from eye
tracking with the high temporal resolution and neurophysiological markers
provided by electroencephalography (EEG). One of the essential eye-tracking
software processing steps is the segmentation of the continuous data stream
into events relevant to eye-tracking applications, such as saccades, fixations,
and blinks.
Here, we introduce DETRtime, a novel framework for time-series segmentation
that creates ocular event detectors that do not require additionally recorded
eye-tracking modality and rely solely on EEG data. Our end-to-end deep
learning-based framework brings recent advances in Computer Vision to the
forefront of the times series segmentation of EEG data. DETRtime achieves
state-of-the-art performance in ocular event detection across diverse
eye-tracking experiment paradigms. In addition to that, we provide evidence
that our model generalizes well in the task of EEG sleep stage segmentation.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：tinySNN: Towards Memory- and Energy-Efficient Spiking Neural Networks</b></summary>
  <p><b>编号</b>：[119]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08656</p>
  <p><b>作者</b>：Rachmad Vidya Wicaksana Putra,  Muhammad Shafique</p>
  <p><b>备注</b>：9 figures</p>
  <p><b>关键词</b>：Larger Spiking Neural, Spiking Neural Network, Spiking Neural, Larger Spiking, offer higher accuracy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Larger Spiking Neural Network (SNN) models are typically favorable as they
can offer higher accuracy. However, employing such models on the resource- and
energy-constrained embedded platforms is inefficient. Towards this, we present
a tinySNN framework that optimizes the memory and energy requirements of SNN
processing in both the training and inference phases, while keeping the
accuracy high. It is achieved by reducing the SNN operations, improving the
learning quality, quantizing the SNN parameters, and selecting the appropriate
SNN model. Furthermore, our tinySNN quantizes different SNN parameters (i.e.,
weights and neuron parameters) to maximize the compression while exploring
different combinations of quantization schemes, precision levels, and rounding
schemes to find the model that provides acceptable accuracy. The experimental
results demonstrate that our tinySNN significantly reduces the memory footprint
and the energy consumption of SNNs without accuracy loss as compared to the
baseline network. Therefore, our tinySNN effectively compresses the given SNN
model to achieve high accuracy in a memory- and energy-efficient manner, hence
enabling the employment of SNNs for the resource- and energy-constrained
embedded applications.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：All Mistakes Are Not Equal: Comprehensive Hierarchy Aware Multi-label  Predictions (CHAMP)</b></summary>
  <p><b>编号</b>：[121]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08653</p>
  <p><b>作者</b>：Ashwin Vaswani,  Gaurav Aggarwal,  Praneeth Netrapalli,  Narayan G Hegde</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：domain-specific hierarchy tree, present Comprehensive Hierarchy, hierarchy tree, Aware Multi-label Predictions, domain-specific hierarchy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper considers the problem of Hierarchical Multi-Label Classification
(HMC), where (i) several labels can be present for each example, and (ii)
labels are related via a domain-specific hierarchy tree. Guided by the
intuition that all mistakes are not equal, we present Comprehensive Hierarchy
Aware Multi-label Predictions (CHAMP), a framework that penalizes a
misprediction depending on its severity as per the hierarchy tree. While there
have been works that apply such an idea to single-label classification, to the
best of our knowledge, there are limited such works for multilabel
classification focusing on the severity of mistakes. The key reason is that
there is no clear way of quantifying the severity of a misprediction a priori
in the multilabel setting. In this work, we propose a simple but effective
metric to quantify the severity of a mistake in HMC, naturally leading to
CHAMP. Extensive experiments on six public HMC datasets across modalities
(image, audio, and text) demonstrate that incorporating hierarchical
information leads to substantial gains as CHAMP improves both AUPRC (2.6%
median percentage improvement) and hierarchical metrics (2.85% median
percentage improvement), over stand-alone hierarchical or multilabel
classification methods. Compared to standard multilabel baselines, CHAMP
provides improved AUPRC in both robustness (8.87% mean percentage improvement )
and less data regimes. Further, our method provides a framework to enhance
existing multilabel classification algorithms with better mistakes (18.1% mean
percentage increment).</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Uncertainty-aware Evaluation of Time-Series Classification for Online  Handwriting Recognition with Domain Shift</b></summary>
  <p><b>编号</b>：[128]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08640</p>
  <p><b>作者</b>：Andreas Klaß,  Sven M. Lorenz,  Martin W. Lauer-Schmaltz,  David Rügamer,  Bernd Bischl,  Christopher Mutschler,  Felix Ott</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：machine learning model, machine learning, computer vision applications, data, applications</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>For many applications, analyzing the uncertainty of a machine learning model
is indispensable. While research of uncertainty quantification (UQ) techniques
is very advanced for computer vision applications, UQ methods for
spatio-temporal data are less studied. In this paper, we focus on models for
online handwriting recognition, one particular type of spatio-temporal data.
The data is observed from a sensor-enhanced pen with the goal to classify
written characters. We conduct a broad evaluation of aleatoric (data) and
epistemic (model) UQ based on two prominent techniques for Bayesian inference,
Stochastic Weight Averaging-Gaussian (SWAG) and Deep Ensembles. Next to a
better understanding of the model, UQ techniques can detect out-of-distribution
data and domain shifts when combining right-handed and left-handed writers (an
underrepresented group).</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Minimum Noticeable Difference based Adversarial Privacy Preserving Image  Generation</b></summary>
  <p><b>编号</b>：[130]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08638</p>
  <p><b>作者</b>：Wen Sun,  Jian Jin,  Weisi Lin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Deep learning models, adversarial, adversarial image generation, Deep learning, learning models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning models are found to be vulnerable to adversarial examples, as
wrong predictions can be caused by small perturbation in input for deep
learning models. Most of the existing works of adversarial image generation try
to achieve attacks for most models, while few of them make efforts on
guaranteeing the perceptual quality of the adversarial examples. High quality
adversarial examples matter for many applications, especially for the privacy
preserving. In this work, we develop a framework based on the Minimum
Noticeable Difference (MND) concept to generate adversarial privacy preserving
images that have minimum perceptual difference from the clean ones but are able
to attack deep learning models. To achieve this, an adversarial loss is firstly
proposed to make the deep learning models attacked by the adversarial images
successfully. Then, a perceptual quality-preserving loss is developed by taking
the magnitude of perturbation and perturbation-caused structural and gradient
changes into account, which aims to preserve high perceptual quality for
adversarial image generation. To the best of our knowledge, this is the first
work on exploring quality-preserving adversarial image generation based on the
MND concept for privacy preserving. To evaluate its performance in terms of
perceptual quality, the deep models on image classification and face
recognition are tested with the proposed method and several anchor methods in
this work. Extensive experimental results demonstrate that the proposed MND
framework is capable of generating adversarial images with remarkably improved
performance metrics (e.g., PSNR, SSIM, and MOS) than that generated with the
anchor methods.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：MSDF: A General Open-Domain Multi-Skill Dialog Framework</b></summary>
  <p><b>编号</b>：[133]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08626</p>
  <p><b>作者</b>：Yu Zhao,  Xinshuo Hu,  Yunxin Li,  Baotian Hu,  Dongfang Li,  Sichao Chen,  Xiaolong Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：achieved significant progress, Dialog, systems have achieved, achieved significant, significant progress</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Dialog systems have achieved significant progress and have been widely used
in various scenarios. The previous researches mainly focused on designing
dialog generation models in a single scenario, while comprehensive abilities
are required to handle tasks under various scenarios in the real world. In this
paper, we propose a general Multi-Skill Dialog Framework, namely MSDF, which
can be applied in different dialog tasks (e.g. knowledge grounded dialog and
persona based dialog). Specifically, we propose a transferable response
generator pre-trained on diverse large-scale dialog corpora as the backbone of
MSDF, consisting of BERT-based encoders and a GPT-based decoder. To select the
response consistent with dialog history, we propose a consistency selector
trained through negative sampling. Moreover, the flexible copy mechanism of
external knowledge is also employed to enhance the utilization of multiform
knowledge in various scenarios. We conduct experiments on knowledge grounded
dialog, recommendation dialog, and persona based dialog tasks. The experimental
results indicate that our MSDF outperforms the baseline models with a large
margin. In the Multi-skill Dialog of 2021 Language and Intelligence Challenge,
our general MSDF won the 3rd prize, which proves our MSDF is effective and
competitive.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：A Graph-Enhanced Click Model for Web Search</b></summary>
  <p><b>编号</b>：[135]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08621</p>
  <p><b>作者</b>：Jianghao Lin,  Weiwen Liu,  Xinyi Dai,  Weinan Zhang,  Shuai Li,  Ruiming Tang,  Xiuqiang He,  Jianye Hao,  Yong Yu</p>
  <p><b>备注</b>：10 pages; Accepted by SIGIR 2021</p>
  <p><b>关键词</b>：users' behavior patterns, extract users' implicit, users' implicit interaction, model users' behavior, implicit interaction feedback</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To better exploit search logs and model users' behavior patterns, numerous
click models are proposed to extract users' implicit interaction feedback. Most
traditional click models are based on the probabilistic graphical model (PGM)
framework, which requires manually designed dependencies and may oversimplify
user behaviors. Recently, methods based on neural networks are proposed to
improve the prediction accuracy of user behaviors by enhancing the expressive
ability and allowing flexible dependencies. However, they still suffer from the
data sparsity and cold-start problems. In this paper, we propose a novel
graph-enhanced click model (GraphCM) for web search. Firstly, we regard each
query or document as a vertex, and propose novel homogeneous graph construction
methods for queries and documents respectively, to fully exploit both
intra-session and inter-session information for the sparsity and cold-start
problems. Secondly, following the examination hypothesis, we separately model
the attractiveness estimator and examination predictor to output the
attractiveness scores and examination probabilities, where graph neural
networks and neighbor interaction techniques are applied to extract the
auxiliary information encoded in the pre-constructed homogeneous graphs.
Finally, we apply combination functions to integrate examination probabilities
and attractiveness scores into click predictions. Extensive experiments
conducted on three real-world session datasets show that GraphCM not only
outperforms the state-of-art models, but also achieves superior performance in
addressing the data sparsity and cold-start problems.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Medical Dialogue Response Generation with Pivotal Information Recalling</b></summary>
  <p><b>编号</b>：[139]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08611</p>
  <p><b>作者</b>：Yu Zhao,  Yunxin Li,  Yuxiang Wu,  Baotian Hu,  Qingcai Chen,  Xiaolong Wang,  Yuxin Ding,  Min Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：challenging task, important yet challenging, dialogue, pivotal information, dialogue graph</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Medical dialogue generation is an important yet challenging task. Most
previous works rely on the attention mechanism and large-scale pretrained
language models. However, these methods often fail to acquire pivotal
information from the long dialogue history to yield an accurate and informative
response, due to the fact that the medical entities usually scatters throughout
multiple utterances along with the complex relationships between them. To
mitigate this problem, we propose a medical response generation model with
Pivotal Information Recalling (MedPIR), which is built on two components, i.e.,
knowledge-aware dialogue graph encoder and recall-enhanced generator. The
knowledge-aware dialogue graph encoder constructs a dialogue graph by
exploiting the knowledge relationships between entities in the utterances, and
encodes it with a graph attention network. Then, the recall-enhanced generator
strengthens the usage of these pivotal information by generating a summary of
the dialogue before producing the actual response. Experimental results on two
large-scale medical dialogue datasets show that MedPIR outperforms the strong
baselines in BLEU scores and medical entities F1 measure.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：An F-shape Click Model for Information Retrieval on Multi-block Mobile  Pages</b></summary>
  <p><b>编号</b>：[144]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08604</p>
  <p><b>作者</b>：Lingyue Fu,  Jianghao Lin,  Weiwen Liu,  Ruiming Tang,  Weinan Zhang,  Rui Zhang,  Yong Yu</p>
  <p><b>备注</b>：Paper under review</p>
  <p><b>关键词</b>：relevance estimation based, provide click simulation, implicit interaction feedback, recent years, click models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To provide click simulation or relevance estimation based on users' implicit
interaction feedback, click models have been much studied during recent years.
Most click models focus on user behaviors towards a single list. However, with
the development of user interface (UI) design, the layout of displayed items on
a result page tends to be multi-block (i.e., multi-list) style instead of a
single list, which requires different assumptions to model user behaviors more
accurately. There exist click models for multi-block pages in desktop contexts,
but they cannot be directly applied to mobile scenarios due to different
interaction manners, result types and especially multi-block presentation
styles. In particular, multi-block mobile pages can normally be decomposed into
interleavings of basic vertical blocks and horizontal blocks, thus resulting in
typically F-shape forms. To mitigate gaps between desktop and mobile contexts
for multi-block pages, we conduct a user eye-tracking study, and identify
users' sequential browsing, block skip and comparison patterns on F-shape
pages. These findings lead to the design of a novel F-shape Click Model (FSCM),
which serves as a general solution to multi-block mobile pages. Firstly, we
construct a directed acyclic graph (DAG) for each page, where each item is
regarded as a vertex and each edge indicates the user's possible examination
flow. Secondly, we propose DAG-structured GRUs and a comparison module to model
users' sequential (sequential browsing, block skip) and non-sequential
(comparison) behaviors respectively. Finally, we combine GRU states and
comparison patterns to perform user click predictions. Experiments on a
large-scale real-world dataset validate the effectiveness of FSCM on user
behavior predictions compared with baseline models.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Bootstrapped Transformer for Offline Reinforcement Learning</b></summary>
  <p><b>编号</b>：[159]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08569</p>
  <p><b>作者</b>：Kerong Wang,  Hanye Zhao,  Xufang Luo,  Kan Ren,  Weinan Zhang,  Dongsheng Li</p>
  <p><b>备注</b>：A complete manuscript under review</p>
  <p><b>关键词</b>：previously collected static, collected static trajectory, Offline reinforcement learning, static trajectory data, reinforcement learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Offline reinforcement learning (RL) aims at learning policies from previously
collected static trajectory data without interacting with the real environment.
Recent works provide a novel perspective by viewing offline RL as a generic
sequence generation problem, adopting sequence models such as Transformer
architecture to model distributions over trajectories, and repurposing beam
search as a planning algorithm. However, the training datasets utilized in
general offline RL tasks are quite limited and often suffer from insufficient
distribution coverage, which could be harmful to training sequence generation
models yet has not drawn enough attention in the previous works. In this paper,
we propose a novel algorithm named Bootstrapped Transformer, which incorporates
the idea of bootstrapping and leverages the learned model to self-generate more
offline data to further boost the sequence model training. We conduct extensive
experiments on two offline RL benchmarks and demonstrate that our model can
largely remedy the existing offline RL training limitations and beat other
strong baseline methods. We also analyze the generated pseudo data and the
revealed characteristics may shed some light on offline RL training. The codes
are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：SOS: Score-based Oversampling for Tabular Data</b></summary>
  <p><b>编号</b>：[168]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08555</p>
  <p><b>作者</b>：Jayoung Kim,  Chaejeong Lee,  Yehjin Shin,  Sewon Park,  Minjung Kim,  Noseong Park,  Jihoon Cho</p>
  <p><b>备注</b>：Accepted by KDD 2022</p>
  <p><b>关键词</b>：generating fake images, generative models, recent breakthrough, Score-based generative models, fake images</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Score-based generative models (SGMs) are a recent breakthrough in generating
fake images. SGMs are known to surpass other generative models, e.g.,
generative adversarial networks (GANs) and variational autoencoders (VAEs).
Being inspired by their big success, in this work, we fully customize them for
generating fake tabular data. In particular, we are interested in oversampling
minor classes since imbalanced classes frequently lead to sub-optimal training
outcomes. To our knowledge, we are the first presenting a score-based tabular
data oversampling method. Firstly, we re-design our own score network since we
have to process tabular data. Secondly, we propose two options for our
generation method: the former is equivalent to a style transfer for tabular
data and the latter uses the standard generative policy of SGMs. Lastly, we
define a fine-tuning method, which further enhances the oversampling quality.
In our experiments with 6 datasets and 10 baselines, our method outperforms
other oversampling methods in all cases.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Accelerating Shapley Explanation via Contributive Cooperator Selection</b></summary>
  <p><b>编号</b>：[176]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08529</p>
  <p><b>作者</b>：Guanchu Wang,  Yu-Neng Chuang,  Mengnan Du,  Fan Yang,  Quan Zhou,  Pushkar Tripathi,  Xuanting Cai,  Xia Hu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：DNN model prediction, exponentially growing complexity, input feature coalitions, DNN model, DNN models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Even though Shapley value provides an effective explanation for a DNN model
prediction, the computation relies on the enumeration of all possible input
feature coalitions, which leads to the exponentially growing complexity. To
address this problem, we propose a novel method SHEAR to significantly
accelerate the Shapley explanation for DNN models, where only a few coalitions
of input features are involved in the computation. The selection of the feature
coalitions follows our proposed Shapley chain rule to minimize the absolute
error from the ground-truth Shapley values, such that the computation can be
both efficient and accurate. To demonstrate the effectiveness, we
comprehensively evaluate SHEAR across multiple metrics including the absolute
error from the ground-truth Shapley value, the faithfulness of the
explanations, and running speed. The experimental results indicate SHEAR
consistently outperforms state-of-the-art baseline methods across different
evaluation metrics, which demonstrates its potentials in real-world
applications where the computational resource is limited.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：SafeRL-Kit: Evaluating Efficient Reinforcement Learning Methods for Safe  Autonomous Driving</b></summary>
  <p><b>编号</b>：[177]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08528</p>
  <p><b>作者</b>：Linrui Zhang,  Qin Zhang,  Li Shen,  Bo Yuan,  Xueqian Wang</p>
  <p><b>备注</b>：The 1st Workshop on Safe Learning for Autonomous Driving (SL4AD) with ICML 2022</p>
  <p><b>关键词</b>：achieved significant success, achieved significant, significant success, success on risk-sensitive, shown promise</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Safe reinforcement learning (RL) has achieved significant success on
risk-sensitive tasks and shown promise in autonomous driving (AD) as well.
Considering the distinctiveness of this community, efficient and reproducible
baselines are still lacking for safe AD. In this paper, we release SafeRL-Kit
to benchmark safe RL methods for AD-oriented tasks. Concretely, SafeRL-Kit
contains several latest algorithms specific to zero-constraint-violation tasks,
including Safety Layer, Recovery RL, off-policy Lagrangian method, and Feasible
Actor-Critic. In addition to existing approaches, we propose a novel
first-order method named Exact Penalty Optimization (EPO) and sufficiently
demonstrate its capability in safe AD. All algorithms in SafeRL-Kit are
implemented (i) under the off-policy setting, which improves sample efficiency
and can better leverage past logs; (ii) with a unified learning framework,
providing off-the-shelf interfaces for researchers to incorporate their
domain-specific knowledge into fundamental safe RL methods. Conclusively, we
conduct a comparative evaluation of the above algorithms in SafeRL-Kit and shed
light on their efficacy for safe autonomous driving. The source code is
available at \href{ this https URL}{this https URL}.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Neural Architecture Adaptation for Object Detection by Searching Channel  Dimensions and Mapping Pre-trained Parameters</b></summary>
  <p><b>编号</b>：[189]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08509</p>
  <p><b>作者</b>：Harim Jung,  Myeong-Seok Oh,  Cheoljong Yang,  Seong-Whan Lee</p>
  <p><b>备注</b>：Accepted to ICPR 2022</p>
  <p><b>关键词</b>：object detection, object detection frameworks, image classification, backbone architectures originally, object</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Most object detection frameworks use backbone architectures originally
designed for image classification, conventionally with pre-trained parameters
on ImageNet. However, image classification and object detection are essentially
different tasks and there is no guarantee that the optimal backbone for
classification is also optimal for object detection. Recent neural architecture
search (NAS) research has demonstrated that automatically designing a backbone
specifically for object detection helps improve the overall accuracy. In this
paper, we introduce a neural architecture adaptation method that can optimize
the given backbone for detection purposes, while still allowing the use of
pre-trained parameters. We propose to adapt both the micro- and
macro-architecture by searching for specific operations and the number of
layers, in addition to the output channel dimensions of each block. It is
important to find the optimal channel depth, as it greatly affects the feature
representation capability and computation cost. We conduct experiments with our
searched backbone for object detection and demonstrate that our backbone
outperforms both manually designed and searched state-of-the-art backbones on
the COCO dataset.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：A Parametric Class of Approximate Gradient Updates for Policy  Optimization</b></summary>
  <p><b>编号</b>：[193]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08499</p>
  <p><b>作者</b>：Ramki Gummadi,  Saurabh Kumar,  Junfeng Wen,  Dale Schuurmans</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：maximizing expected return, versus policy representation, diverse principles, model is interpreted, objective is formulated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Approaches to policy optimization have been motivated from diverse
principles, based on how the parametric model is interpreted (e.g. value versus
policy representation) or how the learning objective is formulated, yet they
share a common goal of maximizing expected return. To better capture the
commonalities and identify key differences between policy optimization methods,
we develop a unified perspective that re-expresses the underlying updates in
terms of a limited choice of gradient form and scaling function. In particular,
we identify a parameterized space of approximate gradient updates for policy
optimization that is highly structured, yet covers both classical and recent
examples, including PPO. As a result, we obtain novel yet well motivated
updates that generalize existing algorithms in a way that can deliver benefits
both in terms of convergence speed and final result quality. An experimental
investigation demonstrates that the additional degrees of freedom provided in
the parameterized family of updates can be leveraged to obtain non-trivial
improvements both in synthetic domains and on popular deep RL benchmarks.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Self-Supervised Contrastive Pre-Training For Time Series via  Time-Frequency Consistency</b></summary>
  <p><b>编号</b>：[195]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08496</p>
  <p><b>作者</b>：Xiang Zhang,  Ziyuan Zhao,  Theodoros Tsiligkaridis,  Marinka Zitnik</p>
  <p><b>备注</b>：Under review; the anonymouse code repo link will be made non-anonymous after acceptance; 21 pages (13 pages main paper + 8 pages supplementary materials)</p>
  <p><b>关键词</b>：short cyclic effects, poor downstream performance, unique challenge due, fast-evolving trends, cyclic effects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pre-training on time series poses a unique challenge due to the potential
mismatch between pre-training and target domains, such as shifts in temporal
dynamics, fast-evolving trends, and long-range and short cyclic effects, which
can lead to poor downstream performance. While domain adaptation methods can
mitigate these shifts, most methods need examples directly from the target
domain, making them suboptimal for pre-training. To address this challenge,
methods need to accommodate target domains with different temporal dynamics and
be capable of doing so without seeing any target examples during pre-training.
Relative to other modalities, in time series, we expect that time-based and
frequency-based representations of the same example are located close together
in the time-frequency space. To this end, we posit that time-frequency
consistency (TF-C) -- embedding a time-based neighborhood of a particular
example close to its frequency-based neighborhood and back -- is desirable for
pre-training. Motivated by TF-C, we define a decomposable pre-training model,
where the self-supervised signal is provided by the distance between time and
frequency components, each individually trained by contrastive estimation. We
evaluate the new method on eight datasets, including electrodiagnostic testing,
human activity recognition, mechanical fault detection, and physical status
monitoring. Experiments against eight state-of-the-art methods show that TF-C
outperforms baselines by 15.4% (F1 score) on average in one-to-one settings
(e.g., fine-tuning an EEG-pretrained model on EMG data) and by up to 8.4% (F1
score) in challenging one-to-many settings, reflecting the breadth of scenarios
that arise in real-world applications. The source code and datasets are
available at https: //anonymous.4open.science/r/TFC-pretraining-6B07.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Yankee Swap: a Fast and Simple Fair Allocation Mechanism for Matroid  Rank Valuations</b></summary>
  <p><b>编号</b>：[196]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08495</p>
  <p><b>作者</b>：Vignesh Viswanathan,  Yair Zick</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：matroid rank valuations, rank valuations, indivisible goods, goods when agents, Lorenz dominating allocations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study fair allocation of indivisible goods when agents have matroid rank
valuations. Our main contribution is a simple algorithm based on the colloquial
Yankee Swap procedure that computes provably fair and efficient Lorenz
dominating allocations. While there exist polynomial time algorithms to compute
such allocations, our proposed method improves on them in two ways. (a) Our
approach is easy to understand and does not use complex matroid optimization
algorithms as subroutines. (b) Our approach is scalable; it is provably faster
than all known algorithms to compute Lorenz dominating allocations. These two
properties are key to the adoption of algorithms in any real fair allocation
setting; our contribution brings us one step closer to this goal.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Factorization Approach for Sparse Spatio-Temporal Brain-Computer  Interface</b></summary>
  <p><b>编号</b>：[197]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08494</p>
  <p><b>作者</b>：Byeong-Hoo Lee,  Jeong-Hyun Cho,  Byoung-Hee Kwon,  Seong-Whan Lee</p>
  <p><b>备注</b>：8 pages</p>
  <p><b>关键词</b>：unlimited potential, potential in solving, advanced technologies, technologies have unlimited, amount of data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, advanced technologies have unlimited potential in solving various
problems with a large amount of data. However, these technologies have yet to
show competitive performance in brain-computer interfaces (BCIs) which deal
with brain signals. Basically, brain signals are difficult to collect in large
quantities, in particular, the amount of information would be sparse in
spontaneous BCIs. In addition, we conjecture that high spatial and temporal
similarities between tasks increase the prediction difficulty. We define this
problem as sparse condition. To solve this, a factorization approach is
introduced to allow the model to obtain distinct representations from latent
space. To this end, we propose two feature extractors: A class-common module is
trained through adversarial learning acting as a generator; Class-specific
module utilizes loss function generated from classification so that features
are extracted with traditional methods. To minimize the latent space shared by
the class-common and class-specific features, the model is trained under
orthogonal constraint. As a result, EEG signals are factorized into two
separate latent spaces. Evaluations were conducted on a single-arm motor
imagery dataset. From the results, we demonstrated that factorizing the EEG
signal allows the model to extract rich and decisive features under sparse
condition.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：TKIL: Tangent Kernel Approach for Class Balanced Incremental Learning</b></summary>
  <p><b>编号</b>：[199]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08492</p>
  <p><b>作者</b>：Jinlin Xiang,  Eli Shlizerman</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：called catastrophic forgetting, phenomenon called catastrophic, previously learned tasks, incremental learning, Tangent Kernel</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>When learning new tasks in a sequential manner, deep neural networks tend to
forget tasks that they previously learned, a phenomenon called catastrophic
forgetting. Class incremental learning methods aim to address this problem by
keeping a memory of a few exemplars from previously learned tasks, and
distilling knowledge from them. However, existing methods struggle to balance
the performance across classes since they typically overfit the model to the
latest task. In our work, we propose to address these challenges with the
introduction of a novel methodology of Tangent Kernel for Incremental Learning
(TKIL) that achieves class-balanced performance. The approach preserves the
representations across classes and balances the accuracy for each class, and as
such achieves better overall accuracy and variance. TKIL approach is based on
Neural Tangent Kernel (NTK), which describes the convergence behavior of neural
networks as a kernel function in the limit of infinite width. In TKIL, the
gradients between feature layers are treated as the distance between the
representations of these layers and can be defined as Gradients Tangent Kernel
loss (GTK loss) such that it is minimized along with averaging weights. This
allows TKIL to automatically identify the task and to quickly adapt to it
during inference. Experiments on CIFAR-100 and ImageNet datasets with various
incremental learning settings show that these strategies allow TKIL to
outperform existing state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Debugging using Orthogonal Gradient Descent</b></summary>
  <p><b>编号</b>：[201]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08489</p>
  <p><b>作者</b>：Narsimha Chilkuri,  Chris Eliasmith</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：partially faulty, Orthogonal Gradient Descent, trained model, continual learning problem, model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this report we consider the following problem: Given a trained model that
is partially faulty, can we correct its behaviour without having to train the
model from scratch? In other words, can we ``debug" neural networks similar to
how we address bugs in our mathematical models and standard computer code. We
base our approach on the hypothesis that debugging can be treated as a two-task
continual learning problem. In particular, we employ a modified version of a
continual learning algorithm called Orthogonal Gradient Descent (OGD) to
demonstrate, via two simple experiments on the MNIST dataset, that we can
in-fact \textit{unlearn} the undesirable behaviour while retaining the general
performance of the model, and we can additionally \textit{relearn} the
appropriate behaviour, both without having to train the model from scratch.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：High-Speed Accurate Robot Control using Learned Forward Kinodynamics and  Non-linear Least Squares Optimization</b></summary>
  <p><b>编号</b>：[203]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08487</p>
  <p><b>作者</b>：Pranav Atreya,  Haresh Karnan,  Kavan Singh Sikand,  Xuesu Xiao,  Garrett Warnell,  Sadegh Rabiee,  Peter Stone,  Joydeep Biswas</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：real world requires, high-speed robot control, control, robot control, robot</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Accurate control of robots in the real world requires a control system that
is capable of taking into account the kinodynamic interactions of the robot
with its environment. At high speeds, the dependence of the movement of the
robot on these kinodynamic interactions becomes more pronounced, making
high-speed, accurate robot control a challenging problem. Previous work has
shown that learning the inverse kinodynamics (IKD) of the robot can be helpful
for high-speed robot control. However a learned inverse kinodynamic model can
only be applied to a limited class of control problems, and different control
problems require the learning of a new IKD model. In this work we present a new
formulation for accurate, high-speed robot control that makes use of a learned
forward kinodynamic (FKD) model and non-linear least squares optimization. By
nature of the formulation, this approach is extensible to a wide array of
control problems without requiring the retraining of a new model. We
demonstrate the ability of this approach to accurately control a scale
one-tenth robot car at high speeds, and show improved results over baselines.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Zero-Shot AutoML with Pretrained Models</b></summary>
  <p><b>编号</b>：[210]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08476</p>
  <p><b>作者</b>：Ekrem Öztürk,  Fabio Ferreira,  Hadi S. Jomaa,  Lars Schmidt-Thieme,  Josif Grabocka,  Frank Hutter</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：low compute budget, compute budget, risking overfitting, low compute, fine-tuning hyperparameters</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Given a new dataset D and a low compute budget, how should we choose a
pre-trained model to fine-tune to D, and set the fine-tuning hyperparameters
without risking overfitting, particularly if D is small? Here, we extend
automated machine learning (AutoML) to best make these choices. Our
domain-independent meta-learning approach learns a zero-shot surrogate model
which, at test time, allows to select the right deep learning (DL) pipeline
(including the pre-trained model and fine-tuning hyperparameters) for a new
dataset D given only trivial meta-features describing D such as image
resolution or the number of classes. To train this zero-shot model, we collect
performance data for many DL pipelines on a large collection of datasets and
meta-train on this data to minimize a pairwise ranking objective. We evaluate
our approach under the strict time limit of the vision track of the ChaLearn
AutoDL challenge benchmark, clearly outperforming all challenge contenders.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence</b></summary>
  <p><b>编号</b>：[211]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08474</p>
  <p><b>作者</b>：Ming Zhu,  Aneesh Jain,  Karthik Suresh,  Roshan Ravindran,  Sindhu Tipirneni,  Chandan K. Reddy</p>
  <p><b>备注</b>：20 pages, 11 tables, 2 figures</p>
  <p><b>关键词</b>：achieved good performance, Recent advances, code, advances in machine, machine learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advances in machine learning have significantly improved the
understanding of source code data and achieved good performance on a number of
downstream tasks. Open source repositories like GitHub enable this process with
rich unlabeled code data. However, the lack of high quality labeled data has
largely hindered the progress of several code related tasks, such as program
translation, summarization, synthesis, and code search. This paper introduces
XLCoST, Cross-Lingual Code SnippeT dataset, a new benchmark dataset for
cross-lingual code intelligence. Our dataset contains fine-grained parallel
data from 8 languages (7 commonly used programming languages and English), and
supports 10 cross-lingual code tasks. To the best of our knowledge, it is the
largest parallel dataset for source code both in terms of size and the number
of languages. We also provide the performance of several state-of-the-art
baseline models for each task. We believe this new dataset can be a valuable
asset for the research community and facilitate the development and validation
of new methods for cross-lingual code intelligence.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Belief-Desire-Intention (BDI) Multi-agent System for Cloud Marketplace  Negotiation</b></summary>
  <p><b>编号</b>：[214]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08468</p>
  <p><b>作者</b>：Saurabh Deochake</p>
  <p><b>备注</b>：19th International Conference on Distributed Computing and Artificial Intelligence</p>
  <p><b>关键词</b>：cloud marketplace system, large enterprises extending, marketplace system, rise of large, extending their infrastructure</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the evolution of cloud computing, there has been a rise of large
enterprises extending their infrastructure and workloads into the public cloud.
This paper proposes a full-fledged framework for a Belief-Desire-Intention
(BDI) multi-agent-based cloud marketplace system for cloud resources. Each
party in the cloud marketplace system supports a BDI agent for autonomous
decision making and negotiation to facilitate automated buying and selling of
resources. Additionally, multiple BDI agents from an enterprise competing for
the same cloud resource can consult with each other via Master Negotiation
Clearing House to minimize the overall cost function for the enterprise while
negotiating for a cloud resource. The cloud marketplace system is further
augmented with assignments of behavior norm and reputation index to the agents
to facilitate trust among them.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Quantifying Feature Contributions to Overall Disparity Using Information  Theory</b></summary>
  <p><b>编号</b>：[219]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08454</p>
  <p><b>作者</b>：Sanghamitra Dutta,  Praveen Venkatesh,  Pulkit Grover</p>
  <p><b>备注</b>：Presented at the AAAI-22 Workshop on Information-Theoretic Methods for Causal Inference and Discovery in March 2022</p>
  <p><b>关键词</b>：machine-learning algorithm makes, algorithm makes biased, makes biased decisions, individual feature, bias exists</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>When a machine-learning algorithm makes biased decisions, it can be helpful
to understand the sources of disparity to explain why the bias exists. Towards
this, we examine the problem of quantifying the contribution of each individual
feature to the observed disparity. If we have access to the decision-making
model, one potential approach (inspired from intervention-based approaches in
explainability literature) is to vary each individual feature (while keeping
the others fixed) and use the resulting change in disparity to quantify its
contribution. However, we may not have access to the model or be able to
test/audit its outputs for individually varying features. Furthermore, the
decision may not always be a deterministic function of the input features
(e.g., with human-in-the-loop). For these situations, we might need to explain
contributions using purely distributional (i.e., observational) techniques,
rather than interventional. We ask the question: what is the "potential"
contribution of each individual feature to the observed disparity in the
decisions when the exact decision-making mechanism is not accessible? We first
provide canonical examples (thought experiments) that help illustrate the
difference between distributional and interventional approaches to explaining
contributions, and when either is better suited. When unable to intervene on
the inputs, we quantify the "redundant" statistical dependency about the
protected attribute that is present in both the final decision and an
individual feature, by leveraging a body of work in information theory called
Partial Information Decomposition. We also perform a simple case study to show
how this technique could be applied to quantify contributions.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：GOOD: A Graph Out-of-Distribution Benchmark</b></summary>
  <p><b>编号</b>：[220]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08452</p>
  <p><b>作者</b>：Shurui Gui,  Xiner Li,  Limei Wang,  Shuiwang Ji</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：test data follow, OOD, follow different distributions, deals with scenarios, training and test</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Out-of-distribution (OOD) learning deals with scenarios in which training and
test data follow different distributions. Although general OOD problems have
been intensively studied in machine learning, graph OOD is only an emerging
area of research. Currently, there lacks a systematic benchmark tailored to
graph OOD method evaluation. In this work, we aim at developing an OOD
benchmark, known as GOOD, for graphs specifically. We explicitly make
distinctions between covariate and concept shifts and design data splits that
accurately reflect different shifts. We consider both graph and node prediction
tasks as there are key differences when designing shifts. Overall, GOOD
contains 8 datasets with 14 domain selections. When combined with covariate,
concept, and no shifts, we obtain 42 different splits. We provide performance
results on 7 commonly used baseline methods with 10 random runs. This results
in 294 dataset-model combinations in total. Our results show significant
performance gaps between in-distribution and OOD settings. Our results also
shed light on different performance trends between covariate and concept shifts
by different methods. Our GOOD benchmark is a growing project and expects to
expand in both quantity and variety of resources as the area develops. The GOOD
benchmark can be accessed via
$\href{this https URL}{\text{this https URL}}$.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：I Know What You Trained Last Summer: A Survey on Stealing Machine  Learning Models and Defences</b></summary>
  <p><b>编号</b>：[221]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08451</p>
  <p><b>作者</b>：Daryna Oliynyk,  Rudolf Mayer,  Andreas Rauber</p>
  <p><b>备注</b>：Under review at ACM Computing Surveys</p>
  <p><b>关键词</b>：complex machine learning, machine learning models, complex machine, machine learning, widespread paradigm</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine Learning-as-a-Service (MLaaS) has become a widespread paradigm,
making even the most complex machine learning models available for clients via
e.g. a pay-per-query principle. This allows users to avoid time-consuming
processes of data collection, hyperparameter tuning, and model training.
However, by giving their customers access to the (predictions of their) models,
MLaaS providers endanger their intellectual property, such as sensitive
training data, optimised hyperparameters, or learned model parameters.
Adversaries can create a copy of the model with (almost) identical behavior
using the the prediction labels only. While many variants of this attack have
been described, only scattered defence strategies have been proposed,
addressing isolated threats. This raises the necessity for a thorough
systematisation of the field of model stealing, to arrive at a comprehensive
understanding why these attacks are successful, and how they could be
holistically defended against. We address this by categorising and comparing
model stealing attacks, assessing their performance, and exploring
corresponding defence techniques in different settings. We propose a taxonomy
for attack and defence approaches, and provide guidelines on how to select the
right attack or defence strategy based on the goal and available resources.
Finally, we analyse which defences are rendered less effective by current
attack strategies.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Methods for Estimating and Improving Robustness of Language Models</b></summary>
  <p><b>编号</b>：[224]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08446</p>
  <p><b>作者</b>：Michal Štefánik</p>
  <p><b>备注</b>：Thesis proposal, accepted & to appear in NAACL SRW 2022</p>
  <p><b>关键词</b>：suffer notorious flaws, surface-level textual relations, notorious flaws related, full semantic complexity, large language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite their outstanding performance, large language models (LLMs) suffer
notorious flaws related to their preference for simple, surface-level textual
relations over full semantic complexity of the problem. This proposal
investigates a common denominator of this problem in their weak ability to
generalise outside of the training domain. We survey diverse research
directions providing estimations of model generalisation ability and find that
incorporating some of these measures in the training objectives leads to
enhanced distributional robustness of neural models. Based on these findings,
we present future research directions towards enhancing the robustness of LLMs.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Understanding Decision-Time vs. Background Planning in Model-Based  Reinforcement Learning</b></summary>
  <p><b>编号</b>：[226]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08442</p>
  <p><b>作者</b>：Safa Alver,  Doina Precup</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：model-based reinforcement learning, planning, model-based reinforcement, agent can leverage, leverage a learned</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In model-based reinforcement learning, an agent can leverage a learned model
to improve its way of behaving in different ways. Two prevalent approaches are
decision-time planning and background planning. In this study, we are
interested in understanding under what conditions and in which settings one of
these two planning styles will perform better than the other in domains that
require fast responses. After viewing them through the lens of dynamic
programming, we first consider the classical instantiations of these planning
styles and provide theoretical results and hypotheses on which one will perform
better in the pure planning, planning & learning, and transfer learning
settings. We then consider the modern instantiations of these planning styles
and provide hypotheses on which one will perform better in the last two of the
considered settings. Lastly, we perform several illustrative experiments to
empirically validate both our theoretical results and hypotheses. Overall, our
findings suggest that even though decision-time planning does not perform as
well as background planning in their classical instantiations, in their modern
instantiations, it can perform on par or better than background planning in
both the planning & learning and transfer learning settings.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：The Case for a Wholistic Serverless Programming Paradigm and Full Stack  Automation for AI and Beyond -- The Philosophy of Jaseci and Jac</b></summary>
  <p><b>编号</b>：[229]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08434</p>
  <p><b>作者</b>：Jason Mars</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：wholistic top-down re-envisioning, system stack architecture, system stack, programming language level, system stack subsume</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, the case is made for a wholistic top-down re-envisioning of the
system stack from the programming language level down through the system
architecture to bridge this complexity gap. The key goal of our design is to
address the critical need for the programmer to articulate solutions with
higher level abstractions at the problem level while having the runtime system
stack subsume and hide a broad scope of diffuse sub-applications and
inter-machine resources. This work also presents the design of a
production-grade realization of such a system stack architecture called Jaseci,
and corresponding programming language Jac. Jac and Jaseci has been released as
open source and has been leveraged by real product teams to accelerate
developing and deploying sophisticated AI products and other applications at
scale. Jac has been utilized in commercial production environments to
accelerate AI development timelines by ~10x, with the Jaseci runtime automating
the decisions and optimizations typically falling in the scope of manual
engineering roles on a team such as what should and should not be a
microservice and changing those dynamically.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Predicting Hate Intensity of Twitter Conversation Threads</b></summary>
  <p><b>编号</b>：[244]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08406</p>
  <p><b>作者</b>：Qing Meng,  Tharun Suresh,  Roy Ka-Wei Lee,  Tanmoy Chakraborty</p>
  <p><b>备注</b>：22 pages, 10 figures, 3 tables</p>
  <p><b>关键词</b>：online social media, social media, hate speech, Online hate speech, social media companies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Tweets are the most concise form of communication in online social media,
wherein a single tweet has the potential to make or break the discourse of the
conversation. Online hate speech is more accessible than ever, and stifling its
propagation is of utmost importance for social media companies and users for
congenial communication. Most of the research barring a recent few has focused
on classifying an individual tweet regardless of the tweet thread/context
leading up to that point. One of the classical approaches to curb hate speech
is to adopt a reactive strategy after the hate speech postage. The ex-post
facto strategy results in neglecting subtle posts that do not show the
potential to instigate hate speech on their own but may portend in the
subsequent discussion ensuing in the post's replies. In this paper, we propose
DRAGNET++, which aims to predict the intensity of hatred that a tweet can bring
in through its reply chain in the future. It uses the semantic and propagating
structure of the tweet threads to maximize the contextual information leading
up to and the fall of hate intensity at each subsequent tweet. We explore three
publicly available Twitter datasets -- Anti-Racism contains the reply tweets of
a collection of social media discourse on racist remarks during US political
and Covid-19 background; Anti-Social presents a dataset of 40 million tweets
amidst the COVID-19 pandemic on anti-social behaviours; and Anti-Asian presents
Twitter datasets collated based on anti-Asian behaviours during COVID-19
pandemic. All the curated datasets consist of structural graph information of
the Tweet threads. We show that DRAGNET++ outperforms all the state-of-the-art
baselines significantly. It beats the best baseline by an 11\% margin on the
Person correlation coefficient and a decrease of 25\% on RMSE for the
Anti-Racism dataset with a similar performance on the other two datasets.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Learning to Teach Fairness-aware Deep Multi-task Learning</b></summary>
  <p><b>编号</b>：[246]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08403</p>
  <p><b>作者</b>：Arjun Roy,  Eirini Ntoutsi</p>
  <p><b>备注</b>：Accepted to be published in the Proceedings of the "European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECMLPKDD)", Sept. 19th to 23rd 2022</p>
  <p><b>关键词</b>：single task learning, focuses on single, Fairness-aware learning, STL, MTL</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Fairness-aware learning mainly focuses on single task learning (STL). The
fairness implications of multi-task learning (MTL) have only recently been
considered and a seminal approach has been proposed that considers the
fairness-accuracy trade-off for each task and the performance trade-off among
different tasks. Instead of a rigid fairness-accuracy trade-off formulation, we
propose a flexible approach that learns how to be fair in a MTL setting by
selecting which objective (accuracy or fairness) to optimize at each step. We
introduce the L2T-FMT algorithm that is a teacher-student network trained
collaboratively; the student learns to solve the fair MTL problem while the
teacher instructs the student to learn from either accuracy or fairness,
depending on what is harder to learn for each task. Moreover, this dynamic
selection of which objective to use at each step for each task reduces the
number of trade-off weights from 2T to T, where T is the number of tasks. Our
experiments on three real datasets show that L2T-FMT improves on both fairness
(12-19%) and accuracy (up to 2%) over state-of-the-art approaches.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Adapting the Linearised Laplace Model Evidence for Modern Deep Learning</b></summary>
  <p><b>编号</b>：[255]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08900</p>
  <p><b>作者</b>：Javier Antorán,  David Janz,  James Urquhart Allingham,  Erik Daxberger,  Riccardo Barbano,  Eric Nalisnick,  José Miguel Hernández-Lobato</p>
  <p><b>备注</b>：Paper appearing at ICML 2022</p>
  <p><b>关键词</b>：linearised Laplace method, received renewed attention, Bayesian deep learning, linearised Laplace, estimating model uncertainty</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The linearised Laplace method for estimating model uncertainty has received
renewed attention in the Bayesian deep learning community. The method provides
reliable error bars and admits a closed-form expression for the model evidence,
allowing for scalable selection of model hyperparameters. In this work, we
examine the assumptions behind this method, particularly in conjunction with
model selection. We show that these interact poorly with some now-standard
tools of deep learning--stochastic approximation methods and normalisation
layers--and make recommendations for how to better adapt this classic method to
the modern setting. We provide theoretical support for our recommendations and
validate them empirically on MLPs, classic CNNs, residual networks with and
without normalisation layers, generative autoencoders and transformers.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：The Sensorium competition on predicting large-scale mouse primary visual  cortex activity</b></summary>
  <p><b>编号</b>：[272]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08666</p>
  <p><b>作者</b>：Konstantin F. Willeke (1 and 2 and 3),  Paul G. Fahey (4 and 5),  Mohammad Bashiri (1 and 2 and 3),  Laura Pede (3),  Max F. Burg (1 and 2 and 3 and 6),  Christoph Blessing (3),  Santiago A. Cadena (1 and 3 and 6),  Zhiwei Ding (4 and 5),  Konstantin-Klemens Lurz (1 and 2 and 3),  Kayla Ponder (4 and 5),  Taliah Muhammad (4 and 5),  Saumil S. Patel (4 and 5),  Alexander S. Ecker (3 and 7),  Andreas S. Tolias (4 and 5 and 8),  Fabian H. Sinz (2 and 3 and 4 and 5) ((1) International Max Planck Research School for Intelligent Systems, University of Tuebingen, Germany, (2) Institute for Bioinformatics and Medical Informatics, University of Tuebingen, Germany (3) Institute of Computer Science and Campus Institute Data Science, University of Goettingen, Germany, (4) Department of Neuroscience, Baylor College of Medicine, Houston, USA, (5) Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine, Houston, USA, (6) Institute for Theoretical Physics, University of Tuebingen, Germany, (7) Max Planck Institute for Dynamics and Self-Organization, Goettingen, Germany, (8) Electrical and Computer Engineering, Rice University, Houston, USA)</p>
  <p><b>备注</b>：NeurIPS 2022 Competition Track</p>
  <p><b>关键词</b>：mouse visual system, activity becomes increasingly, increasingly nonlinear, nonlinear with respect, biological visual system</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The neural underpinning of the biological visual system is challenging to
study experimentally, in particular as the neuronal activity becomes
increasingly nonlinear with respect to visual input. Artificial neural networks
(ANNs) can serve a variety of goals for improving our understanding of this
complex system, not only serving as predictive digital twins of sensory cortex
for novel hypothesis generation in silico, but also incorporating bio-inspired
architectural motifs to progressively bridge the gap between biological and
machine vision. The mouse has recently emerged as a popular model system to
study visual information processing, but no standardized large-scale benchmark
to identify state-of-the-art models of the mouse visual system has been
established. To fill this gap, we propose the Sensorium benchmark competition.
We collected a large-scale dataset from mouse primary visual cortex containing
the responses of more than 28,000 neurons across seven mice stimulated with
thousands of natural images, together with simultaneous behavioral measurements
that include running speed, pupil dilation, and eye movements. The benchmark
challenge will rank models based on predictive performance for neuronal
responses on a held-out test set, and includes two tracks for model input
limited to either stimulus only (Sensorium) or stimulus plus behavior
(Sensorium+). We provide a starting kit to lower the barrier for entry,
including tutorials, pre-trained baseline models, and APIs with one line
commands for data loading and submission. We would like to see this as a
starting point for regular challenges and data releases, and as a standard tool
for measuring progress in large-scale neural system identification models of
the mouse visual system and beyond.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Learning Generic Lung Ultrasound Biomarkers for Decoupling Feature  Extraction from Downstream Tasks</b></summary>
  <p><b>编号</b>：[290]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2206.08398</p>
  <p><b>作者</b>：Gautam Rajendrakumar Gare,  Tom Fox,  Pete Lowery,  Kevin Zamora,  Hai V. Tran,  Laura Hutchins,  David Montgomery,  Amita Krishnan,  Deva Kannan Ramanan,  Ricardo Luis Rodriguez,  Bennett P deBoisblanc,  John Michael Galeotti</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Contemporary artificial neural, artificial neural networks, Contemporary artificial, ANN, artificial neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Contemporary artificial neural networks (ANN) are trained end-to-end, jointly
learning both features and classifiers for the task of interest. Though
enormously effective, this paradigm imposes significant costs in assembling
annotated task-specific datasets and training large-scale networks. We propose
to decouple feature learning from downstream lung ultrasound tasks by
introducing an auxiliary pre-task of visual biomarker classification. We
demonstrate that one can learn an informative, concise, and interpretable
feature space from ultrasound videos by training models for predicting
biomarker labels. Notably, biomarker feature extractors can be trained from
data annotated with weak video-scale supervision. These features can be used by
a variety of downstream Expert models targeted for diverse clinical tasks
(Diagnosis, lung severity, S/F ratio). Crucially, task-specific expert models
are comparable in accuracy to end-to-end models directly trained for such
target tasks, while being significantly lower cost to train.</p>
  </details>
</details>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">徐耀彬</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://louishsu.xyz/2022/06/21/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">http://louishsu.xyz/2022/06/21/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://louishsu.xyz" target="_blank">LOUIS' BLOG</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2021/10/22/%E4%B8%AD%E5%9B%BD%E6%B3%95%E5%BE%8B%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E8%AF%84%E6%B5%8B(CAIL2021)%EF%BC%9A%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96(Rank2).html"><img class="next-cover" src="http://cail.cipsc.org.cn/img/index_mainpic.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">徐耀彬</div><div class="author-info__description">做知识的原创者！</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">11</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/isLouisHsu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/isLouisHsu" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:is.louishsu@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">记录和分享一些学习和开源内容，若有问题可通过邮箱is.louishsu@foxmail.com联系，欢迎交流！！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">统计</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">计算机视觉</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">自然语言处理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">机器学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">人工智能</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/06/21/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2022-06-21)"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv每日速递(2022-06-21)"/></a><div class="content"><a class="title" href="/2022/06/21/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2022-06-21)">Arxiv每日速递(2022-06-21)</a><time datetime="2022-06-21T00:42:49.088Z" title="发表于 2022-06-21 08:42:49">2022-06-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/10/22/%E4%B8%AD%E5%9B%BD%E6%B3%95%E5%BE%8B%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E8%AF%84%E6%B5%8B(CAIL2021)%EF%BC%9A%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96(Rank2).html" title="中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)"><img src="http://cail.cipsc.org.cn/img/index_mainpic.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)"/></a><div class="content"><a class="title" href="/2021/10/22/%E4%B8%AD%E5%9B%BD%E6%B3%95%E5%BE%8B%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E8%AF%84%E6%B5%8B(CAIL2021)%EF%BC%9A%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96(Rank2).html" title="中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)">中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)</a><time datetime="2021-10-22T14:29:06.000Z" title="发表于 2021-10-22 22:29:06">2021-10-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/05/19/%E5%85%A8%E7%90%83%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0%E5%A4%A7%E8%B5%9B%E3%80%90%E8%B5%9B%E9%81%93%E4%B8%80%E3%80%91%EF%BC%9A%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E6%8A%A5%E5%91%8A%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B.html" title="全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测"><img src="https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161037709574435991610377095138.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测"/></a><div class="content"><a class="title" href="/2021/05/19/%E5%85%A8%E7%90%83%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0%E5%A4%A7%E8%B5%9B%E3%80%90%E8%B5%9B%E9%81%93%E4%B8%80%E3%80%91%EF%BC%9A%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E6%8A%A5%E5%91%8A%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B.html" title="全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测">全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测</a><time datetime="2021-05-19T09:57:06.000Z" title="发表于 2021-05-19 17:57:06">2021-05-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/09/16/%E8%AF%A6%E8%A7%A3%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%EF%BC%9ALSTM-CRF.html" title="详解命名实体识别模型：LSTM-CRF"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fpic1.zhimg.com%2Fv2-1ed6a10dbb239a148e42df088c5870a6_1440w.jpg%3Fsource%3D172ae18b&amp;refer=http%3A%2F%2Fpic1.zhimg.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1638183974&amp;t=4632f13fd487ed1cfcb12d67a6b71e52" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="详解命名实体识别模型：LSTM-CRF"/></a><div class="content"><a class="title" href="/2020/09/16/%E8%AF%A6%E8%A7%A3%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%EF%BC%9ALSTM-CRF.html" title="详解命名实体识别模型：LSTM-CRF">详解命名实体识别模型：LSTM-CRF</a><time datetime="2020-09-16T09:26:44.000Z" title="发表于 2020-09-16 17:26:44">2020-09-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/06/14/%E8%AE%B0%E4%B8%80%E6%AC%A1MySQL%E8%A7%A3%E5%86%B3%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E6%97%B6%E7%9A%84%E5%88%86%E8%A1%A8%E9%97%AE%E9%A2%98.html" title="记一次MySQL解决大数据存储时的分表问题"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="记一次MySQL解决大数据存储时的分表问题"/></a><div class="content"><a class="title" href="/2020/06/14/%E8%AE%B0%E4%B8%80%E6%AC%A1MySQL%E8%A7%A3%E5%86%B3%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E6%97%B6%E7%9A%84%E5%88%86%E8%A1%A8%E9%97%AE%E9%A2%98.html" title="记一次MySQL解决大数据存储时的分表问题">记一次MySQL解决大数据存储时的分表问题</a><time datetime="2020-06-14T15:24:27.000Z" title="发表于 2020-06-14 23:24:27">2020-06-14</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2022 By 徐耀彬</div><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender"title="主题采用butterfly"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr"title="本站使用JsDelivr为静态资源提供CDN加速"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub"title="本站项目由Gtihub托管"alt="img"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"alt="img"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></br></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    let initData = {
      el: '#twikoo-wrap',
      envId: 'blog-',
      region: ''
    }

    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    twikoo.init(initData)
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'blog-',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/机器学习/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐱 机器学习 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/自然语言处理/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 自然语言处理 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/竞赛相关/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 竞赛相关 (2)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/阅读笔记/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 阅读笔记 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="http://louishsu.xyz/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style>
  <script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/hexo-github-calendar@1.21/hexo_githubcalendar.js"></script>
  <script data-pjax>
        function GithubCalendarConfig(){
            var git_githubapiurl ="https://python-github-calendar-api.vercel.app/api?isLouisHsu";
            var git_color =['#ebedf0', '#fdcdec', '#fc9bd9', '#fa6ac5', '#f838b2', '#f5089f', '#c4067e', '#92055e', '#540336', '#48022f', '#30021f'];
            var git_user ="isLouisHsu";
            var parent_div_git = document.getElementById('recent-posts');
            var git_div_html = '<div class="recent-post-item" style="width:100%;height:auto;padding:10px;"><div id="github_loading" style="width:10%;height:100%;margin:0 auto;display: block"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"  viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animateTransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animateTransform></path></svg></div><div id="github_container"></div></div>';
            if(parent_div_git && location.pathname =='/'){
                console.log('已挂载github calendar')
                // parent_div_git.innerHTML=git_div_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",git_div_html) // 有报错，但不影响使用(支持pjax跳转)
            };
            GithubCalendar(git_githubapiurl,git_color,git_user)
        }
        if(document.getElementById('recent-posts')){
            GithubCalendarConfig()
        }
    </script>
    <style>#github_container{min-height:280px}@media screen and (max-width:650px) {#github_container{background-image:;min-height:0px}}</style>
    <style></style><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>