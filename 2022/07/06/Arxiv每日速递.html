<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Arxiv每日速递(2022-07-06) | LOUIS' BLOG</title><meta name="author" content="徐耀彬"><meta name="copyright" content="徐耀彬"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。 统计 今日共更新251篇论文，其中：  83篇计算机视觉（cs.CV） 22篇自然语言处理（cs.CL） 77篇机器学习（cs.LG） 40篇人工智能（cs.AI）  计算机视觉    1. 标题：Segmenting Moving Objects via an Objec">
<meta property="og:type" content="article">
<meta property="og:title" content="Arxiv每日速递(2022-07-06)">
<meta property="og:url" content="http://louishsu.xyz/2022/07/06/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">
<meta property="og:site_name" content="LOUIS&#39; BLOG">
<meta property="og:description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。 统计 今日共更新251篇论文，其中：  83篇计算机视觉（cs.CV） 22篇自然语言处理（cs.CL） 77篇机器学习（cs.LG） 40篇人工智能（cs.AI）  计算机视觉    1. 标题：Segmenting Moving Objects via an Objec">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png">
<meta property="article:published_time" content="2022-07-06T00:48:42.989Z">
<meta property="article:modified_time" content="2022-07-06T00:50:29.531Z">
<meta property="article:author" content="徐耀彬">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://louishsu.xyz/2022/07/06/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-07-06 08:50:29'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/background.css"><script src="https://cdn.jsdelivr.net/npm/echarts@4.7.0/dist/echarts.min.js"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">11</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">LOUIS' BLOG</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Arxiv每日速递(2022-07-06)<a class="post-edit-link" href="https://github.com/isLouisHsu/blog/tree/master/source_posts/Arxiv每日速递.md" title="编辑" target="_blank"><i class="fas fa-pencil-square"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-07-06T00:48:42.989Z" title="发表于 2022-07-06 08:48:42">2022-07-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-07-06T00:50:29.531Z" title="更新于 2022-07-06 08:50:29">2022-07-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">阅读笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">44.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>267分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2022/07/06/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。</p>
<h1>统计</h1>
<p>今日共更新251篇论文，其中：</p>
<ul>
<li>83篇计算机视觉（<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>）</li>
<li>22篇自然语言处理（<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>）</li>
<li>77篇机器学习（cs.LG）</li>
<li>40篇人工智能（<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>）</li>
</ul>
<h1>计算机视觉</h1>
<details>
  <summary>1. <b>标题：Segmenting Moving Objects via an Object-Centric Layered Representation</b></summary>
  <p><b>编号</b>：[2]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02206</p>
  <p><b>作者</b>：Junyu Xie,  Weidi Xie,  Andrew Zisserman</p>
  <p><b>备注</b>：Total 27 pages, 13 figures (including main text: 9 pages, 5 figures)</p>
  <p><b>关键词</b>：track and segment, segment multiple moving, multiple moving objects, model, multiple moving</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The objective of this paper is a model that is able to discover, track and
segment multiple moving objects in a video. We make four contributions: First,
we introduce an object-centric segmentation model with a depth-ordered layer
representation. This is implemented using a variant of the transformer
architecture that ingests optical flow, where each query vector specifies an
object and its layer for the entire video. The model can effectively discover
multiple moving objects and handle mutual occlusions; Second, we introduce a
scalable pipeline for generating synthetic training data with multiple objects,
significantly reducing the requirements for labour-intensive annotations, and
supporting Sim2Real generalisation; Third, we show that the model is able to
learn object permanence and temporal shape consistency, and is able to predict
amodal segmentation masks; Fourth, we evaluate the model on standard video
segmentation benchmarks, DAVIS, MoCA, SegTrack, FBMS-59, and achieve
state-of-the-art unsupervised segmentation performance, even outperforming
several supervised approaches. With test-time adaptation, we observe further
performance boosts.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Clustered Saliency Prediction</b></summary>
  <p><b>编号</b>：[3]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02205</p>
  <p><b>作者</b>：Rezvan Sherkati,  James J. Clark</p>
  <p><b>备注</b>：21 pages, 4 figures</p>
  <p><b>关键词</b>：Saliency, saliency maps, saliency prediction, prediction, universal saliency prediction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a new method for image salience prediction, Clustered Saliency
Prediction. This method divides individuals into clusters based on their
personal features and their known saliency maps, and generates a separate image
salience model for each cluster. We test our approach on a public dataset of
personalized saliency maps, with varying importance weights for personal
feature factors and observe the effects on the clusters. For each cluster, we
use an image-to-image translation method, mainly Pix2Pix model, to convert
universal saliency maps to saliency maps of that cluster. We try three
state-of-the-art universal saliency prediction methods, DeepGaze II, ML-Net and
SalGAN, and see their impact on the results. We show that our Clustered
Saliency Prediction technique outperforms the state-of-the-art universal
saliency prediction models. Also we demonstrate the effectiveness of our
clustering method by comparing the results of Clustered Saliency Prediction
using clusters obtained by Subject Similarity Clustering algorithm with two
baseline methods. We propose an approach to assign new people to the most
appropriate cluster, based on their personal features and any known saliency
maps. In our experiments we see that this method of assigning new people to a
cluster on average chooses the cluster that gives higher saliency scores.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Detecting and Recovering Sequential DeepFake Manipulation</b></summary>
  <p><b>编号</b>：[4]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02204</p>
  <p><b>作者</b>：Rui Shao,  Tianxing Wu,  Ziwei Liu</p>
  <p><b>备注</b>：ECCV 2022. Project page: this https URL Code: this https URL</p>
  <p><b>关键词</b>：potential malicious abuse, drawn great concerns, manipulation technologies nowadays, technologies nowadays, potential malicious</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Since photorealistic faces can be readily generated by facial manipulation
technologies nowadays, potential malicious abuse of these technologies has
drawn great concerns. Numerous deepfake detection methods are thus proposed.
However, existing methods only focus on detecting one-step facial manipulation.
As the emergence of easy-accessible facial editing applications, people can
easily manipulate facial components using multi-step operations in a sequential
manner. This new threat requires us to detect a sequence of facial
manipulations, which is vital for both detecting deepfake media and recovering
original faces afterwards. Motivated by this observation, we emphasize the need
and propose a novel research problem called Detecting Sequential DeepFake
Manipulation (Seq-DeepFake). Unlike the existing deepfake detection task only
demanding a binary label prediction, detecting Seq-DeepFake manipulation
requires correctly predicting a sequential vector of facial manipulation
operations. To support a large-scale investigation, we construct the first
Seq-DeepFake dataset, where face images are manipulated sequentially with
corresponding annotations of sequential facial manipulation vectors. Based on
this new dataset, we cast detecting Seq-DeepFake manipulation as a specific
image-to-sequence (e.g. image captioning) task and propose a concise yet
effective Seq-DeepFake Transformer (SeqFakeFormer). Moreover, we build a
comprehensive benchmark and set up rigorous evaluation protocols and metrics
for this new research problem. Extensive experiments demonstrate the
effectiveness of SeqFakeFormer. Several valuable observations are also revealed
to facilitate future research in broader deepfake detection problems.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：CoBEVT: Cooperative Bird's Eye View Semantic Segmentation with Sparse  Transformers</b></summary>
  <p><b>编号</b>：[5]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02202</p>
  <p><b>作者</b>：Runsheng Xu,  Zhengzhong Tu,  Hao Xiang,  Wei Shao,  Bolei Zhou,  Jiaqi Ma</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Bird eye view, Bird eye, plays a crucial, crucial role, BEV</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Bird's eye view (BEV) semantic segmentation plays a crucial role in spatial
sensing for autonomous driving. Although recent literature has made significant
progress on BEV map understanding, they are all based on single-agent
camera-based systems which are difficult to handle occlusions and detect
distant objects in complex traffic scenes. Vehicle-to-Vehicle (V2V)
communication technologies have enabled autonomous vehicles to share sensing
information, which can dramatically improve the perception performance and
range as compared to single-agent systems. In this paper, we propose CoBEVT,
the first generic multi-agent multi-camera perception framework that can
cooperatively generate BEV map predictions. To efficiently fuse camera features
from multi-view and multi-agent data in an underlying Transformer architecture,
we design a fused axial attention or FAX module, which can capture sparsely
local and global spatial interactions across views and agents. The extensive
experiments on the V2V perception dataset, OPV2V, demonstrate that CoBEVT
achieves state-of-the-art performance for cooperative BEV semantic
segmentation. Moreover, CoBEVT is shown to be generalizable to other tasks,
including 1) BEV segmentation with single-agent multi-camera and 2) 3D object
detection with multi-agent LiDAR systems, and achieves state-of-the-art
performance with real-time inference speed.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Efficient Spatial-Temporal Information Fusion for LiDAR-Based 3D Moving  Object Segmentation</b></summary>
  <p><b>编号</b>：[6]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02201</p>
  <p><b>作者</b>：Jiadai Sun,  Yuchao Dai,  Xianjing Zhang,  Jintao Xu,  Rui Ai,  Weihao Gu,  Xieyuanli Chen</p>
  <p><b>备注</b>：Accepted by IROS2022. Code: this https URL</p>
  <p><b>关键词</b>：Accurate moving object, moving object segmentation, autonomous driving, essential task, Accurate moving</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Accurate moving object segmentation is an essential task for autonomous
driving. It can provide effective information for many downstream tasks, such
as collision avoidance, path planning, and static map construction. How to
effectively exploit the spatial-temporal information is a critical question for
3D LiDAR moving object segmentation (LiDAR-MOS). In this work, we propose a
novel deep neural network exploiting both spatial-temporal information and
different representation modalities of LiDAR scans to improve LiDAR-MOS
performance. Specifically, we first use a range image-based dual-branch
structure to separately deal with spatial and temporal information that can be
obtained from sequential LiDAR scans, and later combine them using
motion-guided attention modules. We also use a point refinement module via 3D
sparse convolution to fuse the information from both LiDAR range image and
point cloud representations and reduce the artifacts on the borders of the
objects. We verify the effectiveness of our proposed approach on the LiDAR-MOS
benchmark of SemanticKITTI. Our method outperforms the state-of-the-art methods
significantly in terms of LiDAR-MOS IoU. Benefiting from the devised
coarse-to-fine architecture, our method operates online at sensor frame rate.
The implementation of our method is available as open source at:
this https URL.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Accelerating Score-based Generative Models with Preconditioned Diffusion  Sampling</b></summary>
  <p><b>编号</b>：[8]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02196</p>
  <p><b>作者</b>：Hengyuan Ma,  Li Zhang,  Xiatian Zhu,  Jianfeng Feng</p>
  <p><b>备注</b>：ECCV 2022</p>
  <p><b>关键词</b>：Score-based generative models, generative models, Score-based generative, recently emerged, promising class</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Score-based generative models (SGMs) have recently emerged as a promising
class of generative models. However, a fundamental limitation is that their
inference is very slow due to a need for many (e.g., 2000) iterations of
sequential computations. An intuitive acceleration method is to reduce the
sampling iterations which however causes severe performance degradation. We
investigate this problem by viewing the diffusion sampling process as a
Metropolis adjusted Langevin algorithm, which helps reveal the underlying cause
to be ill-conditioned curvature. Under this insight, we propose a
model-agnostic preconditioned diffusion sampling (PDS) method that leverages
matrix preconditioning to alleviate the aforementioned problem. Crucially, PDS
is proven theoretically to converge to the original target distribution of a
SGM, no need for retraining. Extensive experiments on three image datasets with
a variety of resolutions and diversity validate that PDS consistently
accelerates off-the-shelf SGMs whilst maintaining the synthesis quality. In
particular, PDS can accelerate by up to 29x on more challenging high resolution
(1024x1024) image generation.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：NeuralPassthrough: Learned Real-Time View Synthesis for VR</b></summary>
  <p><b>编号</b>：[12]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02186</p>
  <p><b>作者</b>：Lei Xiao,  Salah Nouri,  Joel Hegland,  Alberto Garcia Garcia,  Douglas Lanman</p>
  <p><b>备注</b>：9 pages, 12 figures</p>
  <p><b>关键词</b>：stereoscopic visual experience, Virtual reality, provide an immersive, visual experience, physical environment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Virtual reality (VR) headsets provide an immersive, stereoscopic visual
experience, but at the cost of blocking users from directly observing their
physical environment. Passthrough techniques are intended to address this
limitation by leveraging outward-facing cameras to reconstruct the images that
would otherwise be seen by the user without the headset. This is inherently a
real-time view synthesis challenge, since passthrough cameras cannot be
physically co-located with the eyes. Existing passthrough techniques suffer
from distracting reconstruction artifacts, largely due to the lack of accurate
depth information (especially for near-field and disoccluded objects), and also
exhibit limited image quality (e.g., being low resolution and monochromatic).
In this paper, we propose the first learned passthrough method and assess its
performance using a custom VR headset that contains a stereo pair of RGB
cameras. Through both simulations and experiments, we demonstrate that our
learned passthrough method delivers superior image quality compared to
state-of-the-art methods, while meeting strict VR requirements for real-time,
perspective-correct stereoscopic view synthesis over a wide field of view for
desktop-connected headsets.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：CLEAR: Improving Vision-Language Navigation with Cross-Lingual,  Environment-Agnostic Representations</b></summary>
  <p><b>编号</b>：[13]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02185</p>
  <p><b>作者</b>：Jialu Li,  Hao Tan,  Mohit Bansal</p>
  <p><b>备注</b>：NAACL 2022 Findings (18 pages)</p>
  <p><b>关键词</b>：VLN, representation, visual, visual representation, language representation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision-and-Language Navigation (VLN) tasks require an agent to navigate
through the environment based on language instructions. In this paper, we aim
to solve two key challenges in this task: utilizing multilingual instructions
for improved instruction-path grounding and navigating through new environments
that are unseen during training. To address these challenges, we propose CLEAR:
Cross-Lingual and Environment-Agnostic Representations. First, our agent learns
a shared and visually-aligned cross-lingual language representation for the
three languages (English, Hindi and Telugu) in the Room-Across-Room dataset.
Our language representation learning is guided by text pairs that are aligned
by visual information. Second, our agent learns an environment-agnostic visual
representation by maximizing the similarity between semantically-aligned image
pairs (with constraints on object-matching) from different environments. Our
environment agnostic visual representation can mitigate the environment bias
induced by low-level visual information. Empirically, on the Room-Across-Room
dataset, we show that our multilingual agent gets large improvements in all
metrics over the strong baseline model when generalizing to unseen environments
with the cross-lingual language representation and the environment-agnostic
visual representation. Furthermore, we show that our learned language and
visual representations can be successfully transferred to the Room-to-Room and
Cooperative Vision-and-Dialogue Navigation task, and present detailed
qualitative and quantitative generalization and grounding analysis. Our code is
available at this https URL</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：ST-CoNAL: Consistency-Based Acquisition Criterion Using Temporal  Self-Ensemble for Active Learning</b></summary>
  <p><b>编号</b>：[14]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02182</p>
  <p><b>作者</b>：Jae Soon Baik,  In Young Yoon,  Jun Won Choi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：achieved great success, Modern deep learning, Modern deep, achieved great, great success</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Modern deep learning has achieved great success in various fields. However,
it requires the labeling of huge amounts of data, which is expensive and
labor-intensive. Active learning (AL), which identifies the most informative
samples to be labeled, is becoming increasingly important to maximize the
efficiency of the training process. The existing AL methods mostly use only a
single final fixed model for acquiring the samples to be labeled. This strategy
may not be good enough in that the structural uncertainty of a model for given
training data is not considered to acquire the samples. In this study, we
propose a novel acquisition criterion based on temporal self-ensemble generated
by conventional stochastic gradient descent (SGD) optimization. These
self-ensemble models are obtained by capturing the intermediate network weights
obtained through SGD iterations. Our acquisition function relies on a
consistency measure between the student and teacher models. The student models
are given a fixed number of temporal self-ensemble models, and the teacher
model is constructed by averaging the weights of the student models. Using the
proposed acquisition criterion, we present an AL algorithm, namely
student-teacher consistency-based AL (ST-CoNAL). Experiments conducted for
image classification tasks on CIFAR-10, CIFAR-100, Caltech-256, and Tiny
ImageNet datasets demonstrate that the proposed ST-CoNAL achieves significantly
better performance than the existing acquisition methods. Furthermore,
extensive experiments show the robustness and effectiveness of our methods.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Activation Template Matching Loss for Explainable Face Recognition</b></summary>
  <p><b>编号</b>：[16]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02179</p>
  <p><b>作者</b>：Huawei Lin,  Haozhe Liu,  Qiufu Li,  Linlin Shen</p>
  <p><b>备注</b>：13 pages, 7 figures, 5 tables</p>
  <p><b>关键词</b>：explainable face recognition, face recognition network, Explainable Channel Loss, facial part-based feature, feature like eyes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Can we construct an explainable face recognition network able to learn a
facial part-based feature like eyes, nose, mouth and so forth, without any
manual annotation or additionalsion datasets? In this paper, we propose a
generic Explainable Channel Loss (ECLoss) to construct an explainable face
recognition network. The explainable network trained with ECLoss can easily
learn the facial part-based representation on the target convolutional layer,
where an individual channel can detect a certain face part. Our experiments on
dozens of datasets show that ECLoss achieves superior explainability metrics,
and at the same time improves the performance of face verification without face
alignment. In addition, our visualization results also illustrate the
effectiveness of the proposed ECLoss.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：DBN-Mix: Training Dual Branch Network Using Bilateral Mixup Augmentation  for Long-Tailed Visual Recognition</b></summary>
  <p><b>编号</b>：[17]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02173</p>
  <p><b>作者</b>：Jae Soon Baik,  In Young Yoon,  Jun Won Choi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：challenging visual perception, visual perception task, long-tailed class distributions, growing interest, perception task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>There is a growing interest in the challenging visual perception task of
learning from long-tailed class distributions. The extreme class imbalance in
the training dataset biases the model to prefer to recognize majority-class
data over minority-class data. Recently, the dual branch network (DBN)
framework has been proposed, where two branch networks; the conventional branch
and the re-balancing branch were employed to improve the accuracy of
long-tailed visual recognition. The re-balancing branch uses a reverse sampler
to generate class-balanced training samples to mitigate bias due to class
imbalance. Although this strategy has been quite successful in handling bias,
using a reversed sampler for training can degrade the representation learning
performance. To alleviate this issue, the conventional method used a carefully
designed cumulative learning strategy, in which the influence of the
re-balancing branch gradually increases throughout the entire training phase.
In this study, we aim to develop a simple yet effective method to improve the
performance of DBN without cumulative learning that is difficult to optimize.
We devise a simple data augmentation method termed bilateral mixup
augmentation, which combines one sample from the uniform sampler with another
sample from the reversed sampler to produce a training sample. Furthermore, we
present class-conditional temperature scaling that mitigates bias toward the
majority class for the proposed DBN architecture. Our experiments performed on
widely used long-tailed visual recognition datasets show that bilateral mixup
augmentation is quite effective in improving the representation learning
performance of DBNs, and that the proposed method achieves state-of-the-art
performance for some categories.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Automatic inspection of cultural monuments using deep and tensor-based  learning on hyperspectral imagery</b></summary>
  <p><b>编号</b>：[20]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02163</p>
  <p><b>作者</b>：Ioannis N. Tzortzis,  Ioannis Rallis,  Konstantinos Makantasis,  Anastasios Doulamis,  Nikolaos Doulamis,  Athanasios Voulodimos</p>
  <p><b>备注</b>：Accepted for presentation in IEEE International Conference on Image Processing (ICIP 2022)</p>
  <p><b>关键词</b>：provide extended information, Cultural Heritage, Cultural Heritage monuments, hyperspectral images, images are commonly</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In Cultural Heritage, hyperspectral images are commonly used since they
provide extended information regarding the optical properties of materials.
Thus, the processing of such high-dimensional data becomes challenging from the
perspective of machine learning techniques to be applied. In this paper, we
propose a Rank-$R$ tensor-based learning model to identify and classify
material defects on Cultural Heritage monuments. In contrast to conventional
deep learning approaches, the proposed high order tensor-based learning
demonstrates greater accuracy and robustness against overfitting. Experimental
results on real-world data from UNESCO protected areas indicate the superiority
of the proposed scheme compared to conventional deep learning models.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Multi-modal Robustness Analysis Against Language and Visual  Perturbations</b></summary>
  <p><b>编号</b>：[23]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02159</p>
  <p><b>作者</b>：Madeline C. Schiappa,  Yogesh S. Rawat,  Shruti Vyas,  Vibhav Vineet,  Hamid Palangi</p>
  <p><b>备注</b>：29 pages, 21 figures. This projects webpage is located at this https URL</p>
  <p><b>关键词</b>：single modal learning, recently shown, shown a good, good progress, progress in multi-modal</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Joint visual and language modeling on large-scale datasets has recently shown
a good progress in multi-modal tasks when compared to single modal learning.
However, robustness of these approaches against real-world perturbations has
not been studied. In this work, we perform the first extensive robustness study
of such models against various real-world perturbations focusing on video and
language. We focus on text-to-video retrieval and propose two large-scale
benchmark datasets, MSRVTT-P and YouCook2-P, which utilize 90 different visual
and 35 different textual perturbations. The study reveals some interesting
findings: 1) The studied models are more robust when text is perturbed versus
when video is perturbed 2) The transformer text encoder is more robust on
non-semantic changing text perturbations and visual perturbations compared to
word embedding approaches. 3) Using two-branch encoders in isolation is
typically more robust than when architectures use cross-attention. We hope this
study will serve as a benchmark and guide future research in robust multimodal
learning.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Class-Specific Semantic Reconstruction for Open Set Recognition</b></summary>
  <p><b>编号</b>：[24]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02158</p>
  <p><b>作者</b>：Hongzhi Huang,  Yu Wang,  Qinghua Hu,  Ming-Ming Cheng</p>
  <p><b>备注</b>：14 pages, 10 figures,</p>
  <p><b>关键词</b>：deep neural networks, enables deep neural, maintaining high classification, high classification accuracy, unknown classes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Open set recognition enables deep neural networks (DNNs) to identify samples
of unknown classes, while maintaining high classification accuracy on samples
of known classes. Existing methods basing on auto-encoder (AE) and prototype
learning show great potential in handling this challenging task. In this study,
we propose a novel method, called Class-Specific Semantic Reconstruction
(CSSR), that integrates the power of AE and prototype learning. Specifically,
CSSR replaces prototype points with manifolds represented by class-specific
AEs. Unlike conventional prototype-based methods, CSSR models each known class
on an individual AE manifold, and measures class belongingness through AE's
reconstruction error. Class-specific AEs are plugged into the top of the DNN
backbone and reconstruct the semantic representations learned by the DNN
instead of the raw image. Through end-to-end learning, the DNN and the AEs
boost each other to learn both discriminative and representative information.
The results of experiments conducted on multiple datasets show that the
proposed method achieves outstanding performance in both close and open set
recognition and is sufficiently simple and flexible to incorporate into
existing frameworks.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Deep Learning for Finger Vein Recognition: A Brief Survey of Recent  Trend</b></summary>
  <p><b>编号</b>：[27]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02148</p>
  <p><b>作者</b>：Renye Zhang,  Yimin Yin,  Wanxia Deng,  Chen Li,  Jinghua Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Finger vein image, vein image recognition, vein image, image recognition, recognition technology plays</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Finger vein image recognition technology plays an important role in biometric
recognition and has been successfully applied in many fields. Because veins are
buried beneath the skin tissue, finger vein image recognition has an
unparalleled advantage, which is not easily disturbed by external factors. This
review summarizes 46 papers about deep learning for finger vein image
recognition from 2017 to 2021. These papers are summarized according to the
tasks of deep neural networks. Besides, we present the challenges and potential
development directions of finger vein image recognition.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Improving Semantic Segmentation in Transformers using Hierarchical  Inter-Level Attention</b></summary>
  <p><b>编号</b>：[33]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02126</p>
  <p><b>作者</b>：Gary Leung,  Jun Gao,  Xiaohui Zeng,  Sanja Fidler</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Existing transformer-based image, propagate feature information, Existing transformer-based, typically propagate feature, transformer-based image backbones</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing transformer-based image backbones typically propagate feature
information in one direction from lower to higher-levels. This may not be ideal
since the localization ability to delineate accurate object boundaries, is most
prominent in the lower, high-resolution feature maps, while the semantics that
can disambiguate image signals belonging to one object vs. another, typically
emerges in a higher level of processing. We present Hierarchical Inter-Level
Attention (HILA), an attention-based method that captures Bottom-Up and
Top-Down Updates between features of different levels. HILA extends
hierarchical vision transformer architectures by adding local connections
between features of higher and lower levels to the backbone encoder. In each
iteration, we construct a hierarchy by having higher-level features compete for
assignments to update lower-level features belonging to them, iteratively
resolving object-part relationships. These improved lower-level features are
then used to re-update the higher-level features. HILA can be integrated into
the majority of hierarchical architectures without requiring any changes to the
base model. We add HILA into SegFormer and the Swin Transformer and show
notable improvements in accuracy in semantic segmentation with fewer parameters
and FLOPS. Project website and code:
this https URL</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Improving Covariance Conditioning of the SVD Meta-layer by Orthogonality</b></summary>
  <p><b>编号</b>：[35]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02119</p>
  <p><b>作者</b>：Yue Song,  Nicu Sebe,  Wei Wang</p>
  <p><b>备注</b>：Accepted by ECCV22</p>
  <p><b>关键词</b>：Inserting an SVD, SVD meta-layer, meta-layer into neural, neural networks, networks is prone</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Inserting an SVD meta-layer into neural networks is prone to make the
covariance ill-conditioned, which could harm the model in the training
stability and generalization abilities. In this paper, we systematically study
how to improve the covariance conditioning by enforcing orthogonality to the
Pre-SVD layer. Existing orthogonal treatments on the weights are first
investigated. However, these techniques can improve the conditioning but would
hurt the performance. To avoid such a side effect, we propose the Nearest
Orthogonal Gradient (NOG) and Optimal Learning Rate (OLR). The effectiveness of
our methods is validated in two applications: decorrelated Batch Normalization
(BN) and Global Covariance Pooling (GCP). Extensive experiments on visual
recognition demonstrate that our methods can simultaneously improve the
covariance conditioning and generalization. Moreover, the combinations with
orthogonal weight can further boost the performances.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：CASHformer: Cognition Aware SHape Transformer for Longitudinal Analysis</b></summary>
  <p><b>编号</b>：[49]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02091</p>
  <p><b>作者</b>：Ignacio Sarasua,  Sebastian Pölsterl,  Christian Wachinger</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Alzheimer disease, subcortical structures, structures is crucial, progression of Alzheimer, Alzheimer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Modeling temporal changes in subcortical structures is crucial for a better
understanding of the progression of Alzheimer's disease (AD). Given their
flexibility to adapt to heterogeneous sequence lengths, mesh-based transformer
architectures have been proposed in the past for predicting hippocampus
deformations across time. However, one of the main limitations of transformers
is the large amount of trainable parameters, which makes the application on
small datasets very challenging. In addition, current methods do not include
relevant non-image information that can help to identify AD-related patterns in
the progression. To this end, we introduce CASHformer, a transformer-based
framework to model longitudinal shape trajectories in AD. CASHformer
incorporates the idea of pre-trained transformers as universal compute engines
that generalize across a wide range of tasks by freezing most layers during
fine-tuning. This reduces the number of parameters by over 90% with respect to
the original model and therefore enables the application of large models on
small datasets without overfitting. In addition, CASHformer models cognitive
decline to reveal AD atrophy patterns in the temporal sequence. Our results
show that CASHformer reduces the reconstruction error by 73% compared to
previously proposed methods. Moreover, the accuracy of detecting patients
progressing to AD increases by 3% with imputing missing longitudinal shape
data.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：SiamMask: A Framework for Fast Online Object Tracking and Segmentation</b></summary>
  <p><b>编号</b>：[50]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02088</p>
  <p><b>作者</b>：Weiming Hu,  Qiang Wang,  Li Zhang,  Luca Bertinetto,  Philip H.S. Torr</p>
  <p><b>备注</b>：17 pages, Accepted by TPAMI 2022. arXiv admin note: substantial text overlap with arXiv:1812.05050</p>
  <p><b>关键词</b>：simple method, visual object tracking, paper we introduce, object tracking, video object segmentation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper we introduce SiamMask, a framework to perform both visual
object tracking and video object segmentation, in real-time, with the same
simple method. We improve the offline training procedure of popular
fully-convolutional Siamese approaches by augmenting their losses with a binary
segmentation task. Once the offline training is completed, SiamMask only
requires a single bounding box for initialization and can simultaneously carry
out visual object tracking and segmentation at high frame-rates. Moreover, we
show that it is possible to extend the framework to handle multiple object
tracking and segmentation by simply re-using the multi-task model in a cascaded
fashion. Experimental results show that our approach has high processing
efficiency, at around 55 frames per second. It yields real-time
state-of-the-art results on visual-object tracking benchmarks, while at the
same time demonstrating competitive performance at a high speed for video
object segmentation benchmarks.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Test-time Adaptation for Real Image Denoising via Meta-transfer Learning</b></summary>
  <p><b>编号</b>：[56]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02066</p>
  <p><b>作者</b>：Agus Gunawan,  Muhammad Adi Nugroho,  Se Jin Park</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：real image denoising, image denoising tasks, image denoising, real image, recent years</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, a ton of research has been conducted on real image denoising
tasks. However, the efforts are more focused on improving real image denoising
through creating a better network architecture. We explore a different
direction where we propose to improve real image denoising performance through
a better learning strategy that can enable test-time adaptation on the
multi-task network. The learning strategy is two stages where the first stage
pre-train the network using meta-auxiliary learning to get better
meta-initialization. Meanwhile, we use meta-learning for fine-tuning
(meta-transfer learning) the network as the second stage of our training to
enable test-time adaptation on real noisy images. To exploit a better learning
strategy, we also propose a network architecture with self-supervised masked
reconstruction loss. Experiments on a real noisy dataset show the contribution
of the proposed method and show that the proposed method can outperform other
SOTA methods.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：RepMix: Representation Mixing for Robust Attribution of Synthesized  Images</b></summary>
  <p><b>编号</b>：[57]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02063</p>
  <p><b>作者</b>：Tu Bui,  Ning Yu,  John Collomosse</p>
  <p><b>备注</b>：Accepted at ECCV 2022</p>
  <p><b>关键词</b>：Generative Adversarial Networks, Adversarial Networks, Generative Adversarial, GAN architecture created, advances in Generative</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Rapid advances in Generative Adversarial Networks (GANs) raise new challenges
for image attribution; detecting whether an image is synthetic and, if so,
determining which GAN architecture created it. Uniquely, we present a solution
to this task capable of 1) matching images invariant to their semantic content;
2) robust to benign transformations (changes in quality, resolution, shape,
etc.) commonly encountered as images are re-shared online. In order to
formalize our research, a challenging benchmark, Attribution88, is collected
for robust and practical image attribution. We then propose RepMix, our GAN
fingerprinting technique based on representation mixing and a novel loss. We
validate its capability of tracing the provenance of GAN-generated images
invariant to the semantic content of the image and also robust to
perturbations. We show our approach improves significantly from existing GAN
fingerprinting works on both semantic generalization and robustness. Data and
code are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Image Amodal Completion: A Survey</b></summary>
  <p><b>编号</b>：[58]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02062</p>
  <p><b>作者</b>：Jiayang Ao,  Krista A. Ehinger,  Qiuhong Ke</p>
  <p><b>备注</b>：Manuscript Submitted for Publication</p>
  <p><b>关键词</b>：Image amodal completion, computer vision systems, amodal completion, Image amodal, partially occluded objects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing computer vision systems can compete with humans in understanding the
visible parts of objects, but still fall far short of humans when it comes to
depicting the invisible parts of partially occluded objects. Image amodal
completion aims to equip computers with human-like amodal completion functions
to understand an intact object despite it being partially occluded. The main
purpose of this survey is to provide an intuitive understanding of the research
hotspots, key technologies and future trends in the field of image amodal
completion. Firstly, we present a comprehensive review of the latest literature
in this emerging field, exploring three key tasks in image amodal completion,
including amodal shape completion, amodal appearance completion, and order
perception. Then we examine popular datasets related to image amodal completion
along with their common data collection methods and evaluation metrics.
Finally, we discuss real-world applications and future research directions for
image amodal completion, facilitating the reader's understanding of the
challenges of existing technologies and upcoming research trends.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：MVP: Robust Multi-View Practice for Driving Action Localization</b></summary>
  <p><b>编号</b>：[65]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02042</p>
  <p><b>作者</b>：Jingjie Shang,  Kunchang Li,  Kaibin Tian,  Haisheng Su,  Yangguang Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：apply deep-learning methods, deaths per year, crucial problem, thousands of deaths, apply deep-learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Distracted driving causes thousands of deaths per year, and how to apply
deep-learning methods to prevent these tragedies has become a crucial problem.
In Track3 of the 6th AI City Challenge, researchers provide a high-quality
video dataset with densely action annotations. Due to the small data scale and
unclear action boundary, the dataset presents a unique challenge to precisely
localize all the different actions and classify their categories. In this
paper, we make good use of the multi-view synchronization among videos, and
conduct robust Multi-View Practice (MVP) for driving action localization. To
avoid overfitting, we fine-tune SlowFast with Kinetics-700 pre-training as the
feature extractor. Then the features of different views are passed to
ActionFormer to generate candidate action proposals. For precisely localizing
all the actions, we design elaborate post-processing, including model voting,
threshold filtering and duplication removal. The results show that our MVP is
robust for driving action localization, which achieves 28.49% F1-score in the
Track3 test set.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：PKD: General Distillation Framework for Object Detectors via Pearson  Correlation Coefficient</b></summary>
  <p><b>编号</b>：[66]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02039</p>
  <p><b>作者</b>：Weihan Cao,  Yifan Zhang,  Jianfei Gao,  Anda Cheng,  Ke Cheng,  Jian Cheng</p>
  <p><b>备注</b>：17 pages, 7 figures, 8 tables</p>
  <p><b>关键词</b>：train compact models, widely-used technique, technique to train, train compact, teacher</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Knowledge distillation(KD) is a widely-used technique to train compact models
in object detection. However, there is still a lack of study on how to distill
between heterogeneous detectors. In this paper, we empirically find that better
FPN features from a heterogeneous teacher detector can help the student
although their detection heads and label assignments are different. However,
directly aligning the feature maps to distill detectors suffers from two
problems. First, the difference in feature magnitude between the teacher and
the student could enforce overly strict constraints on the student. Second, the
FPN stages and channels with large feature magnitude from the teacher model
could dominate the gradient of distillation loss, which will overwhelm the
effects of other features in KD and introduce much noise. To address the above
issues, we propose to imitate features with Pearson Correlation Coefficient to
focus on the relational information from the teacher and relax constraints on
the magnitude of the features. Our method consistently outperforms the existing
detection KD methods and works for both homogeneous and heterogeneous
student-teacher pairs. Furthermore, it converges faster. With a powerful
MaskRCNN-Swin detector as the teacher, ResNet-50 based RetinaNet and FCOS
achieve 41.5% and 43.9% mAP on COCO2017, which are 4.1\% and 4.8\% higher than
the baseline, respectively.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：PRoA: A Probabilistic Robustness Assessment against Functional  Perturbations</b></summary>
  <p><b>编号</b>：[67]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02036</p>
  <p><b>作者</b>：Tianle Zhang,  Wenjie Ruan,  Jonathan E. Fieldsend</p>
  <p><b>备注</b>：The short version of this work will appear in the Proceedings of the 2022 European Conference on Machine Learning and Data Mining (ECML-PKDD 2022)</p>
  <p><b>关键词</b>：vital pre-deployment phase, applications robustness measurement, safety-critical deep learning, deep learning applications, learning applications robustness</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In safety-critical deep learning applications robustness measurement is a
vital pre-deployment phase. However, existing robustness verification methods
are not sufficiently practical for deploying machine learning systems in the
real world. On the one hand, these methods attempt to claim that no
perturbations can ``fool'' deep neural networks (DNNs), which may be too
stringent in practice. On the other hand, existing works rigorously consider
$L_p$ bounded additive perturbations on the pixel space, although
perturbations, such as colour shifting and geometric transformations, are more
practically and frequently occurring in the real world. Thus, from the
practical standpoint, we present a novel and general {\it probabilistic
robustness assessment method} (PRoA) based on the adaptive concentration, and
it can measure the robustness of deep learning models against functional
perturbations. PRoA can provide statistical guarantees on the probabilistic
robustness of a model, \textit{i.e.}, the probability of failure encountered by
the trained model after deployment. Our experiments demonstrate the
effectiveness and flexibility of PRoA in terms of evaluating the probabilistic
robustness against a broad range of functional perturbations, and PRoA can
scale well to various large-scale deep neural networks compared to existing
state-of-the-art baselines. For the purpose of reproducibility, we release our
tool on GitHub: \url{ this https URL}.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：AvatarCap: Animatable Avatar Conditioned Monocular Human Volumetric  Capture</b></summary>
  <p><b>编号</b>：[68]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02031</p>
  <p><b>作者</b>：Zhe Li,  Zerong Zheng,  Hongwen Zhang,  Chaonan Ji,  Yebin Liu</p>
  <p><b>备注</b>：Accepted by ECCV 2022, project page: this http URL</p>
  <p><b>关键词</b>：ill-posed problem caused, introduces animatable avatars, present AvatarCap, address the ill-posed, ill-posed problem</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To address the ill-posed problem caused by partial observations in monocular
human volumetric capture, we present AvatarCap, a novel framework that
introduces animatable avatars into the capture pipeline for high-fidelity
reconstruction in both visible and invisible regions. Our method firstly
creates an animatable avatar for the subject from a small number (~20) of 3D
scans as a prior. Then given a monocular RGB video of this subject, our method
integrates information from both the image observation and the avatar prior,
and accordingly recon-structs high-fidelity 3D textured models with dynamic
details regardless of the visibility. To learn an effective avatar for
volumetric capture from only few samples, we propose GeoTexAvatar, which
leverages both geometry and texture supervisions to constrain the
pose-dependent dynamics in a decomposed implicit manner. An avatar-conditioned
volumetric capture method that involves a canonical normal fusion and a
reconstruction network is further proposed to integrate both image observations
and avatar dynamics for high-fidelity reconstruction in both observed and
invisible regions. Overall, our method enables monocular human volumetric
capture with detailed and pose-dependent dynamics, and the experiments show
that our method outperforms state of the art. Code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：DeepPS2: Revisiting Photometric Stereo Using Two Differently Illuminated  Images</b></summary>
  <p><b>编号</b>：[70]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02025</p>
  <p><b>作者</b>：Ashish Tiwari,  Shanmuganathan Raman</p>
  <p><b>备注</b>：Accepted in ECCV 2022</p>
  <p><b>关键词</b>：computer vision research, vision research, object captured, great interest, interest and importance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Photometric stereo, a problem of recovering 3D surface normals using images
of an object captured under different lightings, has been of great interest and
importance in computer vision research. Despite the success of existing
traditional and deep learning-based methods, it is still challenging due to:
(i) the requirement of three or more differently illuminated images, (ii) the
inability to model unknown general reflectance, and (iii) the requirement of
accurate 3D ground truth surface normals and known lighting information for
training. In this work, we attempt to address an under-explored problem of
photometric stereo using just two differently illuminated images, referred to
as the PS2 problem. It is an intermediate case between a single image-based
reconstruction method like Shape from Shading (SfS) and the traditional
Photometric Stereo (PS), which requires three or more images. We propose an
inverse rendering-based deep learning framework, called DeepPS2, that jointly
performs surface normal, albedo, lighting estimation, and image relighting in a
completely self-supervised manner with no requirement of ground truth data. We
demonstrate how image relighting in conjunction with image reconstruction
enhances the lighting estimation in a self-supervised setting.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Multiview Detection with Cardboard Human Modeling</b></summary>
  <p><b>编号</b>：[73]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02013</p>
  <p><b>作者</b>：Jiahao Ma,  Zicheng Duan,  Yunzhong Hou,  Liang Zheng,  Chuong Nguyen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：locate occluded pedestrians, multiple calibrated cameras, Multiview detection, calibrated cameras, cameras with overlapping</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multiview detection uses multiple calibrated cameras with overlapping fields
of views to locate occluded pedestrians. In this field, existing methods
typically adopt a ``human modeling - aggregation'' strategy. To find robust
pedestrian representations, some intuitively use locations of detected 2D
bounding boxes, while others use entire frame features projected to the ground
plane. However, the former does not consider human appearance and leads to many
ambiguities, and the latter suffers from projection errors due to the lack of
accurate height of the human torso and head. In this paper, we propose a new
pedestrian representation scheme based on human point clouds modeling.
Specifically, using ray tracing for holistic human depth estimation, we model
pedestrians as upright, thin cardboard point clouds on the ground. Then, we
aggregate the point clouds of the pedestrian cardboard across multiple views
for a final decision. Compared with existing representations, the proposed
method explicitly leverages human appearance and reduces projection errors
significantly by relatively accurate height estimation. On two standard
evaluation benchmarks, the proposed method achieves very competitive results.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Open-Vocabulary 3D Detection via Image-level Class and Debiased  Cross-modal Contrastive Learning</b></summary>
  <p><b>编号</b>：[83]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01987</p>
  <p><b>作者</b>：Yuheng Lu,  Chenfeng Xu,  Xiaobao Wei,  Xiaodong Xie,  Masayoshi Tomizuka,  Kurt Keutzer,  Shanghang Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：limited generalization capability, Current point-cloud detection, point-cloud detection, point-cloud, open-vocabulary point-cloud detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Current point-cloud detection methods have difficulty detecting the
open-vocabulary objects in the real world, due to their limited generalization
capability. Moreover, it is extremely laborious and expensive to collect and
fully annotate a point-cloud detection dataset with numerous classes of
objects, leading to the limited classes of existing point-cloud datasets and
hindering the model to learn general representations to achieve open-vocabulary
point-cloud detection. As far as we know, we are the first to study the problem
of open-vocabulary 3D point-cloud detection. Instead of seeking a point-cloud
dataset with full labels, we resort to ImageNet1K to broaden the vocabulary of
the point-cloud detector. We propose OV-3DETIC, an Open-Vocabulary 3D DETector
using Image-level Class supervision. Specifically, we take advantage of two
modalities, the image modality for recognition and the point-cloud modality for
localization, to generate pseudo labels for unseen classes. Then we propose a
novel debiased cross-modal contrastive learning method to transfer the
knowledge from image modality to point-cloud modality during training. Without
hurting the latency during inference, OV-3DETIC makes the point-cloud detector
capable of achieving open-vocabulary detection. Extensive experiments
demonstrate that the proposed OV-3DETIC achieves at least 10.77 % mAP
improvement (absolute value) and 9.56 % mAP improvement (absolute value) by a
wide range of baselines on the SUN-RGBD dataset and ScanNet dataset,
respectively. Besides, we conduct sufficient experiments to shed light on why
the proposed OV-3DETIC works.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Federated Self-supervised Learning for Video Understanding</b></summary>
  <p><b>编号</b>：[87]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01975</p>
  <p><b>作者</b>：Yasar Abbas Ur Rehman,  Yan Gao,  Jiajun Shen,  Pedro Porto Buarque de Gusmao,  Nicholas Lane</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：camera-enabled mobile devices, unlabelled video data, ubiquity of camera-enabled, camera-enabled mobile, mobile devices</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The ubiquity of camera-enabled mobile devices has lead to large amounts of
unlabelled video data being produced at the edge. Although various
self-supervised learning (SSL) methods have been proposed to harvest their
latent spatio-temporal representations for task-specific training, practical
challenges including privacy concerns and communication costs prevent SSL from
being deployed at large scales. To mitigate these issues, we propose the use of
Federated Learning (FL) to the task of video SSL. In this work, we evaluate the
performance of current state-of-the-art (SOTA) video-SSL techniques and
identify their shortcomings when integrated into the large-scale FL setting
simulated with kinetics-400 dataset. We follow by proposing a novel federated
SSL framework for video, dubbed FedVSSL, that integrates different aggregation
strategies and partial weight updating. Extensive experiments demonstrate the
effectiveness and significance of FedVSSL as it outperforms the centralized
SOTA for the downstream retrieval task by 6.66% on UCF-101 and 5.13% on
HMDB-51.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Understanding and Improving Group Normalization</b></summary>
  <p><b>编号</b>：[88]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01972</p>
  <p><b>作者</b>：Agus Gunawan,  Xu Yin,  Kang Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：neural network training, neural network, network training, training, network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Various normalization layers have been proposed to help the training of
neural networks. Group Normalization (GN) is one of the effective and
attractive studies that achieved significant performances in the visual
recognition task. Despite the great success achieved, GN still has several
issues that may negatively impact neural network training. In this paper, we
introduce an analysis framework and discuss the working principles of GN in
affecting the training process of the neural network. From experimental
results, we conclude the real cause of GN's inferior performance against Batch
normalization (BN): 1) \textbf{unstable training performance}, 2) \textbf{more
sensitive} to distortion, whether it comes from external noise or perturbations
introduced by the regularization. In addition, we found that GN can only help
the neural network training in some specific period, unlike BN, which helps the
network throughout the training. To solve these issues, we propose a new
normalization layer built on top of GN, by incorporating the advantages of BN.
Experimental results on the image classification task demonstrated that the
proposed normalization layer outperforms the official GN to improve recognition
accuracy regardless of the batch sizes and stabilize the network training.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：DualAfford: Learning Collaborative Visual Affordance for Dual-gripper  Object Manipulation</b></summary>
  <p><b>编号</b>：[89]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01971</p>
  <p><b>作者</b>：Yan Zhao,  Ruihai Wu,  Zhehuan Chen,  Yourong Zhang,  Qingnan Fan,  Kaichun Mo,  Hao Dong</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：daily human environments, future home-assistant robots, objects in daily, human environments, essential yet challenging</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>It is essential yet challenging for future home-assistant robots to
understand and manipulate diverse 3D objects in daily human environments.
Towards building scalable systems that can perform diverse manipulation tasks
over various 3D shapes, recent works have advocated and demonstrated promising
results learning visual actionable affordance, which labels every point over
the input 3D geometry with an action likelihood of accomplishing the downstream
task (e.g., pushing or picking-up). However, these works only studied
single-gripper manipulation tasks, yet many real-world tasks require two hands
to achieve collaboratively. In this work, we propose a novel learning
framework, DualAfford, to learn collaborative affordance for dual-gripper
manipulation tasks. The core design of the approach is to reduce the quadratic
problem for two grippers into two disentangled yet interconnected subtasks for
efficient learning. Using the large-scale PartNet-Mobility and ShapeNet
datasets, we set up four benchmark tasks for dual-gripper manipulation.
Experiments prove the effectiveness and superiority of our method over three
baselines. Additional results and videos can be found at
this https URL .</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：A Safe Semi-supervised Graph Convolution Network</b></summary>
  <p><b>编号</b>：[92]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01960</p>
  <p><b>作者</b>：Zhi Yang,  Yadong Yan,  Haitao Gan,  Jing Zhao,  Zhiwei Ye</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Graph Convolution Network, Graph Convolution, unlabeled data, introducing convolution, semi-supervised learning field</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the semi-supervised learning field, Graph Convolution Network (GCN), as a
variant model of GNN, has achieved promising results for non-Euclidean data by
introducing convolution into GNN. However, GCN and its variant models fail to
safely use the information of risk unlabeled data, which will degrade the
performance of semi-supervised learning. Therefore, we propose a Safe GCN
framework (Safe-GCN) to improve the learning performance. In the Safe-GCN, we
design an iterative process to label the unlabeled data. In each iteration, a
GCN and its supervised version(S-GCN) are learned to find the unlabeled data
with high confidence. The high-confidence unlabeled data and their pseudo
labels are then added to the label set. Finally, both added unlabeled data and
labeled ones are used to train a S-GCN which can achieve the safe exploration
of the risk unlabeled data and enable safe use of large numbers of unlabeled
data. The performance of Safe-GCN is evaluated on three well-known citation
network datasets and the obtained results demonstrate the effectiveness of the
proposed framework over several graph-based semi-supervised learning methods.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Image Coding for Machines with Omnipotent Feature Learning</b></summary>
  <p><b>编号</b>：[103]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01932</p>
  <p><b>作者</b>：Ruoyu Feng,  Xin Jin,  Zongyu Guo,  Runsen Feng,  Yixin Gao,  Tianyu He,  Zhizheng Zhang,  Simeng Sun,  Zhibo Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Coding for Machines, meeting human perception, Image Coding, aims to compress, human perception</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image Coding for Machines (ICM) aims to compress images for AI tasks analysis
rather than meeting human perception. Learning a kind of feature that is both
general (for AI tasks) and compact (for compression) is pivotal for its
success. In this paper, we attempt to develop an ICM framework by learning
universal features while also considering compression. We name such features as
omnipotent features and the corresponding framework as Omni-ICM. Considering
self-supervised learning (SSL) improves feature generalization, we integrate it
with the compression task into the Omni-ICM framework to learn omnipotent
features. However, it is non-trivial to coordinate semantics modeling in SSL
and redundancy removing in compression, so we design a novel information
filtering (IF) module between them by co-optimization of instance
distinguishment and entropy minimization to adaptively drop information that is
weakly related to AI tasks (e.g., some texture redundancy). Different from
previous task-specific solutions, Omni-ICM could directly support AI tasks
analysis based on the learned omnipotent features without joint training or
extra transformation. Albeit simple and intuitive, Omni-ICM significantly
outperforms existing traditional and learning-based codecs on multiple
fundamental vision tasks.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Drone Detection and Tracking in Real-Time by Fusion of Different Sensing  Modalities</b></summary>
  <p><b>编号</b>：[106]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01927</p>
  <p><b>作者</b>：Fredrik Svanström,  Fernando Alonso-Fernandez,  Cristofer Englund</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：create risky situations, specially if unauthorized, compromise security, key issue, create risky</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automatic detection of flying drones is a key issue where its presence,
specially if unauthorized, can create risky situations or compromise security.
Here, we design and evaluate a multi-sensor drone detection system. In
conjunction with common video cameras and microphone sensors, we explore the
use of thermal infrared cameras, pointed out as a feasible and promising
solution that is scarcely addressed in the related literature. Our solution
integrates a fish-eye camera as well to monitor a wider part of the sky and
steer the other cameras towards objects of interest. The sensing solutions are
complemented with an ADS-B receiver, a GPS receiver, and a radar module,
although the latter has been not included in our final deployment due to its
limited detection range. The thermal camera is shown to be a feasible solution
as good as the video camera, even if the camera employed here has a lower
resolution. Two other novelties of our work are the creation of a new public
dataset of multi-sensor annotated data that expand the number of classes in
comparison to existing ones, as well as the study of the detector performance
as a function of the sensor-to-target distance. Sensor fusion is also explored,
showing that the system can be made more robust in this way, mitigating false
detections of the individual sensors</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：FishFormer: Annulus Slicing-based Transformer for Fisheye Rectification  with Efficacy Domain Exploration</b></summary>
  <p><b>编号</b>：[107]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01925</p>
  <p><b>作者</b>：Shangrong Yang,  Chunyu Lin,  Kang Liao,  Yao Zhao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Numerous significant progress, achieved through CNN, Numerous significant, fisheye image rectification, significant progress</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Numerous significant progress on fisheye image rectification has been
achieved through CNN. Nevertheless, constrained by a fixed receptive field, the
global distribution and the local symmetry of the distortion have not been
fully exploited. To leverage these two characteristics, we introduced
Fishformer that processes the fisheye image as a sequence to enhance global and
local perception. We tuned the Transformer according to the structural
properties of fisheye images. First, the uneven distortion distribution in
patches generated by the existing square slicing method confuses the network,
resulting in difficult training. Therefore, we propose an annulus slicing
method to maintain the consistency of the distortion in each patch, thus
perceiving the distortion distribution well. Second, we analyze that different
distortion parameters have their own efficacy domains. Hence, the perception of
the local area is as important as the global, but Transformer has a weakness
for local texture perception. Therefore, we propose a novel layer attention
mechanism to enhance the local perception and texture transfer. Our network
simultaneously implements global perception and focused local perception
decided by the different parameters. Extensive experiments demonstrate that our
method provides superior performance compared with state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：GLANCE: Global to Local Architecture-Neutral Concept-based Explanations</b></summary>
  <p><b>编号</b>：[110]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01917</p>
  <p><b>作者</b>：Avinash Kori,  Ben Glocker,  Francesca Toni</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：current explainability techniques, explainability techniques focus, techniques focus, focus on capturing, features</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Most of the current explainability techniques focus on capturing the
importance of features in input space. However, given the complexity of models
and data-generating processes, the resulting explanations are far from being
`complete', in that they lack an indication of feature interactions and
visualization of their `effect'. In this work, we propose a novel
twin-surrogate explainability framework to explain the decisions made by any
CNN-based image classifier (irrespective of the architecture). For this, we
first disentangle latent features from the classifier, followed by aligning
these features to observed/human-defined `context' features. These aligned
features form semantically meaningful concepts that are used for extracting a
causal graph depicting the `perceived' data-generating process, describing the
inter- and intra-feature interactions between unobserved latent features and
observed `context' features. This causal graph serves as a global model from
which local explanations of different forms can be extracted. Specifically, we
provide a generator to visualize the `effect' of interactions among features in
latent space and draw feature importance therefrom as local explanations. Our
framework utilizes adversarial knowledge distillation to faithfully learn a
representation from the classifiers' latent space and use it for extracting
visual explanations. We use the styleGAN-v2 architecture with an additional
regularization term to enforce disentanglement and alignment. We demonstrate
and evaluate explanations obtained with our framework on Morpho-MNIST and on
the FFHQ human faces dataset. Our framework is available at
\url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Hierarchical Symbolic Reasoning in Hyperbolic Space for Deep  Discriminative Models</b></summary>
  <p><b>编号</b>：[111]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01916</p>
  <p><b>作者</b>：Ainkaran Santhirasekaram,  Avinash Kori,  Andrea Rockall,  Mathias Winkler,  Francesca Toni,  Ben Glocker</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：understand model decisions, emph, biases and inconsistencies, provide information, understand model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Explanations for \emph{black-box} models help us understand model decisions
as well as provide information on model biases and inconsistencies. Most of the
current explainability techniques provide a single level of explanation, often
in terms of feature importance scores or feature attention maps in input space.
Our focus is on explaining deep discriminative models at \emph{multiple levels
of abstraction}, from fine-grained to fully abstract explanations. We achieve
this by using the natural properties of \emph{hyperbolic geometry} to more
efficiently model a hierarchy of symbolic features and generate
\emph{hierarchical symbolic rules} as part of our explanations. Specifically,
for any given deep discriminative model, we distill the underpinning knowledge
by discretisation of the continuous latent space using vector quantisation to
form symbols, followed by a \emph{hyperbolic reasoning block} to induce an
\emph{abstraction tree}. We traverse the tree to extract explanations in terms
of symbolic rules and its corresponding visual semantics. We demonstrate the
effectiveness of our method on the MNIST and AFHQ high-resolution animal faces
dataset. Our framework is available at
\url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：StyleFlow For Content-Fixed Image to Image Translation</b></summary>
  <p><b>编号</b>：[114]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01909</p>
  <p><b>作者</b>：Weichen Fan,  Jinghuan Chen,  Jiabin Ma,  Jun Hou,  Shuai Yi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：constrained tasks, constrained, computer vision, strongly constrained, challenging topic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image-to-image (I2I) translation is a challenging topic in computer vision.
We divide this problem into three tasks: strongly constrained translation,
normally constrained translation, and weakly constrained translation. The
constraint here indicates the extent to which the content or semantic
information in the original image is preserved. Although previous approaches
have achieved good performance in weakly constrained tasks, they failed to
fully preserve the content in both strongly and normally constrained tasks,
including photo-realism synthesis, style transfer, and colorization, etc. To
achieve content-preserving transfer in strongly constrained and normally
constrained tasks, we propose StyleFlow, a new I2I translation model that
consists of normalizing flows and a novel Style-Aware Normalization (SAN)
module. With the invertible network structure, StyleFlow first projects input
images into deep feature space in the forward pass, while the backward pass
utilizes the SAN module to perform content-fixed feature transformation and
then projects back to image space. Our model supports both image-guided
translation and multi-modal synthesis. We evaluate our model in several I2I
translation benchmarks, and the results show that the proposed model has
advantages over previous methods in both strongly constrained and normally
constrained tasks.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Spatial-Temporal Frequency Forgery Clue for Video Forgery Detection in  VIS and NIR Scenario</b></summary>
  <p><b>编号</b>：[116]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01906</p>
  <p><b>作者</b>：Yukai Wang,  Chunlei Peng,  Decheng Liu,  Nannan Wang,  Xinbo Gao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：extreme public concerns, caused extreme public, recent years, editing and generation, social media</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, with the rapid development of face editing and generation,
more and more fake videos are circulating on social media, which has caused
extreme public concerns. Existing face forgery detection methods based on
frequency domain find that the GAN forged images have obvious grid-like visual
artifacts in the frequency spectrum compared to the real images. But for
synthesized videos, these methods only confine to single frame and pay little
attention to the most discriminative part and temporal frequency clue among
different frames. To take full advantage of the rich information in video
sequences, this paper performs video forgery detection on both spatial and
temporal frequency domains and proposes a Discrete Cosine Transform-based
Forgery Clue Augmentation Network (FCAN-DCT) to achieve a more comprehensive
spatial-temporal feature representation. FCAN-DCT consists of a backbone
network and two branches: Compact Feature Extraction (CFE) module and Frequency
Temporal Attention (FTA) module. We conduct thorough experimental assessments
on two visible light (VIS) based datasets WildDeepfake and Celeb-DF (v2), and
our self-built video forgery dataset DeepfakeNIR, which is the first video
forgery dataset on near-infrared modality. The experimental results demonstrate
the effectiveness of our method on detecting forgery videos in both VIS and NIR
scenarios.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Open-Vocabulary Multi-Label Classification via Multi-modal Knowledge  Transfer</b></summary>
  <p><b>编号</b>：[125]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01887</p>
  <p><b>作者</b>：Sunan He,  Taian Guo,  Tao Dai,  Ruizhi Qiao,  Bo Ren,  Shu-Tao Xia</p>
  <p><b>备注</b>：13 pages, 10 figures</p>
  <p><b>关键词</b>：Real-world recognition system, Real-world recognition, recognition system, system often encounters, encounters a plenty</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Real-world recognition system often encounters a plenty of unseen labels in
practice. To identify such unseen labels, multi-label zero-shot learning
(ML-ZSL) focuses on transferring knowledge by a pre-trained textual label
embedding (e.g., GloVe). However, such methods only exploit singlemodal
knowledge from a language model, while ignoring the rich semantic information
inherent in image-text pairs. Instead, recently developed open-vocabulary (OV)
based methods succeed in exploiting such information of image-text pairs in
object detection, and achieve impressive performance. Inspired by the success
of OV-based methods, we propose a novel open-vocabulary framework, named
multimodal knowledge transfer (MKT), for multi-label classification.
Specifically, our method exploits multi-modal knowledge of image-text pairs
based on a vision and language pretraining (VLP) model. To facilitate
transferring the imagetext matching ability of VLP model, knowledge
distillation is used to guarantee the consistency of image and label
embeddings, along with prompt tuning to further update the label embeddings. To
further recognize multiple objects, a simple but effective two-stream module is
developed to capture both local and global features. Extensive experimental
results show that our method significantly outperforms state-of-theart methods
on public benchmark datasets. Code will be available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Vision-based Uneven BEV Representation Learning with Polar Rasterization  and Surface Estimation</b></summary>
  <p><b>编号</b>：[127]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01878</p>
  <p><b>作者</b>：Zhi Liu,  Shaoyu Chen,  Xiaojie Guo,  Xinggang Wang,  Tianheng Cheng,  Hongmei Zhu,  Qian Zhang,  Wenyu Liu,  Yi Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：vision-based uneven BEV, BEV representation learning, uneven BEV representation, vision-based uneven, uneven BEV</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we propose PolarBEV for vision-based uneven BEV representation
learning. To adapt to the foreshortening effect of camera imaging, we rasterize
the BEV space both angularly and radially, and introduce polar embedding
decomposition to model the associations among polar grids. Polar grids are
rearranged to an array-like regular representation for efficient processing.
Besides, to determine the 2D-to-3D correspondence, we iteratively update the
BEV surface based on a hypothetical plane, and adopt height-based feature
transformation. PolarBEV keeps real-time inference speed on a single 2080Ti
GPU, and outperforms other methods for both BEV semantic segmentation and BEV
instance segmentation. Thorough ablations are presented to validate the design.
The code will be released at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：Latents2Segments: Disentangling the Latent Space of Generative Models  for Semantic Segmentation of Face Images</b></summary>
  <p><b>编号</b>：[130]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01871</p>
  <p><b>作者</b>：Snehal Singh Tomar,  A.N. Rajagopalan</p>
  <p><b>备注</b>：5 pages, 4 figures, 2 tables. The paper has already been accepted to and presented at CVPR Workshop on Computer Vision for Augmented and Virtual Reality, New Orleans, LA, 2022</p>
  <p><b>关键词</b>：Virtual Reality applications, Augmented and Virtual, Virtual Reality, controlled style edits, number of Augmented</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the advent of an increasing number of Augmented and Virtual Reality
applications that aim to perform meaningful and controlled style edits on
images of human faces, the impetus for the task of parsing face images to
produce accurate and fine-grained semantic segmentation maps is more than ever
before. Few State of the Art (SOTA) methods which solve this problem, do so by
incorporating priors with respect to facial structure or other face attributes
such as expression and pose in their deep classifier architecture. Our
endeavour in this work is to do away with the priors and complex pre-processing
operations required by SOTA multi-class face segmentation models by reframing
this operation as a downstream task post infusion of disentanglement with
respect to facial semantic regions of interest (ROIs) in the latent space of a
Generative Autoencoder model. We present results for our model's performance on
the CelebAMask-HQ and HELEN datasets. The encoded latent space of our model
achieves significantly higher disentanglement with respect to semantic ROIs
than that of other SOTA works. Moreover, it achieves a 13\% faster inference
rate and comparable accuracy with respect to the publicly available SOTA for
the downstream task of semantic segmentation of face images.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Distance Matters in Human-Object Interaction Detection</b></summary>
  <p><b>编号</b>：[132]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01869</p>
  <p><b>作者</b>：Guangzhi Wang,  Yangyang Guo,  Yongkang Wong,  Mohan Kankanhalli</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：received considerable attention, distant interactions, received considerable, distant, interactions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Human-Object Interaction (HOI) detection has received considerable attention
in the context of scene understanding. Despite the growing progress on
benchmarks, we realize that existing methods often perform unsatisfactorily on
distant interactions, where the leading causes are two-fold: 1) Distant
interactions are by nature more difficult to recognize than close ones. A
natural scene often involves multiple humans and objects with intricate spatial
relations, making the interaction recognition for distant human-object largely
affected by complex visual context. 2) Insufficient number of distant
interactions in benchmark datasets results in under-fitting on these instances.
To address these problems, in this paper, we propose a novel two-stage method
for better handling distant interactions in HOI detection. One essential
component in our method is a novel Far Near Distance Attention module. It
enables information propagation between humans and objects, whereby the spatial
distance is skillfully taken into consideration. Besides, we devise a novel
Distance-Aware loss function which leads the model to focus more on distant yet
rare interactions. We conduct extensive experiments on two challenging datasets
- HICO-DET and V-COCO. The results demonstrate that the proposed method can
surpass existing approaches by a large margin, resulting in new
state-of-the-art performance.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Bayesian approaches for Quantifying Clinicians' Variability in Medical  Image Quantification</b></summary>
  <p><b>编号</b>：[133]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01868</p>
  <p><b>作者</b>：Jaeik Jeon,  Yeonggul Jang,  Youngtaek Hong,  Hackjoon Shim,  Sekeun Kim</p>
  <p><b>备注</b>：9 pages, 8 figures</p>
  <p><b>关键词</b>：plays a vital, vital role, including MRI, clinical decisions, Bayesian predictive distribution</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Medical imaging, including MRI, CT, and Ultrasound, plays a vital role in
clinical decisions. Accurate segmentation is essential to measure the structure
of interest from the image. However, manual segmentation is highly
operator-dependent, which leads to high inter and intra-variability of
quantitative measurements. In this paper, we explore the feasibility that
Bayesian predictive distribution parameterized by deep neural networks can
capture the clinicians' inter-intra variability. By exploring and analyzing
recently emerged approximate inference schemes, we evaluate whether approximate
Bayesian deep learning with the posterior over segmentations can learn
inter-intra rater variability both in segmentation and clinical measurements.
The experiments are performed with two different imaging modalities: MRI and
ultrasound. We empirically demonstrated that Bayesian predictive distribution
parameterized by deep neural networks could approximate the clinicians'
inter-intra variability. We show a new perspective in analyzing medical images
quantitatively by providing clinical measurement uncertainty.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Efficient Representation Learning via Adaptive Context Pooling</b></summary>
  <p><b>编号</b>：[140]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01844</p>
  <p><b>作者</b>：Chen Huang,  Walter Talbott,  Navdeep Jaitly,  Josh Susskind</p>
  <p><b>备注</b>：ICML 2022</p>
  <p><b>关键词</b>：Self-attention mechanisms model, Self-attention mechanisms, attention, input tokens, mechanisms model long-range</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Self-attention mechanisms model long-range context by using pairwise
attention between all input tokens. In doing so, they assume a fixed attention
granularity defined by the individual tokens (e.g., text characters or image
pixels), which may not be optimal for modeling complex dependencies at higher
levels. In this paper, we propose ContextPool to address this problem by
adapting the attention granularity for each token. Inspired by the success of
ConvNets that are combined with pooling to capture long-range dependencies, we
learn to pool neighboring features for each token before computing attention in
a given attention layer. The pooling weights and support size are adaptively
determined, allowing the pooled features to encode meaningful context with
varying scale. We show that ContextPool makes attention models more expressive,
achieving strong performance often with fewer layers and thus significantly
reduced cost. Experiments validate that our ContextPool module, when plugged
into transformer models, matches or surpasses state-of-the-art performance
using less compute on several language and image benchmarks, outperforms recent
works with learned context sizes or sparse attention patterns, and is also
applicable to ConvNets for efficient feature learning.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：ORF-Net: Deep Omni-supervised Rib Fracture Detection from Chest CT Scans</b></summary>
  <p><b>编号</b>：[142]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01842</p>
  <p><b>作者</b>：Zhizhong Chai,  Huangjing Lin,  Luyang Luo,  Pheng-Ann Heng,  Hao Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：bounding box annotation, bounding box, precise annotated box, box annotation, works are based</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Most of the existing object detection works are based on the bounding box
annotation: each object has a precise annotated box. However, for rib
fractures, the bounding box annotation is very labor-intensive and
time-consuming because radiologists need to investigate and annotate the rib
fractures on a slice-by-slice basis. Although a few studies have proposed
weakly-supervised methods or semi-supervised methods, they could not handle
different forms of supervision simultaneously. In this paper, we proposed a
novel omni-supervised object detection network, which can exploit multiple
different forms of annotated data to further improve the detection performance.
Specifically, the proposed network contains an omni-supervised detection head,
in which each form of annotation data corresponds to a unique classification
branch. Furthermore, we proposed a dynamic label assignment strategy for
different annotated forms of data to facilitate better learning for each
branch. Moreover, we also design a confidence-aware classification loss to
emphasize the samples with high confidence and further improve the model's
performance. Extensive experiments conducted on the testing dataset show our
proposed method outperforms other state-of-the-art approaches consistently,
demonstrating the efficacy of deep omni-supervised learning on improving rib
fracture detection performance.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Learning Local Implicit Fourier Representation for Image Warping</b></summary>
  <p><b>编号</b>：[149]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01831</p>
  <p><b>作者</b>：Jaewon Lee,  Kwang Pyo Choi,  Kyong Hwan Jin</p>
  <p><b>备注</b>：ECCV 2022 camera-ready version (this https URL)</p>
  <p><b>关键词</b>：reshape images defined, aims to reshape, defined on rectangular, rectangular grids, Image warping aims</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image warping aims to reshape images defined on rectangular grids into
arbitrary shapes. Recently, implicit neural functions have shown remarkable
performances in representing images in a continuous manner. However, a
standalone multi-layer perceptron suffers from learning high-frequency Fourier
coefficients. In this paper, we propose a local texture estimator for image
warping (LTEW) followed by an implicit neural representation to deform images
into continuous shapes. Local textures estimated from a deep super-resolution
(SR) backbone are multiplied by locally-varying Jacobian matrices of a
coordinate transformation to predict Fourier responses of a warped image. Our
LTEW-based neural function outperforms existing warping methods for
asymmetric-scale SR and homography transform. Furthermore, our algorithm well
generalizes arbitrary coordinate transformations, such as homography transform
with a large magnification factor and equirectangular projection (ERP)
perspective transform, which are not provided in training.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Scene-Aware Prompt for Multi-modal Dialogue Understanding and Generation</b></summary>
  <p><b>编号</b>：[150]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01823</p>
  <p><b>作者</b>：Bin Li,  Yixuan Weng,  Ziyu Ma,  Bin Sun,  Shutao Li</p>
  <p><b>备注</b>：Accepted in NLPCC 2022</p>
  <p><b>关键词</b>：Team LingJing experiments, schemes of Team, Team LingJing, Multi-modal Dialogue Understanding, paper introduces</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper introduces the schemes of Team LingJing's experiments in
NLPCC-2022-Shared-Task-4 Multi-modal Dialogue Understanding and Generation
(MDUG). The MDUG task can be divided into two phases: multi-modal context
understanding and response generation. To fully leverage the visual information
for both scene understanding and dialogue generation, we propose the
scene-aware prompt for the MDUG task. Specifically, we utilize the
multi-tasking strategy for jointly modelling the scene- and session-
multi-modal understanding. The visual captions are adopted to aware the scene
information, while the fixed-type templated prompt based on the scene- and
session-aware labels are used to further improve the dialogue generation
performance. Extensive experimental results show that the proposed method has
achieved state-of-the-art (SOTA) performance compared with other competitive
methods, where we rank the 1-st in all three subtasks in this MDUG competition.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Toward Explainable and Fine-Grained 3D Grounding through Referring  Textual Phrases</b></summary>
  <p><b>编号</b>：[152]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01821</p>
  <p><b>作者</b>：Zhihao Yuan,  Xu Yan,  Zhuo Li,  Xuhao Li,  Yao Guo,  Shuguang Cui,  Zhen Li</p>
  <p><b>备注</b>：New dataset for 3D visual grounding</p>
  <p><b>关键词</b>：explored visual grounding, target object, Phrase Aware Grounding, Recent progress, language description</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent progress on 3D scene understanding has explored visual grounding
(3DVG) to localize a target object through a language description. However,
existing methods only consider the dependency between the entire sentence and
the target object, thus ignoring fine-grained relationships between contexts
and non-target ones. In this paper, we extend 3DVG to a more reliable and
explainable task, called 3D Phrase Aware Grounding (3DPAG). The 3DPAG task aims
to localize the target object in the 3D scenes by explicitly identifying all
phrase-related objects and then conducting reasoning according to contextual
phrases. To tackle this problem, we label about 400K phrase-level annotations
from 170K sentences in available 3DVG datasets, i.e., Nr3D, Sr3D and ScanRefer.
By tapping on these developed datasets, we propose a novel framework, i.e.,
PhraseRefer, which conducts phrase-aware and object-level representation
learning through phrase-object alignment optimization as well as
phrase-specific pre-training. In our setting, we extend previous 3DVG methods
to the phrase-aware scenario and provide metrics to measure the explainability
of the 3DPAG task. Extensive results confirm that 3DPAG effectively boosts the
3DVG, and PhraseRefer achieves state-of-the-arts across three datasets, i.e.,
63.0%, 54.4% and 55.5% overall accuracy on Sr3D, Nr3D and ScanRefer,
respectively.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Aesthetic Attribute Assessment of Images Numerically on Mixed  Multi-attribute Datasets</b></summary>
  <p><b>编号</b>：[157]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01806</p>
  <p><b>作者</b>：Xin Jin,  Xinning Li,  Hao Lou,  Chenyu Fan,  Qiang Deng,  Chaoen Xiao,  Shuai Cui,  Amit Kumar Singh</p>
  <p><b>备注</b>：7 pages, 9figures, to appear: ACM Transactions on Multimedia Computing Communications and Applications (TOMM)</p>
  <p><b>关键词</b>：multimedia technology, information and socializing, continuous development, development of social, social software</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the continuous development of social software and multimedia technology,
images have become a kind of important carrier for spreading information and
socializing. How to evaluate an image comprehensively has become the focus of
recent researches. The traditional image aesthetic assessment methods often
adopt single numerical overall assessment scores, which has certain
subjectivity and can no longer meet the higher aesthetic requirements. In this
paper, we construct an new image attribute dataset called aesthetic mixed
dataset with attributes(AMD-A) and design external attribute features for
fusion. Besides, we propose a efficient method for image aesthetic attribute
assessment on mixed multi-attribute dataset and construct a multitasking
network architecture by using the EfficientNet-B0 as the backbone network. Our
model can achieve aesthetic classification, overall scoring and attribute
scoring. In each sub-network, we improve the feature extraction through ECA
channel attention module. As for the final overall scoring, we adopt the idea
of the teacher-student network and use the classification sub-network to guide
the aesthetic overall fine-grain regression. Experimental results, using the
MindSpore, show that our proposed method can effectively improve the
performance of the aesthetic overall and attribute assessment.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：ReMix: A General and Efficient Framework for Multiple Instance Learning  based Whole Slide Image Classification</b></summary>
  <p><b>编号</b>：[158]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01805</p>
  <p><b>作者</b>：Jiawei Yang,  Hanbo Chen,  Yu Zhao,  Fan Yang,  Yao Zhang,  Lei He,  Jianhua Yao</p>
  <p><b>备注</b>：Published in MICCAI 2022. Code: this https URL or this https URL</p>
  <p><b>关键词</b>：gigapixel resolution images, weakly supervised multiple, handle gigapixel resolution, deep weakly supervised, supervised multiple instance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Whole slide image (WSI) classification often relies on deep weakly supervised
multiple instance learning (MIL) methods to handle gigapixel resolution images
and slide-level labels. Yet the decent performance of deep learning comes from
harnessing massive datasets and diverse samples, urging the need for efficient
training pipelines for scaling to large datasets and data augmentation
techniques for diversifying samples. However, current MIL-based WSI
classification pipelines are memory-expensive and computation-inefficient since
they usually assemble tens of thousands of patches as bags for computation. On
the other hand, despite their popularity in other tasks, data augmentations are
unexplored for WSI MIL frameworks. To address them, we propose ReMix, a general
and efficient framework for MIL based WSI classification. It comprises two
steps: reduce and mix. First, it reduces the number of instances in WSI bags by
substituting instances with instance prototypes, i.e., patch cluster centroids.
Then, we propose a ``Mix-the-bag'' augmentation that contains four online,
stochastic and flexible latent space augmentations. It brings diverse and
reliable class-identity-preserving semantic changes in the latent space while
enforcing semantic-perturbation invariance. We evaluate ReMix on two public
datasets with two state-of-the-art MIL methods. In our experiments, consistent
improvements in precision, accuracy, and recall have been achieved but with
orders of magnitude reduced training time and memory consumption, demonstrating
ReMix's effectiveness and efficiency. Code is available.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：GSMFlow: Generation Shifts Mitigating Flow for Generalized Zero-Shot  Learning</b></summary>
  <p><b>编号</b>：[161]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01798</p>
  <p><b>作者</b>：Zhi Chen,  Yadan Luo,  Ruihong Qiu,  Sen Wang,  Zi Huang,  Jingjing Li,  Zheng Zhang</p>
  <p><b>备注</b>：IEEE Transactions on Multimedia 2022. arXiv admin note: substantial text overlap with arXiv:2107.03163</p>
  <p><b>关键词</b>：Generalized Zero-Shot Learning, transferring semantic knowledge, unseen classes, Generalized Zero-Shot, aims to recognize</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generalized Zero-Shot Learning (GZSL) aims to recognize images from both the
seen and unseen classes by transferring semantic knowledge from seen to unseen
classes. It is a promising solution to take the advantage of generative models
to hallucinate realistic unseen samples based on the knowledge learned from the
seen classes. However, due to the generation shifts, the synthesized samples by
most existing methods may drift from the real distribution of the unseen data.
To address this issue, we propose a novel flow-based generative framework that
consists of multiple conditional affine coupling layers for learning unseen
data generation. Specifically, we discover and address three potential problems
that trigger the generation shifts, i.e., semantic inconsistency, variance
collapse, and structure disorder. First, to enhance the reflection of the
semantic information in the generated samples, we explicitly embed the semantic
information into the transformation in each conditional affine coupling layer.
Second, to recover the intrinsic variance of the real unseen features, we
introduce a boundary sample mining strategy with entropy maximization to
discover more difficult visual variants of semantic prototypes and hereby
adjust the decision boundary of the classifiers. Third, a relative positioning
strategy is proposed to revise the attribute embeddings, guiding them to fully
preserve the inter-class geometric structure and further avoid structure
disorder in the semantic space. Extensive experimental results on four GZSL
benchmark datasets demonstrate that GSMFlow achieves the state-of-the-art
performance on GZSL.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Deep Parametric 3D Filters for Joint Video Denoising and Illumination  Enhancement in Video Super Resolution</b></summary>
  <p><b>编号</b>：[162]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01797</p>
  <p><b>作者</b>：Xiaogang Xu,  Ruixing Wang,  Chi-Wing Fu,  Jiaya Jia</p>
  <p><b>备注</b>：under submission</p>
  <p><b>关键词</b>：quality improvement brought, low-light and noisy, improvement brought, video super-resolution, quality improvement</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite the quality improvement brought by the recent methods, video
super-resolution (SR) is still very challenging, especially for videos that are
low-light and noisy. The current best solution is to subsequently employ best
models of video SR, denoising, and illumination enhancement, but doing so often
lowers the image quality, due to the inconsistency between the models. This
paper presents a new parametric representation called the Deep Parametric 3D
Filters (DP3DF), which incorporates local spatiotemporal information to enable
simultaneous denoising, illumination enhancement, and SR efficiently in a
single encoder-and-decoder network. Also, a dynamic residual frame is jointly
learned with the DP3DF via a shared backbone to further boost the SR quality.
We performed extensive experiments, including a large-scale user study, to show
our method's effectiveness. Our method consistently surpasses the best
state-of-the-art methods on all the challenging real datasets with top PSNR and
user ratings, yet having a very fast run time.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Task-agnostic Defense against Adversarial Patch Attacks</b></summary>
  <p><b>编号</b>：[164]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01795</p>
  <p><b>作者</b>：Ke Xu,  Yao Xiao,  Zhaoheng Zheng,  Kaijie Cai,  Ram Nevatia</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：mislead neural networks, attacks mislead neural, designated local region, patch attacks mislead, mislead neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Adversarial patch attacks mislead neural networks by injecting adversarial
pixels within a designated local region. Patch attacks can be highly effective
in a variety of tasks and physically realizable via attachment (e.g. a sticker)
to the real-world objects. Despite the diversity in attack patterns,
adversarial patches tend to be highly textured and different in appearance from
natural images. We exploit this property and present PatchZero, a task-agnostic
defense against white-box adversarial patches. Specifically, our defense
detects the adversarial pixels and "zeros out" the patch region by repainting
with mean pixel values. We formulate the patch detection problem as a semantic
segmentation task such that our model can generalize to patches of any size and
shape. We further design a two-stage adversarial training scheme to defend
against the stronger adaptive attacks. We thoroughly evaluate PatchZero on the
image classification (ImageNet, RESISC45), object detection (PASCAL VOC), and
video classification (UCF101) datasets. Our method achieves SOTA robust
accuracy without any degradation in the benign performance.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：A deep cascade of ensemble of dual domain networks with gradient-based  T1 assistance and perceptual refinement for fast MRI reconstruction</b></summary>
  <p><b>编号</b>：[167]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01791</p>
  <p><b>作者</b>：Balamurali Murugesan,  Sriprabha Ramanarayanan,  Sricharan Vijayarangan,  Keerthi Ram,  Naranamangalam R Jagannathan,  Mohanasankar Sivaprakasam</p>
  <p><b>备注</b>：Accepted in CMIG 2021</p>
  <p><b>关键词</b>：shown promising results, fast magnetic resonance, magnetic resonance imaging, Deep learning networks, shown promising</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning networks have shown promising results in fast magnetic
resonance imaging (MRI) reconstruction. In our work, we develop deep networks
to further improve the quantitative and the perceptual quality of
reconstruction. To begin with, we propose reconsynergynet (RSN), a network that
combines the complementary benefits of independently operating on both the
image and the Fourier domain. For a single-coil acquisition, we introduce deep
cascade RSN (DC-RSN), a cascade of RSN blocks interleaved with data fidelity
(DF) units. Secondly, we improve the structure recovery of DC-RSN for T2
weighted Imaging (T2WI) through assistance of T1 weighted imaging (T1WI), a
sequence with short acquisition time. T1 assistance is provided to DC-RSN
through a gradient of log feature (GOLF) fusion. Furthermore, we propose
perceptual refinement network (PRN) to refine the reconstructions for better
visual information fidelity (VIF), a metric highly correlated to radiologists
opinion on the image quality. Lastly, for multi-coil acquisition, we propose
variable splitting RSN (VS-RSN), a deep cascade of blocks, each block
containing RSN, multi-coil DF unit, and a weighted average module. We
extensively validate our models DC-RSN and VS-RSN for single-coil and
multi-coil acquisitions and report the state-of-the-art performance. We obtain
a SSIM of 0.768, 0.923, 0.878 for knee single-coil-4x, multi-coil-4x, and
multi-coil-8x in fastMRI. We also conduct experiments to demonstrate the
efficacy of GOLF based T1 assistance and PRN.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：3D Part Assembly Generation with Instance Encoded Transformer</b></summary>
  <p><b>编号</b>：[173]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01779</p>
  <p><b>作者</b>：Rufeng Zhang,  Tao Kong,  Weihao Wang,  Xuan Han,  Mingyu You</p>
  <p><b>备注</b>：8 pages, 7 figures</p>
  <p><b>关键词</b>：enable robots capable, desirable to enable, capable of automatic, automatic assembly, parts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>It is desirable to enable robots capable of automatic assembly. Structural
understanding of object parts plays a crucial role in this task yet remains
relatively unexplored. In this paper, we focus on the setting of furniture
assembly from a complete set of part geometries, which is essentially a 6-DoF
part pose estimation problem. We propose a multi-layer transformer-based
framework that involves geometric and relational reasoning between parts to
update the part poses iteratively. We carefully design a unique instance
encoding to solve the ambiguity between geometrically-similar parts so that all
parts can be distinguished. In addition to assembling from scratch, we extend
our framework to a new task called in-process part assembly. Analogous to
furniture maintenance, it requires robots to continue with unfinished products
and assemble the remaining parts into appropriate positions. Our method
achieves far more than 10% improvements over the current state-of-the-art in
multiple metrics on the public PartNet dataset. Extensive experiments and
quantitative comparisons demonstrate the effectiveness of the proposed
framework.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Object-Level Targeted Selection via Deep Template Matching</b></summary>
  <p><b>编号</b>：[174]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01778</p>
  <p><b>作者</b>：Suraj Kothawade,  Donna Roy,  Michele Fenzi,  Elmar Haussmann,  Jose M. Alvarez,  Christoph Angerer</p>
  <p><b>备注</b>：In Proceedings of the Intelligent Vehicles Symposium, IV 2022</p>
  <p><b>关键词</b>：OOI, images, data, image, extra labeled data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Retrieving images with objects that are semantically similar to objects of
interest (OOI) in a query image has many practical use cases. A few examples
include fixing failures like false negatives/positives of a learned model or
mitigating class imbalance in a dataset. The targeted selection task requires
finding the relevant data from a large-scale pool of unlabeled data. Manual
mining at this scale is infeasible. Further, the OOI are often small and occupy
less than 1% of image area, are occluded, and co-exist with many semantically
different objects in cluttered scenes. Existing semantic image retrieval
methods often focus on mining for larger sized geographical landmarks, and/or
require extra labeled data, such as images/image-pairs with similar objects,
for mining images with generic objects. We propose a fast and robust template
matching algorithm in the DNN feature space, that retrieves semantically
similar images at the object-level from a large unlabeled pool of data. We
project the region(s) around the OOI in the query image to the DNN feature
space for use as the template. This enables our method to focus on the
semantics of the OOI without requiring extra labeled data. In the context of
autonomous driving, we evaluate our system for targeted selection by using
failure cases of object detectors as OOI. We demonstrate its efficacy on a
large unlabeled dataset with 2.2M images and show high recall in mining for
images with small-sized OOI. We compare our method against a well-known
semantic image retrieval method, which also does not require extra labeled
data. Lastly, we show that our method is flexible and retrieves images with one
or more semantically different co-occurring OOI seamlessly.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：SESS: Saliency Enhancing with Scaling and Sliding</b></summary>
  <p><b>编号</b>：[179]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01769</p>
  <p><b>作者</b>：Osman Tursun,  Simon Denman,  Sridha Sridharan,  Clinton Fookes</p>
  <p><b>备注</b>：This paper will be presented at ECCV2022</p>
  <p><b>关键词</b>：machine learning application, saliency, learning application areas, application areas including, areas including explainable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>High-quality saliency maps are essential in several machine learning
application areas including explainable AI and weakly supervised object
detection and segmentation. Many techniques have been developed to generate
better saliency using neural networks. However, they are often limited to
specific saliency visualisation methods or saliency issues. We propose a novel
saliency enhancing approach called SESS (Saliency Enhancing with Scaling and
Sliding). It is a method and model agnostic extension to existing saliency map
generation methods. With SESS, existing saliency approaches become robust to
scale variance, multiple occurrences of target objects, presence of distractors
and generate less noisy and more discriminative saliency maps. SESS improves
saliency by fusing saliency maps extracted from multiple patches at different
scales from different areas, and combines these individual maps using a novel
fusion scheme that incorporates channel-wise weights and spatial weighted
average. To improve efficiency, we introduce a pre-filtering step that can
exclude uninformative saliency maps to improve efficiency while still enhancing
overall results. We evaluate SESS on object recognition and detection
benchmarks where it achieves significant improvement. The code is released
publicly to enable researchers to verify performance and further development.
Code is available at: this https URL</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Rank-Based Filter Pruning for Real-Time UAV Tracking</b></summary>
  <p><b>编号</b>：[180]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01768</p>
  <p><b>作者</b>：Xucheng Wang,  Dan Zeng,  Qijun Zhao,  Shuiwang Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Unmanned aerial vehicle, wide potential applications, UAV, UAV tracking, Unmanned aerial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Unmanned aerial vehicle (UAV) tracking has wide potential applications in
such as agriculture, navigation, and public security. However, the limitations
of computing resources, battery capacity, and maximum load of UAV hinder the
deployment of deep learning-based tracking algorithms on UAV. Consequently,
discriminative correlation filters (DCF) trackers stand out in the UAV tracking
community because of their high efficiency. However, their precision is usually
much lower than trackers based on deep learning. Model compression is a
promising way to narrow the gap (i.e., effciency, precision) between DCF- and
deep learning- based trackers, which has not caught much attention in UAV
tracking. In this paper, we propose the P-SiamFC++ tracker, which is the first
to use rank-based filter pruning to compress the SiamFC++ model, achieving a
remarkable balance between efficiency and precision. Our method is general and
may encourage further studies on UAV tracking with model compression. Extensive
experiments on four UAV benchmarks, including UAV123@10fps, DTB70, UAVDT and
Vistrone2018, show that P-SiamFC++ tracker significantly outperforms
state-of-the-art UAV tracking methods.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：GP22: A Car Styling Dataset for Automotive Designers</b></summary>
  <p><b>编号</b>：[184]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01760</p>
  <p><b>作者</b>：Gyunpyo Lee,  Taesu Kim,  Hyeon-Jeong Suk</p>
  <p><b>备注</b>：5th CVFAD workshop, CVPR2022</p>
  <p><b>关键词</b>：automated design data, design data archiving, creatively and effectively, data archiving, archiving could reduce</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>An automated design data archiving could reduce the time wasted by designers
from working creatively and effectively. Though many datasets on classifying,
detecting, and instance segmenting on car exterior exist, these large datasets
are not relevant for design practices as the primary purpose lies in autonomous
driving or vehicle verification. Therefore, we release GP22, composed of car
styling features defined by automotive designers. The dataset contains 1480 car
side profile images from 37 brands and ten car segments. It also contains
annotations of design features that follow the taxonomy of the car exterior
design features defined in the eye of the automotive designer. We trained the
baseline model using YOLO v5 as the design feature detection model with the
dataset. The presented model resulted in an mAP score of 0.995 and a recall of
0.984. Furthermore, exploration of the model performance on sketches and
rendering images of the car side profile implies the scalability of the dataset
for design purposes.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：Universal Domain Adaptive Object Detector</b></summary>
  <p><b>编号</b>：[185]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01756</p>
  <p><b>作者</b>：Wenxu Shi,  Lei Zhang,  Weijie Chen,  Shiliang Pu</p>
  <p><b>备注</b>：Accepted to ACM MM2022</p>
  <p><b>关键词</b>：Universal domain adaptive, domain adaptive object, adaptive object detection, Adaptive Faster RCNN, Universal Scale-Aware Domain</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Universal domain adaptive object detection (UniDAOD)is more challenging than
domain adaptive object detection (DAOD) since the label space of the source
domain may not be the same as that of the target and the scale of objects in
the universal scenarios can vary dramatically (i.e, category shift and scale
shift). To this end, we propose US-DAF, namely Universal Scale-Aware Domain
Adaptive Faster RCNN with Multi-Label Learning, to reduce the negative transfer
effect during training while maximizing transferability as well as
discriminability in both domains under a variety of scales. Specifically, our
method is implemented by two modules: 1) We facilitate the feature alignment of
common classes and suppress the interference of private classes by designing a
Filter Mechanism module to overcome the negative transfer caused by category
shift. 2) We fill the blank of scale-aware adaptation in object detection by
introducing a new Multi-Label Scale-Aware Adapter to perform individual
alignment between the corresponding scale for two domains. Experiments show
that US-DAF achieves state-of-the-art results on three scenarios (i.e,
Open-Set, Partial-Set, and Closed-Set) and yields 7.1% and 5.9% relative
improvement on benchmark datasets Clipart1k and Watercolor in particular.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Attention Guided Network for Salient Object Detection in Optical Remote  Sensing Images</b></summary>
  <p><b>编号</b>：[186]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01755</p>
  <p><b>作者</b>：Yuhan Lin,  Han Sun,  Ningzhong Liu,  Yetong Bian,  Jun Cen,  Huiyu Zhou</p>
  <p><b>备注</b>：accepted by ICANN2022, The code is available at this https URL</p>
  <p><b>关键词</b>：remote sensing images, optical remote sensing, sensing images, remote sensing, difficult task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Due to the extreme complexity of scale and shape as well as the uncertainty
of the predicted location, salient object detection in optical remote sensing
images (RSI-SOD) is a very difficult task. The existing SOD methods can satisfy
the detection performance for natural scene images, but they are not well
adapted to RSI-SOD due to the above-mentioned image characteristics in remote
sensing images. In this paper, we propose a novel Attention Guided Network
(AGNet) for SOD in optical RSIs, including position enhancement stage and
detail refinement stage. Specifically, the position enhancement stage consists
of a semantic attention module and a contextual attention module to accurately
describe the approximate location of salient objects. The detail refinement
stage uses the proposed self-refinement module to progressively refine the
predicted results under the guidance of attention and reverse attention. In
addition, the hybrid loss is applied to supervise the training of the network,
which can improve the performance of the model from three perspectives of
pixel, region and statistics. Extensive experiments on two popular benchmarks
demonstrate that AGNet achieves competitive performance compared to other
state-of-the-art methods. The code will be available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：Anomaly-aware multiple instance learning for rare anemia disorder  classification</b></summary>
  <p><b>编号</b>：[194]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01742</p>
  <p><b>作者</b>：Salome Kazeminia,  Ario Sadafi,  Asya Makhro,  Anna Bogdanova,  Shadi Albarqouni,  Carsten Marr</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Deep learning-based classification, Deep learning-based, instance-level annotations, training data, rare anemia</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning-based classification of rare anemia disorders is challenged by
the lack of training data and instance-level annotations. Multiple Instance
Learning (MIL) has shown to be an effective solution, yet it suffers from low
accuracy and limited explainability. Although the inclusion of attention
mechanisms has addressed these issues, their effectiveness highly depends on
the amount and diversity of cells in the training samples. Consequently, the
poor machine learning performance on rare anemia disorder classification from
blood samples remains unresolved. In this paper, we propose an interpretable
pooling method for MIL to address these limitations. By benefiting from
instance-level information of negative bags (i.e., homogeneous benign cells
from healthy individuals), our approach increases the contribution of anomalous
instances. We show that our strategy outperforms standard MIL classification
algorithms and provides a meaningful explanation behind its decisions.
Moreover, it can denote anomalous instances of rare blood diseases that are not
seen during the training phase.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：Are metrics measuring what they should? An evaluation of image  captioning task metrics</b></summary>
  <p><b>编号</b>：[199]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01733</p>
  <p><b>作者</b>：Othón González-Chávez,  Guillermo Ruiz,  Daniela Moctezuma,  Tania A. Ramirez-delReal</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Image Captioning, Image Captioning metrics, Image Captioning methods, Image, Captioning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image Captioning is a current research task to describe the image content
using the objects and their relationships in the scene. To tackle this task,
two important research areas are used, artificial vision, and natural language
processing. In Image Captioning, as in any computational intelligence task, the
performance metrics are crucial for knowing how well (or bad) a method
performs. In recent years, it has been observed that classical metrics based on
n-grams are insufficient to capture the semantics and the critical meaning to
describe the content in an image. Looking to measure how well or not the set of
current and more recent metrics are doing, in this manuscript, we present an
evaluation of several kinds of Image Captioning metrics and a comparison
between them using the well-known MS COCO dataset. For this, we designed two
scenarios; 1) a set of artificially build captions with several quality, and 2)
a comparison of some state-of-the-art Image Captioning methods. We tried to
answer the questions: Are the current metrics helping to produce high quality
captions? How do actual metrics compare to each other? What are the metrics
really measuring?</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：How Much More Data Do I Need? Estimating Requirements for Downstream  Tasks</b></summary>
  <p><b>编号</b>：[201]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01725</p>
  <p><b>作者</b>：Rafid Mahmood,  James Lucas,  David Acuna,  Daiqing Li,  Jonah Philion,  Jose M. Alvarez,  Zhiding Yu,  Sanja Fidler,  Marc T. Law</p>
  <p><b>备注</b>：Accepted to CVPR 2022</p>
  <p><b>关键词</b>：small training data, small training, data, training data set, data set</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Given a small training data set and a learning algorithm, how much more data
is necessary to reach a target validation or test performance? This question is
of critical importance in applications such as autonomous driving or medical
imaging where collecting data is expensive and time-consuming. Overestimating
or underestimating data requirements incurs substantial costs that could be
avoided with an adequate budget. Prior work on neural scaling laws suggest that
the power-law function can fit the validation performance curve and extrapolate
it to larger data set sizes. We find that this does not immediately translate
to the more difficult downstream task of estimating the required data set size
to meet a target performance. In this work, we consider a broad class of
computer vision tasks and systematically investigate a family of functions that
generalize the power-law function to allow for better estimation of data
requirements. Finally, we show that incorporating a tuned correction factor and
collecting over multiple rounds significantly improves the performance of the
data estimators. Using our guidelines, practitioners can accurately estimate
data requirements of machine learning systems to gain savings in both
development time and data acquisition costs.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：Adaptive Fine-Grained Sketch-Based Image Retrieval</b></summary>
  <p><b>编号</b>：[202]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01723</p>
  <p><b>作者</b>：Ayan Kumar Bhunia,  Aneeshan Sain,  Parth Shah,  Animesh Gupta,  Pinaki Nath Chowdhury,  Tao Xiang,  Yi-Zhe Song</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Fine-Grained Sketch-Based Image, Sketch-Based Image Retrieval, Sketch-Based Image, Image Retrieval, recent focus</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The recent focus on Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) has
shifted towards generalising a model to new categories without any training
data from them. In real-world applications, however, a trained FG-SBIR model is
often applied to both new categories and different human sketchers, i.e.,
different drawing styles. Although this complicates the generalisation problem,
fortunately, a handful of examples are typically available, enabling the model
to adapt to the new category/style. In this paper, we offer a novel perspective
-- instead of asking for a model that generalises, we advocate for one that
quickly adapts, with just very few samples during testing (in a few-shot
manner). To solve this new problem, we introduce a novel model-agnostic
meta-learning (MAML) based framework with several key modifications: (1) As a
retrieval task with a margin-based contrastive loss, we simplify the MAML
training in the inner loop to make it more stable and tractable. (2) The margin
in our contrastive loss is also meta-learned with the rest of the model. (3)
Three additional regularisation losses are introduced in the outer loop, to
make the meta-learned FG-SBIR model more effective for category/style
adaptation. Extensive experiments on public datasets suggest a large gain over
generalisation and zero-shot based approaches, and a few strong few-shot
baselines.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：Disentangled Action Recognition with Knowledge Bases</b></summary>
  <p><b>编号</b>：[206]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01708</p>
  <p><b>作者</b>：Zhekun Luo,  Shalini Ghosh,  Devin Guillory,  Keizo Kato,  Trevor Darrell,  Huijuan Xu</p>
  <p><b>备注</b>：NAACL 2022</p>
  <p><b>关键词</b>：video usually involves, involves the interaction, interaction of human, compositional action, compositional action nodes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Action in video usually involves the interaction of human with objects.
Action labels are typically composed of various combinations of verbs and
nouns, but we may not have training data for all possible combinations. In this
paper, we aim to improve the generalization ability of the compositional action
recognition model to novel verbs or novel nouns that are unseen during training
time, by leveraging the power of knowledge graphs. Previous work utilizes
verb-noun compositional action nodes in the knowledge graph, making it
inefficient to scale since the number of compositional action nodes grows
quadratically with respect to the number of verbs and nouns. To address this
issue, we propose our approach: Disentangled Action Recognition with
Knowledge-bases (DARK), which leverages the inherent compositionality of
actions. DARK trains a factorized model by first extracting disentangled
feature representations for verbs and nouns, and then predicting classification
weights using relations in external knowledge graphs. The type constraint
between verb and noun is extracted from external knowledge bases and finally
applied when composing actions. DARK has better scalability in the number of
objects and verbs, and achieves state-of-the-art performance on the Charades
dataset. We further propose a new benchmark split based on the Epic-kitchen
dataset which is an order of magnitude bigger in the numbers of classes and
samples, and benchmark various models on this benchmark.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：WPPG Net: A Non-contact Video Based Heart Rate Extraction Network  Framework with Compatible Training Capability</b></summary>
  <p><b>编号</b>：[211]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01697</p>
  <p><b>作者</b>：Weiyu Sun,  Xinyu Zhang,  Ying Chen,  Yun Ge,  Chunyu Ji,  Xiaolin Huang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：facial skin presents, skin presents subtle, presents subtle color, subtle color change, facial skin</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Our facial skin presents subtle color change known as remote
Photoplethysmography (rPPG) signal, from which we could extract the heart rate
of the subject. Recently many deep learning methods and related datasets on
rPPG signal extraction are proposed. However, because of the time consumption
blood flowing through our body and other factors, label waves such as BVP
signals have uncertain delays with real rPPG signals in some datasets, which
results in the difficulty on training of networks which output predicted rPPG
waves directly. In this paper, by analyzing the common characteristics on
rhythm and periodicity of rPPG signals and label waves, we propose a whole set
of training methodology which wraps these networks so that they could remain
efficient when be trained at the presence of frequent uncertain delay in
datasets and gain more precise and robust heart rate prediction results than
other delay-free rPPG extraction methods.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：TM2T: Stochastic and Tokenized Modeling for the Reciprocal Generation of  3D Human Motions and Texts</b></summary>
  <p><b>编号</b>：[212]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01696</p>
  <p><b>作者</b>：Chuan Guo,  Xinxin Xuo,  Sen Wang,  Li Cheng</p>
  <p><b>备注</b>：Accepted to ECCV 2022</p>
  <p><b>关键词</b>：intimate human sensing, human full-body motions, intimate human, human sensing, human full-body</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Inspired by the strong ties between vision and language, the two intimate
human sensing and communication modalities, our paper aims to explore the
generation of 3D human full-body motions from texts, as well as its reciprocal
task, shorthanded for text2motion and motion2text, respectively. To tackle the
existing challenges, especially to enable the generation of multiple distinct
motions from the same text, and to avoid the undesirable production of trivial
motionless pose sequences, we propose the use of motion token, a discrete and
compact motion representation. This provides one level playing ground when
considering both motions and text signals, as the motion and text tokens,
respectively. Moreover, our motion2text module is integrated into the inverse
alignment process of our text2motion training pipeline, where a significant
deviation of synthesized text from the input text would be penalized by a large
training loss; empirically this is shown to effectively improve performance.
Finally, the mappings in-between the two modalities of motions and texts are
facilitated by adapting the neural model for machine translation (NMT) to our
context. This autoregressive modeling of the distribution over discrete motion
tokens further enables non-deterministic production of pose sequences, of
variable lengths, from an input text. Our approach is flexible, could be used
for both text2motion and motion2text tasks. Empirical evaluations on two
benchmark datasets demonstrate the superior performance of our approach on both
tasks over a variety of state-of-the-art methods. Project page:
this https URL</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：Crime scene classification from skeletal trajectory analysis in  surveillance settings</b></summary>
  <p><b>编号</b>：[215]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01687</p>
  <p><b>作者</b>：Alina-Daniela Matei,  Estefania Talavera,  Maya Aghaei</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：core task actively, task actively pursued, real-world crime detection, Video anomaly analysis, computer vision</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Video anomaly analysis is a core task actively pursued in the field of
computer vision, with applications extending to real-world crime detection in
surveillance footage. In this work, we address the task of human-related crime
classification. In our proposed approach, the human body in video frames,
represented as skeletal joints trajectories, is used as the main source of
exploration. First, we introduce the significance of extending the ground truth
labels for HR-Crime dataset and hence, propose a supervised and unsupervised
methodology to generate trajectory-level ground truth labels. Next, given the
availability of the trajectory-level ground truth, we introduce a
trajectory-based crime classification framework. Ablation studies are conducted
with various architectures and feature fusion strategies for the representation
of the human trajectories. The conducted experiments demonstrate the
feasibility of the task and pave the path for further research in the field.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：Interaction Transformer for Human Reaction Generation</b></summary>
  <p><b>编号</b>：[216]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01685</p>
  <p><b>作者</b>：Baptiste Chopin,  Hao Tang,  Naima Otberdout,  Mohamed Daoudi,  Nicu Sebe</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：human reaction generation, challenging task, task of human, generation which aims, human reaction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We address the challenging task of human reaction generation which aims to
generate a corresponding reaction based on an input action. Most of the
existing works do not focus on generating and predicting the reaction and
cannot generate the motion when only the action is given as input. To address
this limitation, we propose a novel interaction Transformer (InterFormer)
consisting of a Transformer network with both temporal and spatial attentions.
Specifically, the temporal attention captures the temporal dependencies of the
motion of both characters and of their interaction, while the spatial attention
learns the dependencies between the different body parts of each character and
those which are part of the interaction. Moreover, we propose using graphs to
increase the performance of the spatial attention via an interaction distance
module that helps focus on nearby joints from both characters. Extensive
experiments on the SBU interaction, K3HI, and DuetDance datasets demonstrate
the effectiveness of InterFormer. Our method is general and can be used to
generate more complex and long-term interactions.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：Egocentric Video-Language Pretraining @ Ego4D Challenge 2022</b></summary>
  <p><b>编号</b>：[223]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01622</p>
  <p><b>作者</b>：Kevin Qinghong Lin,  Alex Jinpeng Wang,  Mattia Soldan,  Michael Wray,  Rui Yan,  Eric Zhongcong Xu,  Difei Gao,  Rongcheng Tu,  Wenzhe Zhao,  Weijie Kong,  Chengfei Cai,  Hongfa Wang,  Dima Damen,  Bernard Ghanem,  Wei Liu,  Mike Zheng Shou</p>
  <p><b>备注</b>：To appeared in CVPRW22. 4 pages, 2 figures, 5 tables. Code: this https URL arXiv admin note: substantial text overlap with arXiv:2206.01670. substantial text overlap with arXiv:2207.01334</p>
  <p><b>关键词</b>：including Natural, Object State Change, State Change Classification, Moment Query, Object State</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this report, we propose a video-language pretraining (VLP) based solution
\cite{kevin2022egovlp} for four Ego4D challenge tasks, including Natural
Language Query (NLQ), Moment Query (MQ), Object State Change Classification
(OSCC), and PNR Localization (PNR). Especially, we exploit the recently
released Ego4D dataset \cite{grauman2021ego4d} to pioneer Egocentric VLP from
pretraining dataset, pretraining objective, and development set. Based on the
above three designs, we develop a pretrained video-language model that is able
to transfer its egocentric video-text representation or video-only
representation to several video downstream tasks. Our Egocentric VLP achieves
10.46R@1&IoU @0.3 on NLQ, 10.33 mAP on MQ, 74% Acc on OSCC, 0.67 sec error on
PNR. The code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：Is a PET all you need? A multi-modal study for Alzheimer's disease using  3D CNNs</b></summary>
  <p><b>编号</b>：[229]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02094</p>
  <p><b>作者</b>：Marla Narazani,  Ignacio Sarasua,  Sebastian Pölsterl,  Aldana Lizarraga,  Igor Yakushev,  Christian Wachinger</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Alzheimer Disease, common form, difficult to diagnose, diagnose due, multifactorial etiology</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Alzheimer's Disease (AD) is the most common form of dementia and often
difficult to diagnose due to the multifactorial etiology of dementia. Recent
works on neuroimaging-based computer-aided diagnosis with deep neural networks
(DNNs) showed that fusing structural magnetic resonance images (sMRI) and
fluorodeoxyglucose positron emission tomography (FDG-PET) leads to improved
accuracy in a study population of healthy controls and subjects with AD.
However, this result conflicts with the established clinical knowledge that
FDG-PET better captures AD-specific pathologies than sMRI. Therefore, we
propose a framework for the systematic evaluation of multi-modal DNNs and
critically re-evaluate single- and multi-modal DNNs based on FDG-PET and sMRI
for binary healthy vs. AD, and three-way healthy/mild cognitive impairment/AD
classification. Our experiments demonstrate that a single-modality network
using FDG-PET performs better than MRI (accuracy 0.91 vs 0.87) and does not
show improvement when combined. This conforms with the established clinical
knowledge on AD biomarkers, but raises questions about the true benefit of
multi-modal DNNs. We argue that future work on multi-modal fusion should
systematically assess the contribution of individual modalities following our
proposed evaluation framework. Finally, we encourage the community to go beyond
healthy vs. AD classification and focus on differential diagnosis of dementia,
where fusing multi-modal image information conforms with a clinical need.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：A Densely Interconnected Network for Deep Learning Accelerated MRI</b></summary>
  <p><b>编号</b>：[231]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02073</p>
  <p><b>作者</b>：Jon Andre Ottesen,  Matthan W.A. Caan,  Inge Rasmus Groote,  Atle Bjørnerud</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：accelerated MRI reconstruction, improve accelerated MRI, deep learning reconstruction, learning reconstruction framework, accelerated MRI</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Objective: To improve accelerated MRI reconstruction through a densely
connected cascading deep learning reconstruction framework.
Materials and Methods: A cascading deep learning reconstruction framework
(baseline model) was modified by applying three architectural modifications:
Input-level dense connections between cascade inputs and outputs, an improved
deep learning sub-network, and long-range skip-connections between subsequent
deep learning networks. An ablation study was performed, where five model
configurations were trained on the NYU fastMRI neuro dataset with an end-to-end
scheme conjunct on four- and eight-fold acceleration. The trained models were
evaluated by comparing their respective structural similarity index measure
(SSIM), normalized mean square error (NMSE) and peak signal to noise ratio
(PSNR).
Results: The proposed densely interconnected residual cascading network
(DIRCN), utilizing all three suggested modifications, achieved a SSIM
improvement of 8% and 11% for four- and eight-fold acceleration, respectively.
For eight-fold acceleration, the model achieved a 23% decrease in the NMSE when
compared to the baseline model. In an ablation study, the individual
architectural modifications all contributed to this improvement, by reducing
the SSIM and NMSE with approximately 3% and 5% for four-fold acceleration,
respectively.
Conclusion: The proposed architectural modifications allow for simple
adjustments on an already existing cascading framework to further improve the
resulting reconstructions.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：Transformer based Models for Unsupervised Anomaly Segmentation in Brain  MR Images</b></summary>
  <p><b>编号</b>：[233]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02059</p>
  <p><b>作者</b>：Ahmed Ghorbel (1),  Ahmed Aldahdooh (1),  Shadi Albarqouni (2),  Wassim Hamidouche (1) ((1) Univ. Rennes, INSA Rennes, CNRS, IETR - UMR 6164, Rennes, France (2) University Hospital Bonn, Venusberg-Campus 1, D-53127, Bonn, Germany, Helmholtz Munich, Ingolstädter Landstraße 1, D-85764, Neuherberg, Germany, Technical University of Munich, Boltzmannstr. 3, D-85748 Garching, Germany)</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：physician workload, quality of patient, patient care, radiology is proportionate, Convolutional Neural Network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The quality of patient care associated with diagnostic radiology is
proportionate to a physician workload. Segmentation is a fundamental limiting
precursor to diagnostic and therapeutic procedures. Advances in Machine
Learning (ML) aim to increase diagnostic efficiency to replace single
application with generalized algorithms. In Unsupervised Anomaly Detection
(UAD), Convolutional Neural Network (CNN) based Autoencoders (AEs) and
Variational Autoencoders (VAEs) are considered as a de facto approach for
reconstruction based anomaly segmentation. Looking for anomalous regions in
medical images is one of the main applications that use anomaly segmentation.
The restricted receptive field in CNNs limit the CNN to model the global
context and hence if the anomalous regions cover parts of the image, the
CNN-based AEs are not capable to bring semantic understanding of the image. On
the other hand, Vision Transformers (ViTs) have emerged as a competitive
alternative to CNNs. It relies on the self-attention mechanism that is capable
to relate image patches to each other. To reconstruct a coherent and more
realistic image, in this work, we investigate Transformer capabilities in
building AEs for reconstruction based UAD task. We focus on anomaly
segmentation for Brain Magnetic Resonance Imaging (MRI) and present five
Transformer-based models while enabling segmentation performance comparable or
superior to State-of-The-Art (SOTA) models. The source code is available on
Github
this https URL</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：CNN-based Local Vision Transformer for COVID-19 Diagnosis</b></summary>
  <p><b>编号</b>：[234]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02027</p>
  <p><b>作者</b>：Hongyan Xu,  Xiu Su,  Dadong Wang</p>
  <p><b>备注</b>：5 pages, 4 figures</p>
  <p><b>关键词</b>：Deep learning technology, learning technology, assistive technology, Deep learning, accurately identify</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning technology can be used as an assistive technology to help
doctors quickly and accurately identify COVID-19 infections. Recently, Vision
Transformer (ViT) has shown great potential towards image classification due to
its global receptive field. However, due to the lack of inductive biases
inherent to CNNs, the ViT-based structure leads to limited feature richness and
difficulty in model training. In this paper, we propose a new structure called
Transformer for COVID-19 (COVT) to improve the performance of ViT-based
architectures on small COVID-19 datasets. It uses CNN as a feature extractor to
effectively extract local structural information, and introduces average
pooling to ViT's Multilayer Perception(MLP) module for global information.
Experiments show the effectiveness of our method on the two COVID-19 datasets
and the ImageNet dataset.</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：Vector Quantisation for Robust Segmentation</b></summary>
  <p><b>编号</b>：[238]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01919</p>
  <p><b>作者</b>：Ainkaran Santhirasekaram,  Avinash Kori,  Mathias Winkler,  Andrea Rockall,  Ben Glocker</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：medical domain depends, domain depends, robustness, medical imaging exhibiting, medical domain</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The reliability of segmentation models in the medical domain depends on the
model's robustness to perturbations in the input space. Robustness is a
particular challenge in medical imaging exhibiting various sources of image
noise, corruptions, and domain shifts. Obtaining robustness is often attempted
via simulating heterogeneous environments, either heuristically in the form of
data augmentation or by learning to generate specific perturbations in an
adversarial manner. We propose and justify that learning a discrete
representation in a low dimensional embedding space improves robustness of a
segmentation model. This is achieved with a dictionary learning method called
vector quantisation. We use a set of experiments designed to analyse robustness
in both the latent and output space under domain shift and noise perturbations
in the input space. We adapt the popular UNet architecture, inserting a
quantisation block in the bottleneck. We demonstrate improved segmentation
accuracy and better robustness on three segmentation tasks. Code is available
at
\url{this https URL}</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：ACT-Net: Asymmetric Co-Teacher Network for Semi-supervised  Memory-efficient Medical Image Segmentation</b></summary>
  <p><b>编号</b>：[239]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01900</p>
  <p><b>作者</b>：Ziyuan Zhao,  Andong Zhu,  Zeng Zeng,  Bharadwaj Veeravalli,  Cuntai Guan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：shown promising performance, medical image segmentation, well-annotated data, difficult to access, shown promising</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While deep models have shown promising performance in medical image
segmentation, they heavily rely on a large amount of well-annotated data, which
is difficult to access, especially in clinical practice. On the other hand,
high-accuracy deep models usually come in large model sizes, limiting their
employment in real scenarios. In this work, we propose a novel asymmetric
co-teacher framework, ACT-Net, to alleviate the burden on both expensive
annotations and computational costs for semi-supervised knowledge distillation.
We advance teacher-student learning with a co-teacher network to facilitate
asymmetric knowledge distillation from large models to small ones by
alternating student and teacher roles, obtaining tiny but accurate models for
clinical employment. To verify the effectiveness of our ACT-Net, we employ the
ACDC dataset for cardiac substructure segmentation in our experiments.
Extensive experimental results demonstrate that ACT-Net outperforms other
knowledge distillation methods and achieves lossless segmentation performance
with 250x fewer parameters.</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：MMGL: Multi-Scale Multi-View Global-Local Contrastive learning for  Semi-supervised Cardiac Image Segmentation</b></summary>
  <p><b>编号</b>：[240]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01883</p>
  <p><b>作者</b>：Ziyuan Zhao,  Jinxuan Hu,  Zeng Zeng,  Xulei Yang,  Peisheng Qian,  Bharadwaj Veeravalli,  Cuntai Guan</p>
  <p><b>备注</b>：Accepted by IEEE International Conference on Image Processing (ICIP 2022)</p>
  <p><b>关键词</b>：shown significant success, large-scale well-labeled datasets, large-scale well-labeled, significant success, success in medical</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With large-scale well-labeled datasets, deep learning has shown significant
success in medical image segmentation. However, it is challenging to acquire
abundant annotations in clinical practice due to extensive expertise
requirements and costly labeling efforts. Recently, contrastive learning has
shown a strong capacity for visual representation learning on unlabeled data,
achieving impressive performance rivaling supervised learning in many domains.
In this work, we propose a novel multi-scale multi-view global-local
contrastive learning (MMGL) framework to thoroughly explore global and local
features from different scales and views for robust contrastive learning
performance, thereby improving segmentation performance with limited
annotations. Extensive experiments on the MM-WHS dataset demonstrate the
effectiveness of MMGL framework on semi-supervised cardiac image segmentation,
outperforming the state-of-the-art contrastive learning methods by a large
margin.</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：Deriving Surface Resistivity from Polarimetric SAR Data Using Dual-Input  UNet</b></summary>
  <p><b>编号</b>：[242]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01811</p>
  <p><b>作者</b>：Bibin Wilson,  Rajiv Kumar,  Narayanarao Bhogapurapu,  Anand Singh,  Amit Sethi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Synthetic Aperture Radar, labor intensive, SAR data, Coso Geothermal Area, deep learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Traditional survey methods for finding surface resistivity are time-consuming
and labor intensive. Very few studies have focused on finding the
resistivity/conductivity using remote sensing data and deep learning
techniques. In this line of work, we assessed the correlation between surface
resistivity and Synthetic Aperture Radar (SAR) by applying various deep
learning methods and tested our hypothesis in the Coso Geothermal Area, USA.
For detecting the resistivity, L-band full polarimetric SAR data acquired by
UAVSAR were used, and MT (Magnetotellurics) inverted resistivity data of the
area were used as the ground truth. We conducted experiments to compare various
deep learning architectures and suggest the use of Dual Input UNet (DI-UNet)
architecture. DI-UNet uses a deep learning architecture to predict the
resistivity using full polarimetric SAR data by promising a quick survey
addition to the traditional method. Our proposed approach accomplished improved
outcomes for the mapping of MT resistivity from SAR data.</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：FDVTS's Solution for 2nd COV19D Competition on COVID-19 Detection and  Severity Analysis</b></summary>
  <p><b>编号</b>：[246]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01758</p>
  <p><b>作者</b>：Junlin Hou,  Jilan Xu,  Rui Feng,  Yuejie Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Conference on Computer, AIMIA Workshop, European Conference, paper presents, presents our solution</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents our solution for the 2nd COVID-19 Competition, occurring
in the framework of the AIMIA Workshop in the European Conference on Computer
Vision (ECCV 2022). In our approach, we employ an effective 3D Contrastive
Mixup Classification network for COVID-19 diagnosis on chest CT images, which
is composed of contrastive representation learning and mixup classification.
For the COVID-19 detection challenge, our approach reaches 0.9245 macro F1
score on 484 validation CT scans, which significantly outperforms the baseline
method by 16.5%. In the COVID-19 severity detection challenge, our approach
achieves 0.7186 macro F1 score on 61 validation samples, which also surpasses
the baseline by 8.86%.</p>
  </details>
</details>
<details>
  <summary>83. <b>标题：Slice-by-slice deep learning aided oropharyngeal cancer segmentation  with adaptive thresholding for spatial uncertainty on FDG PET and CT images</b></summary>
  <p><b>编号</b>：[251]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01623</p>
  <p><b>作者</b>：Alessia De Biase,  Nanna Maria Sijtsema,  Lisanne van Dijk,  Johannes A. Langendijk,  Peter van Ooijen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：radiotherapy treatment planning, registered FDG PET, FDG PET, treatment planning, fundamental step</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Tumor segmentation is a fundamental step for radiotherapy treatment planning.
To define an accurate segmentation of the primary tumor (GTVp) of oropharyngeal
cancer patients (OPC), simultaneous assessment of different image modalities is
needed, and each image volume is explored slice-by-slice from different
orientations. Moreover, the manual fixed boundary of segmentation neglects the
spatial uncertainty known to occur in tumor delineation. This study proposes a
novel automatic deep learning (DL) model to assist radiation oncologists in a
slice-by-slice adaptive GTVp segmentation on registered FDG PET/CT images. We
included 138 OPC patients treated with (chemo)radiation in our institute. Our
DL framework exploits both inter and intra-slice context. Sequences of 3
consecutive 2D slices of concatenated FDG PET/CT images and GTVp contours were
used as input. A 3-fold cross validation was performed three times, training on
sequences extracted from the Axial (A), Sagittal (S), and Coronal (C) plane of
113 patients. Since consecutive sequences in a volume contain overlapping
slices, each slice resulted in three outcome predictions that were averaged. In
the A, S, and C planes, the output shows areas with different probabilities of
predicting the tumor. The performance of the models was assessed on 25 patients
at different probability thresholds using the mean Dice Score Coefficient
(DSC). Predictions were the closest to the ground truth at a probability
threshold of 0.9 (DSC of 0.70 in the A, 0.77 in the S, and 0.80 in the C
plane). The promising results of the proposed DL model show that the
probability maps on registered FDG PET/CT images could guide radiation
oncologists in a slice-by-slice adaptive GTVp segmentation.</p>
  </details>
</details>
<h1>自然语言处理</h1>
<details>
  <summary>1. <b>标题：CLEAR: Improving Vision-Language Navigation with Cross-Lingual,  Environment-Agnostic Representations</b></summary>
  <p><b>编号</b>：[13]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02185</p>
  <p><b>作者</b>：Jialu Li,  Hao Tan,  Mohit Bansal</p>
  <p><b>备注</b>：NAACL 2022 Findings (18 pages)</p>
  <p><b>关键词</b>：VLN, representation, visual, visual representation, language representation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision-and-Language Navigation (VLN) tasks require an agent to navigate
through the environment based on language instructions. In this paper, we aim
to solve two key challenges in this task: utilizing multilingual instructions
for improved instruction-path grounding and navigating through new environments
that are unseen during training. To address these challenges, we propose CLEAR:
Cross-Lingual and Environment-Agnostic Representations. First, our agent learns
a shared and visually-aligned cross-lingual language representation for the
three languages (English, Hindi and Telugu) in the Room-Across-Room dataset.
Our language representation learning is guided by text pairs that are aligned
by visual information. Second, our agent learns an environment-agnostic visual
representation by maximizing the similarity between semantically-aligned image
pairs (with constraints on object-matching) from different environments. Our
environment agnostic visual representation can mitigate the environment bias
induced by low-level visual information. Empirically, on the Room-Across-Room
dataset, we show that our multilingual agent gets large improvements in all
metrics over the strong baseline model when generalizing to unseen environments
with the cross-lingual language representation and the environment-agnostic
visual representation. Furthermore, we show that our learned language and
visual representations can be successfully transferred to the Room-to-Room and
Cooperative Vision-and-Dialogue Navigation task, and present detailed
qualitative and quantitative generalization and grounding analysis. Our code is
available at this https URL</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：A Comprehensive Review of Visual-Textual Sentiment Analysis from Social  Media Networks</b></summary>
  <p><b>编号</b>：[22]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02160</p>
  <p><b>作者</b>：Israa Khalaf Salman Al-Tameemi,  Mohammad-Reza Feizi-Derakhshi,  Saeed Pashazadeh,  Mohammad Asadpour</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：opinions and emotions, Social media networks, Social media, people lives, people</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Social media networks have become a significant aspect of people's lives,
serving as a platform for their ideas, opinions and emotions. Consequently,
automated sentiment analysis (SA) is critical for recognising people's feelings
in ways that other information sources cannot. The analysis of these feelings
revealed various applications, including brand evaluations, YouTube film
reviews and healthcare applications. As social media continues to develop,
people post a massive amount of information in different forms, including text,
photos, audio and video. Thus, traditional SA algorithms have become limited,
as they do not consider the expressiveness of other modalities. By including
such characteristics from various material sources, these multimodal data
streams provide new opportunities for optimising the expected results beyond
text-based SA. Our study focuses on the forefront field of multimodal SA, which
examines visual and textual data posted on social media networks. Many people
are more likely to utilise this information to express themselves on these
platforms. To serve as a resource for academics in this rapidly growing field,
we introduce a comprehensive overview of textual and visual SA, including data
pre-processing, feature extraction techniques, sentiment benchmark datasets,
and the efficacy of multiple classification methodologies suited to each field.
We also provide a brief introduction of the most frequently utilised data
fusion strategies and a summary of existing research on visual-textual SA.
Finally, we highlight the most significant challenges and investigate several
important sentiment applications.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：A cross-corpus study on speech emotion recognition</b></summary>
  <p><b>编号</b>：[42]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02104</p>
  <p><b>作者</b>：Rosanna Milner,  Md Asif Jalal,  Raymond W. M. Ng,  Thomas Hain</p>
  <p><b>备注</b>：ASRU 2019</p>
  <p><b>关键词</b>：acquire large quantities, expressive emotions displayed, emotions, everyday life, difficult to acquire</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>For speech emotion datasets, it has been difficult to acquire large
quantities of reliable data and acted emotions may be over the top compared to
less expressive emotions displayed in everyday life. Lately, larger datasets
with natural emotions have been created. Instead of ignoring smaller, acted
datasets, this study investigates whether information learnt from acted
emotions is useful for detecting natural emotions. Cross-corpus research has
mostly considered cross-lingual and even cross-age datasets, and difficulties
arise from different methods of annotating emotions causing a drop in
performance. To be consistent, four adult English datasets covering acted,
elicited and natural emotions are considered. A state-of-the-art model is
proposed to accurately investigate the degradation of performance. The system
involves a bi-directional LSTM with an attention mechanism to classify emotions
across datasets. Experiments study the effects of training models in a
cross-corpus and multi-domain fashion and results show the transfer of
information is not successful. Out-of-domain models, followed by adapting to
the missing dataset, and domain adversarial training (DAT) are shown to be more
suitable to generalising to emotions across datasets. This shows positive
information transfer from acted datasets to those with more natural emotions
and the benefits from training on different corpora.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Neural Networks and the Chomsky Hierarchy</b></summary>
  <p><b>编号</b>：[47]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02098</p>
  <p><b>作者</b>：Grégoire Delétang,  Anian Ruoss,  Jordi Grau-Moya,  Tim Genewein,  Li Kevin Wenliang,  Elliot Catt,  Marcus Hutter,  Shane Legg,  Pedro A. Ortega</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Reliable generalization lies, heart of safe, Reliable generalization, tasks, generalization lies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reliable generalization lies at the heart of safe ML and AI. However,
understanding when and how neural networks generalize remains one of the most
important unsolved problems in the field. In this work, we conduct an extensive
empirical study (2200 models, 16 tasks) to investigate whether insights from
the theory of computation can predict the limits of neural network
generalization in practice. We demonstrate that grouping tasks according to the
Chomsky hierarchy allows us to forecast whether certain architectures will be
able to generalize to out-of-distribution inputs. This includes negative
results where even extensive amounts of data and training time never led to any
non-trivial generalization, despite models having sufficient capacity to
perfectly fit the training data. Our results show that, for our subset of
tasks, RNNs and Transformers fail to generalize on non-regular tasks, LSTMs can
solve regular and counter-language tasks, and only networks augmented with
structured memory (such as a stack or memory tape) can successfully generalize
on context-free and context-sensitive tasks.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Block-SCL: Blocking Matters for Supervised Contrastive Learning in  Product Matching</b></summary>
  <p><b>编号</b>：[75]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02008</p>
  <p><b>作者</b>：Mario Almagro,  David Jiménez,  Diego Ortego,  Emilio Almazán,  Eva Martínez</p>
  <p><b>备注</b>：7 pages, 2 figures, e-commerce, conference</p>
  <p><b>关键词</b>：behavior in e-commerce, fundamental step, global understanding, understanding of consumer, consumer behavior</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Product matching is a fundamental step for the global understanding of
consumer behavior in e-commerce. In practice, product matching refers to the
task of deciding if two product offers from different data sources (e.g.
retailers) represent the same product. Standard pipelines use a previous stage
called blocking, where for a given product offer a set of potential matching
candidates are retrieved based on similar characteristics (e.g. same brand,
category, flavor, etc.). From these similar product candidates, those that are
not a match can be considered hard negatives. We present Block-SCL, a strategy
that uses the blocking output to make the most of Supervised Contrastive
Learning (SCL). Concretely, Block-SCL builds enriched batches using the
hard-negatives samples obtained in the blocking stage. These batches provide a
strong training signal leading the model to learn more meaningful sentence
embeddings for product matching. Experimental results in several public
datasets demonstrate that Block-SCL achieves state-of-the-art results despite
only using short product titles as input, no data augmentation, and a lighter
transformer backbone than competing methods.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Making sense of spoken plurals</b></summary>
  <p><b>编号</b>：[96]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01947</p>
  <p><b>作者</b>：Elnaz Shafaei-Bajestan,  Peter Uhrig,  R. Harald Baayen</p>
  <p><b>备注</b>：23 pages including references, 19 pages excluding references, 9 Figures, 3 Tables. This article is under review at journal "The Mental Lexicon"</p>
  <p><b>关键词</b>：Distributional semantics offers, Distributional semantics, semantics, semantics offers, spoken American English</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Distributional semantics offers new ways to study the semantics of
morphology. This study focuses on the semantics of noun singulars and their
plural inflectional variants in English. Our goal is to compare two models for
the conceptualization of plurality. One model (FRACSS) proposes that all
singular-plural pairs should be taken into account when predicting plural
semantics from singular semantics. The other model (CCA) argues that
conceptualization for plurality depends primarily on the semantic class of the
base word. We compare the two models on the basis of how well the speech signal
of plural tokens in a large corpus of spoken American English aligns with the
semantic vectors predicted by the two models. Two measures are employed: the
performance of a form-to-meaning mapping and the correlations between form
distances and meaning distances. Results converge on a superior alignment for
CCA. Our results suggest that usage-based approaches to pluralization in which
a given word's own semantic neighborhood is given priority outperform theories
according to which pluralization is conceptualized as a process building on
high-level abstraction. We see that what has often been conceived of as a
highly abstract concept, [+plural], is better captured via a family of
mid-level partial generalizations.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：MIA 2022 Shared Task Submission: Leveraging Entity Representations,  Dense-Sparse Hybrids, and Fusion-in-Decoder for Cross-Lingual Question  Answering</b></summary>
  <p><b>编号</b>：[98]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01940</p>
  <p><b>作者</b>：Zhucheng Tu,  Sarguna Janani Padmanabhan</p>
  <p><b>备注</b>：System description for the Multilingual Information Access 2022 Shared Task</p>
  <p><b>关键词</b>：Multi-lingual Information Access, Open-Retrieval Question Answering, Cross-Lingual Open-Retrieval Question, Information Access, Shared Task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We describe our two-stage system for the Multi-lingual Information Access
(MIA) 2022 Shared Task on Cross-Lingual Open-Retrieval Question Answering. The
first stage consists of multilingual passage retrieval with a hybrid dense and
sparse retrieval strategy. The second stage consists of a reader which outputs
the answer from the top passages returned by the first stage. We show the
efficacy of using entity representations, sparse retrieval signals to help
dense retrieval, and Fusion-in-Decoder. On the development set, we obtain 43.46
F1 on XOR-TyDi QA and 21.99 F1 on MKQA, for an average F1 score of 32.73. On
the test set, we obtain 40.93 F1 on XOR-TyDi QA and 22.29 F1 on MKQA, for an
average F1 score of 31.61. We improve over the official baseline by over 4 F1
points on both the development and test sets.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Entity Linking in Tabular Data Needs the Right Attention</b></summary>
  <p><b>编号</b>：[100]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01937</p>
  <p><b>作者</b>：Miltiadis Marios Katsakioris,  Yiwei Zhou,  Daniele Masato</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：requires Entity Linking, tabular data, tabular data requires, data requires Entity, Understanding the semantic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Understanding the semantic meaning of tabular data requires Entity Linking
(EL), in order to associate each cell value to a real-world entity in a
Knowledge Base (KB). In this work, we focus on end-to-end solutions for EL on
tabular data that do not rely on fact lookup in the target KB. Tabular data
contains heterogeneous and sparse context, including column headers, cell
values and table captions. We experiment with various models to generate a
vector representation for each cell value to be linked. Our results show that
it is critical to apply an attention mechanism as well as an attention mask, so
that the model can only attend to the most relevant context and avoid
information dilution. The most relevant context includes: same-row cells,
same-column cells, headers and caption. Computational complexity, however,
grows quadratically with the size of tabular data for such a complex model. We
achieve constant memory usage by introducing a Tabular Entity Linking Lite
model (TELL ) that generates vector representation for a cell based only on its
value, the table headers and the table caption. TELL achieves 80.8% accuracy on
Wikipedia tables, which is only 0.1% lower than the state-of-the-art model with
quadratic memory usage.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Cross-Lingual QA as a Stepping Stone for Monolingual Open QA in  Icelandic</b></summary>
  <p><b>编号</b>：[109]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01918</p>
  <p><b>作者</b>：Vésteinn Snæbjarnarson,  Hafsteinn Einarsson</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：English, Icelandic, lack of labeled, system, language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>It can be challenging to build effective open question answering (open QA)
systems for languages other than English, mainly due to a lack of labeled data
for training. We present a data efficient method to bootstrap such a system for
languages other than English. Our approach requires only limited QA resources
in the given language, along with machine-translated data, and at least a
bilingual language model. To evaluate our approach, we build such a system for
the Icelandic language and evaluate performance over trivia style datasets. The
corpora used for training are English in origin but machine translated into
Icelandic. We train a bilingual Icelandic/English language model to embed
English context and Icelandic questions following methodology introduced with
DensePhrases (Lee et al., 2021). The resulting system is an open domain
cross-lingual QA system between Icelandic and English. Finally, the system is
adapted for Icelandic only open QA, demonstrating how it is possible to
efficiently create an open QA system with limited access to curated datasets in
the language of interest.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Betti numbers of attention graphs is all you really need</b></summary>
  <p><b>编号</b>：[117]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01903</p>
  <p><b>作者</b>：Laida Kushnareva,  Dmitri Piontkovski,  Irina Piontkovskaya</p>
  <p><b>备注</b>：This short paper was submitted to "Topological Data Analysis and Beyond" Workshop at NeurIPS 2020 at July 2020, but wasn't accepted. Later the ideas from this short paper found a rich development in arXiv:2109.04825 and arXiv:2205.09630</p>
  <p><b>关键词</b>：BERT model, attention graphs, attention heads, Natural Language Processing, apply methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We apply methods of topological analysis to the attention graphs, calculated
on the attention heads of the BERT model ( arXiv:1810.04805v2 ). Our research
shows that the classifier built upon basic persistent topological features
(namely, Betti numbers) of the trained neural network can achieve
classification results on par with the conventional classification method. We
show the relevance of such topological text representation on three text
classification benchmarks. For the best of our knowledge, it is the first
attempt to analyze the topology of an attention-based neural network, widely
used for Natural Language Processing.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：ASR-Generated Text for Language Model Pre-training Applied to Speech  Tasks</b></summary>
  <p><b>编号</b>：[123]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01893</p>
  <p><b>作者</b>：Valentin Pelloin,  Franck Dary,  Nicolas Herve,  Benoit Favre,  Nathalie Camelin,  Antoine Laurent,  Laurent Besacier</p>
  <p><b>备注</b>：Interspeech 2022 (Camera Ready)</p>
  <p><b>关键词</b>：French National Audiovisual, National Audiovisual Institute, automatically transcribed speech, French National, Audiovisual Institute</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We aim at improving spoken language modeling (LM) using very large amount of
automatically transcribed speech. We leverage the INA (French National
Audiovisual Institute) collection and obtain 19GB of text after applying ASR on
350,000 hours of diverse TV shows. From this, spoken language models are
trained either by fine-tuning an existing LM (FlauBERT) or through training a
LM from scratch. New models (FlauBERT-Oral) are shared with the community and
evaluated for 3 downstream tasks: spoken language understanding, classification
of TV shows and speech syntactic parsing. Results show that FlauBERT-Oral can
be beneficial compared to its initial FlauBERT version demonstrating that,
despite its inherent noisy nature, ASR-generated text can be used to build
spoken language models.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Keyword Extraction in Scientific Documents</b></summary>
  <p><b>编号</b>：[124]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01888</p>
  <p><b>作者</b>：Susie Xi Rao,  Piriyakorn Piriyatamwong,  Parijat Ghoshal,  Sara Nasirian,  Emmanuel de Salis,  Sandra Mitrović,  Michael Wechner,  Vanya Brucker,  Peter Egger,  Ce Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：output grows exponentially, publication output grows, grows exponentially, output grows, scientific publication output</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The scientific publication output grows exponentially. Therefore, it is
increasingly challenging to keep track of trends and changes. Understanding
scientific documents is an important step in downstream tasks such as knowledge
graph building, text mining, and discipline classification. In this workshop,
we provide a better understanding of keyword and keyphrase extraction from the
abstract of scientific publications.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Scene-Aware Prompt for Multi-modal Dialogue Understanding and Generation</b></summary>
  <p><b>编号</b>：[150]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01823</p>
  <p><b>作者</b>：Bin Li,  Yixuan Weng,  Ziyu Ma,  Bin Sun,  Shutao Li</p>
  <p><b>备注</b>：Accepted in NLPCC 2022</p>
  <p><b>关键词</b>：Team LingJing experiments, schemes of Team, Team LingJing, Multi-modal Dialogue Understanding, paper introduces</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper introduces the schemes of Team LingJing's experiments in
NLPCC-2022-Shared-Task-4 Multi-modal Dialogue Understanding and Generation
(MDUG). The MDUG task can be divided into two phases: multi-modal context
understanding and response generation. To fully leverage the visual information
for both scene understanding and dialogue generation, we propose the
scene-aware prompt for the MDUG task. Specifically, we utilize the
multi-tasking strategy for jointly modelling the scene- and session-
multi-modal understanding. The visual captions are adopted to aware the scene
information, while the fixed-type templated prompt based on the scene- and
session-aware labels are used to further improve the dialogue generation
performance. Extensive experimental results show that the proposed method has
achieved state-of-the-art (SOTA) performance compared with other competitive
methods, where we rank the 1-st in all three subtasks in this MDUG competition.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：CodeRL: Mastering Code Generation through Pretrained Models and Deep  Reinforcement Learning</b></summary>
  <p><b>编号</b>：[172]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01780</p>
  <p><b>作者</b>：Hung Le,  Yue Wang,  Akhilesh Deepak Gotmare,  Silvio Savarese,  Steven C.H. Hoi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：code generation aims, aims to generate, code generation, SOTA results, generation aims</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Program synthesis or code generation aims to generate a program that
satisfies a problem specification. Recent approaches using large-scale
pretrained language models (LMs) have shown promising results, yet they have
some critical limitations. In particular, they often follow a standard
supervised fine-tuning procedure to train a code generation model only from the
pairs of natural-language problem descriptions and ground-truth programs. Such
paradigm largely ignores some important but potentially useful signals in the
problem specification such as unit tests, which thus often results in poor
performance when solving complex unseen coding tasks. To address the
limitations, we propose "CodeRL", a new framework for program synthesis tasks
through pretrained LMs and deep reinforcement learning (RL). Specifically,
during training, we treat the code-generating LM as an actor network, and
introduce a critic network that is trained to predict the functional
correctness of generated programs and provide dense feedback signals to the
actor. During inference, we introduce a new generation procedure with a
critical sampling strategy that allows a model to automatically regenerate
programs based on feedback from example unit tests and critic scores. For the
model backbones, we extended the encoder-decoder architecture of CodeT5 with
enhanced learning objectives, larger model sizes, and better pretraining data.
Our method not only achieves new SOTA results on the challenging APPS
benchmark, but also shows strong zero-shot transfer capability with new SOTA
results on the simpler MBPP benchmark.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Vision-and-Language Pretraining</b></summary>
  <p><b>编号</b>：[177]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01772</p>
  <p><b>作者</b>：Thong Nguyen,  Cong-Duy Nguyen,  Xiaobao Wu,  Anh Tuan Luu</p>
  <p><b>备注</b>：35 pages, 3 figures</p>
  <p><b>关键词</b>：burgeoning amount, amount of data, data of image-text, image-text pairs, pairs and diversity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the burgeoning amount of data of image-text pairs and diversity of
Vision-and-Language (V&L) tasks, scholars have introduced an abundance of deep
learning models in this research domain. Furthermore, in recent years, transfer
learning has also shown tremendous success in Computer Vision for tasks such as
Image Classification, Object Detection, etc., and in Natural Language
Processing for Question Answering, Machine Translation, etc. Inheriting the
spirit of Transfer Learning, research works in V&L have devised multiple
pretraining techniques on large-scale datasets in order to enhance the
performance of downstream tasks. The aim of this article is to provide a
comprehensive revision of contemporary V&L pretraining models. In particular,
we categorize and delineate pretraining approaches, along with the summary of
state-of-the-art vision-and-language pre-trained models. Moreover, a list of
training datasets and downstream tasks is supplied to further polish the
perspective on V&L pretraining. Lastly, we decided to take a further step to
discuss numerous directions for future research.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：PReGAN: Answer Oriented Passage Ranking with Weakly Supervised GAN</b></summary>
  <p><b>编号</b>：[182]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01762</p>
  <p><b>作者</b>：Pan Du,  Jian-Yun Nie,  Yutao Zhu,  Hao Jiang,  Lixin Zou,  Xiaohui Yan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：open-domain factoid question, factoid question answering, ranking for open-domain, open-domain factoid, answering also requires</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Beyond topical relevance, passage ranking for open-domain factoid question
answering also requires a passage to contain an answer (answerability). While a
few recent studies have incorporated some reading capability into a ranker to
account for answerability, the ranker is still hindered by the noisy nature of
the training data typically available in this area, which considers any passage
containing an answer entity as a positive sample. However, the answer entity in
a passage is not necessarily mentioned in relation with the given question. To
address the problem, we propose an approach called \ttt{PReGAN} for Passage
Reranking based on Generative Adversarial Neural networks, which incorporates a
discriminator on answerability, in addition to a discriminator on topical
relevance. The goal is to force the generator to rank higher a passage that is
topically relevant and contains an answer. Experiments on five public datasets
show that \ttt{PReGAN} can better rank appropriate passages, which in turn,
boosts the effectiveness of QA systems, and outperforms the existing approaches
without using external data.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Probing via Prompting</b></summary>
  <p><b>编号</b>：[198]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01736</p>
  <p><b>作者</b>：Jiaoda Li,  Ryan Cotterell,  Mrinmaya Sachan</p>
  <p><b>备注</b>：NAACL 2022</p>
  <p><b>关键词</b>：popular method, method to discern, representations of pre-trained, pre-trained language models, linguistic information</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Probing is a popular method to discern what linguistic information is
contained in the representations of pre-trained language models. However, the
mechanism of selecting the probe model has recently been subject to intense
debate, as it is not clear if the probes are merely extracting information or
modeling the linguistic property themselves. To address this challenge, this
paper introduces a novel model-free approach to probing, by formulating probing
as a prompting task. We conduct experiments on five probing tasks and show that
our approach is comparable or better at extracting information than diagnostic
probes while learning much less on its own. We further combine the probing via
prompting approach with attention head pruning to analyze where the model
stores the linguistic information in its architecture. We then examine the
usefulness of a specific linguistic property for pre-training by removing the
heads that are essential to that property and evaluating the resulting model's
performance on language modeling.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：BERT, can HE predict contrastive focus? Predicting and controlling  prominence in neural TTS using a language model</b></summary>
  <p><b>编号</b>：[205]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01718</p>
  <p><b>作者</b>：Brooke Stephenson,  Laurent Besacier,  Laurent Girin,  Thomas Hueber</p>
  <p><b>备注</b>：5 pages</p>
  <p><b>关键词</b>：infer prosodic features, language model representations, transformer language model, transformer language, representations to infer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Several recent studies have tested the use of transformer language model
representations to infer prosodic features for text-to-speech synthesis (TTS).
While these studies have explored prosody in general, in this work, we look
specifically at the prediction of contrastive focus on personal pronouns. This
is a particularly challenging task as it often requires semantic, discursive
and/or pragmatic knowledge to predict correctly. We collect a corpus of
utterances containing contrastive focus and we evaluate the accuracy of a BERT
model, finetuned to predict quantized acoustic prominence features, on these
samples. We also investigate how past utterances can provide relevant
information for this prediction. Furthermore, we evaluate the controllability
of pronoun prominence in a TTS model conditioned on acoustic prominence
features.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Disentangled Action Recognition with Knowledge Bases</b></summary>
  <p><b>编号</b>：[206]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01708</p>
  <p><b>作者</b>：Zhekun Luo,  Shalini Ghosh,  Devin Guillory,  Keizo Kato,  Trevor Darrell,  Huijuan Xu</p>
  <p><b>备注</b>：NAACL 2022</p>
  <p><b>关键词</b>：video usually involves, involves the interaction, interaction of human, compositional action, compositional action nodes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Action in video usually involves the interaction of human with objects.
Action labels are typically composed of various combinations of verbs and
nouns, but we may not have training data for all possible combinations. In this
paper, we aim to improve the generalization ability of the compositional action
recognition model to novel verbs or novel nouns that are unseen during training
time, by leveraging the power of knowledge graphs. Previous work utilizes
verb-noun compositional action nodes in the knowledge graph, making it
inefficient to scale since the number of compositional action nodes grows
quadratically with respect to the number of verbs and nouns. To address this
issue, we propose our approach: Disentangled Action Recognition with
Knowledge-bases (DARK), which leverages the inherent compositionality of
actions. DARK trains a factorized model by first extracting disentangled
feature representations for verbs and nouns, and then predicting classification
weights using relations in external knowledge graphs. The type constraint
between verb and noun is extracted from external knowledge bases and finally
applied when composing actions. DARK has better scalability in the number of
objects and verbs, and achieves state-of-the-art performance on the Charades
dataset. We further propose a new benchmark split based on the Epic-kitchen
dataset which is an order of magnitude bigger in the numbers of classes and
samples, and benchmark various models on this benchmark.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Location reference recognition from texts: A survey and comparison</b></summary>
  <p><b>编号</b>：[218]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01683</p>
  <p><b>作者</b>：Xuke Hu,  Zhiyong Zhou,  Hao Li,  Yingjie Hu,  Fuqiang Gu,  Jens Kersten,  Hongchao Fan,  Friederike Klan</p>
  <p><b>备注</b>：35 pages, 11 figures</p>
  <p><b>关键词</b>：location reference recognition, location reference, scientific articles, web pages, travel blogs</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A vast amount of location information exists in unstructured texts, such as
social media posts, news stories, scientific articles, web pages, travel blogs,
and historical archives. Geoparsing refers to the process of recognizing
location references from texts and identifying their geospatial
representations. While geoparsing can benefit many domains, a summary of the
specific applications is still missing. Further, there lacks a comprehensive
review and comparison of existing approaches for location reference
recognition, which is the first and a core step of geoparsing. To fill these
research gaps, this review first summarizes seven typical application domains
of geoparsing: geographic information retrieval, disaster management, disease
surveillance, traffic management, spatial humanities, tourism management, and
crime management. We then review existing approaches for location reference
recognition by categorizing these approaches into four groups based on their
underlying functional principle: rule-based, gazetteer matching-based,
statistical learning-based, and hybrid approaches. Next, we thoroughly evaluate
the correctness and computational efficiency of the 27 most widely used
approaches for location reference recognition based on 26 public datasets with
different types of texts (e.g., social media posts and news stories) containing
39,736 location references across the world. Results from this thorough
evaluation can help inform future methodological developments for location
reference recognition, and can help guide the selection of proper approaches
based on application needs.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：A Cascade Model for Argument Mining in Japanese Political Discussions:  the QA Lab-PoliInfo-3 Case Study</b></summary>
  <p><b>编号</b>：[221]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01672</p>
  <p><b>作者</b>：Ramon Ruiz-Dolz</p>
  <p><b>备注</b>：Proceedings of the 16th NTCIR Conference on Evaluation of Information Access Technologies, June 14-17, 2022 Tokyo Japan</p>
  <p><b>关键词</b>：Budget Argument Mining, rVRAIN team tackled, Argument Mining, information retrieval sub-tasks, argument classification</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rVRAIN team tackled the Budget Argument Mining (BAM) task, consisting of
a combination of classification and information retrieval sub-tasks. For the
argument classification (AC), the team achieved its best performing results
with a five-class BERT-based cascade model complemented with some handcrafted
rules. The rules were used to determine if the expression was monetary or not.
Then, each monetary expression was classified as a premise or as a conclusion
in the first level of the cascade model. Finally, each premise was classified
into the three premise classes, and each conclusion into the two conclusion
classes. For the information retrieval (i.e., relation ID detection or RID),
our best results were achieved by a combination of a BERT-based binary
classifier, and the cosine similarity of pairs consisting of the monetary
expression and budget dense embeddings.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Quantum Circuit Compiler for a Shuttling-Based Trapped-Ion Quantum  Computer</b></summary>
  <p><b>编号</b>：[237]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01964</p>
  <p><b>作者</b>：Fabian Kreppel,  Christian Melzer,  Janis Wagner,  Janine Hilder,  Ulrich Poschinger,  Ferdinand Schmidt-Kaler,  André Brinkmann</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：realize deep quantum, quantum computing hardware, deep quantum circuits, quantum circuits call, compile quantum circuits</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Increasing capabilities of quantum computing hardware and the challenge to
realize deep quantum circuits call for fully automated and efficient tools to
compile quantum circuits. To express arbitrary circuits in a sequence of native
gates pertaining to the specific quantum computer architecture is necessary to
make algorithms portable across the landscape of quantum hardware providers. In
this work, we present a compiler capable of transforming and optimizing a
quantum circuit, targeting a shuttling-based trapped-ion quantum processor. It
consists of custom algorithms set on top of the Cambridge Quantum Computer's
quantum circuit framework Pytket. The performance is evaluated for a wide range
of quantum circuits, showing that the gate counts can be reduced by a factor of
up to 3.6 compared to standard Pytket and up to 2.2 compared to standard Qiskit
compilation, while we achieve similar gate counts as compared to a Pytket
extension targeting the AQT linear-static trapped ion addressing-based
architecture.</p>
  </details>
</details>
<h1>机器学习</h1>
<details>
  <summary>1. <b>标题：Offline RL Policies Should be Trained to be Adaptive</b></summary>
  <p><b>编号</b>：[7]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02200</p>
  <p><b>作者</b>：Dibya Ghosh,  Anurag Ajay,  Pulkit Agrawal,  Sergey Levine</p>
  <p><b>备注</b>：ICML 2022 (long talk)</p>
  <p><b>关键词</b>：environment unknown, provided may leave, leave many facets, Offline, adaptive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Offline RL algorithms must account for the fact that the dataset they are
provided may leave many facets of the environment unknown. The most common way
to approach this challenge is to employ pessimistic or conservative methods,
which avoid behaviors that are too dissimilar from those in the training
dataset. However, relying exclusively on conservatism has drawbacks:
performance is sensitive to the exact degree of conservatism, and conservative
objectives can recover highly suboptimal policies. In this work, we propose
that offline RL methods should instead be adaptive in the presence of
uncertainty. We show that acting optimally in offline RL in a Bayesian sense
involves solving an implicit POMDP. As a result, optimal policies for offline
RL must be adaptive, depending not just on the current state but rather all the
transitions seen so far during evaluation.We present a model-free algorithm for
approximating this optimal adaptive policy, and demonstrate the efficacy of
learning such adaptive policies in offline RL benchmarks.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Data-driven synchronization-avoiding algorithms in the explicit  distributed structural analysis of soft tissue</b></summary>
  <p><b>编号</b>：[9]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02194</p>
  <p><b>作者</b>：Guoxiang Grayson Tong,  Daniele E. Schiavazzi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：finite element method, explicit finite element, distributed finite element, finite element solver, soft tissue</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a data-driven framework to increase the computational efficiency
of the explicit finite element method in the structural analysis of soft
tissue. An encoder-decoder long short-term memory deep neural network is
trained based on the data produced by an explicit, distributed finite element
solver. We leverage this network to predict synchronized displacements at
shared nodes, minimizing the amount of communication between processors. We
perform extensive numerical experiments to quantify the accuracy and stability
of the proposed synchronization-avoiding algorithm.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：CEN : Cooperatively Evolving Networks</b></summary>
  <p><b>编号</b>：[10]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02192</p>
  <p><b>作者</b>：Sobhan Babu,  Ravindra Guravannavar</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：finitely repeated game, simultaneous game, game, finitely repeated, repeated game</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A finitely repeated game is a dynamic game in which a simultaneous game is
played finitely many times. GANs contain two competing modules: the generator
module is trained to generate new examples, and the discriminator module is
trained to discriminate real examples from generated examples. Training
procedure of GAN is a finitely repeated game in which each module tries to
optimize it's error at every instance of simultaneous game in a non-cooperative
manner. We observed that we can achieve more accurate training, if at each
instance of simultaneous game the stronger module cooperate with weaker module
and only weaker module only optimize it's error.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Accelerating Hamiltonian Monte Carlo via Chebyshev Integration Time</b></summary>
  <p><b>编号</b>：[11]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02189</p>
  <p><b>作者</b>：Jun-Kun Wang,  Andre Wibisono</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Hamiltonian Monte Carlo, Monte Carlo, Hamiltonian Monte, integration time, time-varying integration time</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Hamiltonian Monte Carlo (HMC) is a popular method in sampling. While there
are quite a few works of studying this method on various aspects, an
interesting question is how to choose its integration time to achieve
acceleration. In this work, we consider accelerating the process of sampling
from a distribution $\pi(x) \propto \exp(-f(x))$ via HMC via time-varying
integration time. When the potential $f$ is $L$-smooth and $m$-strongly convex,
i.e.\ for sampling from a log-smooth and strongly log-concave target
distribution $\pi$, it is known that under a constant integration time, the
number of iterations that ideal HMC takes to get an $\epsilon$ Wasserstein-2
distance to the target $\pi$ is $O( \kappa \log \frac{1}{\epsilon} )$, where
$\kappa := \frac{L}{m}$ is the condition number. We propose a scheme of
time-varying integration time based on the roots of Chebyshev polynomials. We
show that in the case of quadratic potential $f$, i.e., when the target $\pi$
is a Gaussian distribution, ideal HMC with this choice of integration time only
takes $O( \sqrt{\kappa} \log \frac{1}{\epsilon} )$ number of iterations to
reach Wasserstein-2 distance less than $\epsilon$; this improvement on the
dependence on condition number is akin to acceleration in optimization. The
design and analysis of HMC with the proposed integration time is built on the
tools of Chebyshev polynomials. Experiments find the advantage of adopting our
scheme of time-varying integration time even for sampling from distributions
with smooth strongly convex potentials that are not quadratic.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：NeuralPassthrough: Learned Real-Time View Synthesis for VR</b></summary>
  <p><b>编号</b>：[12]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02186</p>
  <p><b>作者</b>：Lei Xiao,  Salah Nouri,  Joel Hegland,  Alberto Garcia Garcia,  Douglas Lanman</p>
  <p><b>备注</b>：9 pages, 12 figures</p>
  <p><b>关键词</b>：stereoscopic visual experience, Virtual reality, provide an immersive, visual experience, physical environment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Virtual reality (VR) headsets provide an immersive, stereoscopic visual
experience, but at the cost of blocking users from directly observing their
physical environment. Passthrough techniques are intended to address this
limitation by leveraging outward-facing cameras to reconstruct the images that
would otherwise be seen by the user without the headset. This is inherently a
real-time view synthesis challenge, since passthrough cameras cannot be
physically co-located with the eyes. Existing passthrough techniques suffer
from distracting reconstruction artifacts, largely due to the lack of accurate
depth information (especially for near-field and disoccluded objects), and also
exhibit limited image quality (e.g., being low resolution and monochromatic).
In this paper, we propose the first learned passthrough method and assess its
performance using a custom VR headset that contains a stereo pair of RGB
cameras. Through both simulations and experiments, we demonstrate that our
learned passthrough method delivers superior image quality compared to
state-of-the-art methods, while meeting strict VR requirements for real-time,
perspective-correct stereoscopic view synthesis over a wide field of view for
desktop-connected headsets.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：CLEAR: Improving Vision-Language Navigation with Cross-Lingual,  Environment-Agnostic Representations</b></summary>
  <p><b>编号</b>：[13]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02185</p>
  <p><b>作者</b>：Jialu Li,  Hao Tan,  Mohit Bansal</p>
  <p><b>备注</b>：NAACL 2022 Findings (18 pages)</p>
  <p><b>关键词</b>：VLN, representation, visual, visual representation, language representation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision-and-Language Navigation (VLN) tasks require an agent to navigate
through the environment based on language instructions. In this paper, we aim
to solve two key challenges in this task: utilizing multilingual instructions
for improved instruction-path grounding and navigating through new environments
that are unseen during training. To address these challenges, we propose CLEAR:
Cross-Lingual and Environment-Agnostic Representations. First, our agent learns
a shared and visually-aligned cross-lingual language representation for the
three languages (English, Hindi and Telugu) in the Room-Across-Room dataset.
Our language representation learning is guided by text pairs that are aligned
by visual information. Second, our agent learns an environment-agnostic visual
representation by maximizing the similarity between semantically-aligned image
pairs (with constraints on object-matching) from different environments. Our
environment agnostic visual representation can mitigate the environment bias
induced by low-level visual information. Empirically, on the Room-Across-Room
dataset, we show that our multilingual agent gets large improvements in all
metrics over the strong baseline model when generalizing to unseen environments
with the cross-lingual language representation and the environment-agnostic
visual representation. Furthermore, we show that our learned language and
visual representations can be successfully transferred to the Room-to-Room and
Cooperative Vision-and-Dialogue Navigation task, and present detailed
qualitative and quantitative generalization and grounding analysis. Our code is
available at this https URL</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：ST-CoNAL: Consistency-Based Acquisition Criterion Using Temporal  Self-Ensemble for Active Learning</b></summary>
  <p><b>编号</b>：[14]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02182</p>
  <p><b>作者</b>：Jae Soon Baik,  In Young Yoon,  Jun Won Choi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：achieved great success, Modern deep learning, Modern deep, achieved great, great success</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Modern deep learning has achieved great success in various fields. However,
it requires the labeling of huge amounts of data, which is expensive and
labor-intensive. Active learning (AL), which identifies the most informative
samples to be labeled, is becoming increasingly important to maximize the
efficiency of the training process. The existing AL methods mostly use only a
single final fixed model for acquiring the samples to be labeled. This strategy
may not be good enough in that the structural uncertainty of a model for given
training data is not considered to acquire the samples. In this study, we
propose a novel acquisition criterion based on temporal self-ensemble generated
by conventional stochastic gradient descent (SGD) optimization. These
self-ensemble models are obtained by capturing the intermediate network weights
obtained through SGD iterations. Our acquisition function relies on a
consistency measure between the student and teacher models. The student models
are given a fixed number of temporal self-ensemble models, and the teacher
model is constructed by averaging the weights of the student models. Using the
proposed acquisition criterion, we present an AL algorithm, namely
student-teacher consistency-based AL (ST-CoNAL). Experiments conducted for
image classification tasks on CIFAR-10, CIFAR-100, Caltech-256, and Tiny
ImageNet datasets demonstrate that the proposed ST-CoNAL achieves significantly
better performance than the existing acquisition methods. Furthermore,
extensive experiments show the robustness and effectiveness of our methods.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：DBN-Mix: Training Dual Branch Network Using Bilateral Mixup Augmentation  for Long-Tailed Visual Recognition</b></summary>
  <p><b>编号</b>：[17]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02173</p>
  <p><b>作者</b>：Jae Soon Baik,  In Young Yoon,  Jun Won Choi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：challenging visual perception, visual perception task, long-tailed class distributions, growing interest, perception task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>There is a growing interest in the challenging visual perception task of
learning from long-tailed class distributions. The extreme class imbalance in
the training dataset biases the model to prefer to recognize majority-class
data over minority-class data. Recently, the dual branch network (DBN)
framework has been proposed, where two branch networks; the conventional branch
and the re-balancing branch were employed to improve the accuracy of
long-tailed visual recognition. The re-balancing branch uses a reverse sampler
to generate class-balanced training samples to mitigate bias due to class
imbalance. Although this strategy has been quite successful in handling bias,
using a reversed sampler for training can degrade the representation learning
performance. To alleviate this issue, the conventional method used a carefully
designed cumulative learning strategy, in which the influence of the
re-balancing branch gradually increases throughout the entire training phase.
In this study, we aim to develop a simple yet effective method to improve the
performance of DBN without cumulative learning that is difficult to optimize.
We devise a simple data augmentation method termed bilateral mixup
augmentation, which combines one sample from the uniform sampler with another
sample from the reversed sampler to produce a training sample. Furthermore, we
present class-conditional temperature scaling that mitigates bias toward the
majority class for the proposed DBN architecture. Our experiments performed on
widely used long-tailed visual recognition datasets show that bilateral mixup
augmentation is quite effective in improving the representation learning
performance of DBNs, and that the proposed method achieves state-of-the-art
performance for some categories.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Probability density estimation for sets of large graphs with respect to  spectral information using stochastic block models</b></summary>
  <p><b>编号</b>：[19]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02168</p>
  <p><b>作者</b>：Daniel Ferguson,  François G. Meyer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：graph-valued data sampled, data sampled iid, sampled iid, computed with respect, respective sample moments</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>For graph-valued data sampled iid from a distribution $\mu$, the sample
moments are computed with respect to a choice of metric. In this work, we equip
the set of graphs with the pseudo-metric defined by the $\ell_2$ norm between
the eigenvalues of the respective adjacency matrices. We use this pseudo metric
and the respective sample moments of a graph valued data set to infer the
parameters of a distribution $\hat{\mu}$ and interpret this distribution as an
approximation of $\mu$. We verify experimentally that complex distributions
$\mu$ can be approximated well taking this approach.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Automatic inspection of cultural monuments using deep and tensor-based  learning on hyperspectral imagery</b></summary>
  <p><b>编号</b>：[20]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02163</p>
  <p><b>作者</b>：Ioannis N. Tzortzis,  Ioannis Rallis,  Konstantinos Makantasis,  Anastasios Doulamis,  Nikolaos Doulamis,  Athanasios Voulodimos</p>
  <p><b>备注</b>：Accepted for presentation in IEEE International Conference on Image Processing (ICIP 2022)</p>
  <p><b>关键词</b>：provide extended information, Cultural Heritage, Cultural Heritage monuments, hyperspectral images, images are commonly</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In Cultural Heritage, hyperspectral images are commonly used since they
provide extended information regarding the optical properties of materials.
Thus, the processing of such high-dimensional data becomes challenging from the
perspective of machine learning techniques to be applied. In this paper, we
propose a Rank-$R$ tensor-based learning model to identify and classify
material defects on Cultural Heritage monuments. In contrast to conventional
deep learning approaches, the proposed high order tensor-based learning
demonstrates greater accuracy and robustness against overfitting. Experimental
results on real-world data from UNESCO protected areas indicate the superiority
of the proposed scheme compared to conventional deep learning models.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：UniCR: Universally Approximated Certified Robustness via Randomized  Smoothing</b></summary>
  <p><b>编号</b>：[26]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02152</p>
  <p><b>作者</b>：Hanbin Hong,  Binghui Wang,  Yuan Hong</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：machine learning classifiers, study certified robustness, certified robustness, machine learning, robustness certification</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study certified robustness of machine learning classifiers against
adversarial perturbations. In particular, we propose the first universally
approximated certified robustness (UniCR) framework, which can approximate the
robustness certification of any input on any classifier against any $\ell_p$
perturbations with noise generated by any continuous probability distribution.
Compared with the state-of-the-art certified defenses, UniCR provides many
significant benefits: (1) the first universal robustness certification
framework for the above 4 'any's; (2) automatic robustness certification that
avoids case-by-case analysis, (3) tightness validation of certified robustness,
and (4) optimality validation of noise distributions used by randomized
smoothing. We conduct extensive experiments to validate the above benefits of
UniCR and the advantages of UniCR over state-of-the-art certified defenses
against $\ell_p$ perturbations.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Deterministic Decoupling of Global Features and its Application to Data  Analysis</b></summary>
  <p><b>编号</b>：[30]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02132</p>
  <p><b>作者</b>：Eduardo Martinez-Enriquez (1),  Maria del Mar Gonzalez (2),  Javier Portilla (1) ((1) Consejo Superior de Investigaciones Cientificas CSIC, (2) Universidad Autonoma de Madrid)</p>
  <p><b>备注</b>：29 pages, 12 figures</p>
  <p><b>关键词</b>：improve data analysis, show its applicability, applicability to improve, open new venues, data analysis performance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce a method for deterministic decoupling of global features and
show its applicability to improve data analysis performance, as well as to open
new venues for feature transfer. We propose a new formalism that is based on
defining transformations on submanifolds, by following trajectories along the
features gradients. Through these transformations we define a normalization
that, we demonstrate, allows for decoupling differentiable features. By
applying this to sampling moments, we obtain a quasi-analytic solution for the
orthokurtosis, a normalized version of the kurtosis that is not just decoupled
from mean and variance, but also from skewness. We apply this method in the
original data domain and at the output of a filter bank to regression and
classification problems based on global descriptors, obtaining a consistent and
significant improvement in performance as compared to using classical
(non-decoupled) descriptors.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：A survey of multimodal deep generative models</b></summary>
  <p><b>编号</b>：[32]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02127</p>
  <p><b>作者</b>：Masahiro Suzuki,  Yutaka Matsuo</p>
  <p><b>备注</b>：Published in Advanced Robotics</p>
  <p><b>关键词</b>：make predictions based, generative models, deep generative models, framework for building, make predictions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multimodal learning is a framework for building models that make predictions
based on different types of modalities. Important challenges in multimodal
learning are the inference of shared representations from arbitrary modalities
and cross-modal generation via these representations; however, achieving this
requires taking the heterogeneous nature of multimodal data into account. In
recent years, deep generative models, i.e., generative models in which
distributions are parameterized by deep neural networks, have attracted much
attention, especially variational autoencoders, which are suitable for
accomplishing the above challenges because they can consider heterogeneity and
infer good representations of data. Therefore, various multimodal generative
models based on variational autoencoders, called multimodal deep generative
models, have been proposed in recent years. In this paper, we provide a
categorized survey of studies on multimodal deep generative models.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Adapting to Online Label Shift with Provable Guarantees</b></summary>
  <p><b>编号</b>：[34]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02121</p>
  <p><b>作者</b>：Yong Bai,  Yu-Jie Zhang,  Peng Zhao,  Masashi Sugiyama,  Zhi-Hua Zhou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：standard supervised learning, supervised learning paradigm, learning paradigm works, paradigm works effectively, upcoming testing samples</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The standard supervised learning paradigm works effectively when training
data shares the same distribution as the upcoming testing samples. However,
this assumption is often violated in real-world applications, especially when
testing data appear in an online fashion. In this paper, we formulate and
investigate the problem of online label shift (OLaS): the learner trains an
initial model from the labeled offline data and then deploys it to an unlabeled
online environment where the underlying label distribution changes over time
but the label-conditional density does not. The non-stationarity nature and the
lack of supervision make the problem challenging to be tackled. To address the
difficulty, we construct a new unbiased risk estimator that utilizes the
unlabeled data, which exhibits many benign properties albeit with potential
non-convexity. Building upon that, we propose novel online ensemble algorithms
to deal with the non-stationarity of the environments. Our approach enjoys
optimal dynamic regret, indicating that the performance is competitive with a
clairvoyant who knows the online environments in hindsight and then chooses the
best decision for each round. The obtained dynamic regret bound scales with the
intensity and pattern of label distribution shift, hence exhibiting the
adaptivity in the OLaS problem. Extensive experiments are conducted to validate
the effectiveness and support our theoretical findings.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Improving Covariance Conditioning of the SVD Meta-layer by Orthogonality</b></summary>
  <p><b>编号</b>：[35]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02119</p>
  <p><b>作者</b>：Yue Song,  Nicu Sebe,  Wei Wang</p>
  <p><b>备注</b>：Accepted by ECCV22</p>
  <p><b>关键词</b>：Inserting an SVD, SVD meta-layer, meta-layer into neural, neural networks, networks is prone</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Inserting an SVD meta-layer into neural networks is prone to make the
covariance ill-conditioned, which could harm the model in the training
stability and generalization abilities. In this paper, we systematically study
how to improve the covariance conditioning by enforcing orthogonality to the
Pre-SVD layer. Existing orthogonal treatments on the weights are first
investigated. However, these techniques can improve the conditioning but would
hurt the performance. To avoid such a side effect, we propose the Nearest
Orthogonal Gradient (NOG) and Optimal Learning Rate (OLR). The effectiveness of
our methods is validated in two applications: decorrelated Batch Normalization
(BN) and Global Covariance Pooling (GCP). Extensive experiments on visual
recognition demonstrate that our methods can simultaneously improve the
covariance conditioning and generalization. Moreover, the combinations with
orthogonal weight can further boost the performances.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：An Intrusion Detection System based on Deep Belief Networks</b></summary>
  <p><b>编号</b>：[36]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02117</p>
  <p><b>作者</b>：Othmane Belarbi,  Aftab Khan,  Pietro Carnelli,  Theodoros Spyridopoulos</p>
  <p><b>备注</b>：15 pages, 4 figures, 4 tables. To be presented at the 4th International Conference on Science of Cyber Security (SciSec) in August 2022</p>
  <p><b>关键词</b>：rapid growth, cyber-security threats, proposed DBN approach, growth of connected, proposed DBN</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rapid growth of connected devices has led to the proliferation of novel
cyber-security threats known as zero-day attacks. Traditional behaviour-based
IDS rely on DNN to detect these attacks. The quality of the dataset used to
train the DNN plays a critical role in the detection performance, with
underrepresented samples causing poor performances. In this paper, we develop
and evaluate the performance of DBN on detecting cyber-attacks within a network
of connected devices. The CICIDS2017 dataset was used to train and evaluate the
performance of our proposed DBN approach. Several class balancing techniques
were applied and evaluated. Lastly, we compare our approach against a
conventional MLP model and the existing state-of-the-art. Our proposed DBN
approach shows competitive and promising results, with significant performance
improvement on the detection of attacks underrepresented in the training
dataset.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：An Empirical Study of Implicit Regularization in Deep Offline RL</b></summary>
  <p><b>编号</b>：[46]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02099</p>
  <p><b>作者</b>：Caglar Gulcehre,  Srivatsan Srinivasan,  Jakub Sygnowski,  Georg Ostrovski,  Mehrdad Farajtabar,  Matt Hoffman,  Razvan Pascanu,  Arnaud Doucet</p>
  <p><b>备注</b>：40 pages, 37 figures, 2 tables</p>
  <p><b>关键词</b>：offline Reinforcement Learning, Deep neural networks, effective rank, Reinforcement Learning, offline Reinforcement</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep neural networks are the most commonly used function approximators in
offline Reinforcement Learning these days. Prior works have shown that neural
nets trained with TD-learning and gradient descent can exhibit implicit
regularization that can be characterized by under-parameterization of these
networks. Specifically, the rank of the penultimate feature layer, also called
\textit{effective rank}, has been observed to drastically collapse during the
training. In turn, this collapse has been argued to reduce the model's ability
to further adapt in later stages of learning, leading to the diminished final
performance. Such an association between the effective rank and performance
makes effective rank compelling for offline RL, primarily for offline policy
evaluation. In this work, we conduct a careful empirical study on the relation
between effective rank and performance on three offline RL datasets : bsuite,
Atari, and DeepMind lab. We observe that a direct association exists only in
restricted settings and disappears in the more extensive hyperparameter sweeps.
Also, we empirically identify three phases of learning that explain the impact
of implicit regularization on the learning dynamics and found that
bootstrapping alone is insufficient to explain the collapse of the effective
rank. Further, we show that several other factors could confound the
relationship between effective rank and performance and conclude that studying
this association under simplistic assumptions could be highly misleading.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Neural Networks and the Chomsky Hierarchy</b></summary>
  <p><b>编号</b>：[47]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02098</p>
  <p><b>作者</b>：Grégoire Delétang,  Anian Ruoss,  Jordi Grau-Moya,  Tim Genewein,  Li Kevin Wenliang,  Elliot Catt,  Marcus Hutter,  Shane Legg,  Pedro A. Ortega</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Reliable generalization lies, heart of safe, Reliable generalization, tasks, generalization lies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reliable generalization lies at the heart of safe ML and AI. However,
understanding when and how neural networks generalize remains one of the most
important unsolved problems in the field. In this work, we conduct an extensive
empirical study (2200 models, 16 tasks) to investigate whether insights from
the theory of computation can predict the limits of neural network
generalization in practice. We demonstrate that grouping tasks according to the
Chomsky hierarchy allows us to forecast whether certain architectures will be
able to generalize to out-of-distribution inputs. This includes negative
results where even extensive amounts of data and training time never led to any
non-trivial generalization, despite models having sufficient capacity to
perfectly fit the training data. Our results show that, for our subset of
tasks, RNNs and Transformers fail to generalize on non-regular tasks, LSTMs can
solve regular and counter-language tasks, and only networks augmented with
structured memory (such as a stack or memory tape) can successfully generalize
on context-free and context-sensitive tasks.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Predicting Out-of-Domain Generalization with Local Manifold Smoothness</b></summary>
  <p><b>编号</b>：[48]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02093</p>
  <p><b>作者</b>：Nathan Ng,  Kyunghyun Cho,  Neha Hulkund,  Marzyeh Ghassemi</p>
  <p><b>备注</b>：18 pages, 3 figures</p>
  <p><b>关键词</b>：machine learning models, Understanding how machine, safe deployment, machine learning, critical part</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Understanding how machine learning models generalize to new environments is a
critical part of their safe deployment. Recent work has proposed a variety of
complexity measures that directly predict or theoretically bound the
generalization capacity of a model. However, these methods rely on a strong set
of assumptions that in practice are not always satisfied. Motivated by the
limited settings in which existing measures can be applied, we propose a novel
complexity measure based on the local manifold smoothness of a classifier. We
define local manifold smoothness as a classifier's output sensitivity to
perturbations in the manifold neighborhood around a given test point.
Intuitively, a classifier that is less sensitive to these perturbations should
generalize better. To estimate smoothness we sample points using data
augmentation and measure the fraction of these points classified into the
majority class. Our method only requires selecting a data augmentation method
and makes no other assumptions about the model or data distributions, meaning
it can be applied even in out-of-domain (OOD) settings where existing methods
cannot. In experiments on robustness benchmarks in image classification,
sentiment analysis, and natural language inference, we demonstrate a strong and
robust correlation between our manifold smoothness measure and actual OOD
generalization on over 3,000 models evaluated on over 100 train/test domain
pairs.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Learning to Accelerate Approximate Methods for Solving Integer  Programming via Early Fixing</b></summary>
  <p><b>编号</b>：[51]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02087</p>
  <p><b>作者</b>：Longkang Li,  Baoyuan Wu</p>
  <p><b>备注</b>：16 pages, 11 figures, 6 tables</p>
  <p><b>关键词</b>：early fixing framework, early fixing, fixing framework, important and challenging, Approximate methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Integer programming (IP) is an important and challenging problem. Approximate
methods have shown promising performance on both effectiveness and efficiency
for solving the IP problem. However, we observed that a large fraction of
variables solved by some iterative approximate methods fluctuate around their
final converged discrete states in very long iterations. Inspired by this
observation, we aim to accelerate these approximate methods by early fixing
these fluctuated variables to their converged states while not significantly
harming the solution accuracy. To this end, we propose an early fixing
framework along with the approximate method. We formulate the whole early
fixing process as a Markov decision process, and train it using imitation
learning. A policy network will evaluate the posterior probability of each free
variable concerning its discrete candidate states in each block of iterations.
Specifically, we adopt the powerful multi-headed attention mechanism in the
policy network. Extensive experiments on our proposed early fixing framework
are conducted to three different IP applications: constrained linear
programming, MRF energy minimization and sparse adversarial attack. The former
one is linear IP problem, while the latter two are quadratic IP problems. We
extend the problem scale from regular size to significantly large size. The
extensive experiments reveal the competitiveness of our early fixing framework:
the runtime speeds up significantly, while the solution quality does not
degrade much, even in some cases it is available to obtain better solutions.
Our proposed early fixing framework can be regarded as an acceleration
extension of ADMM methods for solving integer programming. The source codes are
available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Resource Allocation in Multicore Elastic Optical Networks: A Deep  Reinforcement Learning Approach</b></summary>
  <p><b>编号</b>：[54]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02074</p>
  <p><b>作者</b>：Juan Pinto-Ríos,  Felipe Calderón,  Ariel Leiva,  Gabriel Hermosilla,  Alejandra Beghelli,  Danilo Bórquez-Paredes,  Astrid Lozada,  Nicolás Jara,  Ricardo Olivares,  Gabriel Saavedra</p>
  <p><b>备注</b>：11 pages, 10 figures</p>
  <p><b>关键词</b>：deep reinforcement learning, reinforcement learning approach, dynamic multicore fiber, multicore fiber elastic, fiber elastic optical</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A deep reinforcement learning approach is applied, for the first time, to
solve the routing, modulation, spectrum and core allocation (RMSCA) problem in
dynamic multicore fiber elastic optical networks (MCF-EONs). To do so, a new
environment - compatible with OpenAI's Gym - was designed and implemented to
emulate the operation of MCF-EONs. The new environment processes the agent
actions (selection of route, core and spectrum slot) by considering the network
state and physical-layer-related aspects. The latter includes the available
modulation formats and their reach and the inter-core crosstalk (XT), an
MCF-related impairment. If the resulting quality of the signal is acceptable,
the environment allocates the resources selected by the agent. After processing
the agent's action, the environment is configured to give the agent a numerical
reward and information about the new network state. The blocking performance of
four different agents was compared through simulation to 3 baseline heuristics
used in MCF-EONs. Results obtained for the NSFNet and COST239 network
topologies show that the best-performing agent achieves, on average, up to a
four-times decrease in blocking probability concerning the best-performing
baseline heuristic methods.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Image Amodal Completion: A Survey</b></summary>
  <p><b>编号</b>：[58]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02062</p>
  <p><b>作者</b>：Jiayang Ao,  Krista A. Ehinger,  Qiuhong Ke</p>
  <p><b>备注</b>：Manuscript Submitted for Publication</p>
  <p><b>关键词</b>：Image amodal completion, computer vision systems, amodal completion, Image amodal, partially occluded objects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing computer vision systems can compete with humans in understanding the
visible parts of objects, but still fall far short of humans when it comes to
depicting the invisible parts of partially occluded objects. Image amodal
completion aims to equip computers with human-like amodal completion functions
to understand an intact object despite it being partially occluded. The main
purpose of this survey is to provide an intuitive understanding of the research
hotspots, key technologies and future trends in the field of image amodal
completion. Firstly, we present a comprehensive review of the latest literature
in this emerging field, exploring three key tasks in image amodal completion,
including amodal shape completion, amodal appearance completion, and order
perception. Then we examine popular datasets related to image amodal completion
along with their common data collection methods and evaluation metrics.
Finally, we discuss real-world applications and future research directions for
image amodal completion, facilitating the reader's understanding of the
challenges of existing technologies and upcoming research trends.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：PRoA: A Probabilistic Robustness Assessment against Functional  Perturbations</b></summary>
  <p><b>编号</b>：[67]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02036</p>
  <p><b>作者</b>：Tianle Zhang,  Wenjie Ruan,  Jonathan E. Fieldsend</p>
  <p><b>备注</b>：The short version of this work will appear in the Proceedings of the 2022 European Conference on Machine Learning and Data Mining (ECML-PKDD 2022)</p>
  <p><b>关键词</b>：vital pre-deployment phase, applications robustness measurement, safety-critical deep learning, deep learning applications, learning applications robustness</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In safety-critical deep learning applications robustness measurement is a
vital pre-deployment phase. However, existing robustness verification methods
are not sufficiently practical for deploying machine learning systems in the
real world. On the one hand, these methods attempt to claim that no
perturbations can ``fool'' deep neural networks (DNNs), which may be too
stringent in practice. On the other hand, existing works rigorously consider
$L_p$ bounded additive perturbations on the pixel space, although
perturbations, such as colour shifting and geometric transformations, are more
practically and frequently occurring in the real world. Thus, from the
practical standpoint, we present a novel and general {\it probabilistic
robustness assessment method} (PRoA) based on the adaptive concentration, and
it can measure the robustness of deep learning models against functional
perturbations. PRoA can provide statistical guarantees on the probabilistic
robustness of a model, \textit{i.e.}, the probability of failure encountered by
the trained model after deployment. Our experiments demonstrate the
effectiveness and flexibility of PRoA in terms of evaluating the probabilistic
robustness against a broad range of functional perturbations, and PRoA can
scale well to various large-scale deep neural networks compared to existing
state-of-the-art baselines. For the purpose of reproducibility, we release our
tool on GitHub: \url{ this https URL}.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Robust Reinforcement Learning in Continuous Control Tasks with  Uncertainty Set Regularization</b></summary>
  <p><b>编号</b>：[71]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02016</p>
  <p><b>作者</b>：Yuan Zhang,  Jianhong Wang,  Joschka Boedecker</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：environmental perturbations, generalization and robustness, robustness under environmental, excessively restricts, restricts its application</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reinforcement learning (RL) is recognized as lacking generalization and
robustness under environmental perturbations, which excessively restricts its
application for real-world robotics. Prior work claimed that adding
regularization to the value function is equivalent to learning a robust policy
with uncertain transitions. Although the regularization-robustness
transformation is appealing for its simplicity and efficiency, it is still
lacking in continuous control tasks. In this paper, we propose a new
regularizer named $\textbf{U}$ncertainty $\textbf{S}$et $\textbf{R}$egularizer
(USR), by formulating the uncertainty set on the parameter space of the
transition function. In particular, USR is flexible enough to be plugged into
any existing RL framework. To deal with unknown uncertainty sets, we further
propose a novel adversarial approach to generate them based on the value
function. We evaluate USR on the Real-world Reinforcement Learning (RWRL)
benchmark, demonstrating improvements in the robust performance for perturbed
testing environments.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Towards trustworthy Energy Disaggregation: A review of challenges,  methods and perspectives for Non-Intrusive Load Monitoring</b></summary>
  <p><b>编号</b>：[74]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02009</p>
  <p><b>作者</b>：Maria Kaselimi,  Eftychios Protopapadakis,  Athanasios Voulodimos,  Nikolaos Doulamis,  Anastasios Doulamis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Non-intrusive load monitoring, total power consumption, NILM, Non-intrusive load, load monitoring</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Non-intrusive load monitoring (NILM) is the task of disaggregating the total
power consumption into its individual sub-components. Over the years, signal
processing and machine learning algorithms have been combined to achieve this.
A lot of publications and extensive research works are performed on energy
disaggregation or NILM for the state-of-the-art methods to reach on the
desirable performance. The initial interest of the scientific community to
formulate and describe mathematically the NILM problem using machine learning
tools has now shifted into a more practical NILM. Nowadays, we are in the
mature NILM period where there is an attempt for NILM to be applied in
real-life application scenarios. Thus, complexity of the algorithms,
transferability, reliability, practicality and in general trustworthiness are
the main issues of interest. This review narrows the gap between the early
immature NILM era and the mature one. In particular, the paper provides a
comprehensive literature review of the NILM methods for residential appliances
only. The paper analyzes, summarizes and presents the outcomes of a large
number of recently published scholarly articles. Also, the paper discusses the
highlights of these methods and introduces the research dilemmas that should be
taken into consideration by researchers to apply NILM methods. Finally, we show
the need for transferring the traditional disaggregation models into a
practical and trustworthy framework.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：The StarCraft Multi-Agent Challenges+ : Learning of Multi-Stage Tasks  and Environmental Factors without Precise Reward Functions</b></summary>
  <p><b>编号</b>：[76]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02007</p>
  <p><b>作者</b>：Mingyu Kim,  Jihwan Oh,  Yongsik Lee,  Joonkee Kim,  Seonghwan Kim,  Song Chong,  Se-Young Yun</p>
  <p><b>备注</b>：ICML Workshop: AI for Agent Based Modeling 2022 Spotlight</p>
  <p><b>关键词</b>：Multi-Agent Reinforcement Learning, called the StarCraft, reward functions, benchmark called, StarCraft Multi-Agent</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we propose a novel benchmark called the StarCraft Multi-Agent
Challenges+, where agents learn to perform multi-stage tasks and to use
environmental factors without precise reward functions. The previous challenges
(SMAC) recognized as a standard benchmark of Multi-Agent Reinforcement Learning
are mainly concerned with ensuring that all agents cooperatively eliminate
approaching adversaries only through fine manipulation with obvious reward
functions.
This challenge, on the other hand, is interested in the exploration
capability of MARL algorithms to efficiently learn implicit multi-stage tasks
and environmental factors as well as micro-control. This study covers both
offensive and defensive scenarios.
In the offensive scenarios, agents must learn to first find opponents and
then eliminate them. The defensive scenarios require agents to use topographic
features. For example, agents need to position themselves behind protective
structures to make it harder for enemies to attack. We investigate MARL
algorithms under SMAC+ and observe that recent approaches work well in similar
settings to the previous challenges, but misbehave in offensive scenarios.
Additionally, we observe that an enhanced exploration approach has a positive
effect on performance but is not able to completely solve all scenarios. This
study proposes new directions for future research.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Disentangling private classes through regularization</b></summary>
  <p><b>编号</b>：[78]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02000</p>
  <p><b>作者</b>：Enzo Tartaglione,  Francesca Gennari,  Marco Grangetto</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：nowadays broadly deployed, incredibly large variety, General Data Protection, nowadays broadly, broadly deployed</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning models are nowadays broadly deployed to solve an incredibly
large variety of tasks. However, little attention has been devoted to connected
legal aspects. In 2016, the European Union approved the General Data Protection
Regulation which entered into force in 2018. Its main rationale was to protect
the privacy and data protection of its citizens by the way of operating of the
so-called "Data Economy". As data is the fuel of modern Artificial
Intelligence, it is argued that the GDPR can be partly applicable to a series
of algorithmic decision making tasks before a more structured AI Regulation
enters into force. In the meantime, AI should not allow undesired information
leakage deviating from the purpose for which is created. In this work we
propose DisP, an approach for deep learning models disentangling the
information related to some classes we desire to keep private, from the data
processed by AI. In particular, DisP is a regularization strategy
de-correlating the features belonging to the same private class at training
time, hiding the information of private classes membership. Our experiments on
state-of-the-art deep learning models show the effectiveness of DisP,
minimizing the risk of extraction for the classes we desire to keep private.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Local Multi-Label Explanations for Random Forest</b></summary>
  <p><b>编号</b>：[80]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01994</p>
  <p><b>作者</b>：Nikolaos Mylonas,  Ioannis Mollas,  Nick Bassiliades,  Grigorios Tsoumakas</p>
  <p><b>备注</b>：11 pages, 1 figues, 8 tables, submitted to XKDD (workshop of ECML PKDD 2022)</p>
  <p><b>关键词</b>：challenging task, Multi-label classification, Multi-label, Random forest, classification</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multi-label classification is a challenging task, particularly in domains
where the number of labels to be predicted is large. Deep neural networks are
often effective at multi-label classification of images and textual data. When
dealing with tabular data, however, conventional machine learning algorithms,
such as tree ensembles, appear to outperform competition. Random forest, being
a popular ensemble algorithm, has found use in a wide range of real-world
problems. Such problems include fraud detection in the financial domain, crime
hotspot detection in the legal sector, and in the biomedical field, disease
probability prediction when patient records are accessible. Since they have an
impact on people's lives, these domains usually require decision-making systems
to be explainable. Random Forest falls short on this property, especially when
a large number of tree predictors are used. This issue was addressed in a
recent research named LionForests, regarding single label classification and
regression. In this work, we adapt this technique to multi-label classification
problems, by employing three different strategies regarding the labels that the
explanation covers. Finally, we provide a set of qualitative and quantitative
experiments to assess the efficacy of this approach.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Conflicting Interactions Among Protections Mechanisms for Machine  Learning Models</b></summary>
  <p><b>编号</b>：[81]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01991</p>
  <p><b>作者</b>：Sebastian Szyller,  N. Asokan</p>
  <p><b>备注</b>：10 pages, 8 tables, 2 figures</p>
  <p><b>关键词</b>：machine learning, based on machine, systems based, Nowadays, learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Nowadays, systems based on machine learning (ML) are widely used in different
domains. Given their popularity, ML models have become targets for various
attacks. As a result, research at the intersection of security and privacy, and
ML has flourished.
The research community has been exploring the attack vectors and potential
mitigations separately. However, practitioners will likely need to deploy
defences against several threats simultaneously. A solution that is optimal for
a specific concern may interact negatively with solutions intended to address
other concerns.
In this work, we explore the potential for conflicting interactions between
different solutions that enhance the security/privacy of ML-base systems. We
focus on model and data ownership; exploring how ownership verification
techniques interact with other ML security/privacy techniques like
differentially private training, and robustness against model evasion. We
provide a framework, and conduct systematic analysis of pairwise interactions.
We show that many pairs are incompatible. Where possible, we provide
relaxations to the hyperparameters or the techniques themselves that allow for
the simultaneous deployment. Lastly, we discuss the implications and provide
guidelines for future work.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Unsupervised Crowdsourcing with Accuracy and Cost Guarantees</b></summary>
  <p><b>编号</b>：[82]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01988</p>
  <p><b>作者</b>：Yashvardhan Didwania,  Jayakrishnan Nair,  N. Hemachandra</p>
  <p><b>备注</b>：To be presented at WiOpt 2022</p>
  <p><b>关键词</b>：unsupervised classification, crowdsourcing platform, problem of cost-optimal, cost-optimal utilization, prescribed error threshold</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider the problem of cost-optimal utilization of a crowdsourcing
platform for binary, unsupervised classification of a collection of items,
given a prescribed error threshold. Workers on the crowdsourcing platform are
assumed to be divided into multiple classes, based on their skill, experience,
and/or past performance. We model each worker class via an unknown confusion
matrix, and a (known) price to be paid per label prediction. For this setting,
we propose algorithms for acquiring label predictions from workers, and for
inferring the true labels of items. We prove that if the number of (unlabeled)
items available is large enough, our algorithms satisfy the prescribed error
thresholds, incurring a cost that is near-optimal. Finally, we validate our
algorithms, and some heuristics inspired by them, through an extensive case
study.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Defending against the Label-flipping Attack in Federated Learning</b></summary>
  <p><b>编号</b>：[85]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01982</p>
  <p><b>作者</b>：Najeeb Moharram Jebreel,  Josep Domingo-Ferrer,  David Sánchez,  Alberto Blanco-Justicia</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Federated learning, machine learning, source class, source class accuracy, privacy by design</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Federated learning (FL) provides autonomy and privacy by design to
participating peers, who cooperatively build a machine learning (ML) model
while keeping their private data in their devices. However, that same autonomy
opens the door for malicious peers to poison the model by conducting either
untargeted or targeted poisoning attacks. The label-flipping (LF) attack is a
targeted poisoning attack where the attackers poison their training data by
flipping the labels of some examples from one class (i.e., the source class) to
another (i.e., the target class). Unfortunately, this attack is easy to perform
and hard to detect and it negatively impacts on the performance of the global
model. Existing defenses against LF are limited by assumptions on the
distribution of the peers' data and/or do not perform well with
high-dimensional models. In this paper, we deeply investigate the LF attack
behavior and find that the contradicting objectives of attackers and honest
peers on the source class examples are reflected in the parameter gradients
corresponding to the neurons of the source and target classes in the output
layer, making those gradients good discriminative features for the attack
detection. Accordingly, we propose a novel defense that first dynamically
extracts those gradients from the peers' local updates, and then clusters the
extracted gradients, analyzes the resulting clusters and filters out potential
bad updates before model aggregation. Extensive empirical analysis on three
data sets shows the proposed defense's effectiveness against the LF attack
regardless of the data distribution or model dimensionality. Also, the proposed
defense outperforms several state-of-the-art defenses by offering lower test
error, higher overall accuracy, higher source class accuracy, lower attack
success rate, and higher stability of the source class accuracy.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：A Safe Semi-supervised Graph Convolution Network</b></summary>
  <p><b>编号</b>：[92]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01960</p>
  <p><b>作者</b>：Zhi Yang,  Yadong Yan,  Haitao Gan,  Jing Zhao,  Zhiwei Ye</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Graph Convolution Network, Graph Convolution, unlabeled data, introducing convolution, semi-supervised learning field</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the semi-supervised learning field, Graph Convolution Network (GCN), as a
variant model of GNN, has achieved promising results for non-Euclidean data by
introducing convolution into GNN. However, GCN and its variant models fail to
safely use the information of risk unlabeled data, which will degrade the
performance of semi-supervised learning. Therefore, we propose a Safe GCN
framework (Safe-GCN) to improve the learning performance. In the Safe-GCN, we
design an iterative process to label the unlabeled data. In each iteration, a
GCN and its supervised version(S-GCN) are learned to find the unlabeled data
with high confidence. The high-confidence unlabeled data and their pseudo
labels are then added to the label set. Finally, both added unlabeled data and
labeled ones are used to train a S-GCN which can achieve the safe exploration
of the risk unlabeled data and enable safe use of large numbers of unlabeled
data. The performance of Safe-GCN is evaluated on three well-known citation
network datasets and the obtained results demonstrate the effectiveness of the
proposed framework over several graph-based semi-supervised learning methods.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Ask-AC: An Initiative Advisor-in-the-Loop Actor-Critic Framework</b></summary>
  <p><b>编号</b>：[93]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01955</p>
  <p><b>作者</b>：Shunyu Liu,  Xinchao Wang,  Na Yu,  Jie Song,  Kaixuan Chen,  Zunlei Feng,  Mingli Song</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：passively receiving supervision, receiving supervision signals, promising results achieved, interactive reinforcement learning, expensive learning process</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite the promising results achieved, state-of-the-art interactive
reinforcement learning schemes rely on passively receiving supervision signals
from advisor experts, in the form of either continuous monitoring or
pre-defined rules, which inevitably result in a cumbersome and expensive
learning process. In this paper, we introduce a novel initiative
advisor-in-the-loop actor-critic framework, termed as Ask-AC, that replaces the
unilateral advisor-guidance mechanism with a bidirectional learner-initiative
one, and thereby enables a customized and efficacious message exchange between
learner and advisor. At the heart of Ask-AC are two complementary components,
namely action requester and adaptive state selector, that can be readily
incorporated into various discrete actor-critic architectures. The former
component allows the agent to initiatively seek advisor intervention in the
presence of uncertain states, while the latter identifies the unstable states
potentially missed by the former especially when environment changes, and then
learns to promote the ask action on such states. Experimental results on both
stationary and non-stationary environments and across different actor-critic
backbones demonstrate that the proposed framework significantly improves the
learning efficiency of the agent, and achieves the performances on par with
those obtained by continuous advisor monitoring.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Entity Linking in Tabular Data Needs the Right Attention</b></summary>
  <p><b>编号</b>：[100]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01937</p>
  <p><b>作者</b>：Miltiadis Marios Katsakioris,  Yiwei Zhou,  Daniele Masato</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：requires Entity Linking, tabular data, tabular data requires, data requires Entity, Understanding the semantic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Understanding the semantic meaning of tabular data requires Entity Linking
(EL), in order to associate each cell value to a real-world entity in a
Knowledge Base (KB). In this work, we focus on end-to-end solutions for EL on
tabular data that do not rely on fact lookup in the target KB. Tabular data
contains heterogeneous and sparse context, including column headers, cell
values and table captions. We experiment with various models to generate a
vector representation for each cell value to be linked. Our results show that
it is critical to apply an attention mechanism as well as an attention mask, so
that the model can only attend to the most relevant context and avoid
information dilution. The most relevant context includes: same-row cells,
same-column cells, headers and caption. Computational complexity, however,
grows quadratically with the size of tabular data for such a complex model. We
achieve constant memory usage by introducing a Tabular Entity Linking Lite
model (TELL ) that generates vector representation for a cell based only on its
value, the table headers and the table caption. TELL achieves 80.8% accuracy on
Wikipedia tables, which is only 0.1% lower than the state-of-the-art model with
quadratic memory usage.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Explainability in Deep Reinforcement Learning, a Review into Current  Methods and Applications</b></summary>
  <p><b>编号</b>：[112]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01911</p>
  <p><b>作者</b>：Thomas Hickling,  Abdelhafid Zenati,  Nabil Aouf,  Phillippa Spencer</p>
  <p><b>备注</b>：21 pages, 6 figures, Paper Review</p>
  <p><b>关键词</b>：Deep Reinforcement Learning, Reinforcement Learning, Deep Reinforcement, schemes has increased, increased dramatically</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The use of Deep Reinforcement Learning (DRL) schemes has increased
dramatically since their first introduction in 2015. Though uses in many
different applications are being found they still have a problem with the lack
of interpretability. This has bread a lack of understanding and trust in the
use of DRL solutions from researchers and the general public. To solve this
problem the field of explainable artificial intelligence (XAI) has emerged.
This is a variety of different methods that look to open the DRL black boxes,
they range from the use of interpretable symbolic decision trees to numerical
methods like Shapley Values. This review looks at which methods are being used
and what applications they are being used. This is done to identify which
models are the best suited to each application or if a method is being
underutilised.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Multi-Scored Sleep Databases: How to Exploit the Multiple-Labels in  Automated Sleep Scoring</b></summary>
  <p><b>编号</b>：[113]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01910</p>
  <p><b>作者</b>：Luigi Fiorillo,  Davide Pedroncelli,  Paolo Favaro,  Francesca Dalia Faraci</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Inter-scorer variability, Study Objectives, well-known problem, models, hypnodensity-graph generated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Study Objectives: Inter-scorer variability in scoring polysomnograms is a
well-known problem. Most of the existing automated sleep scoring systems are
trained using labels annotated by a single scorer, whose subjective evaluation
is transferred to the model. When annotations from two or more scorers are
available, the scoring models are usually trained on the scorer consensus. The
averaged scorer's subjectivity is transferred into the model, losing
information about the internal variability among different scorers. In this
study, we aim to insert the multiple-knowledge of the different physicians into
the training procedure.The goal is to optimize a model training, exploiting the
full information that can be extracted from the consensus of a group of
scorers.
Methods: We train two lightweight deep learning based models on three
different multi-scored databases. We exploit the label smoothing technique
together with a soft-consensus (LSSC) distribution to insert the
multiple-knowledge in the training procedure of the model. We introduce the
averaged cosine similarity metric (ACS) to quantify the similarity between the
hypnodensity-graph generated by the models with-LSSC and the hypnodensity-graph
generated by the scorer consensus.
Results: The performance of the models improves on all the databases when we
train the models with our LSSC. We found an increase in ACS (up to 6.4%)
between the hypnodensity-graph generated by the models trained with-LSSC and
the hypnodensity-graph generated by the consensus.
Conclusions: Our approach definitely enables a model to better adapt to the
consensus of the group of scorers. Future work will focus on further
investigations on different scoring architectures.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：A Deep Learning Approach for the solution of Probability Density  Evolution of Stochastic Systems</b></summary>
  <p><b>编号</b>：[115]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01907</p>
  <p><b>作者</b>：Seid H. Pourtakdoust,  Amir H. Khodabakhsh</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：probability density evolution, probability density, density evolution, General Density Evolution, density</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Derivation of the probability density evolution provides invaluable insight
into the behavior of many stochastic systems and their performance. However,
for most real-time applica-tions, numerical determination of the probability
density evolution is a formidable task. The latter is due to the required
temporal and spatial discretization schemes that render most computational
solutions prohibitive and impractical. In this respect, the development of an
efficient computational surrogate model is of paramount importance. Recent
studies on the physics-constrained networks show that a suitable surrogate can
be achieved by encoding the physical insight into a deep neural network. To
this aim, the present work introduces DeepPDEM which utilizes the concept of
physics-informed networks to solve the evolution of the probability density via
proposing a deep learning method. DeepPDEM learns the General Density Evolution
Equation (GDEE) of stochastic structures. This approach paves the way for a
mesh-free learning method that can solve the density evolution problem with-out
prior simulation data. Moreover, it can also serve as an efficient surrogate
for the solu-tion at any other spatiotemporal points within optimization
schemes or real-time applica-tions. To demonstrate the potential applicability
of the proposed framework, two network architectures with different activation
functions as well as two optimizers are investigated. Numerical implementation
on three different problems verifies the accuracy and efficacy of the proposed
method.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题："Even if ..." -- Diverse Semifactual Explanations of Reject</b></summary>
  <p><b>编号</b>：[119]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01898</p>
  <p><b>作者</b>：André Artelt,  Barbara Hammer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：safety critical areas, critical areas require, areas require reliable, require reliable high, reliable high certainty</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine learning based decision making systems applied in safety critical
areas require reliable high certainty predictions. For this purpose, the system
can be extended by an reject option which allows the system to reject inputs
where only a prediction with an unacceptably low certainty would be possible.
While being able to reject uncertain samples is important, it is also of
importance to be able to explain why a particular sample was rejected. With the
ongoing rise of eXplainable AI (XAI), a lot of explanation methodologies for
machine learning based systems have been developed -- explaining reject
options, however, is still a novel field where only very little prior work
exists.
In this work, we propose to explain rejects by semifactual explanations, an
instance of example-based explanation methods, which them self have not been
widely considered in the XAI community yet. We propose a conceptual modeling of
semifactual explanations for arbitrary reject options and empirically evaluate
a specific implementation on a conformal prediction based reject option.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：The Deep Ritz Method for Parametric $p$-Dirichlet Problems</b></summary>
  <p><b>编号</b>：[122]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01894</p>
  <p><b>作者</b>：Alex Kaltenbach,  Marius Zeinhofer</p>
  <p><b>备注</b>：30 pages, 11 figures</p>
  <p><b>关键词</b>：Deep Ritz Method, Dirichlet problems deploying, Deep Ritz, Ritz Method, Ritz Method retains</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We establish error estimates for the approximation of parametric
$p$-Dirichlet problems deploying the Deep Ritz Method. Parametric dependencies
include, e.g., varying geometries and exponents $p\in (1,\infty)$. Combining
the derived error estimates with quantitative approximation theorems yields
error decay rates and establishes that the Deep Ritz Method retains the
favorable approximation capabilities of neural networks in the approximation of
high dimensional functions which makes the method attractive for parametric
problems. Finally, we present numerical examples to illustrate potential
applications.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：ICE-NODE: Integration of Clinical Embeddings with Neural Ordinary  Differential Equations</b></summary>
  <p><b>编号</b>：[129]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01873</p>
  <p><b>作者</b>：Asem Alaa,  Erik Mayer,  Mauricio Barahona</p>
  <p><b>备注</b>：Accepted at Machine Learning for Healthcare 2022</p>
  <p><b>关键词</b>：lower treatment costs, higher survival rates, treatment costs, higher survival, survival rates</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Early diagnosis of disease can result in improved health outcomes, such as
higher survival rates and lower treatment costs. With the massive amount of
information in electronic health records (EHRs), there is great potential to
use machine learning (ML) methods to model disease progression aimed at early
prediction of disease onset and other outcomes. In this work, we employ recent
innovations in neural ODEs to harness the full temporal information of EHRs. We
propose ICE-NODE (Integration of Clinical Embeddings with Neural Ordinary
Differential Equations), an architecture that temporally integrates embeddings
of clinical codes and neural ODEs to learn and predict patient trajectories in
EHRs. We apply our method to the publicly available MIMIC-III and MIMIC-IV
datasets, reporting improved prediction results compared to state-of-the-art
methods, specifically for clinical codes that are not frequently observed in
EHRs. We also show that ICE-NODE is more competent at predicting certain
medical conditions, like acute renal failure and pulmonary heart disease, and
is also able to produce patient risk trajectories over time that can be
exploited for further predictions.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Bayesian approaches for Quantifying Clinicians' Variability in Medical  Image Quantification</b></summary>
  <p><b>编号</b>：[133]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01868</p>
  <p><b>作者</b>：Jaeik Jeon,  Yeonggul Jang,  Youngtaek Hong,  Hackjoon Shim,  Sekeun Kim</p>
  <p><b>备注</b>：9 pages, 8 figures</p>
  <p><b>关键词</b>：plays a vital, vital role, including MRI, clinical decisions, Bayesian predictive distribution</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Medical imaging, including MRI, CT, and Ultrasound, plays a vital role in
clinical decisions. Accurate segmentation is essential to measure the structure
of interest from the image. However, manual segmentation is highly
operator-dependent, which leads to high inter and intra-variability of
quantitative measurements. In this paper, we explore the feasibility that
Bayesian predictive distribution parameterized by deep neural networks can
capture the clinicians' inter-intra variability. By exploring and analyzing
recently emerged approximate inference schemes, we evaluate whether approximate
Bayesian deep learning with the posterior over segmentations can learn
inter-intra rater variability both in segmentation and clinical measurements.
The experiments are performed with two different imaging modalities: MRI and
ultrasound. We empirically demonstrated that Bayesian predictive distribution
parameterized by deep neural networks could approximate the clinicians'
inter-intra variability. We show a new perspective in analyzing medical images
quantitatively by providing clinical measurement uncertainty.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Meta-Learning a Real-Time Tabular AutoML Method For Small Data</b></summary>
  <p><b>编号</b>：[137]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01848</p>
  <p><b>作者</b>：Noah Hollmann,  Samuel Müller,  Katharina Eggensperger,  Frank Hutter</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：AutoML method, method, Transformer-based Prior-Data Fitted, Bayesian neural networks, single neural network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present TabPFN, an AutoML method that is competitive with the state of the
art on small tabular datasets while being over 1,000$\times$ faster. Our method
is very simple: it is fully entailed in the weights of a single neural network,
and a single forward pass directly yields predictions for a new dataset. Our
AutoML method is meta-learned using the Transformer-based Prior-Data Fitted
Network (PFN) architecture and approximates Bayesian inference with a prior
that is based on assumptions of simplicity and causal structures. The prior
contains a large space of structural causal models and Bayesian neural networks
with a bias for small architectures and thus low complexity. Furthermore, we
extend the PFN approach to differentiably calibrate the prior's hyperparameters
on real data. By doing so, we separate our abstract prior assumptions from
their heuristic calibration on real data. Afterwards, the calibrated
hyperparameters are fixed and TabPFN can be applied to any new tabular dataset
at the push of a button. Finally, on 30 datasets from the OpenML-CC18 suite we
show that our method outperforms boosted trees and performs on par with complex
state-of-the-art AutoML systems with predictions produced in less than a
second. We provide all our code and our final trained TabPFN in the
supplementary materials.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：PoF: Post-Training of Feature Extractor for Improving Generalization</b></summary>
  <p><b>编号</b>：[138]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01847</p>
  <p><b>作者</b>：Ikuro Sato,  Ryota Yamada,  Masayuki Tanaka,  Nakamasa Inoue,  Rei Kawakami</p>
  <p><b>备注</b>：Accepted to ICML2022. Contains a link to the code</p>
  <p><b>关键词</b>：local shape, Feature Extractor, intensively investigated, plays an important, important role</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>It has been intensively investigated that the local shape, especially
flatness, of the loss landscape near a minimum plays an important role for
generalization of deep models. We developed a training algorithm called PoF:
Post-Training of Feature Extractor that updates the feature extractor part of
an already-trained deep model to search a flatter minimum. The characteristics
are two-fold: 1) Feature extractor is trained under parameter perturbations in
the higher-layer parameter space, based on observations that suggest flattening
higher-layer parameter space, and 2) the perturbation range is determined in a
data-driven manner aiming to reduce a part of test loss caused by the positive
loss curvature. We provide a theoretical analysis that shows the proposed
algorithm implicitly reduces the target Hessian components as well as the loss.
Experimental results show that PoF improved model performance against baseline
methods on both CIFAR-10 and CIFAR-100 datasets for only 10-epoch
post-training, and on SVHN dataset for 50-epoch post-training. Source code is
available at: \url{this https URL</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Efficient Representation Learning via Adaptive Context Pooling</b></summary>
  <p><b>编号</b>：[140]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01844</p>
  <p><b>作者</b>：Chen Huang,  Walter Talbott,  Navdeep Jaitly,  Josh Susskind</p>
  <p><b>备注</b>：ICML 2022</p>
  <p><b>关键词</b>：Self-attention mechanisms model, Self-attention mechanisms, attention, input tokens, mechanisms model long-range</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Self-attention mechanisms model long-range context by using pairwise
attention between all input tokens. In doing so, they assume a fixed attention
granularity defined by the individual tokens (e.g., text characters or image
pixels), which may not be optimal for modeling complex dependencies at higher
levels. In this paper, we propose ContextPool to address this problem by
adapting the attention granularity for each token. Inspired by the success of
ConvNets that are combined with pooling to capture long-range dependencies, we
learn to pool neighboring features for each token before computing attention in
a given attention layer. The pooling weights and support size are adaptively
determined, allowing the pooled features to encode meaningful context with
varying scale. We show that ContextPool makes attention models more expressive,
achieving strong performance often with fewer layers and thus significantly
reduced cost. Experiments validate that our ContextPool module, when plugged
into transformer models, matches or surpasses state-of-the-art performance
using less compute on several language and image benchmarks, outperforms recent
works with learned context sizes or sparse attention patterns, and is also
applicable to ConvNets for efficient feature learning.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Randomized-to-Canonical Model Predictive Control for Real-world Visual  Robotic Manipulation</b></summary>
  <p><b>编号</b>：[144]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01840</p>
  <p><b>作者</b>：Tomoya Yamanokuchi,  Yuhwan Kwon,  Yoshihisa Tsurumine,  Eiji Uchibe,  Jun Morimoto,  Takamitsu Matsubara</p>
  <p><b>备注</b>：8 pages, Accepted by Robotics and Automation Letters</p>
  <p><b>关键词</b>：model predictive control, recently explored, predictive control, model predictive, visual model predictive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Many works have recently explored Sim-to-real transferable visual model
predictive control (MPC). However, such works are limited to one-shot transfer,
where real-world data must be collected once to perform the sim-to-real
transfer, which remains a significant human effort in transferring the models
learned in simulations to new domains in the real world. To alleviate this
problem, we first propose a novel model-learning framework called Kalman
Randomized-to-Canonical Model (KRC-model). This framework is capable of
extracting task-relevant intrinsic features and their dynamics from randomized
images. We then propose Kalman Randomized-to-Canonical Model Predictive Control
(KRC-MPC) as a zero-shot sim-to-real transferable visual MPC using KRC-model.
The effectiveness of our method is evaluated through a valve rotation task by a
robot hand in both simulation and the real world, and a block mating task in
simulation. The experimental results show that KRC-MPC can be applied to
various real domains and tasks in a zero-shot manner.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：What Do Graph Convolutional Neural Networks Learn?</b></summary>
  <p><b>编号</b>：[145]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01839</p>
  <p><b>作者</b>：Sannat Singh Bhasin,  Vaibhav Holani,  Divij Sanjanwala</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：machine learning tasks, numerous machine learning, neural networks, Graph neural networks, Convolutional Neural Networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Graph neural networks (GNNs) have gained traction over the past few years for
their superior performance in numerous machine learning tasks. Graph
Convolutional Neural Networks (GCN) are a common variant of GNNs that are known
to have high performance in semi-supervised node classification (SSNC), and
work well under the assumption of homophily. Recent literature has highlighted
that GCNs can achieve strong performance on heterophilous graphs under certain
"special conditions". These arguments motivate us to understand why, and how,
GCNs learn to perform SSNC. We find a positive correlation between similarity
of latent node embeddings of nodes within a class and the performance of a GCN.
Our investigation on underlying graph structures of a dataset finds that a
GCN's SSNC performance is significantly influenced by the consistency and
uniqueness in neighborhood structure of nodes within a class.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Ensemble feature selection with data-driven thresholding for Alzheimer's  disease biomarker discovery</b></summary>
  <p><b>编号</b>：[151]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01822</p>
  <p><b>作者</b>：Annette Spooner,  Gelareh Mohammadi,  Perminder S. Sachdev,  Henry Brodaty,  Arcot Sowmya (for the Sydney Memory and Ageing Study and the Alzheimer's Disease Neuroimaging Initiative)</p>
  <p><b>备注</b>：18 pages, 5 figures</p>
  <p><b>关键词</b>：Healthcare datasets present, missing information, present many challenges, machine learning, learning and statistics</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Healthcare datasets present many challenges to both machine learning and
statistics as their data are typically heterogeneous, censored,
high-dimensional and have missing information. Feature selection is often used
to identify the important features but can produce unstable results when
applied to high-dimensional data, selecting a different set of features on each
iteration.
The stability of feature selection can be improved with the use of feature
selection ensembles, which aggregate the results of multiple base feature
selectors. A threshold must be applied to the final aggregated feature set to
separate the relevant features from the redundant ones. A fixed threshold,
which is typically applied, offers no guarantee that the final set of selected
features contains only relevant features. This work develops several
data-driven thresholds to automatically identify the relevant features in an
ensemble feature selector and evaluates their predictive accuracy and
stability.
To demonstrate the applicability of these methods to clinical data, they are
applied to data from two real-world Alzheimer's disease (AD) studies. AD is a
progressive neurodegenerative disease with no known cure, that begins at least
2-3 decades before overt symptoms appear, presenting an opportunity for
researchers to identify early biomarkers that might identify patients at risk
of developing AD. Features identified by applying these methods to both
datasets reflect current findings in the AD literature.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Multimodal Frame-Scoring Transformer for Video Summarization</b></summary>
  <p><b>编号</b>：[154]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01814</p>
  <p><b>作者</b>：Jeiyoon Park,  Kiho Kwoun,  Chanhee Lee,  Heuiseok Lim</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：automatic video summarization, recent years, mushroomed in recent, generic video summarization, video summarization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As the number of video content has mushroomed in recent years, automatic
video summarization has come useful when we want to just peek at the content of
the video. However, there are two underlying limitations in generic video
summarization task. First, most previous approaches read in just visual
features as input, leaving other modality features behind. Second, existing
datasets for generic video summarization are relatively insufficient to train a
caption generator and multimodal feature extractors. To address these two
problems, this paper proposes the Multimodal Frame-Scoring Transformer (MFST)
framework exploiting visual, text and audio features and scoring a video with
respect to frames. Our MFST framework first extracts each modality features
(visual-text-audio) using pretrained encoders. Then, MFST trains the multimodal
frame-scoring transformer that uses video-text-audio representations as inputs
and predicts frame-level scores. Our extensive experiments with previous models
and ablation studies on TVSum and SumMe datasets demonstrate the effectiveness
and superiority of our proposed method.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Sedentary Behavior Estimation with Hip-worn Accelerometer Data:  Segmentation, Classification and Thresholding</b></summary>
  <p><b>编号</b>：[155]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01809</p>
  <p><b>作者</b>：Yiren Wang,  Fatima Tuz-Zahra,  Rong Zablocki,  Chongzhi Di,  Marta M. Jankowska,  John Bellettiere,  Jordan A. Carlson,  Andrea Z. LaCroix,  Sheri J. Hartman,  Dori E. Rosenberg,  Jingjing Zou,  Loki Natarajan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：sedentary behavior estimation, Cohort studies, studies are increasingly, increasingly using accelerometers, accelerometers for physical</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cohort studies are increasingly using accelerometers for physical activity
and sedentary behavior estimation. These devices tend to be less error-prone
than self-report, can capture activity throughout the day, and are economical.
However, previous methods for estimating sedentary behavior based on hip-worn
data are often invalid or suboptimal under free-living situations and
subject-to-subject variation. In this paper, we propose a local Markov
switching model that takes this situation into account, and introduce a general
procedure for posture classification and sedentary behavior analysis that fits
the model naturally. Our method features changepoint detection methods in time
series and also a two stage classification step that labels data into 3
classes(sitting, standing, stepping). Through a rigorous training-testing
paradigm, we showed that our approach achieves > 80% accuracy. In addition, our
method is robust and easy to interpret.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：GSMFlow: Generation Shifts Mitigating Flow for Generalized Zero-Shot  Learning</b></summary>
  <p><b>编号</b>：[161]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01798</p>
  <p><b>作者</b>：Zhi Chen,  Yadan Luo,  Ruihong Qiu,  Sen Wang,  Zi Huang,  Jingjing Li,  Zheng Zhang</p>
  <p><b>备注</b>：IEEE Transactions on Multimedia 2022. arXiv admin note: substantial text overlap with arXiv:2107.03163</p>
  <p><b>关键词</b>：Generalized Zero-Shot Learning, transferring semantic knowledge, unseen classes, Generalized Zero-Shot, aims to recognize</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generalized Zero-Shot Learning (GZSL) aims to recognize images from both the
seen and unseen classes by transferring semantic knowledge from seen to unseen
classes. It is a promising solution to take the advantage of generative models
to hallucinate realistic unseen samples based on the knowledge learned from the
seen classes. However, due to the generation shifts, the synthesized samples by
most existing methods may drift from the real distribution of the unseen data.
To address this issue, we propose a novel flow-based generative framework that
consists of multiple conditional affine coupling layers for learning unseen
data generation. Specifically, we discover and address three potential problems
that trigger the generation shifts, i.e., semantic inconsistency, variance
collapse, and structure disorder. First, to enhance the reflection of the
semantic information in the generated samples, we explicitly embed the semantic
information into the transformation in each conditional affine coupling layer.
Second, to recover the intrinsic variance of the real unseen features, we
introduce a boundary sample mining strategy with entropy maximization to
discover more difficult visual variants of semantic prototypes and hereby
adjust the decision boundary of the classifiers. Third, a relative positioning
strategy is proposed to revise the attribute embeddings, guiding them to fully
preserve the inter-class geometric structure and further avoid structure
disorder in the semantic space. Extensive experimental results on four GZSL
benchmark datasets demonstrate that GSMFlow achieves the state-of-the-art
performance on GZSL.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Task-agnostic Defense against Adversarial Patch Attacks</b></summary>
  <p><b>编号</b>：[164]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01795</p>
  <p><b>作者</b>：Ke Xu,  Yao Xiao,  Zhaoheng Zheng,  Kaijie Cai,  Ram Nevatia</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：mislead neural networks, attacks mislead neural, designated local region, patch attacks mislead, mislead neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Adversarial patch attacks mislead neural networks by injecting adversarial
pixels within a designated local region. Patch attacks can be highly effective
in a variety of tasks and physically realizable via attachment (e.g. a sticker)
to the real-world objects. Despite the diversity in attack patterns,
adversarial patches tend to be highly textured and different in appearance from
natural images. We exploit this property and present PatchZero, a task-agnostic
defense against white-box adversarial patches. Specifically, our defense
detects the adversarial pixels and "zeros out" the patch region by repainting
with mean pixel values. We formulate the patch detection problem as a semantic
segmentation task such that our model can generalize to patches of any size and
shape. We further design a two-stage adversarial training scheme to defend
against the stronger adaptive attacks. We thoroughly evaluate PatchZero on the
image classification (ImageNet, RESISC45), object detection (PASCAL VOC), and
video classification (UCF101) datasets. Our method achieves SOTA robust
accuracy without any degradation in the benign performance.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Features Based Adaptive Augmentation for Graph Contrastive Learning</b></summary>
  <p><b>编号</b>：[166]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01792</p>
  <p><b>作者</b>：Adnan Ali (1),  Jinlong Li (2) ((1) University of Science and Technology of China, (2) University of Science and Technology of China)</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Self-Supervised learning aims, graph contrastive learning, data-data pairs, aims to eliminate, expensive annotation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Self-Supervised learning aims to eliminate the need for expensive annotation
in graph representation learning, where graph contrastive learning (GCL) is
trained with the self-supervision signals containing data-data pairs. These
data-data pairs are generated with augmentation employing stochastic functions
on the original graph. We argue that some features can be more critical than
others depending on the downstream task, and applying stochastic function
uniformly, will vandalize the influential features, leading to diminished
accuracy. To fix this issue, we introduce a Feature Based Adaptive Augmentation
(FebAA) approach, which identifies and preserves potentially influential
features and corrupts the remaining ones. We implement FebAA as plug and play
layer and use it with state-of-the-art Deep Graph Contrastive Learning (GRACE)
and Bootstrapped Graph Latents (BGRL). We successfully improved the accuracy of
GRACE and BGRL on eight graph representation learning's benchmark datasets.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：A Unified Meta-Learning Framework for Dynamic Transfer Learning</b></summary>
  <p><b>编号</b>：[170]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01784</p>
  <p><b>作者</b>：Jun Wu,  Jingrui He</p>
  <p><b>备注</b>：Accepted by IJCAI-2022</p>
  <p><b>关键词</b>：target task, dynamic tasks, tasks, Transfer learning refers, relevant source task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transfer learning refers to the transfer of knowledge or information from a
relevant source task to a target task. However, most existing works assume both
tasks are sampled from a stationary task distribution, thereby leading to the
sub-optimal performance for dynamic tasks drawn from a non-stationary task
distribution in real scenarios. To bridge this gap, in this paper, we study a
more realistic and challenging transfer learning setting with dynamic tasks,
i.e., source and target tasks are continuously evolving over time. We
theoretically show that the expected error on the dynamic target task can be
tightly bounded in terms of source knowledge and consecutive distribution
discrepancy across tasks. This result motivates us to propose a generic
meta-learning framework L2E for modeling the knowledge transferability on
dynamic tasks. It is centered around a task-guided meta-learning problem with a
group of meta-pairs of tasks, based on which we are able to learn the prior
model initialization for fast adaptation on the newest target task. L2E enjoys
the following properties: (1) effective knowledge transferability across
dynamic tasks; (2) fast adaptation to the new target task; (3) mitigation of
catastrophic forgetting on historical target tasks; and (4) flexibility in
incorporating any existing static transfer learning algorithms. Extensive
experiments on various image data sets demonstrate the effectiveness of the
proposed L2E framework.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：On A Mallows-type Model For (Ranked) Choices</b></summary>
  <p><b>编号</b>：[171]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01783</p>
  <p><b>作者</b>：Yifan Feng,  Yuxuan Tang</p>
  <p><b>备注</b>：26 pages. This paper was presented in part at the 2022 INFORMS Revenue Management and Pricing Section Conference (RMP 2022). It has been submitted to the Thirty-Sixth Annual Conference on Neural Information Processing Systems (NeurIPS 2022)</p>
  <p><b>关键词</b>：preference learning setting, learning setting, chooses an ordered, ordered list, preferred items</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In a preference learning setting, every participant chooses an ordered list
of $k$ most preferred items among a displayed set of candidates. (The set can
be different for every participant.) We identify a distance-based ranking model
for the population's preferences and their (ranked) choice behavior. The
ranking model resembles the Mallows model but uses a new distance function
called Reverse Major Index (RMJ). We find that despite the need to sum over all
permutations, the RMJ-based ranking distribution aggregates into (ranked)
choice probabilities with simple closed-form expression. We develop effective
methods to estimate the model parameters and showcase their generalization
power using real data, especially when there is a limited variety of display
sets.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：CodeRL: Mastering Code Generation through Pretrained Models and Deep  Reinforcement Learning</b></summary>
  <p><b>编号</b>：[172]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01780</p>
  <p><b>作者</b>：Hung Le,  Yue Wang,  Akhilesh Deepak Gotmare,  Silvio Savarese,  Steven C.H. Hoi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：code generation aims, aims to generate, code generation, SOTA results, generation aims</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Program synthesis or code generation aims to generate a program that
satisfies a problem specification. Recent approaches using large-scale
pretrained language models (LMs) have shown promising results, yet they have
some critical limitations. In particular, they often follow a standard
supervised fine-tuning procedure to train a code generation model only from the
pairs of natural-language problem descriptions and ground-truth programs. Such
paradigm largely ignores some important but potentially useful signals in the
problem specification such as unit tests, which thus often results in poor
performance when solving complex unseen coding tasks. To address the
limitations, we propose "CodeRL", a new framework for program synthesis tasks
through pretrained LMs and deep reinforcement learning (RL). Specifically,
during training, we treat the code-generating LM as an actor network, and
introduce a critic network that is trained to predict the functional
correctness of generated programs and provide dense feedback signals to the
actor. During inference, we introduce a new generation procedure with a
critical sampling strategy that allows a model to automatically regenerate
programs based on feedback from example unit tests and critic scores. For the
model backbones, we extended the encoder-decoder architecture of CodeT5 with
enhanced learning objectives, larger model sizes, and better pretraining data.
Our method not only achieves new SOTA results on the challenging APPS
benchmark, but also shows strong zero-shot transfer capability with new SOTA
results on the simpler MBPP benchmark.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：Approximating Discontinuous Nash Equilibrial Values of Two-Player  General-Sum Differential Games</b></summary>
  <p><b>编号</b>：[176]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01773</p>
  <p><b>作者</b>：Lei Zhang,  Mukesh Ghimire,  Wenlong Zhang,  Zhe Xu,  Yi Ren</p>
  <p><b>备注</b>：Submitted to CoRL 2022</p>
  <p><b>关键词</b>：Finding Nash equilibrial, two-player differential games, Nash equilibrial policies, texttt, differential games requires</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Finding Nash equilibrial policies for two-player differential games requires
solving Hamilton-Jacobi-Isaacs PDEs. Recent studies achieved success in
circumventing the curse of dimensionality in solving such PDEs with underlying
applications to human-robot interactions (HRI), by adopting self-supervised
(physics-informed) neural networks as universal value approximators. This paper
extends from previous SOTA on zero-sum games with continuous values to
general-sum games with discontinuous values, where the discontinuity is caused
by that of the players' losses. We show that due to its lack of convergence
proof and generalization analysis on discontinuous losses, the existing
self-supervised learning technique fails to generalize and raises safety
concerns in an autonomous driving application. Our solution is to first
pre-train the value network on supervised Nash equilibria, and then refine it
by minimizing a loss that combines the supervised data with the PDE and
boundary conditions. Importantly, the demonstrated advantage of the proposed
learning method against purely supervised and self-supervised approaches
requires careful choice of the neural activation function: Among
$\texttt{relu}$, $\texttt{sin}$, and $\texttt{tanh}$, we show that
$\texttt{tanh}$ is the only choice that achieves optimal generalization and
safety performance. Our conjecture is that $\texttt{tanh}$ (similar to
$\texttt{sin}$) allows continuity of value and its gradient, which is
sufficient for the convergence of learning, and at the same time is expressive
enough (similar to $\texttt{relu}$) at approximating discontinuous value
landscapes. Lastly, we apply our method to approximating control policies for
an incomplete-information interaction and demonstrate its contribution to safe
interactions.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：A Generative Framework for Personalized Learning and Estimation: Theory,  Algorithms, and Privacy</b></summary>
  <p><b>编号</b>：[178]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01771</p>
  <p><b>作者</b>：Kaan Ozkara,  Antonious M. Girgis,  Deepesh Data,  Suhas Diggavi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：client data, distinguishing characteristic, characteristic of federated, personalized, statistical heterogeneity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A distinguishing characteristic of federated learning is that the (local)
client data could have statistical heterogeneity. This heterogeneity has
motivated the design of personalized learning, where individual (personalized)
models are trained, through collaboration. There have been various
personalization methods proposed in literature, with seemingly very different
forms and methods ranging from use of a single global model for local
regularization and model interpolation, to use of multiple global models for
personalized clustering, etc. In this work, we begin with a generative
framework that could potentially unify several different algorithms as well as
suggest new algorithms. We apply our generative framework to personalized
estimation, and connect it to the classical empirical Bayes' methodology. We
develop private personalized estimation under this framework. We then use our
generative framework for learning, which unifies several known personalized FL
algorithms and also suggests new ones; we propose and study a new algorithm
AdaPeD based on a Knowledge Distillation, which numerically outperforms several
known algorithms. We also develop privacy for personalized learning methods
with guarantees for user-level privacy and composition. We numerically evaluate
the performance as well as the privacy for both the estimation and learning
problems, demonstrating the advantages of our proposed methods.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：opPINN: Physics-Informed Neural Network with operator learning to  approximate solutions to the Fokker-Planck-Landau equation</b></summary>
  <p><b>编号</b>：[181]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01765</p>
  <p><b>作者</b>：Jae Yong Lee,  Juhi Jang,  Hyung Ju Hwang</p>
  <p><b>备注</b>：28 pages, 12 figures</p>
  <p><b>关键词</b>：FPL equation, operator surrogate models, FPL, hybrid framework opPINN, surrogate models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a hybrid framework opPINN: physics-informed neural network (PINN)
with operator learning for approximating the solution to the
Fokker-Planck-Landau (FPL) equation. The opPINN framework is divided into two
steps: Step 1 and Step 2. After the operator surrogate models are trained
during Step 1, PINN can effectively approximate the solution to the FPL
equation during Step 2 by using the pre-trained surrogate models. The operator
surrogate models greatly reduce the computational cost and boost PINN by
approximating the complex Landau collision integral in the FPL equation. The
operator surrogate models can also be combined with the traditional numerical
schemes. It provides a high efficiency in computational time when the number of
velocity modes becomes larger. Using the opPINN framework, we provide the
neural network solutions for the FPL equation under the various types of
initial conditions, and interaction models in two and three dimensions.
Furthermore, based on the theoretical properties of the FPL equation, we show
that the approximated neural network solution converges to the a priori
classical solution of the FPL equation as the pre-defined loss function is
reduced.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：TT-PINN: A Tensor-Compressed Neural PDE Solver for Edge Computing</b></summary>
  <p><b>编号</b>：[189]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01751</p>
  <p><b>作者</b>：Ziyue Liu,  Xinling Yu,  Zheng Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：complex physics systems, modeling complex physics, Physics-informed neural networks, increasingly employed due, Physics-informed neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Physics-informed neural networks (PINNs) have been increasingly employed due
to their capability of modeling complex physics systems. To achieve better
expressiveness, increasingly larger network sizes are required in many
problems. This has caused challenges when we need to train PINNs on edge
devices with limited memory, computing and energy resources. To enable training
PINNs on edge devices, this paper proposes an end-to-end compressed PINN based
on Tensor-Train decomposition. In solving a Helmholtz equation, our proposed
model significantly outperforms the original PINNs with few parameters and
achieves satisfactory prediction with up to 15$\times$ overall parameter
reduction.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Federated Split GANs</b></summary>
  <p><b>编号</b>：[190]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01750</p>
  <p><b>作者</b>：Pranvera Kortoçi,  Yilei Liang,  Pengyuan Zhou,  Lik-Hang Lee,  Abbas Mehrabi,  Pan Hui,  Sasu Tarkoma,  Jon Crowcroft</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：based applications, immense amount, amount and variety, generate are key, key enablers</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Mobile devices and the immense amount and variety of data they generate are
key enablers of machine learning (ML)-based applications. Traditional ML
techniques have shifted toward new paradigms such as federated (FL) and split
learning (SL) to improve the protection of user's data privacy. However, these
paradigms often rely on server(s) located in the edge or cloud to train
computationally-heavy parts of a ML model to avoid draining the limited
resource on client devices, resulting in exposing device data to such third
parties. This work proposes an alternative approach to train
computationally-heavy ML models in user's devices themselves, where
corresponding device data resides. Specifically, we focus on GANs (generative
adversarial networks) and leverage their inherent privacy-preserving attribute.
We train the discriminative part of a GAN with raw data on user's devices,
whereas the generative model is trained remotely (e.g., server) for which there
is no need to access sensor true data. Moreover, our approach ensures that the
computational load of training the discriminative model is shared among user's
devices-proportional to their computation capabilities-by means of SL. We
implement our proposed collaborative training scheme of a computationally-heavy
GAN model in real resource-constrained devices. The results show that our
system preserves data privacy, keeps a short training time, and yields same
accuracy of model training in unconstrained devices (e.g., cloud). Our code can
be found on this https URL</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：Discrete Tree Flows via Tree-Structured Permutations</b></summary>
  <p><b>编号</b>：[193]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01744</p>
  <p><b>作者</b>：Mai Elkady,  Jim Lim,  David I. Inouye</p>
  <p><b>备注</b>：Accepted to ICML 2022</p>
  <p><b>关键词</b>：extensively researched, recently been explored, discrete, normalizing flows, data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While normalizing flows for continuous data have been extensively researched,
flows for discrete data have only recently been explored. These prior models,
however, suffer from limitations that are distinct from those of continuous
flows. Most notably, discrete flow-based models cannot be straightforwardly
optimized with conventional deep learning methods because gradients of discrete
functions are undefined or zero. Previous works approximate pseudo-gradients of
the discrete functions but do not solve the problem on a fundamental level. In
addition to that, backpropagation can be computationally burdensome compared to
alternative discrete algorithms such as decision tree algorithms. Our approach
seeks to reduce computational burden and remove the need for pseudo-gradients
by developing a discrete flow based on decision trees -- building upon the
success of efficient tree-based methods for classification and regression for
discrete data. We first define a tree-structured permutation (TSP) that
compactly encodes a permutation of discrete data where the inverse is easy to
compute; thus, we can efficiently compute the density value and sample new
data. We then propose a decision tree algorithm to build TSPs that learns the
tree structure and permutations at each node via novel criteria. We empirically
demonstrate the feasibility of our method on multiple datasets.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：Anomaly-aware multiple instance learning for rare anemia disorder  classification</b></summary>
  <p><b>编号</b>：[194]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01742</p>
  <p><b>作者</b>：Salome Kazeminia,  Ario Sadafi,  Asya Makhro,  Anna Bogdanova,  Shadi Albarqouni,  Carsten Marr</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Deep learning-based classification, Deep learning-based, instance-level annotations, training data, rare anemia</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning-based classification of rare anemia disorders is challenged by
the lack of training data and instance-level annotations. Multiple Instance
Learning (MIL) has shown to be an effective solution, yet it suffers from low
accuracy and limited explainability. Although the inclusion of attention
mechanisms has addressed these issues, their effectiveness highly depends on
the amount and diversity of cells in the training samples. Consequently, the
poor machine learning performance on rare anemia disorder classification from
blood samples remains unresolved. In this paper, we propose an interpretable
pooling method for MIL to address these limitations. By benefiting from
instance-level information of negative bags (i.e., homogeneous benign cells
from healthy individuals), our approach increases the contribution of anomalous
instances. We show that our strategy outperforms standard MIL classification
algorithms and provides a meaningful explanation behind its decisions.
Moreover, it can denote anomalous instances of rare blood diseases that are not
seen during the training phase.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Machine Learning in Access Control: A Taxonomy and Survey</b></summary>
  <p><b>编号</b>：[195]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01739</p>
  <p><b>作者</b>：Mohammad Nur Nobi,  Maanak Gupta,  Lopamudra Praharaj,  Mahmoud Abdelsalam,  Ram Krishnan,  Ravi Sandhu</p>
  <p><b>备注</b>：Submitted to ACM Computing Survey</p>
  <p><b>关键词</b>：exploiting machine learning, policy mining, policy verification, access control attributes, machine learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>An increasing body of work has recognized the importance of exploiting
machine learning (ML) advancements to address the need for efficient automation
in extracting access control attributes, policy mining, policy verification,
access decisions, etc. In this work, we survey and summarize various ML
approaches to solve different access control problems. We propose a novel
taxonomy of the ML model's application in the access control domain. We
highlight current limitations and open challenges such as lack of public
real-world datasets, administration of ML-based access control systems,
understanding a black-box ML model's decision, etc., and enumerate future
research directions.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：How Much More Data Do I Need? Estimating Requirements for Downstream  Tasks</b></summary>
  <p><b>编号</b>：[201]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01725</p>
  <p><b>作者</b>：Rafid Mahmood,  James Lucas,  David Acuna,  Daiqing Li,  Jonah Philion,  Jose M. Alvarez,  Zhiding Yu,  Sanja Fidler,  Marc T. Law</p>
  <p><b>备注</b>：Accepted to CVPR 2022</p>
  <p><b>关键词</b>：small training data, small training, data, training data set, data set</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Given a small training data set and a learning algorithm, how much more data
is necessary to reach a target validation or test performance? This question is
of critical importance in applications such as autonomous driving or medical
imaging where collecting data is expensive and time-consuming. Overestimating
or underestimating data requirements incurs substantial costs that could be
avoided with an adequate budget. Prior work on neural scaling laws suggest that
the power-law function can fit the validation performance curve and extrapolate
it to larger data set sizes. We find that this does not immediately translate
to the more difficult downstream task of estimating the required data set size
to meet a target performance. In this work, we consider a broad class of
computer vision tasks and systematically investigate a family of functions that
generalize the power-law function to allow for better estimation of data
requirements. Finally, we show that incorporating a tuned correction factor and
collecting over multiple rounds significantly improves the performance of the
data estimators. Using our guidelines, practitioners can accurately estimate
data requirements of machine learning systems to gain savings in both
development time and data acquisition costs.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：A Causal Approach for Business Optimization: Application on an Online  Marketplace</b></summary>
  <p><b>编号</b>：[203]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01722</p>
  <p><b>作者</b>：Naama Parush,  Ohad Levinkron-Fisch,  Hanan Shteingart,  Amir Bar Sela,  Amir Zilberman,  Jake Klein</p>
  <p><b>备注</b>：6 pages, 2 figures, 2 supplementary pages</p>
  <p><b>关键词</b>：common sales strategy, sales strategy involves, account executives, actively reach, common sales</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A common sales strategy involves having account executives (AEs) actively
reach out and contact potential customers. However, not all contact attempts
have a positive effect: some attempts do not change customer decisions, while
others might even interfere with the desired outcome. In this work we propose
using causal inference to estimate the effect of contacting each potential
customer and setting the contact policy accordingly. We demonstrate this
approach on data from this http URL, an online jewelry marketplace. We examined
the Worthy business process to identify relevant decisions and outcomes, and
formalized assumptions on how they were made. Using causal tools, we selected a
decision point where improving AE contact activity appeared to be promising. We
then generated a personalized policy and recommended reaching out only to
customers for whom it would be beneficial. Finally, we validated the results in
an A\B test over a 3-month period, resulting in an increase in item delivery
rate of the targeted population by 22% (p-value=0.026). This policy is now
being used on an ongoing basis.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：Do Not Take It for Granted: Comparing Open-Source Libraries for Software  Development Effort Estimation</b></summary>
  <p><b>编号</b>：[208]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01705</p>
  <p><b>作者</b>：Rebecca Moussa,  Federica Sarro</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Machine Learning, predictive Software Engineering, past two decades, libraries, Software Engineering</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the past two decades, several Machine Learning (ML) libraries have become
freely available. Many studies have used such libraries to carry out empirical
investigations on predictive Software Engineering (SE) tasks. However, the
differences stemming from using one library over another have been overlooked,
implicitly assuming that using any of these libraries would provide the user
with the same or very similar results. This paper aims at raising awareness of
the differences incurred when using different ML libraries for software
development effort estimation (SEE), one of most widely studied SE prediction
tasks. To this end, we investigate 4 deterministic machine learners as provided
by 3 of the most popular ML open-source libraries written in different
languages (namely, Scikit-Learn, Caret and Weka). We carry out a thorough
empirical study comparing the performance of the machine learners on 5 SEE
datasets in the two most common SEE scenarios (i.e., out-of-the-box-ml and
tuned-ml) as well as an in-depth analysis of the documentation and code of
their APIs. The results of our study reveal that the predictions provided by
the 3 libraries differ in 95% of the cases on average across a total of 105
cases studied. These differences are significantly large in most cases and
yield misestimations of up to approx. 3,000 hours per project. Moreover, our
API analysis reveals that these libraries provide the user with different
levels of control on the parameters one can manipulate, and a lack of clarity
and consistency, overall, which might mislead users. Our findings highlight
that the ML library is an important design choice for SEE studies, which can
lead to a difference in performance. However, such a difference is
under-documented. We conclude by highlighting open-challenges with suggestions
for the developers of libraries as well as for the researchers and
practitioners using them.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：An adaptive music generation architecture for games based on the deep  learning Transformer mode</b></summary>
  <p><b>编号</b>：[210]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01698</p>
  <p><b>作者</b>：Gustavo Amaral Costa dos Santos,  Augusto Baffa,  Jean-Pierre Briot,  Bruno Feijó,  Antonio Luz Furtado</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Transformer deep learning, deep learning model, video games based, Transformer deep, video game music</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents an architecture for generating music for video games
based on the Transformer deep learning model. The system generates music in
various layers, following the standard layering strategy currently used by
composers designing video game music. The music is adaptive to the
psychological context of the player, according to the arousal-valence model.
Our motivation is to customize music according to the player's tastes, who can
select his preferred style of music through a set of training examples of
music. We discuss current limitations and prospects for the future, such as
collaborative and interactive control of the musical components.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：An Approximation Method for Fitted Random Forests</b></summary>
  <p><b>编号</b>：[224]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02184</p>
  <p><b>作者</b>：Sai K Popuri</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Random Forests model, popular machine learning, Random Forests, machine learning method, Forests model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Random Forests (RF) is a popular machine learning method for classification
and regression problems. It involves a bagging application to decision tree
models. One of the primary advantages of the Random Forests model is the
reduction in the variance of the forecast. In large scale applications of the
model with millions of data points and hundreds of features, the size of the
fitted objects can get very large and reach the limits on the available space
in production setups, depending on the number and depth of the trees. This
could be especially challenging when trained models need to be downloaded
on-demand to small devices with limited memory. There is a need to approximate
the trained RF models to significantly reduce the model size without losing too
much of prediction accuracy. In this project we study methods that approximate
each fitted tree in the Random Forests model using the multinomial allocation
of the data points to the leafs. Specifically, we begin by studying whether
fitting a multinomial logistic regression (and subsequently, a generalized
additive model (GAM) extension) to the output of each tree helps reduce the
size while preserving the prediction quality.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：Path Integral Stochastic Optimal Control for Sampling Transition Paths</b></summary>
  <p><b>编号</b>：[226]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02149</p>
  <p><b>作者</b>：Lars Holdijk,  Yuanqi Du,  Ferry Hooft,  Priyank Jaini,  Bernd Ensing,  Max Welling</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Sampling Transition Paths, Transition Paths, Sampling Transition, Transition, called Collective Variables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider the problem of Sampling Transition Paths. Given two metastable
conformational states of a molecular system, eg. a folded and unfolded protein,
we aim to sample the most likely transition path between the two states.
Sampling such a transition path is computationally expensive due to the
existence of high free energy barriers between the two states. To circumvent
this, previous work has focused on simplifying the trajectories to occur along
specific molecular descriptors called Collective Variables (CVs). However,
finding CVs is not trivial and requires chemical intuition. For larger
molecules, where intuition is not sufficient, using these CV-based methods
biases the transition along possibly irrelevant dimensions. Instead, this work
proposes a method for sampling transition paths that consider the entire
geometry of the molecules. To achieve this, we first relate the problem to
recent work on the Schrodinger bridge problem and stochastic optimal control.
Using this relation, we construct a method that takes into account important
characteristics of molecular systems such as second-order dynamics and
invariance to rotations and translations. We demonstrate our method on the
commonly studied Alanine Dipeptide, but also consider larger proteins such as
Polyproline and Chignolin.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：Balancing Profit, Risk, and Sustainability for Portfolio Management</b></summary>
  <p><b>编号</b>：[227]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02134</p>
  <p><b>作者</b>：Charl Maree,  Christian W. Omlin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：reallocation of funds, Stock portfolio optimization, risk and sustainability, selection of stocks, Stock portfolio</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Stock portfolio optimization is the process of continuous reallocation of
funds to a selection of stocks. This is a particularly well-suited problem for
reinforcement learning, as daily rewards are compounding and objective
functions may include more than just profit, e.g., risk and sustainability. We
developed a novel utility function with the Sharpe ratio representing risk and
the environmental, social, and governance score (ESG) representing
sustainability. We show that a state-of-the-art policy gradient method -
multi-agent deep deterministic policy gradients (MADDPG) - fails to find the
optimum policy due to flat policy gradients and we therefore replaced gradient
descent with a genetic algorithm for parameter optimization. We show that our
system outperforms MADDPG while improving on deep Q-learning approaches by
allowing for continuous action spaces. Crucially, by incorporating risk and
sustainability criteria in the utility function, we improve on the
state-of-the-art in reinforcement learning for portfolio optimization; risk and
sustainability are essential in any modern trading strategy and we propose a
system that does not merely report these metrics, but that actively optimizes
the portfolio to improve on them.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：Bayesian NVH metamodels to assess interior cabin noise using measurement  databases</b></summary>
  <p><b>编号</b>：[228]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02120</p>
  <p><b>作者</b>：V. Prakash,  O. Sauvage,  J. Antoni,  L. Gagliardini</p>
  <p><b>备注</b>：ISMA 2022 conference paper</p>
  <p><b>关键词</b>：recent years, great emphasis, put on engineering, engineering the acoustic, acoustic signature</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, a great emphasis has been put on engineering the acoustic
signature of vehicles that represents the overall comfort level for passengers.
Due to highly uncertain behavior of production cars, probabilistic metamodels
or surrogates can be useful to estimate the NVH dispersion and assess different
NVH risks. These metamodels follow physical behaviors and shall aid as a design
space exploration tool during the early stage design process to support the NVH
optimization. The measurement databases constitute different noise paths such
as aerodynamic noise (wind-tunnel test), tire-pavement interaction noise
(rolling noise), and noise due to electric motors (whining noise). This
research work proposes a global NVH metamodeling technique for broadband noises
such as aerodynamic and rolling noises exploiting the Bayesian framework that
takes into account the prior (domain-expert) knowledge about complex physical
mechanisms. Generalized additive models (GAMs) with polynomials and Gaussian
basis functions are used to model the dependency of sound pressure level (SPL)
on predictor variables. Moreover, parametric bootstrap algorithm based on
data-generating mechanism using the point estimates is used to estimate the
dispersion in unknown parameters. Probabilistic modelling is carried out using
an open-source library PyMC3 that utilizes No-U-Turn sampler (NUTS) and the
developed models are validated using Cross-Validation technique.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：Correlation between entropy and generalizability in a neural network</b></summary>
  <p><b>编号</b>：[235]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01996</p>
  <p><b>作者</b>：Ge Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：complex machine-learning problems, Wang-Landau Mote Carlo, Mote Carlo algorithm, machine-learning problems, fully understood</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Although neural networks can solve very complex machine-learning problems,
the theoretical reason for their generalizability is still not fully
understood. Here we use Wang-Landau Mote Carlo algorithm to calculate the
entropy (logarithm of the volume of a part of the parameter space) at a given
test accuracy, and a given training loss function value or training accuracy.
Our results show that entropical forces help generalizability. Although our
study is on a very simple application of neural networks (a spiral dataset and
a small, fully-connected neural network), our approach should be useful in
explaining the generalizability of more complicated neural networks in future
works.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：Vector Quantisation for Robust Segmentation</b></summary>
  <p><b>编号</b>：[238]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01919</p>
  <p><b>作者</b>：Ainkaran Santhirasekaram,  Avinash Kori,  Mathias Winkler,  Andrea Rockall,  Ben Glocker</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：medical domain depends, domain depends, robustness, medical imaging exhibiting, medical domain</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The reliability of segmentation models in the medical domain depends on the
model's robustness to perturbations in the input space. Robustness is a
particular challenge in medical imaging exhibiting various sources of image
noise, corruptions, and domain shifts. Obtaining robustness is often attempted
via simulating heterogeneous environments, either heuristically in the form of
data augmentation or by learning to generate specific perturbations in an
adversarial manner. We propose and justify that learning a discrete
representation in a low dimensional embedding space improves robustness of a
segmentation model. This is achieved with a dictionary learning method called
vector quantisation. We use a set of experiments designed to analyse robustness
in both the latent and output space under domain shift and noise perturbations
in the input space. We adapt the popular UNet architecture, inserting a
quantisation block in the bottleneck. We demonstrate improved segmentation
accuracy and better robustness on three segmentation tasks. Code is available
at
\url{this https URL}</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：Deriving Surface Resistivity from Polarimetric SAR Data Using Dual-Input  UNet</b></summary>
  <p><b>编号</b>：[242]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01811</p>
  <p><b>作者</b>：Bibin Wilson,  Rajiv Kumar,  Narayanarao Bhogapurapu,  Anand Singh,  Amit Sethi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Synthetic Aperture Radar, labor intensive, SAR data, Coso Geothermal Area, deep learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Traditional survey methods for finding surface resistivity are time-consuming
and labor intensive. Very few studies have focused on finding the
resistivity/conductivity using remote sensing data and deep learning
techniques. In this line of work, we assessed the correlation between surface
resistivity and Synthetic Aperture Radar (SAR) by applying various deep
learning methods and tested our hypothesis in the Coso Geothermal Area, USA.
For detecting the resistivity, L-band full polarimetric SAR data acquired by
UAVSAR were used, and MT (Magnetotellurics) inverted resistivity data of the
area were used as the ground truth. We conducted experiments to compare various
deep learning architectures and suggest the use of Dual Input UNet (DI-UNet)
architecture. DI-UNet uses a deep learning architecture to predict the
resistivity using full polarimetric SAR data by promising a quick survey
addition to the traditional method. Our proposed approach accomplished improved
outcomes for the mapping of MT resistivity from SAR data.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：Improved Global Guarantees for the Nonconvex Burer--Monteiro  Factorization via Rank Overparameterization</b></summary>
  <p><b>编号</b>：[245]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01789</p>
  <p><b>作者</b>：Richard Y. Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：positive semidefinite matrix, strongly convex objective, star, minimizing a twice-differentiable, semidefinite matrix</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider minimizing a twice-differentiable, $L$-smooth, and $\mu$-strongly
convex objective $\phi$ over an $n\times n$ positive semidefinite matrix
$M\succeq0$, under the assumption that the minimizer $M^{\star}$ has low rank
$r^{\star}\ll n$. Following the Burer--Monteiro approach, we instead minimize
the nonconvex objective $f(X)=\phi(XX^{T})$ over a factor matrix $X$ of size
$n\times r$. This substantially reduces the number of variables from $O(n^{2})$
to as few as $O(n)$ and also enforces positive semidefiniteness for free, but
at the cost of giving up the convexity of the original problem. In this paper,
we prove that if the search rank $r\ge r^{\star}$ is overparameterized by a
constant factor with respect to the true rank $r^{\star}$, namely as in
$r>\frac{1}{4}(L/\mu-1)^{2}r^{\star}$, then despite nonconvexity, local
optimization is guaranteed to globally converge from any initial point to the
global optimum. This significantly improves upon a previous rank
overparameterization threshold of $r\ge n$, which is known to be sharp if
$\phi$ is allowed to be nonsmooth and/or non-strongly convex, but would
increase the number of variables back up to $O(n^{2})$. Conversely, without
rank overparameterization, we prove that such a global guarantee is possible if
and only if $\phi$ is almost perfectly conditioned, with a condition number of
$L/\mu<3$. therefore, we conclude that a small amount of overparameterization can lead to large improvements in theoretical guarantees for the nonconvex burer--monteiro factorization.< p>
  </3$.></p></details>
</details>
<details>
  <summary>76. <b>标题：FACT: High-Dimensional Random Forests Inference</b></summary>
  <p><b>编号</b>：[249]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01678</p>
  <p><b>作者</b>：Chien-Ming Chi,  Yingying Fan,  Jinchi Lv</p>
  <p><b>备注</b>：81 pages, 5 figures</p>
  <p><b>关键词</b>：outstanding empirical performance, Random forests, random forests learning, Random, forests</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Random forests is one of the most widely used machine learning methods over
the past decade thanks to its outstanding empirical performance. Yet, because
of its black-box nature, the results by random forests can be hard to interpret
in many big data applications. Quantifying the usefulness of individual
features in random forests learning can greatly enhance its interpretability.
Existing studies have shown that some popularly used feature importance
measures for random forests suffer from the bias issue. In addition, there lack
comprehensive size and power analyses for most of these existing methods. In
this paper, we approach the problem via hypothesis testing, and suggest a
framework of the self-normalized feature-residual correlation test (FACT) for
evaluating the significance of a given feature in the random forests model with
bias-resistance property, where our null hypothesis concerns whether the
feature is conditionally independent of the response given all other features.
Such an endeavor on random forests inference is empowered by some recent
developments on high-dimensional random forests consistency. The vanilla
version of our FACT test can suffer from the bias issue in the presence of
feature dependency. We exploit the techniques of imbalancing and conditioning
for bias correction. We further incorporate the ensemble idea into the FACT
statistic through feature transformations for the enhanced power. Under a
fairly general high-dimensional nonparametric model setting with dependent
features, we formally establish that FACT can provide theoretically justified
random forests feature p-values and enjoy appealing power through nonasymptotic
analyses. The theoretical results and finite-sample advantages of the newly
suggested method are illustrated with several simulation examples and an
economic forecasting application in relation to COVID-19.</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：Slice-by-slice deep learning aided oropharyngeal cancer segmentation  with adaptive thresholding for spatial uncertainty on FDG PET and CT images</b></summary>
  <p><b>编号</b>：[251]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01623</p>
  <p><b>作者</b>：Alessia De Biase,  Nanna Maria Sijtsema,  Lisanne van Dijk,  Johannes A. Langendijk,  Peter van Ooijen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：radiotherapy treatment planning, registered FDG PET, FDG PET, treatment planning, fundamental step</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Tumor segmentation is a fundamental step for radiotherapy treatment planning.
To define an accurate segmentation of the primary tumor (GTVp) of oropharyngeal
cancer patients (OPC), simultaneous assessment of different image modalities is
needed, and each image volume is explored slice-by-slice from different
orientations. Moreover, the manual fixed boundary of segmentation neglects the
spatial uncertainty known to occur in tumor delineation. This study proposes a
novel automatic deep learning (DL) model to assist radiation oncologists in a
slice-by-slice adaptive GTVp segmentation on registered FDG PET/CT images. We
included 138 OPC patients treated with (chemo)radiation in our institute. Our
DL framework exploits both inter and intra-slice context. Sequences of 3
consecutive 2D slices of concatenated FDG PET/CT images and GTVp contours were
used as input. A 3-fold cross validation was performed three times, training on
sequences extracted from the Axial (A), Sagittal (S), and Coronal (C) plane of
113 patients. Since consecutive sequences in a volume contain overlapping
slices, each slice resulted in three outcome predictions that were averaged. In
the A, S, and C planes, the output shows areas with different probabilities of
predicting the tumor. The performance of the models was assessed on 25 patients
at different probability thresholds using the mean Dice Score Coefficient
(DSC). Predictions were the closest to the ground truth at a probability
threshold of 0.9 (DSC of 0.70 in the A, 0.77 in the S, and 0.80 in the C
plane). The promising results of the proposed DL model show that the
probability maps on registered FDG PET/CT images could guide radiation
oncologists in a slice-by-slice adaptive GTVp segmentation.</p>
  </details>
</details>
<h1>人工智能</h1>
<details>
  <summary>1. <b>标题：Offline RL Policies Should be Trained to be Adaptive</b></summary>
  <p><b>编号</b>：[7]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02200</p>
  <p><b>作者</b>：Dibya Ghosh,  Anurag Ajay,  Pulkit Agrawal,  Sergey Levine</p>
  <p><b>备注</b>：ICML 2022 (long talk)</p>
  <p><b>关键词</b>：environment unknown, provided may leave, leave many facets, Offline, adaptive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Offline RL algorithms must account for the fact that the dataset they are
provided may leave many facets of the environment unknown. The most common way
to approach this challenge is to employ pessimistic or conservative methods,
which avoid behaviors that are too dissimilar from those in the training
dataset. However, relying exclusively on conservatism has drawbacks:
performance is sensitive to the exact degree of conservatism, and conservative
objectives can recover highly suboptimal policies. In this work, we propose
that offline RL methods should instead be adaptive in the presence of
uncertainty. We show that acting optimally in offline RL in a Bayesian sense
involves solving an implicit POMDP. As a result, optimal policies for offline
RL must be adaptive, depending not just on the current state but rather all the
transitions seen so far during evaluation.We present a model-free algorithm for
approximating this optimal adaptive policy, and demonstrate the efficacy of
learning such adaptive policies in offline RL benchmarks.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：CLEAR: Improving Vision-Language Navigation with Cross-Lingual,  Environment-Agnostic Representations</b></summary>
  <p><b>编号</b>：[13]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02185</p>
  <p><b>作者</b>：Jialu Li,  Hao Tan,  Mohit Bansal</p>
  <p><b>备注</b>：NAACL 2022 Findings (18 pages)</p>
  <p><b>关键词</b>：VLN, representation, visual, visual representation, language representation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision-and-Language Navigation (VLN) tasks require an agent to navigate
through the environment based on language instructions. In this paper, we aim
to solve two key challenges in this task: utilizing multilingual instructions
for improved instruction-path grounding and navigating through new environments
that are unseen during training. To address these challenges, we propose CLEAR:
Cross-Lingual and Environment-Agnostic Representations. First, our agent learns
a shared and visually-aligned cross-lingual language representation for the
three languages (English, Hindi and Telugu) in the Room-Across-Room dataset.
Our language representation learning is guided by text pairs that are aligned
by visual information. Second, our agent learns an environment-agnostic visual
representation by maximizing the similarity between semantically-aligned image
pairs (with constraints on object-matching) from different environments. Our
environment agnostic visual representation can mitigate the environment bias
induced by low-level visual information. Empirically, on the Room-Across-Room
dataset, we show that our multilingual agent gets large improvements in all
metrics over the strong baseline model when generalizing to unseen environments
with the cross-lingual language representation and the environment-agnostic
visual representation. Furthermore, we show that our learned language and
visual representations can be successfully transferred to the Room-to-Room and
Cooperative Vision-and-Dialogue Navigation task, and present detailed
qualitative and quantitative generalization and grounding analysis. Our code is
available at this https URL</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Activation Template Matching Loss for Explainable Face Recognition</b></summary>
  <p><b>编号</b>：[16]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02179</p>
  <p><b>作者</b>：Huawei Lin,  Haozhe Liu,  Qiufu Li,  Linlin Shen</p>
  <p><b>备注</b>：13 pages, 7 figures, 5 tables</p>
  <p><b>关键词</b>：explainable face recognition, face recognition network, Explainable Channel Loss, facial part-based feature, feature like eyes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Can we construct an explainable face recognition network able to learn a
facial part-based feature like eyes, nose, mouth and so forth, without any
manual annotation or additionalsion datasets? In this paper, we propose a
generic Explainable Channel Loss (ECLoss) to construct an explainable face
recognition network. The explainable network trained with ECLoss can easily
learn the facial part-based representation on the target convolutional layer,
where an individual channel can detect a certain face part. Our experiments on
dozens of datasets show that ECLoss achieves superior explainability metrics,
and at the same time improves the performance of face verification without face
alignment. In addition, our visualization results also illustrate the
effectiveness of the proposed ECLoss.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Tackling Real-World Autonomous Driving using Deep Reinforcement Learning</b></summary>
  <p><b>编号</b>：[21]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02162</p>
  <p><b>作者</b>：Paolo Maramotti,  Alessandro Paolo Capasso,  Giulio Bacchiani,  Alberto Broggi</p>
  <p><b>备注</b>：Oral Presentation at Intelligent Vehicles Symposium 2022</p>
  <p><b>关键词</b>：control systems represent, autonomous driving stack, typical autonomous driving, comfortable self-driving behavior, Deep Reinforcement Learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the typical autonomous driving stack, planning and control systems
represent two of the most crucial components in which data retrieved by sensors
and processed by perception algorithms are used to implement a safe and
comfortable self-driving behavior. In particular, the planning module predicts
the path the autonomous car should follow taking the correct high-level
maneuver, while control systems perform a sequence of low-level actions,
controlling steering angle, throttle and brake. In this work, we propose a
model-free Deep Reinforcement Learning Planner training a neural network that
predicts both acceleration and steering angle, thus obtaining a single module
able to drive the vehicle using the data processed by localization and
perception algorithms on board of the self-driving car. In particular, the
system that was fully trained in simulation is able to drive smoothly and
safely in obstacle-free environments both in simulation and in a real-world
urban area of the city of Parma, proving that the system features good
generalization capabilities also driving in those parts outside the training
scenarios. Moreover, in order to deploy the system on board of the real
self-driving car and to reduce the gap between simulated and real-world
performances, we also develop a module represented by a tiny neural network
able to reproduce the real vehicle dynamic behavior during the training in
simulation.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：A Comprehensive Review of Visual-Textual Sentiment Analysis from Social  Media Networks</b></summary>
  <p><b>编号</b>：[22]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02160</p>
  <p><b>作者</b>：Israa Khalaf Salman Al-Tameemi,  Mohammad-Reza Feizi-Derakhshi,  Saeed Pashazadeh,  Mohammad Asadpour</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：opinions and emotions, Social media networks, Social media, people lives, people</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Social media networks have become a significant aspect of people's lives,
serving as a platform for their ideas, opinions and emotions. Consequently,
automated sentiment analysis (SA) is critical for recognising people's feelings
in ways that other information sources cannot. The analysis of these feelings
revealed various applications, including brand evaluations, YouTube film
reviews and healthcare applications. As social media continues to develop,
people post a massive amount of information in different forms, including text,
photos, audio and video. Thus, traditional SA algorithms have become limited,
as they do not consider the expressiveness of other modalities. By including
such characteristics from various material sources, these multimodal data
streams provide new opportunities for optimising the expected results beyond
text-based SA. Our study focuses on the forefront field of multimodal SA, which
examines visual and textual data posted on social media networks. Many people
are more likely to utilise this information to express themselves on these
platforms. To serve as a resource for academics in this rapidly growing field,
we introduce a comprehensive overview of textual and visual SA, including data
pre-processing, feature extraction techniques, sentiment benchmark datasets,
and the efficacy of multiple classification methodologies suited to each field.
We also provide a brief introduction of the most frequently utilised data
fusion strategies and a summary of existing research on visual-textual SA.
Finally, we highlight the most significant challenges and investigate several
important sentiment applications.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Deep Learning for Finger Vein Recognition: A Brief Survey of Recent  Trend</b></summary>
  <p><b>编号</b>：[27]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02148</p>
  <p><b>作者</b>：Renye Zhang,  Yimin Yin,  Wanxia Deng,  Chen Li,  Jinghua Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Finger vein image, vein image recognition, vein image, image recognition, recognition technology plays</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Finger vein image recognition technology plays an important role in biometric
recognition and has been successfully applied in many fields. Because veins are
buried beneath the skin tissue, finger vein image recognition has an
unparalleled advantage, which is not easily disturbed by external factors. This
review summarizes 46 papers about deep learning for finger vein image
recognition from 2017 to 2021. These papers are summarized according to the
tasks of deep neural networks. Besides, we present the challenges and potential
development directions of finger vein image recognition.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Malware and Ransomware Detection Models</b></summary>
  <p><b>编号</b>：[40]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02108</p>
  <p><b>作者</b>：Benjamin Marais,  Tony Quertier,  Stéphane Morucci</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：major digital threats, major digital, digital threats, Deep Learning models, Machine Learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cybercrime is one of the major digital threats of this century. In
particular, ransomware attacks have significantly increased, resulting in
global damage costs of tens of billion dollars. In this paper, we train and
test different Machine Learning and Deep Learning models for malware detection,
malware classification and ransomware detection. We introduce a novel and
flexible ransomware detection model that combines two optimized models. Our
detection results on a limited dataset demonstrate good accuracy and F1 scores.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Generating Game Levels of Diverse Behaviour Engagement</b></summary>
  <p><b>编号</b>：[45]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02100</p>
  <p><b>作者</b>：Keyuan Zhang,  Jiayu Bai,  Jialin Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：procedural level generation, Recent years, experience-driven procedural level, growing interests, interests in experience-driven</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent years, there has been growing interests in experience-driven
procedural level generation. Various metrics have been formulated to model
player experience and help generate personalised levels. In this work, we
question whether experience metrics can adapt to agents with different
personas. We start by reviewing existing metrics for evaluating game levels.
Then, focusing on platformer games, we design a framework integrating various
agents and evaluation metrics. Experimental studies on \emph{Super Mario Bros.}
indicate that using the same evaluation metrics but agents with different
personas can generate levels for particular persona. It implies that, for
simple games, using a game-playing agent of specific player archetype as a
level tester is probably all we need to generate levels of diverse behaviour
engagement.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Neural Networks and the Chomsky Hierarchy</b></summary>
  <p><b>编号</b>：[47]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02098</p>
  <p><b>作者</b>：Grégoire Delétang,  Anian Ruoss,  Jordi Grau-Moya,  Tim Genewein,  Li Kevin Wenliang,  Elliot Catt,  Marcus Hutter,  Shane Legg,  Pedro A. Ortega</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Reliable generalization lies, heart of safe, Reliable generalization, tasks, generalization lies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reliable generalization lies at the heart of safe ML and AI. However,
understanding when and how neural networks generalize remains one of the most
important unsolved problems in the field. In this work, we conduct an extensive
empirical study (2200 models, 16 tasks) to investigate whether insights from
the theory of computation can predict the limits of neural network
generalization in practice. We demonstrate that grouping tasks according to the
Chomsky hierarchy allows us to forecast whether certain architectures will be
able to generalize to out-of-distribution inputs. This includes negative
results where even extensive amounts of data and training time never led to any
non-trivial generalization, despite models having sufficient capacity to
perfectly fit the training data. Our results show that, for our subset of
tasks, RNNs and Transformers fail to generalize on non-regular tasks, LSTMs can
solve regular and counter-language tasks, and only networks augmented with
structured memory (such as a stack or memory tape) can successfully generalize
on context-free and context-sensitive tasks.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Resource Allocation in Multicore Elastic Optical Networks: A Deep  Reinforcement Learning Approach</b></summary>
  <p><b>编号</b>：[54]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02074</p>
  <p><b>作者</b>：Juan Pinto-Ríos,  Felipe Calderón,  Ariel Leiva,  Gabriel Hermosilla,  Alejandra Beghelli,  Danilo Bórquez-Paredes,  Astrid Lozada,  Nicolás Jara,  Ricardo Olivares,  Gabriel Saavedra</p>
  <p><b>备注</b>：11 pages, 10 figures</p>
  <p><b>关键词</b>：deep reinforcement learning, reinforcement learning approach, dynamic multicore fiber, multicore fiber elastic, fiber elastic optical</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A deep reinforcement learning approach is applied, for the first time, to
solve the routing, modulation, spectrum and core allocation (RMSCA) problem in
dynamic multicore fiber elastic optical networks (MCF-EONs). To do so, a new
environment - compatible with OpenAI's Gym - was designed and implemented to
emulate the operation of MCF-EONs. The new environment processes the agent
actions (selection of route, core and spectrum slot) by considering the network
state and physical-layer-related aspects. The latter includes the available
modulation formats and their reach and the inter-core crosstalk (XT), an
MCF-related impairment. If the resulting quality of the signal is acceptable,
the environment allocates the resources selected by the agent. After processing
the agent's action, the environment is configured to give the agent a numerical
reward and information about the new network state. The blocking performance of
four different agents was compared through simulation to 3 baseline heuristics
used in MCF-EONs. Results obtained for the NSFNet and COST239 network
topologies show that the best-performing agent achieves, on average, up to a
four-times decrease in blocking probability concerning the best-performing
baseline heuristic methods.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Online 2-stage Stable Matching</b></summary>
  <p><b>编号</b>：[60]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02057</p>
  <p><b>作者</b>：Evripidis Bampis,  Bruno Escoffier,  Paul Youssef</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：assigned to universities, students, system, stable, online</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We focus on an online 2-stage problem, motivated by the following situation:
consider a system where students shall be assigned to universities. There is a
first round where some students apply, and a first (stable) matching $M_1$ has
to be computed. However, some students may decide to leave the system (change
their plan, go to a foreign university, or to some institution not in the
system). Then, in a second round (after these deletions), we shall compute a
second (final) stable matching $M_2$. As it is undesirable to change
assignments, the goal is to minimize the number of divorces/modifications
between the two stable matchings $M_1$ and $M_2$. Then, how should we choose
$M_1$ and $M_2$? We show that there is an {\it optimal online} algorithm to
solve this problem. In particular, thanks to a dominance property, we show that
we can optimally compute $M_1$ without knowing the students that will leave the
system. We generalize the result to some other possible modifications in the
input (students, open positions).
We also tackle the case of more stages, showing that no competitive (online)
algorithm can be achieved for the considered problem as soon as there are 3
stages.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：PKD: General Distillation Framework for Object Detectors via Pearson  Correlation Coefficient</b></summary>
  <p><b>编号</b>：[66]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02039</p>
  <p><b>作者</b>：Weihan Cao,  Yifan Zhang,  Jianfei Gao,  Anda Cheng,  Ke Cheng,  Jian Cheng</p>
  <p><b>备注</b>：17 pages, 7 figures, 8 tables</p>
  <p><b>关键词</b>：train compact models, widely-used technique, technique to train, train compact, teacher</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Knowledge distillation(KD) is a widely-used technique to train compact models
in object detection. However, there is still a lack of study on how to distill
between heterogeneous detectors. In this paper, we empirically find that better
FPN features from a heterogeneous teacher detector can help the student
although their detection heads and label assignments are different. However,
directly aligning the feature maps to distill detectors suffers from two
problems. First, the difference in feature magnitude between the teacher and
the student could enforce overly strict constraints on the student. Second, the
FPN stages and channels with large feature magnitude from the teacher model
could dominate the gradient of distillation loss, which will overwhelm the
effects of other features in KD and introduce much noise. To address the above
issues, we propose to imitate features with Pearson Correlation Coefficient to
focus on the relational information from the teacher and relax constraints on
the magnitude of the features. Our method consistently outperforms the existing
detection KD methods and works for both homogeneous and heterogeneous
student-teacher pairs. Furthermore, it converges faster. With a powerful
MaskRCNN-Swin detector as the teacher, ResNet-50 based RetinaNet and FCOS
achieve 41.5% and 43.9% mAP on COCO2017, which are 4.1\% and 4.8\% higher than
the baseline, respectively.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：PRoA: A Probabilistic Robustness Assessment against Functional  Perturbations</b></summary>
  <p><b>编号</b>：[67]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02036</p>
  <p><b>作者</b>：Tianle Zhang,  Wenjie Ruan,  Jonathan E. Fieldsend</p>
  <p><b>备注</b>：The short version of this work will appear in the Proceedings of the 2022 European Conference on Machine Learning and Data Mining (ECML-PKDD 2022)</p>
  <p><b>关键词</b>：vital pre-deployment phase, applications robustness measurement, safety-critical deep learning, deep learning applications, learning applications robustness</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In safety-critical deep learning applications robustness measurement is a
vital pre-deployment phase. However, existing robustness verification methods
are not sufficiently practical for deploying machine learning systems in the
real world. On the one hand, these methods attempt to claim that no
perturbations can ``fool'' deep neural networks (DNNs), which may be too
stringent in practice. On the other hand, existing works rigorously consider
$L_p$ bounded additive perturbations on the pixel space, although
perturbations, such as colour shifting and geometric transformations, are more
practically and frequently occurring in the real world. Thus, from the
practical standpoint, we present a novel and general {\it probabilistic
robustness assessment method} (PRoA) based on the adaptive concentration, and
it can measure the robustness of deep learning models against functional
perturbations. PRoA can provide statistical guarantees on the probabilistic
robustness of a model, \textit{i.e.}, the probability of failure encountered by
the trained model after deployment. Our experiments demonstrate the
effectiveness and flexibility of PRoA in terms of evaluating the probabilistic
robustness against a broad range of functional perturbations, and PRoA can
scale well to various large-scale deep neural networks compared to existing
state-of-the-art baselines. For the purpose of reproducibility, we release our
tool on GitHub: \url{ this https URL}.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Robust Reinforcement Learning in Continuous Control Tasks with  Uncertainty Set Regularization</b></summary>
  <p><b>编号</b>：[71]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02016</p>
  <p><b>作者</b>：Yuan Zhang,  Jianhong Wang,  Joschka Boedecker</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：environmental perturbations, generalization and robustness, robustness under environmental, excessively restricts, restricts its application</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reinforcement learning (RL) is recognized as lacking generalization and
robustness under environmental perturbations, which excessively restricts its
application for real-world robotics. Prior work claimed that adding
regularization to the value function is equivalent to learning a robust policy
with uncertain transitions. Although the regularization-robustness
transformation is appealing for its simplicity and efficiency, it is still
lacking in continuous control tasks. In this paper, we propose a new
regularizer named $\textbf{U}$ncertainty $\textbf{S}$et $\textbf{R}$egularizer
(USR), by formulating the uncertainty set on the parameter space of the
transition function. In particular, USR is flexible enough to be plugged into
any existing RL framework. To deal with unknown uncertainty sets, we further
propose a novel adversarial approach to generate them based on the value
function. We evaluate USR on the Real-world Reinforcement Learning (RWRL)
benchmark, demonstrating improvements in the robust performance for perturbed
testing environments.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Towards trustworthy Energy Disaggregation: A review of challenges,  methods and perspectives for Non-Intrusive Load Monitoring</b></summary>
  <p><b>编号</b>：[74]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02009</p>
  <p><b>作者</b>：Maria Kaselimi,  Eftychios Protopapadakis,  Athanasios Voulodimos,  Nikolaos Doulamis,  Anastasios Doulamis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Non-intrusive load monitoring, total power consumption, NILM, Non-intrusive load, load monitoring</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Non-intrusive load monitoring (NILM) is the task of disaggregating the total
power consumption into its individual sub-components. Over the years, signal
processing and machine learning algorithms have been combined to achieve this.
A lot of publications and extensive research works are performed on energy
disaggregation or NILM for the state-of-the-art methods to reach on the
desirable performance. The initial interest of the scientific community to
formulate and describe mathematically the NILM problem using machine learning
tools has now shifted into a more practical NILM. Nowadays, we are in the
mature NILM period where there is an attempt for NILM to be applied in
real-life application scenarios. Thus, complexity of the algorithms,
transferability, reliability, practicality and in general trustworthiness are
the main issues of interest. This review narrows the gap between the early
immature NILM era and the mature one. In particular, the paper provides a
comprehensive literature review of the NILM methods for residential appliances
only. The paper analyzes, summarizes and presents the outcomes of a large
number of recently published scholarly articles. Also, the paper discusses the
highlights of these methods and introduces the research dilemmas that should be
taken into consideration by researchers to apply NILM methods. Finally, we show
the need for transferring the traditional disaggregation models into a
practical and trustworthy framework.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：The StarCraft Multi-Agent Challenges+ : Learning of Multi-Stage Tasks  and Environmental Factors without Precise Reward Functions</b></summary>
  <p><b>编号</b>：[76]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02007</p>
  <p><b>作者</b>：Mingyu Kim,  Jihwan Oh,  Yongsik Lee,  Joonkee Kim,  Seonghwan Kim,  Song Chong,  Se-Young Yun</p>
  <p><b>备注</b>：ICML Workshop: AI for Agent Based Modeling 2022 Spotlight</p>
  <p><b>关键词</b>：Multi-Agent Reinforcement Learning, called the StarCraft, reward functions, benchmark called, StarCraft Multi-Agent</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we propose a novel benchmark called the StarCraft Multi-Agent
Challenges+, where agents learn to perform multi-stage tasks and to use
environmental factors without precise reward functions. The previous challenges
(SMAC) recognized as a standard benchmark of Multi-Agent Reinforcement Learning
are mainly concerned with ensuring that all agents cooperatively eliminate
approaching adversaries only through fine manipulation with obvious reward
functions.
This challenge, on the other hand, is interested in the exploration
capability of MARL algorithms to efficiently learn implicit multi-stage tasks
and environmental factors as well as micro-control. This study covers both
offensive and defensive scenarios.
In the offensive scenarios, agents must learn to first find opponents and
then eliminate them. The defensive scenarios require agents to use topographic
features. For example, agents need to position themselves behind protective
structures to make it harder for enemies to attack. We investigate MARL
algorithms under SMAC+ and observe that recent approaches work well in similar
settings to the previous challenges, but misbehave in offensive scenarios.
Additionally, we observe that an enhanced exploration approach has a positive
effect on performance but is not able to completely solve all scenarios. This
study proposes new directions for future research.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Disentangling private classes through regularization</b></summary>
  <p><b>编号</b>：[78]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02000</p>
  <p><b>作者</b>：Enzo Tartaglione,  Francesca Gennari,  Marco Grangetto</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：nowadays broadly deployed, incredibly large variety, General Data Protection, nowadays broadly, broadly deployed</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning models are nowadays broadly deployed to solve an incredibly
large variety of tasks. However, little attention has been devoted to connected
legal aspects. In 2016, the European Union approved the General Data Protection
Regulation which entered into force in 2018. Its main rationale was to protect
the privacy and data protection of its citizens by the way of operating of the
so-called "Data Economy". As data is the fuel of modern Artificial
Intelligence, it is argued that the GDPR can be partly applicable to a series
of algorithmic decision making tasks before a more structured AI Regulation
enters into force. In the meantime, AI should not allow undesired information
leakage deviating from the purpose for which is created. In this work we
propose DisP, an approach for deep learning models disentangling the
information related to some classes we desire to keep private, from the data
processed by AI. In particular, DisP is a regularization strategy
de-correlating the features belonging to the same private class at training
time, hiding the information of private classes membership. Our experiments on
state-of-the-art deep learning models show the effectiveness of DisP,
minimizing the risk of extraction for the classes we desire to keep private.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Local Multi-Label Explanations for Random Forest</b></summary>
  <p><b>编号</b>：[80]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01994</p>
  <p><b>作者</b>：Nikolaos Mylonas,  Ioannis Mollas,  Nick Bassiliades,  Grigorios Tsoumakas</p>
  <p><b>备注</b>：11 pages, 1 figues, 8 tables, submitted to XKDD (workshop of ECML PKDD 2022)</p>
  <p><b>关键词</b>：challenging task, Multi-label classification, Multi-label, Random forest, classification</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multi-label classification is a challenging task, particularly in domains
where the number of labels to be predicted is large. Deep neural networks are
often effective at multi-label classification of images and textual data. When
dealing with tabular data, however, conventional machine learning algorithms,
such as tree ensembles, appear to outperform competition. Random forest, being
a popular ensemble algorithm, has found use in a wide range of real-world
problems. Such problems include fraud detection in the financial domain, crime
hotspot detection in the legal sector, and in the biomedical field, disease
probability prediction when patient records are accessible. Since they have an
impact on people's lives, these domains usually require decision-making systems
to be explainable. Random Forest falls short on this property, especially when
a large number of tree predictors are used. This issue was addressed in a
recent research named LionForests, regarding single label classification and
regression. In this work, we adapt this technique to multi-label classification
problems, by employing three different strategies regarding the labels that the
explanation covers. Finally, we provide a set of qualitative and quantitative
experiments to assess the efficacy of this approach.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Ask-AC: An Initiative Advisor-in-the-Loop Actor-Critic Framework</b></summary>
  <p><b>编号</b>：[93]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01955</p>
  <p><b>作者</b>：Shunyu Liu,  Xinchao Wang,  Na Yu,  Jie Song,  Kaixuan Chen,  Zunlei Feng,  Mingli Song</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：passively receiving supervision, receiving supervision signals, promising results achieved, interactive reinforcement learning, expensive learning process</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite the promising results achieved, state-of-the-art interactive
reinforcement learning schemes rely on passively receiving supervision signals
from advisor experts, in the form of either continuous monitoring or
pre-defined rules, which inevitably result in a cumbersome and expensive
learning process. In this paper, we introduce a novel initiative
advisor-in-the-loop actor-critic framework, termed as Ask-AC, that replaces the
unilateral advisor-guidance mechanism with a bidirectional learner-initiative
one, and thereby enables a customized and efficacious message exchange between
learner and advisor. At the heart of Ask-AC are two complementary components,
namely action requester and adaptive state selector, that can be readily
incorporated into various discrete actor-critic architectures. The former
component allows the agent to initiatively seek advisor intervention in the
presence of uncertain states, while the latter identifies the unstable states
potentially missed by the former especially when environment changes, and then
learns to promote the ask action on such states. Experimental results on both
stationary and non-stationary environments and across different actor-critic
backbones demonstrate that the proposed framework significantly improves the
learning efficiency of the agent, and achieves the performances on par with
those obtained by continuous advisor monitoring.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：GLANCE: Global to Local Architecture-Neutral Concept-based Explanations</b></summary>
  <p><b>编号</b>：[110]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01917</p>
  <p><b>作者</b>：Avinash Kori,  Ben Glocker,  Francesca Toni</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：current explainability techniques, explainability techniques focus, techniques focus, focus on capturing, features</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Most of the current explainability techniques focus on capturing the
importance of features in input space. However, given the complexity of models
and data-generating processes, the resulting explanations are far from being
`complete', in that they lack an indication of feature interactions and
visualization of their `effect'. In this work, we propose a novel
twin-surrogate explainability framework to explain the decisions made by any
CNN-based image classifier (irrespective of the architecture). For this, we
first disentangle latent features from the classifier, followed by aligning
these features to observed/human-defined `context' features. These aligned
features form semantically meaningful concepts that are used for extracting a
causal graph depicting the `perceived' data-generating process, describing the
inter- and intra-feature interactions between unobserved latent features and
observed `context' features. This causal graph serves as a global model from
which local explanations of different forms can be extracted. Specifically, we
provide a generator to visualize the `effect' of interactions among features in
latent space and draw feature importance therefrom as local explanations. Our
framework utilizes adversarial knowledge distillation to faithfully learn a
representation from the classifiers' latent space and use it for extracting
visual explanations. We use the styleGAN-v2 architecture with an additional
regularization term to enforce disentanglement and alignment. We demonstrate
and evaluate explanations obtained with our framework on Morpho-MNIST and on
the FFHQ human faces dataset. Our framework is available at
\url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Hierarchical Symbolic Reasoning in Hyperbolic Space for Deep  Discriminative Models</b></summary>
  <p><b>编号</b>：[111]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01916</p>
  <p><b>作者</b>：Ainkaran Santhirasekaram,  Avinash Kori,  Andrea Rockall,  Mathias Winkler,  Francesca Toni,  Ben Glocker</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：understand model decisions, emph, biases and inconsistencies, provide information, understand model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Explanations for \emph{black-box} models help us understand model decisions
as well as provide information on model biases and inconsistencies. Most of the
current explainability techniques provide a single level of explanation, often
in terms of feature importance scores or feature attention maps in input space.
Our focus is on explaining deep discriminative models at \emph{multiple levels
of abstraction}, from fine-grained to fully abstract explanations. We achieve
this by using the natural properties of \emph{hyperbolic geometry} to more
efficiently model a hierarchy of symbolic features and generate
\emph{hierarchical symbolic rules} as part of our explanations. Specifically,
for any given deep discriminative model, we distill the underpinning knowledge
by discretisation of the continuous latent space using vector quantisation to
form symbols, followed by a \emph{hyperbolic reasoning block} to induce an
\emph{abstraction tree}. We traverse the tree to extract explanations in terms
of symbolic rules and its corresponding visual semantics. We demonstrate the
effectiveness of our method on the MNIST and AFHQ high-resolution animal faces
dataset. Our framework is available at
\url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Explainability in Deep Reinforcement Learning, a Review into Current  Methods and Applications</b></summary>
  <p><b>编号</b>：[112]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01911</p>
  <p><b>作者</b>：Thomas Hickling,  Abdelhafid Zenati,  Nabil Aouf,  Phillippa Spencer</p>
  <p><b>备注</b>：21 pages, 6 figures, Paper Review</p>
  <p><b>关键词</b>：Deep Reinforcement Learning, Reinforcement Learning, Deep Reinforcement, schemes has increased, increased dramatically</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The use of Deep Reinforcement Learning (DRL) schemes has increased
dramatically since their first introduction in 2015. Though uses in many
different applications are being found they still have a problem with the lack
of interpretability. This has bread a lack of understanding and trust in the
use of DRL solutions from researchers and the general public. To solve this
problem the field of explainable artificial intelligence (XAI) has emerged.
This is a variety of different methods that look to open the DRL black boxes,
they range from the use of interpretable symbolic decision trees to numerical
methods like Shapley Values. This review looks at which methods are being used
and what applications they are being used. This is done to identify which
models are the best suited to each application or if a method is being
underutilised.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：StyleFlow For Content-Fixed Image to Image Translation</b></summary>
  <p><b>编号</b>：[114]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01909</p>
  <p><b>作者</b>：Weichen Fan,  Jinghuan Chen,  Jiabin Ma,  Jun Hou,  Shuai Yi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：constrained tasks, constrained, computer vision, strongly constrained, challenging topic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image-to-image (I2I) translation is a challenging topic in computer vision.
We divide this problem into three tasks: strongly constrained translation,
normally constrained translation, and weakly constrained translation. The
constraint here indicates the extent to which the content or semantic
information in the original image is preserved. Although previous approaches
have achieved good performance in weakly constrained tasks, they failed to
fully preserve the content in both strongly and normally constrained tasks,
including photo-realism synthesis, style transfer, and colorization, etc. To
achieve content-preserving transfer in strongly constrained and normally
constrained tasks, we propose StyleFlow, a new I2I translation model that
consists of normalizing flows and a novel Style-Aware Normalization (SAN)
module. With the invertible network structure, StyleFlow first projects input
images into deep feature space in the forward pass, while the backward pass
utilizes the SAN module to perform content-fixed feature transformation and
then projects back to image space. Our model supports both image-guided
translation and multi-modal synthesis. We evaluate our model in several I2I
translation benchmarks, and the results show that the proposed model has
advantages over previous methods in both strongly constrained and normally
constrained tasks.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题："Even if ..." -- Diverse Semifactual Explanations of Reject</b></summary>
  <p><b>编号</b>：[119]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01898</p>
  <p><b>作者</b>：André Artelt,  Barbara Hammer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：safety critical areas, critical areas require, areas require reliable, require reliable high, reliable high certainty</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine learning based decision making systems applied in safety critical
areas require reliable high certainty predictions. For this purpose, the system
can be extended by an reject option which allows the system to reject inputs
where only a prediction with an unacceptably low certainty would be possible.
While being able to reject uncertain samples is important, it is also of
importance to be able to explain why a particular sample was rejected. With the
ongoing rise of eXplainable AI (XAI), a lot of explanation methodologies for
machine learning based systems have been developed -- explaining reject
options, however, is still a novel field where only very little prior work
exists.
In this work, we propose to explain rejects by semifactual explanations, an
instance of example-based explanation methods, which them self have not been
widely considered in the XAI community yet. We propose a conceptual modeling of
semifactual explanations for arbitrary reject options and empirically evaluate
a specific implementation on a conformal prediction based reject option.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Planning with RL and episodic-memory behavioral priors</b></summary>
  <p><b>编号</b>：[139]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01845</p>
  <p><b>作者</b>：Shivansh Beohar,  Andrew Melnik</p>
  <p><b>备注</b>：Published in ICRA 2022 BPRL Workshop</p>
  <p><b>关键词</b>：requires sample efficient, interpretable algorithms, agents requires sample, practical application, sample efficient</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The practical application of learning agents requires sample efficient and
interpretable algorithms. Learning from behavioral priors is a promising way to
bootstrap agents with a better-than-random exploration policy or a safe-guard
against the pitfalls of early learning. Existing solutions for imitation
learning require a large number of expert demonstrations and rely on
hard-to-interpret learning methods like Deep Q-learning. In this work we
present a planning-based approach that can use these behavioral priors for
effective exploration and learning in a reinforcement learning environment, and
we demonstrate that curated exploration policies in the form of behavioral
priors can help an agent learn faster.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：What Do Graph Convolutional Neural Networks Learn?</b></summary>
  <p><b>编号</b>：[145]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01839</p>
  <p><b>作者</b>：Sannat Singh Bhasin,  Vaibhav Holani,  Divij Sanjanwala</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：machine learning tasks, numerous machine learning, neural networks, Graph neural networks, Convolutional Neural Networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Graph neural networks (GNNs) have gained traction over the past few years for
their superior performance in numerous machine learning tasks. Graph
Convolutional Neural Networks (GCN) are a common variant of GNNs that are known
to have high performance in semi-supervised node classification (SSNC), and
work well under the assumption of homophily. Recent literature has highlighted
that GCNs can achieve strong performance on heterophilous graphs under certain
"special conditions". These arguments motivate us to understand why, and how,
GCNs learn to perform SSNC. We find a positive correlation between similarity
of latent node embeddings of nodes within a class and the performance of a GCN.
Our investigation on underlying graph structures of a dataset finds that a
GCN's SSNC performance is significantly influenced by the consistency and
uniqueness in neighborhood structure of nodes within a class.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Features Based Adaptive Augmentation for Graph Contrastive Learning</b></summary>
  <p><b>编号</b>：[166]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01792</p>
  <p><b>作者</b>：Adnan Ali (1),  Jinlong Li (2) ((1) University of Science and Technology of China, (2) University of Science and Technology of China)</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Self-Supervised learning aims, graph contrastive learning, data-data pairs, aims to eliminate, expensive annotation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Self-Supervised learning aims to eliminate the need for expensive annotation
in graph representation learning, where graph contrastive learning (GCL) is
trained with the self-supervision signals containing data-data pairs. These
data-data pairs are generated with augmentation employing stochastic functions
on the original graph. We argue that some features can be more critical than
others depending on the downstream task, and applying stochastic function
uniformly, will vandalize the influential features, leading to diminished
accuracy. To fix this issue, we introduce a Feature Based Adaptive Augmentation
(FebAA) approach, which identifies and preserves potentially influential
features and corrupts the remaining ones. We implement FebAA as plug and play
layer and use it with state-of-the-art Deep Graph Contrastive Learning (GRACE)
and Bootstrapped Graph Latents (BGRL). We successfully improved the accuracy of
GRACE and BGRL on eight graph representation learning's benchmark datasets.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：opPINN: Physics-Informed Neural Network with operator learning to  approximate solutions to the Fokker-Planck-Landau equation</b></summary>
  <p><b>编号</b>：[181]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01765</p>
  <p><b>作者</b>：Jae Yong Lee,  Juhi Jang,  Hyung Ju Hwang</p>
  <p><b>备注</b>：28 pages, 12 figures</p>
  <p><b>关键词</b>：FPL equation, operator surrogate models, FPL, hybrid framework opPINN, surrogate models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a hybrid framework opPINN: physics-informed neural network (PINN)
with operator learning for approximating the solution to the
Fokker-Planck-Landau (FPL) equation. The opPINN framework is divided into two
steps: Step 1 and Step 2. After the operator surrogate models are trained
during Step 1, PINN can effectively approximate the solution to the FPL
equation during Step 2 by using the pre-trained surrogate models. The operator
surrogate models greatly reduce the computational cost and boost PINN by
approximating the complex Landau collision integral in the FPL equation. The
operator surrogate models can also be combined with the traditional numerical
schemes. It provides a high efficiency in computational time when the number of
velocity modes becomes larger. Using the opPINN framework, we provide the
neural network solutions for the FPL equation under the various types of
initial conditions, and interaction models in two and three dimensions.
Furthermore, based on the theoretical properties of the FPL equation, we show
that the approximated neural network solution converges to the a priori
classical solution of the FPL equation as the pre-defined loss function is
reduced.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：PReGAN: Answer Oriented Passage Ranking with Weakly Supervised GAN</b></summary>
  <p><b>编号</b>：[182]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01762</p>
  <p><b>作者</b>：Pan Du,  Jian-Yun Nie,  Yutao Zhu,  Hao Jiang,  Lixin Zou,  Xiaohui Yan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：open-domain factoid question, factoid question answering, ranking for open-domain, open-domain factoid, answering also requires</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Beyond topical relevance, passage ranking for open-domain factoid question
answering also requires a passage to contain an answer (answerability). While a
few recent studies have incorporated some reading capability into a ranker to
account for answerability, the ranker is still hindered by the noisy nature of
the training data typically available in this area, which considers any passage
containing an answer entity as a positive sample. However, the answer entity in
a passage is not necessarily mentioned in relation with the given question. To
address the problem, we propose an approach called \ttt{PReGAN} for Passage
Reranking based on Generative Adversarial Neural networks, which incorporates a
discriminator on answerability, in addition to a discriminator on topical
relevance. The goal is to force the generator to rank higher a passage that is
topically relevant and contains an answer. Experiments on five public datasets
show that \ttt{PReGAN} can better rank appropriate passages, which in turn,
boosts the effectiveness of QA systems, and outperforms the existing approaches
without using external data.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：GP22: A Car Styling Dataset for Automotive Designers</b></summary>
  <p><b>编号</b>：[184]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01760</p>
  <p><b>作者</b>：Gyunpyo Lee,  Taesu Kim,  Hyeon-Jeong Suk</p>
  <p><b>备注</b>：5th CVFAD workshop, CVPR2022</p>
  <p><b>关键词</b>：automated design data, design data archiving, creatively and effectively, data archiving, archiving could reduce</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>An automated design data archiving could reduce the time wasted by designers
from working creatively and effectively. Though many datasets on classifying,
detecting, and instance segmenting on car exterior exist, these large datasets
are not relevant for design practices as the primary purpose lies in autonomous
driving or vehicle verification. Therefore, we release GP22, composed of car
styling features defined by automotive designers. The dataset contains 1480 car
side profile images from 37 brands and ten car segments. It also contains
annotations of design features that follow the taxonomy of the car exterior
design features defined in the eye of the automotive designer. We trained the
baseline model using YOLO v5 as the design feature detection model with the
dataset. The presented model resulted in an mAP score of 0.995 and a recall of
0.984. Furthermore, exploration of the model performance on sketches and
rendering images of the car side profile implies the scalability of the dataset
for design purposes.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Plan Execution for Multi-Agent Path Finding with Indoor Quadcopters</b></summary>
  <p><b>编号</b>：[188]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01752</p>
  <p><b>作者</b>：Matouš Kulhan,  Pavel Surynek</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：multi-agent path finding, multi-agent path, problem of multi-agent, executing MAPF plans, MAPF</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the planning and acting phase for the problem of multi-agent path
finding (MAPF) in this paper. MAPF is a problem of navigating agents from their
start positions to specified individual goal positions so that agents do not
collide with each other. Specifically we focus on executing MAPF plans with a
group of Crazyflies, small indoor quadcopters . We show how to modify the
existing continuous time conflict-based search algorithm (CCBS) to produce
plans that are suitable for execution with the quadcopters. The acting phase
uses the the Loco positioning system to check if the plan is executed
correctly. Our finding is that the CCBS algorithm allows for extensions that
can produce safe plans for quadcopters, namely cylindrical protection zone
around each quadcopter can be introduced at the planning level.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Federated Split GANs</b></summary>
  <p><b>编号</b>：[190]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01750</p>
  <p><b>作者</b>：Pranvera Kortoçi,  Yilei Liang,  Pengyuan Zhou,  Lik-Hang Lee,  Abbas Mehrabi,  Pan Hui,  Sasu Tarkoma,  Jon Crowcroft</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：based applications, immense amount, amount and variety, generate are key, key enablers</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Mobile devices and the immense amount and variety of data they generate are
key enablers of machine learning (ML)-based applications. Traditional ML
techniques have shifted toward new paradigms such as federated (FL) and split
learning (SL) to improve the protection of user's data privacy. However, these
paradigms often rely on server(s) located in the edge or cloud to train
computationally-heavy parts of a ML model to avoid draining the limited
resource on client devices, resulting in exposing device data to such third
parties. This work proposes an alternative approach to train
computationally-heavy ML models in user's devices themselves, where
corresponding device data resides. Specifically, we focus on GANs (generative
adversarial networks) and leverage their inherent privacy-preserving attribute.
We train the discriminative part of a GAN with raw data on user's devices,
whereas the generative model is trained remotely (e.g., server) for which there
is no need to access sensor true data. Moreover, our approach ensures that the
computational load of training the discriminative model is shared among user's
devices-proportional to their computation capabilities-by means of SL. We
implement our proposed collaborative training scheme of a computationally-heavy
GAN model in real resource-constrained devices. The results show that our
system preserves data privacy, keeps a short training time, and yields same
accuracy of model training in unconstrained devices (e.g., cloud). Our code can
be found on this https URL</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Discrete Tree Flows via Tree-Structured Permutations</b></summary>
  <p><b>编号</b>：[193]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01744</p>
  <p><b>作者</b>：Mai Elkady,  Jim Lim,  David I. Inouye</p>
  <p><b>备注</b>：Accepted to ICML 2022</p>
  <p><b>关键词</b>：extensively researched, recently been explored, discrete, normalizing flows, data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While normalizing flows for continuous data have been extensively researched,
flows for discrete data have only recently been explored. These prior models,
however, suffer from limitations that are distinct from those of continuous
flows. Most notably, discrete flow-based models cannot be straightforwardly
optimized with conventional deep learning methods because gradients of discrete
functions are undefined or zero. Previous works approximate pseudo-gradients of
the discrete functions but do not solve the problem on a fundamental level. In
addition to that, backpropagation can be computationally burdensome compared to
alternative discrete algorithms such as decision tree algorithms. Our approach
seeks to reduce computational burden and remove the need for pseudo-gradients
by developing a discrete flow based on decision trees -- building upon the
success of efficient tree-based methods for classification and regression for
discrete data. We first define a tree-structured permutation (TSP) that
compactly encodes a permutation of discrete data where the inverse is easy to
compute; thus, we can efficiently compute the density value and sample new
data. We then propose a decision tree algorithm to build TSPs that learns the
tree structure and permutations at each node via novel criteria. We empirically
demonstrate the feasibility of our method on multiple datasets.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Anomaly-aware multiple instance learning for rare anemia disorder  classification</b></summary>
  <p><b>编号</b>：[194]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01742</p>
  <p><b>作者</b>：Salome Kazeminia,  Ario Sadafi,  Asya Makhro,  Anna Bogdanova,  Shadi Albarqouni,  Carsten Marr</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Deep learning-based classification, Deep learning-based, instance-level annotations, training data, rare anemia</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning-based classification of rare anemia disorders is challenged by
the lack of training data and instance-level annotations. Multiple Instance
Learning (MIL) has shown to be an effective solution, yet it suffers from low
accuracy and limited explainability. Although the inclusion of attention
mechanisms has addressed these issues, their effectiveness highly depends on
the amount and diversity of cells in the training samples. Consequently, the
poor machine learning performance on rare anemia disorder classification from
blood samples remains unresolved. In this paper, we propose an interpretable
pooling method for MIL to address these limitations. By benefiting from
instance-level information of negative bags (i.e., homogeneous benign cells
from healthy individuals), our approach increases the contribution of anomalous
instances. We show that our strategy outperforms standard MIL classification
algorithms and provides a meaningful explanation behind its decisions.
Moreover, it can denote anomalous instances of rare blood diseases that are not
seen during the training phase.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Disentangled Action Recognition with Knowledge Bases</b></summary>
  <p><b>编号</b>：[206]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01708</p>
  <p><b>作者</b>：Zhekun Luo,  Shalini Ghosh,  Devin Guillory,  Keizo Kato,  Trevor Darrell,  Huijuan Xu</p>
  <p><b>备注</b>：NAACL 2022</p>
  <p><b>关键词</b>：video usually involves, involves the interaction, interaction of human, compositional action, compositional action nodes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Action in video usually involves the interaction of human with objects.
Action labels are typically composed of various combinations of verbs and
nouns, but we may not have training data for all possible combinations. In this
paper, we aim to improve the generalization ability of the compositional action
recognition model to novel verbs or novel nouns that are unseen during training
time, by leveraging the power of knowledge graphs. Previous work utilizes
verb-noun compositional action nodes in the knowledge graph, making it
inefficient to scale since the number of compositional action nodes grows
quadratically with respect to the number of verbs and nouns. To address this
issue, we propose our approach: Disentangled Action Recognition with
Knowledge-bases (DARK), which leverages the inherent compositionality of
actions. DARK trains a factorized model by first extracting disentangled
feature representations for verbs and nouns, and then predicting classification
weights using relations in external knowledge graphs. The type constraint
between verb and noun is extracted from external knowledge bases and finally
applied when composing actions. DARK has better scalability in the number of
objects and verbs, and achieves state-of-the-art performance on the Charades
dataset. We further propose a new benchmark split based on the Epic-kitchen
dataset which is an order of magnitude bigger in the numbers of classes and
samples, and benchmark various models on this benchmark.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Intelligent Exploration of Solution Spaces Exemplified by Industrial  Reconfiguration Management</b></summary>
  <p><b>编号</b>：[213]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01693</p>
  <p><b>作者</b>：Timo Müller,  Benjamin Maschler,  Daniel Dittler,  Nasser Jazdi,  Michael Weyrich</p>
  <p><b>备注</b>：6 pages, 6 figures, 1 table. Accepted at CIRP ICME 2022</p>
  <p><b>关键词</b>：decision-making approaches rely, decision-making approaches, approaches rely, solution spaces, exploration</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Many decision-making approaches rely on the exploration of solution spaces
with regards to specified criteria. However, in complex environments,
brute-force exploration strategies are usually not feasible. As an alternative,
we propose the combination of an exploration task's vertical sub-division into
layers representing different sequentially interdependent sub-problems of the
paramount problem and a horizontal sub-division into self-sustained solution
sub-spaces. In this paper, we present a universal methodology for the
intelligent exploration of solution spaces and derive a use-case specific
example from the field of reconfiguration management in industry 4.0.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Robot Vitals and Robot Health: Towards Systematically Quantifying  Runtime Performance Degradation in Robots Under Adverse Conditions</b></summary>
  <p><b>编号</b>：[217]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01684</p>
  <p><b>作者</b>：Aniketh Ramesh,  Rustam Stolkin,  Manolis Chiou</p>
  <p><b>备注</b>：8 Pages</p>
  <p><b>关键词</b>：task execution, robot, paper addresses, addresses the problem, problem of automatically</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper addresses the problem of automatically detecting and quantifying
performance degradation in remote mobile robots during task execution. A robot
may encounter a variety of uncertainties and adversities during task execution,
which can impair its ability to carry out tasks effectively and cause its
performance to degrade. Such situations can be mitigated or averted by timely
detection and intervention (e.g., by a remote human supervisor taking over
control in teleoperation mode). Inspired by patient triaging systems in
hospitals, we introduce the framework of "robot vitals" for estimating overall
"robot health". A robot's vitals are a set of indicators that estimate the
extent of performance degradation faced by a robot at a given point in time.
Robot health is a metric that combines robot vitals into a single scalar value
estimate of performance degradation. Experiments, both in simulation and on a
real mobile robot, demonstrate that the proposed robot vitals and robot health
can be used effectively to estimate robot performance degradation during
runtime.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Balancing Profit, Risk, and Sustainability for Portfolio Management</b></summary>
  <p><b>编号</b>：[227]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.02134</p>
  <p><b>作者</b>：Charl Maree,  Christian W. Omlin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：reallocation of funds, Stock portfolio optimization, risk and sustainability, selection of stocks, Stock portfolio</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Stock portfolio optimization is the process of continuous reallocation of
funds to a selection of stocks. This is a particularly well-suited problem for
reinforcement learning, as daily rewards are compounding and objective
functions may include more than just profit, e.g., risk and sustainability. We
developed a novel utility function with the Sharpe ratio representing risk and
the environmental, social, and governance score (ESG) representing
sustainability. We show that a state-of-the-art policy gradient method -
multi-agent deep deterministic policy gradients (MADDPG) - fails to find the
optimum policy due to flat policy gradients and we therefore replaced gradient
descent with a genetic algorithm for parameter optimization. We show that our
system outperforms MADDPG while improving on deep Q-learning approaches by
allowing for continuous action spaces. Crucially, by incorporating risk and
sustainability criteria in the utility function, we improve on the
state-of-the-art in reinforcement learning for portfolio optimization; risk and
sustainability are essential in any modern trading strategy and we propose a
system that does not merely report these metrics, but that actively optimizes
the portfolio to improve on them.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：ACT-Net: Asymmetric Co-Teacher Network for Semi-supervised  Memory-efficient Medical Image Segmentation</b></summary>
  <p><b>编号</b>：[239]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01900</p>
  <p><b>作者</b>：Ziyuan Zhao,  Andong Zhu,  Zeng Zeng,  Bharadwaj Veeravalli,  Cuntai Guan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：shown promising performance, medical image segmentation, well-annotated data, difficult to access, shown promising</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While deep models have shown promising performance in medical image
segmentation, they heavily rely on a large amount of well-annotated data, which
is difficult to access, especially in clinical practice. On the other hand,
high-accuracy deep models usually come in large model sizes, limiting their
employment in real scenarios. In this work, we propose a novel asymmetric
co-teacher framework, ACT-Net, to alleviate the burden on both expensive
annotations and computational costs for semi-supervised knowledge distillation.
We advance teacher-student learning with a co-teacher network to facilitate
asymmetric knowledge distillation from large models to small ones by
alternating student and teacher roles, obtaining tiny but accurate models for
clinical employment. To verify the effectiveness of our ACT-Net, we employ the
ACDC dataset for cardiac substructure segmentation in our experiments.
Extensive experimental results demonstrate that ACT-Net outperforms other
knowledge distillation methods and achieves lossless segmentation performance
with 250x fewer parameters.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：MMGL: Multi-Scale Multi-View Global-Local Contrastive learning for  Semi-supervised Cardiac Image Segmentation</b></summary>
  <p><b>编号</b>：[240]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2207.01883</p>
  <p><b>作者</b>：Ziyuan Zhao,  Jinxuan Hu,  Zeng Zeng,  Xulei Yang,  Peisheng Qian,  Bharadwaj Veeravalli,  Cuntai Guan</p>
  <p><b>备注</b>：Accepted by IEEE International Conference on Image Processing (ICIP 2022)</p>
  <p><b>关键词</b>：shown significant success, large-scale well-labeled datasets, large-scale well-labeled, significant success, success in medical</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With large-scale well-labeled datasets, deep learning has shown significant
success in medical image segmentation. However, it is challenging to acquire
abundant annotations in clinical practice due to extensive expertise
requirements and costly labeling efforts. Recently, contrastive learning has
shown a strong capacity for visual representation learning on unlabeled data,
achieving impressive performance rivaling supervised learning in many domains.
In this work, we propose a novel multi-scale multi-view global-local
contrastive learning (MMGL) framework to thoroughly explore global and local
features from different scales and views for robust contrastive learning
performance, thereby improving segmentation performance with limited
annotations. Extensive experiments on the MM-WHS dataset demonstrate the
effectiveness of MMGL framework on semi-supervised cardiac image segmentation,
outperforming the state-of-the-art contrastive learning methods by a large
margin.</p>
  </details>
</details>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">徐耀彬</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://louishsu.xyz/2022/07/06/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">http://louishsu.xyz/2022/07/06/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://louishsu.xyz" target="_blank">LOUIS' BLOG</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2021/10/22/%E4%B8%AD%E5%9B%BD%E6%B3%95%E5%BE%8B%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E8%AF%84%E6%B5%8B(CAIL2021)%EF%BC%9A%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96(Rank2).html"><img class="next-cover" src="http://cail.cipsc.org.cn/img/index_mainpic.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">徐耀彬</div><div class="author-info__description">做知识的原创者！</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">11</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/isLouisHsu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/isLouisHsu" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:is.louishsu@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">记录和分享一些学习和开源内容，若有问题可通过邮箱is.louishsu@foxmail.com联系，欢迎交流！！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">统计</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">计算机视觉</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">自然语言处理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">机器学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">人工智能</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/07/06/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2022-07-06)"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv每日速递(2022-07-06)"/></a><div class="content"><a class="title" href="/2022/07/06/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2022-07-06)">Arxiv每日速递(2022-07-06)</a><time datetime="2022-07-06T00:48:42.989Z" title="发表于 2022-07-06 08:48:42">2022-07-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/10/22/%E4%B8%AD%E5%9B%BD%E6%B3%95%E5%BE%8B%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E8%AF%84%E6%B5%8B(CAIL2021)%EF%BC%9A%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96(Rank2).html" title="中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)"><img src="http://cail.cipsc.org.cn/img/index_mainpic.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)"/></a><div class="content"><a class="title" href="/2021/10/22/%E4%B8%AD%E5%9B%BD%E6%B3%95%E5%BE%8B%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E8%AF%84%E6%B5%8B(CAIL2021)%EF%BC%9A%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96(Rank2).html" title="中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)">中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)</a><time datetime="2021-10-22T14:29:06.000Z" title="发表于 2021-10-22 22:29:06">2021-10-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/05/19/%E5%85%A8%E7%90%83%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0%E5%A4%A7%E8%B5%9B%E3%80%90%E8%B5%9B%E9%81%93%E4%B8%80%E3%80%91%EF%BC%9A%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E6%8A%A5%E5%91%8A%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B.html" title="全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测"><img src="https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161037709574435991610377095138.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测"/></a><div class="content"><a class="title" href="/2021/05/19/%E5%85%A8%E7%90%83%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0%E5%A4%A7%E8%B5%9B%E3%80%90%E8%B5%9B%E9%81%93%E4%B8%80%E3%80%91%EF%BC%9A%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E6%8A%A5%E5%91%8A%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B.html" title="全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测">全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测</a><time datetime="2021-05-19T09:57:06.000Z" title="发表于 2021-05-19 17:57:06">2021-05-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/09/16/%E8%AF%A6%E8%A7%A3%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%EF%BC%9ALSTM-CRF.html" title="详解命名实体识别模型：LSTM-CRF"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fpic1.zhimg.com%2Fv2-1ed6a10dbb239a148e42df088c5870a6_1440w.jpg%3Fsource%3D172ae18b&amp;refer=http%3A%2F%2Fpic1.zhimg.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1638183974&amp;t=4632f13fd487ed1cfcb12d67a6b71e52" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="详解命名实体识别模型：LSTM-CRF"/></a><div class="content"><a class="title" href="/2020/09/16/%E8%AF%A6%E8%A7%A3%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%EF%BC%9ALSTM-CRF.html" title="详解命名实体识别模型：LSTM-CRF">详解命名实体识别模型：LSTM-CRF</a><time datetime="2020-09-16T09:26:44.000Z" title="发表于 2020-09-16 17:26:44">2020-09-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/06/14/%E8%AE%B0%E4%B8%80%E6%AC%A1MySQL%E8%A7%A3%E5%86%B3%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E6%97%B6%E7%9A%84%E5%88%86%E8%A1%A8%E9%97%AE%E9%A2%98.html" title="记一次MySQL解决大数据存储时的分表问题"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="记一次MySQL解决大数据存储时的分表问题"/></a><div class="content"><a class="title" href="/2020/06/14/%E8%AE%B0%E4%B8%80%E6%AC%A1MySQL%E8%A7%A3%E5%86%B3%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E6%97%B6%E7%9A%84%E5%88%86%E8%A1%A8%E9%97%AE%E9%A2%98.html" title="记一次MySQL解决大数据存储时的分表问题">记一次MySQL解决大数据存储时的分表问题</a><time datetime="2020-06-14T15:24:27.000Z" title="发表于 2020-06-14 23:24:27">2020-06-14</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2022 By 徐耀彬</div><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender"title="主题采用butterfly"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr"title="本站使用JsDelivr为静态资源提供CDN加速"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub"title="本站项目由Gtihub托管"alt="img"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"alt="img"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></br></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    let initData = {
      el: '#twikoo-wrap',
      envId: 'blog-',
      region: ''
    }

    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    twikoo.init(initData)
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'blog-',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/机器学习/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐱 机器学习 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/自然语言处理/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 自然语言处理 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/竞赛相关/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 竞赛相关 (2)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/阅读笔记/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 阅读笔记 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="http://louishsu.xyz/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style>
  <script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/hexo-github-calendar@1.21/hexo_githubcalendar.js"></script>
  <script data-pjax>
        function GithubCalendarConfig(){
            var git_githubapiurl ="https://python-github-calendar-api.vercel.app/api?isLouisHsu";
            var git_color =['#ebedf0', '#fdcdec', '#fc9bd9', '#fa6ac5', '#f838b2', '#f5089f', '#c4067e', '#92055e', '#540336', '#48022f', '#30021f'];
            var git_user ="isLouisHsu";
            var parent_div_git = document.getElementById('recent-posts');
            var git_div_html = '<div class="recent-post-item" style="width:100%;height:auto;padding:10px;"><div id="github_loading" style="width:10%;height:100%;margin:0 auto;display: block"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"  viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animateTransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animateTransform></path></svg></div><div id="github_container"></div></div>';
            if(parent_div_git && location.pathname =='/'){
                console.log('已挂载github calendar')
                // parent_div_git.innerHTML=git_div_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",git_div_html) // 有报错，但不影响使用(支持pjax跳转)
            };
            GithubCalendar(git_githubapiurl,git_color,git_user)
        }
        if(document.getElementById('recent-posts')){
            GithubCalendarConfig()
        }
    </script>
    <style>#github_container{min-height:280px}@media screen and (max-width:650px) {#github_container{background-image:;min-height:0px}}</style>
    <style></style><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>