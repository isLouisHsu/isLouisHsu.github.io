<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Arxiv每日速递(2022-11-21) | LOUIS' BLOG</title><meta name="author" content="徐耀彬"><meta name="copyright" content="徐耀彬"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。 统计 今日共更新288篇论文，其中：  75篇计算机视觉（cs.CV） 37篇自然语言处理（cs.CL） 101篇机器学习（cs.LG） 42篇人工智能（cs.AI）  计算机视觉    1. 标题：SPACEx: Speech-driven Portrait Animat">
<meta property="og:type" content="article">
<meta property="og:title" content="Arxiv每日速递(2022-11-21)">
<meta property="og:url" content="http://louishsu.xyz/2022/11/21/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">
<meta property="og:site_name" content="LOUIS&#39; BLOG">
<meta property="og:description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。 统计 今日共更新288篇论文，其中：  75篇计算机视觉（cs.CV） 37篇自然语言处理（cs.CL） 101篇机器学习（cs.LG） 42篇人工智能（cs.AI）  计算机视觉    1. 标题：SPACEx: Speech-driven Portrait Animat">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png">
<meta property="article:published_time" content="2022-11-21T00:46:25.539Z">
<meta property="article:modified_time" content="2022-11-21T00:47:58.951Z">
<meta property="article:author" content="徐耀彬">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://louishsu.xyz/2022/11/21/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-11-21 08:47:58'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/background.css"><script src="https://cdn.jsdelivr.net/npm/echarts@4.7.0/dist/echarts.min.js"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">12</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">LOUIS' BLOG</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Arxiv每日速递(2022-11-21)<a class="post-edit-link" href="https://github.com/isLouisHsu/blog/tree/master/source_posts/Arxiv每日速递.md" title="编辑" target="_blank"><i class="fas fa-pencil-square"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-11-21T00:46:25.539Z" title="发表于 2022-11-21 08:46:25">2022-11-21</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-11-21T00:47:58.951Z" title="更新于 2022-11-21 08:47:58">2022-11-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">阅读笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">21.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>129分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2022/11/21/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。</p>
<h1>统计</h1>
<p>今日共更新288篇论文，其中：</p>
<ul>
<li>75篇计算机视觉（<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>）</li>
<li>37篇自然语言处理（<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>）</li>
<li>101篇机器学习（cs.LG）</li>
<li>42篇人工智能（<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>）</li>
</ul>
<h1>计算机视觉</h1>
<details>
  <summary>1. <b>标题：SPACEx: Speech-driven Portrait Animation with Controllable Expression</b></summary>
  <p><b>编号</b>：[1]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09809</p>
  <p><b>作者</b>：Siddharth Gururani,  Arun Mallya,  Ting-Chun Wang,  Rafael Valle,  Ming-Yu Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：received growing attention, Animating portraits, recent years, practical use cases, received growing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Animating portraits using speech has received growing attention in recent
years, with various creative and practical use cases. An ideal generated video
should have good lip sync with the audio, natural facial expressions and head
motions, and high frame quality. In this work, we present SPACEx, which uses
speech and a single image to generate high-resolution, and expressive videos
with realistic head pose, without requiring a driving video. It uses a
multi-stage approach, combining the controllability of facial landmarks with
the high-quality synthesis power of a pretrained face generator. SPACEx also
allows for the control of emotions and their intensities. Our method
outperforms prior methods in objective metrics for image quality and facial
motions and is strongly preferred by users in pair-wise comparisons. The
project website is available at this https URL</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and  Vision-Language Tasks</b></summary>
  <p><b>编号</b>：[2]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09808</p>
  <p><b>作者</b>：Hao Li,  Jinguo Zhu,  Xiaohu Jiang,  Xizhou Zhu,  Hongsheng Li,  Chun Yuan,  Xiaohua Wang,  Yu Qiao,  Xiaogang Wang,  Wenhai Wang,  Jifeng Dai</p>
  <p><b>备注</b>：Code shall be released at this https URL</p>
  <p><b>关键词</b>：general perception modeling, fine-tuning paradigm makes, remarkable success, success of foundation, paradigm makes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite the remarkable success of foundation models, their task-specific
fine-tuning paradigm makes them inconsistent with the goal of general
perception modeling. The key to eliminating this inconsistency is to use
generalist models for general task modeling. However, existing attempts at
generalist models are inadequate in both versatility and performance. In this
paper, we propose Uni-Perceiver v2, which is the first generalist model capable
of handling major large-scale vision and vision-language tasks with competitive
performance. Specifically, images are encoded as general region proposals,
while texts are encoded via a Transformer-based language model. The encoded
representations are transformed by a task-agnostic decoder. Different tasks are
formulated as a unified maximum likelihood estimation problem. We further
propose an improved optimizer to ensure stable multi-task learning with an
unmixed sampling strategy, which is helpful for tasks requiring large
batch-size training. After being jointly trained on various tasks,
Uni-Perceiver v2 is capable of directly handling downstream tasks without any
task-specific adaptation. Results show that Uni-Perceiver v2 outperforms all
existing generalist models in both versatility and performance. Meanwhile,
compared with the commonly-recognized strong baselines that require
tasks-specific fine-tuning, Uni-Perceiver v2 achieves competitive performance
on a broad range of vision and vision-language tasks.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Towards All-in-one Pre-training via Maximizing Multi-modal Mutual  Information</b></summary>
  <p><b>编号</b>：[3]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09807</p>
  <p><b>作者</b>：Weijie Su,  Xizhou Zhu,  Chenxin Tao,  Lewei Lu,  Bin Li,  Gao Huang,  Yu Qiao,  Xiaogang Wang,  Jie Zhou,  Jifeng Dai</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large-scale models, pre-training strategies supported, pre-training, effectively exploit, exploit the potential</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To effectively exploit the potential of large-scale models, various
pre-training strategies supported by massive data from different sources are
proposed, including supervised pre-training, weakly-supervised pre-training,
and self-supervised pre-training. It has been proved that combining multiple
pre-training strategies and data from various modalities/sources can greatly
boost the training of large-scale models. However, current works adopt a
multi-stage pre-training system, where the complex pipeline may increase the
uncertainty and instability of the pre-training. It is thus desirable that
these strategies can be integrated in a single-stage manner. In this paper, we
first propose a general multi-modal mutual information formula as a unified
optimization target and demonstrate that all existing approaches are special
cases of our framework. Under this unified perspective, we propose an
all-in-one single-stage pre-training approach, named Maximizing Multi-modal
Mutual Information Pre-training (M3I Pre-training). Our approach achieves
better performance than previous pre-training methods on various vision
benchmarks, including ImageNet classification, COCO object detection, LVIS
long-tailed object detection, and ADE20k semantic segmentation. Notably, we
successfully pre-train a billion-level parameter image backbone and achieve
state-of-the-art performance on various benchmarks. Code shall be released.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：InstructPix2Pix: Learning to Follow Image Editing Instructions</b></summary>
  <p><b>编号</b>：[4]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09800</p>
  <p><b>作者</b>：Tim Brooks,  Aleksander Holynski,  Alexei A. Efros</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：propose a method, Stable Diffusion, model, images, instructions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a method for editing images from human instructions: given an
input image and a written instruction that tells the model what to do, our
model follows these instructions to edit the image. To obtain training data for
this problem, we combine the knowledge of two large pretrained models -- a
language model (GPT-3) and a text-to-image model (Stable Diffusion) -- to
generate a large dataset of image editing examples. Our conditional diffusion
model, InstructPix2Pix, is trained on our generated data, and generalizes to
real images and user-written instructions at inference time. Since it performs
edits in the forward pass and does not require per example fine-tuning or
inversion, our model edits images quickly, in a matter of seconds. We show
compelling editing results for a diverse collection of input images and written
instructions.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：CAE v2: Context Autoencoder with CLIP Target</b></summary>
  <p><b>编号</b>：[5]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09799</p>
  <p><b>作者</b>：Xinyu Zhang,  Jiahui Chen,  Junkun Yuan,  Qiang Chen,  Jian Wang,  Xiaodi Wang,  Shumin Han,  Xiaokang Chen,  Jimin Pi,  Kun Yao,  Junyu Han,  Errui Ding,  Jingdong Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：reconstructing image patches, Masked image modeling, learns visual representation, image modeling, reconstructing image</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Masked image modeling (MIM) learns visual representation by masking and
reconstructing image patches. Applying the reconstruction supervision on the
CLIP representation has been proven effective for MIM. However, it is still
under-explored how CLIP supervision in MIM influences performance. To
investigate strategies for refining the CLIP-targeted MIM, we study two
critical elements in MIM, i.e., the supervision position and the mask ratio,
and reveal two interesting perspectives, relying on our developed simple
pipeline, context autodecoder with CLIP target (CAE v2). Firstly, we observe
that the supervision on visible patches achieves remarkable performance, even
better than that on masked patches, where the latter is the standard format in
the existing MIM methods. Secondly, the optimal mask ratio positively
correlates to the model size. That is to say, the smaller the model, the lower
the mask ratio needs to be. Driven by these two discoveries, our simple and
concise approach CAE v2 achieves superior performance on a series of downstream
tasks. For example, a vanilla ViT-Large model achieves 81.7% and 86.7% top-1
accuracy on linear probing and fine-tuning on ImageNet-1K, and 55.9% mIoU on
semantic segmentation on ADE20K with the pre-training for 300 epochs. We hope
our findings can be helpful guidelines for the pre-training in the MIM area,
especially for the small-scale models.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Conffusion: Confidence Intervals for Diffusion Models</b></summary>
  <p><b>编号</b>：[6]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09795</p>
  <p><b>作者</b>：Eliahu Horwitz,  Yedid Hoshen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：super-resolution and inpainting, generation tasks, Diffusion models, generative tasks, go-to method</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diffusion models have become the go-to method for many generative tasks,
particularly for image-to-image generation tasks such as super-resolution and
inpainting. Current diffusion-based methods do not provide statistical
guarantees regarding the generated results, often preventing their use in
high-stakes situations. To bridge this gap, we construct a confidence interval
around each generated pixel such that the true value of the pixel is guaranteed
to fall within the interval with a probability set by the user. Since diffusion
models parametrize the data distribution, a straightforward way of constructing
such intervals is by drawing multiple samples and calculating their bounds.
However, this method has several drawbacks: i) slow sampling speeds ii)
suboptimal bounds iii) requires training a diffusion model per task. To
mitigate these shortcomings we propose Conffusion, wherein we fine-tune a
pre-trained diffusion model to predict interval bounds in a single forward
pass. We show that Conffusion outperforms the baseline method while being three
orders of magnitude faster.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Null-text Inversion for Editing Real Images using Guided Diffusion  Models</b></summary>
  <p><b>编号</b>：[7]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09794</p>
  <p><b>作者</b>：Ron Mokady,  Amir Hertz,  Kfir Aberman,  Yael Pritch,  Daniel Cohen-Or</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Recent text-guided diffusion, image generation capabilities, powerful image generation, Recent text-guided, generation capabilities</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent text-guided diffusion models provide powerful image generation
capabilities. Currently, a massive effort is given to enable the modification
of these images using text only as means to offer intuitive and versatile
editing. To edit a real image using these state-of-the-art tools, one must
first invert the image with a meaningful text prompt into the pretrained
model's domain. In this paper, we introduce an accurate inversion technique and
thus facilitate an intuitive text-based modification of the image. Our proposed
inversion consists of two novel key components: (i) Pivotal inversion for
diffusion models. While current methods aim at mapping random noise samples to
a single input image, we use a single pivotal noise vector for each timestamp
and optimize around it. We demonstrate that a direct inversion is inadequate on
its own, but does provide a good anchor for our optimization. (ii) NULL-text
optimization, where we only modify the unconditional textual embedding that is
used for classifier-free guidance, rather than the input text embedding. This
allows for keeping both the model weights and the conditional embedding intact
and hence enables applying prompt-based editing while avoiding the cumbersome
tuning of the model's weights. Our Null-text inversion, based on the publicly
available Stable Diffusion model, is extensively evaluated on a variety of
images and prompt editing, showing high-fidelity editing of real images.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：MOTRv2: Bootstrapping End-to-End Multi-Object Tracking by Pretrained  Object Detectors</b></summary>
  <p><b>编号</b>：[8]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09791</p>
  <p><b>作者</b>：Yuang Zhang,  Tiancai Wang,  Xiangyu Zhang</p>
  <p><b>备注</b>：Tech report</p>
  <p><b>关键词</b>：pretrained object detector, extra object detector, object detector, MOTR, multi-object tracking</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we propose MOTRv2, a simple yet effective pipeline to
bootstrap end-to-end multi-object tracking with a pretrained object detector.
Existing end-to-end methods, e.g. MOTR and TrackFormer, are inferior to their
tracking-by-detection counterparts mainly due to their poor detection
performance. We aim to improve MOTR by elegantly incorporating an extra object
detector. We first adopt the anchor formulation of queries and then use an
extra object detector to generate proposals as anchors, providing detection
prior to MOTR. The simple modification greatly eases the conflict between joint
learning detection and association tasks in MOTR. MOTRv2 keeps the end-to-end
feature and scales well on large-scale benchmarks. MOTRv2 ranks the 1st place
(73.4% HOTA on DanceTrack) in the 1st Multiple People Tracking in Group Dance
Challenge. Moreover, MOTRv2 achieves state-of-the-art performance on BDD100K
dataset. We hope this simple and effective pipeline can provide some new
insights to the end-to-end MOT community. Code is available at
\url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：ConStruct-VL: Data-Free Continual Structured VL Concepts Learning</b></summary>
  <p><b>编号</b>：[9]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09790</p>
  <p><b>作者</b>：James Seale Smith,  Paola Cascante-Bonilla,  Assaf Arbelle,  Donghyun Kim,  Rameswar Panda,  David Cox,  Diyi Yang,  Zsolt Kira,  Rogerio Feris,  Leonid Karlinsky</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：achieving competitive results, short text prompts, demonstrated remarkable capabilities, recognizing objects defined, large-scale pre-trained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, large-scale pre-trained Vision-and-Language (VL) foundation models
have demonstrated remarkable capabilities in many zero-shot downstream tasks,
achieving competitive results for recognizing objects defined by as little as
short text prompts. However, it has also been shown that VL models are still
brittle in Structured VL Concept (SVLC) reasoning, such as the ability to
recognize object attributes, states, and inter-object relations. This leads to
reasoning mistakes, which need to be corrected as they occur by teaching VL
models the missing SVLC skills; often this must be done using private data
where the issue was found, which naturally leads to a data-free continual (no
task-id) VL learning setting. In this work, we introduce the first Continual
Data-Free Structured VL Concepts Learning (ConStruct-VL) benchmark and show it
is challenging for many existing data-free CL strategies. We, therefore,
propose a data-free method comprised of a new approach of Adversarial
Pseudo-Replay (APR) which generates adversarial reminders of past tasks from
past task models. To use this method efficiently, we also propose a continual
parameter-efficient Layered-LoRA (LaLo) neural architecture allowing
no-memory-cost access to all past models at train time. We show this approach
outperforms all data-free methods by as much as ~7% while even matching some
levels of experience-replay (prohibitive for applications where data-privacy
must be preserved).</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：DiffusionDet: Diffusion Model for Object Detection</b></summary>
  <p><b>编号</b>：[10]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09788</p>
  <p><b>作者</b>：Shoufa Chen,  Peize Sun,  Yibing Song,  Ping Luo</p>
  <p><b>备注</b>：Tech report. Code is available at this https URL</p>
  <p><b>关键词</b>：denoising diffusion process, framework that formulates, denoising diffusion, boxes, diffusion process</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose DiffusionDet, a new framework that formulates object detection as
a denoising diffusion process from noisy boxes to object boxes. During training
stage, object boxes diffuse from ground-truth boxes to random distribution, and
the model learns to reverse this noising process. In inference, the model
refines a set of randomly generated boxes to the output results in a
progressive way. The extensive evaluations on the standard benchmarks,
including MS-COCO and LVIS, show that DiffusionDet achieves favorable
performance compared to previous well-established detectors. Our work brings
two important findings in object detection. First, random boxes, although
drastically different from pre-defined anchors or learned queries, are also
effective object candidates. Second, object detection, one of the
representative perception tasks, can be solved by a generative way. Our code is
available at this https URL.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：SE(3)-Equivariant Relational Rearrangement with Neural Descriptor Fields</b></summary>
  <p><b>编号</b>：[12]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09786</p>
  <p><b>作者</b>：Anthony Simeonov,  Yilun Du,  Lin Yen-Chen,  Alberto Rodriguez,  Leslie Pack Kaelbling,  Tomas Lozano-Perez,  Pulkit Agrawal</p>
  <p><b>备注</b>：CoRL 2022, first two authors contributed equally, website and code: this https URL</p>
  <p><b>关键词</b>：point cloud observations, involving spatial relations, arbitrary poses directly, performing tasks involving, tasks involving spatial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a method for performing tasks involving spatial relations between
novel object instances initialized in arbitrary poses directly from point cloud
observations. Our framework provides a scalable way for specifying new tasks
using only 5-10 demonstrations. Object rearrangement is formalized as the
question of finding actions that configure task-relevant parts of the object in
a desired alignment. This formalism is implemented in three steps: assigning a
consistent local coordinate frame to the task-relevant object parts,
determining the location and orientation of this coordinate frame on unseen
object instances, and executing an action that brings these frames into the
desired alignment. We overcome the key technical challenge of determining
task-relevant local coordinate frames from a few demonstrations by developing
an optimization method based on Neural Descriptor Fields (NDFs) and a single
annotated 3D keypoint. An energy-based learning scheme to model the joint
configuration of the objects that satisfies a desired relational task further
improves performance. The method is tested on three multi-object rearrangement
tasks in simulation and on a real robot. Project website, videos, and code:
this https URL</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Assessing Neural Network Robustness via Adversarial Pivotal Tuning</b></summary>
  <p><b>编号</b>：[14]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09782</p>
  <p><b>作者</b>：Peter Ebert Christensen,  Vésteinn Snæbjarnarson,  Andrea Dittadi,  Serge Belongie,  Sagie Benaim</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：manipulations, image, APT, ability to assess, pretrained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The ability to assess the robustness of image classifiers to a diverse set of
manipulations is essential to their deployment in the real world. Recently,
semantic manipulations of real images have been considered for this purpose, as
they may not arise using standard adversarial settings. However, such semantic
manipulations are often limited to style, color or attribute changes. While
expressive, these manipulations do not consider the full capacity of a
pretrained generator to affect adversarial image manipulations. In this work,
we aim at leveraging the full capacity of a pretrained image generator to
generate highly detailed, diverse and photorealistic image manipulations.
Inspired by recent GAN-based image inversion methods, we propose a method
called Adversarial Pivotal Tuning (APT). APT first finds a pivot latent space
input to a pretrained generator that best reconstructs an input image. It then
adjusts the weights of the generator to create small, but semantic,
manipulations which fool a pretrained classifier. Crucially, APT changes both
the input and the weights of the pretrained generator, while preserving its
expressive latent editing capability, thus allowing the use of its full
capacity in creating semantic adversarial manipulations. We demonstrate that
APT generates a variety of semantic image manipulations, which preserve the
input image class, but which fool a variety of pretrained classifiers. We
further demonstrate that classifiers trained to be robust to other robustness
benchmarks, are not robust to our generated manipulations and propose an
approach to improve the robustness towards our generated manipulations. Code
available at: this https URL</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：I Can't Believe There's No Images! Learning Visual Tasks Using only  Language Data</b></summary>
  <p><b>编号</b>：[15]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09778</p>
  <p><b>作者</b>：Sophia Gu,  Christopher Clark,  Aniruddha Kembhavi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：natural language processing, computer vision tasks, comparing and contrasting, contrasting semantics, writing descriptions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Many high-level skills that are required for computer vision tasks, such as
parsing questions, comparing and contrasting semantics, and writing
descriptions, are also required in other domains such as natural language
processing. In this paper, we ask whether this makes it possible to learn those
skills from text data and then use them to complete vision tasks without ever
training on visual training data. Key to our approach is exploiting the joint
embedding space of contrastively trained vision and language encoders. In
practice, there can be systematic differences between embedding spaces for
different modalities in contrastive models, and we analyze how these
differences affect our approach and study a variety of strategies to mitigate
this concern. We produce models using only text training data on three tasks:
image captioning, visual entailment and visual question answering, and evaluate
them on standard benchmarks using images. We find that this kind of transfer is
possible and results in only a small drop in performance relative to models
trained on images. We also showcase a variety of stylistic image captioning
models that were trained using no image data and no human-curated language
data, but instead text data from books, the web, or language models.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：T-SEA: Transfer-based Self-Ensemble Attack on Object Detection</b></summary>
  <p><b>编号</b>：[17]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09773</p>
  <p><b>作者</b>：Hao Huang,  Ziyan Chen,  Huanran Chen,  Yongtao Wang,  Kevin Zhang</p>
  <p><b>备注</b>：10 pages, 5 figures</p>
  <p><b>关键词</b>：query-based black-box attacks, transfer-based black-box attacks, Compared to query-based, ensures their secrecy, transfer-based black-box</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Compared to query-based black-box attacks, transfer-based black-box attacks
do not require any information of the attacked models, which ensures their
secrecy. However, most existing transfer-based approaches rely on ensembling
multiple models to boost the attack transferability, which is time- and
resource-intensive, not to mention the difficulty of obtaining diverse models
on the same task. To address this limitation, in this work, we focus on the
single-model transfer-based black-box attack on object detection, utilizing
only one model to achieve a high-transferability adversarial attack on multiple
black-box detectors. Specifically, we first make observations on the patch
optimization process of the existing method and propose an enhanced attack
framework by slightly adjusting its training strategies. Then, we analogize
patch optimization with regular model optimization, proposing a series of
self-ensemble approaches on the input data, the attacked model, and the
adversarial patch to efficiently make use of the limited information and
prevent the patch from overfitting. The experimental results show that the
proposed framework can be applied with multiple classical base attack methods
(e.g., PGD and MIM) to greatly improve the black-box transferability of the
well-optimized patch on multiple mainstream detectors, meanwhile boosting
white-box performance. Our code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Boosting Object Representation Learning via Motion and Object Continuity</b></summary>
  <p><b>编号</b>：[18]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09771</p>
  <p><b>作者</b>：Quentin Delfosse,  Wolfgang Stammer,  Thomas Rothenbacher,  Dwarak Vittal,  Kristian Kersting</p>
  <p><b>备注</b>：8 pages main text, 32 tables, 21 Figures</p>
  <p><b>关键词</b>：Recent unsupervised multi-object, architectural inductive biases, unsupervised multi-object detection, shown impressive performance, Recent unsupervised</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent unsupervised multi-object detection models have shown impressive
performance improvements, largely attributed to novel architectural inductive
biases. Unfortunately, they may produce suboptimal object encodings for
downstream tasks. To overcome this, we propose to exploit object motion and
continuity, i.e., objects do not pop in and out of existence. This is
accomplished through two mechanisms: (i) providing priors on the location of
objects through integration of optical flow, and (ii) a contrastive object
continuity loss across consecutive image frames. Rather than developing an
explicit deep architecture, the resulting Motion and Object Continuity (MOC)
scheme can be instantiated using any baseline object detection model. Our
results show large improvements in the performances of a SOTA model in terms of
object discovery, convergence speed and overall latent object representations,
particularly for playing Atari games. Overall, we show clear benefits of
integrating motion and object continuity for downstream tasks, moving beyond
object representation learning based only on reconstruction.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：3DLatNav: Navigating Generative Latent Spaces for Semantic-Aware 3D  Object Manipulation</b></summary>
  <p><b>编号</b>：[19]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09770</p>
  <p><b>作者</b>：Amaya Dharmasiri,  Dinithi Dissanayake,  Mohamed Afham,  Isuru Dissanayake,  Ranga Rodrigo,  Kanchana Thilakarathna</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：shape semantics, point clouds, generative latent spaces, recently successful, successful in generating</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>3D generative models have been recently successful in generating realistic 3D
objects in the form of point clouds. However, most models do not offer
controllability to manipulate the shape semantics of component object parts
without extensive semantic attribute labels or other reference point clouds.
Moreover, beyond the ability to perform simple latent vector arithmetic or
interpolations, there is a lack of understanding of how part-level semantics of
3D shapes are encoded in their corresponding generative latent spaces. In this
paper, we propose 3DLatNav; a novel approach to navigating pretrained
generative latent spaces to enable controlled part-level semantic manipulation
of 3D objects. First, we propose a part-level weakly-supervised shape semantics
identification mechanism using latent representations of 3D shapes. Then, we
transfer that knowledge to a pretrained 3D object generative latent space to
unravel disentangled embeddings to represent different shape semantics of
component parts of an object in the form of linear subspaces, despite the
unavailability of part-level labels during the training. Finally, we utilize
those identified subspaces to show that controllable 3D object part
manipulation can be achieved by applying the proposed framework to any
pretrained 3D generative model. With two novel quantitative metrics to evaluate
the consistency and localization accuracy of part-level manipulations, we show
that 3DLatNav outperforms existing unsupervised latent disentanglement methods
in identifying latent directions that encode part-level shape semantics of 3D
objects. With multiple ablation studies and testing on state-of-the-art
generative models, we show that 3DLatNav can implement controlled part-level
semantic manipulations on an input point cloud while preserving other features
and the realistic nature of the object.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：D$^3$ETR: Decoder Distillation for Detection Transformer</b></summary>
  <p><b>编号</b>：[20]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09768</p>
  <p><b>作者</b>：Xiaokang Chen,  Jiahui Chen,  Yan Liu,  Gang Zeng</p>
  <p><b>备注</b>：Work in progress</p>
  <p><b>关键词</b>：improving small students, DETR-based detectors, Adaptive Matching, matching, CNN-based detectors show</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While various knowledge distillation (KD) methods in CNN-based detectors show
their effectiveness in improving small students, the baselines and recipes for
DETR-based detectors are yet to be built. In this paper, we focus on the
transformer decoder of DETR-based detectors and explore KD methods for them.
The outputs of the transformer decoder lie in random order, which gives no
direct correspondence between the predictions of the teacher and the student,
thus posing a challenge for knowledge distillation. To this end, we propose
MixMatcher to align the decoder outputs of DETR-based teachers and students,
which mixes two teacher-student matching strategies, i.e., Adaptive Matching
and Fixed Matching. Specifically, Adaptive Matching applies bipartite matching
to adaptively match the outputs of the teacher and the student in each decoder
layer, while Fixed Matching fixes the correspondence between the outputs of the
teacher and the student with the same object queries, with the teacher's fixed
object queries fed to the decoder of the student as an auxiliary group.
Based on MixMatcher, we build \textbf{D}ecoder \textbf{D}istillation for
\textbf{DE}tection \textbf{TR}ansformer (D$^3$ETR), which distills knowledge in
decoder predictions and attention maps from the teachers to students. D$^3$ETR
shows superior performance on various DETR-based detectors with different
backbones. For example, D$^3$ETR improves Conditional DETR-R50-C5 by
$\textbf{7.8}/\textbf{2.4}$ mAP under $12/50$ epochs training settings with
Conditional DETR-R101-C5 as the teacher.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Sources of performance variability in deep learning-based polyp  detection</b></summary>
  <p><b>编号</b>：[47]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09708</p>
  <p><b>作者</b>：Thuy Nuong Tran,  Tim Adler,  Amine Yamlahi,  Evangelia Christodoulou,  Patrick Godau,  Annika Reinke,  Minu Dietlinde Tizabi,  Peter Sauer,  Tillmann Persicke,  Jörg Gerhard Albert,  Lena Maier-Hein</p>
  <p><b>备注</b>：12 pages, 9 figures, 3 tables. Submitted to IPCAI 2023</p>
  <p><b>关键词</b>：potential clinical translation, translation of methods, key prerequisite, reliable tracking, tracking of scientific</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Validation metrics are a key prerequisite for the reliable tracking of
scientific progress and for deciding on the potential clinical translation of
methods. While recent initiatives aim to develop comprehensive theoretical
frameworks for understanding metric-related pitfalls in image analysis
problems, there is a lack of experimental evidence on the concrete effects of
common and rare pitfalls on specific applications. We address this gap in the
literature in the context of colon cancer screening. Our contribution is
twofold. Firstly, we present the winning solution of the Endoscopy computer
vision challenge (EndoCV) on colon cancer detection, conducted in conjunction
with the IEEE International Symposium on Biomedical Imaging (ISBI) 2022.
Secondly, we demonstrate the sensitivity of commonly used metrics to a range of
hyperparameters as well as the consequences of poor metric choices. Based on
comprehensive validation studies performed with patient data from six clinical
centers, we found all commonly applied object detection metrics to be subject
to high inter-center variability. Furthermore, our results clearly demonstrate
that the adaptation of standard hyperparameters used in the computer vision
community does not generally lead to the clinically most plausible results.
Finally, we present localization criteria that correspond well to clinical
relevance. Our work could be a first step towards reconsidering common
validation strategies in automatic colon cancer screening applications.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Listen, denoise, action! Audio-driven motion synthesis with diffusion  models</b></summary>
  <p><b>编号</b>：[48]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09707</p>
  <p><b>作者</b>：Simon Alexanderson,  Rajmund Nagy,  Jonas Beskow,  Gustav Eje Henter</p>
  <p><b>备注</b>：15 pages, 6 figures</p>
  <p><b>关键词</b>：efficiently trainable probabilistic, trainable probabilistic models, experienced a surge, expressive yet efficiently, efficiently trainable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diffusion models have experienced a surge of interest as highly expressive
yet efficiently trainable probabilistic models. We show that these models are
an excellent fit for synthesising human motion that co-occurs with audio, for
example co-speech gesticulation, since motion is complex and highly ambiguous
given audio, calling for a probabilistic description. Specifically, we adapt
the DiffWave architecture to model 3D pose sequences, putting Conformers in
place of dilated convolutions for improved accuracy. We also demonstrate
control over motion style, using classifier-free guidance to adjust the
strength of the stylistic expression. Gesture-generation experiments on the
Trinity Speech-Gesture and ZeroEGGS datasets confirm that the proposed method
achieves top-of-the-line motion quality, with distinctive styles whose
expression can be made more or less pronounced. We also synthesise dance motion
and path-driven locomotion using the same model architecture. Finally, we
extend the guidance procedure to perform style interpolation in a manner that
is appealing for synthesis tasks and has connections to product-of-experts
models, a contribution we believe is of independent interest. Video examples
are available at this https URL</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：EfficientTrain: Exploring Generalized Curriculum Learning for Training  Visual Backbones</b></summary>
  <p><b>编号</b>：[50]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09703</p>
  <p><b>作者</b>：Yulin Wang,  Yang Yue,  Rui Lu,  Tianjiao Liu,  Zhao Zhong,  Shiji Song,  Gao Huang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：costly training procedure, modern deep networks, superior performance, performance of modern, deep networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The superior performance of modern deep networks usually comes at the price
of a costly training procedure. In this paper, we present a novel curriculum
learning approach for the efficient training of visual backbones (e.g., vision
Transformers). The proposed method is inspired by the phenomenon that deep
networks mainly learn to recognize some 'easier-to-learn' discriminative
patterns within each example at earlier stages of training, e.g., the
lower-frequency components of images and the original information before data
augmentation. Driven by this observation, we propose a curriculum where the
model always leverages all the training data at each epoch, while the
curriculum starts with only exposing the 'easier-to-learn' patterns of each
example, and introduces gradually more difficult patterns. To implement this
idea, we 1) introduce a cropping operation in the Fourier spectrum of the
inputs, which enables the model to learn from only the lower-frequency
components efficiently, and 2) demonstrate that exposing the features of
original images amounts to adopting weaker data augmentation. Our resulting
algorithm, EfficientTrain, is simple, general, yet surprisingly effective. For
example, it reduces the training time of a wide variety of popular models
(e.g., ConvNeXts, DeiT, PVT, and Swin/CSWin Transformers) by more than
${1.5\times}$ on ImageNet-1K/22K without sacrificing the accuracy. It is
effective for self-supervised learning (i.e., MAE) as well. Code is available
at this https URL.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：PromptCap: Prompt-Guided Task-Aware Image Captioning</b></summary>
  <p><b>编号</b>：[53]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09699</p>
  <p><b>作者</b>：Yushi Hu,  Hang Hua,  Zhengyuan Yang,  Weijia Shi,  Noah A. Smith,  Jiebo Luo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：allowing powerful language, natural language sentence, allowing powerful, Image, powerful language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image captioning aims to describe an image with a natural language sentence,
allowing powerful language models to understand images. The framework of
combining image captioning with language models has been successful on various
vision-language tasks. However, an image contains much more information than a
single sentence, leading to underspecification of which visual entities should
be described in the caption sentence. For example, when performing visual
questioning answering (VQA), generic image captions often miss visual details
that are essential for the language model to answer correctly. To address this
challenge, we propose PromptCap, a captioning model that takes a
natural-language prompt to control the contents of the generated caption. The
prompt contains a question that the caption should help to answer, and also
supports taking auxiliary text inputs such as scene text within the image
itself. To finetune a general image caption model for prompt-guided captioning,
we propose a pipeline to synthesize and filter training examples with GPT-3 and
existing VQA datasets. For evaluation, we start with an existing pipeline in
which a language model is prompted with image captions to carry out VQA. With
the same language model, a higher QA accuracy shows that our generated captions
are more relevant to the question prompts. PromptCap outperforms generic
captions by a large margin on a variety of VQA tasks and achieves the
state-of-the-art accuracy of 58.8 % on OK-VQA and 58.0 % on A-OKVQA. Zero-shot
experiments on WebQA show that PromptCap generalizes well to unseen domains.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：AligNeRF: High-Fidelity Neural Radiance Fields via Alignment-Aware  Training</b></summary>
  <p><b>编号</b>：[57]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09682</p>
  <p><b>作者</b>：Yifan Jiang,  Peter Hedman,  Ben Mildenhall,  Dejia Xu,  Jonathan T. Barron,  Zhangyang Wang,  Tianfan Xue</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Neural Radiance Fields, Radiance Fields, Neural Radiance, continuous function, powerful representation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural Radiance Fields (NeRFs) are a powerful representation for modeling a
3D scene as a continuous function. Though NeRF is able to render complex 3D
scenes with view-dependent effects, few efforts have been devoted to exploring
its limits in a high-resolution setting. Specifically, existing NeRF-based
methods face several limitations when reconstructing high-resolution real
scenes, including a very large number of parameters, misaligned input data, and
overly smooth details. In this work, we conduct the first pilot study on
training NeRF with high-resolution data and propose the corresponding
solutions: 1) marrying the multilayer perceptron (MLP) with convolutional
layers which can encode more neighborhood information while reducing the total
number of parameters; 2) a novel training strategy to address misalignment
caused by moving objects or small camera calibration errors; and 3) a
high-frequency aware loss. Our approach is nearly free without introducing
obvious training/testing costs, while experiments on different datasets
demonstrate that it can recover more high-frequency details compared with the
current state-of-the-art NeRF models. Project page:
\url{this https URL.}</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Multi-Camera Multi-Object Tracking on the Move via Single-Stage Global  Association Approach</b></summary>
  <p><b>编号</b>：[65]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09663</p>
  <p><b>作者</b>：Pha Nguyen,  Kha Gia Quach,  Chi Nhan Duong,  Son Lam Phung,  Ngan Le,  Khoa Luu</p>
  <p><b>备注</b>：In review PR journal. arXiv admin note: text overlap with arXiv:2204.09151</p>
  <p><b>关键词</b>：autonomous vehicles generates, camera sensors capturing, development of autonomous, autonomous vehicles, vehicles generates</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The development of autonomous vehicles generates a tremendous demand for a
low-cost solution with a complete set of camera sensors capturing the
environment around the car. It is essential for object detection and tracking
to address these new challenges in multi-camera settings. In order to address
these challenges, this work introduces novel Single-Stage Global Association
Tracking approaches to associate one or more detection from multi-cameras with
tracked objects. These approaches aim to solve fragment-tracking issues caused
by inconsistent 3D object detection. Moreover, our models also improve the
detection accuracy of the standard vision-based 3D object detectors in the
nuScenes detection challenge. The experimental results on the nuScenes dataset
demonstrate the benefits of the proposed method by outperforming prior
vision-based tracking methods in multi-camera settings.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：HARDVS: Revisiting Human Activity Recognition with Dynamic Vision  Sensors</b></summary>
  <p><b>编号</b>：[72]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09648</p>
  <p><b>作者</b>：Xiao Wang,  Zongzhen Wu,  Bo Jiang,  Zhimin Bao,  Lin Zhu,  Guoqi Li,  Yaowei Wang,  Yonghong Tian</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large energy consumption, RGB cameras, fast motion, suffered from illumination, energy consumption</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The main streams of human activity recognition (HAR) algorithms are developed
based on RGB cameras which are suffered from illumination, fast motion,
privacy-preserving, and large energy consumption. Meanwhile, the biologically
inspired event cameras attracted great interest due to their unique features,
such as high dynamic range, dense temporal but sparse spatial resolution, low
latency, low power, etc. As it is a newly arising sensor, even there is no
realistic large-scale dataset for HAR. Considering its great practical value,
in this paper, we propose a large-scale benchmark dataset to bridge this gap,
termed HARDVS, which contains 300 categories and more than 100K event
sequences. We evaluate and report the performance of multiple popular HAR
algorithms, which provide extensive baselines for future works to compare. More
importantly, we propose a novel spatial-temporal feature learning and fusion
framework, termed ESTF, for event stream based human activity recognition. It
first projects the event streams into spatial and temporal embeddings using
StemNet, then, encodes and fuses the dual-view representations using
Transformer networks. Finally, the dual features are concatenated and fed into
a classification head for activity prediction. Extensive experiments on
multiple datasets fully validated the effectiveness of our model. Both the
dataset and source code will be released on
\url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Language Conditioned Spatial Relation Reasoning for 3D Object Grounding</b></summary>
  <p><b>编号</b>：[73]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09646</p>
  <p><b>作者</b>：Shizhe Chen,  Pierre-Louis Guhur,  Makarand Tapaswi,  Cordelia Schmid,  Ivan Laptev</p>
  <p><b>备注</b>：Accepted in NeurIPS 2022; Project website: this https URL</p>
  <p><b>关键词</b>：natural language requires, language requires understanding, scenes based, based on natural, requires understanding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Localizing objects in 3D scenes based on natural language requires
understanding and reasoning about spatial relations. In particular, it is often
crucial to distinguish similar objects referred by the text, such as "the left
most chair" and "a chair next to the window". In this work we propose a
language-conditioned transformer model for grounding 3D objects and their
spatial relations. To this end, we design a spatial self-attention layer that
accounts for relative distances and orientations between objects in input 3D
point clouds. Training such a layer with visual and language inputs enables to
disambiguate spatial relations and to localize objects referred by the text. To
facilitate the cross-modal learning of relations, we further propose a
teacher-student approach where the teacher model is first trained using
ground-truth object labels, and then helps to train a student model using point
cloud inputs. We perform ablation studies showing advantages of our approach.
We also demonstrate our model to significantly outperform the state of the art
on the challenging Nr3D, Sr3D and ScanRefer 3D object grounding datasets.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：CPT-V: A Contrastive Approach to Post-Training Quantization of Vision  Transformers</b></summary>
  <p><b>编号</b>：[74]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09643</p>
  <p><b>作者</b>：Natalia Frumkin,  Dibakar Gope,  Diana Marculescu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：typically focused, focused on developing, developing a mixed, mixed precision scheme, quantization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>When considering post-training quantization, prior work has typically focused
on developing a mixed precision scheme or learning the best way to partition a
network for quantization. In our work, CPT-V, we look at a general way to
improve the accuracy of networks that have already been quantized, simply by
perturbing the quantization scales. Borrowing the idea of contrastive loss from
self-supervised learning, we find a robust way to jointly minimize a loss
function using just 1,000 calibration images. In order to determine the best
performing quantization scale, CPT-V contrasts the features of quantized and
full precision models in a self-supervised fashion.
Unlike traditional reconstruction-based loss functions, the use of a
contrastive loss function not only rewards similarity between the quantized and
full precision outputs but also helps in distinguishing the quantized output
from other outputs within a given batch. In addition, in contrast to prior
works, CPT-V proposes a block-wise evolutionary search to minimize a global
contrastive loss objective, allowing for accuracy improvement of existing
vision transformer (ViT) quantization schemes. For example, CPT-V improves the
top-1 accuracy of a fully quantized ViT-Base by 10.30%, 0.78%, and 0.15% for
3-bit, 4-bit, and 8-bit weight quantization levels. Extensive experiments on a
variety of other ViT architectures further demonstrate its robustness in
extreme quantization scenarios. Our code is available at <link>.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Cross-Modal Adapter for Text-Video Retrieval</b></summary>
  <p><b>编号</b>：[80]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09623</p>
  <p><b>作者</b>：Haojun Jiang,  Jianke Zhang,  Rui Huang,  Chunjiang Ge,  Zanlin Ni,  Jiwen Lu,  Jie Zhou,  Shiji Song,  Gao Huang</p>
  <p><b>备注</b>：Tech Report</p>
  <p><b>关键词</b>：important multi-modal learning, text query, multi-modal learning task, relevant video, learning task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Text-video retrieval is an important multi-modal learning task, where the
goal is to retrieve the most relevant video for a given text query. Recently,
pre-trained models, e.g., CLIP, show great potential on this task. However, as
pre-trained models are scaling up, fully fine-tuning them on text-video
retrieval datasets has a high risk of overfitting. Moreover, in practice, it
would be costly to train and store a large model for each task. To overcome the
above issues, we present a novel $\textbf{Cross-Modal Adapter}$ for
parameter-efficient fine-tuning. Inspired by adapter-based methods, we adjust
the pre-trained model with a few parameterization layers. However, there are
two notable differences. First, our method is designed for the multi-modal
domain. Secondly, it allows early cross-modal interactions between CLIP's two
encoders. Although surprisingly simple, our approach has three notable
benefits: (1) reduces $\textbf{99.6}\%$ of fine-tuned parameters, and
alleviates the problem of overfitting, (2) saves approximately 30% of training
time, and (3) allows all the pre-trained parameters to be fixed, enabling the
pre-trained model to be shared across datasets. Extensive experiments
demonstrate that, without bells and whistles, it achieves superior or
comparable performance compared to fully fine-tuned methods on MSR-VTT, MSVD,
VATEX, ActivityNet, and DiDeMo datasets. The code will be available at
\url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：TrafficCAM: A Versatile Dataset for Traffic Flow Segmentation</b></summary>
  <p><b>编号</b>：[82]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09620</p>
  <p><b>作者</b>：Zhongying Deng,  Yanqi Chen,  Lihao Liu,  Shujun Wang,  Rihuan Ke,  Carola-Bibiane Schonlieb,  Angelica I Aviles-Rivero</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：revolutionising traffic management, Traffic flow, Traffic flow analysis, Traffic, analysis is revolutionising</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Traffic flow analysis is revolutionising traffic management. Qualifying
traffic flow data, traffic control bureaus could provide drivers with real-time
alerts, advising the fastest routes and therefore optimising transportation
logistics and reducing congestion. The existing traffic flow datasets have two
major limitations. They feature a limited number of classes, usually limited to
one type of vehicle, and the scarcity of unlabelled data. In this paper, we
introduce a new benchmark traffic flow image dataset called TrafficCAM. Our
dataset distinguishes itself by two major highlights. Firstly, TrafficCAM
provides both pixel-level and instance-level semantic labelling along with a
large range of types of vehicles and pedestrians. It is composed of a large and
diverse set of video sequences recorded in streets from eight Indian cities
with stationary cameras. Secondly, TrafficCAM aims to establish a new benchmark
for developing fully-supervised tasks, and importantly, semi-supervised
learning techniques. It is the first dataset that provides a vast amount of
unlabelled data, helping to better capture traffic flow qualification under a
low cost annotation requirement. More precisely, our dataset has 4,402 image
frames with semantic and instance annotations along with 59,944 unlabelled
image frames. We validate our new dataset through a large and comprehensive
range of experiments on several state-of-the-art approaches under four
different settings: fully-supervised semantic and instance segmentation, and
semi-supervised semantic and instance segmentation tasks. Our benchmark dataset
will be released.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：NorMatch: Matching Normalizing Flows with Discriminative Classifiers for  Semi-Supervised Learning</b></summary>
  <p><b>编号</b>：[91]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09593</p>
  <p><b>作者</b>：Zhongying Deng,  Rihuan Ke,  Carola-Bibiane Schonlieb,  Angelica I Aviles-Rivero</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：tiny labeled set, Semi-Supervised Learning, aims to learn, tiny labeled, labeled set</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Semi-Supervised Learning (SSL) aims to learn a model using a tiny labeled set
and massive amounts of unlabeled data. To better exploit the unlabeled data the
latest SSL methods use pseudo-labels predicted from a single discriminative
classifier. However, the generated pseudo-labels are inevitably linked to
inherent confirmation bias and noise which greatly affects the model
performance. In this work we introduce a new framework for SSL named NorMatch.
Firstly, we introduce a new uncertainty estimation scheme based on normalizing
flows, as an auxiliary classifier, to enforce highly certain pseudo-labels
yielding a boost of the discriminative classifiers. Secondly, we introduce a
threshold-free sample weighting strategy to exploit better both high and low
confidence pseudo-labels. Furthermore, we utilize normalizing flows to model,
in an unsupervised fashion, the distribution of unlabeled data. This modelling
assumption can further improve the performance of generative classifiers via
unlabeled data, and thus, implicitly contributing to training a better
discriminative classifier. We demonstrate, through numerical and visual
results, that NorMatch achieves state-of-the-art performance on several
datasets.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Hypergraph Transformer for Skeleton-based Action Recognition</b></summary>
  <p><b>编号</b>：[92]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09590</p>
  <p><b>作者</b>：Yuxuan Zhou,  Chao Li,  Zhi-Qi Cheng,  Yifeng Geng,  Xuansong Xie,  Margret Keuper</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Skeleton-based action recognition, skeletal interconnections, NTU RGB, predict human actions, action recognition aims</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Skeleton-based action recognition aims to predict human actions given human
joint coordinates with skeletal interconnections. To model such off-grid data
points and their co-occurrences, Transformer-based formulations would be a
natural choice. However, Transformers still lag behind state-of-the-art methods
using graph convolutional networks (GCNs). Transformers assume that the input
is permutation-invariant and homogeneous (partially alleviated by positional
encoding), which ignores an important characteristic of skeleton data, i.e.,
bone connectivity. Furthermore, each type of body joint has a clear physical
meaning in human motion, i.e., motion retains an intrinsic relationship
regardless of the joint coordinates, which is not explored in Transformers. In
fact, certain re-occurring groups of body joints are often involved in specific
actions, such as the subconscious hand movement for keeping balance. Vanilla
attention is incapable of describing such underlying relations that are
persistent and beyond pair-wise. In this work, we aim to exploit these unique
aspects of skeleton data to close the performance gap between Transformers and
GCNs. Specifically, we propose a new self-attention (SA) extension, named
Hypergraph Self-Attention (HyperSA), to incorporate inherently higher-order
relations into the model. The K-hop relative positional embeddings are also
employed to take bone connectivity into account. We name the resulting model
Hyperformer, and it achieves comparable or better performance w.r.t. accuracy
and efficiency than state-of-the-art GCN architectures on NTU RGB+D, NTU RGB+D
120, and Northwestern-UCLA datasets. On the largest NTU RGB+D 120 dataset, the
significantly improved performance reached by our Hyperformer demonstrates the
underestimated potential of Transformer models in this field.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：DeepVoxNet2: Yet another CNN framework</b></summary>
  <p><b>编号</b>：[96]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09569</p>
  <p><b>作者</b>：Jeroen Bertels,  David Robben,  Robin Lemmens,  Dirk Vandermeulen</p>
  <p><b>备注</b>：15 pages, part of PhD thesis KU Leuven 2022 "Understanding Final Infarct Prediction in Acute Ischemic Stroke Using Convolutional Neural Networks"</p>
  <p><b>关键词</b>：CNN mapping function, CNN-based image analysis, CNN mapping, sampling scheme, paramount importance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We know that both the CNN mapping function and the sampling scheme are of
paramount importance for CNN-based image analysis. It is clear that both
functions operate in the same space, with an image axis $\mathcal{I}$ and a
feature axis $\mathcal{F}$. Remarkably, we found that no frameworks existed
that unified the two and kept track of the spatial origin of the data
automatically. Based on our own practical experience, we found the latter to
often result in complex coding and pipelines that are difficult to exchange.
This article introduces our framework for 1, 2 or 3D image classification or
segmentation: DeepVoxNet2 (DVN2). This article serves as an interactive
tutorial, and a pre-compiled version, including the outputs of the code blocks,
can be found online in the public DVN2 repository. This tutorial uses data from
the multimodal Brain Tumor Image Segmentation Benchmark (BRATS) of 2018 to show
an example of a 3D segmentation pipeline.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Towards Good Practices in Evaluating Transfer Adversarial Attacks</b></summary>
  <p><b>编号</b>：[98]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09565</p>
  <p><b>作者</b>：Zhengyu Zhao,  Hanwei Zhang,  Renjue Li,  Ronan Sicre,  Laurent Amsaleg,  Michael Backes</p>
  <p><b>备注</b>：Our code and a list of categorized attacks are publicly available at this https URL</p>
  <p><b>关键词</b>：adversarial attacks raise, attacks raise critical, raise critical security, critical security concerns, concerns in real-world</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transfer adversarial attacks raise critical security concerns in real-world,
black-box scenarios. However, the actual progress of attack methods is
difficult to assess due to two main limitations in existing evaluations. First,
existing evaluations are unsystematic and sometimes unfair since new methods
are often directly added to old ones without complete comparisons to similar
methods. Second, existing evaluations mainly focus on transferability but
overlook another key attack property: stealthiness. In this work, we design
good practices to address these limitations. We first introduce a new attack
categorization, which enables our systematic analyses of similar attacks in
each specific category. Our analyses lead to new findings that complement or
even challenge existing knowledge. Furthermore, we comprehensively evaluate 23
representative attacks against 9 defenses on ImageNet. We pay particular
attention to stealthiness, by adopting diverse imperceptibility metrics and
looking into new, finer-grained characteristics. Our evaluation reveals new
important insights: 1) Transferability is highly contextual, and some white-box
defenses may give a false sense of security since they are actually vulnerable
to (black-box) transfer attacks; 2) All transfer attacks are less stealthy, and
their stealthiness can vary dramatically under the same $L_{\infty}$ bound.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Convolutional neural networks for medical image segmentation</b></summary>
  <p><b>编号</b>：[99]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09562</p>
  <p><b>作者</b>：Jeroen Bertels,  David Robben,  Robin Lemmens,  Dirk Vandermeulen</p>
  <p><b>备注</b>：10 pages, 6 figures, part of PhD thesis KU Leuven 2022 "Understanding Final Infarct Prediction in Acute Ischemic Stroke Using Convolutional Neural Networks"</p>
  <p><b>关键词</b>：convolutional neural networks, medical image segmentation, neural networks, essential aspects, aspects of convolutional</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this article, we look into some essential aspects of convolutional neural
networks (CNNs) with the focus on medical image segmentation. First, we discuss
the CNN architecture, thereby highlighting the spatial origin of the data,
voxel-wise classification and the receptive field. Second, we discuss the
sampling of input-output pairs, thereby highlighting the interaction between
voxel-wise classification, patch size and the receptive field. Finally, we give
a historical overview of crucial changes to CNN architectures for
classification and segmentation, giving insights in the relation between three
pivotal CNN architectures: FCN, U-Net and DeepMedic.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：ReLER@ZJU Submission to the Ego4D Moment Queries Challenge 2022</b></summary>
  <p><b>编号</b>：[100]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09558</p>
  <p><b>作者</b>：Jiayi Shao,  Xiaohan Wang,  Yi Yang</p>
  <p><b>备注</b>：3rd place in Ego4D Moment Query Challenge</p>
  <p><b>关键词</b>：present the ReLER, Challenge in ECCV, Moment, Queries Challenge, segment-level recurrence mechanism</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this report, we present the ReLER@ZJU1 submission to the Ego4D Moment
Queries Challenge in ECCV 2022. In this task, the goal is to retrieve and
localize all instances of possible activities in egocentric videos. Ego4D
dataset is challenging for the temporal action localization task as the
temporal duration of the videos is quite long and each video contains multiple
action instances with fine-grained action classes. To address these problems,
we utilize a multi-scale transformer to classify different action categories
and predict the boundary of each instance. Moreover, in order to better capture
the long-term temporal dependencies in the long videos, we propose a
segment-level recurrence mechanism. Compared with directly feeding all video
features to the transformer encoder, the proposed segment-level recurrence
mechanism alleviates the optimization difficulties and achieves better
performance. The final submission achieved Recall@1,tIoU=0.5 score of 37.24,
average mAP score of 17.67 and took 3-rd place on the leaderboard.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：UniFormerV2: Spatiotemporal Learning by Arming Image ViTs with Video  UniFormer</b></summary>
  <p><b>编号</b>：[103]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09552</p>
  <p><b>作者</b>：Kunchang Li,  Yali Wang,  Yinan He,  Yizhuo Li,  Yi Wang,  Limin Wang,  Yu Qiao</p>
  <p><b>备注</b>：24 pages, 4 figures, 20 tables</p>
  <p><b>关键词</b>：discriminative spatiotemporal representation, Learning discriminative spatiotemporal, discriminative spatiotemporal, spatiotemporal representation, key problem</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Learning discriminative spatiotemporal representation is the key problem of
video understanding. Recently, Vision Transformers (ViTs) have shown their
power in learning long-term video dependency with self-attention.
Unfortunately, they exhibit limitations in tackling local video redundancy, due
to the blind global comparison among tokens. UniFormer has successfully
alleviated this issue, by unifying convolution and self-attention as a relation
aggregator in the transformer format. However, this model has to require a
tiresome and complicated image-pretraining phrase, before being finetuned on
videos. This blocks its wide usage in practice. On the contrary, open-sourced
ViTs are readily available and well-pretrained with rich image supervision.
Based on these observations, we propose a generic paradigm to build a powerful
family of video networks, by arming the pretrained ViTs with efficient
UniFormer designs. We call this family UniFormerV2, since it inherits the
concise style of the UniFormer block. But it contains brand-new local and
global relation aggregators, which allow for preferable accuracy-computation
balance by seamlessly integrating advantages from both ViTs and UniFormer.
Without any bells and whistles, our UniFormerV2 gets the state-of-the-art
recognition performance on 8 popular video benchmarks, including scene-related
Kinetics-400/600/700 and Moments in Time, temporal-related Something-Something
V1/V2, untrimmed ActivityNet and HACS. In particular, it is the first model to
achieve 90% top-1 accuracy on Kinetics-400, to our best knowledge. Code will be
available at this https URL.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：InternVideo-Ego4D: A Pack of Champion Solutions to Ego4D Challenges</b></summary>
  <p><b>编号</b>：[108]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09529</p>
  <p><b>作者</b>：Guo Chen,  Sen Xing,  Zhe Chen,  Yi Wang,  Kunchang Li,  Yizhuo Li,  Yi Liu,  Jiahao Wang,  Yin-Dong Zheng,  Bingkun Huang,  Zhiyu Zhao,  Junting Pan,  Yifei Huang,  Zun Wang,  Jiashuo Yu,  Yinan He,  Hongjie Zhang,  Tong Lu,  Yali Wang,  Limin Wang,  Yu Qiao</p>
  <p><b>备注</b>：Technical report in 2nd International Ego4D Workshop@ECCV 2022. Code will be released at this https URL</p>
  <p><b>关键词</b>：Natural Language Queries, including Moment Queries, State Change Object, Change Object Detection, Moment Queries</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this report, we present our champion solutions to five tracks at Ego4D
challenge. We leverage our developed InternVideo, a video foundation model, for
five Ego4D tasks, including Moment Queries, Natural Language Queries, Future
Hand Prediction, State Change Object Detection, and Short-term Object
Interaction Anticipation. InternVideo-Ego4D is an effective paradigm to adapt
the strong foundation model to the downstream ego-centric video understanding
tasks with simple head designs. In these five tasks, the performance of
InternVideo-Ego4D comprehensively surpasses the baseline methods and the
champions of CVPR2022, demonstrating the powerful representation ability of
InternVideo as a video foundation model. Our code will be released at
this https URL</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：ImLiDAR: Cross-Sensor Dynamic Message Propagation Network for 3D Object  Detection</b></summary>
  <p><b>编号</b>：[112]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09518</p>
  <p><b>作者</b>：Yiyang Shen,  Rongwei Yu,  Peng Wu,  Haoran Xie,  Lina Gong,  Jing Qin,  Mingqiang Wei</p>
  <p><b>备注</b>：12 pages</p>
  <p><b>关键词</b>：RGB images, supply geometric, RGB, LiDAR point clouds, point clouds</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>LiDAR and camera, as two different sensors, supply geometric (point clouds)
and semantic (RGB images) information of 3D scenes. However, it is still
challenging for existing methods to fuse data from the two cross sensors,
making them complementary for quality 3D object detection (3OD). We propose
ImLiDAR, a new 3OD paradigm to narrow the cross-sensor discrepancies by
progressively fusing the multi-scale features of camera Images and LiDAR point
clouds. ImLiDAR enables to provide the detection head with cross-sensor yet
robustly fused features. To achieve this, two core designs exist in ImLiDAR.
First, we propose a cross-sensor dynamic message propagation module to combine
the best of the multi-scale image and point features. Second, we raise a direct
set prediction problem that allows designing an effective set-based detector to
tackle the inconsistency of the classification and localization confidences,
and the sensitivity of hand-tuned hyperparameters. Besides, the novel set-based
detector can be detachable and easily integrated into various detection
networks. Comparisons on both the KITTI and SUN-RGBD datasets show clear visual
and numerical improvements of our ImLiDAR over twenty-three state-of-the-art
3OD methods.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：EPCS: Endpoint-based Part-aware Curve Skeleton Extraction for  Low-quality Point Clouds</b></summary>
  <p><b>编号</b>：[121]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09488</p>
  <p><b>作者</b>：Chunhui Li,  Mingquan Zhou,  Zehua Liu,  Yuhe Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：important shape descriptor, point clouds, curve skeleton, machine vision, computer graphics</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The curve skeleton is an important shape descriptor that has been utilized in
various applications in computer graphics, machine vision, and artificial
intelligence. In this study, the endpoint-based part-aware curve skeleton
(EPCS) extraction method for low-quality point clouds is proposed. The novel
random center shift (RCS) method is first proposed for detecting the endpoints
on point clouds. The endpoints are used as the initial seed points for dividing
each part into layers, and then the skeletal points are obtained by computing
the center points of the oriented bounding box (OBB) of the layers.
Subsequently, the skeletal points are connected, thus forming the branches.
Furthermore, the multi-vector momentum-driven (MVMD) method is also proposed
for locating the junction points that connect the branches. Due to the shape
differences between different parts on point clouds, the global topology of the
skeleton is finally optimized by removing the redundant junction points,
re-connecting some branches using the proposed MVMD method, and applying an
interpolation method based on the splitting operator. Consequently, a complete
and smooth curve skeleton is achieved. The proposed EPCS method is compared
with several state-of-the-art methods, and the experimental results verify its
robustness, effectiveness, and efficiency. Furthermore, the skeleton extraction
and model segmentation results on the point clouds of broken Terracotta also
highlight the utility of the proposed method.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：ArcAid: Analysis of Archaeological Artifacts using Drawings</b></summary>
  <p><b>编号</b>：[125]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09480</p>
  <p><b>作者</b>：Offry Hayon,  Stefan Münger,  Ilan Shimshoni,  Ayellet Tal</p>
  <p><b>备注</b>：8 pages, 9 figures</p>
  <p><b>关键词</b>：computer vision, intriguing domain, domain, Archaeology, vision</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Archaeology is an intriguing domain for computer vision. It suffers not only
from shortage in (labeled) data, but also from highly-challenging data, which
is often extremely abraded and damaged. This paper proposes a novel
semi-supervised model for classification and retrieval of images of
archaeological artifacts. This model utilizes unique data that exists in the
domain -- manual drawings made by special artists.These are used during
training to implicitly transfer the domain knowledge from the drawings to their
corresponding images, improving their classification results. We show that
while learning how to classify, our model also learns how to generate drawings
of the artifacts, an important documentation task, which is currently performed
manually. Last but not least, we collected a new dataset of stamp-seals of the
Southern Levant.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Visual Commonsense-aware Representation Network for Video Captioning</b></summary>
  <p><b>编号</b>：[127]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09469</p>
  <p><b>作者</b>：Pengpeng Zeng,  Haonan Zhang,  Lianli Gao,  Xiangpeng Li,  Jin Qian,  Heng Tao Shen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：requires taking full, taking full advantage, Generating consecutive descriptions, Video Captioning, Video</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generating consecutive descriptions for videos, i.e., Video Captioning,
requires taking full advantage of visual representation along with the
generation process. Existing video captioning methods focus on making an
exploration of spatial-temporal representations and their relationships to
produce inferences. However, such methods only exploit the superficial
association contained in the video itself without considering the intrinsic
visual commonsense knowledge that existed in a video dataset, which may hinder
their capabilities of knowledge cognitive to reason accurate descriptions. To
address this problem, we propose a simple yet effective method, called Visual
Commonsense-aware Representation Network (VCRN), for video captioning.
Specifically, we construct a Video Dictionary, a plug-and-play component,
obtained by clustering all video features from the total dataset into multiple
clustered centers without additional annotation. Each center implicitly
represents a visual commonsense concept in the video domain, which is utilized
in our proposed Visual Concept Selection (VCS) to obtain a video-related
concept feature. Next, a Conceptual Integration Generation (CIG) is proposed to
enhance the caption generation. Extensive experiments on three publicly video
captioning benchmarks: MSVD, MSR-VTT, and VATEX, demonstrate that our method
reaches state-of-the-art performance, indicating the effectiveness of our
method. In addition, our approach is integrated into the existing method of
video question answering and improves this performance, further showing the
generalization of our method. Source code has been released at
this https URL.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Progressive Tree-Structured Prototype Network for End-to-End Image  Captioning</b></summary>
  <p><b>编号</b>：[130]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09460</p>
  <p><b>作者</b>：Pengpeng Zeng,  Jinkuan Zhu,  Jingkuan Song,  Lianli Gao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：faster inference speed, transformer-based generation architecture, leveraging powerful visual, powerful visual pre-trained, flexible model training</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Studies of image captioning are shifting towards a trend of a fully
end-to-end paradigm by leveraging powerful visual pre-trained models and
transformer-based generation architecture for more flexible model training and
faster inference speed. State-of-the-art approaches simply extract isolated
concepts or attributes to assist description generation. However, such
approaches do not consider the hierarchical semantic structure in the textual
domain, which leads to an unpredictable mapping between visual representations
and concept words. To this end, we propose a novel Progressive Tree-Structured
prototype Network (dubbed PTSN), which is the first attempt to narrow down the
scope of prediction words with appropriate semantics by modeling the
hierarchical textual semantics. Specifically, we design a novel embedding
method called tree-structured prototype, producing a set of hierarchical
representative embeddings which capture the hierarchical semantic structure in
textual space. To utilize such tree-structured prototypes into visual
cognition, we also propose a progressive aggregation module to exploit semantic
relationships within the image and prototypes. By applying our PTSN to the
end-to-end captioning framework, extensive experiments conducted on MSCOCO
dataset show that our method achieves a new state-of-the-art performance with
144.2% (single model) and 146.5% (ensemble of 4 models) CIDEr scores on
`Karpathy' split and 141.4% (c5) and 143.9% (c40) CIDEr scores on the official
online test server. Trained models and source code have been released at:
this https URL.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：DeepPrivacy2: Towards Realistic Full-Body Anonymization</b></summary>
  <p><b>编号</b>：[134]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09454</p>
  <p><b>作者</b>：Håkon Hukkelås,  Frank Lindseth</p>
  <p><b>备注</b>：Accepted at WACV2023</p>
  <p><b>关键词</b>：Generative Adversarial Networks, Adversarial Networks, Generative Adversarial, widely adapted, Networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generative Adversarial Networks (GANs) are widely adapted for anonymization
of human figures. However, current state-of-the-art limit anonymization to the
task of face anonymization. In this paper, we propose a novel anonymization
framework (DeepPrivacy2) for realistic anonymization of human figures and
faces. We introduce a new large and diverse dataset for human figure synthesis,
which significantly improves image quality and diversity of generated images.
Furthermore, we propose a style-based GAN that produces high quality, diverse
and editable anonymizations. We demonstrate that our full-body anonymization
framework provides stronger privacy guarantees than previously proposed
methods.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：Detecting Arbitrary Keypoints on Limbs and Skis with Sparse Partly  Correct Segmentation Masks</b></summary>
  <p><b>编号</b>：[135]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09446</p>
  <p><b>作者</b>：Katja Ludwig,  Daniel Kienzle,  Julian Lorenz,  Rainer Lienhart</p>
  <p><b>备注</b>：accepted at CV4WS2023 (WACV 2023 Workshops)</p>
  <p><b>关键词</b>：correct segmentation masks, segmentation masks, partly correct segmentation, sports disciplines, body posture</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Analyses based on the body posture are crucial for top-class athletes in many
sports disciplines. If at all, coaches label only the most important keypoints,
since manual annotations are very costly. This paper proposes a method to
detect arbitrary keypoints on the limbs and skis of professional ski jumpers
that requires a few, only partly correct segmentation masks during training.
Our model is based on the Vision Transformer architecture with a special design
for the input tokens to query for the desired keypoints. Since we use
segmentation masks only to generate ground truth labels for the freely
selectable keypoints, partly correct segmentation masks are sufficient for our
training procedure. Hence, there is no need for costly hand-annotated
segmentation masks. We analyze different training techniques for freely
selected and standard keypoints, including pseudo labels, and show in our
experiments that only a few partly correct segmentation masks are sufficient
for learning to detect arbitrary keypoints on limbs and skis.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：aiMotive Dataset: A Multimodal Dataset for Robust Autonomous Driving  with Long-Range Perception</b></summary>
  <p><b>编号</b>：[136]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09445</p>
  <p><b>作者</b>：Tamás Matuszka,  Iván Barton,  Ádám Butykai,  Péter Hajas,  Dávid Kiss,  Domonkos Kovács,  Sándor Kunsági-Máté,  Péter Lengyel,  Gábor Németh,  Levente Pető,  Dezső Ribli,  Dávid Szeghy,  Szabolcs Vajna,  Bálint Varga</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：vision research community, computer vision research, popular research area, research community, popular research</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Autonomous driving is a popular research area within the computer vision
research community. Since autonomous vehicles are highly safety-critical,
ensuring robustness is essential for real-world deployment. While several
public multimodal datasets are accessible, they mainly comprise two sensor
modalities (camera, LiDAR) which are not well suited for adverse weather. In
addition, they lack far-range annotations, making it harder to train neural
networks that are the base of a highway assistant function of an autonomous
vehicle. Therefore, we introduce a multimodal dataset for robust autonomous
driving with long-range perception. The dataset consists of 176 scenes with
synchronized and calibrated LiDAR, camera, and radar sensors covering a
360-degree field of view. The collected data was captured in highway, urban,
and suburban areas during daytime, night, and rain and is annotated with 3D
bounding boxes with consistent identifiers across frames. Furthermore, we
trained unimodal and multimodal baseline models for 3D object detection. Data
are available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Feedback is Needed for Retakes: An Explainable Poor Image Notification  Framework for the Visually Impaired</b></summary>
  <p><b>编号</b>：[141]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09427</p>
  <p><b>作者</b>：Kazuya Ohata,  Shunsuke Kitada,  Hitoshi Iyatomi</p>
  <p><b>备注</b>：6 pages, 4 figures. Accepted at 2022 IEEE 19th International Conference on Smart Communities: Improving Quality of Life Using ICT, IoT and AI (HONET) as a full paper</p>
  <p><b>关键词</b>：propose a simple, simple yet effective, quality, image, effective image captioning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a simple yet effective image captioning framework that can
determine the quality of an image and notify the user of the reasons for any
flaws in the image. Our framework first determines the quality of images and
then generates captions using only those images that are determined to be of
high quality. The user is notified by the flaws feature to retake if image
quality is low, and this cycle is repeated until the input image is deemed to
be of high quality. As a component of the framework, we trained and evaluated a
low-quality image detection model that simultaneously learns difficulty in
recognizing images and individual flaws, and we demonstrated that our proposal
can explain the reasons for flaws with a sufficient score. We also evaluated a
dataset with low-quality images removed by our framework and found improved
values for all four common metrics (e.g., BLEU-4, METEOR, ROUGE-L, CIDEr),
confirming an improvement in general-purpose image captioning capability. Our
framework would assist the visually impaired, who have difficulty judging image
quality.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：DexPoint: Generalizable Point Cloud Reinforcement Learning for  Sim-to-Real Dexterous Manipulation</b></summary>
  <p><b>编号</b>：[143]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09423</p>
  <p><b>作者</b>：Yuzhe Qin,  Binghao Huang,  Zhao-Heng Yin,  Hao Su,  Xiaolong Wang</p>
  <p><b>备注</b>：Conference on Robot Learning (CoRL) 2022</p>
  <p><b>关键词</b>：real world, dexterous hands, dexterous, hand point clouds, framework</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a sim-to-real framework for dexterous manipulation which can
generalize to new objects of the same category in the real world. The key of
our framework is to train the manipulation policy with point cloud inputs and
dexterous hands. We propose two new techniques to enable joint learning on
multiple objects and sim-to-real generalization: (i) using imagined hand point
clouds as augmented inputs; and (ii) designing novel contact-based rewards. We
empirically evaluate our method using an Allegro Hand to grasp novel objects in
both simulation and real world. To the best of our knowledge, this is the first
policy learning-based framework that achieves such generalization results with
dexterous hands. Our project page is available at
this https URL</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Data Dimension Reduction makes ML Algorithms efficient</b></summary>
  <p><b>编号</b>：[156]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09392</p>
  <p><b>作者</b>：Wisal Khan,  Muhammad Turab,  Waqas Ahmad,  Syed Hasnat Ahmad,  Kelash Kumar,  Bin Luo</p>
  <p><b>备注</b>：Our paper is accepted at International Conference On Emerging Technologies In Electronics, Computing And Communication (ICETECC) 2022</p>
  <p><b>关键词</b>：Principal Component Analysis, Principal Component, Component Analysis, Random Projections, Data dimension reduction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Data dimension reduction (DDR) is all about mapping data from high dimensions
to low dimensions, various techniques of DDR are being used for image dimension
reduction like Random Projections, Principal Component Analysis (PCA), the
Variance approach, LSA-Transform, the Combined and Direct approaches, and the
New Random Approach. Auto-encoders (AE) are used to learn end-to-end mapping.
In this paper, we demonstrate that pre-processing not only speeds up the
algorithms but also improves accuracy in both supervised and unsupervised
learning. In pre-processing of DDR, first PCA based DDR is used for supervised
learning, then we explore AE based DDR for unsupervised learning. In PCA based
DDR, we first compare supervised learning algorithms accuracy and time before
and after applying PCA. Similarly, in AE based DDR, we compare unsupervised
learning algorithm accuracy and time before and after AE representation
learning. Supervised learning algorithms including support-vector machines
(SVM), Decision Tree with GINI index, Decision Tree with entropy and Stochastic
Gradient Descent classifier (SGDC) and unsupervised learning algorithm
including K-means clustering, are used for classification purpose. We used two
datasets MNIST and FashionMNIST Our experiment shows that there is massive
improvement in accuracy and time reduction after pre-processing in both
supervised and unsupervised learning.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：BEVDistill: Cross-Modal BEV Distillation for Multi-View 3D Object  Detection</b></summary>
  <p><b>编号</b>：[159]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09386</p>
  <p><b>作者</b>：Zehui Chen,  Zhenyu Li,  Shiquan Zhang,  Liangji Fang,  Qinhong Jiang,  Feng Zhao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：visual scene understanding, scene understanding, fundamental and challenging, challenging task, task for visual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>3D object detection from multiple image views is a fundamental and
challenging task for visual scene understanding. Owing to its low cost and high
efficiency, multi-view 3D object detection has demonstrated promising
application prospects. However, accurately detecting objects through
perspective views is extremely difficult due to the lack of depth information.
Current approaches tend to adopt heavy backbones for image encoders, making
them inapplicable for real-world deployment. Different from the images, LiDAR
points are superior in providing spatial cues, resulting in highly precise
localization. In this paper, we explore the incorporation of LiDAR-based
detectors for multi-view 3D object detection. Instead of directly training a
depth prediction network, we unify the image and LiDAR features in the
Bird-Eye-View (BEV) space and adaptively transfer knowledge across
non-homogenous representations in a teacher-student paradigm. To this end, we
propose \textbf{BEVDistill}, a cross-modal BEV knowledge distillation (KD)
framework for multi-view 3D object detection. Extensive experiments demonstrate
that the proposed method outperforms current KD approaches on a
highly-competitive baseline, BEVFormer, without introducing any extra cost in
the inference phase. Notably, our best model achieves 59.4 NDS on the nuScenes
test leaderboard, achieving new state-of-the-art in comparison with various
image-based detectors. Code will be available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Planning Irregular Object Packing via Hierarchical Reinforcement  Learning</b></summary>
  <p><b>编号</b>：[161]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09382</p>
  <p><b>作者</b>：Sichao Huang,  Ziwei Wang,  Jie Zhou,  Jiwen Lu</p>
  <p><b>备注</b>：This work is accepted by IEEE RAL. 8 pages, 6 figures</p>
  <p><b>关键词</b>：packing, logistics industry, autonomous robots, im-portant challenge, challenge in warehouses</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Object packing by autonomous robots is an im-portant challenge in warehouses
and logistics industry. Most conventional data-driven packing planning
approaches focus on regular cuboid packing, which are usually heuristic and
limit the practical use in realistic applications with everyday objects. In
this paper, we propose a deep hierarchical reinforcement learning approach to
simultaneously plan packing sequence and placement for irregular object
packing. Specifically, the top manager network infers packing sequence from six
principal view heightmaps of all objects, and then the bottom worker network
receives heightmaps of the next object to predict the placement position and
orientation. The two networks are trained hierarchically in a self-supervised
Q-Learning framework, where the rewards are provided by the packing results
based on the top height , object volume and placement stability in the box. The
framework repeats sequence and placement planning iteratively until all objects
have been packed into the box or no space is remained for unpacked items. We
compare our approach with existing robotic packing methods for irregular
objects in a physics simulator. Experiments show that our approach can pack
more objects with less time cost than the state-of-the-art packing methods of
irregular objects. We also implement our packing plan with a robotic
manipulator to show the generalization ability in the real world.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：3D-QueryIS: A Query-based Framework for 3D Instance Segmentation</b></summary>
  <p><b>编号</b>：[167]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09375</p>
  <p><b>作者</b>：Jiaheng Liu,  Tong He,  Honghui Yang,  Rui Su,  Jiayi Tian,  Junran Wu,  Hongcheng Guo,  Ke Xu,  Wanli Ouyang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Previous top-performing methods, Previous top-performing, lack of robustness, segmentation often maintain, maintain inter-task dependencies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Previous top-performing methods for 3D instance segmentation often maintain
inter-task dependencies and the tendency towards a lack of robustness. Besides,
inevitable variations of different datasets make these methods become
particularly sensitive to hyper-parameter values and manifest poor
generalization capability. In this paper, we address the aforementioned
challenges by proposing a novel query-based method, termed as 3D-QueryIS, which
is detector-free, semantic segmentation-free, and cluster-free. Specifically,
we propose to generate representative points in an implicit manner, and use
them together with the initial queries to generate the informative instance
queries. Then, the class and binary instance mask predictions can be produced
by simply applying MLP layers on top of the instance queries and the extracted
point cloud embeddings. Thus, our 3D-QueryIS is free from the accumulated
errors caused by the inter-task dependencies. Extensive experiments on multiple
benchmark datasets demonstrate the effectiveness and efficiency of our proposed
3D-QueryIS method.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：CapEnrich: Enriching Caption Semantics for Web Images via Cross-modal  Pre-trained Knowledge</b></summary>
  <p><b>编号</b>：[170]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09371</p>
  <p><b>作者</b>：Linli Yao,  Weijing Chen,  Qin Jin</p>
  <p><b>备注</b>：Under Review</p>
  <p><b>关键词</b>：greatly benefit realistic, realistic web applications, massive unlabeled images, benefit realistic web, multimodal retrieval</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automatically generating textual descriptions for massive unlabeled images on
the web can greatly benefit realistic web applications, e.g. multimodal
retrieval and recommendation. However, existing models suffer from the problem
of generating ``over-generic'' descriptions, such as their tendency to generate
repetitive sentences with common concepts for different images. These generic
descriptions fail to provide sufficient textual semantics for ever-changing web
images. Inspired by the recent success of Vision-Language Pre-training (VLP)
models that learn diverse image-text concept alignment during pretraining, we
explore leveraging their cross-modal pre-trained knowledge to automatically
enrich the textual semantics of image descriptions. With no need for additional
human annotations, we propose a plug-and-play framework, i.e CapEnrich, to
complement the generic image descriptions with more semantic details.
Specifically, we first propose an automatic data-building strategy to get
desired training sentences, based on which we then adopt prompting strategies,
i.e. learnable and template prompts, to incentivize VLP models to generate more
textual details. For learnable templates, we fix the whole VLP model and only
tune the prompt vectors, which leads to two advantages: 1) the pre-training
knowledge of VLP models can be reserved as much as possible to describe diverse
visual concepts; 2) only lightweight trainable parameters are required, so it
is friendly to low data resources. Extensive experiments show that our method
significantly improves the descriptiveness and diversity of generated sentences
for web images. Our code will be released.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Generalizable Deepfake Detection with Phase-Based Motion Analysis</b></summary>
  <p><b>编号</b>：[173]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09363</p>
  <p><b>作者</b>：Ekta Prashnani,  Michael Goebel,  B. S. Manjunath</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：video detection method, phase-based motion representation, facial temporal dynamics, representation of facial, video detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose PhaseForensics, a DeepFake (DF) video detection method that
leverages a phase-based motion representation of facial temporal dynamics.
Existing methods relying on temporal inconsistencies for DF detection present
many advantages over the typical frame-based methods. However, they still show
limited cross-dataset generalization and robustness to common distortions.
These shortcomings are partially due to error-prone motion estimation and
landmark tracking, or the susceptibility of the pixel intensity-based features
to spatial distortions and the cross-dataset domain shifts. Our key insight to
overcome these issues is to leverage the temporal phase variations in the
band-pass components of the Complex Steerable Pyramid on face sub-regions. This
not only enables a robust estimate of the temporal dynamics in these regions,
but is also less prone to cross-dataset variations. Furthermore, the band-pass
filters used to compute the local per-frame phase form an effective defense
against the perturbations commonly seen in gradient-based adversarial attacks.
Overall, with PhaseForensics, we show improved distortion and adversarial
robustness, and state-of-the-art cross-dataset generalization, with 91.2%
video-level AUC on the challenging CelebDFv2 (a recent state-of-the-art
compares at 86.9%).</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：How to Fine-Tune Vision Models with SGD</b></summary>
  <p><b>编号</b>：[176]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09359</p>
  <p><b>作者</b>：Ananya Kumar,  Ruoqi Shen,  Sébastien Bubeck,  Suriya Gunasekar</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large neural networks, neural networks, networks in computer, computer vision, SGD</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>SGD (with momentum) and AdamW are the two most used optimizers for
fine-tuning large neural networks in computer vision. When the two methods
perform the same, SGD is preferable because it uses less memory (12
bytes/parameter) than AdamW (16 bytes/parameter). However, on a suite of
downstream tasks, especially those with distribution shifts, we show that
fine-tuning with AdamW performs substantially better than SGD on modern Vision
Transformer and ConvNeXt models. We find that large gaps in performance between
SGD and AdamW occur when the fine-tuning gradients in the first "embedding"
layer are much larger than in the rest of the model. Our analysis suggests an
easy fix that works consistently across datasets and models: merely freezing
the embedding layer (less than 1\% of the parameters) leads to SGD performing
competitively with AdamW while using less memory. Our insights result in
state-of-the-art accuracies on five popular distribution shift benchmarks:
WILDS-FMoW, WILDS-Camelyon, Living-17, Waterbirds, and DomainNet.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Learning Domain and Pose Invariance for Thermal-to-Visible Face  Recognition</b></summary>
  <p><b>编号</b>：[178]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09350</p>
  <p><b>作者</b>：Cedric Nimpa Fondje,  Shuowen Hu,  Benjamin S. Riggan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：thermal infrared cameras, visible face recognition, frontal visible faces, visible, frontal visible</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Interest in thermal to visible face recognition has grown significantly over
the last decade due to advancements in thermal infrared cameras and analytics
beyond the visible spectrum. Despite large discrepancies between thermal and
visible spectra, existing approaches bridge domain gaps by either synthesizing
visible faces from thermal faces or by learning the cross-spectrum image
representations. These approaches typically work well with frontal facial
imagery collected at varying ranges and expressions, but exhibit significantly
reduced performance when matching thermal faces with varying poses to frontal
visible faces. We propose a novel Domain and Pose Invariant Framework that
simultaneously learns domain and pose invariant representations. Our proposed
framework is composed of modified networks for extracting the most correlated
intermediate representations from off-pose thermal and frontal visible face
imagery, a sub-network to jointly bridge domain and pose gaps, and a joint-loss
function comprised of cross-spectrum and pose-correction losses. We demonstrate
efficacy and advantages of the proposed method by evaluating on three
thermal-visible datasets: ARL Visible-to-Thermal Face, ARL Multimodal Face, and
Tufts Face. Although DPIF focuses on learning to match off-pose thermal to
frontal visible faces, we also show that DPIF enhances performance when
matching frontal thermal face images to frontal visible face images.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：I see you: A Vehicle-Pedestrian Interaction Dataset from Traffic  Surveillance Cameras</b></summary>
  <p><b>编号</b>：[182]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09342</p>
  <p><b>作者</b>：Hanan Quispe,  Jorshinno Sumire,  Patricia Condori,  Edwin Alvarez,  Harley Vera</p>
  <p><b>备注</b>：paper accepted at LXAI workshop at NeurIPS 2022, github repository this https URL</p>
  <p><b>关键词</b>：autonomous vehicles arises, urban traffic scenarios, pedestrian slows, development of autonomous, arises new challenges</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The development of autonomous vehicles arises new challenges in urban traffic
scenarios where vehicle-pedestrian interactions are frequent e.g. vehicle
yields to pedestrians, pedestrian slows down due approaching to the vehicle.
Over the last years, several datasets have been developed to model these
interactions. However, available datasets do not cover near-accident scenarios
that our dataset covers. We introduce I see you, a new vehicle-pedestrian
interaction dataset that tackles the lack of trajectory data in near-accident
scenarios using YOLOv5 and camera calibration methods. I see you consist of 170
near-accident occurrences in seven intersections in Cusco-Peru. This new
dataset and pipeline code are available on Github.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：Targeted Attention for Generalized- and Zero-Shot Learning</b></summary>
  <p><b>编号</b>：[191]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09322</p>
  <p><b>作者</b>：Abhijit Suprem</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：labeled data, attempts to learn, Learning, ZSL, task attempts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The Zero-Shot Learning (ZSL) task attempts to learn concepts without any
labeled data. Unlike traditional classification/detection tasks, the evaluation
environment is provided unseen classes never encountered during training. As
such, it remains both challenging, and promising on a variety of fronts,
including unsupervised concept learning, domain adaptation, and dataset drift
detection. Recently, there have been a variety of approaches towards solving
ZSL, including improved metric learning methods, transfer learning,
combinations of semantic and image domains using, e.g. word vectors, and
generative models to model the latent space of known classes to classify unseen
classes. We find many approaches require intensive training augmentation with
attributes or features that may be commonly unavailable (attribute-based
learning) or susceptible to adversarial attacks (generative learning). We
propose combining approaches from the related person re-identification task for
ZSL, with key modifications to ensure sufficiently improved performance in the
ZSL setting without the need for feature or training dataset augmentation. We
are able to achieve state-of-the-art performance on the CUB200 and Cars196
datasets in the ZSL setting compared to recent works, with NMI (normalized
mutual inference) of 63.27 and top-1 of 61.04 for CUB200, and NMI 66.03 with
top-1 82.75% in Cars196. We also show state-of-the-art results in the
Generalized Zero-Shot Learning (GZSL) setting, with Harmonic Mean R-1 of 66.14%
on the CUB200 dataset.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Interpretable Dimensionality Reduction by Feature Preserving Manifold  Approximation and Projection</b></summary>
  <p><b>编号</b>：[192]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09321</p>
  <p><b>作者</b>：Yang Yang,  Hongjian Sun,  Jialei Gong,  Yali Du,  Di Yu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Nonlinear dimensionality reduction, dimensionality reduction lacks, reduction lacks interpretability, lacks interpretability due, source features</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Nonlinear dimensionality reduction lacks interpretability due to the absence
of source features in low-dimensional embedding space. We propose an
interpretable method featMAP to preserve source features by tangent space
embedding. The core of our proposal is to utilize local singular value
decomposition (SVD) to approximate the tangent space which is embedded to
low-dimensional space by maintaining the alignment. Based on the embedding
tangent space, featMAP enables the interpretability by locally demonstrating
the source features and feature importance. Furthermore, featMAP embeds the
data points by anisotropic projection to preserve the local similarity and
original density. We apply featMAP to interpreting digit classification, object
detection and MNIST adversarial examples. FeatMAP uses source features to
explicitly distinguish the digits and objects and to explain the
misclassification of adversarial examples. We also compare featMAP with other
state-of-the-art methods on local and global metrics.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Explainable, Domain-Adaptive, and Federated Artificial Intelligence in  Medicine</b></summary>
  <p><b>编号</b>：[194]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09317</p>
  <p><b>作者</b>：Ahmad Chaddad,  Qizong lu,  Jiali Li,  Yousef Katib,  Reem Kateb,  Camel Tanougast,  Ahmed Bouridane,  Ahmed Abdulkadir</p>
  <p><b>备注</b>：This paper is accepted in IEEE CAA Journal of Automatica Sinica, Nov. 10 2022</p>
  <p><b>关键词</b>：transform data analysis, Artificial intelligence, continues to transform, transform data, data analysis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Artificial intelligence (AI) continues to transform data analysis in many
domains. Progress in each domain is driven by a growing body of annotated data,
increased computational resources, and technological innovations. In medicine,
the sensitivity of the data, the complexity of the tasks, the potentially high
stakes, and a requirement of accountability give rise to a particular set of
challenges. In this review, we focus on three key methodological approaches
that address some of the particular challenges in AI-driven medical decision
making. (1) Explainable AI aims to produce a human-interpretable justification
for each output. Such models increase confidence if the results appear
plausible and match the clinicians expectations. However, the absence of a
plausible explanation does not imply an inaccurate model. Especially in highly
non-linear, complex models that are tuned to maximize accuracy, such
interpretable representations only reflect a small portion of the
justification. (2) Domain adaptation and transfer learning enable AI models to
be trained and applied across multiple domains. For example, a classification
task based on images acquired on different acquisition hardware. (3) Federated
learning enables learning large-scale models without exposing sensitive
personal health information. Unlike centralized AI learning, where the
centralized learning machine has access to the entire training data, the
federated learning process iteratively updates models across multiple sites by
exchanging only parameter updates, not personal health data. This narrative
review covers the basic concepts, highlights relevant corner-stone and
state-of-the-art research in the field, and discusses perspectives.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Problem Behaviors Recognition in Videos using Language-Assisted Deep  Learning Model for Children with Autism</b></summary>
  <p><b>编号</b>：[195]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09310</p>
  <p><b>作者</b>：Andong Deng,  Taojiannan Yang,  Chen Chen,  Qian Chen,  Leslie Neely,  Sakiko Oyama</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Autism Spectrum Disorder, Spectrum Disorder, timely early intervention, Autism Spectrum, early intervention</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Correctly recognizing the behaviors of children with Autism Spectrum Disorder
(ASD) is of vital importance for the diagnosis of Autism and timely early
intervention. However, the observation and recording during the treatment from
the parents of autistic children may not be accurate and objective. In such
cases, automatic recognition systems based on computer vision and machine
learning (in particular deep learning) technology can alleviate this issue to a
large extent. Existing human action recognition models can now achieve
persuasive performance on challenging activity datasets, e.g. daily activity,
and sports activity. However, problem behaviors in children with ASD are very
different from these general activities, and recognizing these problem
behaviors via computer vision is less studied. In this paper, we first evaluate
a strong baseline for action recognition, i.e. Video Swin Transformer, on two
autism behaviors datasets (SSBD and ESBD) and show that it can achieve high
accuracy and outperform the previous methods by a large margin, demonstrating
the feasibility of vision-based problem behaviors recognition. Moreover, we
propose language-assisted training to further enhance the action recognition
performance. Specifically, we develop a two-branch multimodal deep learning
framework by incorporating the "freely available" language description for each
type of problem behavior. Experimental results demonstrate that incorporating
additional language supervision can bring an obvious performance boost for the
autism problem behaviors recognition task as compared to using the video
information only (i.e. 3.49% improvement on ESBD and 1.46% on SSBD).</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：You Only Label Once: 3D Box Adaptation from Point Cloud to Image via  Semi-Supervised Learning</b></summary>
  <p><b>编号</b>：[198]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09302</p>
  <p><b>作者</b>：Jieqi Shi,  Peiliang Li,  Xiaozhi Chen,  Shaojie Shen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：object detection task, detection task expects, object detection, object contour, tightness projection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The image-based 3D object detection task expects that the predicted 3D
bounding box has a ``tightness'' projection (also referred to as cuboid), which
fits the object contour well on the image while still keeping the geometric
attribute on the 3D space, e.g., physical dimension, pairwise orthogonal, etc.
These requirements bring significant challenges to the annotation. Simply
projecting the Lidar-labeled 3D boxes to the image leads to non-trivial
misalignment, while directly drawing a cuboid on the image cannot access the
original 3D information. In this work, we propose a learning-based 3D box
adaptation approach that automatically adjusts minimum parameters of the
360$^{\circ}$ Lidar 3D bounding box to perfectly fit the image appearance of
panoramic cameras. With only a few 2D boxes annotation as guidance during the
training phase, our network can produce accurate image-level cuboid annotations
with 3D properties from Lidar boxes. We call our method ``you only label
once'', which means labeling on the point cloud once and automatically adapting
to all surrounding cameras. As far as we know, we are the first to focus on
image-level cuboid refinement, which balances the accuracy and efficiency well
and dramatically reduces the labeling effort for accurate cuboid annotation.
Extensive experiments on the public Waymo and NuScenes datasets show that our
method can produce human-level cuboid annotation on the image without needing
manual adjustment.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：Longitudinal thermal imaging for scalable non-residential HVAC and  occupant behaviour characterization</b></summary>
  <p><b>编号</b>：[201]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09288</p>
  <p><b>作者</b>：Vasantha Ramani,  Miguel Martin,  Pandarasamy Arjunan,  Adrian Chong,  Kameshwar Poolla,  Clayton Miller</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：thermal images, surface temperature measurements, thermal images collected, thermal, truth surface temperature</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This work presents a study on the characterization of the air-conditioning
(AC) usage pattern of non-residential buildings from thermal images collected
from an urban-scale infrared (IR) observatory. To achieve this first, an image
processing scheme, for cleaning and extraction of the temperature time series
from the thermal images is implemented. To test the accuracy of the thermal
measurements using IR camera, the extracted temperature is compared against the
ground truth surface temperature measurements. It is observed that the
detrended thermal measurements match well with the ground truth surface
temperature measurements. Subsequently, the operational pattern of the
water-cooled systems and window AC units are extracted from the analysis of the
thermal signature. It is observed that for the water-cooled system, the
difference between the rate of change of the window and wall can be used to
extract the operational pattern. While, in the case of the window AC units,
wavelet transform of the AC unit temperature is used to extract the frequency
and time domain information of the AC unit operation. The results of the
analysis are compared against the indoor temperature sensors installed in the
office spaces of the building. It is realized that the accuracy in the
prediction of the operational pattern is highest between 8 pm to 10 am, and it
reduces during the day because of solar radiation and high daytime temperature.
Subsequently, a characterization study is conducted for eight window/split AC
units from the thermal image collected during the nighttime. This forms one of
the first studies on the operational behavior of HVAC systems for
non-residential buildings using the longitudinal thermal imaging technique. The
output from this study can be used to better understand the operational and
occupant behavior, without requiring to deploy a large array of sensors in the
building space.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：Prompt Tuning for Parameter-efficient Medical Image Segmentation</b></summary>
  <p><b>编号</b>：[216]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09233</p>
  <p><b>作者</b>：Marc Fischer,  Alexander Bartler,  Bin Yang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：data rich environments, scarce annotations, standard when operating, operating in data, data rich</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural networks pre-trained on a self-supervision scheme have become the
standard when operating in data rich environments with scarce annotations. As
such, fine-tuning a model to a downstream task in a parameter-efficient but
effective way, e.g. for a new set of classes in the case of semantic
segmentation, is of increasing importance. In this work, we propose and
investigate several contributions to achieve a parameter-efficient but
effective adaptation for semantic segmentation on two medical imaging datasets.
Relying on the recently popularized prompt tuning approach, we provide a
prompt-able UNet (PUNet) architecture, that is frozen after pre-training, but
adaptable throughout the network by class-dependent learnable prompt tokens. We
pre-train this architecture with a dedicated dense self-supervision scheme
based on assignments to online generated prototypes (contrastive prototype
assignment, CPA) of a student teacher combination alongside a concurrent
segmentation loss on a subset of classes. We demonstrate that the resulting
neural network model is able to attenuate the gap between fully fine-tuned and
parameter-efficiently adapted models on CT imaging datasets. As such, the
difference between fully fine-tuned and prompt-tuned variants amounts to only
3.83 pp for the TCIA/BTCV dataset and 2.67 pp for the CT-ORG dataset in the
mean Dice Similarity Coefficient (DSC, in %) while only prompt tokens,
corresponding to 0.85% of the pre-trained backbone model with 6.8M frozen
parameters, are adjusted. The code for this work is available on
this https URL .</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Are we certain it's anomalous?</b></summary>
  <p><b>编号</b>：[219]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09224</p>
  <p><b>作者</b>：Alessandro Flaborea,  Bardh Prenkaj,  Bharti Munjal,  Marco Aurelio Sterpa,  Dario Aragona,  Luca Podo,  Fabio Galasso</p>
  <p><b>备注</b>：Submitted at IEEE Transactions on Neural Networks and Learning Systems</p>
  <p><b>关键词</b>：recently revamped research, modelling time series, anomaly detection, progress in modelling, structured-data has recently</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The progress in modelling time series and, more generally, sequences of
structured-data has recently revamped research in anomaly detection. The task
stands for identifying abnormal behaviours in financial series, IT systems,
aerospace measurements, and the medical domain, where anomaly detection may aid
in isolating cases of depression and attend the elderly. Anomaly detection in
time series is a complex task since anomalies are rare due to highly non-linear
temporal correlations and since the definition of anomalous is sometimes
subjective. Here we propose the novel use of Hyperbolic uncertainty for Anomaly
Detection (HypAD). HypAD learns self-supervisedly to reconstruct the input
signal. We adopt best practices from the state-of-the-art to encode the
sequence by an LSTM, jointly learnt with a decoder to reconstruct the signal,
with the aid of GAN critics. Uncertainty is estimated end-to-end by means of a
hyperbolic neural network. By using uncertainty, HypAD may assess whether it is
certain about the input signal but it fails to reconstruct it because this is
anomalous; or whether the reconstruction error does not necessarily imply
anomaly, as the model is uncertain, e.g. a complex but regular input signal.
The novel key idea is that a detectable anomaly is one where the model is
certain but it predicts wrongly. HypAD outperforms the current state-of-the-art
for univariate anomaly detection on established benchmarks based on data from
NASA, Yahoo, Numenta, Amazon, Twitter. It also yields state-of-the-art
performance on a multivariate dataset of anomaly activities in elderly home
residences, and it outperforms the baseline on SWaT. Overall, HypAD yields the
lowest false alarms at the best performance rate, thanks to successfully
identifying detectable anomalies.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：edBB-Demo: Biometrics and Behavior Analysis for Online Educational  Platforms</b></summary>
  <p><b>编号</b>：[220]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09210</p>
  <p><b>作者</b>：Roberto Daza,  Aythami Morales,  Ruben Tolosana,  Luis F. Gomez,  Julian Fierrez,  Javier Ortega-Garcia</p>
  <p><b>备注</b>：Accepted in "AAAI-23 Conference on Artificial Intelligence (Demonstration Program)"</p>
  <p><b>关键词</b>：AI-powered research platform, present edBB-Demo, AI-powered research, remote education, research platform</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present edBB-Demo, a demonstrator of an AI-powered research platform for
student monitoring in remote education. The edBB platform aims to study the
challenges associated to user recognition and behavior understanding in digital
platforms. This platform has been developed for data collection, acquiring
signals from a variety of sensors including keyboard, mouse, webcam,
microphone, smartwatch, and an Electroencephalography band. The information
captured from the sensors during the student sessions is modelled in a
multimodal learning framework. The demonstrator includes: i) Biometric user
authentication in an unsupervised environment; ii) Human action recognition
based on remote video analysis; iii) Heart rate estimation from webcam video;
and iv) Attention level estimation from facial expression analysis.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：Learning to Kindle the Starlight</b></summary>
  <p><b>编号</b>：[223]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09206</p>
  <p><b>作者</b>：Yu Yuan,  Jiaqi Wu,  Lindong Wang,  Zhongliang Jing,  Henry Leung,  Shuyuan Zhu,  Han Pan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：star field image, photographic skills needed, Capturing highly appreciated, star field, field image enhancement</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Capturing highly appreciated star field images is extremely challenging due
to light pollution, the requirements of specialized hardware, and the high
level of photographic skills needed. Deep learning-based techniques have
achieved remarkable results in low-light image enhancement (LLIE) but have not
been widely applied to star field image enhancement due to the lack of training
data. To address this problem, we construct the first Star Field Image
Enhancement Benchmark (SFIEB) that contains 355 real-shot and 854
semi-synthetic star field images, all having the corresponding reference
images. Using the presented dataset, we propose the first star field image
enhancement approach, namely StarDiffusion, based on conditional denoising
diffusion probabilistic models (DDPM). We introduce dynamic stochastic
corruptions to the inputs of conditional DDPM to improve the performance and
generalization of the network on our small-scale dataset. Experiments show
promising results of our method, which outperforms state-of-the-art low-light
image enhancement algorithms. The dataset and codes will be open-sourced.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：Learnable Graph Convolutional Network and Feature Fusion for Multi-view  Learning</b></summary>
  <p><b>编号</b>：[237]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09155</p>
  <p><b>作者</b>：Zhaoliang Chen,  Lele Fu,  Jie Yao,  Wenzhong Guo,  Claudia Plant,  Shiping Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：data depicting objectives, multi-view data depicting, practical applications, Learnable Graph Convolutional, graph convolutional network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In practical applications, multi-view data depicting objectives from assorted
perspectives can facilitate the accuracy increase of learning algorithms.
However, given multi-view data, there is limited work for learning
discriminative node relationships and graph information simultaneously via
graph convolutional network that has drawn the attention from considerable
researchers in recent years. Most of existing methods only consider the
weighted sum of adjacency matrices, yet a joint neural network of both feature
and graph fusion is still under-explored. To cope with these issues, this paper
proposes a joint deep learning framework called Learnable Graph Convolutional
Network and Feature Fusion (LGCN-FF), consisting of two stages: feature fusion
network and learnable graph convolutional network. The former aims to learn an
underlying feature representation from heterogeneous views, while the latter
explores a more discriminative graph fusion via learnable weights and a
parametric activation function dubbed Differentiable Shrinkage Activation (DSA)
function. The proposed LGCN-FF is validated to be superior to various
state-of-the-art methods in multi-view semi-supervised classification.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：A Unified Multimodal De- and Re-coupling Framework for RGB-D Motion  Recognition</b></summary>
  <p><b>编号</b>：[238]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09146</p>
  <p><b>作者</b>：Benjia Zhou,  Pichao Wang,  Jun Wan,  Yanyan Liang,  Fan Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：video classification models, computer vision, considerable parameters, Motion recognition, promising direction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Motion recognition is a promising direction in computer vision, but the
training of video classification models is much harder than images due to
insufficient data and considerable parameters. To get around this, some works
strive to explore multimodal cues from RGB-D data. Although improving motion
recognition to some extent, these methods still face sub-optimal situations in
the following aspects: (i) Data augmentation, i.e., the scale of the RGB-D
datasets is still limited, and few efforts have been made to explore novel data
augmentation strategies for videos; (ii) Optimization mechanism, i.e., the
tightly space-time-entangled network structure brings more challenges to
spatiotemporal information modeling; And (iii) cross-modal knowledge fusion,
i.e., the high similarity between multimodal representations caused to
insufficient late fusion. To alleviate these drawbacks, we propose to improve
RGB-D-based motion recognition both from data and algorithm perspectives in
this paper. In more detail, firstly, we introduce a novel video data
augmentation method dubbed ShuffleMix, which acts as a supplement to MixUp, to
provide additional temporal regularization for motion recognition. Secondly, a
Unified Multimodal De-coupling and multi-stage Re-coupling framework, termed
UMDR, is proposed for video representation learning. Finally, a novel
cross-modal Complement Feature Catcher (CFCer) is explored to mine potential
commonalities features in multimodal information as the auxiliary fusion
stream, to improve the late fusion results. The seamless combination of these
novel designs forms a robust spatiotemporal representation and achieves better
performance than state-of-the-art methods on four public motion datasets.
Specifically, UMDR achieves unprecedented improvements of +4.5% on the Chalearn
IsoGD dataset.Our code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：Mapping Tropical Forest Cover and Deforestation with Planet NICFI  Satellite Images and Deep Learning in Mato Grosso State (Brazil) from 2015 to  2021</b></summary>
  <p><b>编号</b>：[240]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09806</p>
  <p><b>作者</b>：Fabien H Wagner,  Ricardo Dalagnol,  Celso HL Silva-Junior,  Griffin Carter,  Alison L Ritz,  Mayumi CM Hirye,  Jean PHB Ometto,  Sassan Saatchi</p>
  <p><b>备注</b>：18 pages, 10 figures, submitted to Remote Sensing MDPI, Special Issue "Remote Sensing of the Amazon Region"</p>
  <p><b>关键词</b>：climate mitigation policy, tree cover, reducing carbon, December, rapid assessment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Monitoring changes in tree cover for rapid assessment of deforestation is
considered the critical component of any climate mitigation policy for reducing
carbon. Here, we map tropical tree cover and deforestation between 2015 and
2022 using 5 m spatial resolution Planet NICFI satellite images over the state
of Mato Grosso (MT) in Brazil and a U-net deep learning model. The tree cover
for the state was 556510.8 km$^2$ in 2015 (58.1 % of the MT State) and was
reduced to 141598.5 km$^2$ (14.8 % of total area) at the end of 2021. After
reaching a minimum deforested area in December 2016 with 6632.05 km$^2$, the
bi-annual deforestation area only showed a slight increase between December
2016 and December 2019. A year after, the areas of deforestation almost doubled
from 9944.5 km$^2$ in December 2019 to 19817.8 km$^2$ in December 2021. The
high-resolution data product showed relatively consistent agreement with the
official deforestation map from Brazil (67.2%) but deviated significantly from
year of forest cover loss estimates from the Global Forest change (GFC)
product, mainly due to large area of fire degradation observed in the GFC data.
High-resolution imagery from Planet NICFI associated with deep learning
technics can significantly improve mapping deforestation extent in tropics.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：DeepSense 6G: A Large-Scale Real-World Multi-Modal Sensing and  Communication Dataset</b></summary>
  <p><b>编号</b>：[243]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09769</p>
  <p><b>作者</b>：Ahmed Alkhateeb,  Gouranga Charan,  Tawfik Osman,  Andrew Hredzak,  João Morais,  Umut Demirhan,  Nikhil Srinivas</p>
  <p><b>备注</b>：The dataset is available on the DeepSense 6G website this http URL</p>
  <p><b>关键词</b>：co-existing multi-modal sensing, large-scale dataset based, multi-modal sensing, based on real-world, real-world measurements</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This article presents the DeepSense 6G dataset, which is a large-scale
dataset based on real-world measurements of co-existing multi-modal sensing and
communication data. The DeepSense 6G dataset is built to advance deep learning
research in a wide range of applications in the intersection of multi-modal
sensing, communication, and positioning. This article provides a detailed
overview of the DeepSense dataset structure, adopted testbeds, data collection
and processing methodology, deployment scenarios, and example applications,
with the objective of facilitating the adoption and reproducibility of
multi-modal sensing and communication datasets.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：A Survey on Evaluation Metrics for Synthetic Material Micro-Structure  Images from Generative Models</b></summary>
  <p><b>编号</b>：[247]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09727</p>
  <p><b>作者</b>：Devesh Shah (1),  Anirudh Suresh (2),  Alemayehu Admasu (1),  Devesh Upadhyay (1),  Kalyanmoy Deb (2) ((1) Ford Motor Company, (2) Michigan State University)</p>
  <p><b>备注</b>：Accepted in Neural Information Processing Systems (NeurIPS) 2022 Workshop on AI for Accelerated Materials Design (AI4Mat). Selected as spotlight paper for workshop</p>
  <p><b>关键词</b>：Fréchet Inception Distance, materials science research, emerging problem, research have evolved, synthetic micro-structure images</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The evaluation of synthetic micro-structure images is an emerging problem as
machine learning and materials science research have evolved together. Typical
state of the art methods in evaluating synthetic images from generative models
have relied on the Fréchet Inception Distance. However, this and other
similar methods, are limited in the materials domain due to both the unique
features that characterize physically accurate micro-structures and limited
dataset sizes. In this study we evaluate a variety of methods on scanning
electron microscope (SEM) images of graphene-reinforced polyurethane foams. The
primary objective of this paper is to report our findings with regards to the
shortcomings of existing methods so as to encourage the machine learning
community to consider enhancements in metrics for assessing quality of
synthetic images in the material science domain.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：Enabling Collagen Quantification on HE-stained Slides Through Stain  Deconvolution and Restained HE-HES</b></summary>
  <p><b>编号</b>：[256]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09566</p>
  <p><b>作者</b>：Guillaume Balezo,  Christof A. Bertram,  Cyprien Tilmant,  Stéphanie Petit,  Saima Ben Hadj,  Rutger H.J. Fick</p>
  <p><b>备注</b>：This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible</p>
  <p><b>关键词</b>：Hematoxylin and Eosin, routine Hematoxylin, cancer malignancy, extra-cellular matrix, diagnostic and prognostic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In histology, the presence of collagen in the extra-cellular matrix has both
diagnostic and prognostic value for cancer malignancy, and can be highlighted
by adding Saffron (S) to a routine Hematoxylin and Eosin (HE) staining.
However, Saffron is not usually added because of the additional cost and
because pathologists are accustomed to HE, with the exception of France-based
laboratories. In this paper, we show that it is possible to quantify the
collagen content from the HE image alone and to digitally create an HES image.
To do so, we trained a UNet to predict the Saffron densities from HE images. We
created a dataset of registered, restained HE-HES slides and we extracted the
Saffron concentrations as ground truth using stain deconvolution on the HES
images. Our model reached a Mean Absolute Error of 0.0668 $\pm$ 0.0002 (Saffron
values between 0 and 1) on a 3-fold testing set. We hope our approach can aid
in improving the clinical workflow while reducing reagent costs for
laboratories.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：Interpretable HER2 scoring by evaluating clinical Guidelines through a  weakly supervised, constrained Deep Learning Approach</b></summary>
  <p><b>编号</b>：[257]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09559</p>
  <p><b>作者</b>：Manh Dan Pham,  Cyprien Tilmant,  Stéphanie Petit,  Isabelle Salmon,  Saima Ben Hadj,  Rutger H.J. Fick</p>
  <p><b>备注</b>：Submitted to Elsevier</p>
  <p><b>关键词</b>：Human Epidermal growth, Epidermal growth factor, Human Epidermal, cancer treatment selection, important prognostic biomarker</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The evaluation of the Human Epidermal growth factor Receptor-2 (HER2)
expression is an important prognostic biomarker for breast cancer treatment
selection. However, HER2 scoring has notoriously high interobserver variability
due to stain variations between centers and the need to estimate visually the
staining intensity in specific percentages of tumor area. In this paper,
focusing on the interpretability of HER2 scoring by a pathologist, we propose a
semi-automatic, two-stage deep learning approach that directly evaluates the
clinical HER2 guidelines defined by the American Society of Clinical Oncology/
College of American Pathologists (ASCO/CAP). In the first stage, we segment the
invasive tumor over the user-indicated Region of Interest (ROI). Then, in the
second stage, we classify the tumor tissue into four HER2 classes. For the
classification stage, we use weakly supervised, constrained optimization to
find a model that classifies cancerous patches such that the tumor surface
percentage meets the guidelines specification of each HER2 class. We end the
second stage by freezing the model and refining its output logits in a
supervised way to all slide labels in the training set. To ensure the quality
of our dataset's labels, we conducted a multi-pathologist HER2 scoring
consensus. For the assessment of doubtful cases where no consensus was found,
our model can help by interpreting its HER2 class percentages output. We
achieve a performance of 0.78 in F1-score on the test set while keeping our
model interpretable for the pathologist, hopefully contributing to
interpretable AI models in digital pathology.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：Parameter-Efficient Transformer with Hybrid Axial-Attention for Medical  Image Segmentation</b></summary>
  <p><b>编号</b>：[260]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09533</p>
  <p><b>作者</b>：Yiyue Hu,  Lei Zhang,  Nan Mu,  Lei Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：image analysis owing, achieved remarkable success, medical image analysis, intrinsic inductive bias, achieved remarkable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transformers have achieved remarkable success in medical image analysis owing
to their powerful capability to use flexible self-attention mechanism. However,
due to lacking intrinsic inductive bias in modeling visual structural
information, they generally require a large-scale pre-training schedule,
limiting the clinical applications over expensive small-scale medical data. To
this end, we propose a parameter-efficient transformer to explore intrinsic
inductive bias via position information for medical image segmentation.
Specifically, we empirically investigate how different position encoding
strategies affect the prediction quality of the region of interest (ROI), and
observe that ROIs are sensitive to the position encoding strategies. Motivated
by this, we present a novel Hybrid Axial-Attention (HAA), a form of position
self-attention that can be equipped with spatial pixel-wise information and
relative position information as inductive bias. Moreover, we introduce a
gating mechanism to alleviate the burden of training schedule, resulting in
efficient feature selection over small-scale datasets. Experiments on the BraTS
and Covid19 datasets prove the superiority of our method over the baseline and
previous works. Internal workflow visualization with interpretability is
conducted to better validate our success.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：RDRN: Recursively Defined Residual Network for Image Super-Resolution</b></summary>
  <p><b>编号</b>：[267]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09462</p>
  <p><b>作者</b>：Alexander Panaetov,  Karim Elhadji Daou,  Igor Samenko,  Evgeny Tetin,  Ilya Ivanov</p>
  <p><b>备注</b>：Accepted to ACCV 2022</p>
  <p><b>关键词</b>：Deep convolutional neural, obtained remarkable performance, convolutional neural networks, single image super-resolution, convolutional neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep convolutional neural networks (CNNs) have obtained remarkable
performance in single image super-resolution (SISR). However, very deep
networks can suffer from training difficulty and hardly achieve further
performance gain. There are two main trends to solve that problem: improving
the network architecture for better propagation of features through large
number of layers and designing an attention mechanism for selecting most
informative features. Recent SISR solutions propose advanced attention and
self-attention mechanisms. However, constructing a network to use an attention
block in the most efficient way is a challenging problem. To address this
issue, we propose a general recursively defined residual block (RDRB) for
better feature extraction and propagation through network layers. Based on RDRB
we designed recursively defined residual network (RDRN), a novel network
architecture which utilizes attention blocks efficiently. Extensive experiments
show that the proposed model achieves state-of-the-art results on several
popular super-resolution benchmarks and outperforms previous methods by up to
0.43 dB.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：Hard Exudate Segmentation Supplemented by Super-Resolution with  Multi-scale Attention Fusion Module</b></summary>
  <p><b>编号</b>：[269]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09404</p>
  <p><b>作者</b>：Jiayi Zhang,  Xiaoshan Chen,  Zhongxi Qiu,  Mingming Yang,  Yan Hu,  Jiang Liu</p>
  <p><b>备注</b>：Accepted by IEEE BIBM 2022</p>
  <p><b>关键词</b>：retina edema, specific biomarker, biomarker for retina, Hard exudates, tiny lesions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Hard exudates (HE) is the most specific biomarker for retina edema. Precise
HE segmentation is vital for disease diagnosis and treatment, but automatic
segmentation is challenged by its large variation of characteristics including
size, shape and position, which makes it difficult to detect tiny lesions and
lesion boundaries. Considering the complementary features between segmentation
and super-resolution tasks, this paper proposes a novel hard exudates
segmentation method named SS-MAF with an auxiliary super-resolution task, which
brings in helpful detailed features for tiny lesion and boundaries detection.
Specifically, we propose a fusion module named Multi-scale Attention Fusion
(MAF) module for our dual-stream framework to effectively integrate features of
the two tasks. MAF first adopts split spatial convolutional (SSC) layer for
multi-scale features extraction and then utilize attention mechanism for
features fusion of the two tasks. Considering pixel dependency, we introduce
region mutual information (RMI) loss to optimize MAF module for tiny lesions
and boundary detection. We evaluate our method on two public lesion datasets,
IDRiD and E-Ophtha. Our method shows competitive performance with
low-resolution inputs, both quantitatively and qualitatively. On E-Ophtha
dataset, the method can achieve $\geq3\%$ higher dice and recall compared with
the state-of-the-art methods.</p>
  </details>
</details>
<h1>自然语言处理</h1>
<details>
  <summary>1. <b>标题：InstructPix2Pix: Learning to Follow Image Editing Instructions</b></summary>
  <p><b>编号</b>：[4]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09800</p>
  <p><b>作者</b>：Tim Brooks,  Aleksander Holynski,  Alexei A. Efros</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：propose a method, Stable Diffusion, model, images, instructions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a method for editing images from human instructions: given an
input image and a written instruction that tells the model what to do, our
model follows these instructions to edit the image. To obtain training data for
this problem, we combine the knowledge of two large pretrained models -- a
language model (GPT-3) and a text-to-image model (Stable Diffusion) -- to
generate a large dataset of image editing examples. Our conditional diffusion
model, InstructPix2Pix, is trained on our generated data, and generalizes to
real images and user-written instructions at inference time. Since it performs
edits in the forward pass and does not require per example fine-tuning or
inversion, our model edits images quickly, in a matter of seconds. We show
compelling editing results for a diverse collection of input images and written
instructions.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：UniSumm: Unified Few-shot Summarization with Multi-Task Pre-Training and  Prefix-Tuning</b></summary>
  <p><b>编号</b>：[13]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09783</p>
  <p><b>作者</b>：Yulong Chen,  Yang Liu,  Ruochen Xu,  Ziyi Yang,  Chenguang Zhu,  Michael Zeng,  Yue Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：high annotation costs, few-shot summarization, summarization tasks, summarization, few-shot summarization systems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The diverse demands of different summarization tasks and their high
annotation costs are driving a need for few-shot summarization. However,
despite the emergence of many summarization tasks and datasets, the current
training paradigm for few-shot summarization systems ignores potentially
shareable knowledge in heterogeneous datasets. To this end, we propose
\textsc{UniSumm}, a unified few-shot summarization model pre-trained with
multiple summarization tasks and can be prefix-tuned to excel at any few-shot
summarization datasets. Meanwhile, to better evaluate few-shot summarization
systems, under the principles of diversity and robustness, we assemble and
publicize a new benchmark \textsc{SummZoo}. It consists of $8$ diverse
summarization tasks with multiple sets of few-shot samples for each task,
covering both monologue and dialogue domains. Experimental results and ablation
studies show that \textsc{UniSumm} outperforms strong baseline systems by a
large margin across all tasks in \textsc{SummZoo} under both automatic and
human evaluations. We release our code and benchmark at
\url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Efficient Transformers with Dynamic Token Pooling</b></summary>
  <p><b>编号</b>：[21]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09761</p>
  <p><b>作者</b>：Piotr Nawrot,  Jan Chorowski,  Adrian Łańcucki,  Edoardo M. Ponti</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：achieve unrivalled performance, Transformers achieve unrivalled, time complexity, achieve unrivalled, unrivalled performance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transformers achieve unrivalled performance in modelling language, but remain
inefficient in terms of memory and time complexity. A possible remedy is to
reduce the sequence length in the intermediate layers by pooling fixed-length
segments of tokens. Nevertheless, natural units of meaning, such as words or
phrases, display varying sizes. To address this mismatch, we equip language
models with a dynamic-pooling mechanism, which predicts segment boundaries in
an autoregressive fashion. We compare several methods to infer boundaries,
including end-to-end learning through stochastic re-parameterisation,
supervised learning (based on segmentations from subword tokenizers or spikes
in conditional entropy), as well as linguistically motivated boundaries. We
perform character-level evaluation on texts from multiple datasets and
morphologically diverse languages. The results demonstrate that dynamic
pooling, which jointly segments and models language, is often both faster and
more accurate than vanilla Transformers and fixed-length pooling within the
same computational budget.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Probing for Incremental Parse States in Autoregressive Language Models</b></summary>
  <p><b>编号</b>：[25]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09748</p>
  <p><b>作者</b>：Tiwalayo Eisape,  Vineet Gangireddy,  Roger P. Levy,  Yoon Kim</p>
  <p><b>备注</b>：Findings of EMNLP 2022</p>
  <p><b>关键词</b>：show remarkable sensitivity, models show remarkable, sensitivity to syntax, show remarkable, remarkable sensitivity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Next-word predictions from autoregressive neural language models show
remarkable sensitivity to syntax. This work evaluates the extent to which this
behavior arises as a result of a learned ability to maintain implicit
representations of incremental syntactic structures. We extend work in
syntactic probing to the incremental setting and present several probes for
extracting incomplete syntactic structure (operationalized through parse states
from a stack-based parser) from autoregressive language models. We find that
our probes can be used to predict model preferences on ambiguous sentence
prefixes and causally intervene on model representations and steer model
behavior. This suggests implicit incremental syntactic inferences underlie
next-word predictions in autoregressive neural language models.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Zero-Shot Dynamic Quantization for Transformer Inference</b></summary>
  <p><b>编号</b>：[27]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09744</p>
  <p><b>作者</b>：Yousef El-Kurdi,  Jerry Quinn,  Avirup Sil</p>
  <p><b>备注</b>：To appear in EMNLP 2022 industry track</p>
  <p><b>关键词</b>：quantizing BERT-like models, significantly reducing, reducing the accuracy, accuracy loss, quantizing BERT-like</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce a novel run-time method for significantly reducing the accuracy
loss associated with quantizing BERT-like models to 8-bit integers. Existing
methods for quantizing models either modify the training procedure,or they
require an additional calibration step to adjust parameters that also requires
a selected held-out dataset. Our method permits taking advantage of
quantization without the need for these adjustments. We present results on
several NLP tasks demonstrating the usefulness of this technique.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：BERT-Deep CNN: State-of-the-Art for Sentiment Analysis of COVID-19  Tweets</b></summary>
  <p><b>编号</b>：[30]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09733</p>
  <p><b>作者</b>：Javad Hassannataj Joloudari,  Sadiq Hussain,  Mohammad Ali Nematollahi,  Rouhollah Bagheri,  Fatemeh Fazl,  Roohallah Alizadehsani,  Reza Lashgari</p>
  <p><b>备注</b>：19 pages, 5 figures</p>
  <p><b>关键词</b>：social media, social media technology, social, free flow, flow of information</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The free flow of information has been accelerated by the rapid development of
social media technology. There has been a significant social and psychological
impact on the population due to the outbreak of Coronavirus disease (COVID-19).
The COVID-19 pandemic is one of the current events being discussed on social
media platforms. In order to safeguard societies from this pandemic, studying
people's emotions on social media is crucial. As a result of their particular
characteristics, sentiment analysis of texts like tweets remains challenging.
Sentiment analysis is a powerful text analysis tool. It automatically detects
and analyzes opinions and emotions from unstructured data. Texts from a wide
range of sources are examined by a sentiment analysis tool, which extracts
meaning from them, including emails, surveys, reviews, social media posts, and
web articles. To evaluate sentiments, natural language processing (NLP) and
machine learning techniques are used, which assign weights to entities, topics,
themes, and categories in sentences or phrases. Machine learning tools learn
how to detect sentiment without human intervention by examining examples of
emotions in text. In a pandemic situation, analyzing social media texts to
uncover sentimental trends can be very helpful in gaining a better
understanding of society's needs and predicting future trends. We intend to
study society's perception of the COVID-19 pandemic through social media using
state-of-the-art BERT and Deep CNN models. The superiority of BERT models over
other deep models in sentiment analysis is evident and can be concluded from
the comparison of the various research studies mentioned in this article.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Extending Logic Explained Networks to Text Classification</b></summary>
  <p><b>编号</b>：[31]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09732</p>
  <p><b>作者</b>：Rishabh Jain,  Gabriele Ciravegna,  Pietro Barbiero,  Francesco Giannini,  Davide Buffelli,  Pietro Lio</p>
  <p><b>备注</b>：Accepted as short paper at the EMNLP 2022 conference</p>
  <p><b>关键词</b>：Logic Explained Networks, Explained Networks, neural models providing, models providing logic, Logic Explained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, Logic Explained Networks (LENs) have been proposed as
explainable-by-design neural models providing logic explanations for their
predictions. However, these models have only been applied to vision and tabular
data, and they mostly favour the generation of global explanations, while local
ones tend to be noisy and verbose. For these reasons, we propose LENp,
improving local explanations by perturbing input words, and we test it on text
classification. Our results show that (i) LENp provides better local
explanations than LIME in terms of sensitivity and faithfulness, and (ii) logic
explanations are more useful and user-friendly than feature scoring provided by
LIME as attested by a human survey.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Stutter-TTS: Controlled Synthesis and Improved Recognition of Stuttered  Speech</b></summary>
  <p><b>编号</b>：[32]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09731</p>
  <p><b>作者</b>：Xin Zhang,  Iván Vallés-Pérez,  Andreas Stolcke,  Chengzhu Yu,  Jasha Droppo,  Olabanji Shonibare,  Roberto Barra-Chicote,  Venkatesh Ravichandran</p>
  <p><b>备注</b>：8 pages, 3 figures, 2 tables</p>
  <p><b>关键词</b>：interrupted by blocks, repetitions or prolongations, prolongations of syllables, natural flow, speech</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Stuttering is a speech disorder where the natural flow of speech is
interrupted by blocks, repetitions or prolongations of syllables, words and
phrases. The majority of existing automatic speech recognition (ASR) interfaces
perform poorly on utterances with stutter, mainly due to lack of matched
training data. Synthesis of speech with stutter thus presents an opportunity to
improve ASR for this type of speech. We describe Stutter-TTS, an end-to-end
neural text-to-speech model capable of synthesizing diverse types of stuttering
utterances. We develop a simple, yet effective prosody-control strategy whereby
additional tokens are introduced into source text during training to represent
specific stuttering characteristics. By choosing the position of the stutter
tokens, Stutter-TTS allows word-level control of where stuttering occurs in the
synthesized utterance. We are able to synthesize stutter events with high
accuracy (F1-scores between 0.63 and 0.84, depending on stutter type). By
fine-tuning an ASR model on synthetic stuttered speech we are able to reduce
word error by 5.7% relative on stuttered utterances, with only minor (<0.2% relative) degradation for fluent utterances.< p>
  </0.2%></p></details>
</details>
<details>
  <summary>9. <b>标题：Generative Adversarial Training Can Improve Neural Language Models</b></summary>
  <p><b>编号</b>：[34]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09728</p>
  <p><b>作者</b>：Sajad Movahedi,  Azadeh Shakery</p>
  <p><b>备注</b>：An extended abstract selected for poster presentation at the Eastern European Machine Learning Summer School 2019</p>
  <p><b>关键词</b>：neural language modeling, recurrent neural networks, neural language models, unresolved issue, neural language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While deep learning in the form of recurrent neural networks (RNNs) has
caused a significant improvement in neural language modeling, the fact that
they are extremely prone to overfitting is still a mainly unresolved issue. In
this paper we propose a regularization method based on generative adversarial
networks (GANs) and adversarial training (AT), that can prevent overfitting in
neural language models. Unlike common adversarial training methods such as the
fast gradient sign method (FGSM) that require a second back-propagation through
time, and therefore effectively require at least twice the amount of time for
regular training, the overhead of our method does not exceed more than 20% of
the training of the baselines.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Federated Multilingual Models for Medical Transcript Analysis</b></summary>
  <p><b>编号</b>：[37]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09722</p>
  <p><b>作者</b>：Andre Manoel,  Mirian Hipolito Garcia,  Tal Baumel,  Shize Su,  Jialei Chen,  Dan Miller,  Danny Karmon,  Robert Sim,  Dimitrios Dimitriadis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：machine learning approach, decentralized data sources, data, data access constraints, Federated Learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Federated Learning (FL) is a novel machine learning approach that allows the
model trainer to access more data samples, by training the model across
multiple decentralized data sources, while data access constraints are in
place. Such trained models can achieve significantly higher performance beyond
what can be done when trained on a single data source. As part of FL's
promises, none of the training data is ever transmitted to any central
location, ensuring that sensitive data remains local and private. These
characteristics make FL perfectly suited for large-scale applications in
healthcare, where a variety of compliance constraints restrict how data may be
handled, processed, and stored. Despite the apparent benefits of federated
learning, the heterogeneity in the local data distributions pose significant
challenges, and such challenges are even more pronounced in the case of
multilingual data providers. In this paper we present a federated learning
system for training a large-scale multi-lingual model suitable for fine-tuning
on downstream tasks such as medical entity tagging. Our work represents one of
the first such production-scale systems, capable of training across multiple
highly heterogeneous data providers, and achieving levels of accuracy that
could not be otherwise achieved by using central training with public data.
Finally, we show that the global model performance can be further improved by a
training step performed locally.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Numerical Optimizations for Weighted Low-rank Estimation on Language  Model</b></summary>
  <p><b>编号</b>：[40]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09718</p>
  <p><b>作者</b>：Ting Hua,  Yen-Chang Hsu,  Felicity Wang,  Qian Lou,  Yilin Shen,  Hongxia Jin</p>
  <p><b>备注</b>：long paper EMNLP 2022</p>
  <p><b>关键词</b>：popular compression methods, smaller matrices, popular compression, approximate a target, SVD</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Singular value decomposition (SVD) is one of the most popular compression
methods that approximate a target matrix with smaller matrices. However,
standard SVD treats the parameters within the matrix with equal importance,
which is a simple but unrealistic assumption. The parameters of a trained
neural network model may affect task performance unevenly, which suggests
non-equal importance among the parameters. Compared to SVD, the decomposition
method aware of parameter importance is the more practical choice in real
cases. Unlike standard SVD, weighted value decomposition is a non-convex
optimization problem that lacks a closed-form solution. We systematically
investigated multiple optimization strategies to tackle the problem and
examined our method by compressing Transformer-based language models. Further,
we designed a metric to predict when the SVD may introduce a significant
performance drop, for which our method can be a rescue strategy. The extensive
evaluations demonstrate that our method can perform better than current SOTA
methods in compressing Transformer-based language models.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Design Considerations For Hypothesis Rejection Modules In Spoken  Language Understanding Systems</b></summary>
  <p><b>编号</b>：[45]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09711</p>
  <p><b>作者</b>：Aman Alok,  Rahul Gupta,  Shankar Ananthakrishnan</p>
  <p><b>备注</b>：5 pages. ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</p>
  <p><b>关键词</b>：Spoken Language Understanding, Language Understanding, machine learning models, Spoken Language, SLU hypothesis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Spoken Language Understanding (SLU) systems typically consist of a set of
machine learning models that operate in conjunction to produce an SLU
hypothesis. The generated hypothesis is then sent to downstream components for
further action. However, it is desirable to discard an incorrect hypothesis
before sending it downstream. In this work, we present two designs for SLU
hypothesis rejection modules: (i) scheme R1 that performs rejection on domain
specific SLU hypothesis and, (ii) scheme R2 that performs rejection on
hypothesis generated from the overall SLU system. Hypothesis rejection modules
in both schemes reject/accept a hypothesis based on features drawn from the
utterance directed to the SLU system, the associated SLU hypothesis and SLU
confidence score. Our experiments suggest that both the schemes yield similar
results (scheme R1: 2.5% FRR @ 4.5% FAR, scheme R2: 2.5% FRR @ 4.6% FAR), with
the best performing systems using all the available features. We argue that
while either of the rejection schemes can be chosen over the other, they carry
some inherent differences which need to be considered while making this choice.
Additionally, we incorporate ASR features in the rejection module (obtaining an
1.9% FRR @ 3.8% FAR) and analyze the improvements.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Style Classification of Rabbinic Literature for Detection of Lost  Midrash Tanhuma Material</b></summary>
  <p><b>编号</b>：[46]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09710</p>
  <p><b>作者</b>：Shlomo Tannor,  Nachum Dershowitz,  Moshe Lavee</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：complex rabbinic works, multiple languages, written transmission, collections are complex, works that consist</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Midrash collections are complex rabbinic works that consist of text in
multiple languages, which evolved through long processes of unstable oral and
written transmission. Determining the origin of a given passage in such a
compilation is not always straightforward and is often a matter of dispute
among scholars, yet it is essential for scholars' understanding of the passage
and its relationship to other texts in the rabbinic corpus.
To help solve this problem, we propose a system for classification of
rabbinic literature based on its style, leveraging recently released pretrained
Transformer models for Hebrew. Additionally, we demonstrate how our method can
be applied to uncover lost material from Midrash Tanhuma.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：PromptCap: Prompt-Guided Task-Aware Image Captioning</b></summary>
  <p><b>编号</b>：[53]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09699</p>
  <p><b>作者</b>：Yushi Hu,  Hang Hua,  Zhengyuan Yang,  Weijia Shi,  Noah A. Smith,  Jiebo Luo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：allowing powerful language, natural language sentence, allowing powerful, Image, powerful language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image captioning aims to describe an image with a natural language sentence,
allowing powerful language models to understand images. The framework of
combining image captioning with language models has been successful on various
vision-language tasks. However, an image contains much more information than a
single sentence, leading to underspecification of which visual entities should
be described in the caption sentence. For example, when performing visual
questioning answering (VQA), generic image captions often miss visual details
that are essential for the language model to answer correctly. To address this
challenge, we propose PromptCap, a captioning model that takes a
natural-language prompt to control the contents of the generated caption. The
prompt contains a question that the caption should help to answer, and also
supports taking auxiliary text inputs such as scene text within the image
itself. To finetune a general image caption model for prompt-guided captioning,
we propose a pipeline to synthesize and filter training examples with GPT-3 and
existing VQA datasets. For evaluation, we start with an existing pipeline in
which a language model is prompted with image captions to carry out VQA. With
the same language model, a higher QA accuracy shows that our generated captions
are more relevant to the question prompts. PromptCap outperforms generic
captions by a large margin on a variety of VQA tasks and achieves the
state-of-the-art accuracy of 58.8 % on OK-VQA and 58.0 % on A-OKVQA. Zero-shot
experiments on WebQA show that PromptCap generalizes well to unseen domains.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：The Effectiveness of Bidirectional Generative Patent Language Models</b></summary>
  <p><b>编号</b>：[55]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09690</p>
  <p><b>作者</b>：Jieh-Sheng Lee</p>
  <p><b>备注</b>：10 pages, 3 figures, 5 tables</p>
  <p><b>关键词</b>：Generative patent language, patent language models, effectiveness, autocomplete, generative models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generative patent language models can assist humans to write patent text more
effectively. The question is how to measure effectiveness from a human-centric
perspective and how to improve effectiveness. In this manuscript, a simplified
design of the autocomplete function is proposed to increase effectiveness by
more than 10%. With the new design, the effectiveness of autocomplete can reach
more than 60%, which means that more than 60% of keystrokes can be saved by
autocomplete. Since writing patent text does not necessarily start from the
beginning to the end, a question is whether the generative model can assist a
user no matter where to start writing. To answer the question, the generative
models in this manuscript are pre-trained with training data in both
directions. The generative models become bidirectional. Since text generation
is bidirectional, the calculation of autocomplete effectiveness can be
bidirectional and starts from anywhere in the text. After thorough experiments,
a key finding is that the autocomplete effectiveness of a model for the same
text remains similar no matter where the calculation starts. The finding
indicates that such bidirectional models can assist a user at a similar level,
no matter where the user starts to write.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Analyse der Entwicklungstreiber militärischer Schwarmdrohnen durch  Natural Language Processing</b></summary>
  <p><b>编号</b>：[59]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09680</p>
  <p><b>作者</b>：Manuel Mundt</p>
  <p><b>备注</b>：5 pages, in German, 4 figures</p>
  <p><b>关键词</b>：increasingly prominent role, armed conflict, taking an increasingly, increasingly prominent, prominent role</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Military drones are taking an increasingly prominent role in armed conflict,
and the use of multiple drones in a swarm can be useful. Who the drivers of the
research are and what sub-domains exist is analyzed and visually presented in
this research using NLP techniques based on 946 studies. Most research is
conducted in the Western world, led by the United States, the United Kingdom,
and Germany. Through Tf-idf scoring, it is shown that countries have
significant differences in the subdomains studied. Overall, 2019 and 2020 saw
the most works published, with significant interest in military swarm drones as
early as 2008. This study provides a first glimpse into research in this area
and prompts further investigation.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Cross-Modal Adapter for Text-Video Retrieval</b></summary>
  <p><b>编号</b>：[80]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09623</p>
  <p><b>作者</b>：Haojun Jiang,  Jianke Zhang,  Rui Huang,  Chunjiang Ge,  Zanlin Ni,  Jiwen Lu,  Jie Zhou,  Shiji Song,  Gao Huang</p>
  <p><b>备注</b>：Tech Report</p>
  <p><b>关键词</b>：important multi-modal learning, text query, multi-modal learning task, relevant video, learning task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Text-video retrieval is an important multi-modal learning task, where the
goal is to retrieve the most relevant video for a given text query. Recently,
pre-trained models, e.g., CLIP, show great potential on this task. However, as
pre-trained models are scaling up, fully fine-tuning them on text-video
retrieval datasets has a high risk of overfitting. Moreover, in practice, it
would be costly to train and store a large model for each task. To overcome the
above issues, we present a novel $\textbf{Cross-Modal Adapter}$ for
parameter-efficient fine-tuning. Inspired by adapter-based methods, we adjust
the pre-trained model with a few parameterization layers. However, there are
two notable differences. First, our method is designed for the multi-modal
domain. Secondly, it allows early cross-modal interactions between CLIP's two
encoders. Although surprisingly simple, our approach has three notable
benefits: (1) reduces $\textbf{99.6}\%$ of fine-tuned parameters, and
alleviates the problem of overfitting, (2) saves approximately 30% of training
time, and (3) allows all the pre-trained parameters to be fixed, enabling the
pre-trained model to be shared across datasets. Extensive experiments
demonstrate that, without bells and whistles, it achieves superior or
comparable performance compared to fully fine-tuned methods on MSR-VTT, MSVD,
VATEX, ActivityNet, and DiDeMo datasets. The code will be available at
\url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Towards Building Text-To-Speech Systems for the Next Billion Users</b></summary>
  <p><b>编号</b>：[107]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09536</p>
  <p><b>作者</b>：Gokul Karthik Kumar,  Praveen S V,  Pratyush Kumar,  Mitesh M. Khapra,  Karthik Nandakumar</p>
  <p><b>备注</b>：Under review in ICASSP 2023. First two authors contributed equally</p>
  <p><b>关键词</b>：Deep learning based, Deep learning, evolving rapidly, training methodologies, languages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning based text-to-speech (TTS) systems have been evolving rapidly
with advances in model architectures, training methodologies, and
generalization across speakers and languages. However, these advances have not
been thoroughly investigated for Indian language speech synthesis. Such
investigation is computationally expensive given the number and diversity of
Indian languages, relatively lower resource availability, and the diverse set
of advances in neural TTS that remain untested. In this paper, we evaluate the
choice of acoustic models, vocoders, supplementary loss functions, training
schedules, and speaker and language diversity for Dravidian and Indo-Aryan
languages. Based on this, we identify monolingual models with FastPitch and
HiFi-GAN V1, trained jointly on male and female speakers to perform the best.
With this setup, we train and evaluate TTS models for 13 languages and find our
models to significantly improve upon existing models in all languages as
measured by mean opinion scores. We open-source all models on the Bhashini
platform.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Ignore Previous Prompt: Attack Techniques For Language Models</b></summary>
  <p><b>编号</b>：[109]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09527</p>
  <p><b>作者</b>：Fábio Perez,  Ian Ribeiro</p>
  <p><b>备注</b>：ML Safety Workshop NeurIPS 2022</p>
  <p><b>关键词</b>：large-scale customer-facing applications, Transformer-based large language, natural language tasks, Transformer-based large, provide a powerful</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transformer-based large language models (LLMs) provide a powerful foundation
for natural language tasks in large-scale customer-facing applications.
However, studies that explore their vulnerabilities emerging from malicious
user interaction are scarce. By proposing PromptInject, a prosaic alignment
framework for mask-based iterative adversarial prompt composition, we examine
how GPT-3, the most widely deployed language model in production, can be easily
misaligned by simple handcrafted inputs. In particular, we investigate two
types of attacks -- goal hijacking and prompt leaking -- and demonstrate that
even low-aptitude, but sufficiently ill-intentioned agents, can easily exploit
GPT-3's stochastic nature, creating long-tail risks. The code for PromptInject
is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Hey ASR System! Why Aren't You More Inclusive? Automatic Speech  Recognition Systems' Bias and Proposed Bias Mitigation Techniques. A  Literature Review</b></summary>
  <p><b>编号</b>：[113]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09511</p>
  <p><b>作者</b>：Mikel K. Ngueajio,  Gloria Washington</p>
  <p><b>备注</b>：In press at HCI International 2022 - Late Breaking Papers: Interacting with eXtended Reality and Artificial Intelligence, LNCS 13518</p>
  <p><b>关键词</b>：ASR, Speech, Automatic Speech, ASR systems, sophisticated speech technologies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Speech is the fundamental means of communication between humans. The advent
of AI and sophisticated speech technologies have led to the rapid proliferation
of human-to-computer-based interactions, fueled primarily by Automatic Speech
Recognition (ASR) systems. ASR systems normally take human speech in the form
of audio and convert it into words, but for some users, it cannot decode the
speech, and any output text is filled with errors that are incomprehensible to
the human reader. These systems do not work equally for everyone and actually
hinder the productivity of some users. In this paper, we present research that
addresses ASR biases against gender, race, and the sick and disabled, while
exploring studies that propose ASR debiasing techniques for mitigating these
discriminations. We also discuss techniques for designing a more accessible and
inclusive ASR technology. For each approach surveyed, we also provide a summary
of the investigation and methods applied, the ASR systems and corpora used, and
the research findings, and highlight their strengths and/or weaknesses.
Finally, we propose future opportunities for Natural Language Processing
researchers to explore in the next level creation of ASR technologies.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Back-Translation-Style Data Augmentation for Mandarin Chinese Polyphone  Disambiguation</b></summary>
  <p><b>编号</b>：[120]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09495</p>
  <p><b>作者</b>：Chunyu Qiang,  Peng Yang,  Hao Che,  Jinba Xiao,  Xiaorui Wang,  Zhongyuan Wang</p>
  <p><b>备注</b>：Published to APSIPA ASC 2022</p>
  <p><b>关键词</b>：Chinese polyphone disambiguation, mandarin Chinese polyphone, polyphone disambiguation, Mandarin Chinese, plays an important</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Conversion of Chinese Grapheme-to-Phoneme (G2P) plays an important role in
Mandarin Chinese Text-To-Speech (TTS) systems, where one of the biggest
challenges is the task of polyphone disambiguation. Most of the previous
polyphone disambiguation models are trained on manually annotated datasets, and
publicly available datasets for polyphone disambiguation are scarce. In this
paper we propose a simple back-translation-style data augmentation method for
mandarin Chinese polyphone disambiguation, utilizing a large amount of
unlabeled text data. Inspired by the back-translation technique proposed in the
field of machine translation, we build a Grapheme-to-Phoneme (G2P) model to
predict the pronunciation of polyphonic character, and a Phoneme-to-Grapheme
(P2G) model to predict pronunciation into text. Meanwhile, a window-based
matching strategy and a multi-model scoring strategy are proposed to judge the
correctness of the pseudo-label. We design a data balance strategy to improve
the accuracy of some typical polyphonic characters in the training set with
imbalanced distribution or data scarcity. The experimental result shows the
effectiveness of the proposed back-translation-style data augmentation method.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Abstractive Summarization Guided by Latent Hierarchical Document  Structure</b></summary>
  <p><b>编号</b>：[131]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09458</p>
  <p><b>作者</b>：Yifu Qiu,  Shay B. Cohen</p>
  <p><b>备注</b>：EMNLP 2022, 15 pages</p>
  <p><b>关键词</b>：Sequential abstractive neural, abstractive neural summarizers, Sequential abstractive, structure, underlying structure</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sequential abstractive neural summarizers often do not use the underlying
structure in the input article or dependencies between the input sentences.
This structure is essential to integrate and consolidate information from
different parts of the text. To address this shortcoming, we propose a
hierarchy-aware graph neural network (HierGNN) which captures such dependencies
through three main steps: 1) learning a hierarchical document structure through
a latent structure tree learned by a sparse matrix-tree computation; 2)
propagating sentence information over this structure using a novel
message-passing node propagation mechanism to identify salient information; 3)
using graph-level attention to concentrate the decoder on salient information.
Experiments confirm HierGNN improves strong sequence models such as BART, with
a 0.55 and 0.75 margin in average ROUGE-1/2/L for CNN/DM and XSum. Further
human evaluation demonstrates that summaries produced by our model are more
relevant and less redundant than the baselines, into which HierGNN is
incorporated. We also find HierGNN synthesizes summaries by fusing multiple
source sentences more, rather than compressing a single source sentence, and
that it processes long inputs more effectively.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Consultation Checklists: Standardising the Human Evaluation of Medical  Note Generation</b></summary>
  <p><b>编号</b>：[133]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09455</p>
  <p><b>作者</b>：Aleksandar Savkov,  Francesco Moramarco,  Alex Papadopoulos Korfiatis,  Mark Perera,  Anya Belz,  Ehud Reiter</p>
  <p><b>备注</b>：Accepted for publication at EMNLP 2022</p>
  <p><b>关键词</b>：generally hard due, inherently subjective nature, Evaluating automatically generated, automatically generated text, Evaluating automatically</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Evaluating automatically generated text is generally hard due to the
inherently subjective nature of many aspects of the output quality. This
difficulty is compounded in automatic consultation note generation by differing
opinions between medical experts both about which patient statements should be
included in generated notes and about their respective importance in arriving
at a diagnosis. Previous real-world evaluations of note-generation systems saw
substantial disagreement between expert evaluators. In this paper we propose a
protocol that aims to increase objectivity by grounding evaluations in
Consultation Checklists, which are created in a preliminary step and then used
as a common point of reference during quality assessment. We observed good
levels of inter-annotator agreement in a first evaluation study using the
protocol; further, using Consultation Checklists produced in the study as
reference for automatic metrics such as ROUGE or BERTScore improves their
correlation with human judgements compared to using the original human note.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Feature-augmented Machine Reading Comprehension with Auxiliary Tasks</b></summary>
  <p><b>编号</b>：[139]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09438</p>
  <p><b>作者</b>：Yifeng Xie</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：single training objective, reading comprehension rely, machine reading comprehension, encoder layer, learn great representation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While most successful approaches for machine reading comprehension rely on
single training objective, it is assumed that the encoder layer can learn great
representation through the loss function we define in the predict layer, which
is cross entropy in most of time, in the case that we first use neural networks
to encode the question and paragraph, then directly fuse the encoding result of
them. However, due to the distantly loss backpropagating in reading
comprehension, the encoder layer cannot learn effectively and be directly
supervised. Thus, the encoder layer can not learn the representation well at
any time. Base on this, we propose to inject multi granularity information to
the encoding layer. Experiments demonstrate the effect of adding multi
granularity information to the encoding layer can boost the performance of
machine reading comprehension system. Finally, empirical study shows that our
approach can be applied to many existing MRC models.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Feedback is Needed for Retakes: An Explainable Poor Image Notification  Framework for the Visually Impaired</b></summary>
  <p><b>编号</b>：[141]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09427</p>
  <p><b>作者</b>：Kazuya Ohata,  Shunsuke Kitada,  Hitoshi Iyatomi</p>
  <p><b>备注</b>：6 pages, 4 figures. Accepted at 2022 IEEE 19th International Conference on Smart Communities: Improving Quality of Life Using ICT, IoT and AI (HONET) as a full paper</p>
  <p><b>关键词</b>：propose a simple, simple yet effective, quality, image, effective image captioning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a simple yet effective image captioning framework that can
determine the quality of an image and notify the user of the reasons for any
flaws in the image. Our framework first determines the quality of images and
then generates captions using only those images that are determined to be of
high quality. The user is notified by the flaws feature to retake if image
quality is low, and this cycle is repeated until the input image is deemed to
be of high quality. As a component of the framework, we trained and evaluated a
low-quality image detection model that simultaneously learns difficulty in
recognizing images and individual flaws, and we demonstrated that our proposal
can explain the reasons for flaws with a sufficient score. We also evaluated a
dataset with low-quality images removed by our framework and found improved
values for all four common metrics (e.g., BLEU-4, METEOR, ROUGE-L, CIDEr),
confirming an improvement in general-purpose image captioning capability. Our
framework would assist the visually impaired, who have difficulty judging image
quality.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：LongFNT: Long-form Speech Recognition with Factorized Neural Transducer</b></summary>
  <p><b>编号</b>：[148]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09412</p>
  <p><b>作者</b>：Xun Gong,  Yu Wu,  Jinyu Li,  Shujie Liu,  Rui Zhao,  Xie Chen,  Yanmin Qian</p>
  <p><b>备注</b>：Submitted to ICASSP2023</p>
  <p><b>关键词</b>：Traditional automatic speech, automatic speech recognition, Traditional automatic, systems usually focus, individual utterances</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Traditional automatic speech recognition~(ASR) systems usually focus on
individual utterances, without considering long-form speech with useful
historical information, which is more practical in real scenarios. Simply
attending longer transcription history for a vanilla neural transducer model
shows no much gain in our preliminary experiments, since the prediction network
is not a pure language model. This motivates us to leverage the factorized
neural transducer structure, containing a real language model, the vocabulary
predictor. We propose the {LongFNT-Text} architecture, which fuses the
sentence-level long-form features directly with the output of the vocabulary
predictor and then embeds token-level long-form features inside the vocabulary
predictor, with a pre-trained contextual encoder RoBERTa to further boost the
performance. Moreover, we propose the {LongFNT} architecture by extending the
long-form speech to the original speech input and achieve the best performance.
The effectiveness of our LongFNT approach is validated on LibriSpeech and
GigaSpeech corpora with 19% and 12% relative word error rate~(WER) reduction,
respectively.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Open-Domain Conversational Question Answering with Historical Answers</b></summary>
  <p><b>编号</b>：[152]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09401</p>
  <p><b>作者</b>：Hung-Chieh Fang,  Kuo-Han Hung,  Chao-Wei Huang,  Yun-Nung Chen</p>
  <p><b>备注</b>：AACL-IJCNLP 2022</p>
  <p><b>关键词</b>：selecting candidate passages, conversational question answering, candidate passages, relies on selecting, selecting candidate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Open-domain conversational question answering can be viewed as two tasks:
passage retrieval and conversational question answering, where the former
relies on selecting candidate passages from a large corpus and the latter
requires better understanding of a question with contexts to predict the
answers. This paper proposes ConvADR-QA that leverages historical answers to
boost retrieval performance and further achieves better answering performance.
In our proposed framework, the retrievers use a teacher-student framework to
reduce noises from previous turns. Our experiments on the benchmark dataset,
OR-QuAC, demonstrate that our model outperforms existing baselines in both
extractive and generative reader settings, well justifying the effectiveness of
historical answers for open-domain conversational question answering.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：ConNER: Consistency Training for Cross-lingual Named Entity Recognition</b></summary>
  <p><b>编号</b>：[155]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09394</p>
  <p><b>作者</b>：Ran Zhou,  Xin Li,  Lidong Bing,  Erik Cambria,  Luo Si,  Chunyan Miao</p>
  <p><b>备注</b>：Accepted by EMNLP 2022</p>
  <p><b>关键词</b>：named entity recognition, Cross-lingual named entity, entity recognition, zero-shot settings, named entity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cross-lingual named entity recognition (NER) suffers from data scarcity in
the target languages, especially under zero-shot settings. Existing
translate-train or knowledge distillation methods attempt to bridge the
language gap, but often introduce a high level of noise. To solve this problem,
consistency training methods regularize the model to be robust towards
perturbations on data or hidden states. However, such methods are likely to
violate the consistency hypothesis, or mainly focus on coarse-grain
consistency. We propose ConNER as a novel consistency training framework for
cross-lingual NER, which comprises of: (1) translation-based consistency
training on unlabeled target-language data, and (2) dropoutbased consistency
training on labeled source-language data. ConNER effectively leverages
unlabeled target-language data and alleviates overfitting on the source
language to enhance the cross-lingual adaptability. Experimental results show
our ConNER achieves consistent improvement over various baseline methods.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Data-Efficient Autoregressive Document Retrieval for Fact Verification</b></summary>
  <p><b>编号</b>：[158]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09388</p>
  <p><b>作者</b>：James Thorne</p>
  <p><b>备注</b>：To appear at SustaiNLP@EMNLP 2022. Code is available: this https URL</p>
  <p><b>关键词</b>：knowledge-intensive natural language, natural language processing, language processing task, processing task formulations, question answering</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Document retrieval is a core component of many knowledge-intensive natural
language processing task formulations such as fact verification and question
answering. Sources of textual knowledge, such as Wikipedia articles, condition
the generation of answers from the models. Recent advances in retrieval use
sequence-to-sequence models to incrementally predict the title of the
appropriate Wikipedia page given a query. However, this method requires
supervision in the form of human annotation to label which Wikipedia pages
contain appropriate context. This paper introduces a distant-supervision method
that does not require any annotation to train autoregressive retrievers that
attain competitive R-Precision and Recall in a zero-shot setting. Furthermore
we show that with task-specific supervised fine-tuning, autoregressive
retrieval performance for two Wikipedia-based fact verification tasks can
approach or even exceed full supervision using less than $1/4$ of the annotated
data indicating possible directions for data-efficient autoregressive
retrieval.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Self-Training with Purpose Preserving Augmentation Improves Few-shot  Generative Dialogue State Tracking</b></summary>
  <p><b>编号</b>：[164]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09379</p>
  <p><b>作者</b>：Jihyun Lee,  Chaebin Lee,  Yunsu Kim,  Gary Geunbae Lee</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：dialogue state tracking, considerable human labor, dataset involves considerable, involves considerable human, state tracking</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In dialogue state tracking (DST), labeling the dataset involves considerable
human labor. We propose a new self-training framework for few-shot generative
DST that utilize unlabeled data. Our self-training method iteratively improves
the model by pseudo labeling and employs Purpose Preserving Augmentation
(PPAug) to prevent overfitting. We increaese the few-shot 10% performance by
approximately 4% on MultiWOZ 2.1 and enhances the slot-recall 8.34% for unseen
values compared to baseline.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Execution-based Evaluation for Data Science Code Generation Models</b></summary>
  <p><b>编号</b>：[168]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09374</p>
  <p><b>作者</b>：Junjie Huang,  Chenglong Wang,  Jipeng Zhang,  Cong Yan,  Haotian Cui,  Jeevana Priya Inala,  Colin Clement,  Nan Duan,  Jianfeng Gao</p>
  <p><b>备注</b>：Accepted by the 4th Workshop on Data Science with Human-in-the-loop (DaSH) at EMNLP 2022</p>
  <p><b>关键词</b>：automatically generating code, benefit data scientists', data scientists' productivity, scientists' productivity, productivity by automatically</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Code generation models can benefit data scientists' productivity by
automatically generating code from context and text descriptions. An important
measure of the modeling progress is whether a model can generate code that can
correctly execute to solve the task. However, due to the lack of an evaluation
dataset that directly supports execution-based model evaluation, existing work
relies on code surface form similarity metrics (e.g., BLEU, CodeBLEU) for model
selection, which can be inaccurate.
To remedy this, we introduce ExeDS, an evaluation dataset for execution
evaluation for data science code generation tasks. ExeDS contains a set of 534
problems from Jupyter Notebooks, each consisting of code context, task
description, reference program, and the desired execution output. With ExeDS,
we evaluate the execution performance of five state-of-the-art code generation
models that have achieved high surface-form evaluation scores. Our experiments
show that models with high surface-form scores do not necessarily perform well
on execution metrics, and execution-based metrics can better capture model code
generation errors. Source code and data can be found at
this https URL</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Reflect, Not Reflex: Inference-Based Common Ground Improves Dialogue  Response Quality</b></summary>
  <p><b>编号</b>：[206]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09267</p>
  <p><b>作者</b>：Pei Zhou,  Hyundong Cho,  Pegah Jandaghi,  Dong-Ho Lee,  Bill Yuchen Lin,  Jay Pujara,  Xiang Ren</p>
  <p><b>备注</b>：Accepted at EMNLP-2022. 19 pages, 17 figures, 4 tables</p>
  <p><b>关键词</b>：Human communication relies, Human communication, communication relies, common ground, Reflect</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Human communication relies on common ground (CG), the mutual knowledge and
beliefs shared by participants, to produce coherent and interesting
conversations. In this paper, we demonstrate that current response generation
(RG) models produce generic and dull responses in dialogues because they act
reflexively, failing to explicitly model CG, both due to the lack of CG in
training data and the standard RG training procedure. We introduce Reflect, a
dataset that annotates dialogues with explicit CG (materialized as inferences
approximating shared knowledge and beliefs) and solicits 9k diverse
human-generated responses each following one common ground. Using Reflect, we
showcase the limitations of current dialogue data and RG models: less than half
of the responses in current data are rated as high quality (sensible, specific,
and interesting) and models trained using this data have even lower quality,
while most Reflect responses are judged high quality. Next, we analyze whether
CG can help models produce better-quality responses by using Reflect CG to
guide RG models. Surprisingly, we find that simply prompting GPT3 to "think"
about CG generates 30% more quality responses, showing promising benefits to
integrating CG into the RG process.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Task-aware Retrieval with Instructions</b></summary>
  <p><b>编号</b>：[208]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09260</p>
  <p><b>作者</b>：Akari Asai,  Timo Schick,  Patrick Lewis,  Xilun Chen,  Gautier Izacard,  Sebastian Riedel,  Hannaneh Hajishirzi,  Wen-tau Yih</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：system explicitly describe, retrieval system explicitly, study the problem, explicitly describe, describe their intent</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the problem of retrieval with instructions, where users of a
retrieval system explicitly describe their intent along with their queries,
making the system task-aware. We aim to develop a general-purpose task-aware
retrieval systems using multi-task instruction tuning that can follow
human-written instructions to find the best documents for a given query. To
this end, we introduce the first large-scale collection of approximately 40
retrieval datasets with instructions, and present TART, a multi-task retrieval
system trained on the diverse retrieval tasks with instructions. TART shows
strong capabilities to adapt to a new task via instructions and advances the
state of the art on two zero-shot retrieval benchmarks, BEIR and LOTTE,
outperforming models up to three times larger. We further introduce a new
evaluation setup to better reflect real-world scenarios, pooling diverse
documents and tasks. In this setup, TART significantly outperforms competitive
baselines, further demonstrating the effectiveness of guiding retrieval with
instructions.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Artificial Disfluency Detection, Uh No, Disfluency Generation for the  Masses</b></summary>
  <p><b>编号</b>：[215]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09235</p>
  <p><b>作者</b>：T. Passali,  T. Mavropoulos,  G. Tsoumakas,  G. Meditskos,  S. Vrochidis</p>
  <p><b>备注</b>：10 pages</p>
  <p><b>关键词</b>：detection typically require, disfluency detection typically, large annotated datasets, detection typically, existence of large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing approaches for disfluency detection typically require the existence
of large annotated datasets. However, current datasets for this task are
limited, suffer from class imbalance, and lack some types of disfluencies that
can be encountered in real-world scenarios. This work proposes LARD, a method
for automatically generating artificial disfluencies from fluent text. LARD can
simulate all the different types of disfluencies (repetitions, replacements and
restarts) based on the reparandum/interregnum annotation scheme. In addition,
it incorporates contextual embeddings into the disfluency generation to produce
realistic context-aware artificial disfluencies. Since the proposed method
requires only fluent text, it can be used directly for training, bypassing the
requirement of annotated disfluent data. Our empirical evaluation demonstrates
that LARD can indeed be effectively used when no or only a few data are
available. Furthermore, our detailed analysis suggests that the proposed method
generates realistic disfluencies and increases the accuracy of existing
disfluency detectors.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：A Graph-Based Context-Aware Model to Understand Online Conversations</b></summary>
  <p><b>编号</b>：[222]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09207</p>
  <p><b>作者</b>：Vibhor Agarwal,  Anthony P. Young,  Sagar Joglekar,  Nishanth Sastry</p>
  <p><b>备注</b>：25 pages, 9 figures. arXiv admin note: text overlap with arXiv:2202.08175</p>
  <p><b>关键词</b>：participatory engagement, engagement between users, public discussion, comments, Online forums</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Online forums that allow for participatory engagement between users have been
transformative for the public discussion of many important issues. However,
such conversations can sometimes escalate into full-blown exchanges of hate and
misinformation. Existing approaches in natural language processing (NLP), such
as deep learning models for classification tasks, use as inputs only a single
comment or a pair of comments depending upon whether the task concerns the
inference of properties of the individual comments or the replies between pairs
of comments, respectively. But in online conversations, comments and replies
may be based on external context beyond the immediately relevant information
that is input to the model. Therefore, being aware of the conversations'
surrounding contexts should improve the model's performance for the inference
task at hand.
We propose GraphNLI, a novel graph-based deep learning architecture that uses
graph walks to incorporate the wider context of a conversation in a principled
manner. Specifically, a graph walk starts from a given comment and samples
"nearby" comments in the same or parallel conversation threads, which results
in additional embeddings that are aggregated together with the initial
comment's embedding. We then use these enriched embeddings for downstream NLP
prediction tasks that are important for online conversations. We evaluate
GraphNLI on two such tasks - polarity prediction and misogynistic hate speech
detection - and found that our model consistently outperforms all relevant
baselines for both tasks. Specifically, GraphNLI with a biased root-seeking
random walk performs with a macro-F1 score of 3 and 6 percentage points better
than the best-performing BERT-based baselines for the polarity prediction and
hate speech detection tasks, respectively.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Deep Emotion Recognition in Textual Conversations: A Survey</b></summary>
  <p><b>编号</b>：[230]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09172</p>
  <p><b>作者</b>：Patrícia Pereira,  Helena Moniz,  Joao Paulo Carvalho</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Recognition in Conversations, implementation scenarios present, Emotion Recognition, ERC, tremendous advancement</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While Emotion Recognition in Conversations (ERC) has seen a tremendous
advancement in the last few years, new applications and implementation
scenarios present novel challenges and opportunities. These range from
leveraging the conversational context, speaker and emotion dynamics modelling,
to interpreting common sense expressions, informal language and sarcasm,
addressing challenges of real time ERC and recognizing emotion causes. This
survey starts by introducing ERC, elaborating on the challenges and
opportunities pertaining to this task. It proceeds with a description of the
main emotion taxonomies and methods to deal with subjectivity in annotations.
It then describes Deep Learning methods relevant for ERC, word embeddings, and
elaborates on the use of performance metrics for the task and methods to deal
with the typically unbalanced ERC datasets. This is followed by a description
and benchmark of key ERC works along with comprehensive tables comparing
several works regarding their methods and performance across different
datasets. The survey highlights the advantage of leveraging techniques to
address unbalanced data, the exploration of mixed emotions and the benefits of
incorporating annotation subjectivity in the learning phase.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Unified Question Answering in Slovene</b></summary>
  <p><b>编号</b>：[235]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09159</p>
  <p><b>作者</b>：Katja Logar,  Marko Robnik-Šikonja</p>
  <p><b>备注</b>：4 pages,published in Proceedings of the 25th International Multiconference INFORMATION SOCIETY - IS 2012, Volume A -Slovenian Conference on Artificial Intelligence SCAI 2022, Ljubljana, 2022, pp. 23-26</p>
  <p><b>关键词</b>：challenging tasks, language understanding, English, less-resourced Slovene language, Slovene</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Question answering is one of the most challenging tasks in language
understanding. Most approaches are developed for English, while less-resourced
languages are much less researched. We adapt a successful English
question-answering approach, called UnifiedQA, to the less-resourced Slovene
language. Our adaptation uses the encoder-decoder transformer SloT5 and mT5
models to handle four question-answering formats: yes/no, multiple-choice,
abstractive, and extractive. We use existing Slovene adaptations of four
datasets, and machine translate the MCTest dataset. We show that a general
model can answer questions in different formats at least as well as specialized
models. The results are further improved using cross-lingual transfer from
English. While we produce state-of-the-art results for Slovene, the performance
still lags behind English.</p>
  </details>
</details>
<h1>机器学习</h1>
<details>
  <summary>1. <b>标题：InstructPix2Pix: Learning to Follow Image Editing Instructions</b></summary>
  <p><b>编号</b>：[4]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09800</p>
  <p><b>作者</b>：Tim Brooks,  Aleksander Holynski,  Alexei A. Efros</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：propose a method, Stable Diffusion, model, images, instructions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a method for editing images from human instructions: given an
input image and a written instruction that tells the model what to do, our
model follows these instructions to edit the image. To obtain training data for
this problem, we combine the knowledge of two large pretrained models -- a
language model (GPT-3) and a text-to-image model (Stable Diffusion) -- to
generate a large dataset of image editing examples. Our conditional diffusion
model, InstructPix2Pix, is trained on our generated data, and generalizes to
real images and user-written instructions at inference time. Since it performs
edits in the forward pass and does not require per example fine-tuning or
inversion, our model edits images quickly, in a matter of seconds. We show
compelling editing results for a diverse collection of input images and written
instructions.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：ConStruct-VL: Data-Free Continual Structured VL Concepts Learning</b></summary>
  <p><b>编号</b>：[9]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09790</p>
  <p><b>作者</b>：James Seale Smith,  Paola Cascante-Bonilla,  Assaf Arbelle,  Donghyun Kim,  Rameswar Panda,  David Cox,  Diyi Yang,  Zsolt Kira,  Rogerio Feris,  Leonid Karlinsky</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：achieving competitive results, short text prompts, demonstrated remarkable capabilities, recognizing objects defined, large-scale pre-trained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, large-scale pre-trained Vision-and-Language (VL) foundation models
have demonstrated remarkable capabilities in many zero-shot downstream tasks,
achieving competitive results for recognizing objects defined by as little as
short text prompts. However, it has also been shown that VL models are still
brittle in Structured VL Concept (SVLC) reasoning, such as the ability to
recognize object attributes, states, and inter-object relations. This leads to
reasoning mistakes, which need to be corrected as they occur by teaching VL
models the missing SVLC skills; often this must be done using private data
where the issue was found, which naturally leads to a data-free continual (no
task-id) VL learning setting. In this work, we introduce the first Continual
Data-Free Structured VL Concepts Learning (ConStruct-VL) benchmark and show it
is challenging for many existing data-free CL strategies. We, therefore,
propose a data-free method comprised of a new approach of Adversarial
Pseudo-Replay (APR) which generates adversarial reminders of past tasks from
past task models. To use this method efficiently, we also propose a continual
parameter-efficient Layered-LoRA (LaLo) neural architecture allowing
no-memory-cost access to all past models at train time. We show this approach
outperforms all data-free methods by as much as ~7% while even matching some
levels of experience-replay (prohibitive for applications where data-privacy
must be preserved).</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：SE(3)-Equivariant Relational Rearrangement with Neural Descriptor Fields</b></summary>
  <p><b>编号</b>：[12]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09786</p>
  <p><b>作者</b>：Anthony Simeonov,  Yilun Du,  Lin Yen-Chen,  Alberto Rodriguez,  Leslie Pack Kaelbling,  Tomas Lozano-Perez,  Pulkit Agrawal</p>
  <p><b>备注</b>：CoRL 2022, first two authors contributed equally, website and code: this https URL</p>
  <p><b>关键词</b>：point cloud observations, involving spatial relations, arbitrary poses directly, performing tasks involving, tasks involving spatial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a method for performing tasks involving spatial relations between
novel object instances initialized in arbitrary poses directly from point cloud
observations. Our framework provides a scalable way for specifying new tasks
using only 5-10 demonstrations. Object rearrangement is formalized as the
question of finding actions that configure task-relevant parts of the object in
a desired alignment. This formalism is implemented in three steps: assigning a
consistent local coordinate frame to the task-relevant object parts,
determining the location and orientation of this coordinate frame on unseen
object instances, and executing an action that brings these frames into the
desired alignment. We overcome the key technical challenge of determining
task-relevant local coordinate frames from a few demonstrations by developing
an optimization method based on Neural Descriptor Fields (NDFs) and a single
annotated 3D keypoint. An energy-based learning scheme to model the joint
configuration of the objects that satisfies a desired relational task further
improves performance. The method is tested on three multi-object rearrangement
tasks in simulation and on a real robot. Project website, videos, and code:
this https URL</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Assessing Neural Network Robustness via Adversarial Pivotal Tuning</b></summary>
  <p><b>编号</b>：[14]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09782</p>
  <p><b>作者</b>：Peter Ebert Christensen,  Vésteinn Snæbjarnarson,  Andrea Dittadi,  Serge Belongie,  Sagie Benaim</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：manipulations, image, APT, ability to assess, pretrained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The ability to assess the robustness of image classifiers to a diverse set of
manipulations is essential to their deployment in the real world. Recently,
semantic manipulations of real images have been considered for this purpose, as
they may not arise using standard adversarial settings. However, such semantic
manipulations are often limited to style, color or attribute changes. While
expressive, these manipulations do not consider the full capacity of a
pretrained generator to affect adversarial image manipulations. In this work,
we aim at leveraging the full capacity of a pretrained image generator to
generate highly detailed, diverse and photorealistic image manipulations.
Inspired by recent GAN-based image inversion methods, we propose a method
called Adversarial Pivotal Tuning (APT). APT first finds a pivot latent space
input to a pretrained generator that best reconstructs an input image. It then
adjusts the weights of the generator to create small, but semantic,
manipulations which fool a pretrained classifier. Crucially, APT changes both
the input and the weights of the pretrained generator, while preserving its
expressive latent editing capability, thus allowing the use of its full
capacity in creating semantic adversarial manipulations. We demonstrate that
APT generates a variety of semantic image manipulations, which preserve the
input image class, but which fool a variety of pretrained classifiers. We
further demonstrate that classifiers trained to be robust to other robustness
benchmarks, are not robust to our generated manipulations and propose an
approach to improve the robustness towards our generated manipulations. Code
available at: this https URL</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Cheeger Inequalities for Directed Graphs and Hypergraphs Using  Reweighted Eigenvalues</b></summary>
  <p><b>编号</b>：[16]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09776</p>
  <p><b>作者</b>：Lap Chi Lau,  Kam Chuen Tung,  Robert Wang</p>
  <p><b>备注</b>：51 pages, 3 figures</p>
  <p><b>关键词</b>：vec, directed graph, directed, lambda, Cheeger inequality</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We derive Cheeger inequalities for directed graphs and hypergraphs using the
reweighted eigenvalue approach that was recently developed for vertex expansion
in undirected graphs [OZ22,KLT22,JPV22]. The goal is to develop a new spectral
theory for directed graphs and an alternative spectral theory for hypergraphs.
The first main result is a Cheeger inequality relating the vertex expansion
$\vec{\psi}(G)$ of a directed graph $G$ to the vertex-capacitated maximum
reweighted second eigenvalue $\vec{\lambda}_2^{v*}$: \[ \vec{\lambda}_2^{v*}
\lesssim \vec{\psi}(G) \lesssim \sqrt{\vec{\lambda}_2^{v*} \cdot \log
(\Delta/\vec{\lambda}_2^{v*})}. \] This provides a combinatorial
characterization of the fastest mixing time of a directed graph by vertex
expansion, and builds a new connection between reweighted eigenvalued, vertex
expansion, and fastest mixing time for directed graphs.
The second main result is a stronger Cheeger inequality relating the edge
conductance $\vec{\phi}(G)$ of a directed graph $G$ to the edge-capacitated
maximum reweighted second eigenvalue $\vec{\lambda}_2^{e*}$: \[
\vec{\lambda}_2^{e*} \lesssim \vec{\phi}(G) \lesssim \sqrt{\vec{\lambda}_2^{e*}
\cdot \log (1/\vec{\lambda}_2^{e*})}. \] This provides a certificate for a
directed graph to be an expander and a spectral algorithm to find a sparse cut
in a directed graph, playing a similar role as Cheeger's inequality in
certifying graph expansion and in the spectral partitioning algorithm for
undirected graphs.
We also use this reweighted eigenvalue approach to derive the improved
Cheeger inequality for directed graphs, and furthermore to derive several
Cheeger inequalities for hypergraphs that match and improve the existing
results in [Lou15,CLTZ18]. These are supporting results that this provides a
unifying approach to lift the spectral theory for undirected graphs to more
general settings.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Boosting Object Representation Learning via Motion and Object Continuity</b></summary>
  <p><b>编号</b>：[18]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09771</p>
  <p><b>作者</b>：Quentin Delfosse,  Wolfgang Stammer,  Thomas Rothenbacher,  Dwarak Vittal,  Kristian Kersting</p>
  <p><b>备注</b>：8 pages main text, 32 tables, 21 Figures</p>
  <p><b>关键词</b>：Recent unsupervised multi-object, architectural inductive biases, unsupervised multi-object detection, shown impressive performance, Recent unsupervised</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent unsupervised multi-object detection models have shown impressive
performance improvements, largely attributed to novel architectural inductive
biases. Unfortunately, they may produce suboptimal object encodings for
downstream tasks. To overcome this, we propose to exploit object motion and
continuity, i.e., objects do not pop in and out of existence. This is
accomplished through two mechanisms: (i) providing priors on the location of
objects through integration of optical flow, and (ii) a contrastive object
continuity loss across consecutive image frames. Rather than developing an
explicit deep architecture, the resulting Motion and Object Continuity (MOC)
scheme can be instantiated using any baseline object detection model. Our
results show large improvements in the performances of a SOTA model in terms of
object discovery, convergence speed and overall latent object representations,
particularly for playing Atari games. Overall, we show clear benefits of
integrating motion and object continuity for downstream tasks, moving beyond
object representation learning based only on reconstruction.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：VeLO: Training Versatile Learned Optimizers by Scaling Up</b></summary>
  <p><b>编号</b>：[22]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09760</p>
  <p><b>作者</b>：Luke Metz,  James Harrison,  C. Daniel Freeman,  Amil Merchant,  Lucas Beyer,  James Bradbury,  Naman Agrawal,  Ben Poole,  Igor Mordatch,  Adam Roberts,  Jascha Sohl-Dickstein</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：replaced hand-designed features, deep learning models, deep learning, replaced hand-designed, hand-designed features</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While deep learning models have replaced hand-designed features across many
domains, these models are still trained with hand-designed optimizers. In this
work, we leverage the same scaling approach behind the success of deep learning
to learn versatile optimizers. We train an optimizer for deep learning which is
itself a small neural network that ingests gradients and outputs parameter
updates. Meta-trained with approximately four thousand TPU-months of compute on
a wide variety of optimization tasks, our optimizer not only exhibits
compelling performance, but optimizes in interesting and unexpected ways. It
requires no hyperparameter tuning, instead automatically adapting to the
specifics of the problem being optimized. We open source our learned optimizer,
meta-training code, the associated train and test data, and an extensive
optimizer benchmark suite with baselines at this http URL.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Probing for Incremental Parse States in Autoregressive Language Models</b></summary>
  <p><b>编号</b>：[25]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09748</p>
  <p><b>作者</b>：Tiwalayo Eisape,  Vineet Gangireddy,  Roger P. Levy,  Yoon Kim</p>
  <p><b>备注</b>：Findings of EMNLP 2022</p>
  <p><b>关键词</b>：show remarkable sensitivity, models show remarkable, sensitivity to syntax, show remarkable, remarkable sensitivity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Next-word predictions from autoregressive neural language models show
remarkable sensitivity to syntax. This work evaluates the extent to which this
behavior arises as a result of a learned ability to maintain implicit
representations of incremental syntactic structures. We extend work in
syntactic probing to the incremental setting and present several probes for
extracting incomplete syntactic structure (operationalized through parse states
from a stack-based parser) from autoregressive language models. We find that
our probes can be used to predict model preferences on ambiguous sentence
prefixes and causally intervene on model representations and steer model
behavior. This suggests implicit incremental syntactic inferences underlie
next-word predictions in autoregressive neural language models.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Learning 4DVAR inversion directly from observations</b></summary>
  <p><b>编号</b>：[28]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09741</p>
  <p><b>作者</b>：Arthur Filoche,  Julien Brajard,  Anastase Charantonis,  Dominique Béréziat</p>
  <p><b>备注</b>：submitted to ICASSP 2023</p>
  <p><b>关键词</b>：Variational data assimilation, deep learning share, Variational data, aspects in common, share many algorithmic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Variational data assimilation and deep learning share many algorithmic
aspects in common. While the former focuses on system state estimation, the
latter provides great inductive biases to learn complex relationships. We here
design a hybrid architecture learning the assimilation task directly from
partial and noisy observations, using the mechanistic constraint of the 4DVAR
algorithm. Finally, we show in an experiment that the proposed method was able
to learn the desired inversion with interesting regularizing properties and
that it also has computational interests.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Sub-Graph Learning for Spatiotemporal Forecasting via Knowledge  Distillation</b></summary>
  <p><b>编号</b>：[29]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09740</p>
  <p><b>作者</b>：Mehrtash Mehrabi,  Yingxue Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：challenges in studying, diverse pattern, interaction types, large graphs, graph</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>One of the challenges in studying the interactions in large graphs is to
learn their diverse pattern and various interaction types. Hence, considering
only one distribution and model to study all nodes and ignoring their diversity
and local features in their neighborhoods, might severely affect the overall
performance. Based on the structural information of the nodes in the graph and
the interactions between them, the main graph can be divided into multiple
sub-graphs. This graph partitioning can tremendously affect the learning
process, however the overall performance is highly dependent on the clustering
method to avoid misleading the model. In this work, we present a new framework
called KD-SGL to effectively learn the sub-graphs, where we define one global
model to learn the overall structure of the graph and multiple local models for
each sub-graph. We assess the performance of the proposed framework and
evaluate it on public datasets. Based on the achieved results, it can improve
the performance of the state-of-the-arts spatiotemporal models with comparable
results compared to ensemble of models with less complexity.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Stutter-TTS: Controlled Synthesis and Improved Recognition of Stuttered  Speech</b></summary>
  <p><b>编号</b>：[32]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09731</p>
  <p><b>作者</b>：Xin Zhang,  Iván Vallés-Pérez,  Andreas Stolcke,  Chengzhu Yu,  Jasha Droppo,  Olabanji Shonibare,  Roberto Barra-Chicote,  Venkatesh Ravichandran</p>
  <p><b>备注</b>：8 pages, 3 figures, 2 tables</p>
  <p><b>关键词</b>：interrupted by blocks, repetitions or prolongations, prolongations of syllables, natural flow, speech</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Stuttering is a speech disorder where the natural flow of speech is
interrupted by blocks, repetitions or prolongations of syllables, words and
phrases. The majority of existing automatic speech recognition (ASR) interfaces
perform poorly on utterances with stutter, mainly due to lack of matched
training data. Synthesis of speech with stutter thus presents an opportunity to
improve ASR for this type of speech. We describe Stutter-TTS, an end-to-end
neural text-to-speech model capable of synthesizing diverse types of stuttering
utterances. We develop a simple, yet effective prosody-control strategy whereby
additional tokens are introduced into source text during training to represent
specific stuttering characteristics. By choosing the position of the stutter
tokens, Stutter-TTS allows word-level control of where stuttering occurs in the
synthesized utterance. We are able to synthesize stutter events with high
accuracy (F1-scores between 0.63 and 0.84, depending on stutter type). By
fine-tuning an ASR model on synthetic stuttered speech we are able to reduce
word error by 5.7% relative on stuttered utterances, with only minor (<0.2% relative) degradation for fluent utterances.< p>
  </0.2%></p></details>
</details>
<details>
  <summary>12. <b>标题：Generative Adversarial Training Can Improve Neural Language Models</b></summary>
  <p><b>编号</b>：[34]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09728</p>
  <p><b>作者</b>：Sajad Movahedi,  Azadeh Shakery</p>
  <p><b>备注</b>：An extended abstract selected for poster presentation at the Eastern European Machine Learning Summer School 2019</p>
  <p><b>关键词</b>：neural language modeling, recurrent neural networks, neural language models, unresolved issue, neural language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While deep learning in the form of recurrent neural networks (RNNs) has
caused a significant improvement in neural language modeling, the fact that
they are extremely prone to overfitting is still a mainly unresolved issue. In
this paper we propose a regularization method based on generative adversarial
networks (GANs) and adversarial training (AT), that can prevent overfitting in
neural language models. Unlike common adversarial training methods such as the
fast gradient sign method (FGSM) that require a second back-propagation through
time, and therefore effectively require at least twice the amount of time for
regular training, the overhead of our method does not exceed more than 20% of
the training of the baselines.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Deep Reinforcement Learning for IRS Phase Shift Design in  Spatiotemporally Correlated Environments</b></summary>
  <p><b>编号</b>：[35]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09726</p>
  <p><b>作者</b>：Spilios Evmorfos,  Athina P. Petropulu,  H. Vincent Poor</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Intelligent Reflecting Surface, Input Single Output, Multiple Input Single, Reflecting Surface, Single Output</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The paper studies the problem of designing the Intelligent Reflecting Surface
(IRS) phase shifters for Multiple Input Single Output (MISO) communication
systems in spatiotemporally correlated channel environments, where the
destination can move within a confined area. The objective is to maximize the
expected sum of SNRs at the receiver over infinite time horizons. The problem
formulation gives rise to a Markov Decision Process (MDP). We propose a deep
actor-critic algorithm that accounts for channel correlations and destination
motion by constructing the state representation to include the current position
of the receiver and the phase shift values and receiver positions that
correspond to a window of previous time steps. The channel variability induces
high frequency components on the spectrum of the underlying value function. We
propose the preprocessing of the critic's input with a Fourier kernel which
enables stable value learning. Finally, we investigate the use of the
destination SNR as a component of the designed MDP state, which is common
practice in previous work. We provide empirical evidence that, when the
channels are spatiotemporally correlated, the inclusion of the SNR in the state
representation interacts with function approximation in ways that inhibit
convergence.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Fair and Efficient Distributed Edge Learning with Hybrid Multipath TCP</b></summary>
  <p><b>编号</b>：[36]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09723</p>
  <p><b>作者</b>：Shiva Raj Pokhrel,  Jinho Choi,  Anwar Walid</p>
  <p><b>备注</b>：13 pages, 15 figures</p>
  <p><b>关键词</b>：distributed edge learning, primarily the aggregation-averaging, DEL, bottleneck of distributed, distributed edge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The bottleneck of distributed edge learning (DEL) over wireless has shifted
from computing to communication, primarily the aggregation-averaging (Agg-Avg)
process of DEL. The existing transmission control protocol (TCP)-based data
networking schemes for DEL are application-agnostic and fail to deliver
adjustments according to application layer requirements. As a result, they
introduce massive excess time and undesired issues such as unfairness and
stragglers. Other prior mitigation solutions have significant limitations as
they balance data flow rates from workers across paths but often incur
imbalanced backlogs when the paths exhibit variance, causing stragglers. To
facilitate a more productive DEL, we develop a hybrid multipath TCP (MPTCP) by
combining model-based and deep reinforcement learning (DRL) based MPTCP for DEL
that strives to realize quicker iteration of DEL and better fairness (by
ameliorating stragglers). Hybrid MPTCP essentially integrates two radical TCP
developments: i) successful existing model-based MPTCP control strategies and
ii) advanced emerging DRL-based techniques, and introduces a novel hybrid MPTCP
data transport for easing the communication of the Agg-Avg process. Extensive
emulation results demonstrate that the proposed hybrid MPTCP can overcome
excess time consumption and ameliorate the application layer unfairness of DEL
effectively without injecting additional inconstancy and stragglers.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Federated Multilingual Models for Medical Transcript Analysis</b></summary>
  <p><b>编号</b>：[37]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09722</p>
  <p><b>作者</b>：Andre Manoel,  Mirian Hipolito Garcia,  Tal Baumel,  Shize Su,  Jialei Chen,  Dan Miller,  Danny Karmon,  Robert Sim,  Dimitrios Dimitriadis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：machine learning approach, decentralized data sources, data, data access constraints, Federated Learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Federated Learning (FL) is a novel machine learning approach that allows the
model trainer to access more data samples, by training the model across
multiple decentralized data sources, while data access constraints are in
place. Such trained models can achieve significantly higher performance beyond
what can be done when trained on a single data source. As part of FL's
promises, none of the training data is ever transmitted to any central
location, ensuring that sensitive data remains local and private. These
characteristics make FL perfectly suited for large-scale applications in
healthcare, where a variety of compliance constraints restrict how data may be
handled, processed, and stored. Despite the apparent benefits of federated
learning, the heterogeneity in the local data distributions pose significant
challenges, and such challenges are even more pronounced in the case of
multilingual data providers. In this paper we present a federated learning
system for training a large-scale multi-lingual model suitable for fine-tuning
on downstream tasks such as medical entity tagging. Our work represents one of
the first such production-scale systems, capable of training across multiple
highly heterogeneous data providers, and achieving levels of accuracy that
could not be otherwise achieved by using central training with public data.
Finally, we show that the global model performance can be further improved by a
training step performed locally.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：A Finite-Particle Convergence Rate for Stein Variational Gradient  Descent</b></summary>
  <p><b>编号</b>：[38]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09721</p>
  <p><b>作者</b>：Jiaxin Shi,  Lester Mackey</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：variational gradient descent, Stein variational gradient, finite-particle convergence rate, gradient descent, finite-particle convergence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We provide a first finite-particle convergence rate for Stein variational
gradient descent (SVGD). Specifically, whenever the target distribution
satisfies Talagrand's T1 inequality, SVGD with n particles and an appropriate
step size sequence drives the kernel Stein discrepancy to zero at an order
1/sqrt(log log n) rate. We suspect that the dependence on n can be improved,
and we hope that our explicit, non-asymptotic proof strategy will serve as a
template for future refinements.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Learning Adaptive Evolutionary Computation for Solving Multi-Objective  Optimization Problems</b></summary>
  <p><b>编号</b>：[39]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09719</p>
  <p><b>作者</b>：Remco Coppens,  Robbert Reijnen,  Yingqian Zhang,  Laurens Bliek,  Berend Steenhuisen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Multi-objective evolutionary algorithms, Multi-objective evolutionary, multi-objective optimization problems, Deep Reinforcement Learning, solve multi-objective optimization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multi-objective evolutionary algorithms (MOEAs) are widely used to solve
multi-objective optimization problems. The algorithms rely on setting
appropriate parameters to find good solutions. However, this parameter tuning
could be very computationally expensive in solving non-trial (combinatorial)
optimization problems. This paper proposes a framework that integrates MOEAs
with adaptive parameter control using Deep Reinforcement Learning (DRL). The
DRL policy is trained to adaptively set the values that dictate the intensity
and probability of mutation for solutions during optimization. We test the
proposed approach with a simple benchmark problem and a real-world, complex
warehouse design and control problem. The experimental results demonstrate the
advantages of our method in terms of solution quality and computation time to
reach good solutions. In addition, we show the learned policy is transferable,
i.e., the policy trained on a simple benchmark problem can be directly applied
to solve the complex warehouse optimization problem, effectively, without the
need for retraining.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Numerical Optimizations for Weighted Low-rank Estimation on Language  Model</b></summary>
  <p><b>编号</b>：[40]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09718</p>
  <p><b>作者</b>：Ting Hua,  Yen-Chang Hsu,  Felicity Wang,  Qian Lou,  Yilin Shen,  Hongxia Jin</p>
  <p><b>备注</b>：long paper EMNLP 2022</p>
  <p><b>关键词</b>：popular compression methods, smaller matrices, popular compression, approximate a target, SVD</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Singular value decomposition (SVD) is one of the most popular compression
methods that approximate a target matrix with smaller matrices. However,
standard SVD treats the parameters within the matrix with equal importance,
which is a simple but unrealistic assumption. The parameters of a trained
neural network model may affect task performance unevenly, which suggests
non-equal importance among the parameters. Compared to SVD, the decomposition
method aware of parameter importance is the more practical choice in real
cases. Unlike standard SVD, weighted value decomposition is a non-convex
optimization problem that lacks a closed-form solution. We systematically
investigated multiple optimization strategies to tackle the problem and
examined our method by compressing Transformer-based language models. Further,
we designed a metric to predict when the SVD may introduce a significant
performance drop, for which our method can be a rescue strategy. The extensive
evaluations demonstrate that our method can perform better than current SOTA
methods in compressing Transformer-based language models.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Deep Reinforcement Learning for Combined Coverage and Resource  Allocation in UAV-aided RAN-slicing</b></summary>
  <p><b>编号</b>：[43]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09713</p>
  <p><b>作者</b>：Lorenzo Bellone,  Boris Galkin,  Emiliano Traversi,  Enrico Natalizio</p>
  <p><b>备注</b>：6 pages, 4 figures, submitted to IEEE ICC'23 - SAC-08 MLCN Track</p>
  <p><b>关键词</b>：assessed approach enabling, approach enabling virtualization, radio access network, assessed approach, approach enabling</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Network slicing is a well assessed approach enabling virtualization of the
mobile core and radio access network (RAN) in the emerging 5th Generation New
Radio. Slicing is of paramount importance when dealing with the emerging and
diverse vertical applications entailing heterogeneous sets of requirements. 5G
is also envisioning Unmanned Aerial Vehicles (UAVs) to be a key element in the
cellular network standard, aiming at their use as aerial base stations and
exploiting their flexible and quick deployment to enhance the wireless network
performance. This work presents a UAV-assisted 5G network, where the aerial
base stations (UAV-BS) are empowered with network slicing capabilities aiming
at optimizing the Service Level Agreement (SLA) satisfaction ratio of a set of
users. The users belong to three heterogeneous categories of 5G service type,
namely, enhanced mobile broadband (eMBB), ultra-reliable low-latency
communication (URLLC), and massive machine-type communication (mMTC). A first
application of multi-agent and multi-decision deep reinforcement learning for
UAV-BS in a network slicing context is introduced, aiming at the optimization
of the SLA satisfaction ratio of users through the joint allocation of radio
resources to slices and refinement of the UAV-BSs 2-dimensional trajectories.
The performance of the presented strategy have been tested and compared to
benchmark heuristics, highlighting a higher percentage of satisfied users (at
least 27% more) in a variety of scenarios.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：SigT: An Efficient End-to-End MIMO-OFDM Receiver Framework Based on  Transformer</b></summary>
  <p><b>编号</b>：[44]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09712</p>
  <p><b>作者</b>：Ziyou Ren,  Nan Cheng,  Ruijin Sun,  Xiucheng Wang,  Ning Lu,  Wenchao Xu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：orthogonal frequency-division multiplexing, wireless communication systems, subsequent wireless communication, Multiple-input multiple-output, frequency-division multiplexing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multiple-input multiple-output and orthogonal frequency-division multiplexing
(MIMO-OFDM) are the key technologies in 4G and subsequent wireless
communication systems. Conventionally, the MIMO-OFDM receiver is performed by
multiple cascaded blocks with different functions and the algorithm in each
block is designed based on ideal assumptions of wireless channel distributions.
However, these assumptions may fail in practical complex wireless environments.
The deep learning (DL) method has the ability to capture key features from
complex and huge data. In this paper, a novel end-to-end MIMO-OFDM receiver
framework based on \textit{transformer}, named SigT, is proposed. By regarding
the signal received from each antenna as a token of the transformer, the
spatial correlation of different antennas can be learned and the critical
zero-shot problem can be mitigated. Furthermore, the proposed SigT framework
can work well without the inserted pilots, which improves the useful data
transmission efficiency. Experiment results show that SigT achieves much higher
performance in terms of signal recovery accuracy than benchmark methods, even
in a low SNR environment or with a small number of training samples. Code is
available at this https URL.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Design Considerations For Hypothesis Rejection Modules In Spoken  Language Understanding Systems</b></summary>
  <p><b>编号</b>：[45]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09711</p>
  <p><b>作者</b>：Aman Alok,  Rahul Gupta,  Shankar Ananthakrishnan</p>
  <p><b>备注</b>：5 pages. ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</p>
  <p><b>关键词</b>：Spoken Language Understanding, Language Understanding, machine learning models, Spoken Language, SLU hypothesis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Spoken Language Understanding (SLU) systems typically consist of a set of
machine learning models that operate in conjunction to produce an SLU
hypothesis. The generated hypothesis is then sent to downstream components for
further action. However, it is desirable to discard an incorrect hypothesis
before sending it downstream. In this work, we present two designs for SLU
hypothesis rejection modules: (i) scheme R1 that performs rejection on domain
specific SLU hypothesis and, (ii) scheme R2 that performs rejection on
hypothesis generated from the overall SLU system. Hypothesis rejection modules
in both schemes reject/accept a hypothesis based on features drawn from the
utterance directed to the SLU system, the associated SLU hypothesis and SLU
confidence score. Our experiments suggest that both the schemes yield similar
results (scheme R1: 2.5% FRR @ 4.5% FAR, scheme R2: 2.5% FRR @ 4.6% FAR), with
the best performing systems using all the available features. We argue that
while either of the rejection schemes can be chosen over the other, they carry
some inherent differences which need to be considered while making this choice.
Additionally, we incorporate ASR features in the rejection module (obtaining an
1.9% FRR @ 3.8% FAR) and analyze the improvements.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Style Classification of Rabbinic Literature for Detection of Lost  Midrash Tanhuma Material</b></summary>
  <p><b>编号</b>：[46]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09710</p>
  <p><b>作者</b>：Shlomo Tannor,  Nachum Dershowitz,  Moshe Lavee</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：complex rabbinic works, multiple languages, written transmission, collections are complex, works that consist</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Midrash collections are complex rabbinic works that consist of text in
multiple languages, which evolved through long processes of unstable oral and
written transmission. Determining the origin of a given passage in such a
compilation is not always straightforward and is often a matter of dispute
among scholars, yet it is essential for scholars' understanding of the passage
and its relationship to other texts in the rabbinic corpus.
To help solve this problem, we propose a system for classification of
rabbinic literature based on its style, leveraging recently released pretrained
Transformer models for Hebrew. Additionally, we demonstrate how our method can
be applied to uncover lost material from Midrash Tanhuma.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Listen, denoise, action! Audio-driven motion synthesis with diffusion  models</b></summary>
  <p><b>编号</b>：[48]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09707</p>
  <p><b>作者</b>：Simon Alexanderson,  Rajmund Nagy,  Jonas Beskow,  Gustav Eje Henter</p>
  <p><b>备注</b>：15 pages, 6 figures</p>
  <p><b>关键词</b>：efficiently trainable probabilistic, trainable probabilistic models, experienced a surge, expressive yet efficiently, efficiently trainable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diffusion models have experienced a surge of interest as highly expressive
yet efficiently trainable probabilistic models. We show that these models are
an excellent fit for synthesising human motion that co-occurs with audio, for
example co-speech gesticulation, since motion is complex and highly ambiguous
given audio, calling for a probabilistic description. Specifically, we adapt
the DiffWave architecture to model 3D pose sequences, putting Conformers in
place of dilated convolutions for improved accuracy. We also demonstrate
control over motion style, using classifier-free guidance to adjust the
strength of the stylistic expression. Gesture-generation experiments on the
Trinity Speech-Gesture and ZeroEGGS datasets confirm that the proposed method
achieves top-of-the-line motion quality, with distinctive styles whose
expression can be made more or less pronounced. We also synthesise dance motion
and path-driven locomotion using the same model architecture. Finally, we
extend the guidance procedure to perform style interpolation in a manner that
is appealing for synthesis tasks and has connections to product-of-experts
models, a contribution we believe is of independent interest. Video examples
are available at this https URL</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：A Synthetic Dataset for 5G UAV Attacks Based on Observable Network  Parameters</b></summary>
  <p><b>编号</b>：[49]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09706</p>
  <p><b>作者</b>：Joseanne Viana,  Hamed Farkhari,  Pedro Sebastiao,  Sandra Lagen,  Katerina Koutlia,  Biljana Bojovic,  Rui Dinis</p>
  <p><b>备注</b>：6 pages, 4 figures, 1 table</p>
  <p><b>关键词</b>：learning researchers due, researchers due, possibility of experimenting, real data, data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Synthetic datasets are beneficial for machine learning researchers due to the
possibility of experimenting with new strategies and algorithms in the training
and testing phases. These datasets can easily include more scenarios that might
be costly to research with real data or can complement and, in some cases,
replace real data measurements, depending on the quality of the synthetic data.
They can also solve the unbalanced data problem, avoid overfitting, and can be
used in training while testing can be done with real data. In this paper, we
present, to the best of our knowledge, the first synthetic dataset for Unmanned
Aerial Vehicle (UAV) attacks in 5G and beyond networks based on the following
key observable network parameters that indicate power levels: the Received
Signal Strength Indicator (RSSI) and the Signal to Interference-plus-Noise
Ratio (SINR). The main objective of this data is to enable deep network
development for UAV communication security. Especially, for algorithm
development or the analysis of time-series data applied to UAV attack
recognition. Our proposed dataset provides insights into network functionality
when static or moving UAV attackers target authenticated UAVs in an urban
environment. The dataset also considers the presence and absence of
authenticated terrestrial users in the network, which may decrease the deep
networks ability to identify attacks. Furthermore, the data provides deeper
comprehension of the metrics available in the 5G physical and MAC layers for
machine learning and statistics research. The dataset will available at link
this http URL</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：EfficientTrain: Exploring Generalized Curriculum Learning for Training  Visual Backbones</b></summary>
  <p><b>编号</b>：[50]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09703</p>
  <p><b>作者</b>：Yulin Wang,  Yang Yue,  Rui Lu,  Tianjiao Liu,  Zhao Zhong,  Shiji Song,  Gao Huang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：costly training procedure, modern deep networks, superior performance, performance of modern, deep networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The superior performance of modern deep networks usually comes at the price
of a costly training procedure. In this paper, we present a novel curriculum
learning approach for the efficient training of visual backbones (e.g., vision
Transformers). The proposed method is inspired by the phenomenon that deep
networks mainly learn to recognize some 'easier-to-learn' discriminative
patterns within each example at earlier stages of training, e.g., the
lower-frequency components of images and the original information before data
augmentation. Driven by this observation, we propose a curriculum where the
model always leverages all the training data at each epoch, while the
curriculum starts with only exposing the 'easier-to-learn' patterns of each
example, and introduces gradually more difficult patterns. To implement this
idea, we 1) introduce a cropping operation in the Fourier spectrum of the
inputs, which enables the model to learn from only the lower-frequency
components efficiently, and 2) demonstrate that exposing the features of
original images amounts to adopting weaker data augmentation. Our resulting
algorithm, EfficientTrain, is simple, general, yet surprisingly effective. For
example, it reduces the training time of a wide variety of popular models
(e.g., ConvNeXts, DeiT, PVT, and Swin/CSWin Transformers) by more than
${1.5\times}$ on ImageNet-1K/22K without sacrificing the accuracy. It is
effective for self-supervised learning (i.e., MAE) as well. Code is available
at this https URL.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Deep Reinforcement Learning Based Joint Downlink Beamforming and RIS  Configuration in RIS-aided MU-MISO Systems Under Hardware Impairments and  Imperfect CSI</b></summary>
  <p><b>编号</b>：[51]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09702</p>
  <p><b>作者</b>：Baturay Saglam,  Doga Gurgunoglu,  Suleyman S. Kozat</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：reconfigurable intelligent surface, joint transmit beamforming, sum downlink rate, RIS amplitude model, phase-dependent RIS amplitude</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We investigate the joint transmit beamforming and reconfigurable intelligent
surface (RIS) configuration problem to maximize the sum downlink rate of a
RIS-aided cellular multiuser multiple input single output (MU-MISO) system
under imperfect channel state information (CSI) and hardware impairments by
considering a practical phase-dependent RIS amplitude model. To this end, we
present a novel deep reinforcement learning (DRL) framework and compare its
performance against a vanilla DRL agent under two scenarios: the golden
standard where the base station (BS) knows the channel and the phase-dependent
RIS amplitude model perfectly, and the mismatch scenario where the BS has
imperfect CSI and assumes ideal RIS reflections. Our numerical results show
that the introduced framework substantially outperforms the vanilla DRL agent
under mismatch and approaches the golden standard.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：The Effectiveness of Bidirectional Generative Patent Language Models</b></summary>
  <p><b>编号</b>：[55]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09690</p>
  <p><b>作者</b>：Jieh-Sheng Lee</p>
  <p><b>备注</b>：10 pages, 3 figures, 5 tables</p>
  <p><b>关键词</b>：Generative patent language, patent language models, effectiveness, autocomplete, generative models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generative patent language models can assist humans to write patent text more
effectively. The question is how to measure effectiveness from a human-centric
perspective and how to improve effectiveness. In this manuscript, a simplified
design of the autocomplete function is proposed to increase effectiveness by
more than 10%. With the new design, the effectiveness of autocomplete can reach
more than 60%, which means that more than 60% of keystrokes can be saved by
autocomplete. Since writing patent text does not necessarily start from the
beginning to the end, a question is whether the generative model can assist a
user no matter where to start writing. To answer the question, the generative
models in this manuscript are pre-trained with training data in both
directions. The generative models become bidirectional. Since text generation
is bidirectional, the calculation of autocomplete effectiveness can be
bidirectional and starts from anywhere in the text. After thorough experiments,
a key finding is that the autocomplete effectiveness of a model for the same
text remains similar no matter where the calculation starts. The finding
indicates that such bidirectional models can assist a user at a similar level,
no matter where the user starts to write.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Analyse der Entwicklungstreiber militärischer Schwarmdrohnen durch  Natural Language Processing</b></summary>
  <p><b>编号</b>：[59]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09680</p>
  <p><b>作者</b>：Manuel Mundt</p>
  <p><b>备注</b>：5 pages, in German, 4 figures</p>
  <p><b>关键词</b>：increasingly prominent role, armed conflict, taking an increasingly, increasingly prominent, prominent role</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Military drones are taking an increasingly prominent role in armed conflict,
and the use of multiple drones in a swarm can be useful. Who the drivers of the
research are and what sub-domains exist is analyzed and visually presented in
this research using NLP techniques based on 946 studies. Most research is
conducted in the Western world, led by the United States, the United Kingdom,
and Germany. Through Tf-idf scoring, it is shown that countries have
significant differences in the subdomains studied. Overall, 2019 and 2020 saw
the most works published, with significant interest in military swarm drones as
early as 2008. This study provides a first glimpse into research in this area
and prompts further investigation.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Towards Automated Design of Bayesian Optimization via Exploratory  Landscape Analysis</b></summary>
  <p><b>编号</b>：[60]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09678</p>
  <p><b>作者</b>：Carolin Benjamins,  Anja Jankovic,  Elena Raponi,  Koen van der Blom,  Marius Lindauer,  Carola Doerr</p>
  <p><b>备注</b>：6th Workshop on Meta-Learning at NeurIPS 2022, New Orleans</p>
  <p><b>关键词</b>：efficiently computing high-quality, computing high-quality solutions, black-box optimization problems, algorithms form, surrogate-based heuristics</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Bayesian optimization (BO) algorithms form a class of surrogate-based
heuristics, aimed at efficiently computing high-quality solutions for numerical
black-box optimization problems. The BO pipeline is highly modular, with
different design choices for the initial sampling strategy, the surrogate
model, the acquisition function (AF), the solver used to optimize the AF, etc.
We demonstrate in this work that a dynamic selection of the AF can benefit the
BO design. More precisely, we show that already a naïve random forest
regression model, built on top of exploratory landscape analysis features that
are computed from the initial design points, suffices to recommend AFs that
outperform any static choice, when considering performance over the classic
BBOB benchmark suite for derivative-free numerical optimization methods on the
COCO platform. Our work hence paves a way towards AutoML-assisted, on-the-fly
BO designs that adjust their behavior on a run-by-run basis.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Influencer Detection with Dynamic Graph Neural Networks</b></summary>
  <p><b>编号</b>：[64]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09664</p>
  <p><b>作者</b>：Elena Tiukhova,  Emiliano Penaloza,  María Óskarsdóttir,  Hernan Garcia,  Alejandro Correa Bahnsen,  Bart Baesens,  Monique Snoeck,  Cristián Bravo</p>
  <p><b>备注</b>：Conference workshop camera-ready paper - accepted at NeurIPS TGL 2022. 8 pages, 4 figures</p>
  <p><b>关键词</b>：Leveraging network information, common practice, dynamic Graph Neural, Graph Neural, Leveraging network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Leveraging network information for prediction tasks has become a common
practice in many domains. Being an important part of targeted marketing,
influencer detection can potentially benefit from incorporating dynamic network
representation. In this work, we investigate different dynamic Graph Neural
Networks (GNNs) configurations for influencer detection and evaluate their
prediction performance using a unique corporate data set. We show that using
deep multi-head attention in GNN and encoding temporal attributes significantly
improves performance. Furthermore, our empirical evaluation illustrates that
capturing neighborhood representation is more beneficial that using network
centrality measures.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Why Deep Learning Generalizes</b></summary>
  <p><b>编号</b>：[75]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09639</p>
  <p><b>作者</b>：Benjamin L. Badger</p>
  <p><b>备注</b>：11 pages</p>
  <p><b>关键词</b>：fitting large datasets, huge capacity, fitting large, remarkably resistant, time capable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Very large deep learning models trained using gradient descent are remarkably
resistant to memorization given their huge capacity, but are at the same time
capable of fitting large datasets of pure noise. Here methods are introduced by
which models may be trained to memorize datasets that normally are generalized.
We find that memorization is difficult relative to generalization, but that
adding noise makes memorization easier. Increasing the dataset size exaggerates
the characteristics of that dataset: model access to more training samples
makes overfitting easier for random data, but somewhat harder for natural
images. The bias of deep learning towards generalization is explored
theoretically, and we show that generalization results from a model's
parameters being attracted to points of maximal stability with respect to that
model's inputs during gradient descent.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：On the Sample Complexity of Two-Layer Networks: Lipschitz vs.  Element-Wise Lipschitz Activation</b></summary>
  <p><b>编号</b>：[76]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09634</p>
  <p><b>作者</b>：Amit Daniely,  Elad Granot</p>
  <p><b>备注</b>：9 pages with additional 15 pages of supplementary</p>
  <p><b>关键词</b>：two-layer neural networks, textbf, bounded two-layer neural, two-layer neural, neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We investigate the sample complexity of bounded two-layer neural networks
using different activation functions.
In particular, we consider the class
\[
\mathcal{H} = \left\{\textbf{x}\mapsto \langle \textbf{v}, \sigma \circ
W\textbf{x} + \textbf{b} \rangle :
\textbf{b}\in\mathbb{R}^d, W \in \mathbb{R}^{T\times d}, \textbf{v} \in
\mathbb{R}^{T}\right\}
\]
where the spectral norm of $W$ and $\textbf{v}$ is bounded by $O(1)$, the
Frobenius norm of $W$ is bounded from its initialization by $R > 0$, and
$\sigma$ is a Lipschitz activation function.
We prove that if $\sigma$ is element-wise, then the sample complexity of
$\mathcal{H}$ is width independent and that this complexity is tight.
Moreover, we show that the element-wise property of $\sigma$ is essential for
width-independent bound, in the sense that there exist non-element-wise
activation functions whose sample complexity is provably width-dependent.
For the upper bound, we use the recent approach for norm-based bounds named
Approximate Description Length (ADL) by arXiv:1910.05697.
We further develop new techniques and tools for this approach, that will
hopefully inspire future works.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Predicting Human Mobility via Self-supervised Disentanglement Learning</b></summary>
  <p><b>编号</b>：[79]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09625</p>
  <p><b>作者</b>：Qiang Gao,  Jinyu Hong,  Xovee Xu,  Ping Kuang,  Fan Zhou,  Goce Trajcevski</p>
  <p><b>备注</b>：15 pages, 9 figures</p>
  <p><b>关键词</b>：Deep neural networks, recently achieved considerable, human behavioral patterns, achieved considerable improvements, learning human behavioral</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep neural networks have recently achieved considerable improvements in
learning human behavioral patterns and individual preferences from massive
spatial-temporal trajectories data. However, most of the existing research
concentrates on fusing different semantics underlying sequential trajectories
for mobility pattern learning which, in turn, yields a narrow perspective on
comprehending human intrinsic motions. In addition, the inherent sparsity and
under-explored heterogeneous collaborative items pertaining to human check-ins
hinder the potential exploitation of human diverse periodic regularities as
well as common interests. Motivated by recent advances in disentanglement
learning, in this study we propose a novel disentangled solution called SSDL
for tackling the next POI prediction problem. SSDL primarily seeks to
disentangle the potential time-invariant and time-varying factors into
different latent spaces from massive trajectories data, providing an
interpretable view to understand the intricate semantics underlying human
diverse mobility representations. To address the data sparsity issue, we
present two realistic trajectory augmentation approaches to enhance the
understanding of both the human intrinsic periodicity and constantly-changing
intents. In addition, we devise a POI-centric graph structure to explore
heterogeneous collaborative signals underlying historical check-ins. Extensive
experiments conducted on four real-world datasets demonstrate that our proposed
SSDL significantly outperforms the state-of-the-art approaches -- for example,
it yields up to 8.57% improvements on ACC@1.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Introduction to Online Nonstochastic Control</b></summary>
  <p><b>编号</b>：[83]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09619</p>
  <p><b>作者</b>：Elad Hazan,  Karan Singh</p>
  <p><b>备注</b>：Draft; comments/suggestions welcome at nonstochastic.control@gmail.com</p>
  <p><b>关键词</b>：differentiable reinforcement learning, reinforcement learning called, online nonstochastic control, learning called online, called online nonstochastic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This text presents an introduction to an emerging paradigm in control of
dynamical systems and differentiable reinforcement learning called online
nonstochastic control. The new approach applies techniques from online convex
optimization and convex relaxations to obtain new methods with provable
guarantees for classical settings in optimal and robust control.
The primary distinction between online nonstochastic control and other
frameworks is the objective. In optimal control, robust control, and other
control methodologies that assume stochastic noise, the goal is to perform
comparably to an offline optimal strategy. In online nonstochastic control,
both the cost functions as well as the perturbations from the assumed dynamical
model are chosen by an adversary. Thus the optimal policy is not defined a
priori. Rather, the target is to attain low regret against the best policy in
hindsight from a benchmark class of policies.
This objective suggests the use of the decision making framework of online
convex optimization as an algorithmic methodology. The resulting methods are
based on iterative mathematical optimization algorithms, and are accompanied by
finite-time regret and computational complexity guarantees.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Dynamic Pricing with Volume Discounts in Online Settings</b></summary>
  <p><b>编号</b>：[85]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09612</p>
  <p><b>作者</b>：Marco Mussi,  Gianmarco Genalti,  Alessandro Nuara,  Francesco Trovò,  Marcello Restelli,  Nicola Gatti</p>
  <p><b>备注</b>：Accepted to IAAI 2023</p>
  <p><b>关键词</b>：USD worldwide annually, trillion USD worldwide, main international reports, advanced analytic tools, trillion USD</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>According to the main international reports, more pervasive industrial and
business-process automation, thanks to machine learning and advanced analytic
tools, will unlock more than 14 trillion USD worldwide annually by 2030. In the
specific case of pricing problems-which constitute the class of problems we
investigate in this paper-, the estimated unlocked value will be about 0.5
trillion USD per year. In particular, this paper focuses on pricing in
e-commerce when the objective function is profit maximization and only
transaction data are available. This setting is one of the most common in
real-world applications. Our work aims to find a pricing strategy that allows
defining optimal prices at different volume thresholds to serve different
classes of users. Furthermore, we face the major challenge, common in
real-world settings, of dealing with limited data available. We design a
two-phase online learning algorithm, namely PVD-B, capable of exploiting the
data incrementally in an online fashion. The algorithm first estimates the
demand curve and retrieves the optimal average price, and subsequently it
offers discounts to differentiate the prices for each volume threshold. We ran
a real-world 4-month-long A/B testing experiment in collaboration with an
Italian e-commerce company, in which our algorithm PVD-B-corresponding to A
configuration-has been compared with human pricing specialists-corresponding to
B configuration. At the end of the experiment, our algorithm produced a total
turnover of about 300 KEuros, outperforming the B configuration performance by
about 55%. The Italian company we collaborated with decided to adopt our
algorithm for more than 1,200 products since January 2022.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Spatial Graph Convolution Neural Networks for Water Distribution Systems</b></summary>
  <p><b>编号</b>：[93]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09587</p>
  <p><b>作者</b>：Inaam Ashraf,  Luca Hermes,  André Artelt,  Barbara Hammer</p>
  <p><b>备注</b>：Under submission. Python code will be made available soon</p>
  <p><b>关键词</b>：water distribution systems, representative machine learning, machine learning challenge, distribution systems, critical infrastructure</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We investigate the task of missing value estimation in graphs as given by
water distribution systems (WDS) based on sparse signals as a representative
machine learning challenge in the domain of critical infrastructure. The
underlying graphs have a comparably low node degree and high diameter, while
information in the graph is globally relevant, hence graph neural networks face
the challenge of long-term dependencies. We propose a specific architecture
based on message passing which displays excellent results for a number of
benchmark tasks in the WDS domain. Further, we investigate a multi-hop
variation, which requires considerably less resources and opens an avenue
towards big WDS graphs.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：DeepVoxNet2: Yet another CNN framework</b></summary>
  <p><b>编号</b>：[96]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09569</p>
  <p><b>作者</b>：Jeroen Bertels,  David Robben,  Robin Lemmens,  Dirk Vandermeulen</p>
  <p><b>备注</b>：15 pages, part of PhD thesis KU Leuven 2022 "Understanding Final Infarct Prediction in Acute Ischemic Stroke Using Convolutional Neural Networks"</p>
  <p><b>关键词</b>：CNN mapping function, CNN-based image analysis, CNN mapping, sampling scheme, paramount importance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We know that both the CNN mapping function and the sampling scheme are of
paramount importance for CNN-based image analysis. It is clear that both
functions operate in the same space, with an image axis $\mathcal{I}$ and a
feature axis $\mathcal{F}$. Remarkably, we found that no frameworks existed
that unified the two and kept track of the spatial origin of the data
automatically. Based on our own practical experience, we found the latter to
often result in complex coding and pipelines that are difficult to exchange.
This article introduces our framework for 1, 2 or 3D image classification or
segmentation: DeepVoxNet2 (DVN2). This article serves as an interactive
tutorial, and a pre-compiled version, including the outputs of the code blocks,
can be found online in the public DVN2 repository. This tutorial uses data from
the multimodal Brain Tumor Image Segmentation Benchmark (BRATS) of 2018 to show
an example of a 3D segmentation pipeline.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Towards Good Practices in Evaluating Transfer Adversarial Attacks</b></summary>
  <p><b>编号</b>：[98]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09565</p>
  <p><b>作者</b>：Zhengyu Zhao,  Hanwei Zhang,  Renjue Li,  Ronan Sicre,  Laurent Amsaleg,  Michael Backes</p>
  <p><b>备注</b>：Our code and a list of categorized attacks are publicly available at this https URL</p>
  <p><b>关键词</b>：adversarial attacks raise, attacks raise critical, raise critical security, critical security concerns, concerns in real-world</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transfer adversarial attacks raise critical security concerns in real-world,
black-box scenarios. However, the actual progress of attack methods is
difficult to assess due to two main limitations in existing evaluations. First,
existing evaluations are unsystematic and sometimes unfair since new methods
are often directly added to old ones without complete comparisons to similar
methods. Second, existing evaluations mainly focus on transferability but
overlook another key attack property: stealthiness. In this work, we design
good practices to address these limitations. We first introduce a new attack
categorization, which enables our systematic analyses of similar attacks in
each specific category. Our analyses lead to new findings that complement or
even challenge existing knowledge. Furthermore, we comprehensively evaluate 23
representative attacks against 9 defenses on ImageNet. We pay particular
attention to stealthiness, by adopting diverse imperceptibility metrics and
looking into new, finer-grained characteristics. Our evaluation reveals new
important insights: 1) Transferability is highly contextual, and some white-box
defenses may give a false sense of security since they are actually vulnerable
to (black-box) transfer attacks; 2) All transfer attacks are less stealthy, and
their stealthiness can vary dramatically under the same $L_{\infty}$ bound.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：Convolutional neural networks for medical image segmentation</b></summary>
  <p><b>编号</b>：[99]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09562</p>
  <p><b>作者</b>：Jeroen Bertels,  David Robben,  Robin Lemmens,  Dirk Vandermeulen</p>
  <p><b>备注</b>：10 pages, 6 figures, part of PhD thesis KU Leuven 2022 "Understanding Final Infarct Prediction in Acute Ischemic Stroke Using Convolutional Neural Networks"</p>
  <p><b>关键词</b>：convolutional neural networks, medical image segmentation, neural networks, essential aspects, aspects of convolutional</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this article, we look into some essential aspects of convolutional neural
networks (CNNs) with the focus on medical image segmentation. First, we discuss
the CNN architecture, thereby highlighting the spatial origin of the data,
voxel-wise classification and the receptive field. Second, we discuss the
sampling of input-output pairs, thereby highlighting the interaction between
voxel-wise classification, patch size and the receptive field. Finally, we give
a historical overview of crucial changes to CNN architectures for
classification and segmentation, giving insights in the relation between three
pivotal CNN architectures: FCN, U-Net and DeepMedic.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：A Reinforcement Learning Approach for Process Parameter Optimization in  Additive Manufacturing</b></summary>
  <p><b>编号</b>：[104]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09545</p>
  <p><b>作者</b>：Susheel Dharmadhikari,  Nandana Menon,  Amrita Basak</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：metal additive manufacturing, control microstructure, additive manufacturing, ensure repeatability, minimize defects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Process optimization for metal additive manufacturing (AM) is crucial to
ensure repeatability, control microstructure, and minimize defects. Despite
efforts to address this via the traditional design of experiments and
statistical process mapping, there is limited insight on an on-the-fly
optimization framework that can be integrated into a metal AM system.
Additionally, most of these methods, being data-intensive, cannot be supported
by a metal AM alloy or system due to budget restrictions. To tackle this issue,
the article introduces a Reinforcement Learning (RL) methodology transformed
into an optimization problem in the realm of metal AM. An off-policy RL
framework based on Q-learning is proposed to find optimal laser power ($P$) -
scan velocity ($v$) combinations with the objective of maintaining steady-state
melt pool depth. For this, an experimentally validated Eagar-Tsai formulation
is used to emulate the Laser-Directed Energy Deposition environment, where the
laser operates as the agent across the $P-v$ space such that it maximizes
rewards for a melt pool depth closer to the optimum. The culmination of the
training process yields a Q-table where the state ($P,v$) with the highest
Q-value corresponds to the optimized process parameter. The resultant melt pool
depths and the mapping of Q-values to the $P-v$ space show congruence with
experimental observations. The framework, therefore, provides a model-free
approach to learning without any prior.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Neural Langevin Dynamics: towards interpretable Neural Stochastic  Differential Equations</b></summary>
  <p><b>编号</b>：[106]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09537</p>
  <p><b>作者</b>：Simon M. Koop,  Mark A. Peletier,  Jacobus W. Portegies,  Vlado Menkovski</p>
  <p><b>备注</b>：12 pages, 5 figures</p>
  <p><b>关键词</b>：Neural Stochastic Differential, Stochastic Differential Equations, Differential Equations, Neural Stochastic, Stochastic Differential</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural Stochastic Differential Equations (NSDE) have been trained as both
Variational Autoencoders, and as GANs. However, the resulting Stochastic
Differential Equations can be hard to interpret or analyse due to the generic
nature of the drift and diffusion fields. By restricting our NSDE to be of the
form of Langevin dynamics, and training it as a VAE, we obtain NSDEs that lend
themselves to more elaborate analysis and to a wider range of visualisation
techniques than a generic NSDE. More specifically, we obtain an energy
landscape, the minima of which are in one-to-one correspondence with latent
states underlying the used data. This not only allows us to detect states
underlying the data dynamics in an unsupervised manner, but also to infer the
distribution of time spent in each state according to the learned SDE. More in
general, restricting an NSDE to Langevin dynamics enables the use of a large
set of tools from computational molecular dynamics for the analysis of the
obtained results.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Towards Building Text-To-Speech Systems for the Next Billion Users</b></summary>
  <p><b>编号</b>：[107]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09536</p>
  <p><b>作者</b>：Gokul Karthik Kumar,  Praveen S V,  Pratyush Kumar,  Mitesh M. Khapra,  Karthik Nandakumar</p>
  <p><b>备注</b>：Under review in ICASSP 2023. First two authors contributed equally</p>
  <p><b>关键词</b>：Deep learning based, Deep learning, evolving rapidly, training methodologies, languages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning based text-to-speech (TTS) systems have been evolving rapidly
with advances in model architectures, training methodologies, and
generalization across speakers and languages. However, these advances have not
been thoroughly investigated for Indian language speech synthesis. Such
investigation is computationally expensive given the number and diversity of
Indian languages, relatively lower resource availability, and the diverse set
of advances in neural TTS that remain untested. In this paper, we evaluate the
choice of acoustic models, vocoders, supplementary loss functions, training
schedules, and speaker and language diversity for Dravidian and Indo-Aryan
languages. Based on this, we identify monolingual models with FastPitch and
HiFi-GAN V1, trained jointly on male and female speakers to perform the best.
With this setup, we train and evaluate TTS models for 13 languages and find our
models to significantly improve upon existing models in all languages as
measured by mean opinion scores. We open-source all models on the Bhashini
platform.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：Self-supervised Trajectory Representation Learning with Temporal  Regularities and Travel Semantics</b></summary>
  <p><b>编号</b>：[114]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09510</p>
  <p><b>作者</b>：Jiawei Jiang,  Dayan Pan,  Houxing Ren,  Xiaohan Jiang,  Chao Li,  Jingyuan Wang</p>
  <p><b>备注</b>：Accepted by ICDE 2023</p>
  <p><b>关键词</b>：Representation, Trajectory, Trajectory Representation, Trajectory Representation Learning, representation vectors</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Trajectory Representation Learning (TRL) is a powerful tool for
spatial-temporal data analysis and management. TRL aims to convert complicated
raw trajectories into low-dimensional representation vectors, which can be
applied to various downstream tasks, such as trajectory classification,
clustering, and similarity computation. Existing TRL works usually treat
trajectories as ordinary sequence data, while some important spatial-temporal
characteristics, such as temporal regularities and travel semantics, are not
fully exploited. To fill this gap, we propose a novel Self-supervised
trajectory representation learning framework with TemporAl Regularities and
Travel semantics, namely START. The proposed method consists of two stages. The
first stage is a Trajectory Pattern-Enhanced Graph Attention Network (TPE-GAT),
which converts the road network features and travel semantics into
representation vectors of road segments. The second stage is a Time-Aware
Trajectory Encoder (TAT-Enc), which encodes representation vectors of road
segments in the same trajectory as a trajectory representation vector,
meanwhile incorporating temporal regularities with the trajectory
representation. Moreover, we also design two self-supervised tasks, i.e.,
span-masked trajectory recovery and trajectory contrastive learning, to
introduce spatial-temporal characteristics of trajectories into the training
process of our START framework. The effectiveness of the proposed method is
verified by extensive experiments on two large-scale real-world datasets for
three downstream tasks. The experiments also demonstrate that our method can be
transferred across different cities to adapt heterogeneous trajectory datasets.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Solar Power driven EV Charging Optimization with Deep Reinforcement  Learning</b></summary>
  <p><b>编号</b>：[126]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09479</p>
  <p><b>作者</b>：Stavros Sykiotis,  Christoforos Menos-Aikateriniadis,  Anastasios Doulamis,  Nikolaos Doulamis,  Pavlos S. Georgilakis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：sector decarbonization plays, upcoming energy transition, Power sector decarbonization, sustainable future, sector decarbonization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Power sector decarbonization plays a vital role in the upcoming energy
transition towards a more sustainable future. Decentralized energy resources,
such as Electric Vehicles (EV) and solar photovoltaic systems (PV), are
continuously integrated in residential power systems, increasing the risk of
bottlenecks in power distribution networks. This paper aims to address the
challenge of domestic EV charging while prioritizing clean, solar energy
consumption. Real Time-of-Use tariffs are treated as a price-based Demand
Response (DR) mechanism that can incentivize end-users to optimally shift EV
charging load in hours of high solar PV generation with the use of Deep
Reinforcement Learning (DRL). Historical measurements from the Pecan Street
dataset are analyzed to shape a flexibility potential reward to describe
end-user charging preferences. Experimental results show that the proposed DQN
EV optimal charging policy is able to reduce electricity bills by an average
11.5\% by achieving an average utilization of solar power 88.4</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Learning to Control Rapidly Changing Synaptic Connections: An  Alternative Type of Memory in Sequence Processing Artificial Neural Networks</b></summary>
  <p><b>编号</b>：[138]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09440</p>
  <p><b>作者</b>：Kazuki Irie,  Jürgen Schmidhuber</p>
  <p><b>备注</b>：Presented at NeurIPS 2022 Workshop on Memory in Artificial and Real Intelligence</p>
  <p><b>关键词</b>：sequence-processing recurrent neural, recurrent neural networks, sequence-processing recurrent, neural networks, Short-term memory</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Short-term memory in standard, general-purpose, sequence-processing recurrent
neural networks (RNNs) is stored as activations of nodes or "neurons."
Generalising feedforward NNs to such RNNs is mathematically straightforward and
natural, and even historical: already in 1943, McCulloch and Pitts proposed
this as a surrogate to "synaptic modifications" (in effect, generalising the
Lenz-Ising model, the first non-sequence processing RNN architecture of the
1920s). A lesser known alternative approach to storing short-term memory in
"synaptic connections" -- by parameterising and controlling the dynamics of a
context-sensitive time-varying weight matrix through another NN -- yields
another "natural" type of short-term memory in sequence processing NNs: the
Fast Weight Programmers (FWPs) of the early 1990s. FWPs have seen a recent
revival as generic sequence processors, achieving competitive performance
across various tasks. They are formally closely related to the now popular
Transformers. Here we present them in the context of artificial NNs as an
abstraction of biological NNs -- a perspective that has not been stressed
enough in previous FWP work. We first review aspects of FWPs for pedagogical
purposes, then discuss connections to related works motivated by insights from
neuroscience.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Feedback is Needed for Retakes: An Explainable Poor Image Notification  Framework for the Visually Impaired</b></summary>
  <p><b>编号</b>：[141]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09427</p>
  <p><b>作者</b>：Kazuya Ohata,  Shunsuke Kitada,  Hitoshi Iyatomi</p>
  <p><b>备注</b>：6 pages, 4 figures. Accepted at 2022 IEEE 19th International Conference on Smart Communities: Improving Quality of Life Using ICT, IoT and AI (HONET) as a full paper</p>
  <p><b>关键词</b>：propose a simple, simple yet effective, quality, image, effective image captioning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a simple yet effective image captioning framework that can
determine the quality of an image and notify the user of the reasons for any
flaws in the image. Our framework first determines the quality of images and
then generates captions using only those images that are determined to be of
high quality. The user is notified by the flaws feature to retake if image
quality is low, and this cycle is repeated until the input image is deemed to
be of high quality. As a component of the framework, we trained and evaluated a
low-quality image detection model that simultaneously learns difficulty in
recognizing images and individual flaws, and we demonstrated that our proposal
can explain the reasons for flaws with a sufficient score. We also evaluated a
dataset with low-quality images removed by our framework and found improved
values for all four common metrics (e.g., BLEU-4, METEOR, ROUGE-L, CIDEr),
confirming an improvement in general-purpose image captioning capability. Our
framework would assist the visually impaired, who have difficulty judging image
quality.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Machine Learning for Software Engineering: A Tertiary Study</b></summary>
  <p><b>编号</b>：[142]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09425</p>
  <p><b>作者</b>：Zoe Kotti,  Rafaila Galanopoulou,  Diomidis Spinellis</p>
  <p><b>备注</b>：37 pages, 6 figures, 7 tables, journal article</p>
  <p><b>关键词</b>：Machine learning, lifecycle activities, techniques increase, increase the effectiveness, software engineering</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine learning (ML) techniques increase the effectiveness of software
engineering (SE) lifecycle activities. We systematically collected,
quality-assessed, summarized, and categorized 83 reviews in ML for SE published
between 2009-2022, covering 6,117 primary studies. The SE areas most tackled
with ML are software quality and testing, while human-centered areas appear
more challenging for ML. We propose a number of ML for SE research challenges
and actions including: conducting further empirical validation and industrial
studies on ML; reconsidering deficient SE methods; documenting and automating
data collection and pipeline processes; reexamining how industrial
practitioners distribute their proprietary data; and implementing incremental
ML approaches.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：DexPoint: Generalizable Point Cloud Reinforcement Learning for  Sim-to-Real Dexterous Manipulation</b></summary>
  <p><b>编号</b>：[143]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09423</p>
  <p><b>作者</b>：Yuzhe Qin,  Binghao Huang,  Zhao-Heng Yin,  Hao Su,  Xiaolong Wang</p>
  <p><b>备注</b>：Conference on Robot Learning (CoRL) 2022</p>
  <p><b>关键词</b>：real world, dexterous hands, dexterous, hand point clouds, framework</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a sim-to-real framework for dexterous manipulation which can
generalize to new objects of the same category in the real world. The key of
our framework is to train the manipulation policy with point cloud inputs and
dexterous hands. We propose two new techniques to enable joint learning on
multiple objects and sim-to-real generalization: (i) using imagined hand point
clouds as augmented inputs; and (ii) designing novel contact-based rewards. We
empirically evaluate our method using an Allegro Hand to grasp novel objects in
both simulation and real world. To the best of our knowledge, this is the first
policy learning-based framework that achieves such generalization results with
dexterous hands. Our project page is available at
this https URL</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：FedSiam-DA: Dual-aggregated Federated Learning via Siamese Networks  under Non-IID Data</b></summary>
  <p><b>编号</b>：[144]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09421</p>
  <p><b>作者</b>：Ming Yang,  Yanhan Wang,  Xin Wang,  Zhenyong Zhang,  Xiaoming Wu,  Peng Cheng</p>
  <p><b>备注</b>：8 pages, 5 figures</p>
  <p><b>关键词</b>：original data locally, Federated learning, local model, Siamese Network, model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Federated learning is a distributed learning that allows each client to keep
the original data locally and only upload the parameters of the local model to
the server. Despite federated learning can address data island, it remains
challenging to train with data heterogeneous in a real application. In this
paper, we propose FedSiam-DA, a novel dual-aggregated contrastive federated
learning approach, to personalize both local and global models, under various
settings of data heterogeneity. Firstly, based on the idea of contrastive
learning in the Siamese Network, FedSiam-DA regards the local and global model
as different branches of the Siamese Network during the local training and
controls the update direction of the model by constantly changing model
similarity to personalize the local model. Secondly, FedSiam-DA introduces
dynamic weights based on model similarity for each local model and exercises
the dual-aggregated mechanism to further improve the generalization of the
global model. Moreover, we provide extensive experiments on benchmark datasets,
the results demonstrate that FedSiam-DA achieves outperforming several previous
FL approaches on heterogeneous datasets.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Physics-Informed Koopman Network</b></summary>
  <p><b>编号</b>：[145]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09419</p>
  <p><b>作者</b>：Yuying Liu,  Aleksei Sholokhov,  Hassan Mansour,  Saleh Nabi</p>
  <p><b>备注</b>：26 pages, 5 figures</p>
  <p><b>关键词</b>：linearize nonlinear dynamics, receiving increased attention, increased attention due, Koopman operator theory, nonlinear dynamics</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Koopman operator theory is receiving increased attention due to its promise
to linearize nonlinear dynamics. Neural networks that are developed to
represent Koopman operators have shown great success thanks to their ability to
approximate arbitrarily complex functions. However, despite their great
potential, they typically require large training data-sets either from
measurements of a real system or from high-fidelity simulations. In this work,
we propose a novel architecture inspired by physics-informed neural networks,
which leverage automatic differentiation to impose the underlying physical laws
via soft penalty constraints during model training. We demonstrate that it not
only reduces the need of large training data-sets, but also maintains high
effectiveness in approximating Koopman eigenfunctions.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Personalized Federated Learning for Multi-task Fault Diagnosis of  Rotating Machinery</b></summary>
  <p><b>编号</b>：[150]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09406</p>
  <p><b>作者</b>：Sheng Guo,  Zengxiang Li,  Hui Liu,  Shubao Zhao,  Cheng Hao Jin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Intelligent fault diagnosis, personalized federated learning, essential to safe, safe operation, federated learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Intelligent fault diagnosis is essential to safe operation of machinery.
However, due to scarce fault samples and data heterogeneity in field machinery,
deep learning based diagnosis methods are prone to over-fitting with poor
generalization ability. To solve the problem, this paper proposes a
personalized federated learning framework, enabling multi-task fault diagnosis
method across multiple factories in a privacypreserving manner. Firstly,
rotating machines from different factories with similar vibration feature data
are categorized into machine groups using a federated clustering method. Then,
a multi-task deep learning model based on convolutional neural network is
constructed to diagnose the multiple faults of machinery with heterogeneous
information fusion. Finally, a personalized federated learning framework is
proposed to solve data heterogeneity across different machines using adaptive
hierarchical aggregation strategy. The case study on collected data from real
machines verifies the effectiveness of the proposed framework. The result shows
that the diagnosis accuracy could be improved significantly using the proposed
personalized federated learning, especially for those machines with scarce
fault samples.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Data Dimension Reduction makes ML Algorithms efficient</b></summary>
  <p><b>编号</b>：[156]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09392</p>
  <p><b>作者</b>：Wisal Khan,  Muhammad Turab,  Waqas Ahmad,  Syed Hasnat Ahmad,  Kelash Kumar,  Bin Luo</p>
  <p><b>备注</b>：Our paper is accepted at International Conference On Emerging Technologies In Electronics, Computing And Communication (ICETECC) 2022</p>
  <p><b>关键词</b>：Principal Component Analysis, Principal Component, Component Analysis, Random Projections, Data dimension reduction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Data dimension reduction (DDR) is all about mapping data from high dimensions
to low dimensions, various techniques of DDR are being used for image dimension
reduction like Random Projections, Principal Component Analysis (PCA), the
Variance approach, LSA-Transform, the Combined and Direct approaches, and the
New Random Approach. Auto-encoders (AE) are used to learn end-to-end mapping.
In this paper, we demonstrate that pre-processing not only speeds up the
algorithms but also improves accuracy in both supervised and unsupervised
learning. In pre-processing of DDR, first PCA based DDR is used for supervised
learning, then we explore AE based DDR for unsupervised learning. In PCA based
DDR, we first compare supervised learning algorithms accuracy and time before
and after applying PCA. Similarly, in AE based DDR, we compare unsupervised
learning algorithm accuracy and time before and after AE representation
learning. Supervised learning algorithms including support-vector machines
(SVM), Decision Tree with GINI index, Decision Tree with entropy and Stochastic
Gradient Descent classifier (SGDC) and unsupervised learning algorithm
including K-means clustering, are used for classification purpose. We used two
datasets MNIST and FashionMNIST Our experiment shows that there is massive
improvement in accuracy and time reduction after pre-processing in both
supervised and unsupervised learning.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Data-Efficient Autoregressive Document Retrieval for Fact Verification</b></summary>
  <p><b>编号</b>：[158]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09388</p>
  <p><b>作者</b>：James Thorne</p>
  <p><b>备注</b>：To appear at SustaiNLP@EMNLP 2022. Code is available: this https URL</p>
  <p><b>关键词</b>：knowledge-intensive natural language, natural language processing, language processing task, processing task formulations, question answering</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Document retrieval is a core component of many knowledge-intensive natural
language processing task formulations such as fact verification and question
answering. Sources of textual knowledge, such as Wikipedia articles, condition
the generation of answers from the models. Recent advances in retrieval use
sequence-to-sequence models to incrementally predict the title of the
appropriate Wikipedia page given a query. However, this method requires
supervision in the form of human annotation to label which Wikipedia pages
contain appropriate context. This paper introduces a distant-supervision method
that does not require any annotation to train autoregressive retrievers that
attain competitive R-Precision and Recall in a zero-shot setting. Furthermore
we show that with task-specific supervised fine-tuning, autoregressive
retrieval performance for two Wikipedia-based fact verification tasks can
approach or even exceed full supervision using less than $1/4$ of the annotated
data indicating possible directions for data-efficient autoregressive
retrieval.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Balanced Deep CCA for Bird Vocalization Detection</b></summary>
  <p><b>编号</b>：[166]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09376</p>
  <p><b>作者</b>：Sumit Kumar,  B. Anshuman,  Linus Ruettimann,  Richard H.R. Hahnloser,  Vipul Arora</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：data, DCCA, detection, Event detection, labeled data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Event detection improves when events are captured by two different modalities
rather than just one. But to train detection systems on multiple modalities is
challenging, in particular when there is abundance of unlabelled data but
limited amounts of labeled data. We develop a novel self-supervised learning
technique for multi-modal data that learns (hidden) correlations between
simultaneously recorded microphone (sound) signals and accelerometer (body
vibration) signals. The key objective of this work is to learn useful
embeddings associated with high performance in downstream event detection tasks
when labeled data is scarce and the audio events of interest (songbird
vocalizations) are sparse. We base our approach on deep canonical correlation
analysis (DCCA) that suffers from event sparseness. We overcome the sparseness
of positive labels by first learning a data sampling model from the labelled
data and by applying DCCA on the output it produces. This method that we term
balanced DCCA (b-DCCA) improves the performance of the unsupervised embeddings
on the downstream supervised audio detection task compared to classsical DCCA.
Because data labels are frequently imbalanced, our method might be of broad
utility in low-resource scenarios.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：How to Fine-Tune Vision Models with SGD</b></summary>
  <p><b>编号</b>：[176]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09359</p>
  <p><b>作者</b>：Ananya Kumar,  Ruoqi Shen,  Sébastien Bubeck,  Suriya Gunasekar</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large neural networks, neural networks, networks in computer, computer vision, SGD</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>SGD (with momentum) and AdamW are the two most used optimizers for
fine-tuning large neural networks in computer vision. When the two methods
perform the same, SGD is preferable because it uses less memory (12
bytes/parameter) than AdamW (16 bytes/parameter). However, on a suite of
downstream tasks, especially those with distribution shifts, we show that
fine-tuning with AdamW performs substantially better than SGD on modern Vision
Transformer and ConvNeXt models. We find that large gaps in performance between
SGD and AdamW occur when the fine-tuning gradients in the first "embedding"
layer are much larger than in the rest of the model. Our analysis suggests an
easy fix that works consistently across datasets and models: merely freezing
the embedding layer (less than 1\% of the parameters) leads to SGD performing
competitively with AdamW while using less memory. Our insights result in
state-of-the-art accuracies on five popular distribution shift benchmarks:
WILDS-FMoW, WILDS-Camelyon, Living-17, Waterbirds, and DomainNet.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：Securer and Faster Privacy-Preserving Distributed Machine Learning</b></summary>
  <p><b>编号</b>：[177]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09353</p>
  <p><b>作者</b>：Hongxiao Wang,  Zoe L. Jiang,  Yanmin Zhao,  Siu-Ming Yiu,  Peng Yang,  Zejiu Tan,  Bohan Jin,  Shiyuan Xu,  Shimin Pan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：machine learning, distributed machine learning, centralized machine learning, machine learning tasks, machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the development of machine learning, it is difficult for a single server
to process all the data. So machine learning tasks need to be spread across
multiple servers, turning centralized machine learning into a distributed one.
However, privacy remains an unsolved problem in distributed machine learning.
Multi-key homomorphic encryption over torus (MKTFHE) is one of the suitable
candidates to solve the problem. However, there may be security risks in the
decryption of MKTFHE and the most recent result about MKFHE only supports the
Boolean operation and linear operation. So, MKTFHE cannot compute the
non-linear function like Sigmoid directly and it is still hard to perform
common machine learning such as logistic regression and neural networks in high
performance.
This paper first introduces secret sharing to propose a new distributed
decryption protocol for MKTFHE, then designs an MKTFHE-friendly activation
function, and finally utilizes them to implement logistic regression and neural
network training in MKTFHE. We prove the correctness and security of our
decryption protocol and compare the efficiency and accuracy between using
Taylor polynomials of Sigmoid and our proposed function as an activation
function. The experiments show that the efficiency of our function is 10 times
higher than using 7-order Taylor polynomials straightly and the accuracy of the
training model is similar to that of using a high-order polynomial as an
activation function scheme.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：ACon$^2$: Adaptive Conformal Consensus for Provable Blockchain Oracles</b></summary>
  <p><b>编号</b>：[188]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09330</p>
  <p><b>作者</b>：Sangdon Park,  Osbert Bastani,  Taesoo Kim</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：distributed ledger systems, block state consistency, allowing deterministic operations, smart contracts, oracle smart contract</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Blockchains with smart contracts are distributed ledger systems which achieve
block state consistency among distributed nodes by only allowing deterministic
operations of smart contracts. However, the power of smart contracts is enabled
by interacting with stochastic off-chain data, which in turn opens the
possibility to undermine the block state consistency. To address this issue, an
oracle smart contract is used to provide a single consistent source of external
data; but, simultaneously this introduces a single point of failure, which is
called the oracle problem. To address the oracle problem, we propose an
adaptive conformal consensus (ACon$^2$) algorithm, which derives consensus from
multiple oracle contracts via the recent advance in online uncertainty
quantification learning. In particular, the proposed algorithm returns a
consensus set, which quantifies the uncertainty of data and achieves a desired
correctness guarantee in the presence of Byzantine adversaries and distribution
shift. We demonstrate the efficacy of the proposed algorithm on two price
datasets and an Ethereum case study. In particular, the Solidity implementation
of the proposed algorithm shows the practicality of the proposed algorithm,
implying that online machine learning algorithms are applicable to address
issues in blockchains.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Perturbation-Recovery Method for Recommendation</b></summary>
  <p><b>编号</b>：[190]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09324</p>
  <p><b>作者</b>：Jeongwhan Choi,  Seoyoung Hong,  Noseong Park,  Sung-Bae Cho</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：influential recommender system, recommender system types, diffusion models, influential recommender, recommender system</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Collaborative filtering is one of the most influential recommender system
types. Various methods have been proposed for collaborative filtering, ranging
from matrix factorization to graph convolutional methods. Being inspired by
recent successes of GF-CF and diffusion models, we present a novel concept of
blurring-sharpening process model (BSPM). Diffusion models and BSPMs share the
same processing philosophy in that new information is discovered (e.g., a new
image is generated in the case of diffusion models) while original information
is first perturbed and then recovered to its original form. However, diffusion
models and our BSPMs deal with different types of information, and their
optimal perturbation and recovery processes have a fundamental discrepancy.
Therefore, our BSPMs have different forms from diffusion models. In addition,
our concept not only theoretically subsumes many existing collaborative
filtering models but also outperforms them in terms of Recall and NDCG in the
three benchmark datasets, Gowalla, Yelp2018, and Amazon-book. Our model marks
the best accuracy in them. In addition, the processing time of our method is
one of the shortest cases ever in collaborative filtering. Our proposed concept
has much potential in the future to be enhanced by designing better blurring
(i.e., perturbation) and sharpening (i.e., recovery) processes than what we use
in this paper.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Interpretable Dimensionality Reduction by Feature Preserving Manifold  Approximation and Projection</b></summary>
  <p><b>编号</b>：[192]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09321</p>
  <p><b>作者</b>：Yang Yang,  Hongjian Sun,  Jialei Gong,  Yali Du,  Di Yu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Nonlinear dimensionality reduction, dimensionality reduction lacks, reduction lacks interpretability, lacks interpretability due, source features</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Nonlinear dimensionality reduction lacks interpretability due to the absence
of source features in low-dimensional embedding space. We propose an
interpretable method featMAP to preserve source features by tangent space
embedding. The core of our proposal is to utilize local singular value
decomposition (SVD) to approximate the tangent space which is embedded to
low-dimensional space by maintaining the alignment. Based on the embedding
tangent space, featMAP enables the interpretability by locally demonstrating
the source features and feature importance. Furthermore, featMAP embeds the
data points by anisotropic projection to preserve the local similarity and
original density. We apply featMAP to interpreting digit classification, object
detection and MNIST adversarial examples. FeatMAP uses source features to
explicitly distinguish the digits and objects and to explain the
misclassification of adversarial examples. We also compare featMAP with other
state-of-the-art methods on local and global metrics.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Explainable, Domain-Adaptive, and Federated Artificial Intelligence in  Medicine</b></summary>
  <p><b>编号</b>：[194]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09317</p>
  <p><b>作者</b>：Ahmad Chaddad,  Qizong lu,  Jiali Li,  Yousef Katib,  Reem Kateb,  Camel Tanougast,  Ahmed Bouridane,  Ahmed Abdulkadir</p>
  <p><b>备注</b>：This paper is accepted in IEEE CAA Journal of Automatica Sinica, Nov. 10 2022</p>
  <p><b>关键词</b>：transform data analysis, Artificial intelligence, continues to transform, transform data, data analysis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Artificial intelligence (AI) continues to transform data analysis in many
domains. Progress in each domain is driven by a growing body of annotated data,
increased computational resources, and technological innovations. In medicine,
the sensitivity of the data, the complexity of the tasks, the potentially high
stakes, and a requirement of accountability give rise to a particular set of
challenges. In this review, we focus on three key methodological approaches
that address some of the particular challenges in AI-driven medical decision
making. (1) Explainable AI aims to produce a human-interpretable justification
for each output. Such models increase confidence if the results appear
plausible and match the clinicians expectations. However, the absence of a
plausible explanation does not imply an inaccurate model. Especially in highly
non-linear, complex models that are tuned to maximize accuracy, such
interpretable representations only reflect a small portion of the
justification. (2) Domain adaptation and transfer learning enable AI models to
be trained and applied across multiple domains. For example, a classification
task based on images acquired on different acquisition hardware. (3) Federated
learning enables learning large-scale models without exposing sensitive
personal health information. Unlike centralized AI learning, where the
centralized learning machine has access to the entire training data, the
federated learning process iteratively updates models across multiple sites by
exchanging only parameter updates, not personal health data. This narrative
review covers the basic concepts, highlights relevant corner-stone and
state-of-the-art research in the field, and discusses perspectives.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：Proactive Resilient Transmission and Scheduling Mechanisms for mmWave  Networks</b></summary>
  <p><b>编号</b>：[196]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09307</p>
  <p><b>作者</b>：Mine Gokce Dogan,  Martina Cardone,  Christina Fragouli</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2108.00548</p>
  <p><b>关键词</b>：suitably distribute traffic, develop resilient transmission, resilient transmission mechanisms, path selection algorithm, arbitrary millimeter-wave</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper aims to develop resilient transmission mechanisms to suitably
distribute traffic across multiple paths in an arbitrary millimeter-wave
(mmWave) network. The main contributions include: (a) the development of
proactive transmission mechanisms that build resilience against network
disruptions in advance, while achieving a high end-to-end packet rate; (b) the
design of a heuristic path selection algorithm that efficiently selects (in
polynomial time in the network size) multiple proactively resilient paths with
high packet rates; and (c) the development of a hybrid scheduling algorithm
that combines the proposed path selection algorithm with a deep reinforcement
learning (DRL) based online approach for decentralized adaptation to blocked
links and failed paths. To achieve resilience to link failures, a
state-of-the-art Soft Actor-Critic DRL algorithm, which adapts the information
flow through the network, is investigated. The proposed scheduling algorithm
robustly adapts to link failures over different topologies, channel and
blockage realizations while offering a superior performance to alternative
algorithms.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：FedFA: Federated Learning with Feature Anchors to Align Feature and  Classifier for Heterogeneous Data</b></summary>
  <p><b>编号</b>：[199]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09299</p>
  <p><b>作者</b>：Tailin Zhou,  Jun Zhang,  Danny Tsang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：preserving data privacy, collaboratively train, feature, data privacy, preserving data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Federated learning allows multiple clients to collaboratively train a model
without exchanging their data, thus preserving data privacy. Unfortunately, it
suffers significant performance degradation under heterogeneous data at
clients. Common solutions in local training involve designing a specific
auxiliary loss to regularize weight divergence or feature inconsistency.
However, we discover that these approaches fall short of the expected
performance because they ignore the existence of a vicious cycle between
classifier divergence and feature mapping inconsistency across clients, such
that client models are updated in inconsistent feature space with diverged
classifiers. We then propose a simple yet effective framework named Federated
learning with Feature Anchors (FedFA) to align the feature mappings and
calibrate classifier across clients during local training, which allows client
models updating in a shared feature space with consistent classifiers. We
demonstrate that this modification brings similar classifiers and a virtuous
cycle between feature consistency and classifier similarity across clients.
Extensive experiments show that FedFA significantly outperforms the
state-of-the-art federated learning algorithms on various image classification
datasets under label and feature distribution skews.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Permutation-Invariant Tabular Data Synthesis</b></summary>
  <p><b>编号</b>：[202]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09286</p>
  <p><b>作者</b>：Yujin Zhu,  Zilong Zhao,  Robert Birke,  Lydia Y. Chen</p>
  <p><b>备注</b>：Paper is accepted in 2022 IEEE International Conference Big Data in Special Session Privacy and Security of Big Data (PSBD)</p>
  <p><b>关键词</b>：circumvent strict regulations, Tabular data, data, synthetic tabular data, Tabular data synthesis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Tabular data synthesis is an emerging approach to circumvent strict
regulations on data privacy while discovering knowledge through big data.
Although state-of-the-art AI-based tabular data synthesizers, e.g., table-GAN,
CTGAN, TVAE, and CTAB-GAN, are effective at generating synthetic tabular data,
their training is sensitive to column permutations of input data. In this
paper, we first conduct an extensive empirical study to disclose such a
property of permutation invariance and an in-depth analysis of the existing
synthesizers. We show that changing the input column order worsens the
statistical difference between real and synthetic data by up to 38.67% due to
the encoding of tabular data and the network architectures. To fully unleash
the potential of big synthetic tabular data, we propose two solutions: (i)
AE-GAN, a synthesizer that uses an autoencoder network to represent the tabular
data and GAN networks to synthesize the latent representation, and (ii) a
feature sorting algorithm to find the suitable column order of input data for
CNN-based synthesizers. We evaluate the proposed solutions on five datasets in
terms of the sensitivity to the column permutation, the quality of synthetic
data, and the utility in downstream analyses. Our results show that we enhance
the property of permutation-invariance when training synthesizers and further
improve the quality and utility of synthetic data, up to 22%, compared to the
existing synthesizers.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：Active Learning with Expected Error Reduction</b></summary>
  <p><b>编号</b>：[204]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09283</p>
  <p><b>作者</b>：Stephen Mussmann,  Julia Reisler,  Daniel Tsai,  Ehsan Mousavi,  Shayne O'Brien,  Moises Goldszmidt</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Expected Error Reduction, Active learning, studied extensively, Bayesian active learning, efficient data collection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Active learning has been studied extensively as a method for efficient data
collection. Among the many approaches in literature, Expected Error Reduction
(EER) (Roy and McCallum) has been shown to be an effective method for active
learning: select the candidate sample that, in expectation, maximally decreases
the error on an unlabeled set. However, EER requires the model to be retrained
for every candidate sample and thus has not been widely used for modern deep
neural networks due to this large computational cost. In this paper we
reformulate EER under the lens of Bayesian active learning and derive a
computationally efficient version that can use any Bayesian parameter sampling
method (such as arXiv:1506.02142). We then compare the empirical performance of
our method using Monte Carlo dropout for parameter sampling against state of
the art methods in the deep active learning literature. Experiments are
performed on four standard benchmark datasets and three WILDS datasets
(arXiv:2012.07421). The results indicate that our method outperforms all other
methods except one in the data shift scenario: a model dependent,
non-information theoretic method that requires an order of magnitude higher
computational cost (arXiv:1906.03671).</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：Privacy against Real-Time Speech Emotion Detection via Acoustic  Adversarial Evasion of Machine Learning</b></summary>
  <p><b>编号</b>：[205]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09273</p>
  <p><b>作者</b>：Brian Testa,  Yi Xiao,  Avery Gump,  Asif Salekin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：SER classifiers, wide-reaching privacy concerns, black-box SER classifiers, SER, emerging area</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Emotional Surveillance is an emerging area with wide-reaching privacy
concerns. These concerns are exacerbated by ubiquitous IoT devices with
multiple sensors that can support these surveillance use cases. The work
presented here considers one such use case: the use of a speech emotion
recognition (SER) classifier tied to a smart speaker. This work demonstrates
the ability to evade black-box SER classifiers tied to a smart speaker without
compromising the utility of the smart speaker. This privacy concern is
considered through the lens of adversarial evasion of machine learning. Our
solution, Defeating Acoustic Recognition of Emotion via Genetic Programming
(DARE-GP), uses genetic programming to generate non-invasive additive audio
perturbations (AAPs). By constraining the evolution of these AAPs,
transcription accuracy can be protected while simultaneously degrading SER
classifier performance. The additive nature of these AAPs, along with an
approach that generates these AAPs for a fixed set of users in an utterance and
user location-independent manner, supports real-time, real-world evasion of SER
classifiers. DARE-GP's use of spectral features, which underlay the emotional
content of speech, allows the transferability of AAPs to previously unseen
black-box SER classifiers. Further, DARE-GP outperforms state-of-the-art SER
evasion techniques and is robust against defenses employed by a knowledgeable
adversary. The evaluations in this work culminate with acoustic evaluations
against two off-the-shelf commercial smart speakers, where a single AAP could
evade a black box classifier over 70% of the time. The final evaluation
deployed AAP playback on a small-form-factor system (raspberry pi) integrated
with a wake-word system to evaluate the efficacy of a real-world, real-time
deployment where DARE-GP is automatically invoked with the smart speaker's wake
word.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：Reflect, Not Reflex: Inference-Based Common Ground Improves Dialogue  Response Quality</b></summary>
  <p><b>编号</b>：[206]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09267</p>
  <p><b>作者</b>：Pei Zhou,  Hyundong Cho,  Pegah Jandaghi,  Dong-Ho Lee,  Bill Yuchen Lin,  Jay Pujara,  Xiang Ren</p>
  <p><b>备注</b>：Accepted at EMNLP-2022. 19 pages, 17 figures, 4 tables</p>
  <p><b>关键词</b>：Human communication relies, Human communication, communication relies, common ground, Reflect</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Human communication relies on common ground (CG), the mutual knowledge and
beliefs shared by participants, to produce coherent and interesting
conversations. In this paper, we demonstrate that current response generation
(RG) models produce generic and dull responses in dialogues because they act
reflexively, failing to explicitly model CG, both due to the lack of CG in
training data and the standard RG training procedure. We introduce Reflect, a
dataset that annotates dialogues with explicit CG (materialized as inferences
approximating shared knowledge and beliefs) and solicits 9k diverse
human-generated responses each following one common ground. Using Reflect, we
showcase the limitations of current dialogue data and RG models: less than half
of the responses in current data are rated as high quality (sensible, specific,
and interesting) and models trained using this data have even lower quality,
while most Reflect responses are judged high quality. Next, we analyze whether
CG can help models produce better-quality responses by using Reflect CG to
guide RG models. Surprisingly, we find that simply prompting GPT3 to "think"
about CG generates 30% more quality responses, showing promising benefits to
integrating CG into the RG process.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：Informative Initialization and Kernel Selection Improves t-SNE for  Biological Sequences</b></summary>
  <p><b>编号</b>：[207]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09263</p>
  <p><b>作者</b>：Prakash Chourasia,  Sarwan Ali,  Murray Patterson</p>
  <p><b>备注</b>：Accepted to IEEE BigData 2022</p>
  <p><b>关键词</b>：interpreting high dimensional, stochastic neighbor embedding, t-distributed stochastic neighbor, high dimensional, low dimensional</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The t-distributed stochastic neighbor embedding (t- SNE) is a method for
interpreting high dimensional (HD) data by mapping each point to a low
dimensional (LD) space (usually two-dimensional). It seeks to retain the
structure of the data. An important component of the t-SNE algorithm is the
initialization procedure, which begins with the random initialization of an LD
vector. Points in this initial vector are then updated to minimize the loss
function (the KL divergence) iteratively using gradient descent. This leads
comparable points to attract one another while pushing dissimilar points apart.
We believe that, by default, these algorithms should employ some form of
informative initialization. Another essential component of the t-SNE is using a
kernel matrix, a similarity matrix comprising the pairwise distances among the
sequences. For t-SNE-based visualization, the Gaussian kernel is employed by
default in the literature. However, we show that kernel selection can also play
a crucial role in the performance of t-SNE. In this work, we assess the
performance of t-SNE with various alternative initialization methods and
kernels, using four different sets, out of which three are biological sequences
(nucleotide, protein, etc.) datasets obtained from various sources, such as the
well-known GISAID database for sequences of the SARS- CoV-2 virus. We perform
subjective and objective assessments of these alternatives. We use the
resulting t-SNE plots and k- ary neighborhood agreement (k-ANA) to evaluate and
compare the proposed methods with the baselines. We show that by using
different techniques, such as informed initialization and kernel matrix
selection, that t-SNE performs significantly better. Moreover, we show that
t-SNE also takes fewer iterations to converge faster with more intelligent
initialization.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：The Missing Indicator Method: From Low to High Dimensions</b></summary>
  <p><b>编号</b>：[209]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09259</p>
  <p><b>作者</b>：Mike Van Ness,  Tomas M. Bosschieter,  Roberto Halpin-Gregorio,  Madeleine Udell</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：applied data science, social sciences, Missing, natural sciences, MIM</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Missing data is common in applied data science, particularly for tabular data
sets found in healthcare, social sciences, and natural sciences. Most
supervised learning methods work only on complete data, thus requiring
preprocessing, such as missing value imputation, to work on incomplete data
sets. However, imputation discards potentially useful information encoded by
the pattern of missing values. For data sets with informative missing patterns,
the Missing Indicator Method (MIM), which adds indicator variables to indicate
the missing pattern, can be used in conjunction with imputation to improve
model performance. We show experimentally that MIM improves performance for
informative missing values, and we prove that MIM does not hurt linear models
asymptotically for uninformative missing values. Nonetheless, MIM can increase
variance if many of the added indicators are uninformative, causing harm
particularly for high-dimensional data sets. To address this issue, we
introduce Selective MIM (SMIM), a method that adds missing indicators only for
features that have informative missing patterns. We show empirically that SMIM
performs at least as well as MIM across a range of experimental settings, and
improves MIM for high-dimensional data.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：On the Power of Learning-Augmented BSTs</b></summary>
  <p><b>编号</b>：[212]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09251</p>
  <p><b>作者</b>：Jingbang Chen,  Li Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Binary Search Tree, Learning-Augmented Binary Search, Search Tree, Binary Search, Learning-Augmented Binary</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present the first Learning-Augmented Binary Search Tree(BST) that attains
Static Optimality and Working-Set Bound given rough predictions. Following the
recent studies in algorithms with predictions and learned index structures,
Lin, Luo, and Woodruff (ICML 2022) introduced the concept of Learning-Augmented
BSTs, which aim to improve BSTs with learned advice. Unfortunately, their
construction gives only static optimality under strong assumptions on the
input.
In this paper, we present a simple BST maintenance scheme that benefits from
learned advice. With proper predictions, the scheme achieves Static Optimality
and Working-Set Bound, respectively, which are important performance measures
for BSTs. Moreover, the scheme is robust to prediction errors and makes no
assumption on the input.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：Learning unfolded networks with a cyclic group structure</b></summary>
  <p><b>编号</b>：[214]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09238</p>
  <p><b>作者</b>：Emmanouil Theodosis,  Demba Ba</p>
  <p><b>备注</b>：Accepted as an extended abstract in NeurIPS Workshop on Symmetry and Geometry in Neural Representations</p>
  <p><b>关键词</b>：considered black boxes, notoriously considered black, incorporate domain knowledge, networks lack straightforward, black boxes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep neural networks lack straightforward ways to incorporate domain
knowledge and are notoriously considered black boxes. Prior works attempted to
inject domain knowledge into architectures implicitly through data
augmentation. Building on recent advances on equivariant neural networks, we
propose networks that explicitly encode domain knowledge, specifically
equivariance with respect to rotations. By using unfolded architectures, a rich
framework that originated from sparse coding and has theoretical guarantees, we
present interpretable networks with sparse activations. The equivariant
unfolded networks compete favorably with baselines, with only a fraction of
their parameters, as showcased on (rotated) MNIST and CIFAR-10.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：The Surprising Effectiveness of Equivariant Models in Domains with  Latent Symmetry</b></summary>
  <p><b>编号</b>：[217]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09231</p>
  <p><b>作者</b>：Dian Wang,  Jung Yeon Park,  Neel Sortur,  Lawson L.S. Wong,  Robin Walters,  Robert Platt</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：improve sample efficiency, Extensive work, work has demonstrated, sample efficiency, efficiency and generalization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Extensive work has demonstrated that equivariant neural networks can
significantly improve sample efficiency and generalization by enforcing an
inductive bias in the network architecture. These applications typically assume
that the domain symmetry is fully described by explicit transformations of the
model inputs and outputs. However, many real-life applications contain only
latent or partial symmetries which cannot be easily described by simple
transformations of the input. In these cases, it is necessary to learn symmetry
in the environment instead of imposing it mathematically on the network
architecture. We discover, surprisingly, that imposing equivariance constraints
that do not exactly match the domain symmetry is very helpful in learning the
true symmetry in the environment. We differentiate between extrinsic and
incorrect symmetry constraints and show that while imposing incorrect symmetry
can impede the model's performance, imposing extrinsic symmetry can actually
improve performance. We demonstrate that an equivariant model can significantly
outperform non-equivariant methods on domains with latent symmetries both in
supervised learning and in reinforcement learning for robotic manipulation and
control problems.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：Are we certain it's anomalous?</b></summary>
  <p><b>编号</b>：[219]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09224</p>
  <p><b>作者</b>：Alessandro Flaborea,  Bardh Prenkaj,  Bharti Munjal,  Marco Aurelio Sterpa,  Dario Aragona,  Luca Podo,  Fabio Galasso</p>
  <p><b>备注</b>：Submitted at IEEE Transactions on Neural Networks and Learning Systems</p>
  <p><b>关键词</b>：recently revamped research, modelling time series, anomaly detection, progress in modelling, structured-data has recently</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The progress in modelling time series and, more generally, sequences of
structured-data has recently revamped research in anomaly detection. The task
stands for identifying abnormal behaviours in financial series, IT systems,
aerospace measurements, and the medical domain, where anomaly detection may aid
in isolating cases of depression and attend the elderly. Anomaly detection in
time series is a complex task since anomalies are rare due to highly non-linear
temporal correlations and since the definition of anomalous is sometimes
subjective. Here we propose the novel use of Hyperbolic uncertainty for Anomaly
Detection (HypAD). HypAD learns self-supervisedly to reconstruct the input
signal. We adopt best practices from the state-of-the-art to encode the
sequence by an LSTM, jointly learnt with a decoder to reconstruct the signal,
with the aid of GAN critics. Uncertainty is estimated end-to-end by means of a
hyperbolic neural network. By using uncertainty, HypAD may assess whether it is
certain about the input signal but it fails to reconstruct it because this is
anomalous; or whether the reconstruction error does not necessarily imply
anomaly, as the model is uncertain, e.g. a complex but regular input signal.
The novel key idea is that a detectable anomaly is one where the model is
certain but it predicts wrongly. HypAD outperforms the current state-of-the-art
for univariate anomaly detection on established benchmarks based on data from
NASA, Yahoo, Numenta, Amazon, Twitter. It also yields state-of-the-art
performance on a multivariate dataset of anomaly activities in elderly home
residences, and it outperforms the baseline on SWaT. Overall, HypAD yields the
lowest false alarms at the best performance rate, thanks to successfully
identifying detectable anomalies.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：CASPR: Customer Activity Sequence-based Prediction and Representation</b></summary>
  <p><b>编号</b>：[229]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09174</p>
  <p><b>作者</b>：Pin-Jung Chen,  Sahil Bhatnagar,  Damian Konrad Kowalczyk,  Mayank Shrivastava</p>
  <p><b>备注</b>：Presented at the Table Representation Learning Workshop, NeurIPS 2022, New Orleans</p>
  <p><b>关键词</b>：fraudulent account detection, Tasks critical, fraudulent account, lifetime value estimation, account detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Tasks critical to enterprise profitability, such as customer churn
prediction, fraudulent account detection or customer lifetime value estimation,
are often tackled by models trained on features engineered from customer data
in tabular format. Application-specific feature engineering adds development,
operationalization and maintenance costs over time. Recent advances in
representation learning present an opportunity to simplify and generalize
feature engineering across applications. When applying these advancements to
tabular data researchers deal with data heterogeneity, variations in customer
engagement history or the sheer volume of enterprise datasets. In this paper,
we propose a novel approach to encode tabular data containing customer
transactions, purchase history and other interactions into a generic
representation of a customer's association with the business. We then evaluate
these embeddings as features to train multiple models spanning a variety of
applications. CASPR, Customer Activity Sequence-based Prediction and
Representation, applies Transformer architecture to encode activity sequences
to improve model performance and avoid bespoke feature engineering across
applications. Our experiments at scale validate CASPR for both small \& large
enterprise applications.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：Engineering Monosemanticity in Toy Models</b></summary>
  <p><b>编号</b>：[232]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09169</p>
  <p><b>作者</b>：Adam S. Jermyn,  Nicholas Schiefer,  Evan Hubinger</p>
  <p><b>备注</b>：31 pages, 26 figures</p>
  <p><b>关键词</b>：individual neurons correspond, neural networks, correspond to natural, monosemantic, models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In some neural networks, individual neurons correspond to natural
``features'' in the input. Such \emph{monosemantic} neurons are of great help
in interpretability studies, as they can be cleanly understood. In this work we
report preliminary attempts to engineer monosemanticity in toy models. We find
that models can be made more monosemantic without increasing the loss by just
changing which local minimum the training process finds. More monosemantic loss
minima have moderate negative biases, and we are able to use this fact to
engineer highly monosemantic models. We are able to mechanistically interpret
these models, including the residual polysemantic neurons, and uncover a simple
yet surprising algorithm. Finally, we find that providing models with more
neurons per layer makes the models more monosemantic, albeit at increased
computational cost. These findings point to a number of new questions and
avenues for engineering monosemanticity, which we intend to study these in
future work.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：Learnable Graph Convolutional Network and Feature Fusion for Multi-view  Learning</b></summary>
  <p><b>编号</b>：[237]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09155</p>
  <p><b>作者</b>：Zhaoliang Chen,  Lele Fu,  Jie Yao,  Wenzhong Guo,  Claudia Plant,  Shiping Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：data depicting objectives, multi-view data depicting, practical applications, Learnable Graph Convolutional, graph convolutional network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In practical applications, multi-view data depicting objectives from assorted
perspectives can facilitate the accuracy increase of learning algorithms.
However, given multi-view data, there is limited work for learning
discriminative node relationships and graph information simultaneously via
graph convolutional network that has drawn the attention from considerable
researchers in recent years. Most of existing methods only consider the
weighted sum of adjacency matrices, yet a joint neural network of both feature
and graph fusion is still under-explored. To cope with these issues, this paper
proposes a joint deep learning framework called Learnable Graph Convolutional
Network and Feature Fusion (LGCN-FF), consisting of two stages: feature fusion
network and learnable graph convolutional network. The former aims to learn an
underlying feature representation from heterogeneous views, while the latter
explores a more discriminative graph fusion via learnable weights and a
parametric activation function dubbed Differentiable Shrinkage Activation (DSA)
function. The proposed LGCN-FF is validated to be superior to various
state-of-the-art methods in multi-view semi-supervised classification.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：Mapping Tropical Forest Cover and Deforestation with Planet NICFI  Satellite Images and Deep Learning in Mato Grosso State (Brazil) from 2015 to  2021</b></summary>
  <p><b>编号</b>：[240]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09806</p>
  <p><b>作者</b>：Fabien H Wagner,  Ricardo Dalagnol,  Celso HL Silva-Junior,  Griffin Carter,  Alison L Ritz,  Mayumi CM Hirye,  Jean PHB Ometto,  Sassan Saatchi</p>
  <p><b>备注</b>：18 pages, 10 figures, submitted to Remote Sensing MDPI, Special Issue "Remote Sensing of the Amazon Region"</p>
  <p><b>关键词</b>：climate mitigation policy, tree cover, reducing carbon, December, rapid assessment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Monitoring changes in tree cover for rapid assessment of deforestation is
considered the critical component of any climate mitigation policy for reducing
carbon. Here, we map tropical tree cover and deforestation between 2015 and
2022 using 5 m spatial resolution Planet NICFI satellite images over the state
of Mato Grosso (MT) in Brazil and a U-net deep learning model. The tree cover
for the state was 556510.8 km$^2$ in 2015 (58.1 % of the MT State) and was
reduced to 141598.5 km$^2$ (14.8 % of total area) at the end of 2021. After
reaching a minimum deforested area in December 2016 with 6632.05 km$^2$, the
bi-annual deforestation area only showed a slight increase between December
2016 and December 2019. A year after, the areas of deforestation almost doubled
from 9944.5 km$^2$ in December 2019 to 19817.8 km$^2$ in December 2021. The
high-resolution data product showed relatively consistent agreement with the
official deforestation map from Brazil (67.2%) but deviated significantly from
year of forest cover loss estimates from the Global Forest change (GFC)
product, mainly due to large area of fire degradation observed in the GFC data.
High-resolution imagery from Planet NICFI associated with deep learning
technics can significantly improve mapping deforestation extent in tropics.</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：Machine Learned Calabi--Yau Metrics and Curvature</b></summary>
  <p><b>编号</b>：[241]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09801</p>
  <p><b>作者</b>：Per Berglund,  Giorgi Butbaia,  Tristan Hübsch,  Vishnu Jejjala,  Damián Mayorga Peña,  Challenger Mishra,  Justin Tan</p>
  <p><b>备注</b>：36 pages, 21 figures, 7 tables, 2 appendices</p>
  <p><b>关键词</b>：long standing problem, theory and phenomenology, long standing, geometry with deep, deep implications</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Finding Ricci-flat (Calabi--Yau) metrics is a long standing problem in
geometry with deep implications for string theory and phenomenology. A new
attack on this problem uses neural networks to engineer approximations to the
Calabi--Yau metric within a given Kähler class. In this paper we investigate
numerical Ricci-flat metrics over smooth and singular K3 surfaces and
Calabi--Yau threefolds. Using these Ricci-flat metric approximations for the
Cefalú and Dwork family of quartic twofolds and the Dwork family of quintic
threefolds, we study characteristic forms on these geometries. Using persistent
homology, we show that high curvature regions of the manifolds form clusters
near the singular points, but also elsewhere. For our neural network
approximations, we observe a Bogomolov--Yau type inequality $3c_2 \geq c_1^2$
and observe an identity when our geometries have isolated $A_1$ type
singularities. We sketch a proof that
$\chi(X~\smallsetminus~\mathrm{Sing}\,{X}) + 2~|\mathrm{Sing}\,{X}| = 24$ also
holds for our numerical approximations.</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：Monitoring machine learning (ML)-based risk prediction algorithms in the  presence of confounding medical interventions</b></summary>
  <p><b>编号</b>：[242]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09781</p>
  <p><b>作者</b>：Jean Feng,  Alexej Gossmann,  Gene Pennello,  Nicholas Petrick,  Berkman Sahiner,  Romain Pirracchio</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：confounding medical interventions, administer prophylactic treatment, algorithm predicts, algorithm aims, machine learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Monitoring the performance of machine learning (ML)-based risk prediction
models in healthcare is complicated by the issue of confounding medical
interventions (CMI): when an algorithm predicts a patient to be at high risk
for an adverse event, clinicians are more likely to administer prophylactic
treatment and alter the very target that the algorithm aims to predict.
Ignoring CMI by monitoring only the untreated patients--whose outcomes remain
unaltered--can inflate false alarm rates, because the evolution of both the
model and clinician-ML interactions can induce complex dependencies in the data
that violate standard assumptions. A more sophisticated approach is to
explicitly account for CMI by modeling treatment propensities, but its
time-varying nature makes accurate estimation difficult. Given the many sources
of complexity in the data, it is important to determine situations in which a
simple procedure that ignores CMI provides valid inference. Here we describe
the special case of monitoring model calibration, under either the assumption
of conditional exchangeability or time-constant selection bias. We introduce a
new score-based cumulative sum (CUSUM) chart for monitoring in a frequentist
framework and review an alternative approach using Bayesian inference. Through
simulations, we investigate the benefits of combining model updating with
monitoring and study when over-trust in a prediction model does (or does not)
delay detection. Finally, we simulate monitoring an ML-based postoperative
nausea and vomiting risk calculator during the COVID-19 pandemic.</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：DeepSense 6G: A Large-Scale Real-World Multi-Modal Sensing and  Communication Dataset</b></summary>
  <p><b>编号</b>：[243]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09769</p>
  <p><b>作者</b>：Ahmed Alkhateeb,  Gouranga Charan,  Tawfik Osman,  Andrew Hredzak,  João Morais,  Umut Demirhan,  Nikhil Srinivas</p>
  <p><b>备注</b>：The dataset is available on the DeepSense 6G website this http URL</p>
  <p><b>关键词</b>：co-existing multi-modal sensing, large-scale dataset based, multi-modal sensing, based on real-world, real-world measurements</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This article presents the DeepSense 6G dataset, which is a large-scale
dataset based on real-world measurements of co-existing multi-modal sensing and
communication data. The DeepSense 6G dataset is built to advance deep learning
research in a wide range of applications in the intersection of multi-modal
sensing, communication, and positioning. This article provides a detailed
overview of the DeepSense dataset structure, adopted testbeds, data collection
and processing methodology, deployment scenarios, and example applications,
with the objective of facilitating the adoption and reproducibility of
multi-modal sensing and communication datasets.</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：Understanding and eliminating spurious modes in variational Monte Carlo  using collective variables</b></summary>
  <p><b>编号</b>：[244]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09767</p>
  <p><b>作者</b>：Huan Zhang,  Robert J. Webber,  Michael Lindsey,  Timothy C. Berkelbach,  Jonathan Weare</p>
  <p><b>备注</b>：12 pages, 13 figures</p>
  <p><b>关键词</b>：variational Monte Carlo, neural network parametrizations, generated intense interest, Monte Carlo, variational Monte</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The use of neural network parametrizations to represent the ground state in
variational Monte Carlo (VMC) calculations has generated intense interest in
recent years. However, as we demonstrate in the context of the periodic
Heisenberg spin chain, this approach can produce unreliable wave function
approximations. One of the most obvious signs of failure is the occurrence of
random, persistent spikes in the energy estimate during training. These energy
spikes are caused by regions of configuration space that are over-represented
by the wave function density, which are called ``spurious modes'' in the
machine learning literature. After exploring these spurious modes in detail, we
demonstrate that a collective-variable-based penalization yields a
substantially more robust training procedure, preventing the formation of
spurious modes and improving the accuracy of energy estimates. Because the
penalization scheme is cheap to implement and is not specific to the particular
model studied here, it can be extended to other applications of VMC where a
reasonable choice of collective variable is available.</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：An Advantage Using Feature Selection with a Quantum Annealer</b></summary>
  <p><b>编号</b>：[245]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09756</p>
  <p><b>作者</b>：Andrew Vlasic,  Grant Hunter,  Salvatore Certo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：strong statistical connection, target variable, statistical connection, statistical prediction modeling, Feature selection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Feature selection is a technique in statistical prediction modeling that
identifies features in a record with a strong statistical connection to the
target variable. Excluding features with a weak statistical connection to the
target variable in training not only drops the dimension of the data, which
decreases the time complexity of the algorithm, it also decreases noise within
the data which assists in avoiding overfitting. In all, feature selection
assists in training a robust statistical model that performs well and is
stable. Given the lack of scalability in classical computation, current
techniques only consider the predictive power of the feature and not redundancy
between the features themselves. Recent advancements in feature selection that
leverages quantum annealing (QA) gives a scalable technique that aims to
maximize the predictive power of the features while minimizing redundancy. As a
consequence, it is expected that this algorithm would assist in the
bias/variance trade-off yielding better features for training a statistical
model. This paper tests this intuition against classical methods by utilizing
open-source data sets and evaluate the efficacy of each trained statistical
model well-known prediction algorithms. The numerical results display an
advantage utilizing the features selected from the algorithm that leveraged QA.</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：Behavior Score-Embedded Brain Encoder Network for Improved  Classification of Alzheimer Disease Using Resting State fMRI</b></summary>
  <p><b>编号</b>：[246]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09735</p>
  <p><b>作者</b>：Wan-Ting Hsieh,  Jeremy Lefort-Besnard,  Hao-Chun Yang,  Li-Wei Kuo,  Chi-Chun Lee</p>
  <p><b>备注</b>：4 pages, 1 figure</p>
  <p><b>关键词</b>：accurately detect onset, Mild Cognitive Impairment, ability to accurately, accurately detect, detect onset</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The ability to accurately detect onset of dementia is important in the
treatment of the disease. Clinically, the diagnosis of Alzheimer Disease (AD)
and Mild Cognitive Impairment (MCI) patients are based on an integrated
assessment of psychological tests and brain imaging such as positron emission
tomography (PET) and anatomical magnetic resonance imaging (MRI). In this work
using two different datasets, we propose a behavior score-embedded encoder
network (BSEN) that integrates regularly adminstrated psychological tests
information into the encoding procedure of representing subject's restingstate
fMRI data for automatic classification tasks. BSEN is based on a 3D
convolutional autoencoder structure with contrastive loss jointly optimized
using behavior scores from MiniMental State Examination (MMSE) and Clinical
Dementia Rating (CDR). Our proposed classification framework of using BSEN
achieved an overall recognition accuracy of 59.44% (3-class classification: AD,
MCI and Healthy Control), and we further extracted the most discriminative
regions between healthy control (HC) and AD patients.</p>
  </details>
</details>
<details>
  <summary>83. <b>标题：A Survey on Evaluation Metrics for Synthetic Material Micro-Structure  Images from Generative Models</b></summary>
  <p><b>编号</b>：[247]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09727</p>
  <p><b>作者</b>：Devesh Shah (1),  Anirudh Suresh (2),  Alemayehu Admasu (1),  Devesh Upadhyay (1),  Kalyanmoy Deb (2) ((1) Ford Motor Company, (2) Michigan State University)</p>
  <p><b>备注</b>：Accepted in Neural Information Processing Systems (NeurIPS) 2022 Workshop on AI for Accelerated Materials Design (AI4Mat). Selected as spotlight paper for workshop</p>
  <p><b>关键词</b>：Fréchet Inception Distance, materials science research, emerging problem, research have evolved, synthetic micro-structure images</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The evaluation of synthetic micro-structure images is an emerging problem as
machine learning and materials science research have evolved together. Typical
state of the art methods in evaluating synthetic images from generative models
have relied on the Fréchet Inception Distance. However, this and other
similar methods, are limited in the materials domain due to both the unique
features that characterize physically accurate micro-structures and limited
dataset sizes. In this study we evaluate a variety of methods on scanning
electron microscope (SEM) images of graphene-reinforced polyurethane foams. The
primary objective of this paper is to report our findings with regards to the
shortcomings of existing methods so as to encourage the machine learning
community to consider enhancements in metrics for assessing quality of
synthetic images in the material science domain.</p>
  </details>
</details>
<details>
  <summary>84. <b>标题：Physics-informed neural networks for gravity currents reconstruction  from limited data</b></summary>
  <p><b>编号</b>：[248]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09715</p>
  <p><b>作者</b>：Mickaël Delcey,  Yoann Cheny,  Sébastien Kiesgen de Richter</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：present work investigates, unsteady gravity currents, physics-informed neural networks, present work, work investigates</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The present work investigates the use of physics-informed neural networks
(PINNs) for the 3D reconstruction of unsteady gravity currents from limited
data. In the PINN context, the flow fields are reconstructed by training a
neural network whose objective function penalizes the mismatch between the
network predictions and the observed data and embeds the underlying equations
using automatic differentiation. This study relies on a high-fidelity numerical
experiment of the canonical lock-exchange configuration. This allows us to
benchmark quantitatively the PINNs reconstruction capabilities on several
training databases that mimic state-of-the-art experimental measurement
techniques for density and velocity. Notably, spatially averaged density
measurements by light attenuation technique (LAT) are employed for the training
procedure. An optimal experimental setup for flow reconstruction by PINNs is
proposed according to two criteria : the implementation complexity and the
accuracy of the inferred fields.</p>
  </details>
</details>
<details>
  <summary>85. <b>标题：A Review of Deep Learning Techniques for Protein Function Prediction</b></summary>
  <p><b>编号</b>：[249]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09705</p>
  <p><b>作者</b>：Divyanshu Aggarwal,  Yasha Hasija</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：artificial intelligence methods, shown tremendous success, protein function classification, protein function, predicting protein function</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep Learning and big data have shown tremendous success in bioinformatics
and computational biology in recent years; artificial intelligence methods have
also significantly contributed in the task of protein function classification.
This review paper analyzes the recent developments in approaches for the task
of predicting protein function using deep learning. We explain the importance
of determining the protein function and why automating the following task is
crucial. Then, after reviewing the widely used deep learning techniques for
this task, we continue our review and highlight the emergence of the modern
State of The Art (SOTA) deep learning models which have achieved groundbreaking
results in the field of computer vision, natural language processing and
multi-modal learning in the last few years. We hope that this review will
provide a broad view of the current role and advances of deep learning in
biological sciences, especially in predicting protein function tasks and
encourage new researchers to contribute to this area.</p>
  </details>
</details>
<details>
  <summary>86. <b>标题：Thermodynamics of bidirectional associative memories</b></summary>
  <p><b>编号</b>：[250]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09694</p>
  <p><b>作者</b>：Adriano Barra,  Giovanni Catania,  Aurélien Decelle,  Beatriz Seoane</p>
  <p><b>备注</b>：22 pages, 10 figures</p>
  <p><b>关键词</b>：bidirectional associative memories, associative memories, paper we investigate, investigate the equilibrium, equilibrium properties</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper we investigate the equilibrium properties of bidirectional
associative memories (BAMs). Introduced by Kosko in 1988 as a generalization of
the Hopfield model to a bipartite structure, the simplest architecture is
defined by two layers of neurons, with synaptic connections only between units
of different layers: even without internal connections within each layer,
information storage and retrieval are still possible through the reverberation
of neural activities passing from one layer to another. We characterize the
computational capabilities of a stochastic extension of this model in the
thermodynamic limit, by applying rigorous techniques from statistical physics.
A detailed picture of the phase diagram at the replica symmetric level is
provided, both at finite temperature and in the noiseless regime. An analytical
and numerical inspection of the transition curves (namely critical lines
splitting the various modes of operation of the machine) is carried out as the
control parameters - noise, load and asymmetry between the two layer sizes -
are tuned. In particular, with a finite asymmetry between the two layers, it is
shown how the BAM can store information more efficiently than the Hopfield
model by requiring less parameters to encode a fixed number of patterns.
Comparisons are made with numerical simulations of neural dynamics. Finally, a
low-load analysis is carried out to explain the retrieval mechanism in the BAM
by analogy with two interacting Hopfield models. A potential equivalence with
two coupled Restricted Boltmzann Machines is also discussed.</p>
  </details>
</details>
<details>
  <summary>87. <b>标题：Validation Diagnostics for SBI algorithms based on Normalizing Flows</b></summary>
  <p><b>编号</b>：[253]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09602</p>
  <p><b>作者</b>：Julia Linhart (1,2),  Alexandre Gramfort (1),  Pedro L. C. Rodrigues (2) ((1) MIND - INRIA, (2) University of Paris-Saclay, (3) STATIFY - INRIA)</p>
  <p><b>备注</b>：7 pages, 2 figures, 1 appendix, to be published at "Machine Learning and the Physical Sciences" workshop (NeurIPS 2022)</p>
  <p><b>关键词</b>：deep generative models, recent trend, deep generative, high-dimensional data distributions, Normalizing Flows</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Building on the recent trend of new deep generative models known as
Normalizing Flows (NF), simulation-based inference (SBI) algorithms can now
efficiently accommodate arbitrary complex and high-dimensional data
distributions. The development of appropriate validation methods however has
fallen behind. Indeed, most of the existing metrics either require access to
the true posterior distribution, or fail to provide theoretical guarantees on
the consistency of the inferred approximation beyond the one-dimensional
setting. This work proposes easy to interpret validation diagnostics for
multi-dimensional conditional (posterior) density estimators based on NF. It
also offers theoretical guarantees based on results of local consistency. The
proposed workflow can be used to check, analyse and guarantee consistent
behavior of the estimator. The method is illustrated with a challenging example
that involves tightly coupled parameters in the context of computational
neuroscience. This work should help the design of better specified models or
drive the development of novel SBI-algorithms, hence allowing to build up trust
on their ability to address important questions in experimental science.</p>
  </details>
</details>
<details>
  <summary>88. <b>标题：Locating Hidden Exoplanets in ALMA Data Using Machine Learning</b></summary>
  <p><b>编号</b>：[258]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09541</p>
  <p><b>作者</b>：Jason Terry,  Cassandra Hall,  Sean Abreau,  Sergei Gleyzer</p>
  <p><b>备注</b>：12 pages, 9 figures, 3 tables. Accepted to ApJ</p>
  <p><b>关键词</b>：molecular line emission, Keplerian velocity, Exoplanets in protoplanetary, line emission, protoplanetary disks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Exoplanets in protoplanetary disks cause localized deviations from Keplerian
velocity in channel maps of molecular line emission. Current methods of
characterizing these deviations are time consuming, and there is no unified
standard approach. We demonstrate that machine learning can quickly and
accurately detect the presence of planets. We train our model on synthetic
images generated from simulations and apply it to real observations to identify
forming planets in real systems. Machine learning methods, based on computer
vision, are not only capable of correctly identifying the presence of one or
more planets, but they can also correctly constrain the location of those
planets.</p>
  </details>
</details>
<details>
  <summary>89. <b>标题：EmoDiff: Intensity Controllable Emotional Text-to-Speech with Soft-Label  Guidance</b></summary>
  <p><b>编号</b>：[263]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09496</p>
  <p><b>作者</b>：Yiwei Guo,  Chenpeng Du,  Xie Chen,  Kai Yu</p>
  <p><b>备注</b>：Submitted to ICASSP2023</p>
  <p><b>关键词</b>：controllable emotional TTS, intensity controllable emotional, generate high-quality speech, current neural, challenging task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Although current neural text-to-speech (TTS) models are able to generate
high-quality speech, intensity controllable emotional TTS is still a
challenging task. Most existing methods need external optimizations for
intensity calculation, leading to suboptimal results or degraded quality. In
this paper, we propose EmoDiff, a diffusion-based TTS model where emotion
intensity can be manipulated by a proposed soft-label guidance technique
derived from classifier guidance. Specifically, instead of being guided with a
one-hot vector for the specified emotion, EmoDiff is guided with a soft label
where the value of the specified emotion and \textit{Neutral} is set to
$\alpha$ and $1-\alpha$ respectively. The $\alpha$ here represents the emotion
intensity and can be chosen from 0 to 1. Our experiments show that EmoDiff can
precisely control the emotion intensity while maintaining high voice quality.
Moreover, diverse speech with specified emotion intensity can be generated by
sampling in the reverse denoising process.</p>
  </details>
</details>
<details>
  <summary>90. <b>标题：Parameterization of state duration in Hidden semi-Markov Models: an  application in electrocardiography</b></summary>
  <p><b>编号</b>：[265]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09478</p>
  <p><b>作者</b>：Adrián Pérez Herrero,  Paulo Félix Lamas,  Jesús María Rodríguez Presedo</p>
  <p><b>备注</b>：9 pages, 3 figures</p>
  <p><b>关键词</b>：time series, series classification based, work aims, aims at providing, based on learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This work aims at providing a new model for time series classification based
on learning from just one example. We assume that time series can be well
characterized as a parametric random process, a sort of Hidden semi-Markov
Model representing a sequence of regression models with variable duration. We
introduce a parametric stochastic model for time series pattern recognition and
provide a maximum-likelihood estimation of its parameters. Particularly, we are
interested in examining two different representations for state duration: i) a
discrete density distribution requiring an estimate for each possible duration;
and ii) a parametric family of continuous density functions, here the Gamma
distribution, with just two parameters to estimate. An application on heartbeat
classification reveals the main strengths and weaknesses of each alternative.</p>
  </details>
</details>
<details>
  <summary>91. <b>标题：RDRN: Recursively Defined Residual Network for Image Super-Resolution</b></summary>
  <p><b>编号</b>：[267]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09462</p>
  <p><b>作者</b>：Alexander Panaetov,  Karim Elhadji Daou,  Igor Samenko,  Evgeny Tetin,  Ilya Ivanov</p>
  <p><b>备注</b>：Accepted to ACCV 2022</p>
  <p><b>关键词</b>：Deep convolutional neural, obtained remarkable performance, convolutional neural networks, single image super-resolution, convolutional neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep convolutional neural networks (CNNs) have obtained remarkable
performance in single image super-resolution (SISR). However, very deep
networks can suffer from training difficulty and hardly achieve further
performance gain. There are two main trends to solve that problem: improving
the network architecture for better propagation of features through large
number of layers and designing an attention mechanism for selecting most
informative features. Recent SISR solutions propose advanced attention and
self-attention mechanisms. However, constructing a network to use an attention
block in the most efficient way is a challenging problem. To address this
issue, we propose a general recursively defined residual block (RDRB) for
better feature extraction and propagation through network layers. Based on RDRB
we designed recursively defined residual network (RDRN), a novel network
architecture which utilizes attention blocks efficiently. Extensive experiments
show that the proposed model achieves state-of-the-art results on several
popular super-resolution benchmarks and outperforms previous methods by up to
0.43 dB.</p>
  </details>
</details>
<details>
  <summary>92. <b>标题：Learning Mixtures of Markov Chains and MDPs</b></summary>
  <p><b>编号</b>：[270]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09403</p>
  <p><b>作者</b>：Chinmaya Kausik,  Kevin Tan,  Ambuj Tewari</p>
  <p><b>备注</b>：34 pages (13 page paper, 21 page appendix)</p>
  <p><b>关键词</b>：Markov decision processes, offline latent MDPs, Vempala and Wang, roots dating back, work of Vempala</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present an algorithm for use in learning mixtures of both Markov chains
(MCs) and Markov decision processes (offline latent MDPs) from trajectories,
with roots dating back to the work of Vempala and Wang. This amounts to
handling Markov chains with optional control input. The method is modular in
nature and amounts to (1) a subspace estimation step, (2) spectral clustering
of trajectories, and (3) a few iterations of the EM algorithm. We provide
end-to-end performance guarantees where we only explicitly require the number
of trajectories to be linear in states and the trajectory length to be linear
in mixing time. Experimental results suggest it outperforms both EM (95.4% on
average) and a previous method by Gupta et al. (54.1%), obtaining 100% permuted
accuracy on an 8x8 gridworld.</p>
  </details>
</details>
<details>
  <summary>93. <b>标题：Transfer learning for tensor Gaussian graphical models</b></summary>
  <p><b>编号</b>：[271]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09391</p>
  <p><b>作者</b>：Mingyang Ren,  Yaoming Zhen,  Junhui Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Gaussian graphical models, interpreting conditional independence, conditional independence structures, Tensor Gaussian graphical, Gaussian graphical</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Tensor Gaussian graphical models (GGMs), interpreting conditional
independence structures within tensor data, have important applications in
numerous areas. Yet, the available tensor data in one single study is often
limited due to high acquisition costs. Although relevant studies can provide
additional data, it remains an open question how to pool such heterogeneous
data. In this paper, we propose a transfer learning framework for tensor GGMs,
which takes full advantage of informative auxiliary domains even when
non-informative auxiliary domains are present, benefiting from the carefully
designed data-adaptive weights. Our theoretical analysis shows substantial
improvement of estimation errors and variable selection consistency on the
target domain under much relaxed conditions, by leveraging information from
auxiliary domains. Extensive numerical experiments are conducted on both
synthetic tensor graphs and a brain functional connectivity network data, which
demonstrates the satisfactory performance of the proposed method.</p>
  </details>
</details>
<details>
  <summary>94. <b>标题：Testing for context-dependent changes in neural encoding in naturalistic  experiments</b></summary>
  <p><b>编号</b>：[277]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09295</p>
  <p><b>作者</b>：Yenho Chen,  Carl W. Harris,  Xiaoyu Ma,  Zheng Li,  Francisco Pereira,  Charles Y.Zheng</p>
  <p><b>备注</b>：39 pages, 13 figures</p>
  <p><b>关键词</b>：detect context effects, longitudinal neural recording, neural recording data, propose a decoding-based, detect context</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a decoding-based approach to detect context effects on neural
codes in longitudinal neural recording data. The approach is agnostic to how
information is encoded in neural activity, and can control for a variety of
possible confounding factors present in the data. We demonstrate our approach
by determining whether it is possible to decode location encoding from
prefrontal cortex in the mouse and, further, testing whether the encoding
changes due to task engagement.</p>
  </details>
</details>
<details>
  <summary>95. <b>标题：Variable selection for nonlinear Cox regression model via deep learning</b></summary>
  <p><b>编号</b>：[278]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09287</p>
  <p><b>作者</b>：Kexuan Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Cox regression model, nonlinear Cox regression, Variable selection problem, nonlinear Cox model, Cox regression</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Variable selection problem for the nonlinear Cox regression model is
considered. In survival analysis, one main objective is to identify the
covariates that are associated with the risk of experiencing the event of
interest. The Cox proportional hazard model is being used extensively in
survival analysis in studying the relationship between survival times and
covariates, where the model assumes that the covariate has a log-linear effect
on the hazard function. However, this linearity assumption may not be satisfied
in practice. In order to extract a representative subset of features, various
variable selection approaches have been proposed for survival data under the
linear Cox model. However, there exists little literature on variable selection
for the nonlinear Cox model. To break this gap, we extend the recently
developed deep learning-based variable selection model LassoNet to survival
data. Simulations are provided to demonstrate the validity and effectiveness of
the proposed method. Finally, we apply the proposed methodology to analyze a
real data set on diffuse large B-cell lymphoma.</p>
  </details>
</details>
<details>
  <summary>96. <b>标题：A Generalized Latent Factor Model Approach to Mixed-data Matrix  Completion with Entrywise Consistency</b></summary>
  <p><b>编号</b>：[280]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09272</p>
  <p><b>作者</b>：Yunxiao Chen,  Xiaoou Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：partially observed matrix, machine learning methods, class of machine, machine learning, concerns the prediction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Matrix completion is a class of machine learning methods that concerns the
prediction of missing entries in a partially observed matrix. This paper
studies matrix completion for mixed data, i.e., data involving mixed types of
variables (e.g., continuous, binary, ordinal). We formulate it as a low-rank
matrix estimation problem under a general family of non-linear factor models
and then propose entrywise consistent estimators for estimating the low-rank
matrix. Tight probabilistic error bounds are derived for the proposed
estimators. The proposed methods are evaluated by simulation studies and
real-data applications for collaborative filtering and large-scale educational
assessment.</p>
  </details>
</details>
<details>
  <summary>97. <b>标题：Stimulation of soy seeds using environmentally friendly magnetic and  electric fields</b></summary>
  <p><b>编号</b>：[281]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09240</p>
  <p><b>作者</b>：Agata Dziwulska-Hunek,  Agnieszka Niemczynowicz,  Radosław A. Kycia,  Arkadiusz Matwijczuk,  Krzysztof Kornarzyński,  Joanna Stadnik,  Mariusz Szymanek</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：protein content, energy and capacity, mass of seedlings, study analyzes, fresh mass</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The study analyzes the impact of constant and alternating magnetic fields and
alternating electric fields on various growth parameters of soy plants: the
germination energy and capacity, plants emergence and number, the Yield(II) of
the fresh mass of seedlings, protein content, and photosynthetic parameters.
Four cultivars were used: MAVKA, MERLIN, VIOLETTA, and ANUSZKA. Moreover, the
advanced Machine Learning processing pipeline was proposed to distinguish the
impact of physical factors on photosynthetic parameters. It is possible to
distinguish exposition on different physical factors for the first three
cultivars; therefore, it indicates that the EM factors have some observable
effect on soy plants. Moreover, some influence of physical factors on growth
parameters was observed. The use of ELM (Electromagnetic) fields had a positive
impact on the germination rate in Merlin plants. The highest values were
recorded for the constant magnetic field (CMF) - Merlin, and the lowest for the
alternating electric field (AEF) - Violetta. An increase in terms of emergence
and number of plants after seed stimulation was observed for the Mavka
cultivar, except for the AEF treatment (number of plants after 30 days) (...)</p>
  </details>
</details>
<details>
  <summary>98. <b>标题：The non-overlapping statistical approximation to overlapping group lasso</b></summary>
  <p><b>编号</b>：[282]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09221</p>
  <p><b>作者</b>：Mingyu Qi,  Tianxi Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：overlapping group lasso, Group lasso, overlapping group, group lasso penalty, group lasso regularization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Group lasso is a commonly used regularization method in statistical learning
in which parameters are eliminated from the model according to predefined
groups. However, when the groups overlap, optimizing the group lasso penalized
objective can be time-consuming on large-scale problems because of the
non-separability induced by the overlapping groups. This bottleneck has
seriously limited the application of overlapping group lasso regularization in
many modern problems, such as gene pathway selection and graphical model
estimation. In this paper, we propose a separable penalty as an approximation
of the overlapping group lasso penalty. Thanks to the separability, the
computation of regularization based on our penalty is substantially faster than
that of the overlapping group lasso, especially for large-scale and
high-dimensional problems. We show that the penalty is the tightest separable
relaxation of the overlapping group lasso norm within the family of
$\ell_{q_1}/\ell_{q_2}$ norms. Moreover, we show that the estimator based on
the proposed separable penalty is statistically equivalent to the one based on
the overlapping group lasso penalty with respect to their error bounds and the
rate-optimal performance under the squared loss. We demonstrate the faster
computational time and statistical equivalence of our method compared with the
overlapping group lasso in simulation examples and a classification problem of
cancer tumors based on gene expression and multiple gene pathways.</p>
  </details>
</details>
<details>
  <summary>99. <b>标题：Sobolev Spaces, Kernels and Discrepancies over Hyperspheres</b></summary>
  <p><b>编号</b>：[283]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09196</p>
  <p><b>作者</b>：Simon Hubbert,  Emilio Porcu,  Chris. J. Oates,  Mark Girolami</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：hyperspherical context, work provides theoretical, theoretical foundations, Sobolev spaces, spaces</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This work provides theoretical foundations for kernel methods in the
hyperspherical context. Specifically, we characterise the native spaces
(reproducing kernel Hilbert spaces) and the Sobolev spaces associated with
kernels defined over hyperspheres. Our results have direct consequences for
kernel cubature, determining the rate of convergence of the worst case error,
and expanding the applicability of cubature algorithms based on Stein's method.
We first introduce a suitable characterisation on Sobolev spaces on the
$d$-dimensional hypersphere embedded in $(d+1)$-dimensional Euclidean spaces.
Our characterisation is based on the Fourier--Schoenberg sequences associated
with a given kernel. Such sequences are hard (if not impossible) to compute
analytically on $d$-dimensional spheres, but often feasible over Hilbert
spheres. We circumvent this problem by finding a projection operator that
allows to Fourier mapping from Hilbert into finite dimensional hyperspheres. We
illustrate our findings through some parametric families of kernels.</p>
  </details>
</details>
<details>
  <summary>100. <b>标题：An Empirical Analysis of the Advantages of Finite- v.s. Infinite-Width  Bayesian Neural Networks</b></summary>
  <p><b>编号</b>：[285]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09184</p>
  <p><b>作者</b>：Jiayu Yao,  Yaniv Yacoby,  Beau Coker,  Weiwei Pan,  Finale Doshi-Velez</p>
  <p><b>备注</b>：Accepted at ICBINB Workshop, NeurIPS 2022</p>
  <p><b>关键词</b>：Comparing Bayesian neural, Bayesian neural networks, Comparing Bayesian, Bayesian neural, properties change simultaneously</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Comparing Bayesian neural networks (BNNs) with different widths is
challenging because, as the width increases, multiple model properties change
simultaneously, and, inference in the finite-width case is intractable. In this
work, we empirically compare finite- and infinite-width BNNs, and provide
quantitative and qualitative explanations for their performance difference. We
find that when the model is mis-specified, increasing width can hurt BNN
performance. In these cases, we provide evidence that finite-width BNNs
generalize better partially due to the properties of their frequency spectrum
that allows them to adapt under model mismatch.</p>
  </details>
</details>
<details>
  <summary>101. <b>标题：Characterizing 4-string contact interaction using machine learning</b></summary>
  <p><b>编号</b>：[288]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09129</p>
  <p><b>作者</b>：Harold Erbin,  Atakan Hilmi Fırat</p>
  <p><b>备注</b>：28+10 pages, 13 figures, 6 tables</p>
  <p><b>关键词</b>：closed string field, string field theory, field theory, theory is characterized, characterized using machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The geometry of 4-string contact interaction of closed string field theory is
characterized using machine learning. We obtain Strebel quadratic differentials
on 4-punctured spheres as a neural network by performing unsupervised learning
with a custom-built loss function. This allows us to solve for local
coordinates and compute their associated mapping radii numerically. We also
train a neural network distinguishing vertex from Feynman region. As a check,
4-tachyon contact term in the tachyon potential is computed and a good
agreement with the results in the literature is observed. We argue that our
algorithm is manifestly independent of number of punctures and scaling it to
characterize the geometry of $n$-string contact interaction is feasible.</p>
  </details>
</details>
<h1>人工智能</h1>
<details>
  <summary>1. <b>标题：InstructPix2Pix: Learning to Follow Image Editing Instructions</b></summary>
  <p><b>编号</b>：[4]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09800</p>
  <p><b>作者</b>：Tim Brooks,  Aleksander Holynski,  Alexei A. Efros</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：propose a method, Stable Diffusion, model, images, instructions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a method for editing images from human instructions: given an
input image and a written instruction that tells the model what to do, our
model follows these instructions to edit the image. To obtain training data for
this problem, we combine the knowledge of two large pretrained models -- a
language model (GPT-3) and a text-to-image model (Stable Diffusion) -- to
generate a large dataset of image editing examples. Our conditional diffusion
model, InstructPix2Pix, is trained on our generated data, and generalizes to
real images and user-written instructions at inference time. Since it performs
edits in the forward pass and does not require per example fine-tuning or
inversion, our model edits images quickly, in a matter of seconds. We show
compelling editing results for a diverse collection of input images and written
instructions.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：ConStruct-VL: Data-Free Continual Structured VL Concepts Learning</b></summary>
  <p><b>编号</b>：[9]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09790</p>
  <p><b>作者</b>：James Seale Smith,  Paola Cascante-Bonilla,  Assaf Arbelle,  Donghyun Kim,  Rameswar Panda,  David Cox,  Diyi Yang,  Zsolt Kira,  Rogerio Feris,  Leonid Karlinsky</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：achieving competitive results, short text prompts, demonstrated remarkable capabilities, recognizing objects defined, large-scale pre-trained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, large-scale pre-trained Vision-and-Language (VL) foundation models
have demonstrated remarkable capabilities in many zero-shot downstream tasks,
achieving competitive results for recognizing objects defined by as little as
short text prompts. However, it has also been shown that VL models are still
brittle in Structured VL Concept (SVLC) reasoning, such as the ability to
recognize object attributes, states, and inter-object relations. This leads to
reasoning mistakes, which need to be corrected as they occur by teaching VL
models the missing SVLC skills; often this must be done using private data
where the issue was found, which naturally leads to a data-free continual (no
task-id) VL learning setting. In this work, we introduce the first Continual
Data-Free Structured VL Concepts Learning (ConStruct-VL) benchmark and show it
is challenging for many existing data-free CL strategies. We, therefore,
propose a data-free method comprised of a new approach of Adversarial
Pseudo-Replay (APR) which generates adversarial reminders of past tasks from
past task models. To use this method efficiently, we also propose a continual
parameter-efficient Layered-LoRA (LaLo) neural architecture allowing
no-memory-cost access to all past models at train time. We show this approach
outperforms all data-free methods by as much as ~7% while even matching some
levels of experience-replay (prohibitive for applications where data-privacy
must be preserved).</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：T-SEA: Transfer-based Self-Ensemble Attack on Object Detection</b></summary>
  <p><b>编号</b>：[17]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09773</p>
  <p><b>作者</b>：Hao Huang,  Ziyan Chen,  Huanran Chen,  Yongtao Wang,  Kevin Zhang</p>
  <p><b>备注</b>：10 pages, 5 figures</p>
  <p><b>关键词</b>：query-based black-box attacks, transfer-based black-box attacks, Compared to query-based, ensures their secrecy, transfer-based black-box</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Compared to query-based black-box attacks, transfer-based black-box attacks
do not require any information of the attacked models, which ensures their
secrecy. However, most existing transfer-based approaches rely on ensembling
multiple models to boost the attack transferability, which is time- and
resource-intensive, not to mention the difficulty of obtaining diverse models
on the same task. To address this limitation, in this work, we focus on the
single-model transfer-based black-box attack on object detection, utilizing
only one model to achieve a high-transferability adversarial attack on multiple
black-box detectors. Specifically, we first make observations on the patch
optimization process of the existing method and propose an enhanced attack
framework by slightly adjusting its training strategies. Then, we analogize
patch optimization with regular model optimization, proposing a series of
self-ensemble approaches on the input data, the attacked model, and the
adversarial patch to efficiently make use of the limited information and
prevent the patch from overfitting. The experimental results show that the
proposed framework can be applied with multiple classical base attack methods
(e.g., PGD and MIM) to greatly improve the black-box transferability of the
well-optimized patch on multiple mainstream detectors, meanwhile boosting
white-box performance. Our code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Learning to Counterfactually Explain Recommendations</b></summary>
  <p><b>编号</b>：[23]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09752</p>
  <p><b>作者</b>：Yuanshun Yao,  Chong Wang,  Hang Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Recommender system practitioners, facing increasing pressure, Recommender system, explain recommendations, system practitioners</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recommender system practitioners are facing increasing pressure to explain
recommendations. We explore how to explain recommendations using counterfactual
logic, i.e. "Had you not interacted with the following items before, it is
likely we would not recommend this item." Compared to traditional explanation
logic, counterfactual explanations are easier to understand and more
technically verifiable. The major challenge of generating such explanations is
the computational cost because it requires repeatedly retraining the models to
obtain the effect on a recommendation caused by removing user (interaction)
history. We propose a learning-based framework to generate counterfactual
explanations. The key idea is to train a surrogate model to learn the effect of
removing a subset of user history on the recommendation. To this end, we first
artificially simulate the counterfactual outcomes on the recommendation after
deleting subsets of history. Then we train surrogate models to learn the
mapping between a history deletion and the change in the recommendation caused
by the deletion. Finally, to generate an explanation, we find the history
subset predicted by the surrogate model that is most likely to remove the
recommendation. Through offline experiments and online user studies, we show
our method, compared to baselines, can generate explanations that are more
counterfactually valid and more satisfactory considered by users.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Heart Abnormality Detection from Heart Sound Signals using MFCC Feature  and Dual Stream Attention Based Network</b></summary>
  <p><b>编号</b>：[24]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09751</p>
  <p><b>作者</b>：Nayeeb Rashid,  Swapnil Saha,  Mohseu Rashid Subah,  Rizwan Ahmed Robin,  Syed Mortuza Hasan Fahim,  Shahed Ahmed,  Talha Ibn Mahmud</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：heart sound signal, heart sound, sound signal, heart condition plays, raw heart sound</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cardiovascular diseases are one of the leading cause of death in today's
world and early screening of heart condition plays a crucial role in preventing
them. The heart sound signal is one of the primary indicator of heart condition
and can be used to detect abnormality in the heart. The acquisition of heart
sound signal is non-invasive, cost effective and requires minimum equipment.
But currently the detection of heart abnormality from heart sound signal
depends largely on the expertise and experience of the physician. As such an
automatic detection system for heart abnormality detection from heart sound
signal can be a great asset for the people living in underdeveloped areas. In
this paper we propose a novel deep learning based dual stream network with
attention mechanism that uses both the raw heart sound signal and the MFCC
features to detect abnormality in heart condition of a patient. The deep neural
network has a convolutional stream that uses the raw heart sound signal and a
recurrent stream that uses the MFCC features of the signal. The features from
these two streams are merged together using a novel attention network and
passed through the classification network. The model is trained on the largest
publicly available dataset of PCG signal and achieves an accuracy of 87.11,
sensitivity of 82.41, specificty of 91.8 and a MACC of 87.12.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Extending Logic Explained Networks to Text Classification</b></summary>
  <p><b>编号</b>：[31]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09732</p>
  <p><b>作者</b>：Rishabh Jain,  Gabriele Ciravegna,  Pietro Barbiero,  Francesco Giannini,  Davide Buffelli,  Pietro Lio</p>
  <p><b>备注</b>：Accepted as short paper at the EMNLP 2022 conference</p>
  <p><b>关键词</b>：Logic Explained Networks, Explained Networks, neural models providing, models providing logic, Logic Explained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, Logic Explained Networks (LENs) have been proposed as
explainable-by-design neural models providing logic explanations for their
predictions. However, these models have only been applied to vision and tabular
data, and they mostly favour the generation of global explanations, while local
ones tend to be noisy and verbose. For these reasons, we propose LENp,
improving local explanations by perturbing input words, and we test it on text
classification. Our results show that (i) LENp provides better local
explanations than LIME in terms of sensitivity and faithfulness, and (ii) logic
explanations are more useful and user-friendly than feature scoring provided by
LIME as attested by a human survey.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Deep Reinforcement Learning for IRS Phase Shift Design in  Spatiotemporally Correlated Environments</b></summary>
  <p><b>编号</b>：[35]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09726</p>
  <p><b>作者</b>：Spilios Evmorfos,  Athina P. Petropulu,  H. Vincent Poor</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Intelligent Reflecting Surface, Input Single Output, Multiple Input Single, Reflecting Surface, Single Output</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The paper studies the problem of designing the Intelligent Reflecting Surface
(IRS) phase shifters for Multiple Input Single Output (MISO) communication
systems in spatiotemporally correlated channel environments, where the
destination can move within a confined area. The objective is to maximize the
expected sum of SNRs at the receiver over infinite time horizons. The problem
formulation gives rise to a Markov Decision Process (MDP). We propose a deep
actor-critic algorithm that accounts for channel correlations and destination
motion by constructing the state representation to include the current position
of the receiver and the phase shift values and receiver positions that
correspond to a window of previous time steps. The channel variability induces
high frequency components on the spectrum of the underlying value function. We
propose the preprocessing of the critic's input with a Fourier kernel which
enables stable value learning. Finally, we investigate the use of the
destination SNR as a component of the designed MDP state, which is common
practice in previous work. We provide empirical evidence that, when the
channels are spatiotemporally correlated, the inclusion of the SNR in the state
representation interacts with function approximation in ways that inhibit
convergence.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：SigT: An Efficient End-to-End MIMO-OFDM Receiver Framework Based on  Transformer</b></summary>
  <p><b>编号</b>：[44]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09712</p>
  <p><b>作者</b>：Ziyou Ren,  Nan Cheng,  Ruijin Sun,  Xiucheng Wang,  Ning Lu,  Wenchao Xu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：orthogonal frequency-division multiplexing, wireless communication systems, subsequent wireless communication, Multiple-input multiple-output, frequency-division multiplexing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multiple-input multiple-output and orthogonal frequency-division multiplexing
(MIMO-OFDM) are the key technologies in 4G and subsequent wireless
communication systems. Conventionally, the MIMO-OFDM receiver is performed by
multiple cascaded blocks with different functions and the algorithm in each
block is designed based on ideal assumptions of wireless channel distributions.
However, these assumptions may fail in practical complex wireless environments.
The deep learning (DL) method has the ability to capture key features from
complex and huge data. In this paper, a novel end-to-end MIMO-OFDM receiver
framework based on \textit{transformer}, named SigT, is proposed. By regarding
the signal received from each antenna as a token of the transformer, the
spatial correlation of different antennas can be learned and the critical
zero-shot problem can be mitigated. Furthermore, the proposed SigT framework
can work well without the inserted pilots, which improves the useful data
transmission efficiency. Experiment results show that SigT achieves much higher
performance in terms of signal recovery accuracy than benchmark methods, even
in a low SNR environment or with a small number of training samples. Code is
available at this https URL.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：EfficientTrain: Exploring Generalized Curriculum Learning for Training  Visual Backbones</b></summary>
  <p><b>编号</b>：[50]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09703</p>
  <p><b>作者</b>：Yulin Wang,  Yang Yue,  Rui Lu,  Tianjiao Liu,  Zhao Zhong,  Shiji Song,  Gao Huang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：costly training procedure, modern deep networks, superior performance, performance of modern, deep networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The superior performance of modern deep networks usually comes at the price
of a costly training procedure. In this paper, we present a novel curriculum
learning approach for the efficient training of visual backbones (e.g., vision
Transformers). The proposed method is inspired by the phenomenon that deep
networks mainly learn to recognize some 'easier-to-learn' discriminative
patterns within each example at earlier stages of training, e.g., the
lower-frequency components of images and the original information before data
augmentation. Driven by this observation, we propose a curriculum where the
model always leverages all the training data at each epoch, while the
curriculum starts with only exposing the 'easier-to-learn' patterns of each
example, and introduces gradually more difficult patterns. To implement this
idea, we 1) introduce a cropping operation in the Fourier spectrum of the
inputs, which enables the model to learn from only the lower-frequency
components efficiently, and 2) demonstrate that exposing the features of
original images amounts to adopting weaker data augmentation. Our resulting
algorithm, EfficientTrain, is simple, general, yet surprisingly effective. For
example, it reduces the training time of a wide variety of popular models
(e.g., ConvNeXts, DeiT, PVT, and Swin/CSWin Transformers) by more than
${1.5\times}$ on ImageNet-1K/22K without sacrificing the accuracy. It is
effective for self-supervised learning (i.e., MAE) as well. Code is available
at this https URL.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Verified Reversible Programming for Verified Lossless Compression</b></summary>
  <p><b>编号</b>：[61]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09676</p>
  <p><b>作者</b>：James Townsend,  Jan-Willem van de Meent</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Lossless compression implementations, compression implementations typically, Lossless compression, encoder, decoder</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Lossless compression implementations typically contain two programs, an
encoder and a decoder, which are required to be inverse to one another.
Maintaining consistency between two such programs during development requires
care, and incorrect data decoding can be costly and difficult to debug. We
observe that a significant class of compression methods, based on asymmetric
numeral systems (ANS), have shared structure between the encoder and decoder --
the decoder program is the 'reverse' of the encoder program -- allowing both to
be simultaneously specified by a single, reversible, 'codec' function. To
exploit this, we have implemented a small reversible language, embedded in
Agda, which we call 'Flipper'. Agda supports formal verification of program
properties, and the compiler for our reversible language (which is implemented
as an Agda macro), produces not just an encoder/decoder pair of functions but
also a proof that they are inverse to one another. Thus users of the language
get formal verification 'for free'. We give a small example use-case of Flipper
in this paper, and plan to publish a full compression implementation soon.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Features for the 0-1 knapsack problem based on inclusionwise maximal  solutions</b></summary>
  <p><b>编号</b>：[63]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09665</p>
  <p><b>作者</b>：Jorik Jooken,  Pieter Leyman,  Patrick De Causmaecker</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：knapsack problem instances, problem instances, knapsack problem, knapsack problem led, quickly solve large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Decades of research on the 0-1 knapsack problem led to very efficient
algorithms that are able to quickly solve large problem instances to
optimality. This prompted researchers to also investigate whether relatively
small problem instances exist that are hard for existing solvers and
investigate which features characterize their hardness. Previously the authors
proposed a new class of hard 0-1 knapsack problem instances and demonstrated
that the properties of so-called inclusionwise maximal solutions (IMSs) can be
important hardness indicators for this class. In the current paper, we
formulate several new computationally challenging problems related to the IMSs
of arbitrary 0-1 knapsack problem instances. Based on generalizations of
previous work and new structural results about IMSs, we formulate polynomial
and pseudopolynomial time algorithms for solving these problems. From this we
derive a set of 14 computationally expensive features, which we calculate for
two large datasets on a supercomputer in approximately 540 CPU-hours. We show
that the proposed features contain important information related to the
empirical hardness of a problem instance that was missing in earlier features
from the literature by training machine learning models that can accurately
predict the empirical hardness of a wide variety of 0-1 knapsack problem
instances. Using the instance space analysis methodology, we also show that
hard 0-1 knapsack problem instances are clustered together around a relatively
dense region of the instance space and several features behave differently in
the easy and hard parts of the instance space.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Influencer Detection with Dynamic Graph Neural Networks</b></summary>
  <p><b>编号</b>：[64]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09664</p>
  <p><b>作者</b>：Elena Tiukhova,  Emiliano Penaloza,  María Óskarsdóttir,  Hernan Garcia,  Alejandro Correa Bahnsen,  Bart Baesens,  Monique Snoeck,  Cristián Bravo</p>
  <p><b>备注</b>：Conference workshop camera-ready paper - accepted at NeurIPS TGL 2022. 8 pages, 4 figures</p>
  <p><b>关键词</b>：Leveraging network information, common practice, dynamic Graph Neural, Graph Neural, Leveraging network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Leveraging network information for prediction tasks has become a common
practice in many domains. Being an important part of targeted marketing,
influencer detection can potentially benefit from incorporating dynamic network
representation. In this work, we investigate different dynamic Graph Neural
Networks (GNNs) configurations for influencer detection and evaluate their
prediction performance using a unique corporate data set. We show that using
deep multi-head attention in GNN and encoding temporal attributes significantly
improves performance. Furthermore, our empirical evaluation illustrates that
capturing neighborhood representation is more beneficial that using network
centrality measures.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：A Spreader Ranking Algorithm for Extremely Low-budget Influence  Maximization in Social Networks using Community Bridge Nodes</b></summary>
  <p><b>编号</b>：[69]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09657</p>
  <p><b>作者</b>：Aaryan Gupta,  Inder Khatri,  Arjun Choudhry,  Pranav Chandhok,  Dinesh Kumar Vishwakarma,  Mukesh Prasad</p>
  <p><b>备注</b>：21 pages, 7 figures</p>
  <p><b>关键词</b>：gained significant popularity, social networking platforms, recent years, thoughts and opinions, masses like connecting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, social networking platforms have gained significant
popularity among the masses like connecting with people and propagating ones
thoughts and opinions. This has opened the door to user-specific advertisements
and recommendations on these platforms, bringing along a significant focus on
Influence Maximisation (IM) on social networks due to its wide applicability in
target advertising, viral marketing, and personalized recommendations. The aim
of IM is to identify certain nodes in the network which can help maximize the
spread of certain information through a diffusion cascade. While several works
have been proposed for IM, most were inefficient in exploiting community
structures to their full extent. In this work, we propose a community
structures-based approach, which employs a K-Shell algorithm in order to
generate a score for the connections between seed nodes and communities for
low-budget scenarios. Further, our approach employs entropy within communities
to ensure the proper spread of information within the communities. We choose
the Independent Cascade (IC) model to simulate information spread and evaluate
it on four evaluation metrics. We validate our proposed approach on eight
publicly available networks and find that it significantly outperforms the
baseline approaches on these metrics, while still being relatively efficient.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：HARDVS: Revisiting Human Activity Recognition with Dynamic Vision  Sensors</b></summary>
  <p><b>编号</b>：[72]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09648</p>
  <p><b>作者</b>：Xiao Wang,  Zongzhen Wu,  Bo Jiang,  Zhimin Bao,  Lin Zhu,  Guoqi Li,  Yaowei Wang,  Yonghong Tian</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large energy consumption, RGB cameras, fast motion, suffered from illumination, energy consumption</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The main streams of human activity recognition (HAR) algorithms are developed
based on RGB cameras which are suffered from illumination, fast motion,
privacy-preserving, and large energy consumption. Meanwhile, the biologically
inspired event cameras attracted great interest due to their unique features,
such as high dynamic range, dense temporal but sparse spatial resolution, low
latency, low power, etc. As it is a newly arising sensor, even there is no
realistic large-scale dataset for HAR. Considering its great practical value,
in this paper, we propose a large-scale benchmark dataset to bridge this gap,
termed HARDVS, which contains 300 categories and more than 100K event
sequences. We evaluate and report the performance of multiple popular HAR
algorithms, which provide extensive baselines for future works to compare. More
importantly, we propose a novel spatial-temporal feature learning and fusion
framework, termed ESTF, for event stream based human activity recognition. It
first projects the event streams into spatial and temporal embeddings using
StemNet, then, encodes and fuses the dual-view representations using
Transformer networks. Finally, the dual features are concatenated and fed into
a classification head for activity prediction. Extensive experiments on
multiple datasets fully validated the effectiveness of our model. Both the
dataset and source code will be released on
\url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：CPT-V: A Contrastive Approach to Post-Training Quantization of Vision  Transformers</b></summary>
  <p><b>编号</b>：[74]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09643</p>
  <p><b>作者</b>：Natalia Frumkin,  Dibakar Gope,  Diana Marculescu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：typically focused, focused on developing, developing a mixed, mixed precision scheme, quantization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>When considering post-training quantization, prior work has typically focused
on developing a mixed precision scheme or learning the best way to partition a
network for quantization. In our work, CPT-V, we look at a general way to
improve the accuracy of networks that have already been quantized, simply by
perturbing the quantization scales. Borrowing the idea of contrastive loss from
self-supervised learning, we find a robust way to jointly minimize a loss
function using just 1,000 calibration images. In order to determine the best
performing quantization scale, CPT-V contrasts the features of quantized and
full precision models in a self-supervised fashion.
Unlike traditional reconstruction-based loss functions, the use of a
contrastive loss function not only rewards similarity between the quantized and
full precision outputs but also helps in distinguishing the quantized output
from other outputs within a given batch. In addition, in contrast to prior
works, CPT-V proposes a block-wise evolutionary search to minimize a global
contrastive loss objective, allowing for accuracy improvement of existing
vision transformer (ViT) quantization schemes. For example, CPT-V improves the
top-1 accuracy of a fully quantized ViT-Base by 10.30%, 0.78%, and 0.15% for
3-bit, 4-bit, and 8-bit weight quantization levels. Extensive experiments on a
variety of other ViT architectures further demonstrate its robustness in
extreme quantization scenarios. Our code is available at <link>.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Predicting Human Mobility via Self-supervised Disentanglement Learning</b></summary>
  <p><b>编号</b>：[79]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09625</p>
  <p><b>作者</b>：Qiang Gao,  Jinyu Hong,  Xovee Xu,  Ping Kuang,  Fan Zhou,  Goce Trajcevski</p>
  <p><b>备注</b>：15 pages, 9 figures</p>
  <p><b>关键词</b>：Deep neural networks, recently achieved considerable, human behavioral patterns, achieved considerable improvements, learning human behavioral</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep neural networks have recently achieved considerable improvements in
learning human behavioral patterns and individual preferences from massive
spatial-temporal trajectories data. However, most of the existing research
concentrates on fusing different semantics underlying sequential trajectories
for mobility pattern learning which, in turn, yields a narrow perspective on
comprehending human intrinsic motions. In addition, the inherent sparsity and
under-explored heterogeneous collaborative items pertaining to human check-ins
hinder the potential exploitation of human diverse periodic regularities as
well as common interests. Motivated by recent advances in disentanglement
learning, in this study we propose a novel disentangled solution called SSDL
for tackling the next POI prediction problem. SSDL primarily seeks to
disentangle the potential time-invariant and time-varying factors into
different latent spaces from massive trajectories data, providing an
interpretable view to understand the intricate semantics underlying human
diverse mobility representations. To address the data sparsity issue, we
present two realistic trajectory augmentation approaches to enhance the
understanding of both the human intrinsic periodicity and constantly-changing
intents. In addition, we devise a POI-centric graph structure to explore
heterogeneous collaborative signals underlying historical check-ins. Extensive
experiments conducted on four real-world datasets demonstrate that our proposed
SSDL significantly outperforms the state-of-the-art approaches -- for example,
it yields up to 8.57% improvements on ACC@1.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Cross-Modal Adapter for Text-Video Retrieval</b></summary>
  <p><b>编号</b>：[80]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09623</p>
  <p><b>作者</b>：Haojun Jiang,  Jianke Zhang,  Rui Huang,  Chunjiang Ge,  Zanlin Ni,  Jiwen Lu,  Jie Zhou,  Shiji Song,  Gao Huang</p>
  <p><b>备注</b>：Tech Report</p>
  <p><b>关键词</b>：important multi-modal learning, text query, multi-modal learning task, relevant video, learning task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Text-video retrieval is an important multi-modal learning task, where the
goal is to retrieve the most relevant video for a given text query. Recently,
pre-trained models, e.g., CLIP, show great potential on this task. However, as
pre-trained models are scaling up, fully fine-tuning them on text-video
retrieval datasets has a high risk of overfitting. Moreover, in practice, it
would be costly to train and store a large model for each task. To overcome the
above issues, we present a novel $\textbf{Cross-Modal Adapter}$ for
parameter-efficient fine-tuning. Inspired by adapter-based methods, we adjust
the pre-trained model with a few parameterization layers. However, there are
two notable differences. First, our method is designed for the multi-modal
domain. Secondly, it allows early cross-modal interactions between CLIP's two
encoders. Although surprisingly simple, our approach has three notable
benefits: (1) reduces $\textbf{99.6}\%$ of fine-tuned parameters, and
alleviates the problem of overfitting, (2) saves approximately 30% of training
time, and (3) allows all the pre-trained parameters to be fixed, enabling the
pre-trained model to be shared across datasets. Extensive experiments
demonstrate that, without bells and whistles, it achieves superior or
comparable performance compared to fully fine-tuned methods on MSR-VTT, MSVD,
VATEX, ActivityNet, and DiDeMo datasets. The code will be available at
\url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：AlphaSnake: Policy Iteration on a Nondeterministic NP-hard Markov  Decision Process</b></summary>
  <p><b>编号</b>：[81]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09622</p>
  <p><b>作者</b>：Kevin Du,  Ian Gemp,  Yi Wu,  Yingying Wu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：approach well-known NP-hard, well-known NP-hard combinatorial, Hamiltonian cycle problems, NP-hard combinatorial problems, approach well-known</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reinforcement learning has recently been used to approach well-known NP-hard
combinatorial problems in graph theory. Among these problems, Hamiltonian cycle
problems are exceptionally difficult to analyze, even when restricted to
individual instances of structurally complex graphs. In this paper, we use
Monte Carlo Tree Search (MCTS), the search algorithm behind many
state-of-the-art reinforcement learning algorithms such as AlphaZero, to create
autonomous agents that learn to play the game of Snake, a game centered on
properties of Hamiltonian cycles on grid graphs. The game of Snake can be
formulated as a single-player discounted Markov Decision Process (MDP) where
the agent must behave optimally in a stochastic environment. Determining the
optimal policy for Snake, defined as the policy that maximizes the probability
of winning - or win rate - with higher priority and minimizes the expected
number of time steps to win with lower priority, is conjectured to be NP-hard.
Performance-wise, compared to prior work in the Snake game, our algorithm is
the first to achieve a win rate over $0.5$ (a uniform random policy achieves a
win rate $< 2.57 \times 10^{-15}$), demonstrating the versatility of AlphaZero
in approaching NP-hard environments.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：A Reinforcement Learning Approach for Process Parameter Optimization in  Additive Manufacturing</b></summary>
  <p><b>编号</b>：[104]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09545</p>
  <p><b>作者</b>：Susheel Dharmadhikari,  Nandana Menon,  Amrita Basak</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：metal additive manufacturing, control microstructure, additive manufacturing, ensure repeatability, minimize defects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Process optimization for metal additive manufacturing (AM) is crucial to
ensure repeatability, control microstructure, and minimize defects. Despite
efforts to address this via the traditional design of experiments and
statistical process mapping, there is limited insight on an on-the-fly
optimization framework that can be integrated into a metal AM system.
Additionally, most of these methods, being data-intensive, cannot be supported
by a metal AM alloy or system due to budget restrictions. To tackle this issue,
the article introduces a Reinforcement Learning (RL) methodology transformed
into an optimization problem in the realm of metal AM. An off-policy RL
framework based on Q-learning is proposed to find optimal laser power ($P$) -
scan velocity ($v$) combinations with the objective of maintaining steady-state
melt pool depth. For this, an experimentally validated Eagar-Tsai formulation
is used to emulate the Laser-Directed Energy Deposition environment, where the
laser operates as the agent across the $P-v$ space such that it maximizes
rewards for a melt pool depth closer to the optimum. The culmination of the
training process yields a Q-table where the state ($P,v$) with the highest
Q-value corresponds to the optimized process parameter. The resultant melt pool
depths and the mapping of Q-values to the $P-v$ space show congruence with
experimental observations. The framework, therefore, provides a model-free
approach to learning without any prior.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Ignore Previous Prompt: Attack Techniques For Language Models</b></summary>
  <p><b>编号</b>：[109]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09527</p>
  <p><b>作者</b>：Fábio Perez,  Ian Ribeiro</p>
  <p><b>备注</b>：ML Safety Workshop NeurIPS 2022</p>
  <p><b>关键词</b>：large-scale customer-facing applications, Transformer-based large language, natural language tasks, Transformer-based large, provide a powerful</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transformer-based large language models (LLMs) provide a powerful foundation
for natural language tasks in large-scale customer-facing applications.
However, studies that explore their vulnerabilities emerging from malicious
user interaction are scarce. By proposing PromptInject, a prosaic alignment
framework for mask-based iterative adversarial prompt composition, we examine
how GPT-3, the most widely deployed language model in production, can be easily
misaligned by simple handcrafted inputs. In particular, we investigate two
types of attacks -- goal hijacking and prompt leaking -- and demonstrate that
even low-aptitude, but sufficiently ill-intentioned agents, can easily exploit
GPT-3's stochastic nature, creating long-tail risks. The code for PromptInject
is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Back-Translation-Style Data Augmentation for Mandarin Chinese Polyphone  Disambiguation</b></summary>
  <p><b>编号</b>：[120]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09495</p>
  <p><b>作者</b>：Chunyu Qiang,  Peng Yang,  Hao Che,  Jinba Xiao,  Xiaorui Wang,  Zhongyuan Wang</p>
  <p><b>备注</b>：Published to APSIPA ASC 2022</p>
  <p><b>关键词</b>：Chinese polyphone disambiguation, mandarin Chinese polyphone, polyphone disambiguation, Mandarin Chinese, plays an important</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Conversion of Chinese Grapheme-to-Phoneme (G2P) plays an important role in
Mandarin Chinese Text-To-Speech (TTS) systems, where one of the biggest
challenges is the task of polyphone disambiguation. Most of the previous
polyphone disambiguation models are trained on manually annotated datasets, and
publicly available datasets for polyphone disambiguation are scarce. In this
paper we propose a simple back-translation-style data augmentation method for
mandarin Chinese polyphone disambiguation, utilizing a large amount of
unlabeled text data. Inspired by the back-translation technique proposed in the
field of machine translation, we build a Grapheme-to-Phoneme (G2P) model to
predict the pronunciation of polyphonic character, and a Phoneme-to-Grapheme
(P2G) model to predict pronunciation into text. Meanwhile, a window-based
matching strategy and a multi-model scoring strategy are proposed to judge the
correctness of the pseudo-label. We design a data balance strategy to improve
the accuracy of some typical polyphonic characters in the training set with
imbalanced distribution or data scarcity. The experimental result shows the
effectiveness of the proposed back-translation-style data augmentation method.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Feedback is Needed for Retakes: An Explainable Poor Image Notification  Framework for the Visually Impaired</b></summary>
  <p><b>编号</b>：[141]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09427</p>
  <p><b>作者</b>：Kazuya Ohata,  Shunsuke Kitada,  Hitoshi Iyatomi</p>
  <p><b>备注</b>：6 pages, 4 figures. Accepted at 2022 IEEE 19th International Conference on Smart Communities: Improving Quality of Life Using ICT, IoT and AI (HONET) as a full paper</p>
  <p><b>关键词</b>：propose a simple, simple yet effective, quality, image, effective image captioning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a simple yet effective image captioning framework that can
determine the quality of an image and notify the user of the reasons for any
flaws in the image. Our framework first determines the quality of images and
then generates captions using only those images that are determined to be of
high quality. The user is notified by the flaws feature to retake if image
quality is low, and this cycle is repeated until the input image is deemed to
be of high quality. As a component of the framework, we trained and evaluated a
low-quality image detection model that simultaneously learns difficulty in
recognizing images and individual flaws, and we demonstrated that our proposal
can explain the reasons for flaws with a sufficient score. We also evaluated a
dataset with low-quality images removed by our framework and found improved
values for all four common metrics (e.g., BLEU-4, METEOR, ROUGE-L, CIDEr),
confirming an improvement in general-purpose image captioning capability. Our
framework would assist the visually impaired, who have difficulty judging image
quality.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Personalized Federated Learning for Multi-task Fault Diagnosis of  Rotating Machinery</b></summary>
  <p><b>编号</b>：[150]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09406</p>
  <p><b>作者</b>：Sheng Guo,  Zengxiang Li,  Hui Liu,  Shubao Zhao,  Cheng Hao Jin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Intelligent fault diagnosis, personalized federated learning, essential to safe, safe operation, federated learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Intelligent fault diagnosis is essential to safe operation of machinery.
However, due to scarce fault samples and data heterogeneity in field machinery,
deep learning based diagnosis methods are prone to over-fitting with poor
generalization ability. To solve the problem, this paper proposes a
personalized federated learning framework, enabling multi-task fault diagnosis
method across multiple factories in a privacypreserving manner. Firstly,
rotating machines from different factories with similar vibration feature data
are categorized into machine groups using a federated clustering method. Then,
a multi-task deep learning model based on convolutional neural network is
constructed to diagnose the multiple faults of machinery with heterogeneous
information fusion. Finally, a personalized federated learning framework is
proposed to solve data heterogeneity across different machines using adaptive
hierarchical aggregation strategy. The case study on collected data from real
machines verifies the effectiveness of the proposed framework. The result shows
that the diagnosis accuracy could be improved significantly using the proposed
personalized federated learning, especially for those machines with scarce
fault samples.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Data Dimension Reduction makes ML Algorithms efficient</b></summary>
  <p><b>编号</b>：[156]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09392</p>
  <p><b>作者</b>：Wisal Khan,  Muhammad Turab,  Waqas Ahmad,  Syed Hasnat Ahmad,  Kelash Kumar,  Bin Luo</p>
  <p><b>备注</b>：Our paper is accepted at International Conference On Emerging Technologies In Electronics, Computing And Communication (ICETECC) 2022</p>
  <p><b>关键词</b>：Principal Component Analysis, Principal Component, Component Analysis, Random Projections, Data dimension reduction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Data dimension reduction (DDR) is all about mapping data from high dimensions
to low dimensions, various techniques of DDR are being used for image dimension
reduction like Random Projections, Principal Component Analysis (PCA), the
Variance approach, LSA-Transform, the Combined and Direct approaches, and the
New Random Approach. Auto-encoders (AE) are used to learn end-to-end mapping.
In this paper, we demonstrate that pre-processing not only speeds up the
algorithms but also improves accuracy in both supervised and unsupervised
learning. In pre-processing of DDR, first PCA based DDR is used for supervised
learning, then we explore AE based DDR for unsupervised learning. In PCA based
DDR, we first compare supervised learning algorithms accuracy and time before
and after applying PCA. Similarly, in AE based DDR, we compare unsupervised
learning algorithm accuracy and time before and after AE representation
learning. Supervised learning algorithms including support-vector machines
(SVM), Decision Tree with GINI index, Decision Tree with entropy and Stochastic
Gradient Descent classifier (SGDC) and unsupervised learning algorithm
including K-means clustering, are used for classification purpose. We used two
datasets MNIST and FashionMNIST Our experiment shows that there is massive
improvement in accuracy and time reduction after pre-processing in both
supervised and unsupervised learning.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Data-Efficient Autoregressive Document Retrieval for Fact Verification</b></summary>
  <p><b>编号</b>：[158]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09388</p>
  <p><b>作者</b>：James Thorne</p>
  <p><b>备注</b>：To appear at SustaiNLP@EMNLP 2022. Code is available: this https URL</p>
  <p><b>关键词</b>：knowledge-intensive natural language, natural language processing, language processing task, processing task formulations, question answering</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Document retrieval is a core component of many knowledge-intensive natural
language processing task formulations such as fact verification and question
answering. Sources of textual knowledge, such as Wikipedia articles, condition
the generation of answers from the models. Recent advances in retrieval use
sequence-to-sequence models to incrementally predict the title of the
appropriate Wikipedia page given a query. However, this method requires
supervision in the form of human annotation to label which Wikipedia pages
contain appropriate context. This paper introduces a distant-supervision method
that does not require any annotation to train autoregressive retrievers that
attain competitive R-Precision and Recall in a zero-shot setting. Furthermore
we show that with task-specific supervised fine-tuning, autoregressive
retrieval performance for two Wikipedia-based fact verification tasks can
approach or even exceed full supervision using less than $1/4$ of the annotated
data indicating possible directions for data-efficient autoregressive
retrieval.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：ComMU: Dataset for Combinatorial Music Generation</b></summary>
  <p><b>编号</b>：[160]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09385</p>
  <p><b>作者</b>：Lee Hyun,  Taehyun Kim,  Hyolim Kang,  Minjoo Ki,  Hyeonchan Hwang,  Kwanho Park,  Sharang Han,  Seon Joo Kim</p>
  <p><b>备注</b>：19 pages, 12 figures</p>
  <p><b>关键词</b>：combinatorial music generation, music, action games, Commercial adoption, desired context</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Commercial adoption of automatic music composition requires the capability of
generating diverse and high-quality music suitable for the desired context
(e.g., music for romantic movies, action games, restaurants, etc.). In this
paper, we introduce combinatorial music generation, a new task to create
varying background music based on given conditions. Combinatorial music
generation creates short samples of music with rich musical metadata, and
combines them to produce a complete music. In addition, we introduce ComMU, the
first symbolic music dataset consisting of short music samples and their
corresponding 12 musical metadata for combinatorial music generation. Notable
properties of ComMU are that (1) dataset is manually constructed by
professional composers with an objective guideline that induces regularity, and
(2) it has 12 musical metadata that embraces composers' intentions. Our results
show that we can generate diverse high-quality music only with metadata, and
that our unique metadata such as track-role and extended chord quality improves
the capacity of the automatic composition. We highly recommend watching our
video before reading the paper (this https URL).</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：I see you: A Vehicle-Pedestrian Interaction Dataset from Traffic  Surveillance Cameras</b></summary>
  <p><b>编号</b>：[182]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09342</p>
  <p><b>作者</b>：Hanan Quispe,  Jorshinno Sumire,  Patricia Condori,  Edwin Alvarez,  Harley Vera</p>
  <p><b>备注</b>：paper accepted at LXAI workshop at NeurIPS 2022, github repository this https URL</p>
  <p><b>关键词</b>：autonomous vehicles arises, urban traffic scenarios, pedestrian slows, development of autonomous, arises new challenges</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The development of autonomous vehicles arises new challenges in urban traffic
scenarios where vehicle-pedestrian interactions are frequent e.g. vehicle
yields to pedestrians, pedestrian slows down due approaching to the vehicle.
Over the last years, several datasets have been developed to model these
interactions. However, available datasets do not cover near-accident scenarios
that our dataset covers. We introduce I see you, a new vehicle-pedestrian
interaction dataset that tackles the lack of trajectory data in near-accident
scenarios using YOLOv5 and camera calibration methods. I see you consist of 170
near-accident occurrences in seven intersections in Cusco-Peru. This new
dataset and pipeline code are available on Github.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：ACon$^2$: Adaptive Conformal Consensus for Provable Blockchain Oracles</b></summary>
  <p><b>编号</b>：[188]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09330</p>
  <p><b>作者</b>：Sangdon Park,  Osbert Bastani,  Taesoo Kim</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：distributed ledger systems, block state consistency, allowing deterministic operations, smart contracts, oracle smart contract</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Blockchains with smart contracts are distributed ledger systems which achieve
block state consistency among distributed nodes by only allowing deterministic
operations of smart contracts. However, the power of smart contracts is enabled
by interacting with stochastic off-chain data, which in turn opens the
possibility to undermine the block state consistency. To address this issue, an
oracle smart contract is used to provide a single consistent source of external
data; but, simultaneously this introduces a single point of failure, which is
called the oracle problem. To address the oracle problem, we propose an
adaptive conformal consensus (ACon$^2$) algorithm, which derives consensus from
multiple oracle contracts via the recent advance in online uncertainty
quantification learning. In particular, the proposed algorithm returns a
consensus set, which quantifies the uncertainty of data and achieves a desired
correctness guarantee in the presence of Byzantine adversaries and distribution
shift. We demonstrate the efficacy of the proposed algorithm on two price
datasets and an Ethereum case study. In particular, the Solidity implementation
of the proposed algorithm shows the practicality of the proposed algorithm,
implying that online machine learning algorithms are applicable to address
issues in blockchains.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Reflect, Not Reflex: Inference-Based Common Ground Improves Dialogue  Response Quality</b></summary>
  <p><b>编号</b>：[206]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09267</p>
  <p><b>作者</b>：Pei Zhou,  Hyundong Cho,  Pegah Jandaghi,  Dong-Ho Lee,  Bill Yuchen Lin,  Jay Pujara,  Xiang Ren</p>
  <p><b>备注</b>：Accepted at EMNLP-2022. 19 pages, 17 figures, 4 tables</p>
  <p><b>关键词</b>：Human communication relies, Human communication, communication relies, common ground, Reflect</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Human communication relies on common ground (CG), the mutual knowledge and
beliefs shared by participants, to produce coherent and interesting
conversations. In this paper, we demonstrate that current response generation
(RG) models produce generic and dull responses in dialogues because they act
reflexively, failing to explicitly model CG, both due to the lack of CG in
training data and the standard RG training procedure. We introduce Reflect, a
dataset that annotates dialogues with explicit CG (materialized as inferences
approximating shared knowledge and beliefs) and solicits 9k diverse
human-generated responses each following one common ground. Using Reflect, we
showcase the limitations of current dialogue data and RG models: less than half
of the responses in current data are rated as high quality (sensible, specific,
and interesting) and models trained using this data have even lower quality,
while most Reflect responses are judged high quality. Next, we analyze whether
CG can help models produce better-quality responses by using Reflect CG to
guide RG models. Surprisingly, we find that simply prompting GPT3 to "think"
about CG generates 30% more quality responses, showing promising benefits to
integrating CG into the RG process.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：The Missing Indicator Method: From Low to High Dimensions</b></summary>
  <p><b>编号</b>：[209]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09259</p>
  <p><b>作者</b>：Mike Van Ness,  Tomas M. Bosschieter,  Roberto Halpin-Gregorio,  Madeleine Udell</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：applied data science, social sciences, Missing, natural sciences, MIM</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Missing data is common in applied data science, particularly for tabular data
sets found in healthcare, social sciences, and natural sciences. Most
supervised learning methods work only on complete data, thus requiring
preprocessing, such as missing value imputation, to work on incomplete data
sets. However, imputation discards potentially useful information encoded by
the pattern of missing values. For data sets with informative missing patterns,
the Missing Indicator Method (MIM), which adds indicator variables to indicate
the missing pattern, can be used in conjunction with imputation to improve
model performance. We show experimentally that MIM improves performance for
informative missing values, and we prove that MIM does not hurt linear models
asymptotically for uninformative missing values. Nonetheless, MIM can increase
variance if many of the added indicators are uninformative, causing harm
particularly for high-dimensional data sets. To address this issue, we
introduce Selective MIM (SMIM), a method that adds missing indicators only for
features that have informative missing patterns. We show empirically that SMIM
performs at least as well as MIM across a range of experimental settings, and
improves MIM for high-dimensional data.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：A Graph-Based Context-Aware Model to Understand Online Conversations</b></summary>
  <p><b>编号</b>：[222]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09207</p>
  <p><b>作者</b>：Vibhor Agarwal,  Anthony P. Young,  Sagar Joglekar,  Nishanth Sastry</p>
  <p><b>备注</b>：25 pages, 9 figures. arXiv admin note: text overlap with arXiv:2202.08175</p>
  <p><b>关键词</b>：participatory engagement, engagement between users, public discussion, comments, Online forums</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Online forums that allow for participatory engagement between users have been
transformative for the public discussion of many important issues. However,
such conversations can sometimes escalate into full-blown exchanges of hate and
misinformation. Existing approaches in natural language processing (NLP), such
as deep learning models for classification tasks, use as inputs only a single
comment or a pair of comments depending upon whether the task concerns the
inference of properties of the individual comments or the replies between pairs
of comments, respectively. But in online conversations, comments and replies
may be based on external context beyond the immediately relevant information
that is input to the model. Therefore, being aware of the conversations'
surrounding contexts should improve the model's performance for the inference
task at hand.
We propose GraphNLI, a novel graph-based deep learning architecture that uses
graph walks to incorporate the wider context of a conversation in a principled
manner. Specifically, a graph walk starts from a given comment and samples
"nearby" comments in the same or parallel conversation threads, which results
in additional embeddings that are aggregated together with the initial
comment's embedding. We then use these enriched embeddings for downstream NLP
prediction tasks that are important for online conversations. We evaluate
GraphNLI on two such tasks - polarity prediction and misogynistic hate speech
detection - and found that our model consistently outperforms all relevant
baselines for both tasks. Specifically, GraphNLI with a biased root-seeking
random walk performs with a macro-F1 score of 3 and 6 percentage points better
than the best-performing BERT-based baselines for the polarity prediction and
hate speech detection tasks, respectively.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：CASPR: Customer Activity Sequence-based Prediction and Representation</b></summary>
  <p><b>编号</b>：[229]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09174</p>
  <p><b>作者</b>：Pin-Jung Chen,  Sahil Bhatnagar,  Damian Konrad Kowalczyk,  Mayank Shrivastava</p>
  <p><b>备注</b>：Presented at the Table Representation Learning Workshop, NeurIPS 2022, New Orleans</p>
  <p><b>关键词</b>：fraudulent account detection, Tasks critical, fraudulent account, lifetime value estimation, account detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Tasks critical to enterprise profitability, such as customer churn
prediction, fraudulent account detection or customer lifetime value estimation,
are often tackled by models trained on features engineered from customer data
in tabular format. Application-specific feature engineering adds development,
operationalization and maintenance costs over time. Recent advances in
representation learning present an opportunity to simplify and generalize
feature engineering across applications. When applying these advancements to
tabular data researchers deal with data heterogeneity, variations in customer
engagement history or the sheer volume of enterprise datasets. In this paper,
we propose a novel approach to encode tabular data containing customer
transactions, purchase history and other interactions into a generic
representation of a customer's association with the business. We then evaluate
these embeddings as features to train multiple models spanning a variety of
applications. CASPR, Customer Activity Sequence-based Prediction and
Representation, applies Transformer architecture to encode activity sequences
to improve model performance and avoid bespoke feature engineering across
applications. Our experiments at scale validate CASPR for both small \& large
enterprise applications.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Deep Emotion Recognition in Textual Conversations: A Survey</b></summary>
  <p><b>编号</b>：[230]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09172</p>
  <p><b>作者</b>：Patrícia Pereira,  Helena Moniz,  Joao Paulo Carvalho</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Recognition in Conversations, implementation scenarios present, Emotion Recognition, ERC, tremendous advancement</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While Emotion Recognition in Conversations (ERC) has seen a tremendous
advancement in the last few years, new applications and implementation
scenarios present novel challenges and opportunities. These range from
leveraging the conversational context, speaker and emotion dynamics modelling,
to interpreting common sense expressions, informal language and sarcasm,
addressing challenges of real time ERC and recognizing emotion causes. This
survey starts by introducing ERC, elaborating on the challenges and
opportunities pertaining to this task. It proceeds with a description of the
main emotion taxonomies and methods to deal with subjectivity in annotations.
It then describes Deep Learning methods relevant for ERC, word embeddings, and
elaborates on the use of performance metrics for the task and methods to deal
with the typically unbalanced ERC datasets. This is followed by a description
and benchmark of key ERC works along with comprehensive tables comparing
several works regarding their methods and performance across different
datasets. The survey highlights the advantage of leveraging techniques to
address unbalanced data, the exploration of mixed emotions and the benefits of
incorporating annotation subjectivity in the learning phase.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Engineering Monosemanticity in Toy Models</b></summary>
  <p><b>编号</b>：[232]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09169</p>
  <p><b>作者</b>：Adam S. Jermyn,  Nicholas Schiefer,  Evan Hubinger</p>
  <p><b>备注</b>：31 pages, 26 figures</p>
  <p><b>关键词</b>：individual neurons correspond, neural networks, correspond to natural, monosemantic, models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In some neural networks, individual neurons correspond to natural
``features'' in the input. Such \emph{monosemantic} neurons are of great help
in interpretability studies, as they can be cleanly understood. In this work we
report preliminary attempts to engineer monosemanticity in toy models. We find
that models can be made more monosemantic without increasing the loss by just
changing which local minimum the training process finds. More monosemantic loss
minima have moderate negative biases, and we are able to use this fact to
engineer highly monosemantic models. We are able to mechanistically interpret
these models, including the residual polysemantic neurons, and uncover a simple
yet surprising algorithm. Finally, we find that providing models with more
neurons per layer makes the models more monosemantic, albeit at increased
computational cost. These findings point to a number of new questions and
avenues for engineering monosemanticity, which we intend to study these in
future work.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Unified Question Answering in Slovene</b></summary>
  <p><b>编号</b>：[235]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09159</p>
  <p><b>作者</b>：Katja Logar,  Marko Robnik-Šikonja</p>
  <p><b>备注</b>：4 pages,published in Proceedings of the 25th International Multiconference INFORMATION SOCIETY - IS 2012, Volume A -Slovenian Conference on Artificial Intelligence SCAI 2022, Ljubljana, 2022, pp. 23-26</p>
  <p><b>关键词</b>：challenging tasks, language understanding, English, less-resourced Slovene language, Slovene</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Question answering is one of the most challenging tasks in language
understanding. Most approaches are developed for English, while less-resourced
languages are much less researched. We adapt a successful English
question-answering approach, called UnifiedQA, to the less-resourced Slovene
language. Our adaptation uses the encoder-decoder transformer SloT5 and mT5
models to handle four question-answering formats: yes/no, multiple-choice,
abstractive, and extractive. We use existing Slovene adaptations of four
datasets, and machine translate the MCTest dataset. We show that a general
model can answer questions in different formats at least as well as specialized
models. The results are further improved using cross-lingual transfer from
English. While we produce state-of-the-art results for Slovene, the performance
still lags behind English.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Learnable Graph Convolutional Network and Feature Fusion for Multi-view  Learning</b></summary>
  <p><b>编号</b>：[237]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09155</p>
  <p><b>作者</b>：Zhaoliang Chen,  Lele Fu,  Jie Yao,  Wenzhong Guo,  Claudia Plant,  Shiping Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：data depicting objectives, multi-view data depicting, practical applications, Learnable Graph Convolutional, graph convolutional network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In practical applications, multi-view data depicting objectives from assorted
perspectives can facilitate the accuracy increase of learning algorithms.
However, given multi-view data, there is limited work for learning
discriminative node relationships and graph information simultaneously via
graph convolutional network that has drawn the attention from considerable
researchers in recent years. Most of existing methods only consider the
weighted sum of adjacency matrices, yet a joint neural network of both feature
and graph fusion is still under-explored. To cope with these issues, this paper
proposes a joint deep learning framework called Learnable Graph Convolutional
Network and Feature Fusion (LGCN-FF), consisting of two stages: feature fusion
network and learnable graph convolutional network. The former aims to learn an
underlying feature representation from heterogeneous views, while the latter
explores a more discriminative graph fusion via learnable weights and a
parametric activation function dubbed Differentiable Shrinkage Activation (DSA)
function. The proposed LGCN-FF is validated to be superior to various
state-of-the-art methods in multi-view semi-supervised classification.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：A Review of Intelligent Music Generation Systems</b></summary>
  <p><b>编号</b>：[239]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09124</p>
  <p><b>作者</b>：Ziyi Zhao,  Hanwei Liu,  Song Li,  Junwei Pang,  Maoqing Zhang,  Yi Qin,  Lei Wang,  Qidi Wu</p>
  <p><b>备注</b>：Overall 24 Pages, 11 Figures, 2 Tables, 96 References items</p>
  <p><b>关键词</b>：Intelligent music generation, music generation, computer creativity, popular subfields, subfields of computer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Intelligent music generation, one of the most popular subfields of computer
creativity, can lower the creative threshold for non-specialists and increase
the efficiency of music creation. In the last five years, the quality of
algorithm-based automatic music generation has increased significantly,
motivated by the use of modern generative algorithms to learn the patterns
implicit within a piece of music based on rule constraints or a musical corpus,
thus generating music samples in various styles. Some of the available
literature reviews lack a systematic benchmark of generative models and are
traditional and conservative in their perspective, resulting in a vision of the
future development of the field that is not deeply integrated with the current
rapid scientific progress. In this paper, we conduct a comprehensive survey and
analysis of recent intelligent music generation techniques,provide a critical
discussion, explicitly identify their respective characteristics, and present
them in a general table. We first introduce how music as a stream of
information is encoded and the relevant datasets, then compare different types
of generation algorithms, summarize their strengths and weaknesses, and discuss
existing methods for evaluation. Finally, the development of artificial
intelligence in composition is studied, especially by comparing the different
characteristics of music generation techniques in the East and West and
analyzing the development prospects in this field.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：A Review of Deep Learning Techniques for Protein Function Prediction</b></summary>
  <p><b>编号</b>：[249]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09705</p>
  <p><b>作者</b>：Divyanshu Aggarwal,  Yasha Hasija</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：artificial intelligence methods, shown tremendous success, protein function classification, protein function, predicting protein function</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep Learning and big data have shown tremendous success in bioinformatics
and computational biology in recent years; artificial intelligence methods have
also significantly contributed in the task of protein function classification.
This review paper analyzes the recent developments in approaches for the task
of predicting protein function using deep learning. We explain the importance
of determining the protein function and why automating the following task is
crucial. Then, after reviewing the widely used deep learning techniques for
this task, we continue our review and highlight the emergence of the modern
State of The Art (SOTA) deep learning models which have achieved groundbreaking
results in the field of computer vision, natural language processing and
multi-modal learning in the last few years. We hope that this review will
provide a broad view of the current role and advances of deep learning in
biological sciences, especially in predicting protein function tasks and
encourage new researchers to contribute to this area.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：Learning to Communicate with Intent: An Introduction</b></summary>
  <p><b>编号</b>：[252]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09613</p>
  <p><b>作者</b>：Miguel Angel Gutierrez-Estevez,  Yiqun Wu,  Chan Zhou</p>
  <p><b>备注</b>：7 pages, 4 figues, submitted to IEEE ICC 2023</p>
  <p><b>关键词</b>：wireless communication channel, communicate with intent, transmit messages, wireless communication, classical communication systems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a novel framework to learn how to communicate with intent, i.e.,
to transmit messages over a wireless communication channel based on the
end-goal of the communication. This stays in stark contrast to classical
communication systems where the objective is to reproduce at the receiver side
either exactly or approximately the message sent by the transmitter, regardless
of the end-goal. Our procedure is general enough that can be adapted to any
type of goal or task, so long as the said task is a (almost-everywhere)
differentiable function over which gradients can be propagated. We focus on
supervised learning and reinforcement learning (RL) tasks, and propose
algorithms to learn the communication system and the task jointly in an
end-to-end manner. We then delve deeper into the transmission of images and
propose two systems, one for the classification of images and a second one to
play an Atari game based on RL. The performance is compared with a joint source
and channel coding (JSCC) communication system designed to minimize the
reconstruction error, and results show overall great improvement. Further, for
the RL task, we show that while a JSCC strategy is not better than a random
action selection strategy, with our approach we get close to the upper bound
even for low SNRs.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Validation Diagnostics for SBI algorithms based on Normalizing Flows</b></summary>
  <p><b>编号</b>：[253]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09602</p>
  <p><b>作者</b>：Julia Linhart (1,2),  Alexandre Gramfort (1),  Pedro L. C. Rodrigues (2) ((1) MIND - INRIA, (2) University of Paris-Saclay, (3) STATIFY - INRIA)</p>
  <p><b>备注</b>：7 pages, 2 figures, 1 appendix, to be published at "Machine Learning and the Physical Sciences" workshop (NeurIPS 2022)</p>
  <p><b>关键词</b>：deep generative models, recent trend, deep generative, high-dimensional data distributions, Normalizing Flows</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Building on the recent trend of new deep generative models known as
Normalizing Flows (NF), simulation-based inference (SBI) algorithms can now
efficiently accommodate arbitrary complex and high-dimensional data
distributions. The development of appropriate validation methods however has
fallen behind. Indeed, most of the existing metrics either require access to
the true posterior distribution, or fail to provide theoretical guarantees on
the consistency of the inferred approximation beyond the one-dimensional
setting. This work proposes easy to interpret validation diagnostics for
multi-dimensional conditional (posterior) density estimators based on NF. It
also offers theoretical guarantees based on results of local consistency. The
proposed workflow can be used to check, analyse and guarantee consistent
behavior of the estimator. The method is illustrated with a challenging example
that involves tightly coupled parameters in the context of computational
neuroscience. This work should help the design of better specified models or
drive the development of novel SBI-algorithms, hence allowing to build up trust
on their ability to address important questions in experimental science.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：EmoDiff: Intensity Controllable Emotional Text-to-Speech with Soft-Label  Guidance</b></summary>
  <p><b>编号</b>：[263]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09496</p>
  <p><b>作者</b>：Yiwei Guo,  Chenpeng Du,  Xie Chen,  Kai Yu</p>
  <p><b>备注</b>：Submitted to ICASSP2023</p>
  <p><b>关键词</b>：controllable emotional TTS, intensity controllable emotional, generate high-quality speech, current neural, challenging task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Although current neural text-to-speech (TTS) models are able to generate
high-quality speech, intensity controllable emotional TTS is still a
challenging task. Most existing methods need external optimizations for
intensity calculation, leading to suboptimal results or degraded quality. In
this paper, we propose EmoDiff, a diffusion-based TTS model where emotion
intensity can be manipulated by a proposed soft-label guidance technique
derived from classifier guidance. Specifically, instead of being guided with a
one-hot vector for the specified emotion, EmoDiff is guided with a soft label
where the value of the specified emotion and \textit{Neutral} is set to
$\alpha$ and $1-\alpha$ respectively. The $\alpha$ here represents the emotion
intensity and can be chosen from 0 to 1. Our experiments show that EmoDiff can
precisely control the emotion intensity while maintaining high voice quality.
Moreover, diverse speech with specified emotion intensity can be generated by
sampling in the reverse denoising process.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Any-speaker Adaptive Text-To-Speech Synthesis with Diffusion Models</b></summary>
  <p><b>编号</b>：[272]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2211.09383</p>
  <p><b>作者</b>：Minki Kang,  Dongchan Min,  Sung Ju Hwang</p>
  <p><b>备注</b>：Under Review</p>
  <p><b>关键词</b>：neural generative modeling, any-speaker adaptive TTS, synthesis technology, generative modeling, significant progress</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>There has been a significant progress in Text-To-Speech (TTS) synthesis
technology in recent years, thanks to the advancement in neural generative
modeling. However, existing methods on any-speaker adaptive TTS have achieved
unsatisfactory performance, due to their suboptimal accuracy in mimicking the
target speakers' styles. In this work, we present Grad-StyleSpeech, which is an
any-speaker adaptive TTS framework that is based on a diffusion model that can
generate highly natural speech with extremely high similarity to target
speakers' voice, given a few seconds of reference speech. Grad-StyleSpeech
significantly outperforms recent speaker-adaptive TTS baselines on English
benchmarks. Audio samples are available at
this https URL.</p>
  </details>
</details>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">徐耀彬</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://louishsu.xyz/2022/11/21/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">http://louishsu.xyz/2022/11/21/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://louishsu.xyz" target="_blank">LOUIS' BLOG</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2022/11/17/2022%E5%85%A8%E7%90%83%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0%E5%A4%A7%E8%B5%9B(GAIIC2022)%EF%BC%9A%E5%95%86%E5%93%81%E6%A0%87%E9%A2%98%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E4%BA%8C%E7%AD%89%E5%A5%96.html"><img class="next-cover" src="https://cdn.kesci.com/upload/image/r7j60un866.png?imageView2/2/w/2500/h/2500" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别二等奖</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">徐耀彬</div><div class="author-info__description">做知识的原创者！</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">12</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/isLouisHsu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/isLouisHsu" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:is.louishsu@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">记录和分享一些学习和开源内容，若有问题可通过邮箱is.louishsu@foxmail.com联系，欢迎交流！！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">统计</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">计算机视觉</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">自然语言处理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">机器学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">人工智能</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/11/21/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2022-11-21)"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv每日速递(2022-11-21)"/></a><div class="content"><a class="title" href="/2022/11/21/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2022-11-21)">Arxiv每日速递(2022-11-21)</a><time datetime="2022-11-21T00:46:25.539Z" title="发表于 2022-11-21 08:46:25">2022-11-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/11/17/2022%E5%85%A8%E7%90%83%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0%E5%A4%A7%E8%B5%9B(GAIIC2022)%EF%BC%9A%E5%95%86%E5%93%81%E6%A0%87%E9%A2%98%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E4%BA%8C%E7%AD%89%E5%A5%96.html" title="2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别二等奖"><img src="https://cdn.kesci.com/upload/image/r7j60un866.png?imageView2/2/w/2500/h/2500" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别二等奖"/></a><div class="content"><a class="title" href="/2022/11/17/2022%E5%85%A8%E7%90%83%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0%E5%A4%A7%E8%B5%9B(GAIIC2022)%EF%BC%9A%E5%95%86%E5%93%81%E6%A0%87%E9%A2%98%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E4%BA%8C%E7%AD%89%E5%A5%96.html" title="2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别二等奖">2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别二等奖</a><time datetime="2022-11-17T14:29:06.000Z" title="发表于 2022-11-17 22:29:06">2022-11-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/10/22/%E4%B8%AD%E5%9B%BD%E6%B3%95%E5%BE%8B%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E8%AF%84%E6%B5%8B(CAIL2021)%EF%BC%9A%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96(Rank2).html" title="中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)"><img src="http://cail.cipsc.org.cn/img/index_mainpic.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)"/></a><div class="content"><a class="title" href="/2021/10/22/%E4%B8%AD%E5%9B%BD%E6%B3%95%E5%BE%8B%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E8%AF%84%E6%B5%8B(CAIL2021)%EF%BC%9A%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96(Rank2).html" title="中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)">中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)</a><time datetime="2021-10-22T14:29:06.000Z" title="发表于 2021-10-22 22:29:06">2021-10-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/05/19/%E5%85%A8%E7%90%83%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0%E5%A4%A7%E8%B5%9B%E3%80%90%E8%B5%9B%E9%81%93%E4%B8%80%E3%80%91%EF%BC%9A%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E6%8A%A5%E5%91%8A%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B.html" title="全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测"><img src="https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161037709574435991610377095138.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测"/></a><div class="content"><a class="title" href="/2021/05/19/%E5%85%A8%E7%90%83%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0%E5%A4%A7%E8%B5%9B%E3%80%90%E8%B5%9B%E9%81%93%E4%B8%80%E3%80%91%EF%BC%9A%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E6%8A%A5%E5%91%8A%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B.html" title="全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测">全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测</a><time datetime="2021-05-19T09:57:06.000Z" title="发表于 2021-05-19 17:57:06">2021-05-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/09/16/%E8%AF%A6%E8%A7%A3%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%EF%BC%9ALSTM-CRF.html" title="详解命名实体识别模型：LSTM-CRF"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fpic1.zhimg.com%2Fv2-1ed6a10dbb239a148e42df088c5870a6_1440w.jpg%3Fsource%3D172ae18b&amp;refer=http%3A%2F%2Fpic1.zhimg.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1638183974&amp;t=4632f13fd487ed1cfcb12d67a6b71e52" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="详解命名实体识别模型：LSTM-CRF"/></a><div class="content"><a class="title" href="/2020/09/16/%E8%AF%A6%E8%A7%A3%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%EF%BC%9ALSTM-CRF.html" title="详解命名实体识别模型：LSTM-CRF">详解命名实体识别模型：LSTM-CRF</a><time datetime="2020-09-16T09:26:44.000Z" title="发表于 2020-09-16 17:26:44">2020-09-16</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2022 By 徐耀彬</div><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender"title="主题采用butterfly"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr"title="本站使用JsDelivr为静态资源提供CDN加速"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub"title="本站项目由Gtihub托管"alt="img"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"alt="img"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></br></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    let initData = {
      el: '#twikoo-wrap',
      envId: 'blog-',
      region: ''
    }

    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    twikoo.init(initData)
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'blog-',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/机器学习/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐱 机器学习 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/自然语言处理/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 自然语言处理 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/竞赛相关/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 竞赛相关 (3)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/阅读笔记/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 阅读笔记 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="http://louishsu.xyz/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style>
  <script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/hexo-github-calendar@1.21/hexo_githubcalendar.js"></script>
  <script data-pjax>
        function GithubCalendarConfig(){
            var git_githubapiurl ="https://python-github-calendar-api.vercel.app/api?isLouisHsu";
            var git_color =['#ebedf0', '#fdcdec', '#fc9bd9', '#fa6ac5', '#f838b2', '#f5089f', '#c4067e', '#92055e', '#540336', '#48022f', '#30021f'];
            var git_user ="isLouisHsu";
            var parent_div_git = document.getElementById('recent-posts');
            var git_div_html = '<div class="recent-post-item" style="width:100%;height:auto;padding:10px;"><div id="github_loading" style="width:10%;height:100%;margin:0 auto;display: block"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"  viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animateTransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animateTransform></path></svg></div><div id="github_container"></div></div>';
            if(parent_div_git && location.pathname =='/'){
                console.log('已挂载github calendar')
                // parent_div_git.innerHTML=git_div_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",git_div_html) // 有报错，但不影响使用(支持pjax跳转)
            };
            GithubCalendar(git_githubapiurl,git_color,git_user)
        }
        if(document.getElementById('recent-posts')){
            GithubCalendarConfig()
        }
    </script>
    <style>#github_container{min-height:280px}@media screen and (max-width:650px) {#github_container{background-image:;min-height:0px}}</style>
    <style></style><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>