<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Arxiv每日速递(2023-02-02) | LOUIS' BLOG</title><meta name="author" content="徐耀彬"><meta name="copyright" content="徐耀彬"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。 统计 今日共更新317篇论文，其中：  66篇计算机视觉（cs.CV） 35篇自然语言处理（cs.CL） 142篇机器学习（cs.LG） 64篇人工智能（cs.AI）  计算机视觉    1. 标题：From Semi-supervised to Omni-supervis">
<meta property="og:type" content="article">
<meta property="og:title" content="Arxiv每日速递(2023-02-02)">
<meta property="og:url" content="http://louishsu.xyz/2023/02/02/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">
<meta property="og:site_name" content="LOUIS&#39; BLOG">
<meta property="og:description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。 统计 今日共更新317篇论文，其中：  66篇计算机视觉（cs.CV） 35篇自然语言处理（cs.CL） 142篇机器学习（cs.LG） 64篇人工智能（cs.AI）  计算机视觉    1. 标题：From Semi-supervised to Omni-supervis">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png">
<meta property="article:published_time" content="2023-02-02T00:42:13.038Z">
<meta property="article:modified_time" content="2023-02-02T00:44:08.734Z">
<meta property="article:author" content="徐耀彬">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://louishsu.xyz/2023/02/02/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-02-02 08:44:08'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/background.css"><script src="https://cdn.jsdelivr.net/npm/echarts@4.7.0/dist/echarts.min.js"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">13</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-brands fa-app-store"></i><span> 利器</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://code.visualstudio.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> VSCode：微软旗下的跨平台代码编辑软件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mobaxterm.mobatek.net/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> MobaXterm：超好用的全能远程终端</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.zotero.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zotero：便于收集、组织、引用、共享的文献管理工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/CopyTranslator/CopyTranslator"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> CopyTranslator：“复制即翻译”的外文辅助阅读翻译解决方案</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ditto-cp.sourceforge.io/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Ditto：强大的Windows剪贴板增强工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/indiff/qttabbar"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> QtTabBar：在Windows资源管理器中使用多标签功能扩展工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/sentialx/multrin"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Multrin：“窗口合并”辅助小工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/Wox-launcher/Wox"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Wox &amp; Everything：基于名称快速定位文件和文件夹的搜索工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/NickeManarin/ScreenToGif"><i class="fa-fw fa-regular fa-star"></i><span> ScreenToGif：快速录制屏幕指定区域并保存为动图文件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mathpix.com/"><i class="fa-fw fa-regular fa-star"></i><span> Mathpix Snipping：识别数学公式并转换成LaTeX</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">LOUIS' BLOG</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-brands fa-app-store"></i><span> 利器</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://code.visualstudio.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> VSCode：微软旗下的跨平台代码编辑软件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mobaxterm.mobatek.net/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> MobaXterm：超好用的全能远程终端</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.zotero.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zotero：便于收集、组织、引用、共享的文献管理工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/CopyTranslator/CopyTranslator"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> CopyTranslator：“复制即翻译”的外文辅助阅读翻译解决方案</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ditto-cp.sourceforge.io/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Ditto：强大的Windows剪贴板增强工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/indiff/qttabbar"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> QtTabBar：在Windows资源管理器中使用多标签功能扩展工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/sentialx/multrin"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Multrin：“窗口合并”辅助小工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/Wox-launcher/Wox"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Wox &amp; Everything：基于名称快速定位文件和文件夹的搜索工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/NickeManarin/ScreenToGif"><i class="fa-fw fa-regular fa-star"></i><span> ScreenToGif：快速录制屏幕指定区域并保存为动图文件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mathpix.com/"><i class="fa-fw fa-regular fa-star"></i><span> Mathpix Snipping：识别数学公式并转换成LaTeX</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Arxiv每日速递(2023-02-02)<a class="post-edit-link" href="https://github.com/isLouisHsu/blog/tree/master/source_posts/Arxiv每日速递.md" title="编辑" target="_blank"><i class="fas fa-pencil-square"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-02-02T00:42:13.038Z" title="发表于 2023-02-02 08:42:13">2023-02-02</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-02-02T00:44:08.734Z" title="更新于 2023-02-02 08:44:08">2023-02-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">阅读笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">34.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>207分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2023/02/02/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。</p>
<h1>统计</h1>
<p>今日共更新317篇论文，其中：</p>
<ul>
<li><a href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89">66篇计算机视觉（cs.CV）</a></li>
<li><a href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86">35篇自然语言处理（cs.CL）</a></li>
<li><a href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">142篇机器学习（cs.LG）</a></li>
<li><a href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD">64篇人工智能（cs.AI）</a></li>
</ul>
<h1>计算机视觉</h1>
<details>
  <summary>1. <b>标题：From Semi-supervised to Omni-supervised Room Layout Estimation Using  Point Clouds</b></summary>
  <p><b>编号</b>：[4]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13865</p>
  <p><b>作者</b>：Huan-ang Gao,  Beiwen Tian,  Pengfei Li,  Xiaoxue Chen,  Hao Zhao,  Guyue Zhou,  Yurong Chen,  Hongbin Zha</p>
  <p><b>备注</b>：Accepted to ICRA2023. Code: this https URL</p>
  <p><b>关键词</b>：long-existing robotic vision, robotic vision task, Room layout estimation, motion planning, long-existing robotic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Room layout estimation is a long-existing robotic vision task that benefits
both environment sensing and motion planning. However, layout estimation using
point clouds (PCs) still suffers from data scarcity due to annotation
difficulty. As such, we address the semi-supervised setting of this task based
upon the idea of model exponential moving averaging. But adapting this scheme
to the state-of-the-art (SOTA) solution for PC-based layout estimation is not
straightforward. To this end, we define a quad set matching strategy and
several consistency losses based upon metrics tailored for layout quads.
Besides, we propose a new online pseudo-label harvesting algorithm that
decomposes the distribution of a hybrid distance measure between quads and PC
into two components. This technique does not need manual threshold selection
and intuitively encourages quads to align with reliable layout points.
Surprisingly, this framework also works for the fully-supervised setting,
achieving a new SOTA on the ScanNet benchmark. Last but not least, we also push
the semi-supervised setting to the realistic omni-supervised setting,
demonstrating significantly promoted performance on a newly annotated
ARKitScenes testing set. Our codes, data and models are released in this
repository.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Salient Conditional Diffusion for Defending Against Backdoor Attacks</b></summary>
  <p><b>编号</b>：[5]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13862</p>
  <p><b>作者</b>：Brandon B. May,  N. Joseph Tatro,  Piyush Kumar,  Nathan Shnidman</p>
  <p><b>备注</b>：12 pages, 5 figures</p>
  <p><b>关键词</b>：Salient Conditional Diffusion, Conditional Diffusion, Salient Conditional, backdoor attacks, Diffusion</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a novel algorithm, Salient Conditional Diffusion (Sancdifi), a
state-of-the-art defense against backdoor attacks. Sancdifi uses a denoising
diffusion probabilistic model (DDPM) to degrade an image with noise and then
recover said image using the learned reverse diffusion. Critically, we compute
saliency map-based masks to condition our diffusion, allowing for stronger
diffusion on the most salient pixels by the DDPM. As a result, Sancdifi is
highly effective at diffusing out triggers in data poisoned by backdoor
attacks. At the same time, it reliably recovers salient features when applied
to clean data. This performance is achieved without requiring access to the
model parameters of the Trojan network, meaning Sancdifi operates as a
black-box defense.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Image Shortcut Squeezing: Countering Perturbative Availability Poisons  with Compression</b></summary>
  <p><b>编号</b>：[14]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13838</p>
  <p><b>作者</b>：Zhuoran Liu,  Zhengyu Zhao,  Martha Larson</p>
  <p><b>备注</b>：Our code is available at this https URL</p>
  <p><b>关键词</b>：adds small, Image Shortcut Squeezing, Perturbative availability poisoning, PAP, ISS</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Perturbative availability poisoning (PAP) adds small changes to images to
prevent their use for model training. Current research adopts the belief that
practical and effective approaches to countering such poisons do not exist. In
this paper, we argue that it is time to abandon this belief. We present
extensive experiments showing that 12 state-of-the-art PAP methods are
vulnerable to Image Shortcut Squeezing (ISS), which is based on simple
compression. For example, on average, ISS restores the CIFAR-10 model accuracy
to $81.73\%$, surpassing the previous best preprocessing-based countermeasures
by $37.97\%$ absolute. ISS also (slightly) outperforms adversarial training and
has higher generalizability to unseen perturbation norms and also higher
efficiency. Our investigation reveals that the property of PAP perturbations
depends on the type of surrogate model used for poison generation, and it
explains why a specific ISS compression yields the best performance for a
specific type of PAP perturbation. We further test stronger, adaptive
poisoning, and show it falls short of being an ideal defense against ISS.
Overall, our results demonstrate the importance of considering various (simple)
countermeasures to ensure the meaningfulness of analysis carried out during the
development of availability poisons.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image  Diffusion Models</b></summary>
  <p><b>编号</b>：[19]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13826</p>
  <p><b>作者</b>：Hila Chefer,  Yuval Alaluf,  Yael Vinker,  Lior Wolf,  Daniel Cohen-Or</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：creative imagery guided, target text prompt, Stable Diffusion model, text prompt, demonstrated an unparalleled</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent text-to-image generative models have demonstrated an unparalleled
ability to generate diverse and creative imagery guided by a target text
prompt. While revolutionary, current state-of-the-art diffusion models may
still fail in generating images that fully convey the semantics in the given
text prompt. We analyze the publicly available Stable Diffusion model and
assess the existence of catastrophic neglect, where the model fails to generate
one or more of the subjects from the input prompt. Moreover, we find that in
some cases the model also fails to correctly bind attributes (e.g., colors) to
their corresponding subjects. To help mitigate these failure cases, we
introduce the concept of Generative Semantic Nursing (GSN), where we seek to
intervene in the generative process on the fly during inference time to improve
the faithfulness of the generated images. Using an attention-based formulation
of GSN, dubbed Attend-and-Excite, we guide the model to refine the
cross-attention units to attend to all subject tokens in the text prompt and
strengthen - or excite - their activations, encouraging the model to generate
all subjects described in the text prompt. We compare our approach to
alternative approaches and demonstrate that it conveys the desired concepts
more faithfully across a range of text prompts.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Grounding Language Models to Images for Multimodal Generation</b></summary>
  <p><b>编号</b>：[20]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13823</p>
  <p><b>作者</b>：Jing Yu Koh,  Ruslan Salakhutdinov,  Daniel Fried</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：visual domain, language models, text-only language models, ground pretrained text-only, propose an efficient</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose an efficient method to ground pretrained text-only language models
to the visual domain, enabling them to process and generate arbitrarily
interleaved image-and-text data. Our method leverages the abilities of language
models learnt from large scale text-only pretraining, such as in-context
learning and free-form text generation. We keep the language model frozen, and
finetune input and output linear layers to enable cross-modality interactions.
This allows our model to process arbitrarily interleaved image-and-text inputs,
and generate free-form text interleaved with retrieved images. We achieve
strong zero-shot performance on grounded tasks such as contextual image
retrieval and multimodal dialogue, and showcase compelling interactive
abilities. Our approach works with any off-the-shelf language model and paves
the way towards an effective, general solution for leveraging pretrained
language models in visually grounded settings.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Patch Gradient Descent: Training Neural Networks on Very Large Images</b></summary>
  <p><b>编号</b>：[24]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13817</p>
  <p><b>作者</b>：Deepak K. Gupta,  Gowreesh Mago,  Arnav Chavan,  Dilip K. Prasad</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Patch Gradient Descent, Traditional CNN models, low resolution images, large-scale images due, Traditional CNN</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Traditional CNN models are trained and tested on relatively low resolution
images (<300 px), and cannot be directly operated on large-scale images due to compute memory constraints. we propose patch gradient descent (patchgd), an effective learning strategy that allows train the existing cnn architectures in end-to-end manner. patchgd is based hypothesis instead of performing gradient-based updates entire image at once, it should possible achieve a good solution by model only small parts time, ensuring majority covered over course iterations. thus extensively enjoys better efficiency when training models large scale images. thoroughly evaluated two datasets - panda ultramnist with resnet50 mobilenetv2 under different our evaluation clearly shows much more stable efficient than standard gradient-descent method handling images, especially limited.< p>
  </300></p></details>
</details>
<details>
  <summary>7. <b>标题：Ultrasound Based Prosthetic Arm Control</b></summary>
  <p><b>编号</b>：[27]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13809</p>
  <p><b>作者</b>：Ayush Singh,  Harikrishnan Pisharody Gopalkrishnan,  Mahesh Raveendranatha Panicker</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：daily duties independently, perform daily duties, limits a person, person ability, ability to work</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The loss of an upper limb can have a substantial impact on a person's quality
of life since it limits a person's ability to work, interact, and perform daily
duties independently. Artificial limbs are used in prosthetics to help people
who have lost limbs enhance their function and quality of life. Despite
significant breakthroughs in prosthetic technology, rejection rates for complex
prosthetic devices remain high[1]-[5]. A quarter to a third of upper-limb
amputees abandon their prosthetics due to a lack of comprehension of the
technology. The most extensively used method for monitoring muscle activity and
regulating the prosthetic arm, surface electromyography (sEMG), has significant
drawbacks, including a low signal-to-noise ratio and poor amplitude
resolution[6]-[8].Unlike myoelectric control systems, which use electrical
muscle activation to calculate end-effector velocity, our strategy employs
ultrasound to directly monitor mechanical muscle deformation and then uses the
extracted signals to proportionally control end-effector location. This
investigation made use of four separate hand motions performed by three
physically healthy volunteers. A virtual robotic hand simulation was created
using ROS. After witnessing performance comparable to that of a hand with very
less training, we concluded that our control method is reliable and natural.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Fairness-aware Vision Transformer via Debiased Self-Attention</b></summary>
  <p><b>编号</b>：[31]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13803</p>
  <p><b>作者</b>：Yao Qiang,  Chengyin Li,  Prashant Khanduri,  Dongxiao Zhu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：solving computer vision, recently gained significant, gained significant interest, modeling long-range dependencies, Vision Transformer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision Transformer (ViT) has recently gained significant interest in solving
computer vision (CV) problems due to its capability of extracting informative
features and modeling long-range dependencies through the self-attention
mechanism. To fully realize the advantages of ViT in real-world applications,
recent works have explored the trustworthiness of ViT, including its robustness
and explainability. However, another desiderata, fairness has not yet been
adequately addressed in the literature. We establish that the existing
fairness-aware algorithms (primarily designed for CNNs) do not perform well on
ViT. This necessitates the need for developing our novel framework via Debiased
Self-Attention (DSA). DSA is a fairness-through-blindness approach that
enforces ViT to eliminate spurious features correlated with the sensitive
attributes for bias mitigation. Notably, adversarial examples are leveraged to
locate and mask the spurious features in the input image patches. In addition,
DSA utilizes an attention weights alignment regularizer in the training
objective to encourage learning informative features for target prediction.
Importantly, our DSA framework leads to improved fairness guarantees over prior
works on multiple prediction tasks without compromising target prediction
performance</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Zero-shot-Learning Cross-Modality Data Translation Through Mutual  Information Guided Stochastic Diffusion</b></summary>
  <p><b>编号</b>：[51]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13743</p>
  <p><b>作者</b>：Zihao Wang,  Yingyu Yang,  Maxime Sermesant,  Hervé Delingette,  Ona Wu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：attracted great interest, Cross-modality data translation, data translation, data translation Model, Cross-modality data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cross-modality data translation has attracted great interest in image
computing. Deep generative models (\textit{e.g.}, GANs) show performance
improvement in tackling those problems. Nevertheless, as a fundamental
challenge in image translation, the problem of Zero-shot-Learning
Cross-Modality Data Translation with fidelity remains unanswered. This paper
proposes a new unsupervised zero-shot-learning method named Mutual Information
guided Diffusion cross-modality data translation Model (MIDiffusion), which
learns to translate the unseen source data to the target domain. The
MIDiffusion leverages a score-matching-based generative model, which learns the
prior knowledge in the target domain. We propose a differentiable
local-wise-MI-Layer ($LMI$) for conditioning the iterative denoising sampling.
The $LMI$ captures the identical cross-modality features in the statistical
domain for the diffusion guidance; thus, our method does not require retraining
when the source domain is changed, as it does not rely on any direct mapping
between the source and target domains. This advantage is critical for applying
cross-modality data translation methods in practice, as a reasonable amount of
source domain dataset is not always available for supervised training. We
empirically show the advanced performance of MIDiffusion in comparison with an
influential group of generative models, including adversarial-based and other
score-matching-based models.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：UPop: Unified and Progressive Pruning for Compressing Vision-Language  Transformers</b></summary>
  <p><b>编号</b>：[52]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13741</p>
  <p><b>作者</b>：Dachuan Shi,  Chaofan Tao,  Ying Jin,  Zhendong Yang,  Chun Yuan,  Jiaqi Wang</p>
  <p><b>备注</b>：16 pages, 5 figures, 13 tables</p>
  <p><b>关键词</b>：Real-world data, vast amount, vision and language, textbf, vison-language Transformer compression</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Real-world data contains a vast amount of multimodal information, among which
vision and language are the two most representative modalities. Moreover,
increasingly heavier models, e.g., Transformers, have attracted the attention
of researchers to model compression. However, how to compress multimodal
models, especially vison-language Transformers, is still under-explored. This
paper proposes the \textbf{U}nified and \textbf{P}r\textbf{o}gressive
\textbf{P}runing (UPop) as a universal vison-language Transformer compression
framework, which incorporates 1) unifiedly searching multimodal subnets in a
continuous optimization space from the original model, which enables automatic
assignment of pruning ratios among compressible modalities and structures; 2)
progressively searching and retraining the subnet, which maintains convergence
between the search and retrain to attain higher compression ratios. Experiments
on multiple generative and discriminative vision-language tasks, including
Visual Reasoning, Image Caption, Visual Question Answer, Image-Text Retrieval,
Text-Image Retrieval, and Image Classification, demonstrate the effectiveness
and versatility of the proposed UPop framework.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：DisDiff: Unsupervised Disentanglement of Diffusion Probabilistic Models</b></summary>
  <p><b>编号</b>：[60]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13721</p>
  <p><b>作者</b>：Tao Yang,  Yuwang Wang,  Yan Lv,  Nanning Zh</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：remarkable modeling ability, conditional generation process, underlying explainable factors, diffusion probabilistic models, remarkable modeling</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, targeting to understand the underlying explainable factors
behind observations and modeling the conditional generation process on these
factors, we propose a new task, disentanglement of diffusion probabilistic
models (DPMs), to take advantage of the remarkable modeling ability of DPMs. To
tackle this task, we further devise an unsupervised approach named DisDiff. For
the first time, we achieve disentangled representation learning in the
framework of diffusion probabilistic models. Given a pre-trained DPM, DisDiff
can automatically discover the inherent factors behind the image data and
disentangle the gradient fields of DPM into sub-gradient fields, each
conditioned on the representation of each discovered factor. We propose a novel
Disentangling Loss for DisDiff to facilitate the disentanglement of the
representation and sub-gradients. The extensive experiments on synthetic and
real-world datasets demonstrate the effectiveness of DisDiff.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：What Makes Good Examples for Visual In-Context Learning?</b></summary>
  <p><b>编号</b>：[73]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13670</p>
  <p><b>作者</b>：Yuanhan Zhang,  Kaiyang Zhou,  Ziwei Liu</p>
  <p><b>备注</b>：code and models:this https URL</p>
  <p><b>关键词</b>：Large-scale models trained, strong generalization performance, computer vision due, large vision models, trained on broad</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large-scale models trained on broad data have recently become the mainstream
architecture in computer vision due to their strong generalization performance.
In this paper, the main focus is on an emergent ability in large vision models,
known as in-context learning, which allows inference on unseen tasks by
conditioning on in-context examples (a.k.a.~prompt) without updating the model
parameters. This concept has been well-known in natural language processing but
has only been studied very recently for large vision models. We for the first
time provide a comprehensive investigation on the impact of in-context examples
in computer vision, and find that the performance is highly sensitive to the
choice of in-context examples. To overcome the problem, we propose a prompt
retrieval framework to automate the selection of in-context examples.
Specifically, we present (1) an unsupervised prompt retrieval method based on
nearest example search using an off-the-shelf model, and (2) a supervised
prompt retrieval method, which trains a neural network to choose examples that
directly maximize in-context learning performance. The results demonstrate that
our methods can bring non-trivial improvements to visual in-context learning in
comparison to the commonly-used random selection.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Spyker: High-performance Library for Spiking Deep Neural Networks</b></summary>
  <p><b>编号</b>：[77]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13659</p>
  <p><b>作者</b>：Shahriar Rezghi Shirsavar,  Mohammad-Reza A. Dehaqani</p>
  <p><b>备注</b>：11 pages, 6 figures, 6 listings</p>
  <p><b>关键词</b>：promising capabilities, recently brought, brought to light, Spiking neural networks, neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Spiking neural networks (SNNs) have been recently brought to light due to
their promising capabilities. SNNs simulate the brain with higher biological
plausibility compared to previous generations of neural networks. Learning with
fewer samples and consuming less power are among the key features of these
networks. However, the theoretical advantages of SNNs have not been seen in
practice due to the slowness of simulation tools and the impracticality of the
proposed network structures. In this work, we implement a high-performance
library named Spyker using C++/CUDA from scratch that outperforms its
predecessor. Several SNNs are implemented in this work with different learning
rules (spike-timing-dependent plasticity and reinforcement learning) using
Spyker that achieve significantly better runtimes, to prove the practicality of
the library in the simulation of large-scale networks. To our knowledge, no
such tools have been developed to simulate large-scale spiking neural networks
with high performance using a modular structure. Furthermore, a comparison of
the represented stimuli extracted from Spyker to recorded electrophysiology
data is performed to demonstrate the applicability of SNNs in describing the
underlying neural mechanisms of the brain functions. The aim of this library is
to take a significant step toward uncovering the true potential of the brain
computations using SNNs.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：A Survey and Benchmark of Automatic Surface Reconstruction from Point  Clouds</b></summary>
  <p><b>编号</b>：[79]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13656</p>
  <p><b>作者</b>：Raphael Sulzer,  Loic Landrieu,  Renaud Marlet,  Bruno Vallet</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：surface reconstruction, point clouds, algorithms that address, surface, point</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We survey and benchmark traditional and novel learning-based algorithms that
address the problem of surface reconstruction from point clouds. Surface
reconstruction from point clouds is particularly challenging when applied to
real-world acquisitions, due to noise, outliers, non-uniform sampling and
missing data. Traditionally, different handcrafted priors of the input points
or the output surface have been proposed to make the problem more tractable.
However, hyperparameter tuning for adjusting priors to different acquisition
defects can be a tedious task. To this end, the deep learning community has
recently addressed the surface reconstruction problem. In contrast to
traditional approaches, deep surface reconstruction methods can learn priors
directly from a training set of point clouds and corresponding true surfaces.
In our survey, we detail how different handcrafted and learned priors affect
the robustness of methods to defect-laden input and their capability to
generate geometric and topologically accurate reconstructions. In our
benchmark, we evaluate the reconstructions of several traditional and
learning-based methods on the same grounds. We show that learning-based methods
can generalize to unseen shape categories, but their training and test sets
must share the same point cloud characteristics. We also provide the code and
data to compete in our benchmark and to further stimulate the development of
learning-based surface reconstruction
this https URL.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Learning Data Representations with Joint Diffusion Models</b></summary>
  <p><b>编号</b>：[94]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13622</p>
  <p><b>作者</b>：Kamil Deja,  Tomasz Trzcinski,  Jakub M. Tomczak</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：simultaneously learns meaningful, learns meaningful internal, meaningful internal representations, internal representations fit, simultaneously learns</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce a joint diffusion model that simultaneously learns meaningful
internal representations fit for both generative and predictive tasks. Joint
machine learning models that allow synthesizing and classifying data often
offer uneven performance between those tasks or are unstable to train. In this
work, we depart from a set of empirical observations that indicate the
usefulness of internal representations built by contemporary deep
diffusion-based generative models in both generative and predictive settings.
We then introduce an extension of the vanilla diffusion model with a classifier
that allows for stable joint training with shared parametrization between those
objectives. The resulting joint diffusion model offers superior performance
across various tasks, including generative modeling, semi-supervised
classification, and domain adaptation.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Multi-video Moment Ranking with Multimodal Clue</b></summary>
  <p><b>编号</b>：[100]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13606</p>
  <p><b>作者</b>：Danyang Hou,  Liang Pang,  Yanyan Lan,  Huawei Shen,  Xueqi Cheng</p>
  <p><b>备注</b>：9 pages,6 figures</p>
  <p><b>关键词</b>：natural language query, corpus moment retrieval, textbf, large corpus, language query</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Video corpus moment retrieval~(VCMR) is the task of retrieving a relevant
video moment from a large corpus of untrimmed videos via a natural language
query. State-of-the-art work for VCMR is based on two-stage method. In this
paper, we focus on improving two problems of two-stage method: (1) Moment
prediction bias: The predicted moments for most queries come from the top
retrieved videos, ignoring the possibility that the target moment is in the
bottom retrieved videos, which is caused by the inconsistency of Shared
Normalization during training and inference. (2) Latent key content: Different
modalities of video have different key information for moment localization. To
this end, we propose a two-stage model \textbf{M}ult\textbf{I}-video
ra\textbf{N}king with m\textbf{U}l\textbf{T}imodal clu\textbf{E}~(MINUTE).
MINUTE uses Shared Normalization during both training and inference to rank
candidate moments from multiple videos to solve moment predict bias, making it
more efficient to predict target moment. In addition, Mutilmdaol Clue
Mining~(MCM) of MINUTE can discover key content of different modalities in
video to localize moment more accurately. MINUTE outperforms the baselines on
TVR and DiDeMo datasets, achieving a new state-of-the-art of VCMR. Our code
will be available at GitHub.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Priors are Powerful: Improving a Transformer for Multi-camera 3D  Detection with 2D Priors</b></summary>
  <p><b>编号</b>：[104]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13592</p>
  <p><b>作者</b>：Di Feng,  Francesco Ferroni</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Transfomer-based approaches advance, Transfomer-based approaches, development of multi-camera, academia and industry, approaches advance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transfomer-based approaches advance the recent development of multi-camera 3D
detection both in academia and industry. In a vanilla transformer architecture,
queries are randomly initialised and optimised for the whole dataset, without
considering the differences among input frames. In this work, we propose to
leverage the predictions from an image backbone, which is often highly
optimised for 2D tasks, as priors to the transformer part of a 3D detection
network. The method works by (1). augmenting image feature maps with 2D priors,
(2). sampling query locations via ray-casting along 2D box centroids, as well
as (3). initialising query features with object-level image features.
Experimental results shows that 2D priors not only help the model converge
faster, but also largely improve the baseline approach by up to 12% in terms of
average precision.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Zero3D: Semantic-Driven Multi-Category 3D Shape Generation</b></summary>
  <p><b>编号</b>：[105]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13591</p>
  <p><b>作者</b>：Bo Han,  Yitong Liu,  Yixuan Shen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：shape generation aims, generation aims, objects conditioned, shape, aims to generate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Semantic-driven 3D shape generation aims to generate 3D objects conditioned
on text. Previous works face problems with single-category generation,
low-frequency 3D details, and requiring a large number of paired datasets for
training. To tackle these challenges, we propose a multi-category conditional
diffusion model. Specifically, 1) to alleviate the problem of lack of
large-scale paired data, we bridge the text, 2D image and 3D shape based on the
pre-trained CLIP model, and 2) to obtain the multi-category 3D shape feature,
we apply the conditional flow model to generate 3D shape vector conditioned on
CLIP embedding. 3) to generate multi-category 3D shape, we employ the
hidden-layer diffusion model conditioned on the multi-category shape vector,
which greatly reduces the training time and memory consumption.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Sport Task: Fine Grained Action Detection and Classification of Table  Tennis Strokes from Videos for MediaEval 2022</b></summary>
  <p><b>编号</b>：[111]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13576</p>
  <p><b>作者</b>：Pierre-Etienne Martin (MPI-EVA),  Jordan Calandre (MIA),  Boris Mansencal (LaBRI),  Jenny Benois-Pineau (LaBRI),  Renaud Péteri (MIA),  Laurent Mascarilla (MIA),  Julien Morlier</p>
  <p><b>备注</b>：MediaEval 2022 Workshop, Jan 2023, Bergen, Norway. arXiv admin note: substantial text overlap with arXiv:2112.11384</p>
  <p><b>关键词</b>：widespread research topic, research topic, videos, Sports video analysis, widespread research</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sports video analysis is a widespread research topic. Its applications are
very diverse, like events detection during a match, video summary, or
fine-grained movement analysis of athletes. As part of the MediaEval 2022
benchmarking initiative, this task aims at detecting and classifying subtle
movements from sport videos. We focus on recordings of table tennis matches.
Conducted since 2019, this task provides a classification challenge from
untrimmed videos recorded under natural conditions with known temporal
boundaries for each stroke. Since 2021, the task also provides a stroke
detection challenge from unannotated, untrimmed videos. This year, the
training, validation, and test sets are enhanced to ensure that all strokes are
represented in each dataset. The dataset is now similar to the one used in [1,
2]. This research is intended to build tools for coaches and athletes who want
to further evaluate their sport performances.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：NP-Match: Towards a New Probabilistic Model for Semi-Supervised Learning</b></summary>
  <p><b>编号</b>：[114]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13569</p>
  <p><b>作者</b>：Jianfeng Wang,  Xiaolin Hu,  Thomas Lukasiewicz</p>
  <p><b>备注</b>：An journal version of our previous ICML 2022 paper arXiv:2207.01066 . Codes are available at: this https URL</p>
  <p><b>关键词</b>：semi-supervised image classification, semi-supervised image, image classification, recent years, Semi-supervised</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Semi-supervised learning (SSL) has been widely explored in recent years, and
it is an effective way of leveraging unlabeled data to reduce the reliance on
labeled data. In this work, we adjust neural processes (NPs) to the
semi-supervised image classification task, resulting in a new method named
NP-Match. NP-Match is suited to this task for two reasons. Firstly, NP-Match
implicitly compares data points when making predictions, and as a result, the
prediction of each unlabeled data point is affected by the labeled data points
that are similar to it, which improves the quality of pseudo-labels. Secondly,
NP-Match is able to estimate uncertainty that can be used as a tool for
selecting unlabeled samples with reliable pseudo-labels. Compared with
uncertainty-based SSL methods implemented with Monte-Carlo (MC) dropout,
NP-Match estimates uncertainty with much less computational overhead, which can
save time at both the training and the testing phases. We conducted extensive
experiments on five public datasets under three semi-supervised image
classification settings, namely, the standard semi-supervised image
classification, the imbalanced semi-supervised image classification, and the
multi-label semi-supervised image classification, and NP-Match outperforms
state-of-the-art (SOTA) approaches or achieves competitive results on them,
which shows the effectiveness of NP-Match and its potential for SSL. The codes
are at this https URL</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Lidar Upsampling with Sliced Wasserstein Distance</b></summary>
  <p><b>编号</b>：[116]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13558</p>
  <p><b>作者</b>：Artem Savkin,  Yida Wang,  Sebastian Wirkert,  Nassir Navab,  Federico Tombar</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：autonomous driving, important component, perception systems, systems in autonomous, Lidar</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Lidar became an important component of the perception systems in autonomous
driving. But challenges of training data acquisition and annotation made
emphasized the role of the sensor to sensor domain adaptation. In this work, we
address the problem of lidar upsampling. Learning on lidar point clouds is
rather a challenging task due to their irregular and sparse structure. Here we
propose a method for lidar point cloud upsampling which can reconstruct
fine-grained lidar scan patterns. The key idea is to utilize edge-aware dense
convolutions for both feature extraction and feature expansion. Additionally
applying a more accurate Sliced Wasserstein Distance facilitates learning of
the fine lidar sweep structures. This in turn enables our method to employ a
one-stage upsampling paradigm without the need for coarse and fine
reconstruction. We conduct several experiments to evaluate our method and
demonstrate that it provides better upsampling.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：NoiseTransfer: Image Noise Generation with Contrastive Embeddings</b></summary>
  <p><b>编号</b>：[118]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13554</p>
  <p><b>作者</b>：Seunghwan Lee,  Tae Hyun Kim</p>
  <p><b>备注</b>：ACCV 2022 oral</p>
  <p><b>关键词</b>：achieved impressive success, considerably large number, Deep image denoising, achieved impressive, impressive success</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep image denoising networks have achieved impressive success with the help
of a considerably large number of synthetic train datasets. However, real-world
denoising is a still challenging problem due to the dissimilarity between
distributions of real and synthetic noisy datasets. Although several real-world
noisy datasets have been presented, the number of train datasets (i.e., pairs
of clean and real noisy images) is limited, and acquiring more real noise
datasets is laborious and expensive. To mitigate this problem, numerous
attempts to simulate real noise models using generative models have been
studied. Nevertheless, previous works had to train multiple networks to handle
multiple different noise distributions. By contrast, we propose a new
generative model that can synthesize noisy images with multiple different noise
distributions. Specifically, we adopt recent contrastive learning to learn
distinguishable latent features of the noise. Moreover, our model can generate
new noisy images by transferring the noise characteristics solely from a single
reference noisy image. We demonstrate the accuracy and the effectiveness of our
noise model for both known and unknown noise removal.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Review of methods for automatic cerebral microbleeds detection</b></summary>
  <p><b>编号</b>：[120]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13549</p>
  <p><b>作者</b>：Maria Ferlin,  Zuzanna Klawikowska,  Michał Grochowski,  Małgorzata Grzywińska,  Edyta Szurowska</p>
  <p><b>备注</b>：32 pages, 6 figures, 3 tables, 174 references</p>
  <p><b>关键词</b>：Cerebral microbleeds detection, Cerebral microbleeds, challenging task, detect cerebral microbleeds, important and challenging</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cerebral microbleeds detection is an important and challenging task. With the
gaining popularity of the MRI, the ability to detect cerebral microbleeds also
raises. Unfortunately, for radiologists, it is a time-consuming and laborious
procedure. For this reason, various solutions to automate this process have
been proposed for several years, but none of them is currently used in medical
practice. In this context, the need to systematize the existing knowledge and
best practices has been recognized as a factor facilitating the imminent
synthesis of a real CMBs detection system practically applicable in medicine.
To the best of our knowledge, all available publications regarding automatic
cerebral microbleeds detection have been gathered, described, and assessed in
this paper in order to distinguish the current research state and provide a
starting point for future studies.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：AMD: Adaptive Masked Distillation for Object</b></summary>
  <p><b>编号</b>：[127]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13538</p>
  <p><b>作者</b>：Guang Yang,  Yin Tang,  Jun Li,  Jianhua Xu,  Xili Wan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：model compression paradigm, learn expressive features, feature-based knowledge distillation, general model compression, compression paradigm</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As a general model compression paradigm, feature-based knowledge distillation
allows the student model to learn expressive features from the teacher
counterpart. In this paper, we mainly focus on designing an effective
feature-distillation framework and propose a spatial-channel adaptive masked
distillation (AMD) network for object detection. More specifically, in order to
accurately reconstruct important feature regions, we first perform
attention-guided feature masking on the feature map of the student network,
such that we can identify the important features via spatially adaptive feature
masking instead of random masking in the previous methods. In addition, we
employ a simple and efficient module to allow the student network channel to be
adaptive, improving its model capability in object perception and detection. In
contrast to the previous methods, more crucial object-aware features can be
reconstructed and learned from the proposed network, which is conducive to
accurate object detection. The empirical experiments demonstrate the
superiority of our method: with the help of our proposed distillation method,
the student networks report 41.3\%, 42.4\%, and 42.7\% mAP scores when
RetinaNet, Cascade Mask-RCNN and RepPoints are respectively used as the teacher
framework for object detection, which outperforms the previous state-of-the-art
distillation methods including FGD and MGD.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Domain-Generalizable Multiple-Domain Clustering</b></summary>
  <p><b>编号</b>：[132]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13530</p>
  <p><b>作者</b>：Amit Rozner,  Barak Battash,  Lior Wolf,  Ofir Lindenbaum</p>
  <p><b>备注</b>：12 pages, 5 figures</p>
  <p><b>关键词</b>：analyzing scientific data, adequately analyzing scientific, Accurately clustering high-dimensional, clustering high-dimensional measurements, scientific data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Accurately clustering high-dimensional measurements is vital for adequately
analyzing scientific data. Deep learning machinery has remarkably improved
clustering capabilities in recent years due to its ability to extract
meaningful representations. In this work, we are given unlabeled samples from
multiple source domains, and we aim to learn a shared classifier that assigns
the examples to various clusters. Evaluation is done by using the classifier
for predicting cluster assignments in a previously unseen domain. This setting
generalizes the problem of unsupervised domain generalization to the case in
which no supervised learning samples are given (completely unsupervised).
Towards this goal, we present an end-to-end model and evaluate its capabilities
on several multi-domain image datasets. Specifically, we demonstrate that our
model is more accurate than schemes that require fine-tuning using samples from
the target domain or some level of supervision.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Fourier Sensitivity and Regularization of Computer Vision Models</b></summary>
  <p><b>编号</b>：[137]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13514</p>
  <p><b>作者</b>：Kiran Krishnamachari,  See-Kiong Ng,  Chuan-Sheng Foo</p>
  <p><b>备注</b>：Published in TMLR, this https URL</p>
  <p><b>关键词</b>：deep neural networks, Recent work, work has empirically, empirically shown, deep neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent work has empirically shown that deep neural networks latch on to the
Fourier statistics of training data and show increased sensitivity to
Fourier-basis directions in the input. Understanding and modifying this
Fourier-sensitivity of computer vision models may help improve their
robustness. Hence, in this paper we study the frequency sensitivity
characteristics of deep neural networks using a principled approach. We first
propose a basis trick, proving that unitary transformations of the
input-gradient of a function can be used to compute its gradient in the basis
induced by the transformation. Using this result, we propose a general measure
of any differentiable model's Fourier-sensitivity using the unitary
Fourier-transform of its input-gradient. When applied to deep neural networks,
we find that computer vision models are consistently sensitive to particular
frequencies dependent on the dataset, training method and architecture. Based
on this measure, we further propose a Fourier-regularization framework to
modify the Fourier-sensitivities and frequency bias of models. Using our
proposed regularizer-family, we demonstrate that deep neural networks obtain
improved classification accuracy on robustness evaluations.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Monocular Scene Reconstruction with 3D SDF Transformers</b></summary>
  <p><b>编号</b>：[141]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13510</p>
  <p><b>作者</b>：Weihao Yuan,  Xiaodong Gu,  Heng Li,  Zilong Dong,  Siyu Zhu</p>
  <p><b>备注</b>：Accepted to ICLR 2023</p>
  <p><b>关键词</b>：Monocular scene reconstruction, Monocular scene, posed images, images is challenging, challenging due</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Monocular scene reconstruction from posed images is challenging due to the
complexity of a large environment. Recent volumetric methods learn to directly
predict the TSDF volume and have demonstrated promising results in this task.
However, most methods focus on how to extract and fuse the 2D features to a 3D
feature volume, but none of them improve the way how the 3D volume is
aggregated. In this work, we propose an SDF transformer network, which replaces
the role of 3D CNN for better 3D feature aggregation. To reduce the explosive
computation complexity of the 3D multi-head attention, we propose a sparse
window attention module, where the attention is only calculated between the
non-empty voxels within a local window. Then a top-down-bottom-up 3D attention
network is built for 3D feature aggregation, where a dilate-attention structure
is proposed to prevent geometry degeneration, and two global modules are
employed to equip with global receptive fields. The experiments on multiple
datasets show that this 3D transformer network generates a more accurate and
complete reconstruction, which outperforms previous methods by a large margin.
Remarkably, the mesh accuracy is improved by 41.8%, and the mesh completeness
is improved by 25.3% on the ScanNet dataset. Project page:
this https URL.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Transfer Learning and Class Decomposition for Detecting the Cognitive  Decline of Alzheimer Disease</b></summary>
  <p><b>编号</b>：[144]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13504</p>
  <p><b>作者</b>：Maha M. Alwuthaynani,  Zahraa S. Abdallah,  Raul Santos-Rodriguez</p>
  <p><b>备注</b>：12 pages, 3 figures</p>
  <p><b>关键词</b>：Early diagnosis, essential in preventing, Alzheimer disease, Alzheimer, disease progression</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Early diagnosis of Alzheimer's disease (AD) is essential in preventing the
disease's progression. Therefore, detecting AD from neuroimaging data such as
structural magnetic resonance imaging (sMRI) has been a topic of intense
investigation in recent years. Deep learning has gained considerable attention
in Alzheimer's detection. However, training a convolutional neural network from
scratch is challenging since it demands more computational time and a
significant amount of annotated data. By transferring knowledge learned from
other image recognition tasks to medical image classification, transfer
learning can provide a promising and effective solution. Irregularities in the
dataset distribution present another difficulty. Class decomposition can tackle
this issue by simplifying learning a dataset's class boundaries. Motivated by
these approaches, this paper proposes a transfer learning method using class
decomposition to detect Alzheimer's disease from sMRI images. We use two
ImageNet-trained architectures: VGG19 and ResNet50, and an entropy-based
technique to determine the most informative images. The proposed model achieved
state-of-the-art performance in the Alzheimer's disease (AD) vs mild cognitive
impairment (MCI) vs cognitively normal (CN) classification task with a 3\%
increase in accuracy from what is reported in the literature.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Adversarial Training of Self-supervised Monocular Depth Estimation  against Physical-World Attacks</b></summary>
  <p><b>编号</b>：[149]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13487</p>
  <p><b>作者</b>：Zhiyuan Cheng,  James Liang,  Guanhong Tao,  Dongfang Liu,  Xiangyu Zhang</p>
  <p><b>备注</b>：Accepted to ICLR2023 (Spotlight)</p>
  <p><b>关键词</b>：Monocular Depth Estimation, Depth Estimation, MDE, autonomous driving, critical component</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Monocular Depth Estimation (MDE) is a critical component in applications such
as autonomous driving. There are various attacks against MDE networks. These
attacks, especially the physical ones, pose a great threat to the security of
such systems. Traditional adversarial training method requires ground-truth
labels hence cannot be directly applied to self-supervised MDE that does not
have ground-truth depth. Some self-supervised model hardening techniques (e.g.,
contrastive learning) ignore the domain knowledge of MDE and can hardly achieve
optimal performance. In this work, we propose a novel adversarial training
method for self-supervised MDE models based on view synthesis without using
ground-truth depth. We improve adversarial robustness against physical-world
attacks using L0-norm-bounded perturbation in training. We compare our method
with supervised learning based and contrastive learning based methods that are
tailored for MDE. Results on two representative MDE networks show that we
achieve better robustness against various adversarial attacks with nearly no
benign performance degradation.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：CRC-RL: A Novel Visual Feature Representation Architecture for  Unsupervised Reinforcement Learning</b></summary>
  <p><b>编号</b>：[153]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13473</p>
  <p><b>作者</b>：Darshita Jain,  Anima Majumder,  Samrat Dutta,  Swagat Kumar</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：paper addresses, aim to improve, improve the performance, visual feature representation, called CRC loss</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper addresses the problem of visual feature representation learning
with an aim to improve the performance of end-to-end reinforcement learning
(RL) models. Specifically, a novel architecture is proposed that uses a
heterogeneous loss function, called CRC loss, to learn improved visual features
which can then be used for policy learning in RL. The CRC-loss function is a
combination of three individual loss functions, namely, contrastive,
reconstruction and consistency loss. The feature representation is learned in
parallel to the policy learning while sharing the weight updates through a
Siamese Twin encoder model. This encoder model is augmented with a decoder
network and a feature projection network to facilitate computation of the above
loss components. Through empirical analysis involving latent feature
visualization, an attempt is made to provide an insight into the role played by
this loss function in learning new action-dependent features and how they are
linked to the complexity of the problems being solved. The proposed
architecture, called CRC-RL, is shown to outperform the existing
state-of-the-art methods on the challenging Deep mind control suite
environments by a significant margin thereby creating a new benchmark in this
field.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Learning Generalized Hybrid Proximity Representation for Image  Recognition</b></summary>
  <p><b>编号</b>：[156]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13459</p>
  <p><b>作者</b>：Zhiyuan Li,  Anca Ralescu</p>
  <p><b>备注</b>：The paper has been accepted by the IEEE ICTAI 2022</p>
  <p><b>关键词</b>：techniques received attention, unsupervised learning tasks, learning techniques received, deep metric learning, metric learning techniques</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, deep metric learning techniques received attention, as the learned
distance representations are useful to capture the similarity relationship
among samples and further improve the performance of various of supervised or
unsupervised learning tasks. We propose a novel supervised metric learning
method that can learn the distance metrics in both geometric and probabilistic
space for image recognition. In contrast to the previous metric learning
methods which usually focus on learning the distance metrics in Euclidean
space, our proposed method is able to learn better distance representation in a
hybrid approach. To achieve this, we proposed a Generalized Hybrid Metric Loss
(GHM-Loss) to learn the general hybrid proximity features from the image data
by controlling the trade-off between geometric proximity and probabilistic
proximity. To evaluate the effectiveness of our method, we first provide
theoretical derivations and proofs of the proposed loss function, then we
perform extensive experiments on two public datasets to show the advantage of
our method compared to other state-of-the-art metric learning methods.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：A Survey of Explainable AI in Deep Visual Modeling: Methods and Metrics</b></summary>
  <p><b>编号</b>：[165]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13445</p>
  <p><b>作者</b>：Naveed Akhtar</p>
  <p><b>备注</b>：Short accessible survey (9pgs)</p>
  <p><b>关键词</b>：Deep visual models, high-stake domains, widespread applications, applications in high-stake, Deep visual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep visual models have widespread applications in high-stake domains. Hence,
their black-box nature is currently attracting a large interest of the research
community. We present the first survey in Explainable AI that focuses on the
methods and metrics for interpreting deep visual models. Covering the landmark
contributions along the state-of-the-art, we not only provide a taxonomic
organization of the existing techniques, but also excavate a range of
evaluation metrics and collate them as measures of different properties of
model explanations. Along the insightful discussion on the current trends, we
also discuss the challenges and future avenues for this research direction.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Rethinking Soft Label in Label Distribution Learning Perspective</b></summary>
  <p><b>编号</b>：[166]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13444</p>
  <p><b>作者</b>：Seungbum Hong,  Jihun Yoon,  Bogyu Park,  Min-Kook Choi</p>
  <p><b>备注</b>：11 pages main manuscript + references and 11 pages supplementary materials</p>
  <p><b>关键词</b>：convolutional neural networks, early convolutional neural, neural networks, CNN training, model calibration</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The primary goal of training in early convolutional neural networks (CNN) is
the higher generalization performance of the model. However, as the expected
calibration error (ECE), which quantifies the explanatory power of model
inference, was recently introduced, research on training models that can be
explained is in progress. We hypothesized that a gap in supervision criteria
during training and inference leads to overconfidence, and investigated that
performing label distribution learning (LDL) would enhance the model
calibration in CNN training. To verify this assumption, we used a simple LDL
setting with recent data augmentation techniques. Based on a series of
experiments, the following results are obtained: 1) State-of-the-art KD methods
significantly impede model calibration. 2) Training using LDL with recent data
augmentation can have excellent effects on model calibration and even in
generalization performance. 3) Online LDL brings additional improvements in
model calibration and accuracy with long training, especially in large-size
models. Using the proposed approach, we simultaneously achieved a lower ECE and
higher generalization performance for the image classification datasets
CIFAR10, 100, STL10, and ImageNet. We performed several visualizations and
analyses and witnessed several interesting behaviors in CNN training with the
LDL.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face  Synthesis</b></summary>
  <p><b>编号</b>：[171]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13430</p>
  <p><b>作者</b>：Zhenhui Ye,  Ziyue Jiang,  Yi Ren,  Jinglin Liu,  JinZheng He,  Zhou Zhao</p>
  <p><b>备注</b>：Accepted by ICLR2023. Project page: this https URL</p>
  <p><b>关键词</b>：Generating photo-realistic video, photo-realistic video portrait, arbitrary speech audio, Generating photo-realistic, virtual reality</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generating photo-realistic video portrait with arbitrary speech audio is a
crucial problem in film-making and virtual reality. Recently, several works
explore the usage of neural radiance field in this task to improve 3D realness
and image fidelity. However, the generalizability of previous NeRF-based
methods to out-of-domain audio is limited by the small scale of training data.
In this work, we propose GeneFace, a generalized and high-fidelity NeRF-based
talking face generation method, which can generate natural results
corresponding to various out-of-domain audio. Specifically, we learn a
variaitional motion generator on a large lip-reading corpus, and introduce a
domain adaptative post-net to calibrate the result. Moreover, we learn a
NeRF-based renderer conditioned on the predicted facial motion. A head-aware
torso-NeRF is proposed to eliminate the head-torso separation problem.
Extensive experiments show that our method achieves more generalized and
high-fidelity talking face generation compared to previous methods.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Contrast and Clustering: Learning Neighborhood Pair Representation for  Source-free Domain Adaptation</b></summary>
  <p><b>编号</b>：[172]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13428</p>
  <p><b>作者</b>：Yuqi Chen,  Xiangbin Zhu,  Yonggang Li,  Yingjian Li,  Yuanwang Wei,  Haojie Fang</p>
  <p><b>备注</b>：conference paper</p>
  <p><b>关键词</b>：machine learning community, learning community, attracted a great, great deal, deal of attention</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Domain adaptation has attracted a great deal of attention in the machine
learning community, but it requires access to source data, which often raises
concerns about data privacy. We are thus motivated to address these issues and
propose a simple yet efficient method. This work treats domain adaptation as an
unsupervised clustering problem and trains the target model without access to
the source data. Specifically, we propose a loss function called contrast and
clustering (CaC), where a positive pair term pulls neighbors belonging to the
same class together in the feature space to form clusters, while a negative
pair term pushes samples of different classes apart. In addition, extended
neighbors are taken into account by querying the nearest neighbor indexes in
the memory bank to mine for more valuable negative pairs. Extensive experiments
on three common benchmarks, VisDA, Office-Home and Office-31, demonstrate that
our method achieves state-of-the-art performance. The code will be made
publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Anomaly Segmentation for High-Resolution Remote Sensing Images Based on  Pixel Descriptors</b></summary>
  <p><b>编号</b>：[175]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13422</p>
  <p><b>作者</b>：Jingtao Li,  Xinyu Wang,  Hengwei Zhao,  Shaoyu Wang,  Yanfei Zhong</p>
  <p><b>备注</b>：to be published in AAAI2023</p>
  <p><b>关键词</b>：high spatial resolution, segmenting anomaly patterns, remote sensing imagery, Anomaly segmentation, Earth vision applications</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Anomaly segmentation in high spatial resolution (HSR) remote sensing imagery
is aimed at segmenting anomaly patterns of the earth deviating from normal
patterns, which plays an important role in various Earth vision applications.
However, it is a challenging task due to the complex distribution and the
irregular shapes of objects, and the lack of abnormal samples. To tackle these
problems, an anomaly segmentation model based on pixel descriptors (ASD) is
proposed for anomaly segmentation in HSR imagery. Specifically, deep one-class
classification is introduced for anomaly segmentation in the feature space with
discriminative pixel descriptors. The ASD model incorporates the data argument
for generating virtual ab-normal samples, which can force the pixel descriptors
to be compact for normal data and meanwhile to be diverse to avoid the model
collapse problems when only positive samples participated in the training. In
addition, the ASD introduced a multi-level and multi-scale feature extraction
strategy for learning the low-level and semantic information to make the pixel
descriptors feature-rich. The proposed ASD model was validated using four HSR
datasets and compared with the recent state-of-the-art models, showing its
potential value in Earth vision applications.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Recurrent Structure Attention Guidance for Depth Super-Resolution</b></summary>
  <p><b>编号</b>：[178]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13419</p>
  <p><b>作者</b>：Jiayi Yuan,  Haobo Jiang,  Xiang Li,  Jianjun Qian,  Jun Li,  Jian Yang</p>
  <p><b>备注</b>：Accepted by AAAI-2023</p>
  <p><b>关键词</b>：effective strategy, image features, depth, maps, depth maps</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image guidance is an effective strategy for depth super-resolution.
Generally, most existing methods employ hand-crafted operators to decompose the
high-frequency (HF) and low-frequency (LF) ingredients from low-resolution
depth maps and guide the HF ingredients by directly concatenating them with
image features. However, the hand-designed operators usually cause inferior HF
maps (e.g., distorted or structurally missing) due to the diverse appearance of
complex depth maps. Moreover, the direct concatenation often results in weak
guidance because not all image features have a positive effect on the HF maps.
In this paper, we develop a recurrent structure attention guided (RSAG)
framework, consisting of two important parts. First, we introduce a deep
contrastive network with multi-scale filters for adaptive frequency-domain
separation, which adopts contrastive networks from large filters to small ones
to calculate the pixel contrasts for adaptive high-quality HF predictions.
Second, instead of the coarse concatenation guidance, we propose a recurrent
structure attention block, which iteratively utilizes the latest depth
estimation and the image features to jointly select clear patterns and
boundaries, aiming at providing refined guidance for accurate depth recovery.
In addition, we fuse the features of HF maps to enhance the edge structures in
the decomposed LF maps. Extensive experiments show that our approach obtains
superior performance compared with state-of-the-art depth super-resolution
methods.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete  Annotations</b></summary>
  <p><b>编号</b>：[179]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13418</p>
  <p><b>作者</b>：Yuanhong Chen,  Yuyuan Liu,  Chong Wang,  Michael Elliott,  Chun Fung Kwok,  Carlos Pe na-Solorzano,  Yu Tian,  Fengbei Liu,  Helen Frazer,  Davis J. McCarthy,  Gustavo Carneiro</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：screening mammogram datasets, real-world screening mammogram, fully annotated datasets, images are labelled, weakly annotated subset</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Methods to detect malignant lesions from screening mammograms are usually
trained with fully annotated datasets, where images are labelled with the
localisation and classification of cancerous lesions. However, real-world
screening mammogram datasets commonly have a subset that is fully annotated and
another subset that is weakly annotated with just the global classification
(i.e., without lesion localisation). Given the large size of such datasets,
researchers usually face a dilemma with the weakly annotated subset: to not use
it or to fully annotate it. The first option will reduce detection accuracy
because it does not use the whole dataset, and the second option is too
expensive given that the annotation needs to be done by expert radiologists. In
this paper, we propose a middle-ground solution for the dilemma, which is to
formulate the training as a weakly- and semi-supervised learning problem that
we refer to as malignant breast lesion detection with incomplete annotations.
To address this problem, our new method comprises two stages, namely: 1)
pre-training a multi-view mammogram classifier with weak supervision from the
whole dataset, and 2) extending the trained classifier to become a multi-view
detector that is trained with semi-supervised student-teacher learning, where
the training set contains fully and weakly-annotated mammograms. We provide
extensive detection results on two real-world screening mammogram datasets
containing incomplete annotations, and show that our proposed approach achieves
state-of-the-art results in the detection of malignant breast lesions with
incomplete annotations.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：Structure Flow-Guided Network for Real Depth Super-Resolution</b></summary>
  <p><b>编号</b>：[180]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13416</p>
  <p><b>作者</b>：Jiayi Yuan,  Haobo Jiang,  Xiang Li,  Jianjun Qian,  Jun Li,  Jian Yang</p>
  <p><b>备注</b>：Accepted by AAAI-2023</p>
  <p><b>关键词</b>：challenging task due, edge noise caused, unlike synthetic settings, real-world low-resolution, Real depth super-resolution</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Real depth super-resolution (DSR), unlike synthetic settings, is a
challenging task due to the structural distortion and the edge noise caused by
the natural degradation in real-world low-resolution (LR) depth maps. These
defeats result in significant structure inconsistency between the depth map and
the RGB guidance, which potentially confuses the RGB-structure guidance and
thereby degrades the DSR quality. In this paper, we propose a novel structure
flow-guided DSR framework, where a cross-modality flow map is learned to guide
the RGB-structure information transferring for precise depth upsampling.
Specifically, our framework consists of a cross-modality flow-guided upsampling
network (CFUNet) and a flow-enhanced pyramid edge attention network (PEANet).
CFUNet contains a trilateral self-attention module combining both the geometric
and semantic correlations for reliable cross-modality flow learning. Then, the
learned flow maps are combined with the grid-sampling mechanism for coarse
high-resolution (HR) depth prediction. PEANet targets at integrating the
learned flow map as the edge attention into a pyramid network to hierarchically
learn the edge-focused guidance feature for depth edge refinement. Extensive
experiments on real and synthetic DSR datasets verify that our approach
achieves excellent performance compared to state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Few-Shot Object Detection via Variational Feature Aggregation</b></summary>
  <p><b>编号</b>：[184]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13411</p>
  <p><b>作者</b>：Jiaming Han,  Yuqiang Ren,  Jian Ding,  Ke Yan,  Gui-Song Xia</p>
  <p><b>备注</b>：Accepted by AAAI2023</p>
  <p><b>关键词</b>：examples,the learned models, few-shot object detectors, feature aggregation, trained with abundant, examples,the learned</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As few-shot object detectors are often trained with abundant base samples and
fine-tuned on few-shot novel examples,the learned models are usually biased to
base classes and sensitive to the variance of novel examples. To address this
issue, we propose a meta-learning framework with two novel feature aggregation
schemes. More precisely, we first present a Class-Agnostic Aggregation (CAA)
method, where the query and support features can be aggregated regardless of
their categories. The interactions between different classes encourage
class-agnostic representations and reduce confusion between base and novel
classes. Based on the CAA, we then propose a Variational Feature Aggregation
(VFA) method, which encodes support examples into class-level support features
for robust feature aggregation. We use a variational autoencoder to estimate
class distributions and sample variational features from distributions that are
more robust to the variance of support examples. Besides, we decouple
classification and regression tasks so that VFA is performed on the
classification branch without affecting object localization. Extensive
experiments on PASCAL VOC and COCO demonstrate that our method significantly
outperforms a strong baseline (up to 16\%) and previous state-of-the-art
methods (4\% in average). Code will be available at:
\url{this https URL}</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：A Modular Multi-stage Lightweight Graph Transformer Network for Human  Pose and Shape Estimation from 2D Human Pose</b></summary>
  <p><b>编号</b>：[186]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13403</p>
  <p><b>作者</b>：Ayman Ali,  Ekkasit Pinyoanuntapong,  Pu Wang,  Mohsen Dorodchi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：existing deep learning-based, deep learning-based human, challenge faced, faced by existing, existing deep</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this research, we address the challenge faced by existing deep
learning-based human mesh reconstruction methods in balancing accuracy and
computational efficiency. These methods typically prioritize accuracy,
resulting in large network sizes and excessive computational complexity, which
may hinder their practical application in real-world scenarios, such as virtual
reality systems. To address this issue, we introduce a modular multi-stage
lightweight graph-based transformer network for human pose and shape estimation
from 2D human pose, a pose-based human mesh reconstruction approach that
prioritizes computational efficiency without sacrificing reconstruction
accuracy. Our method consists of a 2D-to-3D lifter module that utilizes graph
transformers to analyze structured and implicit joint correlations in 2D human
poses, and a mesh regression module that combines the extracted pose features
with a mesh template to produce the final human mesh parameters.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：ReGANIE: Rectifying GAN Inversion Errors for Accurate Real Image Editing</b></summary>
  <p><b>编号</b>：[187]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13402</p>
  <p><b>作者</b>：Bingchuan Li,  Tianxiang Ma,  Peng Zhang,  Miao Hua,  Wei Liu,  Qian He,  Zili Yi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：latent style space.However, semantic-rich latent style, latent space encounters, StyleGAN family succeed, high-fidelity image generation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The StyleGAN family succeed in high-fidelity image generation and allow for
flexible and plausible editing of generated images by manipulating the
semantic-rich latent style space.However, projecting a real image into its
latent space encounters an inherent trade-off between inversion quality and
editability. Existing encoder-based or optimization-based StyleGAN inversion
methods attempt to mitigate the trade-off but see limited performance. To
fundamentally resolve this problem, we propose a novel two-phase framework by
designating two separate networks to tackle editing and reconstruction
respectively, instead of balancing the two. Specifically, in Phase I, a
W-space-oriented StyleGAN inversion network is trained and used to perform
image inversion and editing, which assures the editability but sacrifices
reconstruction quality. In Phase II, a carefully designed rectifying network is
utilized to rectify the inversion errors and perform ideal reconstruction.
Experimental results show that our approach yields near-perfect reconstructions
without sacrificing the editability, thus allowing accurate manipulation of
real images. Further, we evaluate the performance of our rectifying network,
and see great generalizability towards unseen manipulation types and
out-of-domain images.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：Fisheye traffic data set of point center markers</b></summary>
  <p><b>编号</b>：[195]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13385</p>
  <p><b>作者</b>：Chung-I Huang,  Wei-Yu Chen,  Wei Jan Ko,  Jih-Sheng Chang,  Chen-Kai Sun,  Hui Hung Yu,  Fang-Pang Lin</p>
  <p><b>备注</b>：this https URL</p>
  <p><b>关键词</b>：open data-market platform, study presents, presents an open, open data-market, data-market platform</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This study presents an open data-market platform and a dataset containing
160,000 markers and 18,000 images. We hope that this dataset will bring more
new data value and applications In this paper, we introduce the format and
usage of the dataset, and we show a demonstration of deep learning vehicle
detection trained by this dataset.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：GaitSADA: Self-Aligned Domain Adaptation for mmWave Gait Recognition</b></summary>
  <p><b>编号</b>：[196]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13384</p>
  <p><b>作者</b>：Ekkasit Pinyoanuntapong (1),  Ayman Ali (1),  Kalvik Jakkala (1),  Pu Wang (1),  Minwoo Lee (1),  Qucheng Peng (2),  Chen Chen (2),  Zhi Sun (3) ((1) University of North Carolina at Charlotte, (2) University of Central Florida, (3) Tsinghua University)</p>
  <p><b>备注</b>：Submitted to ACM Transactions on Sensor Networks (TOSN)</p>
  <p><b>关键词</b>：radar return signals, radar-based gait recognition, captures human gait, mmWave radar return, return signals</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>mmWave radar-based gait recognition is a novel user identification method
that captures human gait biometrics from mmWave radar return signals. This
technology offers privacy protection and is resilient to weather and lighting
conditions. However, its generalization performance is yet unknown and limits
its practical deployment. To address this problem, in this paper, a
non-synthetic dataset is collected and analyzed to reveal the presence of
spatial and temporal domain shifts in mmWave gait biometric data, which
significantly impacts identification accuracy. To address this issue, a novel
self-aligned domain adaptation method called GaitSADA is proposed. GaitSADA
improves system generalization performance by using a two-stage semi-supervised
model training approach. The first stage uses semi-supervised contrastive
learning and the second stage uses semi-supervised consistency training with
centroid alignment. Extensive experiments show that GaitSADA outperforms
representative domain adaptation methods by an average of 15.41% in low data
regimes.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：When Source-Free Domain Adaptation Meets Learning with Noisy Labels</b></summary>
  <p><b>编号</b>：[199]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13381</p>
  <p><b>作者</b>：Li Yi,  Gezheng Xu,  Pengcheng Xu,  Jiaqi Li,  Ruizhi Pu,  Charles Ling,  A. Ian McLeod,  Boyu Wang</p>
  <p><b>备注</b>：33 pages, 16 figures, accepted by ICLR 2023</p>
  <p><b>关键词</b>：source-free domain adaptation, meaningful cluster structures, label noise, private source data, unlabeled target domain</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent state-of-the-art source-free domain adaptation (SFDA) methods have
focused on learning meaningful cluster structures in the feature space, which
have succeeded in adapting the knowledge from source domain to unlabeled target
domain without accessing the private source data. However, existing methods
rely on the pseudo-labels generated by source models that can be noisy due to
domain shift. In this paper, we study SFDA from the perspective of learning
with label noise (LLN). Unlike the label noise in the conventional LLN
scenario, we prove that the label noise in SFDA follows a different
distribution assumption. We also prove that such a difference makes existing
LLN methods that rely on their distribution assumptions unable to address the
label noise in SFDA. Empirical evidence suggests that only marginal
improvements are achieved when applying the existing LLN methods to solve the
SFDA problem. On the other hand, although there exists a fundamental difference
between the label noise in the two scenarios, we demonstrate theoretically that
the early-time training phenomenon (ETP), which has been previously observed in
conventional label noise settings, can also be observed in the SFDA problem.
Extensive experiments demonstrate significant improvements to existing SFDA
algorithms by leveraging ETP to address the label noise in SFDA.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Quantized Neural Networks for Low-Precision Accumulation with Guaranteed  Overflow Avoidance</b></summary>
  <p><b>编号</b>：[202]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13376</p>
  <p><b>作者</b>：Ian Colbert,  Alessandro Pappalardo,  Jakoba Petri-Koenig</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：guarantees avoiding numerical, avoiding numerical overflow, quantization-aware training algorithm, introduce a quantization-aware, guarantees avoiding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce a quantization-aware training algorithm that guarantees avoiding
numerical overflow when reducing the precision of accumulators during
inference. We leverage weight normalization as a means of constraining
parameters during training using accumulator bit width bounds that we derive.
We evaluate our algorithm across multiple quantized models that we train for
different tasks, showing that our approach can reduce the precision of
accumulators while maintaining model accuracy with respect to a floating-point
baseline. We then show that this reduction translates to increased design
efficiency for custom FPGA-based accelerators. Finally, we show that our
algorithm not only constrains weights to fit into an accumulator of
user-defined bit width, but also increases the sparsity and compressibility of
the resulting weights. Across all of our benchmark models trained with 8-bit
weights and activations, we observe that constraining the hidden layers of
quantized neural networks to fit into 16-bit accumulators yields an average
98.2% sparsity with an estimated compression rate of 46.5x all while
maintaining 99.2% of the floating-point performance.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Iterative Loop Learning Combining Self-Training and Active Learning for  Domain Adaptive Semantic Segmentation</b></summary>
  <p><b>编号</b>：[209]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13361</p>
  <p><b>作者</b>：Licong Guan,  Xue Yuan</p>
  <p><b>备注</b>：11 pages,5 figures</p>
  <p><b>关键词</b>：active learning, massive unlabeled data, alleviate this problem, proposed to alleviate, learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, self-training and active learning have been proposed to alleviate
this problem. Self-training can improve model accuracy with massive unlabeled
data, but some pseudo labels containing noise would be generated with limited
or imbalanced training data. And there will be suboptimal models if human
guidance is absent. Active learning can select more effective data to
intervene, while the model accuracy can not be improved because the massive
unlabeled data are not used. And the probability of querying sub-optimal
samples will increase when the domain difference is too large, increasing
annotation cost. This paper proposes an iterative loop learning method
combining Self-Training and Active Learning (STAL) for domain adaptive semantic
segmentation. The method first uses self-training to learn massive unlabeled
data to improve model accuracy and provide more accurate selection models for
active learning. Secondly, combined with the sample selection strategy of
active learning, manual intervention is used to correct the self-training
learning. Iterative loop to achieve the best performance with minimal label
cost. Extensive experiments show that our method establishes state-of-the-art
performance on tasks of GTAV to Cityscapes, SYNTHIA to Cityscapes, improving by
4.9% mIoU and 5.2% mIoU, compared to the previous best method, respectively.
Code will be available.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Skeleton-based Human Action Recognition via Convolutional Neural  Networks (CNN)</b></summary>
  <p><b>编号</b>：[210]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13360</p>
  <p><b>作者</b>：Ayman Ali,  Ekkasit Pinyoanuntapong,  Pu Wang,  Mohsen Dorodchi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：including computational efficiency, skeleton-based action recognition, including computational, computational efficiency, illumination invariance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, there has been a remarkable increase in the interest towards
skeleton-based action recognition within the research community, owing to its
various advantageous features, including computational efficiency,
representative features, and illumination invariance. Despite this, researchers
continue to explore and investigate the most optimal way to represent human
actions through skeleton representation and the extracted features. As a
result, the growth and availability of human action recognition datasets have
risen substantially. In addition, deep learning-based algorithms have gained
widespread popularity due to the remarkable advancements in various computer
vision tasks. Most state-of-the-art contributions in skeleton-based action
recognition incorporate a Graph Neural Network (GCN) architecture for
representing the human body and extracting features. Our research demonstrates
that Convolutional Neural Networks (CNNs) can attain comparable results to GCN,
provided that the proper training techniques, augmentations, and optimizers are
applied. Our approach has been rigorously validated, and we have achieved a
score of 95% on the NTU-60 dataset</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing</b></summary>
  <p><b>编号</b>：[211]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13359</p>
  <p><b>作者</b>：Guoyang Xie,  Jinbao Wang,  Jiaqi Liu,  Jiayi Lyu,  Yong Liu,  Chengjie Wang,  Feng Zheng,  Yaochu Jin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：vital computer vision, computer vision task, industrial manufacturing, emerging and vital, vital computer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image anomaly detection (IAD) is an emerging and vital computer vision task
in industrial manufacturing (IM). Recently many advanced algorithms have been
published, but their performance deviates greatly. We realize that the lack of
actual IM settings most probably hinders the development and usage of these
methods in real-world applications. As far as we know, IAD methods are not
evaluated systematically. As a result, this makes it difficult for researchers
to analyze them because they are designed for different or special cases. To
solve this problem, we first propose a uniform IM setting to assess how well
these algorithms perform, which includes several aspects, i.e., various levels
of supervision (unsupervised vs. semi-supervised), few-shot learning, continual
learning, noisy labels, memory usage, and inference speed. Moreover, we
skillfully build a comprehensive image anomaly detection benchmark (IM-IAD)
that includes 16 algorithms on 7 mainstream datasets with uniform settings. Our
extensive experiments (17,017 in total) provide in-depth insights for IAD
algorithm redesign or selection under the IM setting. Next, the proposed
benchmark IM-IAD gives challenges as well as directions for the future. To
foster reproducibility and accessibility, the source code of IM-IAD is uploaded
on the website, this https URL.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Hierarchical Disentangled Representation for Invertible Image Denoising  and Beyond</b></summary>
  <p><b>编号</b>：[212]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13358</p>
  <p><b>作者</b>：Wenchao Du,  Hu Chen,  Yi Zhang,  H. Yang</p>
  <p><b>备注</b>：Technical Report</p>
  <p><b>关键词</b>：ill-posed problem due, typical ill-posed problem, complex degradation, typical ill-posed, due to complex</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image denoising is a typical ill-posed problem due to complex degradation.
Leading methods based on normalizing flows have tried to solve this problem
with an invertible transformation instead of a deterministic mapping. However,
the implicit bijective mapping is not explored well. Inspired by a latent
observation that noise tends to appear in the high-frequency part of the image,
we propose a fully invertible denoising method that injects the idea of
disentangled learning into a general invertible neural network to split noise
from the high-frequency part. More specifically, we decompose the noisy image
into clean low-frequency and hybrid high-frequency parts with an invertible
transformation and then disentangle case-specific noise and high-frequency
components in the latent space. In this way, denoising is made tractable by
inversely merging noiseless low and high-frequency parts. Furthermore, we
construct a flexible hierarchical disentangling framework, which aims to
decompose most of the low-frequency image information while disentangling noise
from the high-frequency part in a coarse-to-fine manner. Extensive experiments
on real image denoising, JPEG compressed artifact removal, and medical low-dose
CT image restoration have demonstrated that the proposed method achieves
competing performance on both quantitative metrics and visual quality, with
significantly less computational cost.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Inference Time Evidences of Adversarial Attacks for Forensic on  Transformers</b></summary>
  <p><b>编号</b>：[213]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13356</p>
  <p><b>作者</b>：Hugo Lemarchant,  Liangzi Li,  Yiming Qian,  Yuta Nakashima,  Hajime Nagahara</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Vision Transformers, vision tasks, popular paradigm, Transformers, adversarial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision Transformers (ViTs) are becoming a very popular paradigm for vision
tasks as they achieve state-of-the-art performance on image classification.
However, although early works implied that this network structure had increased
robustness against adversarial attacks, some works argue ViTs are still
vulnerable. This paper presents our first attempt toward detecting adversarial
attacks during inference time using the network's input and outputs as well as
latent features. We design four quantifications (or derivatives) of input,
output, and latent vectors of ViT-based models that provide a signature of the
inference, which could be beneficial for the attack detection, and empirically
study their behavior over clean samples and adversarial samples. The results
demonstrate that the quantifications from input (images) and output (posterior
probabilities) are promising for distinguishing clean and adversarial samples,
while latent vectors offer less discriminative power, though they give some
insights on how adversarial perturbations work.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Few-Shot Image-to-Semantics Translation for Policy Transfer in  Reinforcement Learning</b></summary>
  <p><b>编号</b>：[221]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13343</p>
  <p><b>作者</b>：Rei Sato,  Kazuto Fukuchi,  Jun Sakuma,  Youhei Akimoto</p>
  <p><b>备注</b>：The 2022 International Joint Conference on Neural Networks (IJCNN2022)</p>
  <p><b>关键词</b>：vision-based robotics control, robotics control agents, mitigate learning difficulties, investigate policy transfer, translation to mitigate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We investigate policy transfer using image-to-semantics translation to
mitigate learning difficulties in vision-based robotics control agents. This
problem assumes two environments: a simulator environment with semantics, that
is, low-dimensional and essential information, as the state space, and a
real-world environment with images as the state space. By learning mapping from
images to semantics, we can transfer a policy, pre-trained in the simulator, to
the real world, thereby eliminating real-world on-policy agent interactions to
learn, which are costly and risky. In addition, using image-to-semantics
mapping is advantageous in terms of the computational efficiency to train the
policy and the interpretability of the obtained policy over other types of
sim-to-real transfer strategies. To tackle the main difficulty in learning
image-to-semantics mapping, namely the human annotation cost for producing a
training dataset, we propose two techniques: pair augmentation with the
transition function in the simulator environment and active learning. We
observed a reduction in the annotation cost without a decline in the
performance of the transfer, and the proposed approach outperformed the
existing approach without annotation.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Continuous Spatiotemporal Transformers</b></summary>
  <p><b>编号</b>：[224]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13338</p>
  <p><b>作者</b>：Antonio H. de O. Fonseca,  Emanuele Zappala,  Josue Ortega Caro,  David van Dijk</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：dynamical systems, continuous, systems, fundamental challenge, successful in NLP</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Modeling spatiotemporal dynamical systems is a fundamental challenge in
machine learning. Transformer models have been very successful in NLP and
computer vision where they provide interpretable representations of data.
However, a limitation of transformers in modeling continuous dynamical systems
is that they are fundamentally discrete time and space models and thus have no
guarantees regarding continuous sampling. To address this challenge, we present
the Continuous Spatiotemporal Transformer (CST), a new transformer architecture
that is designed for the modeling of continuous systems. This new framework
guarantees a continuous and smooth output via optimization in Sobolev space. We
benchmark CST against traditional transformers as well as other spatiotemporal
dynamics modeling methods and achieve superior performance in a number of tasks
on synthetic and real systems, including learning brain dynamics from calcium
imaging data.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：DAFD: Domain Adaptation via Feature Disentanglement for Image  Classification</b></summary>
  <p><b>编号</b>：[225]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13337</p>
  <p><b>作者</b>：Zhize Wu,  Changjiang Du,  Le Zou,  Ming Tan,  Tong Xu,  Fan Cheng,  Fudong Nian,  Thomas Weise</p>
  <p><b>备注</b>：10 pages, 7 figures</p>
  <p><b>关键词</b>：good feature representation, category-relevant features, domain, image classification, features</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A good feature representation is the key to image classification. In
practice, image classifiers may be applied in scenarios different from what
they have been trained on. This so-called domain shift leads to a significant
performance drop in image classification. Unsupervised domain adaptation (UDA)
reduces the domain shift by transferring the knowledge learned from a labeled
source domain to an unlabeled target domain. We perform feature disentanglement
for UDA by distilling category-relevant features and excluding
category-irrelevant features from the global feature maps. This disentanglement
prevents the network from overfitting to category-irrelevant information and
makes it focus on information useful for classification. This reduces the
difficulty of domain alignment and improves the classification accuracy on the
target domain. We propose a coarse-to-fine domain adaptation method called
Domain Adaptation via Feature Disentanglement~(DAFD), which has two components:
(1)the Category-Relevant Feature Selection (CRFS) module, which disentangles
the category-relevant features from the category-irrelevant features, and
(2)the Dynamic Local Maximum Mean Discrepancy (DLMMD) module, which achieves
fine-grained alignment by reducing the discrepancy within the category-relevant
features from different domains. Combined with the CRFS, the DLMMD module can
align the category-relevant features properly. We conduct comprehensive
experiment on four standard datasets. Our results clearly demonstrate the
robustness and effectiveness of our approach in domain adaptive image
classification tasks and its competitiveness to the state of the art.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Pseudo 3D Perception Transformer with Multi-level Confidence  Optimization for Visual Commonsense Reasoning</b></summary>
  <p><b>编号</b>：[227]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13335</p>
  <p><b>作者</b>：Jian Zhu,  Hanli Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：performing Visual Commonsense, Visual Commonsense Reasoning, rationale justifying based, Visual Commonsense, Commonsense Reasoning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A framework performing Visual Commonsense Reasoning(VCR) needs to choose an
answer and further provide a rationale justifying based on the given image and
question, where the image contains all the facts for reasoning and requires to
be sufficiently understood. Previous methods use a detector applied on the
image to obtain a set of visual objects without considering the exact positions
of them in the scene, which is inadequate for properly understanding spatial
and semantic relationships between objects. In addition, VCR samples are quite
diverse, and parameters of the framework tend to be trained suboptimally based
on mini-batches. To address above challenges, pseudo 3D perception Transformer
with multi-level confidence optimization named PPTMCO is proposed for VCR in
this paper. Specifically, image depth is introduced to represent pseudo
3-dimension(3D) positions of objects along with 2-dimension(2D) coordinates in
the image and further enhance visual features. Then, considering that
relationships between objects are influenced by depth, depth-aware Transformer
is proposed to do attention mechanism guided by depth differences from answer
words and objects to objects, where each word is tagged with pseudo depth value
according to related objects. To better optimize parameters of the framework, a
model parameter estimation method is further proposed to weightedly integrate
parameters optimized by mini-batches based on multi-level reasoning confidence.
Experiments on the benchmark VCR dataset demonstrate the proposed framework
performs better against the state-of-the-art approaches.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：Efficient and Effective Methods for Mixed Precision Neural Network  Quantization for Faster, Energy-efficient Inference</b></summary>
  <p><b>编号</b>：[230]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13330</p>
  <p><b>作者</b>：Deepika Bablani,  Jeffrey L. Mckinstry,  Steven K. Esser,  Rathinakumar Appuswamy,  Dharmendra S. Modha</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：efficient deep neural, neural network inference, deep neural network, effective and efficient, efficient deep</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>For effective and efficient deep neural network inference, it is desirable to
achieve state-of-the-art accuracy with the simplest networks requiring the
least computation, memory, and power. Quantizing networks to lower precision is
a powerful technique for simplifying networks. It is generally desirable to
quantize as aggressively as possible without incurring significant accuracy
degradation. As each layer of a network may have different sensitivity to
quantization, mixed precision quantization methods selectively tune the
precision of individual layers of a network to achieve a minimum drop in task
performance (e.g., accuracy). To estimate the impact of layer precision choice
on task performance two methods are introduced: i) Entropy Approximation Guided
Layer selection (EAGL) is fast and uses the entropy of the weight distribution,
and ii) Accuracy-aware Layer Precision Selection (ALPS) is straightforward and
relies on single epoch fine-tuning after layer precision reduction. Using EAGL
and ALPS for layer precision selection, full-precision accuracy is recovered
with a mix of 4-bit and 2-bit layers for ResNet-50 and ResNet-101
classification networks, demonstrating improved performance across the entire
accuracy-throughput frontier, and equivalent performance for the PSPNet
segmentation network in our own commensurate comparison over leading mixed
precision layer selection techniques, while requiring orders of magnitude less
compute time to reach a solution.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：[Work in progress] Scalable, out-of-the box segmentation of individual  particles from mineral samples acquired with micro CT</b></summary>
  <p><b>编号</b>：[236]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13319</p>
  <p><b>作者</b>：Karol Gotkowski,  Shuvam Gupta,  Jose R. A. Godinho,  Camila G. S. Tochtrop,  Klaus H. Maier-Hein,  Fabian Isensee</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：functioning modern society, modern society, functioning modern, particles, materials</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Minerals are indispensable for a functioning modern society. Yet, their
supply is limited causing a need for optimizing their exploration and
extraction both from ores and recyclable materials. Typically, these processes
must be meticulously adapted to the precise properties of the processed
particles, requiring an extensive characterization of their shapes, appearances
as well as the overall material composition. Current approaches perform this
analysis based on bulk segmentation and characterization of particles, and rely
on rudimentary postprocessing techniques to separate touching particles.
However, due to their inability to reliably perform this separation as well as
the need to retrain or reconfigure most methods for each new image, these
approaches leave untapped potential to be leveraged. Here, we propose an
instance segmentation method that is able to extract individual particles from
large micro CT images taken from mineral samples embedded in an epoxy matrix.
Our approach is based on the powerful nnU-Net framework, introduces a particle
size normalization, makes use of a border-core representation to enable
instance segmentation and is trained with a large dataset containing particles
of numerous different materials and minerals. We demonstrate that our approach
can be applied out-of-the box to a large variety of particle types, including
materials and appearances that have not been part of the training set. Thus, no
further manual annotations and retraining are required when applying the method
to new mineral samples, enabling substantially higher scalability of
experiments than existing methods. Our code and dataset are made publicly
available.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Emergence of Maps in the Memories of Blind Navigation Agents</b></summary>
  <p><b>编号</b>：[264]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13261</p>
  <p><b>作者</b>：Erik Wijmans,  Manolis Savva,  Irfan Essa,  Stefan Lee,  Ari S. Morcos,  Dhruv Batra</p>
  <p><b>备注</b>：Accepted to ICLR 2023</p>
  <p><b>关键词</b>：maintain internal spatial, navigation research posits, internal spatial representations, Animal navigation research, research posits</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Animal navigation research posits that organisms build and maintain internal
spatial representations, or maps, of their environment. We ask if machines --
specifically, artificial intelligence (AI) navigation agents -- also build
implicit (or 'mental') maps. A positive answer to this question would (a)
explain the surprising phenomenon in recent literature of ostensibly map-free
neural-networks achieving strong performance, and (b) strengthen the evidence
of mapping as a fundamental mechanism for navigation by intelligent embodied
agents, whether they be biological or artificial. Unlike animal navigation, we
can judiciously design the agent's perceptual system and control the learning
paradigm to nullify alternative navigation mechanisms. Specifically, we train
'blind' agents -- with sensing limited to only egomotion and no other sensing
of any kind -- to perform PointGoal navigation ('go to $\Delta$ x, $\Delta$ y')
via reinforcement learning. Our agents are composed of navigation-agnostic
components (fully-connected and recurrent neural networks), and our
experimental setup provides no inductive bias towards mapping. Despite these
harsh conditions, we find that blind agents are (1) surprisingly effective
navigators in new environments (~95% success); (2) they utilize memory over
long horizons (remembering ~1,000 steps of past experience in an episode); (3)
this memory enables them to exhibit intelligent behavior (following walls,
detecting collisions, taking shortcuts); (4) there is emergence of maps and
collision detection neurons in the representations of the environment built by
a blind agent as it navigates; and (5) the emergent maps are selective and task
dependent (e.g. the agent 'forgets' exploratory detours). Overall, this paper
presents no new techniques for the AI audience, but a surprising finding, an
insight, and an explanation.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Deep Monocular Hazard Detection for Safe Small Body Landing</b></summary>
  <p><b>编号</b>：[265]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13254</p>
  <p><b>作者</b>：Travis Driver,  Kento Tomita,  Koki Ho,  Panagiotis Tsiotras</p>
  <p><b>备注</b>：Presented at the AAS/AIAA Space Flight Mechanics Meeting, January 14-19, 2023, Austin, TX, USA</p>
  <p><b>关键词</b>：future robotic small, robotic small body, small body sample, body sample return, Hazard detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Hazard detection and avoidance is a key technology for future robotic small
body sample return and lander missions. Current state-of-the-practice methods
rely on high-fidelity, a priori terrain maps, which require extensive
human-in-the-loop verification and expensive reconnaissance campaigns to
resolve mapping uncertainties. We propose a novel safety mapping paradigm that
leverages deep semantic segmentation techniques to predict landing safety
directly from a single monocular image, thus reducing reliance on
high-fidelity, a priori data products. We demonstrate precise and accurate
safety mapping performance on real in-situ imagery of prospective sample sites
from the OSIRIS-REx mission.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Mono-STAR: Mono-camera Scene-level Tracking and Reconstruction</b></summary>
  <p><b>编号</b>：[269]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13244</p>
  <p><b>作者</b>：Haonan Chang,  Dhruv Metha Ramesh,  Shijie Geng,  Yuqiu Gan,  Abdeslam Boularias</p>
  <p><b>备注</b>：This paper has been accepted by ICRA2023</p>
  <p><b>关键词</b>：supports semantic fusion, simultaneously supports semantic, non-rigid object deformation, fast motion tracking, present Mono-STAR</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present Mono-STAR, the first real-time 3D reconstruction system that
simultaneously supports semantic fusion, fast motion tracking, non-rigid object
deformation, and topological change under a unified framework. The proposed
system solves a new optimization problem incorporating optical-flow-based 2D
constraints to deal with fast motion and a novel semantic-aware deformation
graph (SAD-graph) for handling topology change. We test the proposed system
under various challenging scenes and demonstrate that it significantly
outperforms existing state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：Deep learning-based lung segmentation and automatic regional template in  chest X-ray images for pediatric tuberculosis</b></summary>
  <p><b>编号</b>：[277]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13786</p>
  <p><b>作者</b>：Daniel Capellán-Martín,  Juan J. Gómez-Valverde,  Ramon Sanchez-Jacob,  David Bermejo-Peláez,  Lara García-Delgado,  Elisa López-Varela,  Maria J. Ledesma-Carbayo</p>
  <p><b>备注</b>：This work has been accepted at the SPIE Medical Imaging 2023, Image Processing conference</p>
  <p><b>关键词</b>：global child health, child health, considered a leading, substantial threat, threat to global</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Tuberculosis (TB) is still considered a leading cause of death and a
substantial threat to global child health. Both TB infection and disease are
curable using antibiotics. However, most children who die of TB are never
diagnosed or treated. In clinical practice, experienced physicians assess TB by
examining chest X-rays (CXR). Pediatric CXR has specific challenges compared to
adult CXR, which makes TB diagnosis in children more difficult. Computer-aided
diagnosis systems supported by Artificial Intelligence have shown performance
comparable to experienced radiologist TB readings, which could ease mass TB
screening and reduce clinical burden. We propose a multi-view deep
learning-based solution which, by following a proposed template, aims to
automatically regionalize and extract lung and mediastinal regions of interest
from pediatric CXR images where key TB findings may be present. Experimental
results have shown accurate region extraction, which can be used for further
analysis to confirm TB finding presence and severity assessment. Code publicly
available at this https URL.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：A relaxed proximal gradient descent algorithm for convergent  plug-and-play with proximal denoiser</b></summary>
  <p><b>编号</b>：[281]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13731</p>
  <p><b>作者</b>：Samuel Hurault,  Antonin Chambolle,  Arthur Leclaire,  Nicolas Papadakis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：paper presents, Proximal Gradient Descent, PGD algorithm, algorithm, PnP</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a new convergent Plug-and-Play (PnP) algorithm. PnP
methods are efficient iterative algorithms for solving image inverse problems
formulated as the minimization of the sum of a data-fidelity term and a
regularization term. PnP methods perform regularization by plugging a
pre-trained denoiser in a proximal algorithm, such as Proximal Gradient Descent
(PGD). To ensure convergence of PnP schemes, many works study specific
parametrizations of deep denoisers. However, existing results require either
unverifiable or suboptimal hypotheses on the denoiser, or assume restrictive
conditions on the parameters of the inverse problem. Observing that these
limitations can be due to the proximal algorithm in use, we study a relaxed
version of the PGD algorithm for minimizing the sum of a convex function and a
weakly convex one. When plugged with a relaxed proximal denoiser, we show that
the proposed PnP-$\alpha$PGD algorithm converges for a wider range of
regularization parameters, thus allowing more accurate image restoration.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Improved distinct bone segmentation in upper-body CT through  multi-resolution networks</b></summary>
  <p><b>编号</b>：[286]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13674</p>
  <p><b>作者</b>：Eva Schnider,  Julia Wolleb,  Antal Huck,  Mireille Toranelli,  Georg Rauter,  Magdalena Müller-Gerbl,  Philippe C. Cattin</p>
  <p><b>备注</b>：Under submission</p>
  <p><b>关键词</b>：Automated distinct bone, Automated distinct, distinct bone segmentation, navigation workflows, bone segmentation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Purpose: Automated distinct bone segmentation from CT scans is widely used in
planning and navigation workflows. U-Net variants are known to provide
excellent results in supervised semantic segmentation. However, in distinct
bone segmentation from upper body CTs a large field of view and a
computationally taxing 3D architecture are required. This leads to
low-resolution results lacking detail or localisation errors due to missing
spatial context when using high-resolution inputs.
Methods: We propose to solve this problem by using end-to-end trainable
segmentation networks that combine several 3D U-Nets working at different
resolutions. Our approach, which extends and generalizes HookNet and MRN,
captures spatial information at a lower resolution and skips the encoded
information to the target network, which operates on smaller high-resolution
inputs. We evaluated our proposed architecture against single resolution
networks and performed an ablation study on information concatenation and the
number of context networks.
Results: Our proposed best network achieves a median DSC of 0.86 taken over
all 125 segmented bone classes and reduces the confusion among similar-looking
bones in different locations. These results outperform our previously published
3D U-Net baseline results on the task and distinct-bone segmentation results
reported by other groups.
Conclusion: The presented multi-resolution 3D U-Nets address current
shortcomings in bone segmentation from upper-body CT scans by allowing for
capturing a larger field of view while avoiding the cubic growth of the input
pixels and intermediate computations that quickly outgrow the computational
capacities in 3D. The approach thus improves the accuracy and efficiency of
distinct bone segmentation from upper-body CT.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：CSDN: Combing Shallow and Deep Networks for Accurate Real-time  Segmentation of High-definition Intravascular Ultrasound Images</b></summary>
  <p><b>编号</b>：[288]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13648</p>
  <p><b>作者</b>：Shaofeng Yuan,  Feng Yang</p>
  <p><b>备注</b>：5 pages, 2 figures, 1 table, submitted to the 20th IEEE International Symposium on Biomedical Imaging (IEEE ISBI 2023)</p>
  <p><b>关键词</b>：resolution IVUS images, IVUS images, Intravascular ultrasound, IVUS images involves, resolution cross-sectional images</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Intravascular ultrasound (IVUS) is the preferred modality for capturing
real-time and high resolution cross-sectional images of the coronary arteries,
and evaluating the stenosis. Accurate and real-time segmentation of IVUS images
involves the delineation of lumen and external elastic membrane borders. In
this paper, we propose a two-stream framework for efficient segmentation of 60
MHz high resolution IVUS images. It combines shallow and deep networks, namely,
CSDN. The shallow network with thick channels focuses to extract low-level
details. The deep network with thin channels takes charge of learning
high-level semantics. Treating the above information separately enables
learning a model to achieve high accuracy and high efficiency for accurate
real-time segmentation. To further improve the segmentation performance, mutual
guided fusion module is used to enhance and fuse both different types of
feature representation. The experimental results show that our CSDN
accomplishes a good trade-off between analysis speed and segmentation accuracy.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：Demystifying Disagreement-on-the-Line in High Dimensions</b></summary>
  <p><b>编号</b>：[304]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13371</p>
  <p><b>作者</b>：Donghwan Lee,  Behrad Moniri,  Xinmeng Huang,  Edgar Dobriban,  Hamed Hassani</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：machine learning models, unlabeled data, labeled data, shift is challenging, machine learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Evaluating the performance of machine learning models under distribution
shift is challenging, especially when we only have unlabeled data from the
shifted (target) domain, along with labeled data from the original (source)
domain. Recent work suggests that the notion of disagreement, the degree to
which two models trained with different randomness differ on the same input, is
a key to tackle this problem. Experimentally, disagreement and prediction error
have been shown to be strongly connected, which has been used to estimate model
performance. Experiments have lead to the discovery of the
disagreement-on-the-line phenomenon, whereby the classification error under the
target domain is often a linear function of the classification error under the
source domain; and whenever this property holds, disagreement under the source
and target domain follow the same linear relation. In this work, we develop a
theoretical foundation for analyzing disagreement in high-dimensional random
features regression; and study under what conditions the
disagreement-on-the-line phenomenon occurs in our setting. Experiments on
CIFAR-10-C, Tiny ImageNet-C, and Camelyon17 are consistent with our theory and
support the universality of the theoretical findings.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：CaraNet: Context Axial Reverse Attention Network for Segmentation of  Small Medical Objects</b></summary>
  <p><b>编号</b>：[306]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13366</p>
  <p><b>作者</b>：Ange Lou,  Shuyue Guan,  Murray Loew</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2108.07368</p>
  <p><b>关键词</b>：Segmenting medical images, medical images accurately, diagnosis and treatment, images accurately, accurately and reliably</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Segmenting medical images accurately and reliably is important for disease
diagnosis and treatment. It is a challenging task because of the wide variety
of objects' sizes, shapes, and scanning modalities. Recently, many
convolutional neural networks (CNN) have been designed for segmentation tasks
and achieved great success. Few studies, however, have fully considered the
sizes of objects, and thus most demonstrate poor performance for small objects
segmentation. This can have a significant impact on the early detection of
diseases. This paper proposes a Context Axial Reverse Attention Network
(CaraNet) to improve the segmentation performance on small objects compared
with several recent state-of-the-art models. CaraNet applies axial reserve
attention (ARA) and channel-wise feature pyramid (CFP) module to dig feature
information of small medical object. And we evaluate our model by six different
measurement metrics. We test our CaraNet on brain tumor (BraTS 2018) and polyp
(Kvasir-SEG, CVC-ColonDB, CVC-ClinicDB, CVC-300, and ETIS-LaribPolypDB)
segmentation datasets. Our CaraNet achieves the top-rank mean Dice segmentation
accuracy, and results show a distinct advantage of CaraNet in the segmentation
of small medical objects.</p>
  </details>
</details>
<h1>自然语言处理</h1>
<details>
  <summary>1. <b>标题：PADL: Language-Directed Physics-Based Character Control</b></summary>
  <p><b>编号</b>：[2]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13868</p>
  <p><b>作者</b>：Jordan Juravsky,  Yunrong Guo,  Sanja Fidler,  Xue Bin Peng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：focus for computer, Natural language, life-like motions, language, language commands</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Developing systems that can synthesize natural and life-like motions for
simulated characters has long been a focus for computer animation. But in order
for these systems to be useful for downstream applications, they need not only
produce high-quality motions, but must also provide an accessible and versatile
interface through which users can direct a character's behaviors. Natural
language provides a simple-to-use and expressive medium for specifying a user's
intent. Recent breakthroughs in natural language processing (NLP) have
demonstrated effective use of language-based interfaces for applications such
as image generation and program synthesis. In this work, we present PADL, which
leverages recent innovations in NLP in order to take steps towards developing
language-directed controllers for physics-based character animation. PADL
allows users to issue natural language commands for specifying both high-level
tasks and low-level skills that a character should perform. We present an
adversarial imitation learning approach for training policies to map high-level
language commands to low-level controls that enable a character to perform the
desired task and skill specified by a user's commands. Furthermore, we propose
a multi-task aggregation method that leverages a language-based multiple-choice
question-answering approach to determine high-level task objectives from
language commands. We show that our framework can be applied to effectively
direct a simulated humanoid character to perform a diverse array of complex
motor skills.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Mathematical Capabilities of ChatGPT</b></summary>
  <p><b>编号</b>：[3]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13867</p>
  <p><b>作者</b>：Simon Frieder,  Luca Pinchetti,  Ryan-Rhys Griffiths,  Tommaso Salvatori,  Thomas Lukasiewicz,  Philipp Christian Petersen,  Alexis Chevalier,  Julius Berner</p>
  <p><b>备注</b>：The GHOSTS dataset will be available at this https URL</p>
  <p><b>关键词</b>：Lean Mathematical Library, mathematical, ChatGPT, mathematics, mathematical corpus</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We investigate the mathematical capabilities of ChatGPT by testing it on
publicly available datasets, as well as hand-crafted ones, and measuring its
performance against other models trained on a mathematical corpus, such as
Minerva. We also test whether ChatGPT can be a useful assistant to professional
mathematicians by emulating various use cases that come up in the daily
professional activities of mathematicians (question answering, theorem
searching). In contrast to formal mathematics, where large databases of formal
proofs are available (e.g., the Lean Mathematical Library), current datasets of
natural-language mathematics, used to benchmark language models, only cover
elementary mathematics. We address this issue by introducing a new dataset:
GHOSTS. It is the first natural-language dataset made and curated by working
researchers in mathematics that (1) aims to cover graduate-level mathematics
and (2) provides a holistic overview of the mathematical capabilities of
language models. We benchmark ChatGPT on GHOSTS and evaluate performance
against fine-grained criteria. We make this new dataset publicly available to
assist a community-driven comparison of ChatGPT with (future) large language
models in terms of advanced mathematical comprehension. We conclude that
contrary to many positive reports in the media (a potential case of selection
bias), ChatGPT's mathematical abilities are significantly below those of an
average mathematics graduate student. Our results show that ChatGPT often
understands the question but fails to provide correct solutions. Hence, if your
goal is to use it to pass a university exam, you would be better off copying
from your average peer!</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：LAGAN: Deep Semi-Supervised Linguistic-Anthropology Classification with  Conditional Generative Adversarial Neural Network</b></summary>
  <p><b>编号</b>：[8]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13853</p>
  <p><b>作者</b>：Rossi Kamal,  Zuzana Kubincova</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ethnic minority, Ethnic minority group, ethnic minority education, fourth industrial revolution, minority group</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Education is a right of all, however, every individual is different than
others. Teachers in post-communism era discover inherent individualism to
equally train all towards job market of fourth industrial revolution. We can
consider scenario of ethnic minority education in academic practices. Ethnic
minority group has grown in their own culture and would prefer to be taught in
their native way. We have formulated such linguistic anthropology(how people
learn)based engagement as semi-supervised problem. Then, we have developed an
conditional deep generative adversarial network algorithm namely LA-GAN to
classify linguistic ethnographic features in student engagement. Theoretical
justification proves the objective, regularization and loss function of our
semi-supervised adversarial model. Survey questions are prepared to reach some
form of assumptions about z-generation and ethnic minority group, whose
learning style, learning approach and preference are our main area of interest.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：ChatGPT or Human? Detect and Explain. Explaining Decisions of Machine  Learning Model for Detecting Short ChatGPT-generated Text</b></summary>
  <p><b>编号</b>：[9]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13852</p>
  <p><b>作者</b>：Sandra Mitrović,  Davide Andreoletti,  Omran Ayoub</p>
  <p><b>备注</b>：11 pages, 8 figures, 2 tables</p>
  <p><b>关键词</b>：generate grammatically flawless, ability to generate, generate grammatically, grammatically flawless, flawless and seemingly-human</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>ChatGPT has the ability to generate grammatically flawless and
seemingly-human replies to different types of questions from various domains.
The number of its users and of its applications is growing at an unprecedented
rate. Unfortunately, use and abuse come hand in hand. In this paper, we study
whether a machine learning model can be effectively trained to accurately
distinguish between original human and seemingly human (that is,
ChatGPT-generated) text, especially when this text is short. Furthermore, we
employ an explainable artificial intelligence framework to gain insight into
the reasoning behind the model trained to differentiate between
ChatGPT-generated and human-generated text. The goal is to analyze model's
decisions and determine if any specific patterns or characteristics can be
identified. Our study focuses on short online reviews, conducting two
experiments comparing human-generated and ChatGPT-generated text. The first
experiment involves ChatGPT text generated from custom queries, while the
second experiment involves text generated by rephrasing original
human-generated reviews. We fine-tune a Transformer-based model and use it to
make predictions, which are then explained using SHAP. We compare our model
with a perplexity score-based approach and find that disambiguation between
human and ChatGPT-generated reviews is more challenging for the ML model when
using rephrased text. However, our proposed approach still achieves an accuracy
of 79%. Using explainability, we observe that ChatGPT's writing is polite,
without specific details, using fancy and atypical vocabulary, impersonal, and
typically it does not express feelings.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Benchmarking Large Language Models for News Summarization</b></summary>
  <p><b>编号</b>：[10]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13848</p>
  <p><b>作者</b>：Tianyi Zhang,  Faisal Ladhak,  Esin Durmus,  Percy Liang,  Kathleen McKeown,  Tatsunori B. Hashimoto</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large language models, Large language, poorly understood, shown promise, promise for automatic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have shown promise for automatic summarization
but the reasons behind their successes are poorly understood. By conducting a
human evaluation on ten LLMs across different pretraining methods, prompts, and
model scales, we make two important observations. First, we find instruction
tuning, and not model size, is the key to the LLM's zero-shot summarization
capability. Second, existing studies have been limited by low-quality
references, leading to underestimates of human performance and lower few-shot
and finetuning performance. To better evaluate LLMs, we perform human
evaluation over high-quality summaries we collect from freelance writers.
Despite major stylistic differences such as the amount of paraphrasing, we find
that LMM summaries are judged to be on par with human written summaries.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Do Multi-Document Summarization Models Synthesize?</b></summary>
  <p><b>编号</b>：[12]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13844</p>
  <p><b>作者</b>：Jay DeYoung,  Stephanie C. Martinez,  Iain J. Marshall,  Byron C. Wallace</p>
  <p><b>备注</b>：22 Pages, 13 Figures, 22 Tables. ACL Formatted paper; expanded version of rejected ICLR submisssion this https URL Paper de-anonymized ahead of ICLR de-anonymization due to ACL policies/additional conference submission</p>
  <p><b>关键词</b>：entails producing concise, producing concise synopses, summarization entails producing, entails producing, producing concise</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multi-document summarization entails producing concise synopses of
collections of inputs. For some applications, the synopsis should accurately
\emph{synthesize} inputs with respect to a key property or aspect. For example,
a synopsis of film reviews all written about a particular movie should reflect
the average critic consensus. As a more consequential example, consider
narrative summaries that accompany biomedical \emph{systematic reviews} of
clinical trial results. These narratives should fairly summarize the
potentially conflicting results from individual trials.
In this paper we ask: To what extent do modern multi-document summarization
models implicitly perform this type of synthesis? To assess this we perform a
suite of experiments that probe the degree to which conditional generation
models trained for summarization using standard methods yield outputs that
appropriately synthesize inputs. We find that existing models do partially
perform synthesis, but do so imperfectly. In particular, they are
over-sensitive to changes in input ordering and under-sensitive to changes in
input compositions (e.g., the ratio of positive to negative movie reviews). We
propose a simple, general method for improving model synthesis capabilities by
generating an explicitly diverse set of candidate outputs, and then selecting
from these the string best aligned with the expected aggregate measure for the
inputs, or \emph{abstaining} when the model produces no good candidate. This
approach improves model synthesis performance. We hope highlighting the need
for synthesis (in some summarization settings), motivates further research into
multi-document summarization methods and learning objectives that explicitly
account for the need to synthesize.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image  Diffusion Models</b></summary>
  <p><b>编号</b>：[19]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13826</p>
  <p><b>作者</b>：Hila Chefer,  Yuval Alaluf,  Yael Vinker,  Lior Wolf,  Daniel Cohen-Or</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：creative imagery guided, target text prompt, Stable Diffusion model, text prompt, demonstrated an unparalleled</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent text-to-image generative models have demonstrated an unparalleled
ability to generate diverse and creative imagery guided by a target text
prompt. While revolutionary, current state-of-the-art diffusion models may
still fail in generating images that fully convey the semantics in the given
text prompt. We analyze the publicly available Stable Diffusion model and
assess the existence of catastrophic neglect, where the model fails to generate
one or more of the subjects from the input prompt. Moreover, we find that in
some cases the model also fails to correctly bind attributes (e.g., colors) to
their corresponding subjects. To help mitigate these failure cases, we
introduce the concept of Generative Semantic Nursing (GSN), where we seek to
intervene in the generative process on the fly during inference time to improve
the faithfulness of the generated images. Using an attention-based formulation
of GSN, dubbed Attend-and-Excite, we guide the model to refine the
cross-attention units to attend to all subject tokens in the text prompt and
strengthen - or excite - their activations, encouraging the model to generate
all subjects described in the text prompt. We compare our approach to
alternative approaches and demonstrate that it conveys the desired concepts
more faithfully across a range of text prompts.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Grounding Language Models to Images for Multimodal Generation</b></summary>
  <p><b>编号</b>：[20]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13823</p>
  <p><b>作者</b>：Jing Yu Koh,  Ruslan Salakhutdinov,  Daniel Fried</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：visual domain, language models, text-only language models, ground pretrained text-only, propose an efficient</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose an efficient method to ground pretrained text-only language models
to the visual domain, enabling them to process and generate arbitrarily
interleaved image-and-text data. Our method leverages the abilities of language
models learnt from large scale text-only pretraining, such as in-context
learning and free-form text generation. We keep the language model frozen, and
finetune input and output linear layers to enable cross-modality interactions.
This allows our model to process arbitrarily interleaved image-and-text inputs,
and generate free-form text interleaved with retrieved images. We achieve
strong zero-shot performance on grounded tasks such as contextual image
retrieval and multimodal dialogue, and showcase compelling interactive
abilities. Our approach works with any off-the-shelf language model and paves
the way towards an effective, general solution for leveraging pretrained
language models in visually grounded settings.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Explaining Large Language Model-Based Neural Semantic Parsers (Student  Abstract)</b></summary>
  <p><b>编号</b>：[22]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13820</p>
  <p><b>作者</b>：Daking Rai (1),  Yilun Zhou (2),  Bailin Wang (2),  Ziyu Yao (1) ((1) George Mason University, (2) Massachusetts Institute of Technology)</p>
  <p><b>备注</b>：2 pages, 5 figures, to be published in AAAI-23 Student Abstract and Poster Program</p>
  <p><b>关键词</b>：demonstrated strong capability, structured prediction tasks, large language models, large language, demonstrated strong</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While large language models (LLMs) have demonstrated strong capability in
structured prediction tasks such as semantic parsing, few amounts of research
have explored the underlying mechanisms of their success. Our work studies
different methods for explaining an LLM-based semantic parser and qualitatively
discusses the explained model behaviors, hoping to inspire future research
toward better understanding them.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Causal-Discovery Performance of ChatGPT in the context of Neuropathic  Pain Diagnosis</b></summary>
  <p><b>编号</b>：[23]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13819</p>
  <p><b>作者</b>：Ruibo Tu,  Chao Ma,  Cheng Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：natural language conversation, demonstrated exceptional proficiency, previous large language, large language models, ChatGPT has demonstrated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>ChatGPT has demonstrated exceptional proficiency in natural language
conversation, e.g., it can answer a wide range of questions while no previous
large language models can. Thus, we would like to push its limit and explore
its ability to answer causal discovery questions by using a medical benchmark
(Tu et al. 2019) in causal discovery.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Execution-based Code Generation using Deep Reinforcement Learning</b></summary>
  <p><b>编号</b>：[25]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13816</p>
  <p><b>作者</b>：Parshin Shojaee,  Aneesh Jain,  Sindhu Tipirneni,  Chandan K. Reddy</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：automating software engineering, software engineering processes, demonstrated considerable potential, large-scale code corpora, programming language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The utilization of programming language (PL) models, pretrained on
large-scale code corpora, as a means of automating software engineering
processes has demonstrated considerable potential in streamlining various code
generation tasks such as code completion, code translation, and program
synthesis. However, current approaches mainly rely on supervised fine-tuning
objectives borrowed from text generation, neglecting specific sequence-level
features of code, including but not limited to compilability as well as
syntactic and functional correctness. To address this limitation, we propose
PPOCoder, a new framework for code generation that combines pretrained PL
models with Proximal Policy Optimization (PPO) deep reinforcement learning and
employs execution feedback as the external source of knowledge into the model
optimization. PPOCoder is transferable across different code generation tasks
and PLs. Extensive experiments on three code generation tasks demonstrate the
effectiveness of our proposed approach compared to SOTA methods, improving the
success rate of compilation and functional correctness over different PLs. Our
code can be found at this https URL .</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Large Language Models are Versatile Decomposers: Decompose Evidence and  Questions for Table-based Reasoning</b></summary>
  <p><b>编号</b>：[28]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13808</p>
  <p><b>作者</b>：Yunhu Ye,  Binyuan Hui,  Min Yang,  Binhua Li,  Fei Huang,  Yongbin Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：structured tabular data, shown remarkable progress, free-form natural language, combining deep models, Table-based reasoning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Table-based reasoning has shown remarkable progress in combining deep models
with discrete reasoning, which requires reasoning over both free-form natural
language (NL) questions and structured tabular data. However, previous
table-based reasoning solutions usually suffer from significant performance
degradation on huge evidence (tables). In addition, most existing methods
struggle to reason over complex questions since the required information is
scattered in different places. To alleviate the above challenges, we exploit
large language models (LLMs) as decomposers for effective table-based
reasoning, which (i) decompose huge evidence (a huge table) into sub-evidence
(a small table) to mitigate the interference of useless information for table
reasoning; and (ii) decompose complex questions into simpler sub-questions for
text reasoning. Specifically, we first use the LLMs to break down the evidence
(tables) involved in the current question, retaining the relevant evidence and
excluding the remaining irrelevant evidence from the huge table. In addition,
we propose a "parsing-execution-filling" strategy to alleviate the
hallucination dilemma of the chain of thought by decoupling logic and numerical
computation in each step. Extensive experiments show that our method can
effectively leverage decomposed evidence and questions and outperforms the
strong baselines on TabFact, WikiTableQuestion, and FetaQA datasets. Notably,
our model outperforms human performance for the first time on the TabFact
dataset.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：The Touché23-ValueEval Dataset for Identifying Human Values behind  Arguments</b></summary>
  <p><b>编号</b>：[38]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13771</p>
  <p><b>作者</b>：Nailia Mirzakhmedova,  Johannes Kiesel,  Milad Alshomary,  Maximilian Heinrich,  Nicolas Handke,  Xiaoni Cai,  Barriere Valentin,  Doratossadat Dastgheib,  Omid Ghahroodi,  Mohammad Ali Sadraei,  Ehsaneddin Asgari,  Lea Kawaletz,  Henning Wachsmuth,  Benno Stein</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Identifying Human, Arguments, Identifying, Human, Dataset for Identifying</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present the Touché23-ValueEval Dataset for Identifying Human Values
behind Arguments. To investigate approaches for the automated detection of
human values behind arguments, we collected 9324 arguments from 6 diverse
sources, covering religious texts, political discussions, free-text arguments,
newspaper editorials, and online democracy platforms. Each argument was
annotated by 3 crowdworkers for 54 values. The Touché23-ValueEval dataset
extends the Webis-ArgValues-22. In comparison to the previous dataset, the
effectiveness of a 1-Baseline decreases, but that of an out-of-the-box BERT
model increases. Therefore, though the classification difficulty increased as
per the label distribution, the larger dataset allows for training better
models.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Dynamic Scheduled Sampling with Imitation Loss for Neural Text  Generation</b></summary>
  <p><b>编号</b>：[49]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13753</p>
  <p><b>作者</b>：Xiang Lin,  Prathyusha Jwalapuram,  Shafiq Joty</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：previous target tokens, ground-truth sequence conditioned, typically trained, trained to maximize, maximize the likelihood</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>State-of-the-art neural text generation models are typically trained to
maximize the likelihood of each token in the ground-truth sequence conditioned
on the previous target tokens. However, during inference, the model needs to
make a prediction conditioned on the tokens generated by itself. This
train-test discrepancy is referred to as exposure bias. Scheduled sampling is a
curriculum learning strategy that gradually exposes the model to its own
predictions during training to mitigate this bias. Most of the proposed
approaches design a scheduler based on training steps, which generally requires
careful tuning depending on the training setup. In this work, we introduce
Dynamic Scheduled Sampling with Imitation Loss (DySI), which maintains the
schedule based solely on the training time accuracy, while enhancing the
curriculum learning by introducing an imitation loss, which attempts to make
the behavior of the decoder indistinguishable from the behavior of a
teacher-forced decoder. DySI is universally applicable across training setups
with minimal tuning. Extensive experiments and analysis show that DySI not only
achieves notable improvements on standard machine translation benchmarks, but
also significantly improves the robustness of other text generation models.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：UPop: Unified and Progressive Pruning for Compressing Vision-Language  Transformers</b></summary>
  <p><b>编号</b>：[52]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13741</p>
  <p><b>作者</b>：Dachuan Shi,  Chaofan Tao,  Ying Jin,  Zhendong Yang,  Chun Yuan,  Jiaqi Wang</p>
  <p><b>备注</b>：16 pages, 5 figures, 13 tables</p>
  <p><b>关键词</b>：Real-world data, vast amount, vision and language, textbf, vison-language Transformer compression</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Real-world data contains a vast amount of multimodal information, among which
vision and language are the two most representative modalities. Moreover,
increasingly heavier models, e.g., Transformers, have attracted the attention
of researchers to model compression. However, how to compress multimodal
models, especially vison-language Transformers, is still under-explored. This
paper proposes the \textbf{U}nified and \textbf{P}r\textbf{o}gressive
\textbf{P}runing (UPop) as a universal vison-language Transformer compression
framework, which incorporates 1) unifiedly searching multimodal subnets in a
continuous optimization space from the original model, which enables automatic
assignment of pruning ratios among compressible modalities and structures; 2)
progressively searching and retraining the subnet, which maintains convergence
between the search and retrain to attain higher compression ratios. Experiments
on multiple generative and discriminative vision-language tasks, including
Visual Reasoning, Image Caption, Visual Question Answer, Image-Text Retrieval,
Text-Image Retrieval, and Image Classification, demonstrate the effectiveness
and versatility of the proposed UPop framework.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Zero-shot cross-lingual transfer language selection using linguistic  similarity</b></summary>
  <p><b>编号</b>：[61]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13720</p>
  <p><b>作者</b>：Juuso Eronen,  Michal Ptaszynski,  Fumito Masui</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Natural Language, transfer, Language, Natural, transfer language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the selection of transfer languages for different Natural Language
Processing tasks, specifically sentiment analysis, named entity recognition and
dependency parsing. In order to select an optimal transfer language, we propose
to utilize different linguistic similarity metrics to measure the distance
between languages and make the choice of transfer language based on this
information instead of relying on intuition. We demonstrate that linguistic
similarity correlates with cross-lingual transfer performance for all of the
proposed tasks. We also show that there is a statistically significant
difference in choosing the optimal language as the transfer source instead of
English. This allows us to select a more suitable transfer language which can
be used to better leverage knowledge from high-resource languages in order to
improve the performance of language applications lacking data. For the study,
we used datasets from eight different languages from three language families.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Recursive Neural Networks with Bottlenecks Diagnose  (Non-)Compositionality</b></summary>
  <p><b>编号</b>：[62]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13714</p>
  <p><b>作者</b>：Verna Dankers,  Ivan Titov</p>
  <p><b>备注</b>：Published in EMNLP 2023 findings; 18 pages total (9 in the main paper, 3 pages of limitations and references and 6 pages with appendices)</p>
  <p><b>关键词</b>：work in NLP, NLP focuses, recent line, line of work, generalise compositionally</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A recent line of work in NLP focuses on the (dis)ability of models to
generalise compositionally for artificial languages. However, when considering
natural language tasks, the data involved is not strictly, or locally,
compositional. Quantifying the compositionality of data is a challenging task,
which has been investigated primarily for short utterances. We use recursive
neural models (Tree-LSTMs) with bottlenecks that limit the transfer of
information between nodes. We illustrate that comparing data's representations
in models with and without the bottleneck can be used to produce a
compositionality metric. The procedure is applied to the evaluation of
arithmetic expressions using synthetic data, and sentiment classification using
natural language data. We demonstrate that compression through a bottleneck
impacts non-compositional examples disproportionately and then use the
bottleneck compositionality metric (BCM) to distinguish compositional from
non-compositional samples, yielding a compositionality ranking over a dataset.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：The Flan Collection: Designing Data and Methods for Effective  Instruction Tuning</b></summary>
  <p><b>编号</b>：[68]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13688</p>
  <p><b>作者</b>：Shayne Longpre,  Le Hou,  Tu Vu,  Albert Webson,  Hyung Won Chung,  Yi Tay,  Denny Zhou,  Quoc V. Le,  Barret Zoph,  Jason Wei,  Adam Roberts</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：design decisions, instruction tuning, instruction tuning methods, Flan, Flan Collection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the design decisions of publicly available instruction tuning
methods, and break down the development of Flan 2022 (Chung et al., 2022).
Through careful ablation studies on the Flan Collection of tasks and methods,
we tease apart the effect of design decisions which enable Flan-T5 to
outperform prior work by 3-17%+ across evaluation settings. We find task
balancing and enrichment techniques are overlooked but critical to effective
instruction tuning, and in particular, training with mixed prompt settings
(zero-shot, few-shot, and chain-of-thought) actually yields stronger (2%+)
performance in all settings. In further experiments, we show Flan-T5 requires
less finetuning to converge higher and faster than T5 on single downstream
tasks, motivating instruction-tuned models as more computationally-efficient
starting checkpoints for new tasks. Finally, to accelerate research on
instruction tuning, we make the Flan 2022 collection of datasets, templates,
and methods publicly available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Friend-training: Learning from Models of Different but Related Tasks</b></summary>
  <p><b>编号</b>：[71]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13683</p>
  <p><b>作者</b>：Mian Zhang,  Lifeng Jin,  Linfeng Song,  Haitao Mi,  Xiabing Zhou,  Dong Yu</p>
  <p><b>备注</b>：Accepted by EACL2023</p>
  <p><b>关键词</b>：Current self-training methods, utilizing differences, input features, focus on improving, differences in input</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Current self-training methods such as standard self-training, co-training,
tri-training, and others often focus on improving model performance on a single
task, utilizing differences in input features, model architectures, and
training processes. However, many tasks in natural language processing are
about different but related aspects of language, and models trained for one
task can be great teachers for other related tasks. In this work, we propose
friend-training, a cross-task self-training framework, where models trained to
do different tasks are used in an iterative training, pseudo-labeling, and
retraining process to help each other for better selection of pseudo-labels.
With two dialogue understanding tasks, conversational semantic role labeling
and dialogue rewriting, chosen for a case study, we show that the models
trained with the friend-training framework achieve the best performance
compared to strong baselines.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Automated Sentiment and Hate Speech Analysis of Facebook Data by  Employing Multilingual Transformer Models</b></summary>
  <p><b>编号</b>：[74]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13668</p>
  <p><b>作者</b>：Ritumbra Manuvie,  Saikat Chatterjee</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Social Media Platforms, Media Platforms, Social Media, heightened negative discourse, amplify the spread</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, there has been a heightened consensus within academia and in
the public discourse that Social Media Platforms (SMPs), amplify the spread of
hateful and negative sentiment content. Researchers have identified how hateful
content, political propaganda, and targeted messaging contributed to real-world
harms including insurrections against democratically elected governments,
genocide, and breakdown of social cohesion due to heightened negative discourse
towards certain communities in parts of the world. To counter these issues,
SMPs have created semi-automated systems that can help identify toxic speech.
In this paper we analyse the statistical distribution of hateful and negative
sentiment contents within a representative Facebook dataset (n= 604,703)
scrapped through 648 public Facebook pages which identify themselves as
proponents (and followers) of far-right Hindutva actors. These pages were
identified manually using keyword searches on Facebook and on CrowdTangleand
classified as far-right Hindutva pages based on page names, page descriptions,
and discourses shared on these pages. We employ state-of-the-art, open-source
XLM-T multilingual transformer-based language models to perform sentiment and
hate speech analysis of the textual contents shared on these pages over a
period of 5.5 years. The result shows the statistical distributions of the
predicted sentiment and the hate speech labels; top actors, and top page
categories. We further discuss the benchmark performances and limitations of
these pre-trained language models.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：TopoBERT: Plug and Play Toponym Recognition Module Harnessing Fine-tuned  BERT</b></summary>
  <p><b>编号</b>：[89]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13631</p>
  <p><b>作者</b>：Bing Zhou,  Lei Zou,  Yingjie Hu,  Yi Qiang</p>
  <p><b>备注</b>：9 Pages, 6 figures</p>
  <p><b>关键词</b>：Extracting precise geographical, precise geographical information, Extracting precise, precise geographical, textual contents</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Extracting precise geographical information from textual contents is crucial
in a plethora of applications. For example, during hazardous events, a robust
and unbiased toponym extraction framework can provide an avenue to tie the
location concerned to the topic discussed by news media posts and pinpoint
humanitarian help requests or damage reports from social media. Early studies
have leveraged rule-based, gazetteer-based, deep learning, and hybrid
approaches to address this problem. However, the performance of existing tools
is deficient in supporting operations like emergency rescue, which relies on
fine-grained, accurate geographic information. The emerging pretrained language
models can better capture the underlying characteristics of text information,
including place names, offering a promising pathway to optimize toponym
recognition to underpin practical applications. In this paper, TopoBERT, a
toponym recognition module based on a one dimensional Convolutional Neural
Network (CNN1D) and Bidirectional Encoder Representation from Transformers
(BERT), is proposed and fine-tuned. Three datasets (CoNLL2003-Train,
Wikipedia3000, WNUT2017) are leveraged to tune the hyperparameters, discover
the best training strategy, and train the model. Another two datasets
(CoNLL2003-Test and Harvey2017) are used to evaluate the performance. Three
distinguished classifiers, linear, multi-layer perceptron, and CNN1D, are
benchmarked to determine the optimal model architecture. TopoBERT achieves
state-of-the-art performance (f1-score=0.865) compared to the other five
baseline models and can be applied to diverse toponym recognition tasks without
additional training.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Archive TimeLine Summarization (ATLS): Conceptual Framework for Timeline  Generation over Historical Document Collections</b></summary>
  <p><b>编号</b>：[151]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13479</p>
  <p><b>作者</b>：Nicolas Gutehrlé (CRIT),  Antoine Doucet (L3I),  Adam Jatowt</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：search engines interfaces, issuing queries, search engines, user to retrieve, engines interfaces</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Archive collections are nowadays mostly available through search engines
interfaces, which allow a user to retrieve documents by issuing queries. The
study of these collections may be, however, impaired by some aspects of search
engines, such as the overwhelming number of documents returned or the lack of
contextual knowledge provided. New methods that could work independently or in
combination with search engines are then required to access these collections.
In this position paper, we propose to extend TimeLine Summarization (TLS)
methods on archive collections to assist in their studies. We provide an
overview of existing TLS methods and we describe a conceptual framework for an
Archive TimeLine Summarization (ATLS) system, which aims to generate
informative, readable and interpretable timelines.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：ZhichunRoad at Amazon KDD Cup 2022: MultiTask Pre-Training for  E-Commerce Product Search</b></summary>
  <p><b>编号</b>：[158]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13455</p>
  <p><b>作者</b>：Xuange Cui,  Wei Xiong,  Songlin Wang</p>
  <p><b>备注</b>：KDD Cup Workshop @ KDD 2022</p>
  <p><b>关键词</b>：robust multilingual model, propose a robust, robust multilingual, quality of search, improve the quality</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we propose a robust multilingual model to improve the quality
of search results. Our model not only leverage the processed class-balanced
dataset, but also benefit from multitask pre-training that leads to more
general representations. In pre-training stage, we adopt mlm task,
classification task and contrastive learning task to achieve considerably
performance. In fine-tuning stage, we use confident learning, exponential
moving average method (EMA), adversarial training (FGM) and regularized dropout
strategy (R-Drop) to improve the model's generalization and robustness.
Moreover, we use a multi-granular semantic unit to discover the queries and
products textual metadata for enhancing the representation of the model. Our
approach obtained competitive results and ranked top-8 in three tasks. We
release the source code and pre-trained models associated with this work.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Numeracy from Literacy: Data Science as an Emergent Skill from Large  Language Models</b></summary>
  <p><b>编号</b>：[198]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13382</p>
  <p><b>作者</b>：David Noever,  Forrest McKee</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：offer unique testbeds, Large language models, Large language, offer unique, OpenAI ChatGPT</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLM) such as OpenAI's ChatGPT and GPT-3 offer unique
testbeds for exploring the translation challenges of turning literacy into
numeracy. Previous publicly-available transformer models from eighteen months
prior and 1000 times smaller failed to provide basic arithmetic. The
statistical analysis of four complex datasets described here combines
arithmetic manipulations that cannot be memorized or encoded by simple rules.
The work examines whether next-token prediction succeeds from sentence
completion into the realm of actual numerical understanding. For example, the
work highlights cases for descriptive statistics on in-memory datasets that the
LLM initially loads from memory or generates randomly using python libraries.
The resulting exploratory data analysis showcases the model's capabilities to
group by or pivot categorical sums, infer feature importance, derive
correlations, and predict unseen test cases using linear regression. To extend
the model's testable range, the research deletes and appends random rows such
that recall alone cannot explain emergent numeracy.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Faithful Chain-of-Thought Reasoning</b></summary>
  <p><b>编号</b>：[201]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13379</p>
  <p><b>作者</b>：Qing Lyu,  Shreya Havaldar,  Adam Stein,  Li Zhang,  Delip Rao,  Eric Wong,  Marianna Apidianaki,  Chris Callison-Burch</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：boosts Language Models', Language Models', Natural Language query, prompting boosts Language, generated reasoning chain</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While Chain-of-Thought (CoT) prompting boosts Language Models' (LM)
performance on a gamut of complex reasoning tasks, the generated reasoning
chain does not necessarily reflect how the model arrives at the answer (aka.
faithfulness). We propose Faithful CoT, a faithful-by-construction framework
that decomposes a reasoning task into two stages: Translation (Natural Language
query $\rightarrow$ symbolic reasoning chain) and Problem Solving (reasoning
chain $\rightarrow$ answer), using an LM and a deterministic solver
respectively. We demonstrate the efficacy of our approach on 10 reasoning
datasets from 4 diverse domains. It outperforms traditional CoT prompting on 9
out of the 10 datasets, with an average accuracy gain of 4.4 on Math Word
Problems, 1.9 on Planning, 4.0 on Multi-hop Question Answering (QA), and 18.1
on Logical Inference, under greedy decoding. Together with self-consistency
decoding, we achieve new state-of-the-art few-shot performance on 7 out of the
10 datasets, showing a strong synergy between faithfulness and accuracy.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Improving Open-Domain Dialogue Evaluation with a Causal Inference Model</b></summary>
  <p><b>编号</b>：[205]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13372</p>
  <p><b>作者</b>：Cat P. Le,  Luke Dai,  Michael Johnston,  Yang Liu,  Marilyn Walker,  Reza Ghanadan</p>
  <p><b>备注</b>：Accepted as a conference paper at IWSDS 2023</p>
  <p><b>关键词</b>：remain a significant, significant challenge, challenge for research, ratings, Effective evaluation methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Effective evaluation methods remain a significant challenge for research on
open-domain conversational dialogue systems. Explicit satisfaction ratings can
be elicited from users, but users often do not provide ratings when asked, and
those they give can be highly subjective. Post-hoc ratings by experts are an
alternative, but these can be both expensive and complex to collect. Here, we
explore the creation of automated methods for predicting both expert and user
ratings of open-domain dialogues. We compare four different approaches. First,
we train a baseline model using an end-to-end transformer to predict ratings
directly from the raw dialogue text. The other three methods are variants of a
two-stage approach in which we first extract interpretable features at the turn
level that capture, among other aspects, user dialogue behaviors indicating
contradiction, repetition, disinterest, compliments, or criticism. We project
these features to the dialogue level and train a dialogue-level MLP regression
model, a dialogue-level LSTM, and a novel causal inference model called
counterfactual-LSTM (CF-LSTM) to predict ratings. The proposed CF-LSTM is a
sequential model over turn-level features which predicts ratings using multiple
regressors depending on hypotheses derived from the turn-level features. As a
causal inference model, CF-LSTM aims to learn the underlying causes of a
specific event, such as a low rating. We also bin the user ratings and perform
classification experiments with all four models. In evaluation experiments on
conversational data from the Alexa Prize SocialBot, we show that the CF-LSTM
achieves the best performance for predicting dialogue ratings and
classification.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Sentence Identification with BOS and EOS Label Combinations</b></summary>
  <p><b>编号</b>：[214]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13352</p>
  <p><b>作者</b>：Takuma Udagawa,  Hiroshi Kanayama,  Issei Yoshida</p>
  <p><b>备注</b>：Accepted to EACL 2023 (Findings)</p>
  <p><b>关键词</b>：NLP applications, sentence, sentence identification, NLP, task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The sentence is a fundamental unit in many NLP applications. Sentence
segmentation is widely used as the first preprocessing task, where an input
text is split into consecutive sentences considering the end of the sentence
(EOS) as their boundaries. This task formulation relies on a strong assumption
that the input text consists only of sentences, or what we call the sentential
units (SUs). However, real-world texts often contain non-sentential units
(NSUs) such as metadata, sentence fragments, nonlinguistic markers, etc. which
are unreasonable or undesirable to be treated as a part of an SU. To tackle
this issue, we formulate a novel task of sentence identification, where the
goal is to identify SUs while excluding NSUs in a given text. To conduct
sentence identification, we propose a simple yet effective method which
combines the beginning of the sentence (BOS) and EOS labels to determine the
most probable SUs and NSUs based on dynamic programming. To evaluate this task,
we design an automatic, language-independent procedure to convert the Universal
Dependencies corpora into sentence identification benchmarks. Finally, our
experiments on the sentence identification task demonstrate that our proposed
method generally outperforms sentence segmentation baselines which only utilize
EOS labels.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Differentiable Entailment for Parameter Efficient Few Shot Learning</b></summary>
  <p><b>编号</b>：[220]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13345</p>
  <p><b>作者</b>：Ethan Kim,  Jerry Yang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：adapt to downstream, limited number, pre-trained language models, pre-trained language, downstream tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Few-shot learning allows pre-trained language models to adapt to downstream
tasks while using a limited number of training examples. However, practical
applications are limited when all model parameters must be optimized. In this
work we apply a new technique for parameter efficient few shot learning while
adopting a strict definition of parameter efficiency. Our training method
combines 1) intermediate training by reformulating natural language tasks as
entailment tasks \cite{wang_entailment_2021} and 2) differentiable optimization
of template and label tokens \cite{zhang_differentiable_2021}. We quantify the
tradeoff between parameter efficiency and performance in the few-shot regime
and propose a simple model agnostic approach that can be extended to any task
By achieving competitive performance while only optimizing 3\% of a model's
parameters and allowing for batched inference, we allow for more efficient
practical deployment of models.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Proxy-based Zero-Shot Entity Linking by Effective Candidate Retrieval</b></summary>
  <p><b>编号</b>：[237]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13318</p>
  <p><b>作者</b>：Maciej Wiatrak,  Eirini Arvaniti,  Angus Brayne,  Jonas Vetterle,  Aaron Sim</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：powerful two-stage algorithms, biomedical Entity Linking, Entity Linking, candidate retrieval stage, initial candidate retrieval</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A recent advancement in the domain of biomedical Entity Linking is the
development of powerful two-stage algorithms, an initial candidate retrieval
stage that generates a shortlist of entities for each mention, followed by a
candidate ranking stage. However, the effectiveness of both stages are
inextricably dependent on computationally expensive components. Specifically,
in candidate retrieval via dense representation retrieval it is important to
have hard negative samples, which require repeated forward passes and nearest
neighbour searches across the entire entity label set throughout training. In
this work, we show that pairing a proxy-based metric learning loss with an
adversarial regularizer provides an efficient alternative to hard negative
sampling in the candidate retrieval stage. In particular, we show competitive
performance on the recall@1 metric, thereby providing the option to leave out
the expensive candidate ranking step. Finally, we demonstrate how the model can
be used in a zero-shot setting to discover out of knowledge base biomedical
entities.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Alternating Updates for Efficient Transformers</b></summary>
  <p><b>编号</b>：[242]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13310</p>
  <p><b>作者</b>：Cenk Baykal,  Dylan Cutler,  Nishanth Dikkala,  Nikhil Ghosh,  Rina Panigrahy,  Xin Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：deep transformer networks, transformer networks leads, quality and performance, networks leads, leads to improved</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>It is well established that increasing scale in deep transformer networks
leads to improved quality and performance. This increase in scale often comes
with an increase in compute cost and inference latency. Consequently, research
into methods which help realize the benefits of increased scale without leading
to an increase in the compute cost becomes important. We introduce Alternating
Updates (AltUp), a simple-to-implement method to increase a model's capacity
without the computational burden. AltUp enables the widening of the learned
representation without increasing the computation time by working on a subblock
of the representation at each layer. Our experiments on various transformer
models and language tasks demonstrate the consistent effectiveness of
alternating updates on a diverse set of benchmarks. Finally, we present
extensions of AltUp to the sequence dimension, and demonstrate how AltUp can be
synergistically combined with existing approaches, such as Sparse
Mixture-of-Experts models, to obtain efficient models with even higher
capacity.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：LongEval: Guidelines for Human Evaluation of Faithfulness in Long-form  Summarization</b></summary>
  <p><b>编号</b>：[248]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13298</p>
  <p><b>作者</b>：Kalpesh Krishna,  Erin Bransom,  Bailey Kuehl,  Mohit Iyyer,  Pradeep Dasigi,  Arman Cohan,  Kyle Lo</p>
  <p><b>备注</b>：EACL 2023 camera ready. Code and data can be found in this https URL</p>
  <p><b>关键词</b>：human evaluation remains, human evaluation, evaluating long-form summaries, accurately judging, solutions exist</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While human evaluation remains best practice for accurately judging the
faithfulness of automatically-generated summaries, few solutions exist to
address the increased difficulty and workload when evaluating long-form
summaries. Through a survey of 162 papers on long-form summarization, we first
shed light on current human evaluation practices surrounding long-form
summaries. We find that 73% of these papers do not perform any human evaluation
on model-generated summaries, while other works face new difficulties that
manifest when dealing with long documents (e.g., low inter-annotator
agreement). Motivated by our survey, we present LongEval, a set of guidelines
for human evaluation of faithfulness in long-form summaries that addresses the
following challenges: (1) How can we achieve high inter-annotator agreement on
faithfulness scores? (2) How can we minimize annotator workload while
maintaining accurate faithfulness scores? and (3) Do humans benefit from
automated alignment between summary and source snippets? We deploy LongEval in
annotation studies on two long-form summarization datasets in different domains
(SQuALITY and PubMed), and we find that switching to a finer granularity of
judgment (e.g., clause-level) reduces inter-annotator variance in faithfulness
scores (e.g., std-dev from 18.5 to 6.8). We also show that scores from a
partial annotation of fine-grained units highly correlates with scores from a
full annotation workload (0.89 Kendall's tau using 50% judgments). We release
our human judgments, annotation templates, and our software as a Python library
for future research.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Adaptive Machine Translation with Large Language Models</b></summary>
  <p><b>编号</b>：[249]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13294</p>
  <p><b>作者</b>：Yasmin Moslem,  Rejwanul Haque,  Andy Way</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：key requirement, requirement of high-quality, translation, in-context learning, high-quality translation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Consistency is a key requirement of high-quality translation. It is
especially important to adhere to pre-approved terminology and corrected
translations in domain-specific projects. Machine translation (MT) has achieved
significant progress in the area of domain adaptation. However, real-time
adaptation remains challenging. Large-scale language models (LLMs) have
recently shown interesting capabilities of in-context learning, where they
learn to replicate certain input-output text generation patterns, without
further fine-tuning. By feeding an LLM with a prompt that consists of a list of
translation pairs, it can then simulate the domain and style characteristics at
inference time. This work aims to investigate how we can utilize in-context
learning to improve real-time adaptive MT. Our extensive experiments show
promising results at translation time. For example, GPT-3.5 can adapt to a set
of in-domain sentence pairs and/or terminology while translating a new
sentence. We observe that the translation quality with few-shot in-context
learning can surpass that of strong encoder-decoder MT systems, especially for
high-resource languages. Moreover, we investigate whether we can combine MT
from strong encoder-decoder models with fuzzy matches, which can further
improve the translation, especially for less supported languages. We conduct
our experiments across five diverse languages, namely English-to-Arabic
(EN-AR), English-to-Chinese (EN-ZH), English-to-French (EN-FR),
English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES) language pairs.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Contextual Dynamic Prompting for Response Generation in Task-oriented  Dialog Systems</b></summary>
  <p><b>编号</b>：[261]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13268</p>
  <p><b>作者</b>：Sandesh Swamy,  Narges Tabari,  Chacha Chen,  Rashmi Gangadharaiah</p>
  <p><b>备注</b>：Accepted at EACL 2023 as a short paper</p>
  <p><b>关键词</b>：critical components, task-oriented dialog systems, Response generation, components in task-oriented, dialog</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Response generation is one of the critical components in task-oriented dialog
systems. Existing studies have shown that large pre-trained language models can
be adapted to this task. The typical paradigm of adapting such extremely large
language models would be by fine-tuning on the downstream tasks which is not
only time-consuming but also involves significant resources and access to
fine-tuning data. Prompting \citep{schick2020exploiting} has been an
alternative to fine-tuning in many NLP tasks. In our work, we explore the idea
of using prompting for response generation in task-oriented dialog systems.
Specifically, we propose an approach that performs \textit{contextual dynamic
prompting} where the prompts are learnt from dialog contexts. We aim to distill
useful prompting signals from the dialog context. On experiments with MultiWOZ
2.2 dataset \cite{zang2020multiwoz}, we show that contextual dynamic prompts
improve response generation in terms of \textit{combined score}
\cite{mehri-etal-2019-structured} by 3 absolute points, and a massive 20 points
when dialog states are incorporated. Furthermore, human annotation on these
conversations found that agents which incorporate context were preferred over
agents with vanilla prefix-tuning.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：ArchiSound: Audio Generation with Diffusion</b></summary>
  <p><b>编号</b>：[262]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13267</p>
  <p><b>作者</b>：Flavio Schneider</p>
  <p><b>备注</b>：Master Thesis at ETH Zurich</p>
  <p><b>关键词</b>：diffusion models, recent surge, surge in popularity, brought new attention, audio generation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The recent surge in popularity of diffusion models for image generation has
brought new attention to the potential of these models in other areas of media
generation. One area that has yet to be fully explored is the application of
diffusion models to audio generation. Audio generation requires an
understanding of multiple aspects, such as the temporal dimension, long term
structure, multiple layers of overlapping sounds, and the nuances that only
trained listeners can detect. In this work, we investigate the potential of
diffusion models for audio generation. We propose a set of models to tackle
multiple aspects, including a new method for text-conditional latent audio
diffusion with stacked 1D U-Nets, that can generate multiple minutes of music
from a textual description. For each model, we make an effort to maintain
reasonable inference speed, targeting real-time on a single consumer GPU. In
addition to trained models, we provide a collection of open source libraries
with the hope of simplifying future work in the field. Samples can be found at
this https URL. Codes are at
this https URL.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Physarum Inspired Bicycle Lane Network Design in a Congested Mega City</b></summary>
  <p><b>编号</b>：[290]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13609</p>
  <p><b>作者</b>：Md. Ahsan Habib,  M. A. H. Akhand</p>
  <p><b>备注</b>：15 tables, 8 figures, 59 pages</p>
  <p><b>关键词</b>：factor in urban, urban life, plays a vital, vital role, unplanned mega city</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Mobility is a key factor in urban life and transport network plays a vital
role in mobility. Worse transport network having less mobility is one of the
key reasons to decline the living standard in any unplanned mega city.
Transport mobility enhancement in an unplanned mega city is always challenging
due to various constraints including complex design and high cost involvement.
The aim of this thesis is to enhance transport mobility in a megacity
introducing a bicycle lane. To design the bicycle lane natural Physarum,
brainless single celled multi-nucleated protist, is studied and modified for
better optimization. Recently Physarum inspired techniques are drawn
significant attention to the construction of effective networks. Exiting
Physarum inspired models effectively and efficiently solves different problems
including transport network design and modification and implication for bicycle
lane is the unique contribution of this study. Central area of Dhaka, the
capital city of Bangladesh, is considered to analyze and design the bicycle
lane network bypassing primary roads.</p>
  </details>
</details>
<h1>机器学习</h1>
<details>
  <summary>1. <b>标题：PADL: Language-Directed Physics-Based Character Control</b></summary>
  <p><b>编号</b>：[2]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13868</p>
  <p><b>作者</b>：Jordan Juravsky,  Yunrong Guo,  Sanja Fidler,  Xue Bin Peng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：focus for computer, Natural language, life-like motions, language, language commands</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Developing systems that can synthesize natural and life-like motions for
simulated characters has long been a focus for computer animation. But in order
for these systems to be useful for downstream applications, they need not only
produce high-quality motions, but must also provide an accessible and versatile
interface through which users can direct a character's behaviors. Natural
language provides a simple-to-use and expressive medium for specifying a user's
intent. Recent breakthroughs in natural language processing (NLP) have
demonstrated effective use of language-based interfaces for applications such
as image generation and program synthesis. In this work, we present PADL, which
leverages recent innovations in NLP in order to take steps towards developing
language-directed controllers for physics-based character animation. PADL
allows users to issue natural language commands for specifying both high-level
tasks and low-level skills that a character should perform. We present an
adversarial imitation learning approach for training policies to map high-level
language commands to low-level controls that enable a character to perform the
desired task and skill specified by a user's commands. Furthermore, we propose
a multi-task aggregation method that leverages a language-based multiple-choice
question-answering approach to determine high-level task objectives from
language commands. We show that our framework can be applied to effectively
direct a simulated humanoid character to perform a diverse array of complex
motor skills.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Mathematical Capabilities of ChatGPT</b></summary>
  <p><b>编号</b>：[3]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13867</p>
  <p><b>作者</b>：Simon Frieder,  Luca Pinchetti,  Ryan-Rhys Griffiths,  Tommaso Salvatori,  Thomas Lukasiewicz,  Philipp Christian Petersen,  Alexis Chevalier,  Julius Berner</p>
  <p><b>备注</b>：The GHOSTS dataset will be available at this https URL</p>
  <p><b>关键词</b>：Lean Mathematical Library, mathematical, ChatGPT, mathematics, mathematical corpus</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We investigate the mathematical capabilities of ChatGPT by testing it on
publicly available datasets, as well as hand-crafted ones, and measuring its
performance against other models trained on a mathematical corpus, such as
Minerva. We also test whether ChatGPT can be a useful assistant to professional
mathematicians by emulating various use cases that come up in the daily
professional activities of mathematicians (question answering, theorem
searching). In contrast to formal mathematics, where large databases of formal
proofs are available (e.g., the Lean Mathematical Library), current datasets of
natural-language mathematics, used to benchmark language models, only cover
elementary mathematics. We address this issue by introducing a new dataset:
GHOSTS. It is the first natural-language dataset made and curated by working
researchers in mathematics that (1) aims to cover graduate-level mathematics
and (2) provides a holistic overview of the mathematical capabilities of
language models. We benchmark ChatGPT on GHOSTS and evaluate performance
against fine-grained criteria. We make this new dataset publicly available to
assist a community-driven comparison of ChatGPT with (future) large language
models in terms of advanced mathematical comprehension. We conclude that
contrary to many positive reports in the media (a potential case of selection
bias), ChatGPT's mathematical abilities are significantly below those of an
average mathematics graduate student. Our results show that ChatGPT often
understands the question but fails to provide correct solutions. Hence, if your
goal is to use it to pass a university exam, you would be better off copying
from your average peer!</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Salient Conditional Diffusion for Defending Against Backdoor Attacks</b></summary>
  <p><b>编号</b>：[5]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13862</p>
  <p><b>作者</b>：Brandon B. May,  N. Joseph Tatro,  Piyush Kumar,  Nathan Shnidman</p>
  <p><b>备注</b>：12 pages, 5 figures</p>
  <p><b>关键词</b>：Salient Conditional Diffusion, Conditional Diffusion, Salient Conditional, backdoor attacks, Diffusion</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a novel algorithm, Salient Conditional Diffusion (Sancdifi), a
state-of-the-art defense against backdoor attacks. Sancdifi uses a denoising
diffusion probabilistic model (DDPM) to degrade an image with noise and then
recover said image using the learned reverse diffusion. Critically, we compute
saliency map-based masks to condition our diffusion, allowing for stronger
diffusion on the most salient pixels by the DDPM. As a result, Sancdifi is
highly effective at diffusing out triggers in data poisoned by backdoor
attacks. At the same time, it reliably recovers salient features when applied
to clean data. This performance is achieved without requiring access to the
model parameters of the Trojan network, meaning Sancdifi operates as a
black-box defense.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Learning in POMDPs is Sample-Efficient with Hindsight Observability</b></summary>
  <p><b>编号</b>：[7]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13857</p>
  <p><b>作者</b>：Jonathan N. Lee,  Alekh Agarwal,  Christoph Dann,  Tong Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：hardness results suggest, decision making problems, inherent partial observability, simple settings due, capture a broad</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>POMDPs capture a broad class of decision making problems, but hardness
results suggest that learning is intractable even in simple settings due to the
inherent partial observability. However, in many realistic problems, more
information is either revealed or can be computed during some point of the
learning process. Motivated by diverse applications ranging from robotics to
data center scheduling, we formulate a \setting (\setshort) as a POMDP where
the latent states are revealed to the learner in hindsight and only during
training. We introduce new algorithms for the tabular and function
approximation settings that are provably sample-efficient with hindsight
observability, even in POMDPs that would otherwise be statistically
intractable. We give a lower bound showing that the tabular algorithm is
optimal in its dependence on latent state and observation cardinalities.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：ChatGPT or Human? Detect and Explain. Explaining Decisions of Machine  Learning Model for Detecting Short ChatGPT-generated Text</b></summary>
  <p><b>编号</b>：[9]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13852</p>
  <p><b>作者</b>：Sandra Mitrović,  Davide Andreoletti,  Omran Ayoub</p>
  <p><b>备注</b>：11 pages, 8 figures, 2 tables</p>
  <p><b>关键词</b>：generate grammatically flawless, ability to generate, generate grammatically, grammatically flawless, flawless and seemingly-human</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>ChatGPT has the ability to generate grammatically flawless and
seemingly-human replies to different types of questions from various domains.
The number of its users and of its applications is growing at an unprecedented
rate. Unfortunately, use and abuse come hand in hand. In this paper, we study
whether a machine learning model can be effectively trained to accurately
distinguish between original human and seemingly human (that is,
ChatGPT-generated) text, especially when this text is short. Furthermore, we
employ an explainable artificial intelligence framework to gain insight into
the reasoning behind the model trained to differentiate between
ChatGPT-generated and human-generated text. The goal is to analyze model's
decisions and determine if any specific patterns or characteristics can be
identified. Our study focuses on short online reviews, conducting two
experiments comparing human-generated and ChatGPT-generated text. The first
experiment involves ChatGPT text generated from custom queries, while the
second experiment involves text generated by rephrasing original
human-generated reviews. We fine-tune a Transformer-based model and use it to
make predictions, which are then explained using SHAP. We compare our model
with a perplexity score-based approach and find that disambiguation between
human and ChatGPT-generated reviews is more challenging for the ML model when
using rephrased text. However, our proposed approach still achieves an accuracy
of 79%. Using explainability, we observe that ChatGPT's writing is polite,
without specific details, using fancy and atypical vocabulary, impersonal, and
typically it does not express feelings.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Benchmarking Large Language Models for News Summarization</b></summary>
  <p><b>编号</b>：[10]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13848</p>
  <p><b>作者</b>：Tianyi Zhang,  Faisal Ladhak,  Esin Durmus,  Percy Liang,  Kathleen McKeown,  Tatsunori B. Hashimoto</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large language models, Large language, poorly understood, shown promise, promise for automatic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have shown promise for automatic summarization
but the reasons behind their successes are poorly understood. By conducting a
human evaluation on ten LLMs across different pretraining methods, prompts, and
model scales, we make two important observations. First, we find instruction
tuning, and not model size, is the key to the LLM's zero-shot summarization
capability. Second, existing studies have been limited by low-quality
references, leading to underestimates of human performance and lower few-shot
and finetuning performance. To better evaluate LLMs, we perform human
evaluation over high-quality summaries we collect from freelance writers.
Despite major stylistic differences such as the amount of paraphrasing, we find
that LMM summaries are judged to be on par with human written summaries.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Interpreting Robustness Proofs of Deep Neural Networks</b></summary>
  <p><b>编号</b>：[11]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13845</p>
  <p><b>作者</b>：Debangshu Banerjee,  Avaljot Singh,  Gagandeep Singh</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：deep neural networks, recent years numerous, years numerous methods, neural networks, recent years</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years numerous methods have been developed to formally verify the
robustness of deep neural networks (DNNs). Though the proposed techniques are
effective in providing mathematical guarantees about the DNNs behavior, it is
not clear whether the proofs generated by these methods are
human-interpretable. In this paper, we bridge this gap by developing new
concepts, algorithms, and representations to generate human understandable
interpretations of the proofs. Leveraging the proposed method, we show that the
robustness proofs of standard DNNs rely on spurious input features, while the
proofs of DNNs trained to be provably robust filter out even the semantically
meaningful features. The proofs for the DNNs combining adversarial and provably
robust training are the most effective at selectively filtering out spurious
features as well as relying on human-understandable input features.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Image Shortcut Squeezing: Countering Perturbative Availability Poisons  with Compression</b></summary>
  <p><b>编号</b>：[14]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13838</p>
  <p><b>作者</b>：Zhuoran Liu,  Zhengyu Zhao,  Martha Larson</p>
  <p><b>备注</b>：Our code is available at this https URL</p>
  <p><b>关键词</b>：adds small, Image Shortcut Squeezing, Perturbative availability poisoning, PAP, ISS</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Perturbative availability poisoning (PAP) adds small changes to images to
prevent their use for model training. Current research adopts the belief that
practical and effective approaches to countering such poisons do not exist. In
this paper, we argue that it is time to abandon this belief. We present
extensive experiments showing that 12 state-of-the-art PAP methods are
vulnerable to Image Shortcut Squeezing (ISS), which is based on simple
compression. For example, on average, ISS restores the CIFAR-10 model accuracy
to $81.73\%$, surpassing the previous best preprocessing-based countermeasures
by $37.97\%$ absolute. ISS also (slightly) outperforms adversarial training and
has higher generalizability to unseen perturbation norms and also higher
efficiency. Our investigation reveals that the property of PAP perturbations
depends on the type of surrogate model used for poison generation, and it
explains why a specific ISS compression yields the best performance for a
specific type of PAP perturbation. We further test stronger, adaptive
poisoning, and show it falls short of being an ideal defense against ISS.
Overall, our results demonstrate the importance of considering various (simple)
countermeasures to ensure the meaningfulness of analysis carried out during the
development of availability poisons.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：A Mathematical Model for Curriculum Learning</b></summary>
  <p><b>编号</b>：[16]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13833</p>
  <p><b>作者</b>：Elisabetta Cornacchia,  Elchanan Mossel</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：machine learning context, meaningful order, decade ago, generated and presented, learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Curriculum learning (CL) - training using samples that are generated and
presented in a meaningful order - was introduced in the machine learning
context around a decade ago. While CL has been extensively used and analysed
empirically, there has been very little mathematical justification for its
advantages. We introduce a CL model for learning the class of k-parities on d
bits of a binary string with a neural network trained by stochastic gradient
descent (SGD). We show that a wise choice of training examples, involving two
or more product distributions, allows to reduce significantly the computational
cost of learning this class of functions, compared to learning under the
uniform distribution. We conduct experiments to support our analysis.
Furthermore, we show that for another class of functions - namely the `Hamming
mixtures' - CL strategies involving a bounded number of product distributions
are not beneficial, while we conjecture that CL with unbounded many curriculum
steps can learn this class efficiently.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image  Diffusion Models</b></summary>
  <p><b>编号</b>：[19]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13826</p>
  <p><b>作者</b>：Hila Chefer,  Yuval Alaluf,  Yael Vinker,  Lior Wolf,  Daniel Cohen-Or</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：creative imagery guided, target text prompt, Stable Diffusion model, text prompt, demonstrated an unparalleled</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent text-to-image generative models have demonstrated an unparalleled
ability to generate diverse and creative imagery guided by a target text
prompt. While revolutionary, current state-of-the-art diffusion models may
still fail in generating images that fully convey the semantics in the given
text prompt. We analyze the publicly available Stable Diffusion model and
assess the existence of catastrophic neglect, where the model fails to generate
one or more of the subjects from the input prompt. Moreover, we find that in
some cases the model also fails to correctly bind attributes (e.g., colors) to
their corresponding subjects. To help mitigate these failure cases, we
introduce the concept of Generative Semantic Nursing (GSN), where we seek to
intervene in the generative process on the fly during inference time to improve
the faithfulness of the generated images. Using an attention-based formulation
of GSN, dubbed Attend-and-Excite, we guide the model to refine the
cross-attention units to attend to all subject tokens in the text prompt and
strengthen - or excite - their activations, encouraging the model to generate
all subjects described in the text prompt. We compare our approach to
alternative approaches and demonstrate that it conveys the desired concepts
more faithfully across a range of text prompts.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Grounding Language Models to Images for Multimodal Generation</b></summary>
  <p><b>编号</b>：[20]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13823</p>
  <p><b>作者</b>：Jing Yu Koh,  Ruslan Salakhutdinov,  Daniel Fried</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：visual domain, language models, text-only language models, ground pretrained text-only, propose an efficient</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose an efficient method to ground pretrained text-only language models
to the visual domain, enabling them to process and generate arbitrarily
interleaved image-and-text data. Our method leverages the abilities of language
models learnt from large scale text-only pretraining, such as in-context
learning and free-form text generation. We keep the language model frozen, and
finetune input and output linear layers to enable cross-modality interactions.
This allows our model to process arbitrarily interleaved image-and-text inputs,
and generate free-form text interleaved with retrieved images. We achieve
strong zero-shot performance on grounded tasks such as contextual image
retrieval and multimodal dialogue, and showcase compelling interactive
abilities. Our approach works with any off-the-shelf language model and paves
the way towards an effective, general solution for leveraging pretrained
language models in visually grounded settings.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Complete Neural Networks for Euclidean Graphs</b></summary>
  <p><b>编号</b>：[21]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13821</p>
  <p><b>作者</b>：Snir Hordan,  Tal Amir,  Steven J. Gortler,  Nadav Dym</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：graph isomorphism test, geometric graph isomorphism, Euclidean Graphs, applied to Euclidean, graph isomorphism</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a 2-WL-like geometric graph isomorphism test and prove it is
complete when applied to Euclidean Graphs in $\mathbb{R}^3$. We then use recent
results on multiset embeddings to devise an efficient geometric GNN model with
equivalent separation power. We verify empirically that our GNN model is able
to separate particularly challenging synthetic examples, and demonstrate its
usefulness for a chemical property prediction problem.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Causal-Discovery Performance of ChatGPT in the context of Neuropathic  Pain Diagnosis</b></summary>
  <p><b>编号</b>：[23]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13819</p>
  <p><b>作者</b>：Ruibo Tu,  Chao Ma,  Cheng Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：natural language conversation, demonstrated exceptional proficiency, previous large language, large language models, ChatGPT has demonstrated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>ChatGPT has demonstrated exceptional proficiency in natural language
conversation, e.g., it can answer a wide range of questions while no previous
large language models can. Thus, we would like to push its limit and explore
its ability to answer causal discovery questions by using a medical benchmark
(Tu et al. 2019) in causal discovery.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Execution-based Code Generation using Deep Reinforcement Learning</b></summary>
  <p><b>编号</b>：[25]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13816</p>
  <p><b>作者</b>：Parshin Shojaee,  Aneesh Jain,  Sindhu Tipirneni,  Chandan K. Reddy</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：automating software engineering, software engineering processes, demonstrated considerable potential, large-scale code corpora, programming language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The utilization of programming language (PL) models, pretrained on
large-scale code corpora, as a means of automating software engineering
processes has demonstrated considerable potential in streamlining various code
generation tasks such as code completion, code translation, and program
synthesis. However, current approaches mainly rely on supervised fine-tuning
objectives borrowed from text generation, neglecting specific sequence-level
features of code, including but not limited to compilability as well as
syntactic and functional correctness. To address this limitation, we propose
PPOCoder, a new framework for code generation that combines pretrained PL
models with Proximal Policy Optimization (PPO) deep reinforcement learning and
employs execution feedback as the external source of knowledge into the model
optimization. PPOCoder is transferable across different code generation tasks
and PLs. Extensive experiments on three code generation tasks demonstrate the
effectiveness of our proposed approach compared to SOTA methods, improving the
success rate of compilation and functional correctness over different PLs. Our
code can be found at this https URL .</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Identifying the Hazard Boundary of ML-enabled Autonomous Systems Using  Cooperative Co-Evolutionary Search</b></summary>
  <p><b>编号</b>：[29]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13807</p>
  <p><b>作者</b>：Sepehr Sharifi,  Donghwan Shin,  Lionel C. Briand,  Nathan Aschbacher</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Machine Learning, hazard boundary, enabled autonomous systems, Component Safety Hazard, essential to identify</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In Machine Learning (ML)-enabled autonomous systems (MLASs), it is essential
to identify the hazard boundary of ML Components (MLCs) in the MLAS under
analysis. Given that such boundary captures the conditions in terms of MLC
behavior and system context that can lead to hazards, it can then be used to,
for example, build a safety monitor that can take any predefined fallback
mechanisms at runtime when reaching the hazard boundary. However, determining
such hazard boundary for an ML component is challenging. This is due to the
space combining system contexts (i.e., scenarios) and MLC behaviors (i.e.,
inputs and outputs) being far too large for exhaustive exploration and even to
handle using conventional metaheuristics, such as genetic algorithms.
Additionally, the high computational cost of simulations required to determine
any MLAS safety violations makes the problem even more challenging.
Furthermore, it is unrealistic to consider a region in the problem space
deterministically safe or unsafe due to the uncontrollable parameters in
simulations and the non-linear behaviors of ML models (e.g., deep neural
networks) in the MLAS under analysis. To address the challenges, we propose
MLCSHE (ML Component Safety Hazard Envelope), a novel method based on a
Cooperative Co-Evolutionary Algorithm (CCEA), which aims to tackle a
high-dimensional problem by decomposing it into two lower-dimensional search
subproblems. Moreover, we take a probabilistic view of safe and unsafe regions
and define a novel fitness function to measure the distance from the
probabilistic hazard boundary and thus drive the search effectively. We
evaluate the effectiveness and efficiency of MLCSHE on a complex Autonomous
Vehicle (AV) case study. Our evaluation results show that MLCSHE is
significantly more effective and efficient compared to a standard genetic
algorithm and random search.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Fairness-aware Vision Transformer via Debiased Self-Attention</b></summary>
  <p><b>编号</b>：[31]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13803</p>
  <p><b>作者</b>：Yao Qiang,  Chengyin Li,  Prashant Khanduri,  Dongxiao Zhu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：solving computer vision, recently gained significant, gained significant interest, modeling long-range dependencies, Vision Transformer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision Transformer (ViT) has recently gained significant interest in solving
computer vision (CV) problems due to its capability of extracting informative
features and modeling long-range dependencies through the self-attention
mechanism. To fully realize the advantages of ViT in real-world applications,
recent works have explored the trustworthiness of ViT, including its robustness
and explainability. However, another desiderata, fairness has not yet been
adequately addressed in the literature. We establish that the existing
fairness-aware algorithms (primarily designed for CNNs) do not perform well on
ViT. This necessitates the need for developing our novel framework via Debiased
Self-Attention (DSA). DSA is a fairness-through-blindness approach that
enforces ViT to eliminate spurious features correlated with the sensitive
attributes for bias mitigation. Notably, adversarial examples are leveraged to
locate and mask the spurious features in the input image patches. In addition,
DSA utilizes an attention weights alignment regularizer in the training
objective to encourage learning informative features for target prediction.
Importantly, our DSA framework leads to improved fairness guarantees over prior
works on multiple prediction tasks without compromising target prediction
performance</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Partitioning Distributed Compute Jobs with Reinforcement Learning and  Graph Neural Networks</b></summary>
  <p><b>编号</b>：[33]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13799</p>
  <p><b>作者</b>：Christopher W. F. Parsonson,  Zacharaya Shabka,  Alessandro Ottino,  Georgios Zervas</p>
  <p><b>备注</b>：Pre-print</p>
  <p><b>关键词</b>：natural language processing, large-scale machine learning, genome sequencing, range of fields, natural language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>From natural language processing to genome sequencing, large-scale machine
learning models are bringing advances to a broad range of fields. Many of these
models are too large to be trained on a single machine, and instead must be
distributed across multiple devices. This has motivated the research of new
compute and network systems capable of handling such tasks. In particular,
recent work has focused on developing management schemes which decide how to
allocate distributed resources such that some overall objective, such as
minimising the job completion time (JCT), is optimised. However, such studies
omit explicit consideration of how much a job should be distributed, usually
assuming that maximum distribution is desirable. In this work, we show that
maximum parallelisation is sub-optimal in relation to user-critical metrics
such as throughput and blocking rate. To address this, we propose PAC-ML
(partitioning for asynchronous computing with machine learning). PAC-ML
leverages a graph neural network and reinforcement learning to learn how much
to partition computation graphs such that the number of jobs which meet
arbitrary user-defined JCT requirements is maximised. In experiments with five
real deep learning computation graphs on a recently proposed optical
architecture across four user-defined JCT requirement distributions, we
demonstrate PAC-ML achieving up to 56.2% lower blocking rates in dynamic job
arrival settings than the canonical maximum parallelisation strategy used by
most prior works.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Multicalibration as Boosting for Regression</b></summary>
  <p><b>编号</b>：[41]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13767</p>
  <p><b>作者</b>：Ira Globus-Harris,  Declan Harrison,  Michael Kearns,  Aaron Roth,  Jessica Sorrell</p>
  <p><b>备注</b>：Code available here: this https URL</p>
  <p><b>关键词</b>：squared error regression, squared error, study the connection, error regression, multicalibration</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the connection between multicalibration and boosting for squared
error regression. First we prove a useful characterization of multicalibration
in terms of a ``swap regret'' like condition on squared error. Using this
characterization, we give an exceedingly simple algorithm that can be analyzed
both as a boosting algorithm for regression and as a multicalibration algorithm
for a class H that makes use only of a standard squared error regression oracle
for H. We give a weak learning assumption on H that ensures convergence to
Bayes optimality without the need to make any realizability assumptions --
giving us an agnostic boosting algorithm for regression. We then show that our
weak learning assumption on H is both necessary and sufficient for
multicalibration with respect to H to imply Bayes optimality. We also show that
if H satisfies our weak learning condition relative to another class C then
multicalibration with respect to H implies multicalibration with respect to C.
Finally we investigate the empirical performance of our algorithm
experimentally using an open source implementation that we make available. Our
code repository can be found at
this https URL.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Semi-Supervised Classification with Graph Convolutional Kernel Machines</b></summary>
  <p><b>编号</b>：[42]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13764</p>
  <p><b>作者</b>：Sonny Achten,  Francesco Tonin,  Panagiotis Patrinos,  Johan A. K. Suykens</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Convolutional Kernel Machine, Graph Convolutional Kernel, Kernel Machine, deep Graph Convolutional, Convolutional Kernel</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a deep Graph Convolutional Kernel Machine (GCKM) for
semi-supervised node classification in graphs. First, we introduce an
unsupervised kernel machine propagating the node features in a one-hop
neighbourhood. Then, we specify a semi-supervised classification kernel machine
through the lens of the Fenchel-Young inequality. The deep graph convolutional
kernel machine is obtained by stacking multiple shallow kernel machines. After
showing that unsupervised and semi-supervised layer corresponds to an
eigenvalue problem and a linear system on the aggregated node features,
respectively, we derive an efficient end-to-end training algorithm in the dual
variables. Numerical experiments demonstrate that our approach is competitive
with state-of-the-art graph neural networks for homophilious and heterophilious
benchmark datasets. Notably, GCKM achieves superior performance when very few
labels are available.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Learning, Fast and Slow: A Goal-Directed Memory-Based Approach for  Dynamic Environments</b></summary>
  <p><b>编号</b>：[45]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13758</p>
  <p><b>作者</b>：Tan Chong Min John,  Mehul Motani</p>
  <p><b>备注</b>：22 pages</p>
  <p><b>关键词</b>：state prediction, neural network, state, prediction, goal-directed exploration</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Model-based next state prediction and state value prediction are slow to
converge. To address these challenges, we do the following: i) Instead of a
neural network, we do model-based planning using a parallel memory retrieval
system (which we term the slow mechanism); ii) Instead of learning state
values, we guide the agent's actions using goal-directed exploration, by using
a neural network to choose the next action given the current state and the goal
state (which we term the fast mechanism). The goal-directed exploration is
trained online using hippocampal replay of visited states and future imagined
states every single time step, leading to fast and efficient training.
Empirical studies show that our proposed method has a 92% solve rate across 100
episodes in a dynamically changing grid world, significantly outperforming
state-of-the-art actor critic mechanisms such as PPO (54%), TRPO (50%) and A2C
(24%). Ablation studies demonstrate that both mechanisms are crucial. We posit
that the future of Reinforcement Learning (RL) will be to model goals and
sub-goals for various tasks, and plan it out in a goal-directed memory-based
approach.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Toward Efficient Gradient-Based Value Estimation</b></summary>
  <p><b>编号</b>：[46]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13757</p>
  <p><b>作者</b>：Arsalan Sharifnassab,  Richard Sutton</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：favorable stability properties, Square Bellman Error, stability properties, estimation in reinforcement, favorable stability</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Gradient-based methods for value estimation in reinforcement learning have
favorable stability properties, but they are typically much slower than
Temporal Difference (TD) learning methods. We study the root causes of this
slowness and show that Mean Square Bellman Error (MSBE) is an ill-conditioned
loss function in the sense that its Hessian has large condition-number. To
resolve the adverse effect of poor conditioning of MSBE on gradient based
methods, we propose a low complexity batch-free proximal method that
approximately follows the Gauss-Newton direction and is asymptotically robust
to parameterization. Our main algorithm, called RANS, is efficient in the sense
that it is significantly faster than the residual gradient methods while having
almost the same computational complexity, and is competitive with TD on the
classic problems that we tested.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Retrosynthetic Planning with Dual Value Networks</b></summary>
  <p><b>编号</b>：[48]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13755</p>
  <p><b>作者</b>：Guoqing Liu,  Di Xue,  Shufang Xie,  Yingce Xia,  Austin Tripp,  Krzysztof Maziarz,  Marwin Segler,  Tao Qin,  Zongzhang Zhang,  Tie-Yan Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：single-step accuracy, starting materials, single-step, synthesize a target, commercially available starting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Retrosynthesis, which aims to find a route to synthesize a target molecule
from commercially available starting materials, is a critical task in drug
discovery and materials design. Recently, the combination of ML-based
single-step reaction predictors with multi-step planners has led to promising
results. However, the single-step predictors are mostly trained offline to
optimize the single-step accuracy, without considering complete routes. Here,
we leverage reinforcement learning (RL) to improve the single-step predictor,
by using a tree-shaped MDP to optimize complete routes while retaining
single-step accuracy. Desirable routes should be both synthesizable and of low
cost. We propose an online training algorithm, called Planning with Dual Value
Networks (PDVN), in which two value networks predict the synthesizability and
cost of molecules, respectively. To maintain the single-step accuracy, we
design a two-branch network structure for the single-step predictor. On the
widely-used USPTO dataset, our PDVN algorithm improves the search success rate
of existing multi-step planners (e.g., increasing the success rate from 85.79%
to 98.95% for Retro*, and reducing the number of model calls by half while
solving 99.47% molecules for RetroGraph). Furthermore, PDVN finds shorter
synthesis routes (e.g., reducing the average route length from 5.76 to 4.83 for
Retro*, and from 5.63 to 4.78 for RetroGraph).</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Archetypal Analysis++: Rethinking the Initialization Strategy</b></summary>
  <p><b>编号</b>：[50]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13748</p>
  <p><b>作者</b>：Sebastian Mair,  Jens Sjölund</p>
  <p><b>备注</b>：20 pages, 13 figures, preprint</p>
  <p><b>关键词</b>：matrix factorization method, convexity constraints, matrix factorization, Archetypal analysis, local minima</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Archetypal analysis is a matrix factorization method with convexity
constraints. Due to local minima, a good initialization is essential.
Frequently used initialization methods yield either sub-optimal starting points
or are prone to get stuck in poor local minima. In this paper, we propose
archetypal analysis++ (AA++), a probabilistic initialization strategy for
archetypal analysis that sequentially samples points based on their influence
on the objective, similar to $k$-means++. In fact, we argue that $k$-means++
already approximates the proposed initialization method. Furthermore, we
suggest to adapt an efficient Monte Carlo approximation of $k$-means++ to AA++.
In an extensive empirical evaluation of 13 real-world data sets of varying
sizes and dimensionalities and considering two pre-processing strategies, we
show that AA++ almost consistently outperforms all baselines, including the
most frequently used ones.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Zero-shot-Learning Cross-Modality Data Translation Through Mutual  Information Guided Stochastic Diffusion</b></summary>
  <p><b>编号</b>：[51]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13743</p>
  <p><b>作者</b>：Zihao Wang,  Yingyu Yang,  Maxime Sermesant,  Hervé Delingette,  Ona Wu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：attracted great interest, Cross-modality data translation, data translation, data translation Model, Cross-modality data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cross-modality data translation has attracted great interest in image
computing. Deep generative models (\textit{e.g.}, GANs) show performance
improvement in tackling those problems. Nevertheless, as a fundamental
challenge in image translation, the problem of Zero-shot-Learning
Cross-Modality Data Translation with fidelity remains unanswered. This paper
proposes a new unsupervised zero-shot-learning method named Mutual Information
guided Diffusion cross-modality data translation Model (MIDiffusion), which
learns to translate the unseen source data to the target domain. The
MIDiffusion leverages a score-matching-based generative model, which learns the
prior knowledge in the target domain. We propose a differentiable
local-wise-MI-Layer ($LMI$) for conditioning the iterative denoising sampling.
The $LMI$ captures the identical cross-modality features in the statistical
domain for the diffusion guidance; thus, our method does not require retraining
when the source domain is changed, as it does not rely on any direct mapping
between the source and target domains. This advantage is critical for applying
cross-modality data translation methods in practice, as a reasonable amount of
source domain dataset is not always available for supervised training. We
empirically show the advanced performance of MIDiffusion in comparison with an
influential group of generative models, including adversarial-based and other
score-matching-based models.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：UPop: Unified and Progressive Pruning for Compressing Vision-Language  Transformers</b></summary>
  <p><b>编号</b>：[52]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13741</p>
  <p><b>作者</b>：Dachuan Shi,  Chaofan Tao,  Ying Jin,  Zhendong Yang,  Chun Yuan,  Jiaqi Wang</p>
  <p><b>备注</b>：16 pages, 5 figures, 13 tables</p>
  <p><b>关键词</b>：Real-world data, vast amount, vision and language, textbf, vison-language Transformer compression</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Real-world data contains a vast amount of multimodal information, among which
vision and language are the two most representative modalities. Moreover,
increasingly heavier models, e.g., Transformers, have attracted the attention
of researchers to model compression. However, how to compress multimodal
models, especially vison-language Transformers, is still under-explored. This
paper proposes the \textbf{U}nified and \textbf{P}r\textbf{o}gressive
\textbf{P}runing (UPop) as a universal vison-language Transformer compression
framework, which incorporates 1) unifiedly searching multimodal subnets in a
continuous optimization space from the original model, which enables automatic
assignment of pruning ratios among compressible modalities and structures; 2)
progressively searching and retraining the subnet, which maintains convergence
between the search and retrain to attain higher compression ratios. Experiments
on multiple generative and discriminative vision-language tasks, including
Visual Reasoning, Image Caption, Visual Question Answer, Image-Text Retrieval,
Text-Image Retrieval, and Image Classification, demonstrate the effectiveness
and versatility of the proposed UPop framework.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Self-Consistent Velocity Matching of Probability Flows</b></summary>
  <p><b>编号</b>：[53]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13737</p>
  <p><b>作者</b>：Lingxiao Li,  Samuel Hurault,  Justin Solomon</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Wasserstein gradient flow, mass-conserving partial differential, partial differential equations, discretization-free scalable framework, Wasserstein gradient</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a discretization-free scalable framework for solving a large class
of mass-conserving partial differential equations (PDEs), including the
time-dependent Fokker-Planck equation and the Wasserstein gradient flow. The
main observation is that the time-varying velocity field of the PDE solution
needs to be self-consistent: it must satisfy a fixed-point equation involving
the flow characterized by the same velocity field. By parameterizing the flow
as a time-dependent neural network, we propose an end-to-end iterative
optimization framework called self-consistent velocity matching to solve this
class of PDEs. Compared to existing approaches, our method does not suffer from
temporal or spatial discretization, covers a wide range of PDEs, and scales to
high dimensions. Experimentally, our method recovers analytical solutions
accurately when they are available and achieves comparable or better
performance in high dimensions with less training time compared to recent
large-scale JKO-based methods that are designed for solving a more restrictive
family of PDEs.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Improving Monte Carlo Evaluation with Offline Data</b></summary>
  <p><b>编号</b>：[55]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13734</p>
  <p><b>作者</b>：Shuze Liu,  Shangtong Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Monte Carlo, online samples, samples, policy, online</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Monte Carlo (MC) methods are the most widely used methods to estimate the
performance of a policy. Given an interested policy, MC methods give estimates
by repeatedly running this policy to collect samples and taking the average of
the outcomes. Samples collected during this process are called online samples.
To get an accurate estimate, MC methods consume massive online samples. When
online samples are expensive, e.g., online recommendations and inventory
management, we want to reduce the number of online samples while achieving the
same estimate accuracy. To this end, we use off-policy MC methods that evaluate
the interested policy by running a different policy called behavior policy. We
design a tailored behavior policy such that the variance of the off-policy MC
estimator is provably smaller than the ordinary MC estimator. Importantly, this
tailored behavior policy can be efficiently learned from existing offline data,
i,e., previously logged data, which are much cheaper than online samples. With
reduced variance, our off-policy MC method requires fewer online samples to
evaluate the performance of a policy compared with the ordinary MC method.
Moreover, our off-policy MC estimator is always unbiased.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：A Bayesian Generative Adversarial Network (GAN) to Generate Synthetic  Time-Series Data, Application in Combined Sewer Flow Prediction</b></summary>
  <p><b>编号</b>：[56]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13733</p>
  <p><b>作者</b>：Amin E. Bakhshipour,  Alireza Koochali,  Ulrich Dittmer,  Ali Haghighi,  Sheraz Ahmad,  Andreas Dengel</p>
  <p><b>备注</b>：Accepted in WDSA/CCWI 2022 Conference</p>
  <p><b>关键词</b>：urban water infrastructures, improving smart operation, key limitations obstruct, data analysis techniques, data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite various breakthroughs in machine learning and data analysis
techniques for improving smart operation and management of urban water
infrastructures, some key limitations obstruct this progress. Among these
shortcomings, the absence of freely available data due to data privacy or high
costs of data gathering and the nonexistence of adequate rare or extreme events
in the available data plays a crucial role. Here, Generative Adversarial
Networks (GANs) can help overcome these challenges. In machine learning,
generative models are a class of methods capable of learning data distribution
to generate artificial data. In this study, we developed a GAN model to
generate synthetic time series to balance our limited recorded time series data
and improve the accuracy of a data-driven model for combined sewer flow
prediction. We considered the sewer system of a small town in Germany as the
test case. Precipitation and inflow to the storage tanks are used for the
Data-Driven model development. The aim is to predict the flow using
precipitation data and examine the impact of data augmentation using synthetic
data in model performance. Results show that GAN can successfully generate
synthetic time series from real data distribution, which helps more accurate
peak flow prediction. However, the model without data augmentation works better
for dry weather prediction. Therefore, an ensemble model is suggested to
combine the advantages of both models.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Preserving local densities in low-dimensional embeddings</b></summary>
  <p><b>编号</b>：[57]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13732</p>
  <p><b>作者</b>：Jonas Fischer,  Rebekka Burkholz,  Jilles Vreeken</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Low-dimensional embeddings, embeddings and visualizations, indispensable tool, high-dimensional data, Fig.</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Low-dimensional embeddings and visualizations are an indispensable tool for
analysis of high-dimensional data. State-of-the-art methods, such as tSNE and
UMAP, excel in unveiling local structures hidden in high-dimensional data and
are therefore routinely applied in standard analysis pipelines in biology. We
show, however, that these methods fail to reconstruct local properties, such as
relative differences in densities (Fig. 1) and that apparent differences in
cluster size can arise from computational artifact caused by differing sample
sizes (Fig. 2). Providing a theoretical analysis of this issue, we then suggest
dtSNE, which approximately conserves local densities. In an extensive study on
synthetic benchmark and real world data comparing against five state-of-the-art
methods, we empirically show that dtSNE provides similar global reconstruction,
but yields much more accurate depictions of local distances and relative
densities.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Dissecting the Effects of SGD Noise in Distinct Regimes of Deep Learning</b></summary>
  <p><b>编号</b>：[64]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13703</p>
  <p><b>作者</b>：Antonio Sclocchi,  Mario Geiger,  Matthieu Wyart</p>
  <p><b>备注</b>：18 pages, 14 figures</p>
  <p><b>关键词</b>：deep neural networks, neural networks remains, stochastic gradient descent, SGD noise, distinct training regimes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Understanding when the noise in stochastic gradient descent (SGD) affects
generalization of deep neural networks remains a challenge, complicated by the
fact that networks can operate in distinct training regimes. Here we study how
the magnitude of this noise $T$ affects performance as the size of the training
set $P$ and the scale of initialization $\alpha$ are varied. For gradient
descent, $\alpha$ is a key parameter that controls if the network is `lazy'
($\alpha\gg 1$) or instead learns features ($\alpha\ll 1$). For classification
of MNIST and CIFAR10 images, our central results are: (i) obtaining phase
diagrams for performance in the $(\alpha,T)$ plane. They show that SGD noise
can be detrimental or instead useful depending on the training regime.
Moreover, although increasing $T$ or decreasing $\alpha$ both allow the net to
escape the lazy regime, these changes can have opposite effects on performance.
(ii) Most importantly, we find that key dynamical quantities (including the
total variations of weights during training) depend on both $T$ and $P$ as
power laws, and the characteristic temperature $T_c$, where the noise of SGD
starts affecting performance, is a power law of $P$. These observations
indicate that a key effect of SGD noise occurs late in training, by affecting
the stopping process whereby all data are fitted. We argue that due to SGD
noise, nets must develop a stronger `signal', i.e. larger informative weights,
to fit the data, leading to a longer training time. The same effect occurs at
larger training set $P$. We confirm this view in the perceptron model, where
signal and noise can be precisely measured. Interestingly, exponents
characterizing the effect of SGD depend on the density of data near the
decision boundary, as we explain.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Are Defenses for Graph Neural Networks Robust?</b></summary>
  <p><b>编号</b>：[65]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13694</p>
  <p><b>作者</b>：Felix Mujkanovic,  Simon Geisler,  Stephan Günnemann,  Aleksandar Bojchevski</p>
  <p><b>备注</b>：34 pages, 36th Conference on Neural Information Processing Systems (NeurIPS 2022)</p>
  <p><b>关键词</b>：Graph Neural Networks, Neural Networks, effective adversarial defenses, designing effective adversarial, Graph Neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A cursory reading of the literature suggests that we have made a lot of
progress in designing effective adversarial defenses for Graph Neural Networks
(GNNs). Yet, the standard methodology has a serious flaw - virtually all of the
defenses are evaluated against non-adaptive attacks leading to overly
optimistic robustness estimates. We perform a thorough robustness analysis of 7
of the most popular defenses spanning the entire spectrum of strategies, i.e.,
aimed at improving the graph, the architecture, or the training. The results
are sobering - most defenses show no or only marginal improvement compared to
an undefended baseline. We advocate using custom adaptive attacks as a gold
standard and we outline the lessons we learned from successfully designing such
attacks. Moreover, our diverse collection of perturbed graphs forms a
(black-box) unit test offering a first glance at a model's robustness.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Time Series Forecasting via Semi-Asymmetric Convolutional Architecture  with Global Atrous Sliding Window</b></summary>
  <p><b>编号</b>：[67]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13691</p>
  <p><b>作者</b>：Yuanpeng He</p>
  <p><b>备注</b>：13pages,8 figures</p>
  <p><b>关键词</b>：time series, time series forecasting, time, series, series forecasting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The proposed method in this paper is designed to address the problem of time
series forecasting. Although some exquisitely designed models achieve excellent
prediction performances, how to extract more useful information and make
accurate predictions is still an open issue. Most of modern models only focus
on a short range of information, which are fatal for problems such as time
series forecasting which needs to capture long-term information
characteristics. As a result, the main concern of this work is to further mine
relationship between local and global information contained in time series to
produce more precise predictions. In this paper, to satisfactorily realize the
purpose, we make three main contributions that are experimentally verified to
have performance advantages. Firstly, original time series is transformed into
difference sequence which serves as input to the proposed model. And secondly,
we introduce the global atrous sliding window into the forecasting model which
references the concept of fuzzy time series to associate relevant global
information with temporal data within a time period and utilizes
central-bidirectional atrous algorithm to capture underlying-related features
to ensure validity and consistency of captured data. Thirdly, a variation of
widely-used asymmetric convolution which is called semi-asymmetric convolution
is devised to more flexibly extract relationships in adjacent elements and
corresponding associated global features with adjustable ranges of convolution
on vertical and horizontal directions. The proposed model in this paper
achieves state-of-the-art on most of time series datasets provided compared
with competitive modern models.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：The Flan Collection: Designing Data and Methods for Effective  Instruction Tuning</b></summary>
  <p><b>编号</b>：[68]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13688</p>
  <p><b>作者</b>：Shayne Longpre,  Le Hou,  Tu Vu,  Albert Webson,  Hyung Won Chung,  Yi Tay,  Denny Zhou,  Quoc V. Le,  Barret Zoph,  Jason Wei,  Adam Roberts</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：design decisions, instruction tuning, instruction tuning methods, Flan, Flan Collection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the design decisions of publicly available instruction tuning
methods, and break down the development of Flan 2022 (Chung et al., 2022).
Through careful ablation studies on the Flan Collection of tasks and methods,
we tease apart the effect of design decisions which enable Flan-T5 to
outperform prior work by 3-17%+ across evaluation settings. We find task
balancing and enrichment techniques are overlooked but critical to effective
instruction tuning, and in particular, training with mixed prompt settings
(zero-shot, few-shot, and chain-of-thought) actually yields stronger (2%+)
performance in all settings. In further experiments, we show Flan-T5 requires
less finetuning to converge higher and faster than T5 on single downstream
tasks, motivating instruction-tuned models as more computationally-efficient
starting checkpoints for new tasks. Finally, to accelerate research on
instruction tuning, we make the Flan 2022 collection of datasets, templates,
and methods publicly available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Enhancing Hyper-To-Real Space Projections Through Euclidean Norm  Meta-Heuristic Optimization</b></summary>
  <p><b>编号</b>：[72]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13671</p>
  <p><b>作者</b>：Luiz C. F. Ribeiro,  Mateus Roder,  Gustavo H. de Rosa,  Leandro A. Passos,  João P. Papa</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：continuous computational power, computational power growth, optimization problems significant, tractable task, sophisticated algorithms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The continuous computational power growth in the last decades has made
solving several optimization problems significant to humankind a tractable
task; however, tackling some of them remains a challenge due to the
overwhelming amount of candidate solutions to be evaluated, even by using
sophisticated algorithms. In such a context, a set of nature-inspired
stochastic methods, called meta-heuristic optimization, can provide robust
approximate solutions to different kinds of problems with a small computational
burden, such as derivative-free real function optimization. Nevertheless, these
methods may converge to inadequate solutions if the function landscape is too
harsh, e.g., enclosing too many local optima. Previous works addressed this
issue by employing a hypercomplex representation of the search space, like
quaternions, where the landscape becomes smoother and supposedly easier to
optimize. Under this approach, meta-heuristic computations happen in the
hypercomplex space, whereas variables are mapped back to the real domain before
function evaluation. Despite this latter operation being performed by the
Euclidean norm, we have found that after the optimization procedure has
finished, it is usually possible to obtain even better solutions by employing
the Minkowski $p$-norm instead and fine-tuning $p$ through an auxiliary
sub-problem with neglecting additional cost and no hyperparameters. Such
behavior was observed in eight well-established benchmarking functions, thus
fostering a new research direction for hypercomplex meta-heuristic
optimization.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Automated Sentiment and Hate Speech Analysis of Facebook Data by  Employing Multilingual Transformer Models</b></summary>
  <p><b>编号</b>：[74]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13668</p>
  <p><b>作者</b>：Ritumbra Manuvie,  Saikat Chatterjee</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Social Media Platforms, Media Platforms, Social Media, heightened negative discourse, amplify the spread</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, there has been a heightened consensus within academia and in
the public discourse that Social Media Platforms (SMPs), amplify the spread of
hateful and negative sentiment content. Researchers have identified how hateful
content, political propaganda, and targeted messaging contributed to real-world
harms including insurrections against democratically elected governments,
genocide, and breakdown of social cohesion due to heightened negative discourse
towards certain communities in parts of the world. To counter these issues,
SMPs have created semi-automated systems that can help identify toxic speech.
In this paper we analyse the statistical distribution of hateful and negative
sentiment contents within a representative Facebook dataset (n= 604,703)
scrapped through 648 public Facebook pages which identify themselves as
proponents (and followers) of far-right Hindutva actors. These pages were
identified manually using keyword searches on Facebook and on CrowdTangleand
classified as far-right Hindutva pages based on page names, page descriptions,
and discourses shared on these pages. We employ state-of-the-art, open-source
XLM-T multilingual transformer-based language models to perform sentiment and
hate speech analysis of the textual contents shared on these pages over a
period of 5.5 years. The result shows the statistical distributions of the
predicted sentiment and the hate speech labels; top actors, and top page
categories. We further discuss the benchmark performances and limitations of
these pre-trained language models.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Collision-aware In-hand 6D Object Pose Estimation using Multiple  Vision-based Tactile Sensors</b></summary>
  <p><b>编号</b>：[75]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13667</p>
  <p><b>作者</b>：Gabriele M. Caddeo,  Nicola A. Piga,  Fabrizio Bottarel,  Lorenzo Natale</p>
  <p><b>备注</b>：Accepted for publication at 2023 IEEE International Conference on Robotics and Automation (ICRA)</p>
  <p><b>关键词</b>：estimating the in-hand, address the problem, problem of estimating, multiple vision-based tactile, sensors</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we address the problem of estimating the in-hand 6D pose of an
object in contact with multiple vision-based tactile sensors. We reason on the
possible spatial configurations of the sensors along the object surface.
Specifically, we filter contact hypotheses using geometric reasoning and a
Convolutional Neural Network (CNN), trained on simulated object-agnostic
images, to promote those that better comply with the actual tactile images from
the sensors. We use the selected sensors configurations to optimize over the
space of 6D poses using a Gradient Descent-based approach. We finally rank the
obtained poses by penalizing those that are in collision with the sensors. We
carry out experiments in simulation using the DIGIT vision-based sensor with
several objects, from the standard YCB model set. The results demonstrate that
our approach estimates object poses that are compatible with actual
object-sensor contacts in $87.5\%$ of cases while reaching an average
positional error in the order of $2$ centimeters. Our analysis also includes
qualitative results of experiments with a real DIGIT sensor.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Exploring QSAR Models for Activity-Cliff Prediction</b></summary>
  <p><b>编号</b>：[83]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13644</p>
  <p><b>作者</b>：Markus Dablander,  Thierry Hanser,  Renaud Lambiotte,  Garrett M. Morris</p>
  <p><b>备注</b>：Submitted to Journal of Cheminformatics</p>
  <p><b>关键词</b>：small structural modification, small structural, structural modification, modification but exhibit, exhibit a large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pairs of similar compounds that only differ by a small structural
modification but exhibit a large difference in their binding affinity for a
given target are known as activity cliffs (ACs). It has been hypothesised that
quantitative structure-activity relationship (QSAR) models struggle to predict
ACs and that ACs thus form a major source of prediction error. However, a study
to explore the AC-prediction power of modern QSAR methods and its relationship
to general QSAR-prediction performance is lacking. We systematically construct
nine distinct QSAR models by combining three molecular representation methods
(extended-connectivity fingerprints, physicochemical-descriptor vectors and
graph isomorphism networks) with three regression techniques (random forests,
k-nearest neighbours and multilayer perceptrons); we then use each resulting
model to classify pairs of similar compounds as ACs or non-ACs and to predict
the activities of individual molecules in three case studies: dopamine receptor
D2, factor Xa, and SARS-CoV-2 main protease. We observe low AC-sensitivity
amongst the tested models when the activities of both compounds are unknown,
but a substantial increase in AC-sensitivity when the actual activity of one of
the compounds is given. Graph isomorphism features are found to be competitive
with or superior to classical molecular representations for AC-classification
and can thus be employed as baseline AC-prediction models or simple
compound-optimisation tools. For general QSAR-prediction, however,
extended-connectivity fingerprints still consistently deliver the best
performance. Our results provide strong support for the hypothesis that indeed
QSAR methods frequently fail to predict ACs. We propose twin-network training
for deep learning models as a potential future pathway to increase
AC-sensitivity and thus overall QSAR performance.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：An Efficient Solution to s-Rectangular Robust Markov Decision Processes</b></summary>
  <p><b>编号</b>：[84]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13642</p>
  <p><b>作者</b>：Navdeep Kumar,  Kfir Levy,  Kaixin Wang,  Shie Mannor</p>
  <p><b>备注</b>：arXiv admin note: substantial text overlap with arXiv:2205.14327</p>
  <p><b>关键词</b>：Markov Decision Processes, robust Markov Decision, rectangular robust Markov, Decision Processes, Markov Decision</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present an efficient robust value iteration for \texttt{s}-rectangular
robust Markov Decision Processes (MDPs) with a time complexity comparable to
standard (non-robust) MDPs which is significantly faster than any existing
method. We do so by deriving the optimal robust Bellman operator in concrete
forms using our $L_p$ water filling lemma. We unveil the exact form of the
optimal policies, which turn out to be novel threshold policies with the
probability of playing an action proportional to its advantage.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：Tricking AI chips into Simulating the Human Brain: A Detailed  Performance Analysis</b></summary>
  <p><b>编号</b>：[85]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13637</p>
  <p><b>作者</b>：Lennart P. L. Landsmeer,  Max C. W. Engelen,  Rene Miedema,  Christos Strydis</p>
  <p><b>备注</b>：11 pages, 4 figures</p>
  <p><b>关键词</b>：dedicated AI-accelerator chips, modern deep neural, modern computers, modern deep, deep neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Challenging the Nvidia monopoly, dedicated AI-accelerator chips have begun
emerging for tackling the computational challenge that the inference and,
especially, the training of modern deep neural networks (DNNs) poses to modern
computers. The field has been ridden with studies assessing the performance of
these contestants across various DNN model types. However, AI-experts are aware
of the limitations of current DNNs and have been working towards the fourth AI
wave which will, arguably, rely on more biologically inspired models,
predominantly on spiking neural networks (SNNs). At the same time, GPUs have
been heavily used for simulating such models in the field of computational
neuroscience, yet AI-chips have not been tested on such workloads. The current
paper aims at filling this important gap by evaluating multiple, cutting-edge
AI-chips (Graphcore IPU, GroqChip, Nvidia GPU with Tensor Cores and Google TPU)
on simulating a highly biologically detailed model of a brain region, the
inferior olive (IO). This IO application stress-tests the different
AI-platforms for highlighting architectural tradeoffs by varying its compute
density, memory requirements and floating-point numerical accuracy. Our
performance analysis reveals that the simulation problem maps extremely well
onto the GPU and TPU architectures, which for networks of 125,000 cells leads
to a 28x respectively 1,208x speedup over CPU runtimes. At this speed, the TPU
sets a new record for largest real-time IO simulation. The GroqChip outperforms
both platforms for small networks but, due to implementing some floating-point
operations at reduced accuracy, is found not yet usable for brain simulation.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Transport with Support: Data-Conditional Diffusion Bridges</b></summary>
  <p><b>编号</b>：[86]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13636</p>
  <p><b>作者</b>：Ella Tamir,  Martin Trapp,  Arno Solin</p>
  <p><b>备注</b>：23 pages, 11 figures</p>
  <p><b>关键词</b>：Schrödinger bridge problem, dynamic Schrödinger bridge, optimal transport problems, dynamic Schrödinger, solving optimal transport</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The dynamic Schrödinger bridge problem provides an appealing setting for
solving optimal transport problems by learning non-linear diffusion processes
using efficient iterative solvers. Recent works have demonstrated
state-of-the-art results (eg. in modelling single-cell embryo RNA sequences or
sampling from complex posteriors) but are limited to learning bridges with only
initial and terminal constraints. Our work extends this paradigm by proposing
the Iterative Smoothing Bridge (ISB). We integrate Bayesian filtering and
optimal control into learning the diffusion process, enabling constrained
stochastic processes governed by sparse observations at intermediate stages and
terminal constraints. We assess the effectiveness of our method on synthetic
and real-world data and show that the ISB generalises well to high-dimensional
data, is computationally efficient, and provides accurate estimates of the
marginals at intermediate and terminal times.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Active Learning-based Domain Adaptive Localized Polynomial Chaos  Expansion</b></summary>
  <p><b>编号</b>：[87]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13635</p>
  <p><b>作者</b>：Lukáš Novák,  Michael D. Shields,  Václav Sadílek,  Miroslav Vořechovský</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：input random space, adaptive localized polynomial, localized polynomial chaos, domain adaptive localized, input random</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The paper presents a novel methodology to build surrogate models of
complicated functions by an active learning-based sequential decomposition of
the input random space and construction of localized polynomial chaos
expansions, referred to as domain adaptive localized polynomial chaos expansion
(DAL-PCE). The approach utilizes sequential decomposition of the input random
space into smaller sub-domains approximated by low-order polynomial expansions.
This allows approximation of functions with strong nonlinearties,
discontinuities, and/or singularities. Decomposition of the input random space
and local approximations alleviates the Gibbs phenomenon for these types of
problems and confines error to a very small vicinity near the non-linearity.
The global behavior of the surrogate model is therefore significantly better
than existing methods as shown in numerical examples. The whole process is
driven by an active learning routine that uses the recently proposed $\Theta$
criterion to assess local variance contributions. The proposed approach
balances both \emph{exploitation} of the surrogate model and \emph{exploration}
of the input random space and thus leads to efficient and accurate
approximation of the original mathematical model. The numerical results show
the superiority of the DAL-PCE in comparison to (i) a single global polynomial
chaos expansion and (ii) the recently proposed stochastic spectral embedding
(SSE) method developed as an accurate surrogate model and which is based on a
similar domain decomposition process. This method represents general framework
upon which further extensions and refinements can be based, and which can be
combined with any technique for non-intrusive polynomial chaos expansion
construction.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：DiffSTG: Probabilistic Spatio-Temporal Graph Forecasting with Denoising  Diffusion Models</b></summary>
  <p><b>编号</b>：[91]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13629</p>
  <p><b>作者</b>：Haomin Wen,  Youfang Lin,  Yutong Xia,  Huaiyu Wan,  Roger Zimmermann,  Yuxuan Liang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Spatio-temporal graph neural, graph neural networks, graph neural, Spatio-temporal graph, probabilistic STG forecasting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Spatio-temporal graph neural networks (STGNN) have emerged as the dominant
model for spatio-temporal graph (STG) forecasting. Despite their success, they
fail to model intrinsic uncertainties within STG data, which cripples their
practicality in downstream tasks for decision-making. To this end, this paper
focuses on probabilistic STG forecasting, which is challenging due to the
difficulty in modeling uncertainties and complex ST dependencies. In this
study, we present the first attempt to generalize the popular denoising
diffusion probabilistic models to STGs, leading to a novel non-autoregressive
framework called DiffSTG, along with the first denoising network UGnet for STG
in the framework. Our approach combines the spatio-temporal learning
capabilities of STGNNs with the uncertainty measurements of diffusion models.
Extensive experiments validate that DiffSTG reduces the Continuous Ranked
Probability Score (CRPS) by 4%-14%, and Root Mean Squared Error (RMSE) by 2%-7%
over existing methods on three real-world datasets.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：Learning Data Representations with Joint Diffusion Models</b></summary>
  <p><b>编号</b>：[94]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13622</p>
  <p><b>作者</b>：Kamil Deja,  Tomasz Trzcinski,  Jakub M. Tomczak</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：simultaneously learns meaningful, learns meaningful internal, meaningful internal representations, internal representations fit, simultaneously learns</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce a joint diffusion model that simultaneously learns meaningful
internal representations fit for both generative and predictive tasks. Joint
machine learning models that allow synthesizing and classifying data often
offer uneven performance between those tasks or are unstable to train. In this
work, we depart from a set of empirical observations that indicate the
usefulness of internal representations built by contemporary deep
diffusion-based generative models in both generative and predictive settings.
We then introduce an extension of the vanilla diffusion model with a classifier
that allows for stable joint training with shared parametrization between those
objectives. The resulting joint diffusion model offers superior performance
across various tasks, including generative modeling, semi-supervised
classification, and domain adaptation.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Scheduling Inference Workloads on Distributed Edge Clusters with  Reinforcement Learning</b></summary>
  <p><b>编号</b>：[95]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13618</p>
  <p><b>作者</b>：Gabriele Castellano,  Juan-José Nieto,  Jordi Luque,  Ferrán Diego,  Carlos Segura,  Diego Perino,  Flavio Esposito,  Fulvio Risso,  Aravindh Raman</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Deep Neural Networks, Virtual Reality, Deep Neural, rely on Deep, process inference tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Many real-time applications (e.g., Augmented/Virtual Reality, cognitive
assistance) rely on Deep Neural Networks (DNNs) to process inference tasks.
Edge computing is considered a key infrastructure to deploy such applications,
as moving computation close to the data sources enables us to meet stringent
latency and throughput requirements. However, the constrained nature of edge
networks poses several additional challenges to the management of inference
workloads: edge clusters can not provide unlimited processing power to DNN
models, and often a trade-off between network and processing time should be
considered when it comes to end-to-end delay requirements. In this paper, we
focus on the problem of scheduling inference queries on DNN models in edge
networks at short timescales (i.e., few milliseconds). By means of simulations,
we analyze several policies in the realistic network settings and workloads of
a large ISP, highlighting the need for a dynamic scheduling policy that can
adapt to network conditions and workloads. We therefore design ASET, a
Reinforcement Learning based scheduling algorithm able to adapt its decisions
according to the system conditions. Our results show that ASET effectively
provides the best performance compared to static policies when scheduling over
a distributed pool of edge resources.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Anti-Exploration by Random Network Distillation</b></summary>
  <p><b>编号</b>：[97]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13616</p>
  <p><b>作者</b>：Alexander Nikulin,  Vladislav Kurenkov,  Denis Tarasov,  Sergey Kolesnikov</p>
  <p><b>备注</b>：Source code: this https URL</p>
  <p><b>关键词</b>：Random Network Distillation, offline reinforcement learning, Network Distillation, Random Network, success of Random</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite the success of Random Network Distillation (RND) in various domains,
it was shown as not discriminative enough to be used as an uncertainty
estimator for penalizing out-of-distribution actions in offline reinforcement
learning. In this paper, we revisit these results and show that, with a naive
choice of conditioning for the RND prior, it becomes infeasible for the actor
to effectively minimize the anti-exploration bonus and discriminativity is not
an issue. We show that this limitation can be avoided with conditioning based
on Feature-wise Linear Modulation (FiLM), resulting in a simple and efficient
ensemble-free algorithm based on Soft Actor-Critic. We evaluate it on the D4RL
benchmark, showing that it is capable of achieving performance comparable to
ensemble-based methods and outperforming ensemble-free approaches by a wide
margin.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Policy Gradient for s-Rectangular Robust Markov Decision Processes</b></summary>
  <p><b>编号</b>：[106]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13589</p>
  <p><b>作者</b>：Navdeep Kumar,  Esther Derman,  Matthieu Geist,  Kfir Levy,  Shie Mannor</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Markov Decision Processes, robust Markov Decision, Decision Processes, Markov Decision, s-rectangular robust Markov</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a novel robust policy gradient method (RPG) for s-rectangular
robust Markov Decision Processes (MDPs). We are the first to derive the
adversarial kernel in a closed form and demonstrate that it is a one-rank
perturbation of the nominal kernel. This allows us to derive an RPG that is
similar to the one used in non-robust MDPs, except with a robust Q-value
function and an additional correction term. Both robust Q-values and correction
terms are efficiently computable, thus the time complexity of our method
matches that of non-robust MDPs, which is significantly faster compared to
existing black box methods.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Support Exploration Algorithm for Sparse Support Recovery</b></summary>
  <p><b>编号</b>：[107]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13584</p>
  <p><b>作者</b>：Mimoun Mohamed (LIS, I2M),  François Malgouyres (IMT),  Valentin Emiya (QARMA),  Caroline Chaux (IPAL)</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：promoting sparsity called, algorithm promoting sparsity, sparsity called, promoting sparsity, support recovery</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce a new algorithm promoting sparsity called {\it Support
Exploration Algorithm (SEA)} and analyze it in the context of support
recovery/model selection problems.The algorithm can be interpreted as an
instance of the {\it straight-through estimator (STE)} applied to the
resolution of a sparse linear inverse problem. SEA uses a non-sparse
exploratory vector and makes it evolve in the input space to select the sparse
support. We put to evidence an oracle update rule for the exploratory vector
and consider the STE update. The theoretical analysis establishes general
sufficient conditions of support recovery. The general conditions are
specialized to the case where the matrix $A$ performing the linear measurements
satisfies the {\it Restricted Isometry Property (RIP)}.Experiments show that
SEA can efficiently improve the results of any algorithm. Because of its
exploratory nature, SEA also performs remarkably well when the columns of $A$
are strongly coherent.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Sport Task: Fine Grained Action Detection and Classification of Table  Tennis Strokes from Videos for MediaEval 2022</b></summary>
  <p><b>编号</b>：[111]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13576</p>
  <p><b>作者</b>：Pierre-Etienne Martin (MPI-EVA),  Jordan Calandre (MIA),  Boris Mansencal (LaBRI),  Jenny Benois-Pineau (LaBRI),  Renaud Péteri (MIA),  Laurent Mascarilla (MIA),  Julien Morlier</p>
  <p><b>备注</b>：MediaEval 2022 Workshop, Jan 2023, Bergen, Norway. arXiv admin note: substantial text overlap with arXiv:2112.11384</p>
  <p><b>关键词</b>：widespread research topic, research topic, videos, Sports video analysis, widespread research</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sports video analysis is a widespread research topic. Its applications are
very diverse, like events detection during a match, video summary, or
fine-grained movement analysis of athletes. As part of the MediaEval 2022
benchmarking initiative, this task aims at detecting and classifying subtle
movements from sport videos. We focus on recordings of table tennis matches.
Conducted since 2019, this task provides a classification challenge from
untrimmed videos recorded under natural conditions with known temporal
boundaries for each stroke. Since 2021, the task also provides a stroke
detection challenge from unannotated, untrimmed videos. This year, the
training, validation, and test sets are enhanced to ensure that all strokes are
represented in each dataset. The dataset is now similar to the one used in [1,
2]. This research is intended to build tools for coaches and athletes who want
to further evaluate their sport performances.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Skill Decision Transformer</b></summary>
  <p><b>编号</b>：[112]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13573</p>
  <p><b>作者</b>：Shyam Sudhakaran,  Sebastian Risi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, sequence modelling problem, offline reinforcement learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent work has shown that Large Language Models (LLMs) can be incredibly
effective for offline reinforcement learning (RL) by representing the
traditional RL problem as a sequence modelling problem (Chen et al., 2021;
Janner et al., 2021). However many of these methods only optimize for high
returns, and may not extract much information from a diverse dataset of
trajectories. Generalized Decision Transformers (GDTs) (Furuta et al., 2021)
have shown that utilizing future trajectory information, in the form of
information statistics, can help extract more information from offline
trajectory data. Building upon this, we propose Skill Decision Transformer
(Skill DT). Skill DT draws inspiration from hindsight relabelling (Andrychowicz
et al., 2017) and skill discovery methods to discover a diverse set of
primitive behaviors, or skills. We show that Skill DT can not only perform
offline state-marginal matching (SMM), but can discovery descriptive behaviors
that can be easily sampled. Furthermore, we show that through purely
reward-free optimization, Skill DT is still competitive with supervised offline
RL approaches on the D4RL benchmark. The code and videos can be found on our
project page: this https URL</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：BALANCE: Bayesian Linear Attribution for Root Cause Localization</b></summary>
  <p><b>编号</b>：[113]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13572</p>
  <p><b>作者</b>：Chaoyu Chen,  Hang Yu,  Zhichao Lei,  Jianguo Li,  Shaokang Ren,  Tingkai Zhang,  Silin Hu,  Jianchao Wang,  Wenhui Shi</p>
  <p><b>备注</b>：Accepted by SIGMOD 2023; 15 pages</p>
  <p><b>关键词</b>：candidate root, Root, plays an indispensable, maintenance and operations, RCA</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Root Cause Analysis (RCA) plays an indispensable role in distributed data
system maintenance and operations, as it bridges the gap between fault
detection and system recovery. Existing works mainly study multidimensional
localization or graph-based root cause localization. This paper opens up the
possibilities of exploiting the recently developed framework of explainable AI
(XAI) for the purpose of RCA. In particular, we propose BALANCE (BAyesian
Linear AttributioN for root CausE localization), which formulates the problem
of RCA through the lens of attribution in XAI and seeks to explain the
anomalies in the target KPIs by the behavior of the candidate root causes.
BALANCE consists of three innovative components. First, we propose a Bayesian
multicollinear feature selection (BMFS) model to predict the target KPIs given
the candidate root causes in a forward manner while promoting sparsity and
concurrently paying attention to the correlation between the candidate root
causes. Second, we introduce attribution analysis to compute the attribution
score for each candidate in a backward manner. Third, we merge the estimated
root causes related to each KPI if there are multiple KPIs. We extensively
evaluate the proposed BALANCE method on one synthesis dataset as well as three
real-world RCA tasks, that is, bad SQL localization, container fault
localization, and fault type diagnosis for Exathlon. Results show that BALANCE
outperforms the state-of-the-art (SOTA) methods in terms of accuracy with the
least amount of running time, and achieves at least $6\%$ notably higher
accuracy than SOTA methods for real tasks. BALANCE has been deployed to
production to tackle real-world RCA problems, and the online results further
advocate its usage for real-time diagnosis in distributed data systems.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：NP-Match: Towards a New Probabilistic Model for Semi-Supervised Learning</b></summary>
  <p><b>编号</b>：[114]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13569</p>
  <p><b>作者</b>：Jianfeng Wang,  Xiaolin Hu,  Thomas Lukasiewicz</p>
  <p><b>备注</b>：An journal version of our previous ICML 2022 paper arXiv:2207.01066 . Codes are available at: this https URL</p>
  <p><b>关键词</b>：semi-supervised image classification, semi-supervised image, image classification, recent years, Semi-supervised</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Semi-supervised learning (SSL) has been widely explored in recent years, and
it is an effective way of leveraging unlabeled data to reduce the reliance on
labeled data. In this work, we adjust neural processes (NPs) to the
semi-supervised image classification task, resulting in a new method named
NP-Match. NP-Match is suited to this task for two reasons. Firstly, NP-Match
implicitly compares data points when making predictions, and as a result, the
prediction of each unlabeled data point is affected by the labeled data points
that are similar to it, which improves the quality of pseudo-labels. Secondly,
NP-Match is able to estimate uncertainty that can be used as a tool for
selecting unlabeled samples with reliable pseudo-labels. Compared with
uncertainty-based SSL methods implemented with Monte-Carlo (MC) dropout,
NP-Match estimates uncertainty with much less computational overhead, which can
save time at both the training and the testing phases. We conducted extensive
experiments on five public datasets under three semi-supervised image
classification settings, namely, the standard semi-supervised image
classification, the imbalanced semi-supervised image classification, and the
multi-label semi-supervised image classification, and NP-Match outperforms
state-of-the-art (SOTA) approaches or achieves competitive results on them,
which shows the effectiveness of NP-Match and its potential for SSL. The codes
are at this https URL</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Learning Against Distributional Uncertainty: On the Trade-off Between  Robustness and Specificity</b></summary>
  <p><b>编号</b>：[115]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13565</p>
  <p><b>作者</b>：Shixiong Wang,  Haowei Wang,  Jean Honorio</p>
  <p><b>备注</b>：23 Pages, 3 Figures</p>
  <p><b>关键词</b>：Trustworthy machine learning, machine learning aims, combating distributional uncertainties, data distributions compared, Trustworthy machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Trustworthy machine learning aims at combating distributional uncertainties
in training data distributions compared to population distributions. Typical
treatment frameworks include the Bayesian approach, (min-max) distributionally
robust optimization (DRO), and regularization. However, two issues have to be
raised: 1) All these methods are biased estimators of the true optimal cost; 2)
the prior distribution in the Bayesian method, the radius of the distributional
ball in the DRO method, and the regularizer in the regularization method are
difficult to specify. This paper studies a new framework that unifies the three
approaches and that addresses the two challenges mentioned above. The
asymptotic properties (e.g., consistency and asymptotic normalities),
non-asymptotic properties (e.g., unbiasedness and generalization error bound),
and a Monte--Carlo-based solution method of the proposed model are studied. The
new model reveals the trade-off between the robustness to the unseen data and
the specificity to the training data.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Review of methods for automatic cerebral microbleeds detection</b></summary>
  <p><b>编号</b>：[120]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13549</p>
  <p><b>作者</b>：Maria Ferlin,  Zuzanna Klawikowska,  Michał Grochowski,  Małgorzata Grzywińska,  Edyta Szurowska</p>
  <p><b>备注</b>：32 pages, 6 figures, 3 tables, 174 references</p>
  <p><b>关键词</b>：Cerebral microbleeds detection, Cerebral microbleeds, challenging task, detect cerebral microbleeds, important and challenging</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cerebral microbleeds detection is an important and challenging task. With the
gaining popularity of the MRI, the ability to detect cerebral microbleeds also
raises. Unfortunately, for radiologists, it is a time-consuming and laborious
procedure. For this reason, various solutions to automate this process have
been proposed for several years, but none of them is currently used in medical
practice. In this context, the need to systematize the existing knowledge and
best practices has been recognized as a factor facilitating the imminent
synthesis of a real CMBs detection system practically applicable in medicine.
To the best of our knowledge, all available publications regarding automatic
cerebral microbleeds detection have been gathered, described, and assessed in
this paper in order to distinguish the current research state and provide a
starting point for future studies.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Holistic Graph-based Motion Prediction</b></summary>
  <p><b>编号</b>：[124]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13545</p>
  <p><b>作者</b>：Daniel Grimm,  Philip Schörner,  Moritz Dreßler,  J.-Marius Zöllner</p>
  <p><b>备注</b>：Accepted on ICRA 2023</p>
  <p><b>关键词</b>：automated vehicles, complex environments, difficult task, vehicles in complex, mastered when automated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Motion prediction for automated vehicles in complex environments is a
difficult task that is to be mastered when automated vehicles are to be used in
arbitrary situations. Many factors influence the future motion of traffic
participants starting with traffic rules and reaching from the interaction
between each other to personal habits of human drivers. Therefore we present a
novel approach for a graph-based prediction based on a heterogeneous holistic
graph representation that combines temporal information, properties and
relations between traffic participants as well as relations with static
elements like the road network. The information are encoded through different
types of nodes and edges that both are enriched with arbitrary features. We
evaluated the approach on the INTERACTION and the Argoverse dataset and
conducted an informative ablation study to demonstrate the benefit of different
types of information for the motion prediction quality.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Low Complexity Adaptive Machine Learning Approaches for End-to-End  Latency Prediction</b></summary>
  <p><b>编号</b>：[129]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13536</p>
  <p><b>作者</b>：Pierre Larrenie (LIGM),  Jean-François Bercher (LIGM),  Olivier Venard (ESYCOM),  Iyad Lahsen-Cherif (INPT)</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Software Defined Networks, Software Defined, efficiency of networking, Defined Networks, opened the door</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Software Defined Networks have opened the door to statistical and AI-based
techniques to improve efficiency of networking. Especially to ensure a certain
Quality of Service (QoS) for specific applications by routing packets with
awareness on content nature (VoIP, video, files, etc.) and its needs (latency,
bandwidth, etc.) to use efficiently resources of a network. Monitoring and
predicting various Key Performance Indicators (KPIs) at any level may handle
such problems while preserving network bandwidth. The question addressed in
this work is the design of efficient, low-cost adaptive algorithms for KPI
estimation, monitoring and prediction. We focus on end-to-end latency
prediction, for which we illustrate our approaches and results on data obtained
from a public generator provided after the recent international challenge on
GNN [12]. In this paper, we improve our previously proposed low-cost estimators
[6] by adding the adaptive dimension, and show that the performances are
minimally modified while gaining the ability to track varying networks.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：Domain-Generalizable Multiple-Domain Clustering</b></summary>
  <p><b>编号</b>：[132]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13530</p>
  <p><b>作者</b>：Amit Rozner,  Barak Battash,  Lior Wolf,  Ofir Lindenbaum</p>
  <p><b>备注</b>：12 pages, 5 figures</p>
  <p><b>关键词</b>：analyzing scientific data, adequately analyzing scientific, Accurately clustering high-dimensional, clustering high-dimensional measurements, scientific data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Accurately clustering high-dimensional measurements is vital for adequately
analyzing scientific data. Deep learning machinery has remarkably improved
clustering capabilities in recent years due to its ability to extract
meaningful representations. In this work, we are given unlabeled samples from
multiple source domains, and we aim to learn a shared classifier that assigns
the examples to various clusters. Evaluation is done by using the classifier
for predicting cluster assignments in a previously unseen domain. This setting
generalizes the problem of unsupervised domain generalization to the case in
which no supervised learning samples are given (completely unsupervised).
Towards this goal, we present an end-to-end model and evaluate its capabilities
on several multi-domain image datasets. Specifically, we demonstrate that our
model is more accurate than schemes that require fine-tuning using samples from
the target domain or some level of supervision.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Real-Time Outlier Detection with Dynamic Process Limits</b></summary>
  <p><b>编号</b>：[133]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13527</p>
  <p><b>作者</b>：Marek Wadinger,  Michal Kvasnica</p>
  <p><b>备注</b>：7 pages, 4 figures, 24th International Conference on Process Control</p>
  <p><b>关键词</b>：Anomaly detection methods, Anomaly detection, Online anomaly detection, environmental aspects, systems where rare</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Anomaly detection methods are part of the systems where rare events may
endanger an operation's profitability, safety, and environmental aspects.
Although many state-of-the-art anomaly detection methods were developed to
date, their deployment is limited to the operation conditions present during
the model training. Online anomaly detection brings the capability to adapt to
data drifts and change points that may not be represented during model
development resulting in prolonged service life. This paper proposes an online
anomaly detection algorithm for existing real-time infrastructures where
low-latency detection is required and novel patterns in data occur
unpredictably. The online inverse cumulative distribution-based approach is
introduced to eliminate common problems of offline anomaly detectors, meanwhile
providing dynamic process limits to normal operation. The benefit of the
proposed method is the ease of use, fast computation, and deployability as
shown in two case studies of real microgrid operation data.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Recurrences reveal shared causal drivers of complex time series</b></summary>
  <p><b>编号</b>：[136]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13516</p>
  <p><b>作者</b>：William Gilpin</p>
  <p><b>备注</b>：8 pages, 5 figures</p>
  <p><b>关键词</b>：experimental time series, series measurements share, time series measurements, time series, share an unobserved</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Many experimental time series measurements share an unobserved causal driver.
Examples include genes targeted by transcription factors, ocean flows
influenced by large-scale atmospheric currents, and motor circuits steered by
descending neurons. Reliably inferring this unseen driving force is necessary
to understand the intermittent nature of top-down control schemes in diverse
biological and engineered systems. Here, we introduce a new unsupervised
learning algorithm that uses recurrences in time series measurements to
gradually reconstruct an unobserved driving signal. Drawing on the mathematical
theory of skew-product dynamical systems, we identify recurrence events shared
across response time series, which implicitly define a recurrence graph with
glass-like structure. As the amount or quality of observed data improves, this
recurrence graph undergoes a percolation transition manifesting as weak
ergodicity breaking for random walks on the induced landscape -- revealing the
shared driver's dynamics, even in the presence of strongly corrupted or noisy
measurements. Across several thousand random dynamical systems, we empirically
quantify the dependence of reconstruction accuracy on the rate of information
transfer from a chaotic driver to the response systems, and we find that
effective reconstruction proceeds through gradual approximation of the driver's
dominant unstable periodic orbits. Through extensive benchmarks against
classical and neural-network-based signal processing techniques, we demonstrate
our method's strong ability to extract causal driving signals from diverse
real-world datasets spanning neuroscience, genomics, fluid dynamics, and
physiology.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Fourier Sensitivity and Regularization of Computer Vision Models</b></summary>
  <p><b>编号</b>：[137]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13514</p>
  <p><b>作者</b>：Kiran Krishnamachari,  See-Kiong Ng,  Chuan-Sheng Foo</p>
  <p><b>备注</b>：Published in TMLR, this https URL</p>
  <p><b>关键词</b>：deep neural networks, Recent work, work has empirically, empirically shown, deep neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent work has empirically shown that deep neural networks latch on to the
Fourier statistics of training data and show increased sensitivity to
Fourier-basis directions in the input. Understanding and modifying this
Fourier-sensitivity of computer vision models may help improve their
robustness. Hence, in this paper we study the frequency sensitivity
characteristics of deep neural networks using a principled approach. We first
propose a basis trick, proving that unitary transformations of the
input-gradient of a function can be used to compute its gradient in the basis
induced by the transformation. Using this result, we propose a general measure
of any differentiable model's Fourier-sensitivity using the unitary
Fourier-transform of its input-gradient. When applied to deep neural networks,
we find that computer vision models are consistently sensitive to particular
frequencies dependent on the dataset, training method and architecture. Based
on this measure, we further propose a Fourier-regularization framework to
modify the Fourier-sensitivities and frequency bias of models. Using our
proposed regularizer-family, we demonstrate that deep neural networks obtain
improved classification accuracy on robustness evaluations.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：An Analysis of Classification Approaches for Hit Song Prediction using  Engineered Metadata Features with Lyrics and Audio Features</b></summary>
  <p><b>编号</b>：[142]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13507</p>
  <p><b>作者</b>：Mengyisong Zhao,  Morgan Harvey,  David Cameron,  Frank Hopfgartner,  Valerie J. Gillet</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：music information retrieval, information retrieval, remains a considerable, considerable challenge, emerging fields</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Hit song prediction, one of the emerging fields in music information
retrieval (MIR), remains a considerable challenge. Being able to understand
what makes a given song a hit is clearly beneficial to the whole music
industry. Previous approaches to hit song prediction have focused on using
audio features of a record. This study aims to improve the prediction result of
the top 10 hits among Billboard Hot 100 songs using more alternative metadata,
including song audio features provided by Spotify, song lyrics, and novel
metadata-based features (title topic, popularity continuity and genre class).
Five machine learning approaches are applied, including: k-nearest neighbours,
Naive Bayes, Random Forest, Logistic Regression and Multilayer Perceptron. Our
results show that Random Forest (RF) and Logistic Regression (LR) with all
features (including novel features, song audio features and lyrics features)
outperforms other models, achieving 89.1% and 87.2% accuracy, and 0.91 and 0.93
AUC, respectively. Our findings also demonstrate the utility of our novel music
metadata features, which contributed most to the models' discriminative
performance.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：DNN Explanation for Safety Analysis: an Empirical Evaluation of  Clustering-based Approaches</b></summary>
  <p><b>编号</b>：[143]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13506</p>
  <p><b>作者</b>：Mohammed Oualid Attaoui,  Hazem Fahmy,  Fabrizio Pastore,  Lionel Briand</p>
  <p><b>备注</b>：10 Tables, 14 Figures</p>
  <p><b>关键词</b>：deep neural networks, DNN failures, neural networks, adoption of deep, deep neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The adoption of deep neural networks (DNNs) in safety-critical contexts is
often prevented by the lack of effective means to explain their results,
especially when they are erroneous. In our previous work, we proposed a
white-box approach (HUDD) and a black-box approach (SAFE) to automatically
characterize DNN failures. They both identify clusters of similar images from a
potentially large set of images leading to DNN failures. However, the analysis
pipelines for HUDD and SAFE were instantiated in specific ways according to
common practices, deferring the analysis of other pipelines to future work. In
this paper, we report on an empirical evaluation of 99 different pipelines for
root cause analysis of DNN failures. They combine transfer learning,
autoencoders, heatmaps of neuron relevance, dimensionality reduction
techniques, and different clustering algorithms. Our results show that the best
pipeline combines transfer learning, DBSCAN, and UMAP. It leads to clusters
almost exclusively capturing images of the same failure scenario, thus
facilitating root cause analysis. Further, it generates distinct clusters for
each root cause of failure, thus enabling engineers to detect all the unsafe
scenarios. Interestingly, these results hold even for failure scenarios that
are only observed in a small percentage of the failing images.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：Auxiliary Learning as an Asymmetric Bargaining Game</b></summary>
  <p><b>编号</b>：[145]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13501</p>
  <p><b>作者</b>：Aviv Shamsian,  Aviv Navon,  Neta Glazer,  Kenji Kawaguchi,  Gal Chechik,  Ethan Fetaya</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：trained models, small datasets, enhancing the generalization, generalization capabilities, capabilities of trained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Auxiliary learning is an effective method for enhancing the generalization
capabilities of trained models, particularly when dealing with small datasets.
However, this approach may present several difficulties: (i) optimizing
multiple objectives can be more challenging, and (ii) how to balance the
auxiliary tasks to best assist the main task is unclear. In this work, we
propose a novel approach, named AuxiNash, for balancing tasks in auxiliary
learning by formalizing the problem as generalized bargaining game with
asymmetric task bargaining power. Furthermore, we describe an efficient
procedure for learning the bargaining power of tasks based on their
contribution to the performance of the main task and derive theoretical
guarantees for its convergence. Finally, we evaluate AuxiNash on multiple
multi-task benchmarks and find that it consistently outperforms competing
methods.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Company-as-Tribe: Company Financial Risk Assessment on Tribe-Style Graph  with Hierarchical Graph Neural Networks</b></summary>
  <p><b>编号</b>：[148]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13492</p>
  <p><b>作者</b>：Wendong Bi,  Bingbing Xu,  Xiaoqian Sun,  Zidong Wang,  Huawei Shen,  Xueqi Cheng</p>
  <p><b>备注</b>：accepted by SIGKDD2022</p>
  <p><b>关键词</b>：avoid considerable losses, financial risk assessment, Company financial risk, financial risk, early risk assessment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Company financial risk is ubiquitous and early risk assessment for listed
companies can avoid considerable losses. Traditional methods mainly focus on
the financial statements of companies and lack the complex relationships among
them. However, the financial statements are often biased and lagged, making it
difficult to identify risks accurately and timely. To address the challenges,
we redefine the problem as \textbf{company financial risk assessment on
tribe-style graph} by taking each listed company and its shareholders as a
tribe and leveraging financial news to build inter-tribe connections. Such
tribe-style graphs present different patterns to distinguish risky companies
from normal ones. However, most nodes in the tribe-style graph lack attributes,
making it difficult to directly adopt existing graph learning methods (e.g.,
Graph Neural Networks(GNNs)). In this paper, we propose a novel Hierarchical
Graph Neural Network (TH-GNN) for Tribe-style graphs via two levels, with the
first level to encode the structure pattern of the tribes with contrastive
learning, and the second level to diffuse information based on the inter-tribe
relations, achieving effective and efficient risk assessment. Extensive
experiments on the real-world company dataset show that our method achieves
significant improvements on financial risk assessment over previous competing
methods. Also, the extensive ablation studies and visualization comprehensively
show the effectiveness of our method.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：An investigation of challenges encountered when specifying training data  and runtime monitors for safety critical ML applications</b></summary>
  <p><b>编号</b>：[152]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13476</p>
  <p><b>作者</b>：Hans-Martin Heyn,  Eric Knauss,  Iswarya Malleswaran,  Shruthi Dinakaran</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：models requires diligence, Context and motivation, machine learning, established processes, training data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Context and motivation: The development and operation of critical software
that contains machine learning (ML) models requires diligence and established
processes. Especially the training data used during the development of ML
models have major influences on the later behaviour of the system. Runtime
monitors are used to provide guarantees for that behaviour. Question / problem:
We see major uncertainty in how to specify training data and runtime monitoring
for critical ML models and by this specifying the final functionality of the
system. In this interview-based study we investigate the underlying challenges
for these difficulties. Principal ideas/results: Based on ten interviews with
practitioners who develop ML models for critical applications in the automotive
and telecommunication sector, we identified 17 underlying challenges in 6
challenge groups that relate to the challenge of specifying training data and
runtime monitoring. Contribution: The article provides a list of the identified
underlying challenges related to the difficulties practitioners experience when
specifying training data and runtime monitoring for ML models. Furthermore,
interconnection between the challenges were found and based on these
connections recommendation proposed to overcome the root causes for the
challenges.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：GDOD: Effective Gradient Descent using Orthogonal Decomposition for  Multi-Task Learning</b></summary>
  <p><b>编号</b>：[154]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13465</p>
  <p><b>作者</b>：Xin Dong,  Ruize Wu,  Chao Xiong,  Hai Li,  Lei Cheng,  Yong He,  Shiyou Qian,  Jian Cao,  Linjian Mo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：experienced rapid growth, solving multiple related, aims at solving, recent years, solving multiple</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multi-task learning (MTL) aims at solving multiple related tasks
simultaneously and has experienced rapid growth in recent years. However, MTL
models often suffer from performance degeneration with negative transfer due to
learning several tasks simultaneously. Some related work attributed the source
of the problem is the conflicting gradients. In this case, it is needed to
select useful gradient updates for all tasks carefully. To this end, we propose
a novel optimization approach for MTL, named GDOD, which manipulates gradients
of each task using an orthogonal basis decomposed from the span of all task
gradients. GDOD decomposes gradients into task-shared and task-conflict
components explicitly and adopts a general update rule for avoiding
interference across all task gradients. This allows guiding the update
directions depending on the task-shared components. Moreover, we prove the
convergence of GDOD theoretically under both convex and non-convex assumptions.
Experiment results on several multi-task datasets not only demonstrate the
significant improvement of GDOD performed to existing MTL models but also prove
that our algorithm outperforms state-of-the-art optimization methods in terms
of AUC and Logloss metrics.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：Training with Mixed-Precision Floating-Point Assignments</b></summary>
  <p><b>编号</b>：[155]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13464</p>
  <p><b>作者</b>：Wonyeol Lee,  Rahul Sharma,  Alex Aiken</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：deep neural networks, training deep neural, neural networks, keeping all tensors, precision</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>When training deep neural networks, keeping all tensors in high precision
(e.g., 32-bit or even 16-bit floats) is often wasteful. However, keeping all
tensors in low precision (e.g., 8-bit floats) can lead to unacceptable accuracy
loss. Hence, it is important to use a precision assignment -- a mapping from
all tensors (arising in training) to precision levels (high or low) -- that
keeps most of the tensors in low precision and leads to sufficiently accurate
models. We provide a technique that explores this memory-accuracy tradeoff by
generating precision assignments that (i) use less memory and (ii) lead to more
accurate models at the same time, compared to the precision assignments
considered by prior work in low-precision floating-point training. Our method
typically provides > 2x memory reduction over a baseline precision assignment
while preserving training accuracy, and gives further reductions by trading off
accuracy. Compared to other baselines which sometimes cause training to
diverge, our method provides similar or better memory reduction while avoiding
divergence.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：Learning Generalized Hybrid Proximity Representation for Image  Recognition</b></summary>
  <p><b>编号</b>：[156]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13459</p>
  <p><b>作者</b>：Zhiyuan Li,  Anca Ralescu</p>
  <p><b>备注</b>：The paper has been accepted by the IEEE ICTAI 2022</p>
  <p><b>关键词</b>：techniques received attention, unsupervised learning tasks, learning techniques received, deep metric learning, metric learning techniques</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, deep metric learning techniques received attention, as the learned
distance representations are useful to capture the similarity relationship
among samples and further improve the performance of various of supervised or
unsupervised learning tasks. We propose a novel supervised metric learning
method that can learn the distance metrics in both geometric and probabilistic
space for image recognition. In contrast to the previous metric learning
methods which usually focus on learning the distance metrics in Euclidean
space, our proposed method is able to learn better distance representation in a
hybrid approach. To achieve this, we proposed a Generalized Hybrid Metric Loss
(GHM-Loss) to learn the general hybrid proximity features from the image data
by controlling the trade-off between geometric proximity and probabilistic
proximity. To evaluate the effectiveness of our method, we first provide
theoretical derivations and proofs of the proposed loss function, then we
perform extensive experiments on two public datasets to show the advantage of
our method compared to other state-of-the-art metric learning methods.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：A Data-Driven Modeling and Control Framework for Physics-Based Building  Emulators</b></summary>
  <p><b>编号</b>：[163]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13447</p>
  <p><b>作者</b>：Chihyeon Song,  Aayushman Sharma,  Raman Goyal,  Alejandro Brito,  Saman Mostafavi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：present a data-driven, physics-based building emulators, HVAC MPC problems, building, HVAC MPC</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a data-driven modeling and control framework for physics-based
building emulators. Our approach comprises: (a) Offline training of
differentiable surrogate models that speed up model evaluations, provide cheap
gradients, and have good predictive accuracy for the receding horizon in Model
Predictive Control (MPC) and (b) Formulating and solving nonlinear building
HVAC MPC problems. We extensively verify the modeling and control performance
using multiple surrogate models and optimization frameworks for different
available test cases in the Building Optimization Testing Framework (BOPTEST).
The framework is compatible with other modeling techniques and customizable
with different control formulations. The modularity makes the approach
future-proof for test cases currently in development for physics-based building
emulators and provides a path toward prototyping predictive controllers in
large buildings.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：Sharp Variance-Dependent Bounds in Reinforcement Learning: Best of Both  Worlds in Stochastic and Deterministic Environments</b></summary>
  <p><b>编号</b>：[164]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13446</p>
  <p><b>作者</b>：Runlong Zhou,  Zihan Zhang,  Simon S. Du</p>
  <p><b>备注</b>：43 pages, 1 figure</p>
  <p><b>关键词</b>：Markov decision processes, Markov decision, decision processes, variance-dependent regret, deterministic MDPs</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study variance-dependent regret bounds for Markov decision processes
(MDPs). Algorithms with variance-dependent regret guarantees can automatically
exploit environments with low variance (e.g., enjoying constant regret on
deterministic MDPs). The existing algorithms are either variance-independent or
suboptimal. We first propose two new environment norms to characterize the
fine-grained variance properties of the environment. For model-based methods,
we design a variant of the MVP algorithm (Zhang et al., 2021a) and use new
analysis techniques show to this algorithm enjoys variance-dependent bounds
with respect to our proposed norms. In particular, this bound is simultaneously
minimax optimal for both stochastic and deterministic MDPs, the first result of
its kind. We further initiate the study on model-free algorithms with
variance-dependent regret bounds by designing a reference-function-based
algorithm with a novel capped-doubling reference update schedule. Lastly, we
also provide lower bounds to complement our upper bounds.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：A Survey of Explainable AI in Deep Visual Modeling: Methods and Metrics</b></summary>
  <p><b>编号</b>：[165]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13445</p>
  <p><b>作者</b>：Naveed Akhtar</p>
  <p><b>备注</b>：Short accessible survey (9pgs)</p>
  <p><b>关键词</b>：Deep visual models, high-stake domains, widespread applications, applications in high-stake, Deep visual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep visual models have widespread applications in high-stake domains. Hence,
their black-box nature is currently attracting a large interest of the research
community. We present the first survey in Explainable AI that focuses on the
methods and metrics for interpreting deep visual models. Covering the landmark
contributions along the state-of-the-art, we not only provide a taxonomic
organization of the existing techniques, but also excavate a range of
evaluation metrics and collate them as measures of different properties of
model explanations. Along the insightful discussion on the current trends, we
also discuss the challenges and future avenues for this research direction.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：Retiring $Δ$DP: New Distribution-Level Metrics for Demographic  Parity</b></summary>
  <p><b>编号</b>：[167]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13443</p>
  <p><b>作者</b>：Xiaotian Han,  Zhimeng Jiang,  Hongye Jin,  Zirui Liu,  Na Zou,  Qifan Wang,  Xia Hu</p>
  <p><b>备注</b>：Under review</p>
  <p><b>关键词</b>：textsf, Demographic parity, ensures equal treatment, Demographic, widely recognized measure</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Demographic parity is the most widely recognized measure of group fairness in
machine learning, which ensures equal treatment of different demographic
groups. Numerous works aim to achieve demographic parity by pursuing the
commonly used metric $\Delta DP$. Unfortunately, in this paper, we reveal that
the fairness metric $\Delta DP$ can not precisely measure the violation of
demographic parity, because it inherently has the following drawbacks:
\textit{i)} zero-value $\Delta DP$ does not guarantee zero violation of
demographic parity, \textit{ii)} $\Delta DP$ values can vary with different
classification thresholds. To this end, we propose two new fairness metrics,
\textsf{A}rea \textsf{B}etween \textsf{P}robability density function
\textsf{C}urves (\textsf{ABPC}) and \textsf{A}rea \textsf{B}etween
\textsf{C}umulative density function \textsf{C}urves (\textsf{ABCC}), to
precisely measure the violation of demographic parity in distribution level.
The new fairness metrics directly measure the difference between the
distributions of the prediction probability for different demographic groups.
Thus our proposed new metrics enjoy: \textit{i)} zero-value
\textsf{ABCC}/\textsf{ABPC} guarantees zero violation of demographic parity;
\textit{ii)} \textsf{ABCC}/\textsf{ABPC} guarantees demographic parity while
the classification threshold adjusted. We further re-evaluate the existing fair
models with our proposed fairness metrics and observe different fairness
behaviors of those models under the new metrics.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：Scaling laws for single-agent reinforcement learning</b></summary>
  <p><b>编号</b>：[168]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13442</p>
  <p><b>作者</b>：Jacob Hilton,  Jie Tang,  John Schulman</p>
  <p><b>备注</b>：33 pages</p>
  <p><b>关键词</b>：cross-entropy loss improves, loss improves smoothly, constant scaling law, Recent work, cross-entropy loss</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent work has shown that, in generative modeling, cross-entropy loss
improves smoothly with model size and training compute, following a power law
plus constant scaling law. One challenge in extending these results to
reinforcement learning is that the main performance objective of interest, mean
episode return, need not vary smoothly. To overcome this, we introduce
*intrinsic performance*, a monotonic function of the return defined as the
minimum compute required to achieve the given return across a family of models
of different sizes. We find that, across a range of environments, intrinsic
performance scales as a power law in model size and environment interactions.
Consequently, as in generative modeling, the optimal model size scales as a
power law in the training compute budget. Furthermore, we study how this
relationship varies with the environment and with other properties of the
training setup. In particular, using a toy MNIST-based environment, we show
that varying the "horizon length" of the task mostly changes the coefficient
but not the exponent of this relationship.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：CMLCompiler: A Unified Compiler for Classical Machine Learning</b></summary>
  <p><b>编号</b>：[169]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13441</p>
  <p><b>作者</b>：Xu Wen,  Wanling Gao,  Anzheng Li,  Lei Wang,  Zihan Jiang,  Zihan Jiang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Classical machine learning, machine learning, Classical machine, occupies nearly half, production applications</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Classical machine learning (CML) occupies nearly half of machine learning
pipelines in production applications. Unfortunately, it fails to utilize the
state-of-the-practice devices fully and performs poorly. Without a unified
framework, the hybrid deployments of deep learning (DL) and CML also suffer
from severe performance and portability issues. This paper presents the design
of a unified compiler, called CMLCompiler, for CML inference. We propose two
unified abstractions: operator representations and extended computational
graphs. The CMLCompiler framework performs the conversion and graph
optimization based on two unified abstractions, then outputs an optimized
computational graph to DL compilers or frameworks. We implement CMLCompiler on
TVM. The evaluation shows CMLCompiler's portability and superior performance.
It achieves up to 4.38x speedup on CPU, 3.31x speedup on GPU, and 5.09x speedup
on IoT devices, compared to the state-of-the-art solutions -- scikit-learn,
intel sklearn, and hummingbird. Our performance of CML and DL mixed pipelines
achieves up to 3.04x speedup compared with cross-framework implementations.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：Contrast and Clustering: Learning Neighborhood Pair Representation for  Source-free Domain Adaptation</b></summary>
  <p><b>编号</b>：[172]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13428</p>
  <p><b>作者</b>：Yuqi Chen,  Xiangbin Zhu,  Yonggang Li,  Yingjian Li,  Yuanwang Wei,  Haojie Fang</p>
  <p><b>备注</b>：conference paper</p>
  <p><b>关键词</b>：machine learning community, learning community, attracted a great, great deal, deal of attention</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Domain adaptation has attracted a great deal of attention in the machine
learning community, but it requires access to source data, which often raises
concerns about data privacy. We are thus motivated to address these issues and
propose a simple yet efficient method. This work treats domain adaptation as an
unsupervised clustering problem and trains the target model without access to
the source data. Specifically, we propose a loss function called contrast and
clustering (CaC), where a positive pair term pulls neighbors belonging to the
same class together in the feature space to form clusters, while a negative
pair term pushes samples of different classes apart. In addition, extended
neighbors are taken into account by querying the nearest neighbor indexes in
the memory bank to mine for more valuable negative pairs. Extensive experiments
on three common benchmarks, VisDA, Office-Home and Office-31, demonstrate that
our method achieves state-of-the-art performance. The code will be made
publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：Superhuman Fairness</b></summary>
  <p><b>编号</b>：[177]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13420</p>
  <p><b>作者</b>：Omid Memarrast,  Linh Vu,  Brian Ziebart</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：increasingly important focus, increasingly important, important focus, design of supervised, machine learning methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The fairness of machine learning-based decisions has become an increasingly
important focus in the design of supervised machine learning methods. Most
fairness approaches optimize a specified trade-off between performance
measure(s) (e.g., accuracy, log loss, or AUC) and fairness metric(s) (e.g.,
demographic parity, equalized odds). This begs the question: are the right
performance-fairness trade-offs being specified? We instead re-cast fair
machine learning as an imitation learning task by introducing superhuman
fairness, which seeks to simultaneously outperform human decisions on multiple
predictive performance and fairness measures. We demonstrate the benefits of
this approach given suboptimal decisions.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete  Annotations</b></summary>
  <p><b>编号</b>：[179]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13418</p>
  <p><b>作者</b>：Yuanhong Chen,  Yuyuan Liu,  Chong Wang,  Michael Elliott,  Chun Fung Kwok,  Carlos Pe na-Solorzano,  Yu Tian,  Fengbei Liu,  Helen Frazer,  Davis J. McCarthy,  Gustavo Carneiro</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：screening mammogram datasets, real-world screening mammogram, fully annotated datasets, images are labelled, weakly annotated subset</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Methods to detect malignant lesions from screening mammograms are usually
trained with fully annotated datasets, where images are labelled with the
localisation and classification of cancerous lesions. However, real-world
screening mammogram datasets commonly have a subset that is fully annotated and
another subset that is weakly annotated with just the global classification
(i.e., without lesion localisation). Given the large size of such datasets,
researchers usually face a dilemma with the weakly annotated subset: to not use
it or to fully annotate it. The first option will reduce detection accuracy
because it does not use the whole dataset, and the second option is too
expensive given that the annotation needs to be done by expert radiologists. In
this paper, we propose a middle-ground solution for the dilemma, which is to
formulate the training as a weakly- and semi-supervised learning problem that
we refer to as malignant breast lesion detection with incomplete annotations.
To address this problem, our new method comprises two stages, namely: 1)
pre-training a multi-view mammogram classifier with weak supervision from the
whole dataset, and 2) extending the trained classifier to become a multi-view
detector that is trained with semi-supervised student-teacher learning, where
the training set contains fully and weakly-annotated mammograms. We provide
extensive detection results on two real-world screening mammogram datasets
containing incomplete annotations, and show that our proposed approach achieves
state-of-the-art results in the detection of malignant breast lesions with
incomplete annotations.</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：LogAI: A Library for Log Analytics and Intelligence</b></summary>
  <p><b>编号</b>：[181]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13415</p>
  <p><b>作者</b>：Qian Cheng,  Amrita Saha,  Wenzhuo Yang,  Chenghao Liu,  Doyen Sahoo,  Steven Hoi</p>
  <p><b>备注</b>：17 pages, 7 figures, technical report for open source code, paper release with code</p>
  <p><b>关键词</b>：record runtime information, logs record runtime, System logs record, record runtime, runtime information</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Software and System logs record runtime information about processes executing
within a system. These logs have become the most critical and ubiquitous forms
of observability data that help developers understand system behavior, monitor
system health and resolve issues. However, the volume of logs generated can be
humongous (of the order of petabytes per day) especially for complex
distributed systems, such as cloud, search engine, social media, etc. This has
propelled a lot of research on developing AI-based log based analytics and
intelligence solutions that can process huge volume of raw logs and generate
insights. In order to enable users to perform multiple types of AI-based log
analysis tasks in a uniform manner, we introduce LogAI
(this https URL), a one-stop open source library for log
analytics and intelligence. LogAI supports tasks such as log summarization, log
clustering and log anomaly detection. It adopts the OpenTelemetry data model,
to enable compatibility with different log management platforms. LogAI provides
a unified model interface and provides popular time-series, statistical
learning and deep learning models. Alongside this, LogAI also provides an
out-of-the-box GUI for users to conduct interactive analysis. With LogAI, we
can also easily benchmark popular deep learning algorithms for log anomaly
detection without putting in redundant effort to process the logs. We have
opensourced LogAI to cater to a wide range of applications benefiting both
academic research and industrial prototyping.</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：Fine Robotic Manipulation without Force/Torque Sensor</b></summary>
  <p><b>编号</b>：[182]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13413</p>
  <p><b>作者</b>：Shilin Shan,  Quang-Cuong Pham</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Force Control, Force Sensing, Force, high-precision force control, external wrench</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Force Sensing and Force Control are essential to many industrial
applications. Typically, a 6-axis Force/Torque (F/T) sensor is mounted between
the robot's wrist and the end-effector in order to measure the forces and
torques exerted by the environment onto the robot (the external wrench).
Although a typical 6-axis F/T sensor can provide highly accurate measurements,
it is expensive and vulnerable to drift and external impacts. Existing methods
aiming at estimating the external wrench using only the robot's internal
signals are limited in scope: for example, wrench estimation accuracy was
mostly validated in free-space motions and simple contacts as opposed to tasks
like assembly that require high-precision force control. Here we present a
Neural Network based method and argue that by devoting particular attention to
the training data structure, it is possible to accurately estimate the external
wrench in a wide range of scenarios based solely on internal signals. As an
illustration, we demonstrate a pin insertion experiment with 100-micron
clearance and a hand-guiding experiment, both performed without external F/T
sensors or joint torque sensors. Our result opens the possibility of equipping
the existing 2.7 million industrial robots with Force Sensing and Force Control
capabilities without any additional hardware.</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：Classified as unknown: A novel Bayesian neural network</b></summary>
  <p><b>编号</b>：[188]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13401</p>
  <p><b>作者</b>：Tianbo Yang,  Tianshuo Yang</p>
  <p><b>备注</b>：12 pages, 12 figures</p>
  <p><b>关键词</b>：softmax activation function, activation function, probit function, establish estimations, output distribution</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We establish estimations for the parameters of the output distribution for
the softmax activation function using the probit function. As an application,
we develop a new efficient Bayesian learning algorithm for fully connected
neural networks, where training and predictions are performed within the
Bayesian inference framework in closed-form. This approach allows sequential
learning and requires no computationally expensive gradient calculation and
Monte Carlo sampling. Our work generalizes the Bayesian algorithm for a single
perceptron for binary classification in \cite{H} to multi-layer perceptrons for
multi-class classification.</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：Sequential Strategic Screening</b></summary>
  <p><b>编号</b>：[189]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13397</p>
  <p><b>作者</b>：Lee Cohen,  Saeed Sharifi-Malvajerd,  Kevin Stangl,  Ali Vakilian,  Juba Ziani</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：initiate the study, study of strategic, screening processes, multiple classifiers, screening</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We initiate the study of strategic behavior in screening processes with
multiple classifiers. We focus on two contrasting settings: a conjunctive
setting in which an individual must satisfy all classifiers simultaneously, and
a sequential setting in which an individual to succeed must satisfy classifiers
one at a time. In other words, we introduce the combination of strategic
classification with screening processes.
We show that sequential screening pipelines exhibit new and surprising
behavior where individuals can exploit the sequential ordering of the tests to
zig-zag between classifiers without having to simultaneously satisfy all of
them. We demonstrate an individual can obtain a positive outcome using a
limited manipulation budget even when far from the intersection of the positive
regions of every classifier. Finally, we consider a learner whose goal is to
design a sequential screening process that is robust to such manipulations, and
provide a construction for the learner that optimizes a natural objective.</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：Faster Predict-and-Optimize with Three-Operator Splitting</b></summary>
  <p><b>编号</b>：[190]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13395</p>
  <p><b>作者</b>：Daniel McKenzie,  Samy Wu Fung,  Howard Heaton</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：practical settings, solved with similar, repeatedly solved, distinct parameters, combinatorial problem</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In many practical settings, a combinatorial problem must be repeatedly solved
with similar, but distinct parameters w. Yet, w is not directly observed; only
contextual data d that correlates with w is available. It is tempting to use a
neural network to predict w given d, but training such a model requires
reconciling the discrete nature of combinatorial optimization with the
gradient-based frameworks used to train neural networks. One approach to
overcoming this issue is to consider a continuous relaxation of the
combinatorial problem.
While existing such approaches have shown to be highly effective on small
problems (10-100 variables) they do not scale well to large problems. In this
work, we show how recent results in operator splitting can be used to design
such a system which is easy to train and scales effortlessly to problems with
thousands of variables.</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：Probably Anytime-Safe Stochastic Combinatorial Semi-Bandits</b></summary>
  <p><b>编号</b>：[191]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13393</p>
  <p><b>作者</b>：Yunlong Hou,  Vincent Y. F. Tan,  Zixin Zhong</p>
  <p><b>备注</b>：56 pages, 5 figures</p>
  <p><b>关键词</b>：making online decisions, stochastic combinatorial semi-bandits, combinatorial semi-bandits problem, incur undue amount, anytime-safe stochastic combinatorial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Motivated by concerns about making online decisions that incur undue amount
of risk at each time step, in this paper, we formulate the probably
anytime-safe stochastic combinatorial semi-bandits problem. In this problem,
the agent is given the option to select a subset of size at most $K$ from a set
of $L$ ground items. Each item is associated to a certain mean reward as well
as a variance that represents its risk. To mitigate the risk that the agent
incurs, we require that with probability at least $1-\delta$, over the entire
horizon of time $T$, each of the choices that the agent makes should contain
items whose sum of variances does not exceed a certain variance budget. We call
this probably anytime-safe constraint. Under this constraint, we design and
analyze an algorithm {\sc PASCombUCB} that minimizes the regret over the
horizon of time $T$. By developing accompanying information-theoretic lower
bounds, we show under both the problem-dependent and problem-independent
paradigms, {\sc PASCombUCB} is almost asymptotically optimal. Our problem
setup, the proposed {\sc PASCombUCB} algorithm, and novel analyses are
applicable to domains such as recommendation systems and transportation in
which an agent is allowed to choose multiple items at a single time step and
wishes to control the risk over the whole time horizon.</p>
  </details>
</details>
<details>
  <summary>83. <b>标题：Combinatorial Causal Bandits without Graph Skeleton</b></summary>
  <p><b>编号</b>：[192]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13392</p>
  <p><b>作者</b>：Shi Feng,  Nuoya Xiong,  Wei Chen</p>
  <p><b>备注</b>：36 pages, 2 figures</p>
  <p><b>关键词</b>：learning agent chooses, combinatorial causal bandits, CCB problem, general causal models, observed variables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In combinatorial causal bandits (CCB), the learning agent chooses a subset of
variables in each round to intervene and collects feedback from the observed
variables to minimize expected regret or sample complexity. Previous works
study this problem in both general causal models and binary generalized linear
models (BGLMs). However, all of them require prior knowledge of causal graph
structure. This paper studies the CCB problem without the graph structure on
binary general causal models and BGLMs. We first provide an exponential lower
bound of cumulative regrets for the CCB problem on general causal models. To
overcome the exponentially large space of parameters, we then consider the CCB
problem on BGLMs. We design a regret minimization algorithm for BGLMs even
without the graph skeleton and show that it still achieves $O(\sqrt{T}\ln T)$
expected regret. This asymptotic regret is the same as the state-of-art
algorithms relying on the graph structure. Moreover, we sacrifice the regret to
$O(T^{\frac{2}{3}}\ln T)$ to remove the weight gap covered by the asymptotic
notation. At last, we give some discussions and algorithms for pure exploration
of the CCB problem without the graph structure.</p>
  </details>
</details>
<details>
  <summary>84. <b>标题：Differentially Private Kernel Inducing Points (DP-KIP) for  Privacy-preserving Data Distillation</b></summary>
  <p><b>编号</b>：[193]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13389</p>
  <p><b>作者</b>：Margarita Vinaroz,  Mi Jung Park</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：data empirical robustness, empirical robustness, imply a provable, data distillation preserves, data distillation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While it is tempting to believe that data distillation preserves privacy,
distilled data's empirical robustness against known attacks does not imply a
provable privacy guarantee. Here, we develop a provably privacy-preserving data
distillation algorithm, called differentially private kernel inducing points
(DP-KIP). DP-KIP is an instantiation of DP-SGD on kernel ridge regression
(KRR). Following a recent work, we use neural tangent kernels and minimize the
KRR loss to estimate the distilled datapoints (i.e., kernel inducing points).
We provide a computationally efficient JAX implementation of DP-KIP, which we
test on several popular image and tabular datasets to show its efficacy in data
distillation with differential privacy guarantees.</p>
  </details>
</details>
<details>
  <summary>85. <b>标题：Large Music Recommendation Studies for Small Teams</b></summary>
  <p><b>编号</b>：[194]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13388</p>
  <p><b>作者</b>：Kyle Robinson,  Dan Brown</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Running live music, prohibitively daunting task, live music recommendation, music recommendation studies, direct industry partnerships</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Running live music recommendation studies without direct industry
partnerships can be a prohibitively daunting task, especially for small teams.
In order to help future researchers interested in such evaluations, we present
a number of struggles we faced in the process of generating our own such
evaluation system alongside potential solutions. These problems span the topics
of users, data, computation, and application architecture.</p>
  </details>
</details>
<details>
  <summary>86. <b>标题：An Comparative Analysis of Different Pitch and Metrical Grid Encoding  Methods in the Task of Sequential Music Generation</b></summary>
  <p><b>编号</b>：[197]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13383</p>
  <p><b>作者</b>：Yuqiang Li,  Shengchen Li,  George Fazekas</p>
  <p><b>备注</b>：This is a draft before submitted to TISMIR as a journal paper</p>
  <p><b>关键词</b>：encoding methods depending, symbolic music generation, fundamental music features, specific goals, researchers usually choose</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pitch and meter are two fundamental music features for symbolic music
generation tasks, where researchers usually choose different encoding methods
depending on specific goals. However, the advantages and drawbacks of different
encoding methods have not been frequently discussed. This paper presents a
integrated analysis of the influence of two low-level feature, pitch and meter,
on the performance of a token-based sequential music generation model. First,
the commonly used MIDI number encoding and a less used class-octave encoding
are compared. Second, an dense intra-bar metric grid is imposed to the encoded
sequence as auxiliary features. Different complexity and resolutions of the
metric grid are compared. For complexity, the single token approach and the
multiple token approach are compared; for grid resolution, 0 (ablation), 1
(bar-level), 4 (downbeat-level) 12, (8th-triplet-level) up to 64
(64th-note-grid-level) are compared; for duration resolution, 4, 8, 12 and 16
subdivisions per beat are compared. All different encodings are tested on
separately trained Transformer-XL models for a melody generation task.
Regarding distribution similarity of several objective evaluation metrics to
the test dataset, results suggest that the class-octave encoding significantly
outperforms the taken-for-granted MIDI encoding on pitch-related metrics; finer
grids and multiple-token grids improve the rhythmic quality, but also suffer
from over-fitting at early training stage. Results display a general phenomenon
of over-fitting from two aspects, the pitch embedding space and the test loss
of the single-token grid encoding. From a practical perspective, we both
demonstrate the feasibility and raise the concern of easy over-fitting problem
of using smaller networks and lower embedding dimensions on the generation
task. The findings can also contribute to futural models in terms of feature
engineering.</p>
  </details>
</details>
<details>
  <summary>87. <b>标题：When Source-Free Domain Adaptation Meets Learning with Noisy Labels</b></summary>
  <p><b>编号</b>：[199]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13381</p>
  <p><b>作者</b>：Li Yi,  Gezheng Xu,  Pengcheng Xu,  Jiaqi Li,  Ruizhi Pu,  Charles Ling,  A. Ian McLeod,  Boyu Wang</p>
  <p><b>备注</b>：33 pages, 16 figures, accepted by ICLR 2023</p>
  <p><b>关键词</b>：source-free domain adaptation, meaningful cluster structures, label noise, private source data, unlabeled target domain</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent state-of-the-art source-free domain adaptation (SFDA) methods have
focused on learning meaningful cluster structures in the feature space, which
have succeeded in adapting the knowledge from source domain to unlabeled target
domain without accessing the private source data. However, existing methods
rely on the pseudo-labels generated by source models that can be noisy due to
domain shift. In this paper, we study SFDA from the perspective of learning
with label noise (LLN). Unlike the label noise in the conventional LLN
scenario, we prove that the label noise in SFDA follows a different
distribution assumption. We also prove that such a difference makes existing
LLN methods that rely on their distribution assumptions unable to address the
label noise in SFDA. Empirical evidence suggests that only marginal
improvements are achieved when applying the existing LLN methods to solve the
SFDA problem. On the other hand, although there exists a fundamental difference
between the label noise in the two scenarios, we demonstrate theoretically that
the early-time training phenomenon (ETP), which has been previously observed in
conventional label noise settings, can also be observed in the SFDA problem.
Extensive experiments demonstrate significant improvements to existing SFDA
algorithms by leveraging ETP to address the label noise in SFDA.</p>
  </details>
</details>
<details>
  <summary>88. <b>标题：Automated Time-frequency Domain Audio Crossfades using Graph Cuts</b></summary>
  <p><b>编号</b>：[200]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13380</p>
  <p><b>作者</b>：Kyle Robinson,  Dan Brown</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：music consumption scenarios, personal playback devices, music consumption, consumption scenarios, audio clip</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The problem of transitioning smoothly from one audio clip to another arises
in many music consumption scenarios, especially as music consumption has moved
from professionally curated and live-streamed radios to personal playback
devices and services. we present the first steps toward a new method of
automatically transitioning from one audio clip to another by discretizing the
frequency spectrum into bins and then finding transition times for each bin. We
phrase the problem as one of graph flow optimization; specifically
min-cut/max-flow.</p>
  </details>
</details>
<details>
  <summary>89. <b>标题：Quantized Neural Networks for Low-Precision Accumulation with Guaranteed  Overflow Avoidance</b></summary>
  <p><b>编号</b>：[202]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13376</p>
  <p><b>作者</b>：Ian Colbert,  Alessandro Pappalardo,  Jakoba Petri-Koenig</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：guarantees avoiding numerical, avoiding numerical overflow, quantization-aware training algorithm, introduce a quantization-aware, guarantees avoiding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce a quantization-aware training algorithm that guarantees avoiding
numerical overflow when reducing the precision of accumulators during
inference. We leverage weight normalization as a means of constraining
parameters during training using accumulator bit width bounds that we derive.
We evaluate our algorithm across multiple quantized models that we train for
different tasks, showing that our approach can reduce the precision of
accumulators while maintaining model accuracy with respect to a floating-point
baseline. We then show that this reduction translates to increased design
efficiency for custom FPGA-based accelerators. Finally, we show that our
algorithm not only constrains weights to fit into an accumulator of
user-defined bit width, but also increases the sparsity and compressibility of
the resulting weights. Across all of our benchmark models trained with 8-bit
weights and activations, we observe that constraining the hidden layers of
quantized neural networks to fit into 16-bit accumulators yields an average
98.2% sparsity with an estimated compression rate of 46.5x all while
maintaining 99.2% of the floating-point performance.</p>
  </details>
</details>
<details>
  <summary>90. <b>标题：Optimal Transport Perturbations for Safe Reinforcement Learning with  Robustness Guarantees</b></summary>
  <p><b>编号</b>：[203]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13375</p>
  <p><b>作者</b>：James Queeney,  Erhan Can Ozcan,  Ioannis Ch. Paschalidis,  Christos G. Cassandras</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：decision making applications, deep reinforcement learning, reinforcement learning, real-world decision making, trustworthy deployment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Robustness and safety are critical for the trustworthy deployment of deep
reinforcement learning in real-world decision making applications. In
particular, we require algorithms that can guarantee robust, safe performance
in the presence of general environment disturbances, while making limited
assumptions on the data collection process during training. In this work, we
propose a safe reinforcement learning framework with robustness guarantees
through the use of an optimal transport cost uncertainty set. We provide an
efficient, theoretically supported implementation based on Optimal Transport
Perturbations, which can be applied in a completely offline fashion using only
data collected in a nominal training environment. We demonstrate the robust,
safe performance of our approach on a variety of continuous control tasks with
safety constraints in the Real-World Reinforcement Learning Suite.</p>
  </details>
</details>
<details>
  <summary>91. <b>标题：On the Correctness of Automatic Differentiation for Neural Networks with  Machine-Representable Parameters</b></summary>
  <p><b>编号</b>：[206]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13370</p>
  <p><b>作者</b>：Wonyeol Lee,  Sejun Park,  Alex Aiken</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：mathematically precise sense, automatic differentiation, precise sense, mathematically precise, Recent work</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent work has shown that automatic differentiation over the reals is almost
always correct in a mathematically precise sense. However, actual programs work
with machine-representable numbers (e.g., floating-point numbers), not reals.
In this paper, we study the correctness of automatic differentiation when the
parameter space of a neural network consists solely of machine-representable
numbers. For a neural network with bias parameters, we prove that automatic
differentiation is correct at all parameters where the network is
differentiable. In contrast, it is incorrect at all parameters where the
network is non-differentiable, since it never informs non-differentiability. To
better understand this non-differentiable set of parameters, we prove a tight
bound on its size, which is linear in the number of non-differentiabilities in
activation functions, and provide a simple necessary and sufficient condition
for a parameter to be in this set. We further prove that automatic
differentiation always computes a Clarke subderivative, even on the
non-differentiable set. We also extend these results to neural networks
possibly without bias parameters.</p>
  </details>
</details>
<details>
  <summary>92. <b>标题：Optimizing DDPM Sampling with Shortcut Fine-Tuning</b></summary>
  <p><b>编号</b>：[208]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13362</p>
  <p><b>作者</b>：Ying Fan,  Kangwook Lee</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：pretrained Denoising Diffusion, pretrained Denoising, Denoising Diffusion, Integral Probability Metrics, backward diffusion process</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this study, we propose Shortcut Fine-tuning (SFT), a new approach for
addressing the challenge of fast sampling of pretrained Denoising Diffusion
Probabilistic Models (DDPMs). SFT advocates for the fine-tuning of DDPM
samplers through the direct minimization of Integral Probability Metrics (IPM),
instead of learning the backward diffusion process. This enables samplers to
discover an alternative and more efficient sampling shortcut, deviating from
the backward diffusion process. We also propose a new algorithm that is similar
to the policy gradient method for fine-tuning DDPMs by proving that under
certain assumptions, the gradient descent of diffusion models is equivalent to
the policy gradient approach. Through empirical evaluation, we demonstrate that
our fine-tuning method can further enhance existing fast DDPM samplers,
resulting in sample quality comparable to or even surpassing that of the
full-step model across various datasets.</p>
  </details>
</details>
<details>
  <summary>93. <b>标题：Inference Time Evidences of Adversarial Attacks for Forensic on  Transformers</b></summary>
  <p><b>编号</b>：[213]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13356</p>
  <p><b>作者</b>：Hugo Lemarchant,  Liangzi Li,  Yiming Qian,  Yuta Nakashima,  Hajime Nagahara</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Vision Transformers, vision tasks, popular paradigm, Transformers, adversarial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision Transformers (ViTs) are becoming a very popular paradigm for vision
tasks as they achieve state-of-the-art performance on image classification.
However, although early works implied that this network structure had increased
robustness against adversarial attacks, some works argue ViTs are still
vulnerable. This paper presents our first attempt toward detecting adversarial
attacks during inference time using the network's input and outputs as well as
latent features. We design four quantifications (or derivatives) of input,
output, and latent vectors of ViT-based models that provide a signature of the
inference, which could be beneficial for the attack detection, and empirically
study their behavior over clean samples and adversarial samples. The results
demonstrate that the quantifications from input (images) and output (posterior
probabilities) are promising for distinguishing clean and adversarial samples,
while latent vectors offer less discriminative power, though they give some
insights on how adversarial perturbations work.</p>
  </details>
</details>
<details>
  <summary>94. <b>标题：Unconstrained Dynamic Regret via Sparse Coding</b></summary>
  <p><b>编号</b>：[217]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13349</p>
  <p><b>作者</b>：Zhiyu Zhang,  Ashok Cutkosky,  Ioannis Ch. Paschalidis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Online Linear Optimization, study Online Linear, Linear Optimization, Online Linear, time series forecasting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Motivated by time series forecasting, we study Online Linear Optimization
(OLO) under the coupling of two problem structures: the domain is unbounded,
and the performance of an algorithm is measured by its dynamic regret. Handling
either of them requires the regret bound to depend on certain complexity
measure of the comparator sequence -- specifically, the comparator norm in
unconstrained OLO, and the path length in dynamic regret. In contrast to a
recent work (Jacobsen & Cutkosky, 2022) that adapts to the combination of these
two complexity measures, we propose an alternative complexity measure by
recasting the problem into sparse coding. Adaptivity can be achieved by a
simple modular framework, which naturally exploits more intricate prior
knowledge of the environment. Along the way, we also present a new gradient
adaptive algorithm for static unconstrained OLO, designed using novel
continuous time machinery. This could be of independent interest.</p>
  </details>
</details>
<details>
  <summary>95. <b>标题：Few-Shot Image-to-Semantics Translation for Policy Transfer in  Reinforcement Learning</b></summary>
  <p><b>编号</b>：[221]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13343</p>
  <p><b>作者</b>：Rei Sato,  Kazuto Fukuchi,  Jun Sakuma,  Youhei Akimoto</p>
  <p><b>备注</b>：The 2022 International Joint Conference on Neural Networks (IJCNN2022)</p>
  <p><b>关键词</b>：vision-based robotics control, robotics control agents, mitigate learning difficulties, investigate policy transfer, translation to mitigate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We investigate policy transfer using image-to-semantics translation to
mitigate learning difficulties in vision-based robotics control agents. This
problem assumes two environments: a simulator environment with semantics, that
is, low-dimensional and essential information, as the state space, and a
real-world environment with images as the state space. By learning mapping from
images to semantics, we can transfer a policy, pre-trained in the simulator, to
the real world, thereby eliminating real-world on-policy agent interactions to
learn, which are costly and risky. In addition, using image-to-semantics
mapping is advantageous in terms of the computational efficiency to train the
policy and the interpretability of the obtained policy over other types of
sim-to-real transfer strategies. To tackle the main difficulty in learning
image-to-semantics mapping, namely the human annotation cost for producing a
training dataset, we propose two techniques: pair augmentation with the
transition function in the simulator environment and active learning. We
observed a reduction in the annotation cost without a decline in the
performance of the transfer, and the proposed approach outperformed the
existing approach without annotation.</p>
  </details>
</details>
<details>
  <summary>96. <b>标题：Affinity Uncertainty-based Hard Negative Mining in Graph Contrastive  Learning</b></summary>
  <p><b>编号</b>：[222]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13340</p>
  <p><b>作者</b>：Chaoxi Niu,  Guansong Pang,  Ling Chen</p>
  <p><b>备注</b>：11 pages, 4 figures</p>
  <p><b>关键词</b>：self-supervised contrastive learning, enhancing self-supervised contrastive, graph contrastive learning, contrastive learning, including graph contrastive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Hard negative mining has shown effective in enhancing self-supervised
contrastive learning (CL) on diverse data types, including graph contrastive
learning (GCL). Existing hardness-aware CL methods typically treat negative
instances that are most similar to the anchor instance as hard negatives, which
helps improve the CL performance, especially on image data. However, this
approach often fails to identify the hard negatives but leads to many false
negatives on graph data. This is mainly due to that the learned graph
representations are not sufficiently discriminative due to over-smooth
representations and/or non-i.i.d. issues in graph data. To tackle this problem,
this paper proposes a novel approach that builds a discriminative model on
collective affinity information (i.e, two sets of pairwise affinities between
the negative instances and the anchor instance) to mine hard negatives in GCL.
In particular, the proposed approach evaluates how confident/uncertain the
discriminative model is about the affinity of each negative instance to an
anchor instance to determine its hardness weight relative to the anchor
instance. This uncertainty information is then incorporated into existing GCL
loss functions via a weighting term to enhance their performance. The enhanced
GCL is theoretically grounded that the resulting GCL loss is equivalent to a
triplet loss with an adaptive margin being exponentially proportional to the
learned uncertainty of each negative instance. Extensive experiments on 10
graph datasets show that our approach i) consistently enhances different
state-of-the-art GCL methods in both graph and node classification tasks, and
ii) significantly improves their robustness against adversarial attacks.</p>
  </details>
</details>
<details>
  <summary>97. <b>标题：Continuous Spatiotemporal Transformers</b></summary>
  <p><b>编号</b>：[224]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13338</p>
  <p><b>作者</b>：Antonio H. de O. Fonseca,  Emanuele Zappala,  Josue Ortega Caro,  David van Dijk</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：dynamical systems, continuous, systems, fundamental challenge, successful in NLP</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Modeling spatiotemporal dynamical systems is a fundamental challenge in
machine learning. Transformer models have been very successful in NLP and
computer vision where they provide interpretable representations of data.
However, a limitation of transformers in modeling continuous dynamical systems
is that they are fundamentally discrete time and space models and thus have no
guarantees regarding continuous sampling. To address this challenge, we present
the Continuous Spatiotemporal Transformer (CST), a new transformer architecture
that is designed for the modeling of continuous systems. This new framework
guarantees a continuous and smooth output via optimization in Sobolev space. We
benchmark CST against traditional transformers as well as other spatiotemporal
dynamics modeling methods and achieve superior performance in a number of tasks
on synthetic and real systems, including learning brain dynamics from calcium
imaging data.</p>
  </details>
</details>
<details>
  <summary>98. <b>标题：The Fair Value of Data Under Heterogeneous Privacy Constraints</b></summary>
  <p><b>编号</b>：[226]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13336</p>
  <p><b>作者</b>：Justin Kang,  Ramtin Pedarsani,  Kannan Ramchandran</p>
  <p><b>备注</b>：29 pages, 5 figures</p>
  <p><b>关键词</b>：Modern data aggregation, data, platform collecting data, users, Modern data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Modern data aggregation often takes the form of a platform collecting data
from a network of users. More than ever, these users are now requesting that
the data they provide is protected with a guarantee of privacy. This has led to
the study of optimal data acquisition frameworks, where the optimality
criterion is typically the maximization of utility for the agent trying to
acquire the data. This involves determining how to allocate payments to users
for the purchase of their data at various privacy levels. The main goal of this
paper is to characterize a fair amount to pay users for their data at a given
privacy level. We propose an axiomatic definition of fairness, analogous to the
celebrated Shapley value. Two concepts for fairness are introduced. The first
treats the platform and users as members of a common coalition and provides a
complete description of how to divide the utility among the platform and users.
In the second concept, fairness is defined only among users, leading to a
potential fairness-constrained mechanism design problem for the platform. We
consider explicit examples involving private heterogeneous data and show how
these notions of fairness can be applied. To the best of our knowledge, these
are the first fairness concepts for data that explicitly consider privacy
constraints.</p>
  </details>
</details>
<details>
  <summary>99. <b>标题：Fast Resolution Agnostic Neural Techniques to Solve Partial Differential  Equations</b></summary>
  <p><b>编号</b>：[229]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13331</p>
  <p><b>作者</b>：Hrishikesh Viswanath,  Md Ashiqur Rahman,  Abhijeet Vyas,  Andrey Shor,  Beatriz Medeiros,  Stephanie Hernandez,  Suhas Eswarappa Prameela,  Aniket Bera</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：partial differential equations, mathematical problems involving, problems involving functions, fluid flow, Numerical approximations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Numerical approximations of partial differential equations (PDEs) are
routinely employed to formulate the solution of physics, engineering and
mathematical problems involving functions of several variables, such as the
propagation of heat or sound, fluid flow, elasticity, electrostatics,
electrodynamics, and more. While this has led to solving many complex
phenomena, there are still significant limitations. Conventional approaches
such as Finite Element Methods (FEMs) and Finite Differential Methods (FDMs)
require considerable time and are computationally expensive. In contrast,
machine learning-based methods such as neural networks are faster once trained,
but tend to be restricted to a specific discretization. This article aims to
provide a comprehensive summary of conventional methods and recent machine
learning-based methods to approximate PDEs numerically. Furthermore, we
highlight several key architectures centered around the neural operator, a
novel and fast approach (1000x) to learning the solution operator of a PDE. We
will note how these new computational approaches can bring immense advantages
in tackling many problems in fundamental and applied physics.</p>
  </details>
</details>
<details>
  <summary>100. <b>标题：Efficient and Effective Methods for Mixed Precision Neural Network  Quantization for Faster, Energy-efficient Inference</b></summary>
  <p><b>编号</b>：[230]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13330</p>
  <p><b>作者</b>：Deepika Bablani,  Jeffrey L. Mckinstry,  Steven K. Esser,  Rathinakumar Appuswamy,  Dharmendra S. Modha</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：efficient deep neural, neural network inference, deep neural network, effective and efficient, efficient deep</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>For effective and efficient deep neural network inference, it is desirable to
achieve state-of-the-art accuracy with the simplest networks requiring the
least computation, memory, and power. Quantizing networks to lower precision is
a powerful technique for simplifying networks. It is generally desirable to
quantize as aggressively as possible without incurring significant accuracy
degradation. As each layer of a network may have different sensitivity to
quantization, mixed precision quantization methods selectively tune the
precision of individual layers of a network to achieve a minimum drop in task
performance (e.g., accuracy). To estimate the impact of layer precision choice
on task performance two methods are introduced: i) Entropy Approximation Guided
Layer selection (EAGL) is fast and uses the entropy of the weight distribution,
and ii) Accuracy-aware Layer Precision Selection (ALPS) is straightforward and
relies on single epoch fine-tuning after layer precision reduction. Using EAGL
and ALPS for layer precision selection, full-precision accuracy is recovered
with a mix of 4-bit and 2-bit layers for ResNet-50 and ResNet-101
classification networks, demonstrating improved performance across the entire
accuracy-throughput frontier, and equivalent performance for the PSPNet
segmentation network in our own commensurate comparison over leading mixed
precision layer selection techniques, while requiring orders of magnitude less
compute time to reach a solution.</p>
  </details>
</details>
<details>
  <summary>101. <b>标题：A Framework for Adapting Offline Algorithms to Solve Combinatorial  Multi-Armed Bandit Problems with Bandit Feedback</b></summary>
  <p><b>编号</b>：[233]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13326</p>
  <p><b>作者</b>：Guanyu Nie,  Yididiya Y Nadew,  Yanhui Zhu,  Vaneet Aggarwal,  Christopher John Quinn</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：combinatorial multi-armed bandits, require bandit feedback, bandit feedback, combinatorial multi-armed, offline approximation algorithms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We investigate the problem of stochastic, combinatorial multi-armed bandits
where the learner only has access to bandit feedback and the reward function
can be non-linear. We provide a general framework for adapting discrete offline
approximation algorithms into sublinear $\alpha$-regret methods that only
require bandit feedback, achieving
$\mathcal{O}\left(T^\frac{2}{3}\log(T)^\frac{1}{3}\right)$ expected cumulative
$\alpha$-regret dependence on the horizon $T$. The framework only requires the
offline algorithms to be robust to small errors in function evaluation. The
adaptation procedure does not even require explicit knowledge of the offline
approximation algorithm -- the offline algorithm can be used as black box
subroutine.
To demonstrate the utility of the proposed framework, the proposed framework
is applied to multiple problems in submodular maximization, adapting
approximation algorithms for cardinality and for knapsack constraints. The new
CMAB algorithms for knapsack constraints outperform a full-bandit method
developed for the adversarial setting in experiments with real-world data.</p>
  </details>
</details>
<details>
  <summary>102. <b>标题：V2N Service Scaling with Deep Reinforcement Learning</b></summary>
  <p><b>编号</b>：[234]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13324</p>
  <p><b>作者</b>：Cyril Shih-Huan Hsu,  Jorge Martín-Pérez,  Chrysa Papagianni,  Paola Grosso</p>
  <p><b>备注</b>：Accepted at the 36th IEEE/IFIP Network Operations and Management Symposium (NOMS 2023)</p>
  <p><b>关键词</b>：wireless networks, meet the stringent, stringent requirements, Edge computing, Edge computing resources</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The fifth generation (5G) of wireless networks is set out to meet the
stringent requirements of vehicular use cases. Edge computing resources can aid
in this direction by moving processing closer to end-users, reducing latency.
However, given the stochastic nature of traffic loads and availability of
physical resources, appropriate auto-scaling mechanisms need to be employed to
support cost-efficient and performant services. To this end, we employ Deep
Reinforcement Learning (DRL) for vertical scaling in Edge computing to support
vehicular-to-network communications. We address the problem using Deep
Deterministic Policy Gradient (DDPG). As DDPG is a model-free off-policy
algorithm for learning continuous actions, we introduce a discretization
approach to support discrete scaling actions. Thus we address scalability
problems inherent to high-dimensional discrete action spaces. Employing a
real-world vehicular trace data set, we show that DDPG outperforms existing
solutions, reducing (at minimum) the average number of active CPUs by 23% while
increasing the long-term reward by 24%.</p>
  </details>
</details>
<details>
  <summary>103. <b>标题：Fairness and Accuracy under Domain Generalization</b></summary>
  <p><b>编号</b>：[235]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13323</p>
  <p><b>作者</b>：Thai-Hoang Pham,  Xueru Zhang,  Ping Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：high-stakes applications, concerns have arisen, social groups, accuracy, deployment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As machine learning (ML) algorithms are increasingly used in high-stakes
applications, concerns have arisen that they may be biased against certain
social groups. Although many approaches have been proposed to make ML models
fair, they typically rely on the assumption that data distributions in training
and deployment are identical. Unfortunately, this is commonly violated in
practice and a model that is fair during training may lead to an unexpected
outcome during its deployment. Although the problem of designing robust ML
models under dataset shifts has been widely studied, most existing works focus
only on the transfer of accuracy. In this paper, we study the transfer of both
fairness and accuracy under domain generalization where the data at test time
may be sampled from never-before-seen domains. We first develop theoretical
bounds on the unfairness and expected loss at deployment, and then derive
sufficient conditions under which fairness and accuracy can be perfectly
transferred via invariant representation learning. Guided by this, we design a
learning algorithm such that fair ML models learned with training data still
have high fairness and accuracy when deployment environments change.
Experiments on real-world data validate the proposed algorithm. Model
implementation is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>104. <b>标题：Proxy-based Zero-Shot Entity Linking by Effective Candidate Retrieval</b></summary>
  <p><b>编号</b>：[237]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13318</p>
  <p><b>作者</b>：Maciej Wiatrak,  Eirini Arvaniti,  Angus Brayne,  Jonas Vetterle,  Aaron Sim</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：powerful two-stage algorithms, biomedical Entity Linking, Entity Linking, candidate retrieval stage, initial candidate retrieval</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A recent advancement in the domain of biomedical Entity Linking is the
development of powerful two-stage algorithms, an initial candidate retrieval
stage that generates a shortlist of entities for each mention, followed by a
candidate ranking stage. However, the effectiveness of both stages are
inextricably dependent on computationally expensive components. Specifically,
in candidate retrieval via dense representation retrieval it is important to
have hard negative samples, which require repeated forward passes and nearest
neighbour searches across the entire entity label set throughout training. In
this work, we show that pairing a proxy-based metric learning loss with an
adversarial regularizer provides an efficient alternative to hard negative
sampling in the candidate retrieval stage. In particular, we show competitive
performance on the recall@1 metric, thereby providing the option to leave out
the expensive candidate ranking step. Finally, we demonstrate how the model can
be used in a zero-shot setting to discover out of knowledge base biomedical
entities.</p>
  </details>
</details>
<details>
  <summary>105. <b>标题：Incorporating Recurrent Reinforcement Learning into Model Predictive  Control for Adaptive Control in Autonomous Driving</b></summary>
  <p><b>编号</b>：[239]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13313</p>
  <p><b>作者</b>：Yuan Zhang,  Joschka Boedecker,  Chuxuan Li,  Guyue Zhou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：attracting tremendous attention, autonomous driving task, Model Predictive Control, powerful control technique, Model Predictive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Model Predictive Control (MPC) is attracting tremendous attention in the
autonomous driving task as a powerful control technique. The success of an MPC
controller strongly depends on an accurate internal dynamics model. However,
the static parameters, usually learned by system identification, often fail to
adapt to both internal and external perturbations in real-world scenarios. In
this paper, we firstly (1) reformulate the problem as a Partially Observed
Markov Decision Process (POMDP) that absorbs the uncertainties into
observations and maintains Markov property into hidden states; and (2) learn a
recurrent policy continually adapting the parameters of the dynamics model via
Recurrent Reinforcement Learning (RRL) for optimal and adaptive control; and
(3) finally evaluate the proposed algorithm (referred as $\textit{MPC-RRL}$) in
CARLA simulator and leading to robust behaviours under a wide range of
perturbations.</p>
  </details>
</details>
<details>
  <summary>106. <b>标题：Alternating Updates for Efficient Transformers</b></summary>
  <p><b>编号</b>：[242]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13310</p>
  <p><b>作者</b>：Cenk Baykal,  Dylan Cutler,  Nishanth Dikkala,  Nikhil Ghosh,  Rina Panigrahy,  Xin Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：deep transformer networks, transformer networks leads, quality and performance, networks leads, leads to improved</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>It is well established that increasing scale in deep transformer networks
leads to improved quality and performance. This increase in scale often comes
with an increase in compute cost and inference latency. Consequently, research
into methods which help realize the benefits of increased scale without leading
to an increase in the compute cost becomes important. We introduce Alternating
Updates (AltUp), a simple-to-implement method to increase a model's capacity
without the computational burden. AltUp enables the widening of the learned
representation without increasing the computation time by working on a subblock
of the representation at each layer. Our experiments on various transformer
models and language tasks demonstrate the consistent effectiveness of
alternating updates on a diverse set of benchmarks. Finally, we present
extensions of AltUp to the sequence dimension, and demonstrate how AltUp can be
synergistically combined with existing approaches, such as Sparse
Mixture-of-Experts models, to obtain efficient models with even higher
capacity.</p>
  </details>
</details>
<details>
  <summary>107. <b>标题：Autobidders with Budget and ROI Constraints: Efficiency, Regret, and  Pacing Dynamics</b></summary>
  <p><b>编号</b>：[245]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13306</p>
  <p><b>作者</b>：Brendan Lucier,  Sarath Pattathil,  Aleksandrs Slivkins,  Mengxiao Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：online advertising platform, advertising platform, study a game, game between autobidding, online advertising</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study a game between autobidding algorithms that compete in an online
advertising platform. Each autobidder is tasked with maximizing its
advertiser's total value over multiple rounds of a repeated auction, subject to
budget and/or return-on-investment constraints. We propose a gradient-based
learning algorithm that is guaranteed to satisfy all constraints and achieves
vanishing individual regret. Our algorithm uses only bandit feedback and can be
used with the first- or second-price auction, as well as with any
"intermediate" auction format. Our main result is that when these autobidders
play against each other, the resulting expected liquid welfare over all rounds
is at least half of the expected optimal liquid welfare achieved by any
allocation. This holds whether or not the bidding dynamics converges to an
equilibrium and regardless of the correlation structure between advertiser
valuations.</p>
  </details>
</details>
<details>
  <summary>108. <b>标题：Understanding Self-Distillation in the Presence of Label Noise</b></summary>
  <p><b>编号</b>：[246]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13304</p>
  <p><b>作者</b>：Rudrajit Das,  Sujay Sanghavi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：student predictions, text, student, student objective function, enquote</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Self-distillation (SD) is the process of first training a \enquote{teacher}
model and then using its predictions to train a \enquote{student} model with
the \textit{same} architecture. Specifically, the student's objective function
is $\big(\xi*\ell(\text{teacher's predictions}, \text{ student's predictions})
+ (1-\xi)*\ell(\text{given labels}, \text{ student's predictions})\big)$, where
$\ell$ is some loss function and $\xi$ is some parameter $\in [0,1]$.
Empirically, SD has been observed to provide performance gains in several
settings. In this paper, we theoretically characterize the effect of SD in two
supervised learning problems with \textit{noisy labels}. We first analyze SD
for regularized linear regression and show that in the high label noise regime,
the optimal value of $\xi$ that minimizes the expected error in estimating the
ground truth parameter is surprisingly greater than 1. Empirically, we show
that $\xi > 1$ works better than $\xi \leq 1$ even with the cross-entropy loss
for several classification datasets when 50\% or 30\% of the labels are
corrupted. Further, we quantify when optimal SD is better than optimal
regularization. Next, we analyze SD in the case of logistic regression for
binary classification with random label corruption and quantify the range of
label corruption in which the student outperforms the teacher in terms of
accuracy. To our knowledge, this is the first result of its kind for the
cross-entropy loss.</p>
  </details>
</details>
<details>
  <summary>109. <b>标题：Sifer: Overcoming simplicity bias in deep networks using a feature sieve</b></summary>
  <p><b>编号</b>：[250]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13293</p>
  <p><b>作者</b>：Rishabh Tiwari,  Pradeep Shenoy</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：weakly predictive features, over-depend on simple, weakly predictive, exclusion of stronger, Simplicity bias</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Simplicity bias is the concerning tendency of deep networks to over-depend on
simple, weakly predictive features, to the exclusion of stronger, more complex
features. This causes biased, incorrect model predictions in many real-world
applications, exacerbated by incomplete training data containing spurious
feature-label correlations. We propose a direct, interventional method for
addressing simplicity bias in DNNs, which we call the feature sieve. We aim to
automatically identify and suppress easily-computable spurious features in
lower layers of the network, thereby allowing the higher network levels to
extract and utilize richer, more meaningful representations. We provide
concrete evidence of this differential suppression & enhancement of relevant
features on both controlled datasets and real-world images, and report
substantial gains on many real-world debiasing benchmarks (11.4% relative gain
on Imagenet-A; 3.2% on BAR, etc). Crucially, we outperform many baselines that
incorporate knowledge about known spurious or biased attributes, despite our
method not using any such information. We believe that our feature sieve work
opens up exciting new research directions in automated adversarial feature
extraction & representation learning for deep networks.</p>
  </details>
</details>
<details>
  <summary>110. <b>标题：On the Statistical Benefits of Temporal Difference Learning</b></summary>
  <p><b>编号</b>：[251]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13289</p>
  <p><b>作者</b>：David Cheikhi,  Daniel Russo</p>
  <p><b>备注</b>：26 pages, 7 figures, submitted to ICML 2023</p>
  <p><b>关键词</b>：resulting long-term rewards, direct estimation approach, estimation approach fits, minimize prediction error, long-term rewards</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Given a dataset on actions and resulting long-term rewards, a direct
estimation approach fits value functions that minimize prediction error on the
training data. Temporal difference learning (TD) methods instead fit value
functions by minimizing the degree of temporal inconsistency between estimates
made at successive time-steps. Focusing on finite state Markov chains, we
provide a crisp asymptotic theory of the statistical advantages of this
approach. First, we show that an intuitive inverse trajectory pooling
coefficient completely characterizes the percent reduction in mean-squared
error of value estimates. Depending on problem structure, the reduction could
be enormous or nonexistent. Next, we prove that there can be dramatic
improvements in estimates of the difference in value-to-go for two states: TD's
errors are bounded in terms of a novel measure - the problem's trajectory
crossing time - which can be much smaller than the problem's time horizon.</p>
  </details>
</details>
<details>
  <summary>111. <b>标题：MILO: Model-Agnostic Subset Selection Framework for Efficient Model  Training and Tuning</b></summary>
  <p><b>编号</b>：[253]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13287</p>
  <p><b>作者</b>：Kirshnateja Killamsetty,  Alexandre V. Evfimievski,  Tejaswini Pedapati,  Kiran Kate,  Lucian Popa,  Rishabh Iyer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Training deep networks, computationally intensive, subset selection, deep networks, large datasets</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Training deep networks and tuning hyperparameters on large datasets is
computationally intensive. One of the primary research directions for efficient
training is to reduce training costs by selecting well-generalizable subsets of
training data. Compared to simple adaptive random subset selection baselines,
existing intelligent subset selection approaches are not competitive due to the
time-consuming subset selection step, which involves computing model-dependent
gradients and feature embeddings and applies greedy maximization of submodular
objectives. Our key insight is that removing the reliance on downstream model
parameters enables subset selection as a pre-processing step and enables one to
train multiple models at no additional cost. In this work, we propose MILO, a
model-agnostic subset selection framework that decouples the subset selection
from model training while enabling superior model convergence and performance
by using an easy-to-hard curriculum. Our empirical results indicate that MILO
can train models $3\times - 10 \times$ faster and tune hyperparameters
$20\times - 75 \times$ faster than full-dataset training or tuning without
compromising performance.</p>
  </details>
</details>
<details>
  <summary>112. <b>标题：Learning Coordination Policies over Heterogeneous Graphs for Human-Robot  Teams via Recurrent Neural Schedule Propagation</b></summary>
  <p><b>编号</b>：[257]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13279</p>
  <p><b>作者</b>：Batuhan Altundas,  Zheyuan Wang,  Joshua Bishop,  Matthew Gombolay</p>
  <p><b>备注</b>：8 pages, 2 figures, 3 Tables</p>
  <p><b>关键词</b>：human-robot collaboration increases, efficiently and intuitively, collaboration increases, coordinate efficiently, Heterogeneous Graph Attention</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As human-robot collaboration increases in the workforce, it becomes essential
for human-robot teams to coordinate efficiently and intuitively. Traditional
approaches for human-robot scheduling either utilize exact methods that are
intractable for large-scale problems and struggle to account for stochastic,
time varying human task performance, or application-specific heuristics that
require expert domain knowledge to develop. We propose a deep learning-based
framework, called HybridNet, combining a heterogeneous graph-based encoder with
a recurrent schedule propagator for scheduling stochastic human-robot teams
under upper- and lower-bound temporal constraints. The HybridNet's encoder
leverages Heterogeneous Graph Attention Networks to model the initial
environment and team dynamics while accounting for the constraints. By
formulating task scheduling as a sequential decision-making process, the
HybridNet's recurrent neural schedule propagator leverages Long Short-Term
Memory (LSTM) models to propagate forward consequences of actions to carry out
fast schedule generation, removing the need to interact with the environment
between every task-agent pair selection. The resulting scheduling policy
network provides a computationally lightweight yet highly expressive model that
is end-to-end trainable via Reinforcement Learning algorithms. We develop a
virtual task scheduling environment for mixed human-robot teams in a
multi-round setting, capable of modeling the stochastic learning behaviors of
human workers. Experimental results showed that HybridNet outperformed other
human-robot scheduling solutions across problem sizes for both deterministic
and stochastic human performance, with faster runtime compared to
pure-GNN-based schedulers.</p>
  </details>
</details>
<details>
  <summary>113. <b>标题：Near Optimal Private and Robust Linear Regression</b></summary>
  <p><b>编号</b>：[259]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13273</p>
  <p><b>作者</b>：Xiyang Liu,  Prateek Jain,  Weihao Kong,  Sewoong Oh,  Arun Sai Suggala</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：canonical statistical estimation, statistical estimation problem, differential privacy, adversarially corrupted, study the canonical</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the canonical statistical estimation problem of linear regression
from $n$ i.i.d.~examples under $(\varepsilon,\delta)$-differential privacy when
some response variables are adversarially corrupted. We propose a variant of
the popular differentially private stochastic gradient descent (DP-SGD)
algorithm with two innovations: a full-batch gradient descent to improve sample
complexity and a novel adaptive clipping to guarantee robustness. When there is
no adversarial corruption, this algorithm improves upon the existing
state-of-the-art approach and achieves a near optimal sample complexity. Under
label-corruption, this is the first efficient linear regression algorithm to
guarantee both $(\varepsilon,\delta)$-DP and robustness. Synthetic experiments
confirm the superiority of our approach.</p>
  </details>
</details>
<details>
  <summary>114. <b>标题：Probabilistic Neural Data Fusion for Learning from an Arbitrary Number  of Multi-fidelity Data Sets</b></summary>
  <p><b>编号</b>：[260]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13271</p>
  <p><b>作者</b>：Carlos Mora,  Jonathan Tammer Eweis-Labolle,  Tyler Johnson,  Likith Gadde,  Ramin Bostanabad</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：sciences analysts, analysts have simultaneous, simultaneous access, access to multiple, multiple data sources</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In many applications in engineering and sciences analysts have simultaneous
access to multiple data sources. In such cases, the overall cost of acquiring
information can be reduced via data fusion or multi-fidelity (MF) modeling
where one leverages inexpensive low-fidelity (LF) sources to reduce the
reliance on expensive high-fidelity (HF) data. In this paper, we employ neural
networks (NNs) for data fusion in scenarios where data is very scarce and
obtained from an arbitrary number of sources with varying levels of fidelity
and cost. We introduce a unique NN architecture that converts MF modeling into
a nonlinear manifold learning problem. Our NN architecture inversely learns
non-trivial (e.g., non-additive and non-hierarchical) biases of the LF sources
in an interpretable and visualizable manifold where each data source is encoded
via a low-dimensional distribution. This probabilistic manifold quantifies
model form uncertainties such that LF sources with small bias are encoded close
to the HF source. Additionally, we endow the output of our NN with a parametric
distribution not only to quantify aleatoric uncertainties, but also to
reformulate the network's loss function based on strictly proper scoring rules
which improve robustness and accuracy on unseen HF data. Through a set of
analytic and engineering examples, we demonstrate that our approach provides a
high predictive power while quantifying various sources uncertainties.</p>
  </details>
</details>
<details>
  <summary>115. <b>标题：Emergence of Maps in the Memories of Blind Navigation Agents</b></summary>
  <p><b>编号</b>：[264]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13261</p>
  <p><b>作者</b>：Erik Wijmans,  Manolis Savva,  Irfan Essa,  Stefan Lee,  Ari S. Morcos,  Dhruv Batra</p>
  <p><b>备注</b>：Accepted to ICLR 2023</p>
  <p><b>关键词</b>：maintain internal spatial, navigation research posits, internal spatial representations, Animal navigation research, research posits</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Animal navigation research posits that organisms build and maintain internal
spatial representations, or maps, of their environment. We ask if machines --
specifically, artificial intelligence (AI) navigation agents -- also build
implicit (or 'mental') maps. A positive answer to this question would (a)
explain the surprising phenomenon in recent literature of ostensibly map-free
neural-networks achieving strong performance, and (b) strengthen the evidence
of mapping as a fundamental mechanism for navigation by intelligent embodied
agents, whether they be biological or artificial. Unlike animal navigation, we
can judiciously design the agent's perceptual system and control the learning
paradigm to nullify alternative navigation mechanisms. Specifically, we train
'blind' agents -- with sensing limited to only egomotion and no other sensing
of any kind -- to perform PointGoal navigation ('go to $\Delta$ x, $\Delta$ y')
via reinforcement learning. Our agents are composed of navigation-agnostic
components (fully-connected and recurrent neural networks), and our
experimental setup provides no inductive bias towards mapping. Despite these
harsh conditions, we find that blind agents are (1) surprisingly effective
navigators in new environments (~95% success); (2) they utilize memory over
long horizons (remembering ~1,000 steps of past experience in an episode); (3)
this memory enables them to exhibit intelligent behavior (following walls,
detecting collisions, taking shortcuts); (4) there is emergence of maps and
collision detection neurons in the representations of the environment built by
a blind agent as it navigates; and (5) the emergent maps are selective and task
dependent (e.g. the agent 'forgets' exploratory detours). Overall, this paper
presents no new techniques for the AI audience, but a surprising finding, an
insight, and an explanation.</p>
  </details>
</details>
<details>
  <summary>116. <b>标题：Online Loss Function Learning</b></summary>
  <p><b>编号</b>：[266]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13247</p>
  <p><b>作者</b>：Christian Raymond,  Qi Chen,  Bing Xue,  Mengjie Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Loss function learning, Loss function, function learning, machine learning model, Loss</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Loss function learning is a new meta-learning paradigm that aims to automate
the essential task of designing a loss function for a machine learning model.
Existing techniques for loss function learning have shown promising results,
often improving a model's training dynamics and final inference performance.
However, a significant limitation of these techniques is that the loss
functions are meta-learned in an offline fashion, where the meta-objective only
considers the very first few steps of training, which is a significantly
shorter time horizon than the one typically used for training deep neural
networks. This causes significant bias towards loss functions that perform well
at the very start of training but perform poorly at the end of training. To
address this issue we propose a new loss function learning technique for
adaptively updating the loss function online after each update to the base
model parameters. The experimental results show that our proposed method
consistently outperforms the cross-entropy loss and offline loss function
learning techniques on a diverse range of neural network architectures and
datasets.</p>
  </details>
</details>
<details>
  <summary>117. <b>标题：Conversational Automated Program Repair</b></summary>
  <p><b>编号</b>：[267]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13246</p>
  <p><b>作者</b>：Chunqiu Steven Xia,  Lingming Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：developers automatically generate, automatically generate patches, Automated Program Repair, conversational APR, APR</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automated Program Repair (APR) can help developers automatically generate
patches for bugs. Due to the impressive performance obtained using Large
Pre-Trained Language Models (LLMs) on many code related tasks, researchers have
started to directly use LLMs for APR. However, prior approaches simply
repeatedly sample the LLM given the same constructed input/prompt created from
the original buggy code, which not only leads to generating the same incorrect
patches repeatedly but also miss the critical information in testcases. To
address these limitations, we propose conversational APR, a new paradigm for
program repair that alternates between patch generation and validation in a
conversational manner. In conversational APR, we iteratively build the input to
the model by combining previously generated patches with validation feedback.
As such, we leverage the long-term context window of LLMs to not only avoid
generating previously incorrect patches but also incorporate validation
feedback to help the model understand the semantic meaning of the program under
test. We evaluate 10 different LLM including the newly developed ChatGPT model
to demonstrate the improvement of conversational APR over the prior LLM for APR
approach.</p>
  </details>
</details>
<details>
  <summary>118. <b>标题：SoftTreeMax: Exponential Variance Reduction in Policy Gradient via Tree  Search</b></summary>
  <p><b>编号</b>：[271]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13236</p>
  <p><b>作者</b>：Gal Dalal,  Assaf Hallak,  Gugan Thoppe,  Shie Mannor,  Gal Chechik</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2209.13966</p>
  <p><b>关键词</b>：suffer from large, policy gradient methods, variance, policy, gradient methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite the popularity of policy gradient methods, they are known to suffer
from large variance and high sample complexity. To mitigate this, we introduce
SoftTreeMax -- a generalization of softmax that takes planning into account. In
SoftTreeMax, we extend the traditional logits with the multi-step discounted
cumulative reward, topped with the logits of future states. We consider two
variants of SoftTreeMax, one for cumulative reward and one for exponentiated
reward. For both, we analyze the gradient variance and reveal for the first
time the role of a tree expansion policy in mitigating this variance. We prove
that the resulting variance decays exponentially with the planning horizon as a
function of the expansion policy. Specifically, we show that the closer the
resulting state transitions are to uniform, the faster the decay. In a
practical implementation, we utilize a parallelized GPU-based simulator for
fast and efficient tree search. Our differentiable tree-based policy leverages
all gradients at the tree leaves in each environment step instead of the
traditional single-sample-based gradient. We then show in simulation how the
variance of the gradient is reduced by three orders of magnitude, leading to
better sample complexity compared to the standard policy gradient. On Atari,
SoftTreeMax demonstrates up to 5x better performance in a faster run time
compared to distributed PPO. Lastly, we demonstrate that high reward correlates
with lower variance.</p>
  </details>
</details>
<details>
  <summary>119. <b>标题：Simplex Random Features</b></summary>
  <p><b>编号</b>：[272]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13856</p>
  <p><b>作者</b>：Isaac Reid,  Krzysztof Choromanski,  Valerii Likhosherstov,  Adrian Weller</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Simplex Random Features, present Simplex Random, Orthogonal Random Features, softmax and Gaussian, Simplex Random</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present Simplex Random Features (SimRFs), a new random feature (RF)
mechanism for unbiased approximation of the softmax and Gaussian kernels by
geometrical correlation of random projection vectors. We prove that SimRFs
provide the smallest possible mean square error (MSE) on unbiased estimates of
these kernels among the class of weight-independent geometrically-coupled
positive random feature (PRF) mechanisms, substantially outperforming the
previously most accurate Orthogonal Random Features at no observable extra
cost. We present a more computationally expensive SimRFs+ variant, which we
prove is asymptotically optimal in the broader family of weight-dependent
geometrical coupling schemes (which permit correlations between random vector
directions and norms). In extensive empirical studies, we show consistent gains
provided by SimRFs in settings including pointwise kernel estimation,
nonparametric classification and scalable Transformers.</p>
  </details>
</details>
<details>
  <summary>120. <b>标题：Gaussian Noise is Nearly Instance Optimal for Private Unbiased Mean  Estimation</b></summary>
  <p><b>编号</b>：[273]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13850</p>
  <p><b>作者</b>：Aleksandar Nikolov,  Haohua Tang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：differential privacy, investigate unbiased high-dimensional, privacy, concentrated differential privacy, high-dimensional mean estimators</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We investigate unbiased high-dimensional mean estimators in differential
privacy. We consider differentially private mechanisms whose expected output
equals the mean of the input dataset, for every dataset drawn from a fixed
convex domain $K$ in $\mathbb{R}^d$. In the setting of concentrated
differential privacy, we show that, for every input such an unbiased mean
estimator introduces approximately at least as much error as a mechanism that
adds Gaussian noise with a carefully chosen covariance. This is true when the
error is measured with respect to $\ell_p$ error for any $p \ge 2$. We extend
this result to local differential privacy, and to approximate differential
privacy, but for the latter the error lower bound holds either for a dataset or
for a neighboring dataset. We also extend our results to mechanisms that take
i.i.d.~samples from a distribution over $K$ and are unbiased with respect to
the mean of the distribution.</p>
  </details>
</details>
<details>
  <summary>121. <b>标题：Improved Algorithms for Multi-period Multi-class Packing Problems  with~Bandit~Feedback</b></summary>
  <p><b>编号</b>：[276]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13791</p>
  <p><b>作者</b>：Wonyoung Kim,  Garud Iyengar,  Assaf Zeevi</p>
  <p><b>备注</b>：42 pages including Appendix</p>
  <p><b>关键词</b>：multi-period packing problem, multi-class multi-period packing, LMMP includes linear, total vector, contextual multi-class multi-period</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider the linear contextual multi-class multi-period packing
problem~(LMMP) where the goal is to pack items such that the total vector of
consumption is below a given budget vector and the total value is as large as
possible. We consider the setting where the reward and the consumption vector
associated with each action is a class-dependent linear function of the
context, and the decision-maker receives bandit feedback. LMMP includes linear
contextual bandits with knapsacks and online revenue management as special
cases. We establish a new more efficient estimator which guarantees a faster
convergence rate, and consequently, a lower regret in such problems. We propose
a bandit policy that is a closed-form function of said estimated parameters.
When the contexts are non-degenerate, the regret of the proposed policy is
sublinear in the context dimension, the number of classes, and the time
horizon~$T$ when the budget grows at least as $\sqrt{T}$. We also resolve an
open problem posed in Agrawal & Devanur (2016), and extend the result to a
multi-class setting. Our numerical experiments clearly demonstrate that the
performance of our policy is superior to other benchmarks in the literature.</p>
  </details>
</details>
<details>
  <summary>122. <b>标题：Deep learning-based lung segmentation and automatic regional template in  chest X-ray images for pediatric tuberculosis</b></summary>
  <p><b>编号</b>：[277]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13786</p>
  <p><b>作者</b>：Daniel Capellán-Martín,  Juan J. Gómez-Valverde,  Ramon Sanchez-Jacob,  David Bermejo-Peláez,  Lara García-Delgado,  Elisa López-Varela,  Maria J. Ledesma-Carbayo</p>
  <p><b>备注</b>：This work has been accepted at the SPIE Medical Imaging 2023, Image Processing conference</p>
  <p><b>关键词</b>：global child health, child health, considered a leading, substantial threat, threat to global</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Tuberculosis (TB) is still considered a leading cause of death and a
substantial threat to global child health. Both TB infection and disease are
curable using antibiotics. However, most children who die of TB are never
diagnosed or treated. In clinical practice, experienced physicians assess TB by
examining chest X-rays (CXR). Pediatric CXR has specific challenges compared to
adult CXR, which makes TB diagnosis in children more difficult. Computer-aided
diagnosis systems supported by Artificial Intelligence have shown performance
comparable to experienced radiologist TB readings, which could ease mass TB
screening and reduce clinical burden. We propose a multi-view deep
learning-based solution which, by following a proposed template, aims to
automatically regionalize and extract lung and mediastinal regions of interest
from pediatric CXR images where key TB findings may be present. Experimental
results have shown accurate region extraction, which can be used for further
analysis to confirm TB finding presence and severity assessment. Code publicly
available at this https URL.</p>
  </details>
</details>
<details>
  <summary>123. <b>标题：Differentially Private Distributed Bayesian Linear Regression with MCMC</b></summary>
  <p><b>编号</b>：[279]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13778</p>
  <p><b>作者</b>：Barış Alparslan,  Sinan Yıldırım,  Ş. İlker Birbil</p>
  <p><b>备注</b>：20 pages, 3 figures, code available at: this https URL</p>
  <p><b>关键词</b>：Bayesian inference framework, distributed differentially private, differentially private linear, inference framework, differentially private</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a novel Bayesian inference framework for distributed
differentially private linear regression. We consider a distributed setting
where multiple parties hold parts of the data and share certain summary
statistics of their portions in privacy-preserving noise. We develop a novel
generative statistical model for privately shared statistics, which exploits a
useful distributional relation between the summary statistics of linear
regression. Bayesian estimation of the regression coefficients is conducted
mainly using Markov chain Monte Carlo algorithms, while we also provide a fast
version to perform Bayesian estimation in one iteration. The proposed methods
have computational advantages over their competitors. We provide numerical
results on both real and simulated data, which demonstrate that the proposed
algorithms provide well-rounded estimation and prediction.</p>
  </details>
</details>
<details>
  <summary>124. <b>标题：Multi-fidelity covariance estimation in the log-Euclidean geometry</b></summary>
  <p><b>编号</b>：[280]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13749</p>
  <p><b>作者</b>：Aimee Maurais,  Terrence Alsup,  Benjamin Peherstorfer,  Youssef Marzouk</p>
  <p><b>备注</b>：27 pages, 10 figures, code supplement</p>
  <p><b>关键词</b>：symmetric positive-definite manifold, positive-definite manifold, introduce a multi-fidelity, matrices that employs, employs the log-Euclidean</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce a multi-fidelity estimator of covariance matrices that employs
the log-Euclidean geometry of the symmetric positive-definite manifold. The
estimator fuses samples from a hierarchy of data sources of differing
fidelities and costs for variance reduction while guaranteeing definiteness, in
contrast with previous approaches. The new estimator makes covariance
estimation tractable in applications where simulation or data collection is
expensive; to that end, we develop an optimal sample allocation scheme that
minimizes the mean-squared error of the estimator given a fixed budget.
Guaranteed definiteness is crucial to metric learning, data assimilation, and
other downstream tasks. Evaluations of our approach using data from physical
applications (heat conduction, fluid dynamics) demonstrate more accurate metric
learning and speedups of more than one order of magnitude compared to
benchmarks.</p>
  </details>
</details>
<details>
  <summary>125. <b>标题：A relaxed proximal gradient descent algorithm for convergent  plug-and-play with proximal denoiser</b></summary>
  <p><b>编号</b>：[281]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13731</p>
  <p><b>作者</b>：Samuel Hurault,  Antonin Chambolle,  Arthur Leclaire,  Nicolas Papadakis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：paper presents, Proximal Gradient Descent, PGD algorithm, algorithm, PnP</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a new convergent Plug-and-Play (PnP) algorithm. PnP
methods are efficient iterative algorithms for solving image inverse problems
formulated as the minimization of the sum of a data-fidelity term and a
regularization term. PnP methods perform regularization by plugging a
pre-trained denoiser in a proximal algorithm, such as Proximal Gradient Descent
(PGD). To ensure convergence of PnP schemes, many works study specific
parametrizations of deep denoisers. However, existing results require either
unverifiable or suboptimal hypotheses on the denoiser, or assume restrictive
conditions on the parameters of the inverse problem. Observing that these
limitations can be due to the proximal algorithm in use, we study a relaxed
version of the PGD algorithm for minimizing the sum of a convex function and a
weakly convex one. When plugged with a relaxed proximal denoiser, we show that
the proposed PnP-$\alpha$PGD algorithm converges for a wider range of
regularization parameters, thus allowing more accurate image restoration.</p>
  </details>
</details>
<details>
  <summary>126. <b>标题：Convolutional autoencoder for the spatiotemporal latent representation  of turbulence</b></summary>
  <p><b>编号</b>：[282]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13728</p>
  <p><b>作者</b>：Nguyen Anh Khoa Doan,  Alberto Racca,  Luca Magri</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：high-dimensional state space, challenging to predict, chaotic dynamics, make the phenomenon, phenomenon challenging</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Turbulence is characterised by chaotic dynamics and a high-dimensional state
space, which make the phenomenon challenging to predict. However, turbulent
flows are often characterised by coherent spatiotemporal structures, such as
vortices or large-scale modes, which can help obtain a latent description of
turbulent flows. However, current approaches are often limited by either the
need to use some form of thresholding on quantities defining the isosurfaces to
which the flow structures are associated or the linearity of traditional modal
flow decomposition approaches, such as those based on proper orthogonal
decomposition. This problem is exacerbated in flows that exhibit extreme
events, which are rare and sudden changes in a turbulent state. The goal of
this paper is to obtain an efficient and accurate reduced-order latent
representation of a turbulent flow that exhibits extreme events. Specifically,
we employ a three-dimensional multiscale convolutional autoencoder (CAE) to
obtain such latent representation. We apply it to a three-dimensional turbulent
flow. We show that the Multiscale CAE is efficient, requiring less than 10%
degrees of freedom than proper orthogonal decomposition for compressing the
data and is able to accurately reconstruct flow states related to extreme
events. The proposed deep learning architecture opens opportunities for
nonlinear reduced-order modeling of turbulent flows from data.</p>
  </details>
</details>
<details>
  <summary>127. <b>标题：The passive symmetries of machine learning</b></summary>
  <p><b>编号</b>：[283]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13724</p>
  <p><b>作者</b>：Soledad Villar (JHU),  David W. Hogg (NYU, MPIA, Flatiron),  Weichi Yao (NYU),  George A. Kevrekidis (JHU, LANL),  Bernhard Schölkopf (MPI-IS)</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：data involves arbitrary, involves arbitrary investigator, arbitrary investigator choices, passive symmetries, data involves</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Any representation of data involves arbitrary investigator choices. Because
those choices are external to the data-generating process, each choice leads to
an exact symmetry, corresponding to the group of transformations that takes one
possible representation to another. These are the passive symmetries; they
include coordinate freedom, gauge symmetry and units covariance, all of which
have led to important results in physics. Our goal is to understand the
implications of passive symmetries for machine learning: Which passive
symmetries play a role (e.g., permutation symmetry in graph neural networks)?
What are dos and don'ts in machine learning practice? We assay conditions under
which passive symmetries can be implemented as group equivariances. We also
discuss links to causal modeling, and argue that the implementation of passive
symmetries is particularly valuable when the goal of the learning problem is to
generalize out of sample. While this paper is purely conceptual, we believe
that it can have a significant impact on helping machine learning make the
transition that took place for modern physics in the first half of the
Twentieth century.</p>
  </details>
</details>
<details>
  <summary>128. <b>标题：On the Initialisation of Wide Low-Rank Feedforward Neural Networks</b></summary>
  <p><b>编号</b>：[285]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13710</p>
  <p><b>作者</b>：Thiziri Nait Saada,  Jared Tanner</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：wide randomly initialized, initialized low-rank feedforward, randomly initialized low-rank, dynamics of wide, low-rank feedforward networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The edge-of-chaos dynamics of wide randomly initialized low-rank feedforward
networks are analyzed. Formulae for the optimal weight and bias variances are
extended from the full-rank to low-rank setting and are shown to follow from
multiplicative scaling. The principle second order effect, the variance of the
input-output Jacobian, is derived and shown to increase as the rank to width
ratio decreases. These results inform practitioners how to randomly initialize
feedforward networks with a reduced number of learnable parameters while in the
same ambient dimension, allowing reductions in the computational cost and
memory constraints of the associated network.</p>
  </details>
</details>
<details>
  <summary>129. <b>标题：Improved distinct bone segmentation in upper-body CT through  multi-resolution networks</b></summary>
  <p><b>编号</b>：[286]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13674</p>
  <p><b>作者</b>：Eva Schnider,  Julia Wolleb,  Antal Huck,  Mireille Toranelli,  Georg Rauter,  Magdalena Müller-Gerbl,  Philippe C. Cattin</p>
  <p><b>备注</b>：Under submission</p>
  <p><b>关键词</b>：Automated distinct bone, Automated distinct, distinct bone segmentation, navigation workflows, bone segmentation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Purpose: Automated distinct bone segmentation from CT scans is widely used in
planning and navigation workflows. U-Net variants are known to provide
excellent results in supervised semantic segmentation. However, in distinct
bone segmentation from upper body CTs a large field of view and a
computationally taxing 3D architecture are required. This leads to
low-resolution results lacking detail or localisation errors due to missing
spatial context when using high-resolution inputs.
Methods: We propose to solve this problem by using end-to-end trainable
segmentation networks that combine several 3D U-Nets working at different
resolutions. Our approach, which extends and generalizes HookNet and MRN,
captures spatial information at a lower resolution and skips the encoded
information to the target network, which operates on smaller high-resolution
inputs. We evaluated our proposed architecture against single resolution
networks and performed an ablation study on information concatenation and the
number of context networks.
Results: Our proposed best network achieves a median DSC of 0.86 taken over
all 125 segmented bone classes and reduces the confusion among similar-looking
bones in different locations. These results outperform our previously published
3D U-Net baseline results on the task and distinct-bone segmentation results
reported by other groups.
Conclusion: The presented multi-resolution 3D U-Nets address current
shortcomings in bone segmentation from upper-body CT scans by allowing for
capturing a larger field of view while avoiding the cubic growth of the input
pixels and intermediate computations that quickly outgrow the computational
capacities in 3D. The approach thus improves the accuracy and efficiency of
distinct bone segmentation from upper-body CT.</p>
  </details>
</details>
<details>
  <summary>130. <b>标题：Reinforcement learning and decision making via single-photon quantum  walks</b></summary>
  <p><b>编号</b>：[287]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13669</p>
  <p><b>作者</b>：Fulvio Flamini,  Marius Krumm,  Lukas J. Fiderer,  Thomas Müller,  Hans J. Briegel</p>
  <p><b>备注</b>：10+6 pages, 6+5 figures, 2 tables. F. Flamini and M. Krumm contributed equally to this work</p>
  <p><b>关键词</b>：parametrized quantum circuits, classical neural networks, represent a promising, neural networks, networks are replaced</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Variational quantum algorithms represent a promising approach to quantum
machine learning where classical neural networks are replaced by parametrized
quantum circuits. Here, we present a variational approach to quantize
projective simulation (PS), a reinforcement learning model aimed at
interpretable artificial intelligence. Decision making in PS is modeled as a
random walk on a graph describing the agent's memory. To implement the
quantized model, we consider quantum walks of single photons in a lattice of
tunable Mach-Zehnder interferometers. We propose variational algorithms
tailored to reinforcement learning tasks, and we show, using an example from
transfer learning, that the quantized PS learning model can outperform its
classical counterpart. Finally, we discuss the role of quantum interference for
training and decision making, paving the way for realizations of interpretable
quantum learning agents.</p>
  </details>
</details>
<details>
  <summary>131. <b>标题：Population-wise Labeling of Sulcal Graphs using Multi-graph Matching</b></summary>
  <p><b>编号</b>：[295]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13532</p>
  <p><b>作者</b>：Rohit Yadav (AMU, INT, LIS),  François-Xavier Dupé (LIS, QARMA),  S. Takerkart (INT),  Guillaume Auzias (INT)</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：psychiatric disorders, identify biomarkers, biomarkers of neurological, neurological or psychiatric, multi-graph matching</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Population-wise matching of the cortical fold is necessary to identify
biomarkers of neurological or psychiatric disorders. The difficulty comes from
the massive interindividual variations in the morphology and spatial
organization of the folds. This task is challenging at both methodological and
conceptual levels. In the widely used registration-based techniques, these
variations are considered as noise and the matching of folds is only implicit.
Alternative approaches are based on the extraction and explicit identification
of the cortical folds. In particular, representing cortical folding patterns as
graphs of sulcal basins-termed sulcal graphs-enables to formalize the task as a
graph-matching problem. In this paper, we propose to address the problem of
sulcal graph matching directly at the population level using multi-graph
matching techniques. First, we motivate the relevance of multi-graph matching
framework in this context. We then introduce a procedure to generate
populations of artificial sulcal graphs, which allows us benchmarking several
state of the art multi-graph matching methods. Our results on both artificial
and real data demonstrate the effectiveness of multi-graph matching techniques
to obtain a population-wise consistent labeling of cortical folds at the sulcal
basins level.</p>
  </details>
</details>
<details>
  <summary>132. <b>标题：Quantum contextual bandits and recommender systems for quantum data</b></summary>
  <p><b>编号</b>：[296]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13524</p>
  <p><b>作者</b>：Shrigyan Brahmachari,  Josep Lumbreras,  Marco Tomamichel</p>
  <p><b>备注</b>：15 pages, 9 figures</p>
  <p><b>关键词</b>：contextual bandit framework, linear contextual bandit, bandit framework, recommender system, linear contextual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study a recommender system for quantum data using the linear contextual
bandit framework. In each round, a learner receives an observable (the context)
and has to recommend from a finite set of unknown quantum states (the actions)
which one to measure. The learner has the goal of maximizing the reward in each
round, that is the outcome of the measurement on the unknown state. Using this
model we formulate the low energy quantum state recommendation problem where
the context is a Hamiltonian and the goal is to recommend the state with the
lowest energy. For this task, we study two families of contexts: the Ising
model and a generalized cluster model. We observe that if we interpret the
actions as different phases of the models then the recommendation is done by
classifying the correct phase of the given Hamiltonian and the strategy can be
interpreted as an online quantum phase classifier.</p>
  </details>
</details>
<details>
  <summary>133. <b>标题：Robust Linear Regression: Gradient-descent, Early-stopping, and Beyond</b></summary>
  <p><b>编号</b>：[297]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13486</p>
  <p><b>作者</b>：Meyer Scetbon,  Elvis Dohmatob</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Euclidean-norm adversarial attacks, strategies on gradient-descent, methods for linear, linear regression, study the robustness</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work we study the robustness to adversarial attacks, of
early-stopping strategies on gradient-descent (GD) methods for linear
regression. More precisely, we show that early-stopped GD is optimally robust
(up to an absolute constant) against Euclidean-norm adversarial attacks.
However, we show that this strategy can be arbitrarily sub-optimal in the case
of general Mahalanobis attacks. This observation is compatible with recent
findings in the case of classification~\cite{Vardi2022GradientMP} that show
that GD provably converges to non-robust models. To alleviate this issue, we
propose to apply instead a GD scheme on a transformation of the data adapted to
the attack. This data transformation amounts to apply feature-depending
learning rates and we show that this modified GD is able to handle any
Mahalanobis attack, as well as more general attacks under some conditions.
Unfortunately, choosing such adapted transformations can be hard for general
attacks. To the rescue, we design a simple and tractable estimator whose
adversarial risk is optimal up to within a multiplicative constant of 1.1124 in
the population regime, and works for any norm.</p>
  </details>
</details>
<details>
  <summary>134. <b>标题：Towards Learned Emulation of Interannual Water Isotopologue Variations  in General Circulation Models</b></summary>
  <p><b>编号</b>：[298]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13462</p>
  <p><b>作者</b>：Jonathan Wider,  Jakob Kruse,  Nils Weitzel,  Janica C. Bühler,  Ullrich Köthe,  Kira Rehfeld</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：stable water isotopologues, simulating water isotopologues, varying climatic conditions, water isotopologues, isotopic composition</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Simulating abundances of stable water isotopologues, i.e. molecules differing
in their isotopic composition, within climate models allows for comparisons
with proxy data and, thus, for testing hypotheses about past climate and
validating climate models under varying climatic conditions. However, many
models are run without explicitly simulating water isotopologues. We
investigate the possibility to replace the explicit physics-based simulation of
oxygen isotopic composition in precipitation using machine learning methods.
These methods estimate isotopic composition at each time step for given fields
of surface temperature and precipitation amount. We implement convolutional
neural networks (CNNs) based on the successful UNet architecture and test
whether a spherical network architecture outperforms the naive approach of
treating Earth's latitude-longitude grid as a flat image. Conducting a case
study on a last millennium run with the iHadCM3 climate model, we find that
roughly 40\% of the temporal variance in the isotopic composition is explained
by the emulations on interannual and monthly timescale, with spatially varying
emulation quality. A modified version of the standard UNet architecture for
flat images yields results that are equally good as the predictions by the
spherical CNN. We test generalization to last millennium runs of other climate
models and find that while the tested deep learning methods yield the best
results on iHadCM3 data, the performance drops when predicting on other models
and is comparable to simple pixel-wise linear regression. An extended choice of
predictor variables and improving the robustness of learned climate--oxygen
isotope relationships should be explored in future work.</p>
  </details>
</details>
<details>
  <summary>135. <b>标题：Deep Learning for Reference-Free Geolocation for Poplar Trees</b></summary>
  <p><b>编号</b>：[303]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13387</p>
  <p><b>作者</b>：Cai W. John,  Owen Queen,  Wellington Muchero,  Scott J. Emrich</p>
  <p><b>备注</b>：Accepted at NeurIPS 2022 AI for Science Workshop</p>
  <p><b>关键词</b>：core task, identification of climatic, climatic and ecological, ecological conditions, Department of Energy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A core task in precision agriculture is the identification of climatic and
ecological conditions that are advantageous for a given crop. The most succinct
approach is geolocation, which is concerned with locating the native region of
a given sample based on its genetic makeup. Here, we investigate genomic
geolocation of Populus trichocarpa, or poplar, which has been identified by the
US Department of Energy as a fast-rotation biofuel crop to be harvested
nationwide. In particular, we approach geolocation from a reference-free
perspective, circumventing the need for compute-intensive processes such as
variant calling and alignment. Our model, MashNet, predicts latitude and
longitude for poplar trees from randomly-sampled, unaligned sequence fragments.
We show that our model performs comparably to Locator, a state-of-the-art
method based on aligned whole-genome sequence data. MashNet achieves an error
of 34.0 km^2 compared to Locator's 22.1 km^2. MashNet allows growers to quickly
and efficiently identify natural varieties that will be most productive in
their growth environment based on genotype. This paper explores geolocation for
precision agriculture while providing a framework and data source for further
development by the machine learning community.</p>
  </details>
</details>
<details>
  <summary>136. <b>标题：Demystifying Disagreement-on-the-Line in High Dimensions</b></summary>
  <p><b>编号</b>：[304]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13371</p>
  <p><b>作者</b>：Donghwan Lee,  Behrad Moniri,  Xinmeng Huang,  Edgar Dobriban,  Hamed Hassani</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：machine learning models, unlabeled data, labeled data, shift is challenging, machine learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Evaluating the performance of machine learning models under distribution
shift is challenging, especially when we only have unlabeled data from the
shifted (target) domain, along with labeled data from the original (source)
domain. Recent work suggests that the notion of disagreement, the degree to
which two models trained with different randomness differ on the same input, is
a key to tackle this problem. Experimentally, disagreement and prediction error
have been shown to be strongly connected, which has been used to estimate model
performance. Experiments have lead to the discovery of the
disagreement-on-the-line phenomenon, whereby the classification error under the
target domain is often a linear function of the classification error under the
source domain; and whenever this property holds, disagreement under the source
and target domain follow the same linear relation. In this work, we develop a
theoretical foundation for analyzing disagreement in high-dimensional random
features regression; and study under what conditions the
disagreement-on-the-line phenomenon occurs in our setting. Experiments on
CIFAR-10-C, Tiny ImageNet-C, and Camelyon17 are consistent with our theory and
support the universality of the theoretical findings.</p>
  </details>
</details>
<details>
  <summary>137. <b>标题：Misspecification-robust Sequential Neural Likelihood</b></summary>
  <p><b>编号</b>：[305]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13368</p>
  <p><b>作者</b>：Ryan P. Kelly,  David J. Nott,  David T. Frazier,  David J. Warne,  Chris Drovandi</p>
  <p><b>备注</b>：21 pages, 5 figures</p>
  <p><b>关键词</b>：Simulation-based inference, essential tool, parameter estimation, estimation of mechanistic, mechanistic and simulatable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Simulation-based inference (SBI) techniques are now an essential tool for the
parameter estimation of mechanistic and simulatable models with intractable
likelihoods. Statistical approaches to SBI such as approximate Bayesian
computation and Bayesian synthetic likelihood have been well studied in the
well specified and misspecified settings. However, most implementations are
inefficient in that many model simulations are wasted. Neural approaches such
as sequential neural likelihood (SNL) have been developed that exploit all
model simulations to build a surrogate of the likelihood function. However, SNL
approaches have been shown to perform poorly under model misspecification. In
this paper, we develop a new method for SNL that is robust to model
misspecification and can identify areas where the model is deficient. We
demonstrate the usefulness of the new approach on several illustrative
examples.</p>
  </details>
</details>
<details>
  <summary>138. <b>标题：A Reinforcement Learning Framework for Dynamic Mediation Analysis</b></summary>
  <p><b>编号</b>：[307]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13348</p>
  <p><b>作者</b>：Lin Ge,  Jitao Wang,  Chengchun Shi,  Zhenke Wu,  Rui Song</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：receives increasing attention, elucidate causal relations, transmitted via mediator, mediator variables, increasing attention</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Mediation analysis learns the causal effect transmitted via mediator
variables between treatments and outcomes and receives increasing attention in
various scientific domains to elucidate causal relations. Most existing works
focus on point-exposure studies where each subject only receives one treatment
at a single time point. However, there are a number of applications (e.g.,
mobile health) where the treatments are sequentially assigned over time and the
dynamic mediation effects are of primary interest. Proposing a reinforcement
learning (RL) framework, we are the first to evaluate dynamic mediation effects
in settings with infinite horizons. We decompose the average treatment effect
into an immediate direct effect, an immediate mediation effect, a delayed
direct effect, and a delayed mediation effect. Upon the identification of each
effect component, we further develop robust and semi-parametrically efficient
estimators under the RL framework to infer these causal effects. The superior
performance of the proposed method is demonstrated through extensive numerical
studies, theoretical results, and an analysis of a mobile health dataset.</p>
  </details>
</details>
<details>
  <summary>139. <b>标题：Single-Loop Switching Subgradient Methods for Non-Smooth Weakly Convex  Optimization with Non-Smooth Convex Constraints</b></summary>
  <p><b>编号</b>：[311]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13314</p>
  <p><b>作者</b>：Yankun Huang,  Qihang Lin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：general non-convex constrained, constrained optimization problem, non-convex constrained optimization, function is weakly, objective function</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we consider a general non-convex constrained optimization
problem, where the objective function is weakly convex and the constraint
function is convex while they can both be non-smooth. This class of problems
arises from many applications in machine learning such as fairness-aware
supervised learning. To solve this problem, we consider the classical switching
subgradient method by Polyak (1965), which is an intuitive and easily
implementable first-order method. Before this work, its iteration complexity
was only known for convex optimization. We prove its oracle complexity for
finding a nearly stationary point when the objective function is non-convex.
The analysis is derived separately when the constraint function is
deterministic and stochastic. Compared to existing methods, especially the
double-loop methods, the switching gradient method can be applied to non-smooth
problems and only has a single loop, which saves the effort on tuning the
number of inner iterations.</p>
  </details>
</details>
<details>
  <summary>140. <b>标题：Variational sparse inverse Cholesky approximation for latent Gaussian  processes via double Kullback-Leibler minimization</b></summary>
  <p><b>编号</b>：[312]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13303</p>
  <p><b>作者</b>：Jian Cao,  Myeongjong Kang,  Felix Jimenez,  Huiyan Sang,  Florian Schafer,  Matthias Katzfuss</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：latent Gaussian processes, sparse inverse Cholesky, Gaussian processes, inverse Cholesky, latent Gaussian</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To achieve scalable and accurate inference for latent Gaussian processes, we
propose a variational approximation based on a family of Gaussian distributions
whose covariance matrices have sparse inverse Cholesky (SIC) factors. We
combine this variational approximation of the posterior with a similar and
efficient SIC-restricted Kullback-Leibler-optimal approximation of the prior.
We then focus on a particular SIC ordering and nearest-neighbor-based sparsity
pattern resulting in highly accurate prior and posterior approximations. For
this setting, our variational approximation can be computed via stochastic
gradient descent in polylogarithmic time per iteration. We provide numerical
comparisons showing that the proposed double-Kullback-Leibler-optimal
Gaussian-process approximation (DKLGP) can sometimes be vastly more accurate
than alternative approaches such as inducing-point and mean-field
approximations at similar computational complexity.</p>
  </details>
</details>
<details>
  <summary>141. <b>标题：Structure Learning and Parameter Estimation for Graphical Models via  Penalized Maximum Likelihood Methods</b></summary>
  <p><b>编号</b>：[313]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13269</p>
  <p><b>作者</b>：Maryia Shpak (Maria Curie-Sklodowska University in Lublin)</p>
  <p><b>备注</b>：PhD Thesis. arXiv admin note: text overlap with arXiv:1207.1401, arXiv:1207.1402, arXiv:2002.00269, arXiv:1504.05006 by other authors</p>
  <p><b>关键词</b>：complex real-life phenomena, Probabilistic graphical models, Probabilistic graphical, provide a compact, real-life phenomena</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Probabilistic graphical models (PGMs) provide a compact and flexible
framework to model very complex real-life phenomena. They combine the
probability theory which deals with uncertainty and logical structure
represented by a graph which allows one to cope with the computational
complexity and also interpret and communicate the obtained knowledge. In the
thesis, we consider two different types of PGMs: Bayesian networks (BNs) which
are static, and continuous time Bayesian networks which, as the name suggests,
have a temporal component. We are interested in recovering their true
structure, which is the first step in learning any PGM. This is a challenging
task, which is interesting in itself from the causal point of view, for the
purposes of interpretation of the model and the decision-making process. All
approaches for structure learning in the thesis are united by the same idea of
maximum likelihood estimation with the LASSO penalty. The problem of structure
learning is reduced to the problem of finding non-zero coefficients in the
LASSO estimator for a generalized linear model. In the case of CTBNs, we
consider the problem both for complete and incomplete data. We support the
theoretical results with experiments.</p>
  </details>
</details>
<details>
  <summary>142. <b>标题：Temporal Consistency Loss for Physics-Informed Neural Networks</b></summary>
  <p><b>编号</b>：[314]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13262</p>
  <p><b>作者</b>：Sukirt Thakur,  Maziar Raissi,  Harsa Mitra,  Arezoo Ardekani</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Physics-informed neural networks, deep neural networks, solve partial differential, neural networks, partial differential equations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Physics-informed neural networks (PINNs) have been widely used to solve
partial differential equations in a forward and inverse manner using deep
neural networks. However, training these networks can be challenging for
multiscale problems. While statistical methods can be employed to scale the
regression loss on data, it is generally challenging to scale the loss terms
for equations. This paper proposes a method for scaling the mean squared loss
terms in the objective function used to train PINNs. Instead of using automatic
differentiation to calculate the temporal derivative, we use backward Euler
discretization. This provides us with a scaling term for the equations. In this
work, we consider the two and three-dimensional Navier-Stokes equations and
determine the kinematic viscosity using the spatio-temporal data on the
velocity and pressure fields. We first consider numerical datasets to test our
method. We test the sensitivity of our method to the time step size, the number
of timesteps, noise in the data, and spatial resolution. Finally, we use the
velocity field obtained using Particle Image Velocimetry (PIV) experiments to
generate a reference pressure field. We then test our framework using the
velocity and reference pressure field.</p>
  </details>
</details>
<h1>人工智能</h1>
<details>
  <summary>1. <b>标题：Reverse engineering adversarial attacks with fingerprints from  adversarial examples</b></summary>
  <p><b>编号</b>：[1]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13869</p>
  <p><b>作者</b>：David Aaron Nicholson (1),  Vincent Emanuele (1) ((1) Embedded Intelligence)</p>
  <p><b>备注</b>：8 pages, 6 figures</p>
  <p><b>关键词</b>：intense research efforts, produce incorrect outputs, confidently produce incorrect, networks remain vulnerable, research efforts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In spite of intense research efforts, deep neural networks remain vulnerable
to adversarial examples: an input that forces the network to confidently
produce incorrect outputs. Adversarial examples are typically generated by an
attack algorithm that optimizes a perturbation added to a benign input. Many
such algorithms have been developed. If it were possible to reverse engineer
attack algorithms from adversarial examples, this could deter bad actors
because of the possibility of attribution. Here we formulate reverse
engineering as a supervised learning problem where the goal is to assign an
adversarial example to a class that represents the algorithm and parameters
used. To our knowledge it has not been previously shown whether this is even
possible. We first test whether we can classify the perturbations added to
images by attacks on undefended single-label image classification models.
Taking a ``fight fire with fire'' approach, we leverage the sensitivity of deep
neural networks to adversarial examples, training them to classify these
perturbations. On a 17-class dataset (5 attacks, 4 bounded with 4 epsilon
values each), we achieve an accuracy of 99.4\% with a ResNet50 model trained on
the perturbations. We then ask whether we can perform this task without access
to the perturbations, obtaining an estimate of them with signal processing
algorithms, an approach we call ``fingerprinting''. We find the JPEG algorithm
serves as a simple yet effective fingerprinter (85.05\% accuracy), providing a
strong baseline for future work. We discuss how our approach can be extended to
attack agnostic, learnable fingerprints, and to open-world scenarios with
unknown attacks.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：PADL: Language-Directed Physics-Based Character Control</b></summary>
  <p><b>编号</b>：[2]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13868</p>
  <p><b>作者</b>：Jordan Juravsky,  Yunrong Guo,  Sanja Fidler,  Xue Bin Peng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：focus for computer, Natural language, life-like motions, language, language commands</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Developing systems that can synthesize natural and life-like motions for
simulated characters has long been a focus for computer animation. But in order
for these systems to be useful for downstream applications, they need not only
produce high-quality motions, but must also provide an accessible and versatile
interface through which users can direct a character's behaviors. Natural
language provides a simple-to-use and expressive medium for specifying a user's
intent. Recent breakthroughs in natural language processing (NLP) have
demonstrated effective use of language-based interfaces for applications such
as image generation and program synthesis. In this work, we present PADL, which
leverages recent innovations in NLP in order to take steps towards developing
language-directed controllers for physics-based character animation. PADL
allows users to issue natural language commands for specifying both high-level
tasks and low-level skills that a character should perform. We present an
adversarial imitation learning approach for training policies to map high-level
language commands to low-level controls that enable a character to perform the
desired task and skill specified by a user's commands. Furthermore, we propose
a multi-task aggregation method that leverages a language-based multiple-choice
question-answering approach to determine high-level task objectives from
language commands. We show that our framework can be applied to effectively
direct a simulated humanoid character to perform a diverse array of complex
motor skills.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Mathematical Capabilities of ChatGPT</b></summary>
  <p><b>编号</b>：[3]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13867</p>
  <p><b>作者</b>：Simon Frieder,  Luca Pinchetti,  Ryan-Rhys Griffiths,  Tommaso Salvatori,  Thomas Lukasiewicz,  Philipp Christian Petersen,  Alexis Chevalier,  Julius Berner</p>
  <p><b>备注</b>：The GHOSTS dataset will be available at this https URL</p>
  <p><b>关键词</b>：Lean Mathematical Library, mathematical, ChatGPT, mathematics, mathematical corpus</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We investigate the mathematical capabilities of ChatGPT by testing it on
publicly available datasets, as well as hand-crafted ones, and measuring its
performance against other models trained on a mathematical corpus, such as
Minerva. We also test whether ChatGPT can be a useful assistant to professional
mathematicians by emulating various use cases that come up in the daily
professional activities of mathematicians (question answering, theorem
searching). In contrast to formal mathematics, where large databases of formal
proofs are available (e.g., the Lean Mathematical Library), current datasets of
natural-language mathematics, used to benchmark language models, only cover
elementary mathematics. We address this issue by introducing a new dataset:
GHOSTS. It is the first natural-language dataset made and curated by working
researchers in mathematics that (1) aims to cover graduate-level mathematics
and (2) provides a holistic overview of the mathematical capabilities of
language models. We benchmark ChatGPT on GHOSTS and evaluate performance
against fine-grained criteria. We make this new dataset publicly available to
assist a community-driven comparison of ChatGPT with (future) large language
models in terms of advanced mathematical comprehension. We conclude that
contrary to many positive reports in the media (a potential case of selection
bias), ChatGPT's mathematical abilities are significantly below those of an
average mathematics graduate student. Our results show that ChatGPT often
understands the question but fails to provide correct solutions. Hence, if your
goal is to use it to pass a university exam, you would be better off copying
from your average peer!</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Learning in POMDPs is Sample-Efficient with Hindsight Observability</b></summary>
  <p><b>编号</b>：[7]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13857</p>
  <p><b>作者</b>：Jonathan N. Lee,  Alekh Agarwal,  Christoph Dann,  Tong Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：hardness results suggest, decision making problems, inherent partial observability, simple settings due, capture a broad</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>POMDPs capture a broad class of decision making problems, but hardness
results suggest that learning is intractable even in simple settings due to the
inherent partial observability. However, in many realistic problems, more
information is either revealed or can be computed during some point of the
learning process. Motivated by diverse applications ranging from robotics to
data center scheduling, we formulate a \setting (\setshort) as a POMDP where
the latent states are revealed to the learner in hindsight and only during
training. We introduce new algorithms for the tabular and function
approximation settings that are provably sample-efficient with hindsight
observability, even in POMDPs that would otherwise be statistically
intractable. We give a lower bound showing that the tabular algorithm is
optimal in its dependence on latent state and observation cardinalities.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Benchmarking Large Language Models for News Summarization</b></summary>
  <p><b>编号</b>：[10]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13848</p>
  <p><b>作者</b>：Tianyi Zhang,  Faisal Ladhak,  Esin Durmus,  Percy Liang,  Kathleen McKeown,  Tatsunori B. Hashimoto</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large language models, Large language, poorly understood, shown promise, promise for automatic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have shown promise for automatic summarization
but the reasons behind their successes are poorly understood. By conducting a
human evaluation on ten LLMs across different pretraining methods, prompts, and
model scales, we make two important observations. First, we find instruction
tuning, and not model size, is the key to the LLM's zero-shot summarization
capability. Second, existing studies have been limited by low-quality
references, leading to underestimates of human performance and lower few-shot
and finetuning performance. To better evaluate LLMs, we perform human
evaluation over high-quality summaries we collect from freelance writers.
Despite major stylistic differences such as the amount of paraphrasing, we find
that LMM summaries are judged to be on par with human written summaries.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Grounding Language Models to Images for Multimodal Generation</b></summary>
  <p><b>编号</b>：[20]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13823</p>
  <p><b>作者</b>：Jing Yu Koh,  Ruslan Salakhutdinov,  Daniel Fried</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：visual domain, language models, text-only language models, ground pretrained text-only, propose an efficient</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose an efficient method to ground pretrained text-only language models
to the visual domain, enabling them to process and generate arbitrarily
interleaved image-and-text data. Our method leverages the abilities of language
models learnt from large scale text-only pretraining, such as in-context
learning and free-form text generation. We keep the language model frozen, and
finetune input and output linear layers to enable cross-modality interactions.
This allows our model to process arbitrarily interleaved image-and-text inputs,
and generate free-form text interleaved with retrieved images. We achieve
strong zero-shot performance on grounded tasks such as contextual image
retrieval and multimodal dialogue, and showcase compelling interactive
abilities. Our approach works with any off-the-shelf language model and paves
the way towards an effective, general solution for leveraging pretrained
language models in visually grounded settings.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Explaining Large Language Model-Based Neural Semantic Parsers (Student  Abstract)</b></summary>
  <p><b>编号</b>：[22]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13820</p>
  <p><b>作者</b>：Daking Rai (1),  Yilun Zhou (2),  Bailin Wang (2),  Ziyu Yao (1) ((1) George Mason University, (2) Massachusetts Institute of Technology)</p>
  <p><b>备注</b>：2 pages, 5 figures, to be published in AAAI-23 Student Abstract and Poster Program</p>
  <p><b>关键词</b>：demonstrated strong capability, structured prediction tasks, large language models, large language, demonstrated strong</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While large language models (LLMs) have demonstrated strong capability in
structured prediction tasks such as semantic parsing, few amounts of research
have explored the underlying mechanisms of their success. Our work studies
different methods for explaining an LLM-based semantic parser and qualitatively
discusses the explained model behaviors, hoping to inspire future research
toward better understanding them.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Execution-based Code Generation using Deep Reinforcement Learning</b></summary>
  <p><b>编号</b>：[25]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13816</p>
  <p><b>作者</b>：Parshin Shojaee,  Aneesh Jain,  Sindhu Tipirneni,  Chandan K. Reddy</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：automating software engineering, software engineering processes, demonstrated considerable potential, large-scale code corpora, programming language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The utilization of programming language (PL) models, pretrained on
large-scale code corpora, as a means of automating software engineering
processes has demonstrated considerable potential in streamlining various code
generation tasks such as code completion, code translation, and program
synthesis. However, current approaches mainly rely on supervised fine-tuning
objectives borrowed from text generation, neglecting specific sequence-level
features of code, including but not limited to compilability as well as
syntactic and functional correctness. To address this limitation, we propose
PPOCoder, a new framework for code generation that combines pretrained PL
models with Proximal Policy Optimization (PPO) deep reinforcement learning and
employs execution feedback as the external source of knowledge into the model
optimization. PPOCoder is transferable across different code generation tasks
and PLs. Extensive experiments on three code generation tasks demonstrate the
effectiveness of our proposed approach compared to SOTA methods, improving the
success rate of compilation and functional correctness over different PLs. Our
code can be found at this https URL .</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Learning Roles with Emergent Social Value Orientations</b></summary>
  <p><b>编号</b>：[26]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13812</p>
  <p><b>作者</b>：Wenhao Li,  Xiangfeng Wang,  Bo Jin,  Jingyi Lu,  Hongyuan Zha</p>
  <p><b>备注</b>：40 pages, 24 figures</p>
  <p><b>关键词</b>：individual rationality leads, collective irrationality, considered situations, rationality leads, leads to collective</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Social dilemmas can be considered situations where individual rationality
leads to collective irrationality. The multi-agent reinforcement learning
community has leveraged ideas from social science, such as social value
orientations (SVO), to solve social dilemmas in complex cooperative tasks. In
this paper, by first introducing the typical "division of labor or roles"
mechanism in human society, we provide a promising solution for intertemporal
social dilemmas (ISD) with SVOs. A novel learning framework, called Learning
Roles with Emergent SVOs (RESVO), is proposed to transform the learning of
roles into the social value orientation emergence, which is symmetrically
solved by endowing agents with altruism to share rewards with other agents. An
SVO-based role embedding space is then constructed by individual conditioning
policies on roles with a novel rank regularizer and mutual information
maximizer. Experiments show that RESVO achieves a stable division of labor and
cooperation in ISDs with different complexity.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Fairness-aware Vision Transformer via Debiased Self-Attention</b></summary>
  <p><b>编号</b>：[31]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13803</p>
  <p><b>作者</b>：Yao Qiang,  Chengyin Li,  Prashant Khanduri,  Dongxiao Zhu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：solving computer vision, recently gained significant, gained significant interest, modeling long-range dependencies, Vision Transformer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision Transformer (ViT) has recently gained significant interest in solving
computer vision (CV) problems due to its capability of extracting informative
features and modeling long-range dependencies through the self-attention
mechanism. To fully realize the advantages of ViT in real-world applications,
recent works have explored the trustworthiness of ViT, including its robustness
and explainability. However, another desiderata, fairness has not yet been
adequately addressed in the literature. We establish that the existing
fairness-aware algorithms (primarily designed for CNNs) do not perform well on
ViT. This necessitates the need for developing our novel framework via Debiased
Self-Attention (DSA). DSA is a fairness-through-blindness approach that
enforces ViT to eliminate spurious features correlated with the sensitive
attributes for bias mitigation. Notably, adversarial examples are leveraged to
locate and mask the spurious features in the input image patches. In addition,
DSA utilizes an attention weights alignment regularizer in the training
objective to encourage learning informative features for target prediction.
Importantly, our DSA framework leads to improved fairness guarantees over prior
works on multiple prediction tasks without compromising target prediction
performance</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：FLAME: A small language model for spreadsheet formulas</b></summary>
  <p><b>编号</b>：[36]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13779</p>
  <p><b>作者</b>：Harshit Joshi,  Abishai Ebenezer,  José Cambronero,  Sumit Gulwani,  Aditya Kanade,  Vu Le,  Ivan Radiček,  Gust Verbruggen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：formula-authoring assistance, spreadsheet environments, unique opportunity, opportunity for formula-authoring, billions of users</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The widespread use of spreadsheet environments by billions of users presents
a unique opportunity for formula-authoring assistance. Although large language
models, such as Codex, can assist in general-purpose languages, they are
expensive to train and challenging to deploy due to their large model sizes (up
to billions of parameters). Moreover, they require hundreds of gigabytes of
training data. We present FLAME, a T5-based model trained on Excel formulas
that leverages domain insights to achieve competitive performance with a
substantially smaller model (60M parameters) and two orders of magnitude less
training data. We curate a training dataset using sketch deduplication,
introduce an Excel-specific formula tokenizer for our model, and use
domain-specific versions of masked span prediction and noisy auto-encoding as
pretraining objectives. We evaluate FLAME on formula repair, formula
auto-completion, and a novel task called syntax reconstruction. FLAME (60M) can
outperform much larger models, such as Codex-Davinci (175B), Codex-Cushman
(12B), and CodeT5 (220M), in 6 out of 10 settings.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Learning, Fast and Slow: A Goal-Directed Memory-Based Approach for  Dynamic Environments</b></summary>
  <p><b>编号</b>：[45]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13758</p>
  <p><b>作者</b>：Tan Chong Min John,  Mehul Motani</p>
  <p><b>备注</b>：22 pages</p>
  <p><b>关键词</b>：state prediction, neural network, state, prediction, goal-directed exploration</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Model-based next state prediction and state value prediction are slow to
converge. To address these challenges, we do the following: i) Instead of a
neural network, we do model-based planning using a parallel memory retrieval
system (which we term the slow mechanism); ii) Instead of learning state
values, we guide the agent's actions using goal-directed exploration, by using
a neural network to choose the next action given the current state and the goal
state (which we term the fast mechanism). The goal-directed exploration is
trained online using hippocampal replay of visited states and future imagined
states every single time step, leading to fast and efficient training.
Empirical studies show that our proposed method has a 92% solve rate across 100
episodes in a dynamically changing grid world, significantly outperforming
state-of-the-art actor critic mechanisms such as PPO (54%), TRPO (50%) and A2C
(24%). Ablation studies demonstrate that both mechanisms are crucial. We posit
that the future of Reinforcement Learning (RL) will be to model goals and
sub-goals for various tasks, and plan it out in a goal-directed memory-based
approach.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Toward Efficient Gradient-Based Value Estimation</b></summary>
  <p><b>编号</b>：[46]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13757</p>
  <p><b>作者</b>：Arsalan Sharifnassab,  Richard Sutton</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：favorable stability properties, Square Bellman Error, stability properties, estimation in reinforcement, favorable stability</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Gradient-based methods for value estimation in reinforcement learning have
favorable stability properties, but they are typically much slower than
Temporal Difference (TD) learning methods. We study the root causes of this
slowness and show that Mean Square Bellman Error (MSBE) is an ill-conditioned
loss function in the sense that its Hessian has large condition-number. To
resolve the adverse effect of poor conditioning of MSBE on gradient based
methods, we propose a low complexity batch-free proximal method that
approximately follows the Gauss-Newton direction and is asymptotically robust
to parameterization. Our main algorithm, called RANS, is efficient in the sense
that it is significantly faster than the residual gradient methods while having
almost the same computational complexity, and is competitive with TD on the
classic problems that we tested.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：PAC learning and stabilizing Hedonic Games: towards a unifying approach</b></summary>
  <p><b>编号</b>：[47]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13756</p>
  <p><b>作者</b>：Simone Fioravanti,  Michele Flammini,  Bojana Kodric,  Giovanna Varricchio</p>
  <p><b>备注</b>：Accepted paper at AAAI 2023</p>
  <p><b>关键词</b>：efficiently inferring preferences, study PAC learnability, efficiently inferring, partitions from samples, PAC stability</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study PAC learnability and PAC stabilizability of Hedonic Games (HGs),
i.e., efficiently inferring preferences or core-stable partitions from samples.
We first expand the known learnability/stabilizability landscape for some of
the most prominent HGs classes, providing results for Friends and Enemies
Games, Bottom Responsive, and Anonymous HGs. Then, having a broader view in
mind, we attempt to shed light on the structural properties leading to
learnability/stabilizability, or lack thereof, for specific HGs classes. Along
this path, we focus on the fully expressive Hedonic Coalition Nets
representation of HGs. We identify two sets of conditions that lead to
efficient learnability, and which encompass all of the known positive
learnability results. On the side of stability, we reveal that, while the
freedom of choosing an ad hoc adversarial distribution is the most obvious
hurdle to achieving PAC stability, it is not the only one. First, we show a
distribution independent necessary condition for PAC stability. Then, we focus
on $\W$-games, where players have individual preferences over other players and
evaluate coalitions based on the least preferred member. We prove that these
games are PAC stabilizable under the class of bounded distributions, which
assign positive probability mass to all coalitions. Finally, we discuss why
such a result is not easily extendable to other HGs classes even in this
promising scenario. Namely, we establish a purely computational property
necessary for achieving PAC stability.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Retrosynthetic Planning with Dual Value Networks</b></summary>
  <p><b>编号</b>：[48]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13755</p>
  <p><b>作者</b>：Guoqing Liu,  Di Xue,  Shufang Xie,  Yingce Xia,  Austin Tripp,  Krzysztof Maziarz,  Marwin Segler,  Tao Qin,  Zongzhang Zhang,  Tie-Yan Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：single-step accuracy, starting materials, single-step, synthesize a target, commercially available starting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Retrosynthesis, which aims to find a route to synthesize a target molecule
from commercially available starting materials, is a critical task in drug
discovery and materials design. Recently, the combination of ML-based
single-step reaction predictors with multi-step planners has led to promising
results. However, the single-step predictors are mostly trained offline to
optimize the single-step accuracy, without considering complete routes. Here,
we leverage reinforcement learning (RL) to improve the single-step predictor,
by using a tree-shaped MDP to optimize complete routes while retaining
single-step accuracy. Desirable routes should be both synthesizable and of low
cost. We propose an online training algorithm, called Planning with Dual Value
Networks (PDVN), in which two value networks predict the synthesizability and
cost of molecules, respectively. To maintain the single-step accuracy, we
design a two-branch network structure for the single-step predictor. On the
widely-used USPTO dataset, our PDVN algorithm improves the search success rate
of existing multi-step planners (e.g., increasing the success rate from 85.79%
to 98.95% for Retro*, and reducing the number of model calls by half while
solving 99.47% molecules for RetroGraph). Furthermore, PDVN finds shorter
synthesis routes (e.g., reducing the average route length from 5.76 to 4.83 for
Retro*, and from 5.63 to 4.78 for RetroGraph).</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Zero-shot-Learning Cross-Modality Data Translation Through Mutual  Information Guided Stochastic Diffusion</b></summary>
  <p><b>编号</b>：[51]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13743</p>
  <p><b>作者</b>：Zihao Wang,  Yingyu Yang,  Maxime Sermesant,  Hervé Delingette,  Ona Wu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：attracted great interest, Cross-modality data translation, data translation, data translation Model, Cross-modality data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cross-modality data translation has attracted great interest in image
computing. Deep generative models (\textit{e.g.}, GANs) show performance
improvement in tackling those problems. Nevertheless, as a fundamental
challenge in image translation, the problem of Zero-shot-Learning
Cross-Modality Data Translation with fidelity remains unanswered. This paper
proposes a new unsupervised zero-shot-learning method named Mutual Information
guided Diffusion cross-modality data translation Model (MIDiffusion), which
learns to translate the unseen source data to the target domain. The
MIDiffusion leverages a score-matching-based generative model, which learns the
prior knowledge in the target domain. We propose a differentiable
local-wise-MI-Layer ($LMI$) for conditioning the iterative denoising sampling.
The $LMI$ captures the identical cross-modality features in the statistical
domain for the diffusion guidance; thus, our method does not require retraining
when the source domain is changed, as it does not rely on any direct mapping
between the source and target domains. This advantage is critical for applying
cross-modality data translation methods in practice, as a reasonable amount of
source domain dataset is not always available for supervised training. We
empirically show the advanced performance of MIDiffusion in comparison with an
influential group of generative models, including adversarial-based and other
score-matching-based models.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Time Series Forecasting via Semi-Asymmetric Convolutional Architecture  with Global Atrous Sliding Window</b></summary>
  <p><b>编号</b>：[67]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13691</p>
  <p><b>作者</b>：Yuanpeng He</p>
  <p><b>备注</b>：13pages,8 figures</p>
  <p><b>关键词</b>：time series, time series forecasting, time, series, series forecasting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The proposed method in this paper is designed to address the problem of time
series forecasting. Although some exquisitely designed models achieve excellent
prediction performances, how to extract more useful information and make
accurate predictions is still an open issue. Most of modern models only focus
on a short range of information, which are fatal for problems such as time
series forecasting which needs to capture long-term information
characteristics. As a result, the main concern of this work is to further mine
relationship between local and global information contained in time series to
produce more precise predictions. In this paper, to satisfactorily realize the
purpose, we make three main contributions that are experimentally verified to
have performance advantages. Firstly, original time series is transformed into
difference sequence which serves as input to the proposed model. And secondly,
we introduce the global atrous sliding window into the forecasting model which
references the concept of fuzzy time series to associate relevant global
information with temporal data within a time period and utilizes
central-bidirectional atrous algorithm to capture underlying-related features
to ensure validity and consistency of captured data. Thirdly, a variation of
widely-used asymmetric convolution which is called semi-asymmetric convolution
is devised to more flexibly extract relationships in adjacent elements and
corresponding associated global features with adjustable ranges of convolution
on vertical and horizontal directions. The proposed model in this paper
achieves state-of-the-art on most of time series datasets provided compared
with competitive modern models.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：The Flan Collection: Designing Data and Methods for Effective  Instruction Tuning</b></summary>
  <p><b>编号</b>：[68]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13688</p>
  <p><b>作者</b>：Shayne Longpre,  Le Hou,  Tu Vu,  Albert Webson,  Hyung Won Chung,  Yi Tay,  Denny Zhou,  Quoc V. Le,  Barret Zoph,  Jason Wei,  Adam Roberts</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：design decisions, instruction tuning, instruction tuning methods, Flan, Flan Collection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the design decisions of publicly available instruction tuning
methods, and break down the development of Flan 2022 (Chung et al., 2022).
Through careful ablation studies on the Flan Collection of tasks and methods,
we tease apart the effect of design decisions which enable Flan-T5 to
outperform prior work by 3-17%+ across evaluation settings. We find task
balancing and enrichment techniques are overlooked but critical to effective
instruction tuning, and in particular, training with mixed prompt settings
(zero-shot, few-shot, and chain-of-thought) actually yields stronger (2%+)
performance in all settings. In further experiments, we show Flan-T5 requires
less finetuning to converge higher and faster than T5 on single downstream
tasks, motivating instruction-tuned models as more computationally-efficient
starting checkpoints for new tasks. Finally, to accelerate research on
instruction tuning, we make the Flan 2022 collection of datasets, templates,
and methods publicly available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：TopoBERT: Plug and Play Toponym Recognition Module Harnessing Fine-tuned  BERT</b></summary>
  <p><b>编号</b>：[89]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13631</p>
  <p><b>作者</b>：Bing Zhou,  Lei Zou,  Yingjie Hu,  Yi Qiang</p>
  <p><b>备注</b>：9 Pages, 6 figures</p>
  <p><b>关键词</b>：Extracting precise geographical, precise geographical information, Extracting precise, precise geographical, textual contents</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Extracting precise geographical information from textual contents is crucial
in a plethora of applications. For example, during hazardous events, a robust
and unbiased toponym extraction framework can provide an avenue to tie the
location concerned to the topic discussed by news media posts and pinpoint
humanitarian help requests or damage reports from social media. Early studies
have leveraged rule-based, gazetteer-based, deep learning, and hybrid
approaches to address this problem. However, the performance of existing tools
is deficient in supporting operations like emergency rescue, which relies on
fine-grained, accurate geographic information. The emerging pretrained language
models can better capture the underlying characteristics of text information,
including place names, offering a promising pathway to optimize toponym
recognition to underpin practical applications. In this paper, TopoBERT, a
toponym recognition module based on a one dimensional Convolutional Neural
Network (CNN1D) and Bidirectional Encoder Representation from Transformers
(BERT), is proposed and fine-tuned. Three datasets (CoNLL2003-Train,
Wikipedia3000, WNUT2017) are leveraged to tune the hyperparameters, discover
the best training strategy, and train the model. Another two datasets
(CoNLL2003-Test and Harvey2017) are used to evaluate the performance. Three
distinguished classifiers, linear, multi-layer perceptron, and CNN1D, are
benchmarked to determine the optimal model architecture. TopoBERT achieves
state-of-the-art performance (f1-score=0.865) compared to the other five
baseline models and can be applied to diverse toponym recognition tasks without
additional training.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Anti-Exploration by Random Network Distillation</b></summary>
  <p><b>编号</b>：[97]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13616</p>
  <p><b>作者</b>：Alexander Nikulin,  Vladislav Kurenkov,  Denis Tarasov,  Sergey Kolesnikov</p>
  <p><b>备注</b>：Source code: this https URL</p>
  <p><b>关键词</b>：Random Network Distillation, offline reinforcement learning, Network Distillation, Random Network, success of Random</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite the success of Random Network Distillation (RND) in various domains,
it was shown as not discriminative enough to be used as an uncertainty
estimator for penalizing out-of-distribution actions in offline reinforcement
learning. In this paper, we revisit these results and show that, with a naive
choice of conditioning for the RND prior, it becomes infeasible for the actor
to effectively minimize the anti-exploration bonus and discriminativity is not
an issue. We show that this limitation can be avoided with conditioning based
on Feature-wise Linear Modulation (FiLM), resulting in a simple and efficient
ensemble-free algorithm based on Soft Actor-Critic. We evaluate it on the D4RL
benchmark, showing that it is capable of achieving performance comparable to
ensemble-based methods and outperforming ensemble-free approaches by a wide
margin.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Policy Gradient for s-Rectangular Robust Markov Decision Processes</b></summary>
  <p><b>编号</b>：[106]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13589</p>
  <p><b>作者</b>：Navdeep Kumar,  Esther Derman,  Matthieu Geist,  Kfir Levy,  Shie Mannor</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Markov Decision Processes, robust Markov Decision, Decision Processes, Markov Decision, s-rectangular robust Markov</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a novel robust policy gradient method (RPG) for s-rectangular
robust Markov Decision Processes (MDPs). We are the first to derive the
adversarial kernel in a closed form and demonstrate that it is a one-rank
perturbation of the nominal kernel. This allows us to derive an RPG that is
similar to the one used in non-robust MDPs, except with a robust Q-value
function and an additional correction term. Both robust Q-values and correction
terms are efficiently computable, thus the time complexity of our method
matches that of non-robust MDPs, which is significantly faster compared to
existing black box methods.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Sport Task: Fine Grained Action Detection and Classification of Table  Tennis Strokes from Videos for MediaEval 2022</b></summary>
  <p><b>编号</b>：[111]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13576</p>
  <p><b>作者</b>：Pierre-Etienne Martin (MPI-EVA),  Jordan Calandre (MIA),  Boris Mansencal (LaBRI),  Jenny Benois-Pineau (LaBRI),  Renaud Péteri (MIA),  Laurent Mascarilla (MIA),  Julien Morlier</p>
  <p><b>备注</b>：MediaEval 2022 Workshop, Jan 2023, Bergen, Norway. arXiv admin note: substantial text overlap with arXiv:2112.11384</p>
  <p><b>关键词</b>：widespread research topic, research topic, videos, Sports video analysis, widespread research</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sports video analysis is a widespread research topic. Its applications are
very diverse, like events detection during a match, video summary, or
fine-grained movement analysis of athletes. As part of the MediaEval 2022
benchmarking initiative, this task aims at detecting and classifying subtle
movements from sport videos. We focus on recordings of table tennis matches.
Conducted since 2019, this task provides a classification challenge from
untrimmed videos recorded under natural conditions with known temporal
boundaries for each stroke. Since 2021, the task also provides a stroke
detection challenge from unannotated, untrimmed videos. This year, the
training, validation, and test sets are enhanced to ensure that all strokes are
represented in each dataset. The dataset is now similar to the one used in [1,
2]. This research is intended to build tools for coaches and athletes who want
to further evaluate their sport performances.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Purposeful and Operation-based Cognitive System for AGI</b></summary>
  <p><b>编号</b>：[117]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13556</p>
  <p><b>作者</b>：Shimon Komarovsky</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：paper proposes, main component, model, AGI agent, AGI</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper proposes a new cognitive model, acting as the main component of an
AGI agent. The model is introduced in its mature state, and as an extension of
previous models, DENN, and especially AKREM, by including operational models
(frames/classes) and will. In addition, it is mainly based on the duality
principle in every known intelligent aspect, such as exhibiting both top-down
and bottom-up model learning, generalization verse specialization, and more.
Furthermore, a holistic approach is advocated for AGI designing and cognition
under constraints or efficiency is proposed, in the form of reusability and
simplicity. Finally, reaching this mature state is described via a cognitive
evolution from infancy to adulthood, utilizing a consolidation principle. The
final product of this cognitive model is a dynamic operational memory of models
and instances.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Real-Time Outlier Detection with Dynamic Process Limits</b></summary>
  <p><b>编号</b>：[133]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13527</p>
  <p><b>作者</b>：Marek Wadinger,  Michal Kvasnica</p>
  <p><b>备注</b>：7 pages, 4 figures, 24th International Conference on Process Control</p>
  <p><b>关键词</b>：Anomaly detection methods, Anomaly detection, Online anomaly detection, environmental aspects, systems where rare</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Anomaly detection methods are part of the systems where rare events may
endanger an operation's profitability, safety, and environmental aspects.
Although many state-of-the-art anomaly detection methods were developed to
date, their deployment is limited to the operation conditions present during
the model training. Online anomaly detection brings the capability to adapt to
data drifts and change points that may not be represented during model
development resulting in prolonged service life. This paper proposes an online
anomaly detection algorithm for existing real-time infrastructures where
low-latency detection is required and novel patterns in data occur
unpredictably. The online inverse cumulative distribution-based approach is
introduced to eliminate common problems of offline anomaly detectors, meanwhile
providing dynamic process limits to normal operation. The benefit of the
proposed method is the ease of use, fast computation, and deployability as
shown in two case studies of real microgrid operation data.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Fourier Sensitivity and Regularization of Computer Vision Models</b></summary>
  <p><b>编号</b>：[137]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13514</p>
  <p><b>作者</b>：Kiran Krishnamachari,  See-Kiong Ng,  Chuan-Sheng Foo</p>
  <p><b>备注</b>：Published in TMLR, this https URL</p>
  <p><b>关键词</b>：deep neural networks, Recent work, work has empirically, empirically shown, deep neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent work has empirically shown that deep neural networks latch on to the
Fourier statistics of training data and show increased sensitivity to
Fourier-basis directions in the input. Understanding and modifying this
Fourier-sensitivity of computer vision models may help improve their
robustness. Hence, in this paper we study the frequency sensitivity
characteristics of deep neural networks using a principled approach. We first
propose a basis trick, proving that unitary transformations of the
input-gradient of a function can be used to compute its gradient in the basis
induced by the transformation. Using this result, we propose a general measure
of any differentiable model's Fourier-sensitivity using the unitary
Fourier-transform of its input-gradient. When applied to deep neural networks,
we find that computer vision models are consistently sensitive to particular
frequencies dependent on the dataset, training method and architecture. Based
on this measure, we further propose a Fourier-regularization framework to
modify the Fourier-sensitivities and frequency bias of models. Using our
proposed regularizer-family, we demonstrate that deep neural networks obtain
improved classification accuracy on robustness evaluations.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Monocular Scene Reconstruction with 3D SDF Transformers</b></summary>
  <p><b>编号</b>：[141]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13510</p>
  <p><b>作者</b>：Weihao Yuan,  Xiaodong Gu,  Heng Li,  Zilong Dong,  Siyu Zhu</p>
  <p><b>备注</b>：Accepted to ICLR 2023</p>
  <p><b>关键词</b>：Monocular scene reconstruction, Monocular scene, posed images, images is challenging, challenging due</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Monocular scene reconstruction from posed images is challenging due to the
complexity of a large environment. Recent volumetric methods learn to directly
predict the TSDF volume and have demonstrated promising results in this task.
However, most methods focus on how to extract and fuse the 2D features to a 3D
feature volume, but none of them improve the way how the 3D volume is
aggregated. In this work, we propose an SDF transformer network, which replaces
the role of 3D CNN for better 3D feature aggregation. To reduce the explosive
computation complexity of the 3D multi-head attention, we propose a sparse
window attention module, where the attention is only calculated between the
non-empty voxels within a local window. Then a top-down-bottom-up 3D attention
network is built for 3D feature aggregation, where a dilate-attention structure
is proposed to prevent geometry degeneration, and two global modules are
employed to equip with global receptive fields. The experiments on multiple
datasets show that this 3D transformer network generates a more accurate and
complete reconstruction, which outperforms previous methods by a large margin.
Remarkably, the mesh accuracy is improved by 41.8%, and the mesh completeness
is improved by 25.3% on the ScanNet dataset. Project page:
this https URL.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Transfer Learning and Class Decomposition for Detecting the Cognitive  Decline of Alzheimer Disease</b></summary>
  <p><b>编号</b>：[144]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13504</p>
  <p><b>作者</b>：Maha M. Alwuthaynani,  Zahraa S. Abdallah,  Raul Santos-Rodriguez</p>
  <p><b>备注</b>：12 pages, 3 figures</p>
  <p><b>关键词</b>：Early diagnosis, essential in preventing, Alzheimer disease, Alzheimer, disease progression</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Early diagnosis of Alzheimer's disease (AD) is essential in preventing the
disease's progression. Therefore, detecting AD from neuroimaging data such as
structural magnetic resonance imaging (sMRI) has been a topic of intense
investigation in recent years. Deep learning has gained considerable attention
in Alzheimer's detection. However, training a convolutional neural network from
scratch is challenging since it demands more computational time and a
significant amount of annotated data. By transferring knowledge learned from
other image recognition tasks to medical image classification, transfer
learning can provide a promising and effective solution. Irregularities in the
dataset distribution present another difficulty. Class decomposition can tackle
this issue by simplifying learning a dataset's class boundaries. Motivated by
these approaches, this paper proposes a transfer learning method using class
decomposition to detect Alzheimer's disease from sMRI images. We use two
ImageNet-trained architectures: VGG19 and ResNet50, and an entropy-based
technique to determine the most informative images. The proposed model achieved
state-of-the-art performance in the Alzheimer's disease (AD) vs mild cognitive
impairment (MCI) vs cognitively normal (CN) classification task with a 3\%
increase in accuracy from what is reported in the literature.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Adversarial Training of Self-supervised Monocular Depth Estimation  against Physical-World Attacks</b></summary>
  <p><b>编号</b>：[149]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13487</p>
  <p><b>作者</b>：Zhiyuan Cheng,  James Liang,  Guanhong Tao,  Dongfang Liu,  Xiangyu Zhang</p>
  <p><b>备注</b>：Accepted to ICLR2023 (Spotlight)</p>
  <p><b>关键词</b>：Monocular Depth Estimation, Depth Estimation, MDE, autonomous driving, critical component</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Monocular Depth Estimation (MDE) is a critical component in applications such
as autonomous driving. There are various attacks against MDE networks. These
attacks, especially the physical ones, pose a great threat to the security of
such systems. Traditional adversarial training method requires ground-truth
labels hence cannot be directly applied to self-supervised MDE that does not
have ground-truth depth. Some self-supervised model hardening techniques (e.g.,
contrastive learning) ignore the domain knowledge of MDE and can hardly achieve
optimal performance. In this work, we propose a novel adversarial training
method for self-supervised MDE models based on view synthesis without using
ground-truth depth. We improve adversarial robustness against physical-world
attacks using L0-norm-bounded perturbation in training. We compare our method
with supervised learning based and contrastive learning based methods that are
tailored for MDE. Results on two representative MDE networks show that we
achieve better robustness against various adversarial attacks with nearly no
benign performance degradation.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：CRC-RL: A Novel Visual Feature Representation Architecture for  Unsupervised Reinforcement Learning</b></summary>
  <p><b>编号</b>：[153]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13473</p>
  <p><b>作者</b>：Darshita Jain,  Anima Majumder,  Samrat Dutta,  Swagat Kumar</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：paper addresses, aim to improve, improve the performance, visual feature representation, called CRC loss</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper addresses the problem of visual feature representation learning
with an aim to improve the performance of end-to-end reinforcement learning
(RL) models. Specifically, a novel architecture is proposed that uses a
heterogeneous loss function, called CRC loss, to learn improved visual features
which can then be used for policy learning in RL. The CRC-loss function is a
combination of three individual loss functions, namely, contrastive,
reconstruction and consistency loss. The feature representation is learned in
parallel to the policy learning while sharing the weight updates through a
Siamese Twin encoder model. This encoder model is augmented with a decoder
network and a feature projection network to facilitate computation of the above
loss components. Through empirical analysis involving latent feature
visualization, an attempt is made to provide an insight into the role played by
this loss function in learning new action-dependent features and how they are
linked to the complexity of the problems being solved. The proposed
architecture, called CRC-RL, is shown to outperform the existing
state-of-the-art methods on the challenging Deep mind control suite
environments by a significant margin thereby creating a new benchmark in this
field.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Compliance Costs of AI Technology Commercialization: A Field Deployment  Perspective</b></summary>
  <p><b>编号</b>：[159]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13454</p>
  <p><b>作者</b>：Weiyue Wu,  Shaoshan Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Artificial Intelligence, huge financial burden, development budgets, progressing fast, constrained on research</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While Artificial Intelligence (AI) technologies are progressing fast,
compliance costs have become a huge financial burden for AI startups, which are
already constrained on research & development budgets. This situation creates a
compliance trap, as many AI startups are not financially prepared to cope with
a broad spectrum of regulatory requirements. Particularly, the complex and
varying regulatory processes across the globe subtly give advantages to
well-established and resourceful technology firms over resource-constrained AI
startups [1]. The continuation of this trend may phase out the majority of AI
startups and lead to giant technology firms' monopolies of AI technologies. To
demonstrate the reality of the compliance trap, from a field deployment
perspective, we delve into the details of compliance costs of AI commercial
operations.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：A Data-Driven Modeling and Control Framework for Physics-Based Building  Emulators</b></summary>
  <p><b>编号</b>：[163]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13447</p>
  <p><b>作者</b>：Chihyeon Song,  Aayushman Sharma,  Raman Goyal,  Alejandro Brito,  Saman Mostafavi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：present a data-driven, physics-based building emulators, HVAC MPC problems, building, HVAC MPC</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a data-driven modeling and control framework for physics-based
building emulators. Our approach comprises: (a) Offline training of
differentiable surrogate models that speed up model evaluations, provide cheap
gradients, and have good predictive accuracy for the receding horizon in Model
Predictive Control (MPC) and (b) Formulating and solving nonlinear building
HVAC MPC problems. We extensively verify the modeling and control performance
using multiple surrogate models and optimization frameworks for different
available test cases in the Building Optimization Testing Framework (BOPTEST).
The framework is compatible with other modeling techniques and customizable
with different control formulations. The modularity makes the approach
future-proof for test cases currently in development for physics-based building
emulators and provides a path toward prototyping predictive controllers in
large buildings.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：A Survey of Explainable AI in Deep Visual Modeling: Methods and Metrics</b></summary>
  <p><b>编号</b>：[165]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13445</p>
  <p><b>作者</b>：Naveed Akhtar</p>
  <p><b>备注</b>：Short accessible survey (9pgs)</p>
  <p><b>关键词</b>：Deep visual models, high-stake domains, widespread applications, applications in high-stake, Deep visual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep visual models have widespread applications in high-stake domains. Hence,
their black-box nature is currently attracting a large interest of the research
community. We present the first survey in Explainable AI that focuses on the
methods and metrics for interpreting deep visual models. Covering the landmark
contributions along the state-of-the-art, we not only provide a taxonomic
organization of the existing techniques, but also excavate a range of
evaluation metrics and collate them as measures of different properties of
model explanations. Along the insightful discussion on the current trends, we
also discuss the challenges and future avenues for this research direction.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Retiring $Δ$DP: New Distribution-Level Metrics for Demographic  Parity</b></summary>
  <p><b>编号</b>：[167]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13443</p>
  <p><b>作者</b>：Xiaotian Han,  Zhimeng Jiang,  Hongye Jin,  Zirui Liu,  Na Zou,  Qifan Wang,  Xia Hu</p>
  <p><b>备注</b>：Under review</p>
  <p><b>关键词</b>：textsf, Demographic parity, ensures equal treatment, Demographic, widely recognized measure</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Demographic parity is the most widely recognized measure of group fairness in
machine learning, which ensures equal treatment of different demographic
groups. Numerous works aim to achieve demographic parity by pursuing the
commonly used metric $\Delta DP$. Unfortunately, in this paper, we reveal that
the fairness metric $\Delta DP$ can not precisely measure the violation of
demographic parity, because it inherently has the following drawbacks:
\textit{i)} zero-value $\Delta DP$ does not guarantee zero violation of
demographic parity, \textit{ii)} $\Delta DP$ values can vary with different
classification thresholds. To this end, we propose two new fairness metrics,
\textsf{A}rea \textsf{B}etween \textsf{P}robability density function
\textsf{C}urves (\textsf{ABPC}) and \textsf{A}rea \textsf{B}etween
\textsf{C}umulative density function \textsf{C}urves (\textsf{ABCC}), to
precisely measure the violation of demographic parity in distribution level.
The new fairness metrics directly measure the difference between the
distributions of the prediction probability for different demographic groups.
Thus our proposed new metrics enjoy: \textit{i)} zero-value
\textsf{ABCC}/\textsf{ABPC} guarantees zero violation of demographic parity;
\textit{ii)} \textsf{ABCC}/\textsf{ABPC} guarantees demographic parity while
the classification threshold adjusted. We further re-evaluate the existing fair
models with our proposed fairness metrics and observe different fairness
behaviors of those models under the new metrics.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Scaling laws for single-agent reinforcement learning</b></summary>
  <p><b>编号</b>：[168]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13442</p>
  <p><b>作者</b>：Jacob Hilton,  Jie Tang,  John Schulman</p>
  <p><b>备注</b>：33 pages</p>
  <p><b>关键词</b>：cross-entropy loss improves, loss improves smoothly, constant scaling law, Recent work, cross-entropy loss</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent work has shown that, in generative modeling, cross-entropy loss
improves smoothly with model size and training compute, following a power law
plus constant scaling law. One challenge in extending these results to
reinforcement learning is that the main performance objective of interest, mean
episode return, need not vary smoothly. To overcome this, we introduce
*intrinsic performance*, a monotonic function of the return defined as the
minimum compute required to achieve the given return across a family of models
of different sizes. We find that, across a range of environments, intrinsic
performance scales as a power law in model size and environment interactions.
Consequently, as in generative modeling, the optimal model size scales as a
power law in the training compute budget. Furthermore, we study how this
relationship varies with the environment and with other properties of the
training setup. In particular, using a toy MNIST-based environment, we show
that varying the "horizon length" of the task mostly changes the coefficient
but not the exponent of this relationship.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Anomaly Segmentation for High-Resolution Remote Sensing Images Based on  Pixel Descriptors</b></summary>
  <p><b>编号</b>：[175]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13422</p>
  <p><b>作者</b>：Jingtao Li,  Xinyu Wang,  Hengwei Zhao,  Shaoyu Wang,  Yanfei Zhong</p>
  <p><b>备注</b>：to be published in AAAI2023</p>
  <p><b>关键词</b>：high spatial resolution, segmenting anomaly patterns, remote sensing imagery, Anomaly segmentation, Earth vision applications</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Anomaly segmentation in high spatial resolution (HSR) remote sensing imagery
is aimed at segmenting anomaly patterns of the earth deviating from normal
patterns, which plays an important role in various Earth vision applications.
However, it is a challenging task due to the complex distribution and the
irregular shapes of objects, and the lack of abnormal samples. To tackle these
problems, an anomaly segmentation model based on pixel descriptors (ASD) is
proposed for anomaly segmentation in HSR imagery. Specifically, deep one-class
classification is introduced for anomaly segmentation in the feature space with
discriminative pixel descriptors. The ASD model incorporates the data argument
for generating virtual ab-normal samples, which can force the pixel descriptors
to be compact for normal data and meanwhile to be diverse to avoid the model
collapse problems when only positive samples participated in the training. In
addition, the ASD introduced a multi-level and multi-scale feature extraction
strategy for learning the low-level and semantic information to make the pixel
descriptors feature-rich. The proposed ASD model was validated using four HSR
datasets and compared with the recent state-of-the-art models, showing its
potential value in Earth vision applications.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Superhuman Fairness</b></summary>
  <p><b>编号</b>：[177]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13420</p>
  <p><b>作者</b>：Omid Memarrast,  Linh Vu,  Brian Ziebart</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：increasingly important focus, increasingly important, important focus, design of supervised, machine learning methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The fairness of machine learning-based decisions has become an increasingly
important focus in the design of supervised machine learning methods. Most
fairness approaches optimize a specified trade-off between performance
measure(s) (e.g., accuracy, log loss, or AUC) and fairness metric(s) (e.g.,
demographic parity, equalized odds). This begs the question: are the right
performance-fairness trade-offs being specified? We instead re-cast fair
machine learning as an imitation learning task by introducing superhuman
fairness, which seeks to simultaneously outperform human decisions on multiple
predictive performance and fairness measures. We demonstrate the benefits of
this approach given suboptimal decisions.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete  Annotations</b></summary>
  <p><b>编号</b>：[179]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13418</p>
  <p><b>作者</b>：Yuanhong Chen,  Yuyuan Liu,  Chong Wang,  Michael Elliott,  Chun Fung Kwok,  Carlos Pe na-Solorzano,  Yu Tian,  Fengbei Liu,  Helen Frazer,  Davis J. McCarthy,  Gustavo Carneiro</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：screening mammogram datasets, real-world screening mammogram, fully annotated datasets, images are labelled, weakly annotated subset</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Methods to detect malignant lesions from screening mammograms are usually
trained with fully annotated datasets, where images are labelled with the
localisation and classification of cancerous lesions. However, real-world
screening mammogram datasets commonly have a subset that is fully annotated and
another subset that is weakly annotated with just the global classification
(i.e., without lesion localisation). Given the large size of such datasets,
researchers usually face a dilemma with the weakly annotated subset: to not use
it or to fully annotate it. The first option will reduce detection accuracy
because it does not use the whole dataset, and the second option is too
expensive given that the annotation needs to be done by expert radiologists. In
this paper, we propose a middle-ground solution for the dilemma, which is to
formulate the training as a weakly- and semi-supervised learning problem that
we refer to as malignant breast lesion detection with incomplete annotations.
To address this problem, our new method comprises two stages, namely: 1)
pre-training a multi-view mammogram classifier with weak supervision from the
whole dataset, and 2) extending the trained classifier to become a multi-view
detector that is trained with semi-supervised student-teacher learning, where
the training set contains fully and weakly-annotated mammograms. We provide
extensive detection results on two real-world screening mammogram datasets
containing incomplete annotations, and show that our proposed approach achieves
state-of-the-art results in the detection of malignant breast lesions with
incomplete annotations.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：LogAI: A Library for Log Analytics and Intelligence</b></summary>
  <p><b>编号</b>：[181]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13415</p>
  <p><b>作者</b>：Qian Cheng,  Amrita Saha,  Wenzhuo Yang,  Chenghao Liu,  Doyen Sahoo,  Steven Hoi</p>
  <p><b>备注</b>：17 pages, 7 figures, technical report for open source code, paper release with code</p>
  <p><b>关键词</b>：record runtime information, logs record runtime, System logs record, record runtime, runtime information</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Software and System logs record runtime information about processes executing
within a system. These logs have become the most critical and ubiquitous forms
of observability data that help developers understand system behavior, monitor
system health and resolve issues. However, the volume of logs generated can be
humongous (of the order of petabytes per day) especially for complex
distributed systems, such as cloud, search engine, social media, etc. This has
propelled a lot of research on developing AI-based log based analytics and
intelligence solutions that can process huge volume of raw logs and generate
insights. In order to enable users to perform multiple types of AI-based log
analysis tasks in a uniform manner, we introduce LogAI
(this https URL), a one-stop open source library for log
analytics and intelligence. LogAI supports tasks such as log summarization, log
clustering and log anomaly detection. It adopts the OpenTelemetry data model,
to enable compatibility with different log management platforms. LogAI provides
a unified model interface and provides popular time-series, statistical
learning and deep learning models. Alongside this, LogAI also provides an
out-of-the-box GUI for users to conduct interactive analysis. With LogAI, we
can also easily benchmark popular deep learning algorithms for log anomaly
detection without putting in redundant effort to process the logs. We have
opensourced LogAI to cater to a wide range of applications benefiting both
academic research and industrial prototyping.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：A Modular Multi-stage Lightweight Graph Transformer Network for Human  Pose and Shape Estimation from 2D Human Pose</b></summary>
  <p><b>编号</b>：[186]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13403</p>
  <p><b>作者</b>：Ayman Ali,  Ekkasit Pinyoanuntapong,  Pu Wang,  Mohsen Dorodchi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：existing deep learning-based, deep learning-based human, challenge faced, faced by existing, existing deep</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this research, we address the challenge faced by existing deep
learning-based human mesh reconstruction methods in balancing accuracy and
computational efficiency. These methods typically prioritize accuracy,
resulting in large network sizes and excessive computational complexity, which
may hinder their practical application in real-world scenarios, such as virtual
reality systems. To address this issue, we introduce a modular multi-stage
lightweight graph-based transformer network for human pose and shape estimation
from 2D human pose, a pose-based human mesh reconstruction approach that
prioritizes computational efficiency without sacrificing reconstruction
accuracy. Our method consists of a 2D-to-3D lifter module that utilizes graph
transformers to analyze structured and implicit joint correlations in 2D human
poses, and a mesh regression module that combines the extracted pose features
with a mesh template to produce the final human mesh parameters.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Probably Anytime-Safe Stochastic Combinatorial Semi-Bandits</b></summary>
  <p><b>编号</b>：[191]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13393</p>
  <p><b>作者</b>：Yunlong Hou,  Vincent Y. F. Tan,  Zixin Zhong</p>
  <p><b>备注</b>：56 pages, 5 figures</p>
  <p><b>关键词</b>：making online decisions, stochastic combinatorial semi-bandits, combinatorial semi-bandits problem, incur undue amount, anytime-safe stochastic combinatorial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Motivated by concerns about making online decisions that incur undue amount
of risk at each time step, in this paper, we formulate the probably
anytime-safe stochastic combinatorial semi-bandits problem. In this problem,
the agent is given the option to select a subset of size at most $K$ from a set
of $L$ ground items. Each item is associated to a certain mean reward as well
as a variance that represents its risk. To mitigate the risk that the agent
incurs, we require that with probability at least $1-\delta$, over the entire
horizon of time $T$, each of the choices that the agent makes should contain
items whose sum of variances does not exceed a certain variance budget. We call
this probably anytime-safe constraint. Under this constraint, we design and
analyze an algorithm {\sc PASCombUCB} that minimizes the regret over the
horizon of time $T$. By developing accompanying information-theoretic lower
bounds, we show under both the problem-dependent and problem-independent
paradigms, {\sc PASCombUCB} is almost asymptotically optimal. Our problem
setup, the proposed {\sc PASCombUCB} algorithm, and novel analyses are
applicable to domains such as recommendation systems and transportation in
which an agent is allowed to choose multiple items at a single time step and
wishes to control the risk over the whole time horizon.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：An Comparative Analysis of Different Pitch and Metrical Grid Encoding  Methods in the Task of Sequential Music Generation</b></summary>
  <p><b>编号</b>：[197]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13383</p>
  <p><b>作者</b>：Yuqiang Li,  Shengchen Li,  George Fazekas</p>
  <p><b>备注</b>：This is a draft before submitted to TISMIR as a journal paper</p>
  <p><b>关键词</b>：encoding methods depending, symbolic music generation, fundamental music features, specific goals, researchers usually choose</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pitch and meter are two fundamental music features for symbolic music
generation tasks, where researchers usually choose different encoding methods
depending on specific goals. However, the advantages and drawbacks of different
encoding methods have not been frequently discussed. This paper presents a
integrated analysis of the influence of two low-level feature, pitch and meter,
on the performance of a token-based sequential music generation model. First,
the commonly used MIDI number encoding and a less used class-octave encoding
are compared. Second, an dense intra-bar metric grid is imposed to the encoded
sequence as auxiliary features. Different complexity and resolutions of the
metric grid are compared. For complexity, the single token approach and the
multiple token approach are compared; for grid resolution, 0 (ablation), 1
(bar-level), 4 (downbeat-level) 12, (8th-triplet-level) up to 64
(64th-note-grid-level) are compared; for duration resolution, 4, 8, 12 and 16
subdivisions per beat are compared. All different encodings are tested on
separately trained Transformer-XL models for a melody generation task.
Regarding distribution similarity of several objective evaluation metrics to
the test dataset, results suggest that the class-octave encoding significantly
outperforms the taken-for-granted MIDI encoding on pitch-related metrics; finer
grids and multiple-token grids improve the rhythmic quality, but also suffer
from over-fitting at early training stage. Results display a general phenomenon
of over-fitting from two aspects, the pitch embedding space and the test loss
of the single-token grid encoding. From a practical perspective, we both
demonstrate the feasibility and raise the concern of easy over-fitting problem
of using smaller networks and lower embedding dimensions on the generation
task. The findings can also contribute to futural models in terms of feature
engineering.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Quantized Neural Networks for Low-Precision Accumulation with Guaranteed  Overflow Avoidance</b></summary>
  <p><b>编号</b>：[202]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13376</p>
  <p><b>作者</b>：Ian Colbert,  Alessandro Pappalardo,  Jakoba Petri-Koenig</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：guarantees avoiding numerical, avoiding numerical overflow, quantization-aware training algorithm, introduce a quantization-aware, guarantees avoiding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce a quantization-aware training algorithm that guarantees avoiding
numerical overflow when reducing the precision of accumulators during
inference. We leverage weight normalization as a means of constraining
parameters during training using accumulator bit width bounds that we derive.
We evaluate our algorithm across multiple quantized models that we train for
different tasks, showing that our approach can reduce the precision of
accumulators while maintaining model accuracy with respect to a floating-point
baseline. We then show that this reduction translates to increased design
efficiency for custom FPGA-based accelerators. Finally, we show that our
algorithm not only constrains weights to fit into an accumulator of
user-defined bit width, but also increases the sparsity and compressibility of
the resulting weights. Across all of our benchmark models trained with 8-bit
weights and activations, we observe that constraining the hidden layers of
quantized neural networks to fit into 16-bit accumulators yields an average
98.2% sparsity with an estimated compression rate of 46.5x all while
maintaining 99.2% of the floating-point performance.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：Optimal Transport Perturbations for Safe Reinforcement Learning with  Robustness Guarantees</b></summary>
  <p><b>编号</b>：[203]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13375</p>
  <p><b>作者</b>：James Queeney,  Erhan Can Ozcan,  Ioannis Ch. Paschalidis,  Christos G. Cassandras</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：decision making applications, deep reinforcement learning, reinforcement learning, real-world decision making, trustworthy deployment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Robustness and safety are critical for the trustworthy deployment of deep
reinforcement learning in real-world decision making applications. In
particular, we require algorithms that can guarantee robust, safe performance
in the presence of general environment disturbances, while making limited
assumptions on the data collection process during training. In this work, we
propose a safe reinforcement learning framework with robustness guarantees
through the use of an optimal transport cost uncertainty set. We provide an
efficient, theoretically supported implementation based on Optimal Transport
Perturbations, which can be applied in a completely offline fashion using only
data collected in a nominal training environment. We demonstrate the robust,
safe performance of our approach on a variety of continuous control tasks with
safety constraints in the Real-World Reinforcement Learning Suite.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Improving Open-Domain Dialogue Evaluation with a Causal Inference Model</b></summary>
  <p><b>编号</b>：[205]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13372</p>
  <p><b>作者</b>：Cat P. Le,  Luke Dai,  Michael Johnston,  Yang Liu,  Marilyn Walker,  Reza Ghanadan</p>
  <p><b>备注</b>：Accepted as a conference paper at IWSDS 2023</p>
  <p><b>关键词</b>：remain a significant, significant challenge, challenge for research, ratings, Effective evaluation methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Effective evaluation methods remain a significant challenge for research on
open-domain conversational dialogue systems. Explicit satisfaction ratings can
be elicited from users, but users often do not provide ratings when asked, and
those they give can be highly subjective. Post-hoc ratings by experts are an
alternative, but these can be both expensive and complex to collect. Here, we
explore the creation of automated methods for predicting both expert and user
ratings of open-domain dialogues. We compare four different approaches. First,
we train a baseline model using an end-to-end transformer to predict ratings
directly from the raw dialogue text. The other three methods are variants of a
two-stage approach in which we first extract interpretable features at the turn
level that capture, among other aspects, user dialogue behaviors indicating
contradiction, repetition, disinterest, compliments, or criticism. We project
these features to the dialogue level and train a dialogue-level MLP regression
model, a dialogue-level LSTM, and a novel causal inference model called
counterfactual-LSTM (CF-LSTM) to predict ratings. The proposed CF-LSTM is a
sequential model over turn-level features which predicts ratings using multiple
regressors depending on hypotheses derived from the turn-level features. As a
causal inference model, CF-LSTM aims to learn the underlying causes of a
specific event, such as a low rating. We also bin the user ratings and perform
classification experiments with all four models. In evaluation experiments on
conversational data from the Alexa Prize SocialBot, we show that the CF-LSTM
achieves the best performance for predicting dialogue ratings and
classification.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Skeleton-based Human Action Recognition via Convolutional Neural  Networks (CNN)</b></summary>
  <p><b>编号</b>：[210]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13360</p>
  <p><b>作者</b>：Ayman Ali,  Ekkasit Pinyoanuntapong,  Pu Wang,  Mohsen Dorodchi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：including computational efficiency, skeleton-based action recognition, including computational, computational efficiency, illumination invariance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, there has been a remarkable increase in the interest towards
skeleton-based action recognition within the research community, owing to its
various advantageous features, including computational efficiency,
representative features, and illumination invariance. Despite this, researchers
continue to explore and investigate the most optimal way to represent human
actions through skeleton representation and the extracted features. As a
result, the growth and availability of human action recognition datasets have
risen substantially. In addition, deep learning-based algorithms have gained
widespread popularity due to the remarkable advancements in various computer
vision tasks. Most state-of-the-art contributions in skeleton-based action
recognition incorporate a Graph Neural Network (GCN) architecture for
representing the human body and extracting features. Our research demonstrates
that Convolutional Neural Networks (CNNs) can attain comparable results to GCN,
provided that the proper training techniques, augmentations, and optimizers are
applied. Our approach has been rigorously validated, and we have achieved a
score of 95% on the NTU-60 dataset</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing</b></summary>
  <p><b>编号</b>：[211]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13359</p>
  <p><b>作者</b>：Guoyang Xie,  Jinbao Wang,  Jiaqi Liu,  Jiayi Lyu,  Yong Liu,  Chengjie Wang,  Feng Zheng,  Yaochu Jin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：vital computer vision, computer vision task, industrial manufacturing, emerging and vital, vital computer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image anomaly detection (IAD) is an emerging and vital computer vision task
in industrial manufacturing (IM). Recently many advanced algorithms have been
published, but their performance deviates greatly. We realize that the lack of
actual IM settings most probably hinders the development and usage of these
methods in real-world applications. As far as we know, IAD methods are not
evaluated systematically. As a result, this makes it difficult for researchers
to analyze them because they are designed for different or special cases. To
solve this problem, we first propose a uniform IM setting to assess how well
these algorithms perform, which includes several aspects, i.e., various levels
of supervision (unsupervised vs. semi-supervised), few-shot learning, continual
learning, noisy labels, memory usage, and inference speed. Moreover, we
skillfully build a comprehensive image anomaly detection benchmark (IM-IAD)
that includes 16 algorithms on 7 mainstream datasets with uniform settings. Our
extensive experiments (17,017 in total) provide in-depth insights for IAD
algorithm redesign or selection under the IM setting. Next, the proposed
benchmark IM-IAD gives challenges as well as directions for the future. To
foster reproducibility and accessibility, the source code of IM-IAD is uploaded
on the website, this https URL.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Sentence Identification with BOS and EOS Label Combinations</b></summary>
  <p><b>编号</b>：[214]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13352</p>
  <p><b>作者</b>：Takuma Udagawa,  Hiroshi Kanayama,  Issei Yoshida</p>
  <p><b>备注</b>：Accepted to EACL 2023 (Findings)</p>
  <p><b>关键词</b>：NLP applications, sentence, sentence identification, NLP, task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The sentence is a fundamental unit in many NLP applications. Sentence
segmentation is widely used as the first preprocessing task, where an input
text is split into consecutive sentences considering the end of the sentence
(EOS) as their boundaries. This task formulation relies on a strong assumption
that the input text consists only of sentences, or what we call the sentential
units (SUs). However, real-world texts often contain non-sentential units
(NSUs) such as metadata, sentence fragments, nonlinguistic markers, etc. which
are unreasonable or undesirable to be treated as a part of an SU. To tackle
this issue, we formulate a novel task of sentence identification, where the
goal is to identify SUs while excluding NSUs in a given text. To conduct
sentence identification, we propose a simple yet effective method which
combines the beginning of the sentence (BOS) and EOS labels to determine the
most probable SUs and NSUs based on dynamic programming. To evaluate this task,
we design an automatic, language-independent procedure to convert the Universal
Dependencies corpora into sentence identification benchmarks. Finally, our
experiments on the sentence identification task demonstrate that our proposed
method generally outperforms sentence segmentation baselines which only utilize
EOS labels.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Few-Shot Image-to-Semantics Translation for Policy Transfer in  Reinforcement Learning</b></summary>
  <p><b>编号</b>：[221]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13343</p>
  <p><b>作者</b>：Rei Sato,  Kazuto Fukuchi,  Jun Sakuma,  Youhei Akimoto</p>
  <p><b>备注</b>：The 2022 International Joint Conference on Neural Networks (IJCNN2022)</p>
  <p><b>关键词</b>：vision-based robotics control, robotics control agents, mitigate learning difficulties, investigate policy transfer, translation to mitigate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We investigate policy transfer using image-to-semantics translation to
mitigate learning difficulties in vision-based robotics control agents. This
problem assumes two environments: a simulator environment with semantics, that
is, low-dimensional and essential information, as the state space, and a
real-world environment with images as the state space. By learning mapping from
images to semantics, we can transfer a policy, pre-trained in the simulator, to
the real world, thereby eliminating real-world on-policy agent interactions to
learn, which are costly and risky. In addition, using image-to-semantics
mapping is advantageous in terms of the computational efficiency to train the
policy and the interpretability of the obtained policy over other types of
sim-to-real transfer strategies. To tackle the main difficulty in learning
image-to-semantics mapping, namely the human annotation cost for producing a
training dataset, we propose two techniques: pair augmentation with the
transition function in the simulator environment and active learning. We
observed a reduction in the annotation cost without a decline in the
performance of the transfer, and the proposed approach outperformed the
existing approach without annotation.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Fast Resolution Agnostic Neural Techniques to Solve Partial Differential  Equations</b></summary>
  <p><b>编号</b>：[229]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13331</p>
  <p><b>作者</b>：Hrishikesh Viswanath,  Md Ashiqur Rahman,  Abhijeet Vyas,  Andrey Shor,  Beatriz Medeiros,  Stephanie Hernandez,  Suhas Eswarappa Prameela,  Aniket Bera</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：partial differential equations, mathematical problems involving, problems involving functions, fluid flow, Numerical approximations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Numerical approximations of partial differential equations (PDEs) are
routinely employed to formulate the solution of physics, engineering and
mathematical problems involving functions of several variables, such as the
propagation of heat or sound, fluid flow, elasticity, electrostatics,
electrodynamics, and more. While this has led to solving many complex
phenomena, there are still significant limitations. Conventional approaches
such as Finite Element Methods (FEMs) and Finite Differential Methods (FDMs)
require considerable time and are computationally expensive. In contrast,
machine learning-based methods such as neural networks are faster once trained,
but tend to be restricted to a specific discretization. This article aims to
provide a comprehensive summary of conventional methods and recent machine
learning-based methods to approximate PDEs numerically. Furthermore, we
highlight several key architectures centered around the neural operator, a
novel and fast approach (1000x) to learning the solution operator of a PDE. We
will note how these new computational approaches can bring immense advantages
in tackling many problems in fundamental and applied physics.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：On the Complexity of Enumerating Prime Implicants from Decision-DNNF  Circuits</b></summary>
  <p><b>编号</b>：[232]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13328</p>
  <p><b>作者</b>：Alexis de Colnet,  Pierre Marquis</p>
  <p><b>备注</b>：13 pages, including appendices</p>
  <p><b>关键词</b>：Boolean functions represented, negation normal form, decision decomposable negation, decomposable negation normal, Boolean functions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider the problem EnumIP of enumerating prime implicants of Boolean
functions represented by decision decomposable negation normal form (dec-DNNF)
circuits. We study EnumIP from dec-DNNF within the framework of enumeration
complexity and prove that it is in OutputP, the class of output polynomial
enumeration problems, and more precisely in IncP, the class of polynomial
incremental time enumeration problems. We then focus on two closely related,
but seemingly harder, enumeration problems where further restrictions are put
on the prime implicants to be generated. In the first problem, one is only
interested in prime implicants representing subset-minimal abductive
explanations, a notion much investigated in AI for more than three decades. In
the second problem, the target is prime implicants representing sufficient
reasons, a recent yet important notion in the emerging field of eXplainable AI,
since they aim to explain predictions achieved by machine learning classifiers.
We provide evidence showing that enumerating specific prime implicants
corresponding to subset-minimal abductive explanations or to sufficient reasons
is not in OutputP.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：A Framework for Adapting Offline Algorithms to Solve Combinatorial  Multi-Armed Bandit Problems with Bandit Feedback</b></summary>
  <p><b>编号</b>：[233]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13326</p>
  <p><b>作者</b>：Guanyu Nie,  Yididiya Y Nadew,  Yanhui Zhu,  Vaneet Aggarwal,  Christopher John Quinn</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：combinatorial multi-armed bandits, require bandit feedback, bandit feedback, combinatorial multi-armed, offline approximation algorithms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We investigate the problem of stochastic, combinatorial multi-armed bandits
where the learner only has access to bandit feedback and the reward function
can be non-linear. We provide a general framework for adapting discrete offline
approximation algorithms into sublinear $\alpha$-regret methods that only
require bandit feedback, achieving
$\mathcal{O}\left(T^\frac{2}{3}\log(T)^\frac{1}{3}\right)$ expected cumulative
$\alpha$-regret dependence on the horizon $T$. The framework only requires the
offline algorithms to be robust to small errors in function evaluation. The
adaptation procedure does not even require explicit knowledge of the offline
approximation algorithm -- the offline algorithm can be used as black box
subroutine.
To demonstrate the utility of the proposed framework, the proposed framework
is applied to multiple problems in submodular maximization, adapting
approximation algorithms for cardinality and for knapsack constraints. The new
CMAB algorithms for knapsack constraints outperform a full-bandit method
developed for the adversarial setting in experiments with real-world data.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：V2N Service Scaling with Deep Reinforcement Learning</b></summary>
  <p><b>编号</b>：[234]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13324</p>
  <p><b>作者</b>：Cyril Shih-Huan Hsu,  Jorge Martín-Pérez,  Chrysa Papagianni,  Paola Grosso</p>
  <p><b>备注</b>：Accepted at the 36th IEEE/IFIP Network Operations and Management Symposium (NOMS 2023)</p>
  <p><b>关键词</b>：wireless networks, meet the stringent, stringent requirements, Edge computing, Edge computing resources</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The fifth generation (5G) of wireless networks is set out to meet the
stringent requirements of vehicular use cases. Edge computing resources can aid
in this direction by moving processing closer to end-users, reducing latency.
However, given the stochastic nature of traffic loads and availability of
physical resources, appropriate auto-scaling mechanisms need to be employed to
support cost-efficient and performant services. To this end, we employ Deep
Reinforcement Learning (DRL) for vertical scaling in Edge computing to support
vehicular-to-network communications. We address the problem using Deep
Deterministic Policy Gradient (DDPG). As DDPG is a model-free off-policy
algorithm for learning continuous actions, we introduce a discretization
approach to support discrete scaling actions. Thus we address scalability
problems inherent to high-dimensional discrete action spaces. Employing a
real-world vehicular trace data set, we show that DDPG outperforms existing
solutions, reducing (at minimum) the average number of active CPUs by 23% while
increasing the long-term reward by 24%.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Incorporating Recurrent Reinforcement Learning into Model Predictive  Control for Adaptive Control in Autonomous Driving</b></summary>
  <p><b>编号</b>：[239]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13313</p>
  <p><b>作者</b>：Yuan Zhang,  Joschka Boedecker,  Chuxuan Li,  Guyue Zhou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：attracting tremendous attention, autonomous driving task, Model Predictive Control, powerful control technique, Model Predictive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Model Predictive Control (MPC) is attracting tremendous attention in the
autonomous driving task as a powerful control technique. The success of an MPC
controller strongly depends on an accurate internal dynamics model. However,
the static parameters, usually learned by system identification, often fail to
adapt to both internal and external perturbations in real-world scenarios. In
this paper, we firstly (1) reformulate the problem as a Partially Observed
Markov Decision Process (POMDP) that absorbs the uncertainties into
observations and maintains Markov property into hidden states; and (2) learn a
recurrent policy continually adapting the parameters of the dynamics model via
Recurrent Reinforcement Learning (RRL) for optimal and adaptive control; and
(3) finally evaluate the proposed algorithm (referred as $\textit{MPC-RRL}$) in
CARLA simulator and leading to robust behaviours under a wide range of
perturbations.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Understanding Self-Distillation in the Presence of Label Noise</b></summary>
  <p><b>编号</b>：[246]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13304</p>
  <p><b>作者</b>：Rudrajit Das,  Sujay Sanghavi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：student predictions, text, student, student objective function, enquote</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Self-distillation (SD) is the process of first training a \enquote{teacher}
model and then using its predictions to train a \enquote{student} model with
the \textit{same} architecture. Specifically, the student's objective function
is $\big(\xi*\ell(\text{teacher's predictions}, \text{ student's predictions})
+ (1-\xi)*\ell(\text{given labels}, \text{ student's predictions})\big)$, where
$\ell$ is some loss function and $\xi$ is some parameter $\in [0,1]$.
Empirically, SD has been observed to provide performance gains in several
settings. In this paper, we theoretically characterize the effect of SD in two
supervised learning problems with \textit{noisy labels}. We first analyze SD
for regularized linear regression and show that in the high label noise regime,
the optimal value of $\xi$ that minimizes the expected error in estimating the
ground truth parameter is surprisingly greater than 1. Empirically, we show
that $\xi > 1$ works better than $\xi \leq 1$ even with the cross-entropy loss
for several classification datasets when 50\% or 30\% of the labels are
corrupted. Further, we quantify when optimal SD is better than optimal
regularization. Next, we analyze SD in the case of logistic regression for
binary classification with random label corruption and quantify the range of
label corruption in which the student outperforms the teacher in terms of
accuracy. To our knowledge, this is the first result of its kind for the
cross-entropy loss.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Sifer: Overcoming simplicity bias in deep networks using a feature sieve</b></summary>
  <p><b>编号</b>：[250]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13293</p>
  <p><b>作者</b>：Rishabh Tiwari,  Pradeep Shenoy</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：weakly predictive features, over-depend on simple, weakly predictive, exclusion of stronger, Simplicity bias</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Simplicity bias is the concerning tendency of deep networks to over-depend on
simple, weakly predictive features, to the exclusion of stronger, more complex
features. This causes biased, incorrect model predictions in many real-world
applications, exacerbated by incomplete training data containing spurious
feature-label correlations. We propose a direct, interventional method for
addressing simplicity bias in DNNs, which we call the feature sieve. We aim to
automatically identify and suppress easily-computable spurious features in
lower layers of the network, thereby allowing the higher network levels to
extract and utilize richer, more meaningful representations. We provide
concrete evidence of this differential suppression & enhancement of relevant
features on both controlled datasets and real-world images, and report
substantial gains on many real-world debiasing benchmarks (11.4% relative gain
on Imagenet-A; 3.2% on BAR, etc). Crucially, we outperform many baselines that
incorporate knowledge about known spurious or biased attributes, despite our
method not using any such information. We believe that our feature sieve work
opens up exciting new research directions in automated adversarial feature
extraction & representation learning for deep networks.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：Team Plan Recognition: A Review of the State of the Art</b></summary>
  <p><b>编号</b>：[252]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13288</p>
  <p><b>作者</b>：Loren Rieffer-Champlin</p>
  <p><b>备注</b>：10 pages, 1 figure, 1 table. Abstract accepted, paper submitted to 14th International Conference on Applied Human Factors and Ergonomics (AHFE 2023)</p>
  <p><b>关键词</b>：develop artificial intelligence, artificial intelligence systems, coordinated tasks, humans working, develop artificial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>There is an increasing need to develop artificial intelligence systems that
assist groups of humans working on coordinated tasks. These systems must
recognize and understand the plans and relationships between actions for a team
of humans working toward a common objective. This article reviews the
literature on team plan recognition and surveys the most recent logic-based
approaches for implementing it. First, we provide some background knowledge,
including a general definition of plan recognition in a team setting and a
discussion of implementation challenges. Next, we explain our reasoning for
focusing on logic-based methods. Finally, we survey recent approaches from two
primary classes of logic-based methods (plan library-based and domain
theory-based). We aim to bring more attention to this sparse but vital topic
and inspire new directions for implementing team plan recognition.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：MILO: Model-Agnostic Subset Selection Framework for Efficient Model  Training and Tuning</b></summary>
  <p><b>编号</b>：[253]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13287</p>
  <p><b>作者</b>：Kirshnateja Killamsetty,  Alexandre V. Evfimievski,  Tejaswini Pedapati,  Kiran Kate,  Lucian Popa,  Rishabh Iyer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Training deep networks, computationally intensive, subset selection, deep networks, large datasets</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Training deep networks and tuning hyperparameters on large datasets is
computationally intensive. One of the primary research directions for efficient
training is to reduce training costs by selecting well-generalizable subsets of
training data. Compared to simple adaptive random subset selection baselines,
existing intelligent subset selection approaches are not competitive due to the
time-consuming subset selection step, which involves computing model-dependent
gradients and feature embeddings and applies greedy maximization of submodular
objectives. Our key insight is that removing the reliance on downstream model
parameters enables subset selection as a pre-processing step and enables one to
train multiple models at no additional cost. In this work, we propose MILO, a
model-agnostic subset selection framework that decouples the subset selection
from model training while enabling superior model convergence and performance
by using an easy-to-hard curriculum. Our empirical results indicate that MILO
can train models $3\times - 10 \times$ faster and tune hyperparameters
$20\times - 75 \times$ faster than full-dataset training or tuning without
compromising performance.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Learning Coordination Policies over Heterogeneous Graphs for Human-Robot  Teams via Recurrent Neural Schedule Propagation</b></summary>
  <p><b>编号</b>：[257]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13279</p>
  <p><b>作者</b>：Batuhan Altundas,  Zheyuan Wang,  Joshua Bishop,  Matthew Gombolay</p>
  <p><b>备注</b>：8 pages, 2 figures, 3 Tables</p>
  <p><b>关键词</b>：human-robot collaboration increases, efficiently and intuitively, collaboration increases, coordinate efficiently, Heterogeneous Graph Attention</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As human-robot collaboration increases in the workforce, it becomes essential
for human-robot teams to coordinate efficiently and intuitively. Traditional
approaches for human-robot scheduling either utilize exact methods that are
intractable for large-scale problems and struggle to account for stochastic,
time varying human task performance, or application-specific heuristics that
require expert domain knowledge to develop. We propose a deep learning-based
framework, called HybridNet, combining a heterogeneous graph-based encoder with
a recurrent schedule propagator for scheduling stochastic human-robot teams
under upper- and lower-bound temporal constraints. The HybridNet's encoder
leverages Heterogeneous Graph Attention Networks to model the initial
environment and team dynamics while accounting for the constraints. By
formulating task scheduling as a sequential decision-making process, the
HybridNet's recurrent neural schedule propagator leverages Long Short-Term
Memory (LSTM) models to propagate forward consequences of actions to carry out
fast schedule generation, removing the need to interact with the environment
between every task-agent pair selection. The resulting scheduling policy
network provides a computationally lightweight yet highly expressive model that
is end-to-end trainable via Reinforcement Learning algorithms. We develop a
virtual task scheduling environment for mixed human-robot teams in a
multi-round setting, capable of modeling the stochastic learning behaviors of
human workers. Experimental results showed that HybridNet outperformed other
human-robot scheduling solutions across problem sizes for both deterministic
and stochastic human performance, with faster runtime compared to
pure-GNN-based schedulers.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Distributed Swarm Intelligence</b></summary>
  <p><b>编号</b>：[258]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13276</p>
  <p><b>作者</b>：Karthik Reddy Kanjula,  Sai Meghana Kolla</p>
  <p><b>备注</b>：7 pages, 3 Figure, 1 Algorithm</p>
  <p><b>关键词</b>：solving optimization problems, optimization problems, paper presents, presents the development, solving optimization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents the development of a distributed application that
facilitates the understanding and application of swarm intelligence in solving
optimization problems. The platform comprises a search space of customizable
random particles, allowing users to tailor the solution to their specific
needs. By leveraging the power of Ray distributed computing, the application
can support multiple users simultaneously, offering a flexible and scalable
solution. The primary objective of this project is to provide a user-friendly
platform that enhances the understanding and practical use of swarm
intelligence in problem-solving.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Emergence of Maps in the Memories of Blind Navigation Agents</b></summary>
  <p><b>编号</b>：[264]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13261</p>
  <p><b>作者</b>：Erik Wijmans,  Manolis Savva,  Irfan Essa,  Stefan Lee,  Ari S. Morcos,  Dhruv Batra</p>
  <p><b>备注</b>：Accepted to ICLR 2023</p>
  <p><b>关键词</b>：maintain internal spatial, navigation research posits, internal spatial representations, Animal navigation research, research posits</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Animal navigation research posits that organisms build and maintain internal
spatial representations, or maps, of their environment. We ask if machines --
specifically, artificial intelligence (AI) navigation agents -- also build
implicit (or 'mental') maps. A positive answer to this question would (a)
explain the surprising phenomenon in recent literature of ostensibly map-free
neural-networks achieving strong performance, and (b) strengthen the evidence
of mapping as a fundamental mechanism for navigation by intelligent embodied
agents, whether they be biological or artificial. Unlike animal navigation, we
can judiciously design the agent's perceptual system and control the learning
paradigm to nullify alternative navigation mechanisms. Specifically, we train
'blind' agents -- with sensing limited to only egomotion and no other sensing
of any kind -- to perform PointGoal navigation ('go to $\Delta$ x, $\Delta$ y')
via reinforcement learning. Our agents are composed of navigation-agnostic
components (fully-connected and recurrent neural networks), and our
experimental setup provides no inductive bias towards mapping. Despite these
harsh conditions, we find that blind agents are (1) surprisingly effective
navigators in new environments (~95% success); (2) they utilize memory over
long horizons (remembering ~1,000 steps of past experience in an episode); (3)
this memory enables them to exhibit intelligent behavior (following walls,
detecting collisions, taking shortcuts); (4) there is emergence of maps and
collision detection neurons in the representations of the environment built by
a blind agent as it navigates; and (5) the emergent maps are selective and task
dependent (e.g. the agent 'forgets' exploratory detours). Overall, this paper
presents no new techniques for the AI audience, but a surprising finding, an
insight, and an explanation.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：Online Loss Function Learning</b></summary>
  <p><b>编号</b>：[266]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13247</p>
  <p><b>作者</b>：Christian Raymond,  Qi Chen,  Bing Xue,  Mengjie Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Loss function learning, Loss function, function learning, machine learning model, Loss</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Loss function learning is a new meta-learning paradigm that aims to automate
the essential task of designing a loss function for a machine learning model.
Existing techniques for loss function learning have shown promising results,
often improving a model's training dynamics and final inference performance.
However, a significant limitation of these techniques is that the loss
functions are meta-learned in an offline fashion, where the meta-objective only
considers the very first few steps of training, which is a significantly
shorter time horizon than the one typically used for training deep neural
networks. This causes significant bias towards loss functions that perform well
at the very start of training but perform poorly at the end of training. To
address this issue we propose a new loss function learning technique for
adaptively updating the loss function online after each update to the base
model parameters. The experimental results show that our proposed method
consistently outperforms the cross-entropy loss and offline loss function
learning techniques on a diverse range of neural network architectures and
datasets.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：SoftTreeMax: Exponential Variance Reduction in Policy Gradient via Tree  Search</b></summary>
  <p><b>编号</b>：[271]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13236</p>
  <p><b>作者</b>：Gal Dalal,  Assaf Hallak,  Gugan Thoppe,  Shie Mannor,  Gal Chechik</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2209.13966</p>
  <p><b>关键词</b>：suffer from large, policy gradient methods, variance, policy, gradient methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite the popularity of policy gradient methods, they are known to suffer
from large variance and high sample complexity. To mitigate this, we introduce
SoftTreeMax -- a generalization of softmax that takes planning into account. In
SoftTreeMax, we extend the traditional logits with the multi-step discounted
cumulative reward, topped with the logits of future states. We consider two
variants of SoftTreeMax, one for cumulative reward and one for exponentiated
reward. For both, we analyze the gradient variance and reveal for the first
time the role of a tree expansion policy in mitigating this variance. We prove
that the resulting variance decays exponentially with the planning horizon as a
function of the expansion policy. Specifically, we show that the closer the
resulting state transitions are to uniform, the faster the decay. In a
practical implementation, we utilize a parallelized GPU-based simulator for
fast and efficient tree search. Our differentiable tree-based policy leverages
all gradients at the tree leaves in each environment step instead of the
traditional single-sample-based gradient. We then show in simulation how the
variance of the gradient is reduced by three orders of magnitude, leading to
better sample complexity compared to the standard policy gradient. On Atari,
SoftTreeMax demonstrates up to 5x better performance in a faster run time
compared to distributed PPO. Lastly, we demonstrate that high reward correlates
with lower variance.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Reinforcement learning and decision making via single-photon quantum  walks</b></summary>
  <p><b>编号</b>：[287]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.13669</p>
  <p><b>作者</b>：Fulvio Flamini,  Marius Krumm,  Lukas J. Fiderer,  Thomas Müller,  Hans J. Briegel</p>
  <p><b>备注</b>：10+6 pages, 6+5 figures, 2 tables. F. Flamini and M. Krumm contributed equally to this work</p>
  <p><b>关键词</b>：parametrized quantum circuits, classical neural networks, represent a promising, neural networks, networks are replaced</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Variational quantum algorithms represent a promising approach to quantum
machine learning where classical neural networks are replaced by parametrized
quantum circuits. Here, we present a variational approach to quantize
projective simulation (PS), a reinforcement learning model aimed at
interpretable artificial intelligence. Decision making in PS is modeled as a
random walk on a graph describing the agent's memory. To implement the
quantized model, we consider quantum walks of single photons in a lattice of
tunable Mach-Zehnder interferometers. We propose variational algorithms
tailored to reinforcement learning tasks, and we show, using an example from
transfer learning, that the quantized PS learning model can outperform its
classical counterpart. Finally, we discuss the role of quantum interference for
training and decision making, paving the way for realizations of interpretable
quantum learning agents.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：Deep Quantum Error Correction</b></summary>
  <p><b>编号</b>：[317]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2301.11930</p>
  <p><b>作者</b>：Yoni Choukroun,  Lior Wolf</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Quantum error correction, error correction codes, key component, component for realizing, realizing the potential</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Quantum error correction codes (QECC) are a key component for realizing the
potential of quantum computing. QECC, as its classical counterpart (ECC),
enables the reduction of error rates, by distributing quantum logical
information across redundant physical qubits, such that errors can be detected
and corrected. In this work, we efficiently train novel deep quantum error
decoders. We resolve the quantum measurement collapse by augmenting syndrome
decoding to predict an initial estimate of the system noise, which is then
refined iteratively through a deep neural network. The logical error rates
calculated over finite fields are directly optimized via a differentiable
objective, enabling efficient decoding under the constraints imposed by the
code. Finally, our architecture is extended to support faulty syndrome
measurement, to allow efficient decoding over repeated syndrome sampling. The
proposed method demonstrates the power of neural decoders for QECC by achieving
state-of-the-art accuracy, outperforming, for a broad range of topological
codes, the existing neural and classical decoders, which are often
computationally prohibitive.</p>
  </details>
</details>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">徐耀彬</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://louishsu.xyz/2023/02/02/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">http://louishsu.xyz/2023/02/02/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://louishsu.xyz" target="_blank">LOUIS' BLOG</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2022/11/26/%E5%8D%87%E7%BA%A7%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%85%A8%E6%94%BB%E7%95%A5.html"><img class="next-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">升级深度学习开发环境全攻略</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">徐耀彬</div><div class="author-info__description">专注于自然语言处理前沿技术与应用价值！</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">13</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/isLouisHsu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/isLouisHsu" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:is.louishsu@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">记录和分享一些学习和开源内容，若有问题可通过邮箱is.louishsu@foxmail.com联系，欢迎交流！！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">统计</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">计算机视觉</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">自然语言处理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">机器学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">人工智能</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/02/02/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2023-02-02)"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv每日速递(2023-02-02)"/></a><div class="content"><a class="title" href="/2023/02/02/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2023-02-02)">Arxiv每日速递(2023-02-02)</a><time datetime="2023-02-02T00:42:13.038Z" title="发表于 2023-02-02 08:42:13">2023-02-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/11/26/%E5%8D%87%E7%BA%A7%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%85%A8%E6%94%BB%E7%95%A5.html" title="升级深度学习开发环境全攻略"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="升级深度学习开发环境全攻略"/></a><div class="content"><a class="title" href="/2022/11/26/%E5%8D%87%E7%BA%A7%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%85%A8%E6%94%BB%E7%95%A5.html" title="升级深度学习开发环境全攻略">升级深度学习开发环境全攻略</a><time datetime="2022-11-26T15:29:06.000Z" title="发表于 2022-11-26 23:29:06">2022-11-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/11/17/2022%E5%85%A8%E7%90%83%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0%E5%A4%A7%E8%B5%9B(GAIIC2022)%EF%BC%9A%E5%95%86%E5%93%81%E6%A0%87%E9%A2%98%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB(%E4%BA%8C%E7%AD%89%E5%A5%96).html" title="2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖)"><img src="https://cdn.kesci.com/upload/image/r7j60un866.png?imageView2/2/w/2500/h/2500" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖)"/></a><div class="content"><a class="title" href="/2022/11/17/2022%E5%85%A8%E7%90%83%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0%E5%A4%A7%E8%B5%9B(GAIIC2022)%EF%BC%9A%E5%95%86%E5%93%81%E6%A0%87%E9%A2%98%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB(%E4%BA%8C%E7%AD%89%E5%A5%96).html" title="2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖)">2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖)</a><time datetime="2022-11-17T14:29:06.000Z" title="发表于 2022-11-17 22:29:06">2022-11-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/10/22/%E4%B8%AD%E5%9B%BD%E6%B3%95%E5%BE%8B%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E8%AF%84%E6%B5%8B(CAIL2021)%EF%BC%9A%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96(Rank2).html" title="中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)"><img src="http://cail.cipsc.org.cn/img/index_mainpic.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)"/></a><div class="content"><a class="title" href="/2021/10/22/%E4%B8%AD%E5%9B%BD%E6%B3%95%E5%BE%8B%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E8%AF%84%E6%B5%8B(CAIL2021)%EF%BC%9A%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96(Rank2).html" title="中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)">中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)</a><time datetime="2021-10-22T14:29:06.000Z" title="发表于 2021-10-22 22:29:06">2021-10-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/05/19/%E5%85%A8%E7%90%83%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0%E5%A4%A7%E8%B5%9B%E3%80%90%E8%B5%9B%E9%81%93%E4%B8%80%E3%80%91%EF%BC%9A%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E6%8A%A5%E5%91%8A%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B(%E4%B8%89%E7%AD%89%E5%A5%96).html" title="全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖)"><img src="https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161037709574435991610377095138.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖)"/></a><div class="content"><a class="title" href="/2021/05/19/%E5%85%A8%E7%90%83%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0%E5%A4%A7%E8%B5%9B%E3%80%90%E8%B5%9B%E9%81%93%E4%B8%80%E3%80%91%EF%BC%9A%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E6%8A%A5%E5%91%8A%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B(%E4%B8%89%E7%AD%89%E5%A5%96).html" title="全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖)">全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖)</a><time datetime="2021-05-19T09:57:06.000Z" title="发表于 2021-05-19 17:57:06">2021-05-19</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2023 By 徐耀彬</div><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender"title="主题采用butterfly"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr"title="本站使用JsDelivr为静态资源提供CDN加速"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub"title="本站项目由Gtihub托管"alt="img"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"alt="img"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></br></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    let initData = {
      el: '#twikoo-wrap',
      envId: 'blog-',
      region: ''
    }

    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    twikoo.init(initData)
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'blog-',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/机器学习/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐱 机器学习 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/自然语言处理/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 自然语言处理 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/竞赛相关/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 竞赛相关 (3)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/阅读笔记/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 阅读笔记 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="http://louishsu.xyz/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style>
  <script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/hexo-github-calendar@1.21/hexo_githubcalendar.js"></script>
  <script data-pjax>
        function GithubCalendarConfig(){
            var git_githubapiurl ="https://python-github-calendar-api.vercel.app/api?isLouisHsu";
            var git_color =['#ebedf0', '#fdcdec', '#fc9bd9', '#fa6ac5', '#f838b2', '#f5089f', '#c4067e', '#92055e', '#540336', '#48022f', '#30021f'];
            var git_user ="isLouisHsu";
            var parent_div_git = document.getElementById('recent-posts');
            var git_div_html = '<div class="recent-post-item" style="width:100%;height:auto;padding:10px;"><div id="github_loading" style="width:10%;height:100%;margin:0 auto;display: block"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"  viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animateTransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animateTransform></path></svg></div><div id="github_container"></div></div>';
            if(parent_div_git && location.pathname =='/'){
                console.log('已挂载github calendar')
                // parent_div_git.innerHTML=git_div_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",git_div_html) // 有报错，但不影响使用(支持pjax跳转)
            };
            GithubCalendar(git_githubapiurl,git_color,git_user)
        }
        if(document.getElementById('recent-posts')){
            GithubCalendarConfig()
        }
    </script>
    <style>#github_container{min-height:280px}@media screen and (max-width:650px) {#github_container{background-image:;min-height:0px}}</style>
    <style></style><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>