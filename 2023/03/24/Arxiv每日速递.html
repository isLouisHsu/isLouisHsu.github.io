<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Arxiv每日速递(2023-03-24) | LOUIS' BLOG</title><meta name="author" content="徐耀彬"><meta name="copyright" content="徐耀彬"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。 统计 今日共更新359篇论文，其中：  136篇计算机视觉（cs.CV） 42篇自然语言处理（cs.CL） 105篇机器学习（cs.LG） 86篇人工智能（cs.AI）  计算机视觉    1. 标题：Learning and Verification of Task St">
<meta property="og:type" content="article">
<meta property="og:title" content="Arxiv每日速递(2023-03-24)">
<meta property="og:url" content="http://louishsu.xyz/2023/03/24/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">
<meta property="og:site_name" content="LOUIS&#39; BLOG">
<meta property="og:description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。 统计 今日共更新359篇论文，其中：  136篇计算机视觉（cs.CV） 42篇自然语言处理（cs.CL） 105篇机器学习（cs.LG） 86篇人工智能（cs.AI）  计算机视觉    1. 标题：Learning and Verification of Task St">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png">
<meta property="article:published_time" content="2023-03-24T00:39:44.495Z">
<meta property="article:modified_time" content="2023-03-24T00:41:11.324Z">
<meta property="article:author" content="徐耀彬">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://louishsu.xyz/2023/03/24/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-03-24 08:41:11'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/background.css"><script src="https://cdn.jsdelivr.net/npm/echarts@4.7.0/dist/echarts.min.js"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">14</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-brands fa-app-store"></i><span> 利器</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://code.visualstudio.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> VSCode：微软旗下的跨平台代码编辑软件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mobaxterm.mobatek.net/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> MobaXterm：超好用的全能远程终端</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.zotero.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zotero：便于收集、组织、引用、共享的文献管理工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/CopyTranslator/CopyTranslator"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> CopyTranslator：“复制即翻译”的外文辅助阅读翻译解决方案</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ditto-cp.sourceforge.io/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Ditto：强大的Windows剪贴板增强工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/indiff/qttabbar"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> QtTabBar：在Windows资源管理器中使用多标签功能扩展工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/sentialx/multrin"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Multrin：“窗口合并”辅助小工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/Wox-launcher/Wox"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Wox &amp; Everything：基于名称快速定位文件和文件夹的搜索工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/NickeManarin/ScreenToGif"><i class="fa-fw fa-regular fa-star"></i><span> ScreenToGif：快速录制屏幕指定区域并保存为动图文件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mathpix.com/"><i class="fa-fw fa-regular fa-star"></i><span> Mathpix Snipping：识别数学公式并转换成LaTeX</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">LOUIS' BLOG</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-brands fa-app-store"></i><span> 利器</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://code.visualstudio.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> VSCode：微软旗下的跨平台代码编辑软件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mobaxterm.mobatek.net/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> MobaXterm：超好用的全能远程终端</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.zotero.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zotero：便于收集、组织、引用、共享的文献管理工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/CopyTranslator/CopyTranslator"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> CopyTranslator：“复制即翻译”的外文辅助阅读翻译解决方案</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ditto-cp.sourceforge.io/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Ditto：强大的Windows剪贴板增强工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/indiff/qttabbar"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> QtTabBar：在Windows资源管理器中使用多标签功能扩展工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/sentialx/multrin"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Multrin：“窗口合并”辅助小工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/Wox-launcher/Wox"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Wox &amp; Everything：基于名称快速定位文件和文件夹的搜索工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/NickeManarin/ScreenToGif"><i class="fa-fw fa-regular fa-star"></i><span> ScreenToGif：快速录制屏幕指定区域并保存为动图文件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mathpix.com/"><i class="fa-fw fa-regular fa-star"></i><span> Mathpix Snipping：识别数学公式并转换成LaTeX</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Arxiv每日速递(2023-03-24)<a class="post-edit-link" href="https://github.com/isLouisHsu/blog/tree/master/source_posts/Arxiv每日速递.md" title="编辑" target="_blank"><i class="fas fa-pencil-square"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-03-24T00:39:44.495Z" title="发表于 2023-03-24 08:39:44">2023-03-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-03-24T00:41:11.324Z" title="更新于 2023-03-24 08:41:11">2023-03-24</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">阅读笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">70.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>420分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2023/03/24/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。</p>
<h1>统计</h1>
<p>今日共更新359篇论文，其中：</p>
<ul>
<li><a href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89">136篇计算机视觉（cs.CV）</a></li>
<li><a href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86">42篇自然语言处理（cs.CL）</a></li>
<li><a href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">105篇机器学习（cs.LG）</a></li>
<li><a href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD">86篇人工智能（cs.AI）</a></li>
</ul>
<h1>计算机视觉</h1>
<details>
  <summary>1. <b>标题：Learning and Verification of Task Structure in Instructional Videos</b></summary>
  <p><b>编号</b>：[1]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13519</p>
  <p><b>作者</b>：Medhini Narasimhan,  Licheng Yu,  Sean Bell,  Ning Zhang,  Trevor Darrell</p>
  <p><b>备注</b>：Wesbite at this https URL</p>
  <p><b>关键词</b>：multi-step task models, enormous number, diverse array, array of multi-step, instructional videos</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Given the enormous number of instructional videos available online, learning
a diverse array of multi-step task models from videos is an appealing goal. We
introduce a new pre-trained video model, VideoTaskformer, focused on
representing the semantics and structure of instructional videos. We pre-train
VideoTaskformer using a simple and effective objective: predicting weakly
supervised textual labels for steps that are randomly masked out from an
instructional video (masked step modeling). Compared to prior work which learns
step representations locally, our approach involves learning them globally,
leveraging video of the entire surrounding task as context. From these learned
representations, we can verify if an unseen video correctly executes a given
task, as well as forecast which steps are likely to be taken after a given
step. We introduce two new benchmarks for detecting mistakes in instructional
videos, to verify if there is an anomalous step and if steps are executed in
the right order. We also introduce a long-term forecasting benchmark, where the
goal is to predict long-range future steps from a given step. Our method
outperforms previous baselines on these tasks, and we believe the tasks will be
a valuable way for the community to measure the quality of step
representations. Additionally, we evaluate VideoTaskformer on 3 existing
benchmarks -- procedural activity recognition, step classification, and step
forecasting -- and demonstrate on each that our method outperforms existing
baselines and achieves new state-of-the-art performance.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Three ways to improve feature alignment for open vocabulary detection</b></summary>
  <p><b>编号</b>：[2]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13518</p>
  <p><b>作者</b>：Relja Arandjelović,  Alex Andonian,  Arthur Mensch,  Olivier J. Hénaff,  Jean-Baptiste Alayrac,  Andrew Zisserman</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：open vocabulary detection, zero-shot open vocabulary, vision-text feature alignment, core problem, open vocabulary</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The core problem in zero-shot open vocabulary detection is how to align
visual and text features, so that the detector performs well on unseen classes.
Previous approaches train the feature pyramid and detection head from scratch,
which breaks the vision-text feature alignment established during pretraining,
and struggles to prevent the language model from forgetting unseen classes.
We propose three methods to alleviate these issues. Firstly, a simple scheme
is used to augment the text embeddings which prevents overfitting to a small
number of classes seen during training, while simultaneously saving memory and
computation. Secondly, the feature pyramid network and the detection head are
modified to include trainable gated shortcuts, which encourages vision-text
feature alignment and guarantees it at the start of detection training.
Finally, a self-training approach is used to leverage a larger corpus of
image-text pairs thus improving detection performance on classes with no human
annotated bounding boxes.
Our three methods are evaluated on the zero-shot version of the LVIS
benchmark, each of them showing clear and significant benefits. Our final
network achieves the new stateof-the-art on the mAP-all metric and demonstrates
competitive performance for mAP-rare, as well as superior transfer to COCO and
Objects365.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Ablating Concepts in Text-to-Image Diffusion Models</b></summary>
  <p><b>编号</b>：[3]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13516</p>
  <p><b>作者</b>：Nupur Kumari,  Bingliang Zhang,  Sheng-Yu Wang,  Eli Shechtman,  Richard Zhang,  Jun-Yan Zhu</p>
  <p><b>备注</b>：project website: this https URL</p>
  <p><b>关键词</b>：powerful compositional ability, generate high-fidelity images, compositional ability, generate high-fidelity, powerful compositional</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large-scale text-to-image diffusion models can generate high-fidelity images
with powerful compositional ability. However, these models are typically
trained on an enormous amount of Internet data, often containing copyrighted
material, licensed images, and personal photos. Furthermore, they have been
found to replicate the style of various living artists or memorize exact
training samples. How can we remove such copyrighted concepts or images without
retraining the model from scratch? To achieve this goal, we propose an
efficient method of ablating concepts in the pretrained model, i.e., preventing
the generation of a target concept. Our algorithm learns to match the image
distribution for a target style, instance, or text prompt we wish to ablate to
the distribution corresponding to an anchor concept. This prevents the model
from generating target concepts given its text condition. Extensive experiments
show that our method can successfully prevent the generation of the ablated
concept while preserving closely related concepts in the model.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Persistent Nature: A Generative Model of Unbounded 3D Worlds</b></summary>
  <p><b>编号</b>：[4]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13515</p>
  <p><b>作者</b>：Lucy Chai,  Richard Tucker,  Zhengqi Li,  Phillip Isola,  Noah Snavely</p>
  <p><b>备注</b>：CVPR camera ready version, project page: this https URL</p>
  <p><b>关键词</b>：realistic image quality, increasingly realistic image, limited camera motions, image quality, realistic image</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite increasingly realistic image quality, recent 3D image generative
models often operate on 3D volumes of fixed extent with limited camera motions.
We investigate the task of unconditionally synthesizing unbounded nature
scenes, enabling arbitrarily large camera motion while maintaining a persistent
3D world model. Our scene representation consists of an extendable, planar
scene layout grid, which can be rendered from arbitrary camera poses via a 3D
decoder and volume rendering, and a panoramic skydome. Based on this
representation, we learn a generative world model solely from single-view
internet photos. Our method enables simulating long flights through 3D
landscapes, while maintaining global scene consistency--for instance, returning
to the starting point yields the same view of the scene. Our approach enables
scene extrapolation beyond the fixed bounds of current 3D generative models,
while also supporting a persistent, camera-independent world representation
that stands in contrast to auto-regressive 3D prediction models. Our project
page: this https URL.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：SAOR: Single-View Articulated Object Reconstruction</b></summary>
  <p><b>编号</b>：[5]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13514</p>
  <p><b>作者</b>：Mehmet Aygün,  Oisin Mac Aodha</p>
  <p><b>备注</b>：this https URL</p>
  <p><b>关键词</b>：single image captured, introduce SAOR, approach for estimating, single image, image captured</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce SAOR, a novel approach for estimating the 3D shape, texture, and
viewpoint of an articulated object from a single image captured in the wild.
Unlike prior approaches that rely on pre-defined category-specific 3D templates
or tailored 3D skeletons, SAOR learns to articulate shapes from single-view
image collections with a skeleton-free part-based model without requiring any
3D object shape priors. To prevent ill-posed solutions, we propose a
cross-instance consistency loss that exploits disentangled object shape
deformation and articulation. This is helped by a new silhouette-based sampling
mechanism to enhance viewpoint diversity during training. Our method only
requires estimated object silhouettes and relative depth maps from
off-the-shelf pre-trained networks during training. At inference time, given a
single-view image, it efficiently outputs an explicit mesh representation. We
obtain improved qualitative and quantitative results on challenging quadruped
animals compared to relevant existing work.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Neural Preset for Color Style Transfer</b></summary>
  <p><b>编号</b>：[7]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13511</p>
  <p><b>作者</b>：Zhanghan Ke,  Yuhao Liu,  Lei Zhu,  Nanxuan Zhao,  Rynson W.H. Lau</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：vast memory requirement, Neural Preset technique, Neural Preset, technique to address, address the limitations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we present a Neural Preset technique to address the
limitations of existing color style transfer methods, including visual
artifacts, vast memory requirement, and slow style switching speed. Our method
is based on two core designs. First, we propose Deterministic Neural Color
Mapping (DNCM) to consistently operate on each pixel via an image-adaptive
color mapping matrix, avoiding artifacts and supporting high-resolution inputs
with a small memory footprint. Second, we develop a two-stage pipeline by
dividing the task into color normalization and stylization, which allows
efficient style switching by extracting color styles as presets and reusing
them on normalized input images. Due to the unavailability of pairwise
datasets, we describe how to train Neural Preset via a self-supervised
strategy. Various advantages of Neural Preset over existing methods are
demonstrated through comprehensive evaluations. Besides, we show that our
trained model can naturally support multiple applications without fine-tuning,
including low-light image enhancement, underwater image correction, image
dehazing, and image harmonization.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：MV-JAR: Masked Voxel Jigsaw and Reconstruction for LiDAR-Based  Self-Supervised Pre-Training</b></summary>
  <p><b>编号</b>：[8]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13510</p>
  <p><b>作者</b>：Runsen Xu,  Tai Wang,  Wenwei Zhang,  Runjian Chen,  Jinkun Cao,  Jiangmiao Pang,  Dahua Lin</p>
  <p><b>备注</b>：Accepted by CVPR 2023 with a carefully designed benchmark on Waymo. Codes and the benchmark will be available at this https URL</p>
  <p><b>关键词</b>：Masked Voxel Jigsaw, Jigsaw and Reconstruction, introduces the Masked, Masked Voxel, Voxel Jigsaw</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper introduces the Masked Voxel Jigsaw and Reconstruction (MV-JAR)
method for LiDAR-based self-supervised pre-training and a carefully designed
data-efficient 3D object detection benchmark on the Waymo dataset. Inspired by
the scene-voxel-point hierarchy in downstream 3D object detectors, we design
masking and reconstruction strategies accounting for voxel distributions in the
scene and local point distributions within the voxel. We employ a
Reversed-Furthest-Voxel-Sampling strategy to address the uneven distribution of
LiDAR points and propose MV-JAR, which combines two techniques for modeling the
aforementioned distributions, resulting in superior performance. Our
experiments reveal limitations in previous data-efficient experiments, which
uniformly sample fine-tuning splits with varying data proportions from each
LiDAR sequence, leading to similar data diversity across splits. To address
this, we propose a new benchmark that samples scene sequences for diverse
fine-tuning splits, ensuring adequate model convergence and providing a more
accurate evaluation of pre-training methods. Experiments on our Waymo benchmark
and the KITTI dataset demonstrate that MV-JAR consistently and significantly
improves 3D detection performance across various data scales, achieving up to a
6.3% increase in mAPH compared to training from scratch. Codes and the
benchmark will be available at this https URL .</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Position-Guided Point Cloud Panoptic Segmentation Transformer</b></summary>
  <p><b>编号</b>：[9]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13509</p>
  <p><b>作者</b>：Zeqi Xiao,  Wenwei Zhang,  Tai Wang,  Chen Change Loy,  Dahua Lin,  Jiangmiao Pang</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：unified visual perception, started a trend, visual perception, group of learnable, unified visual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>DEtection TRansformer (DETR) started a trend that uses a group of learnable
queries for unified visual perception. This work begins by applying this
appealing paradigm to LiDAR-based point cloud segmentation and obtains a simple
yet effective baseline. Although the naive adaptation obtains fair results, the
instance segmentation performance is noticeably inferior to previous works. By
diving into the details, we observe that instances in the sparse point clouds
are relatively small to the whole scene and often have similar geometry but
lack distinctive appearance for segmentation, which are rare in the image
domain. Considering instances in 3D are more featured by their positional
information, we emphasize their roles during the modeling and design a robust
Mixed-parameterized Positional Embedding (MPE) to guide the segmentation
process. It is embedded into backbone features and later guides the mask
prediction and query update processes iteratively, leading to Position-Aware
Segmentation (PA-Seg) and Masked Focal Attention (MFA). All these designs impel
the queries to attend to specific regions and identify various instances. The
method, named Position-guided Point cloud Panoptic segmentation transFormer
(P3Former), outperforms previous state-of-the-art methods by 3.4% and 1.2% PQ
on SemanticKITTI and nuScenes benchmark, respectively. The source code and
models are available at this https URL .</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：DreamBooth3D: Subject-Driven Text-to-3D Generation</b></summary>
  <p><b>编号</b>：[10]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13508</p>
  <p><b>作者</b>：Amit Raj,  Srinivas Kaza,  Ben Poole,  Michael Niemeyer,  Nataniel Ruiz,  Ben Mildenhall,  Shiran Zada,  Kfir Aberman,  Michael Rubinstein,  Jonathan Barron,  Yuanzhen Li,  Varun Jampani</p>
  <p><b>备注</b>：Project page at this https URL Video Summary at this https URL</p>
  <p><b>关键词</b>：casually captured images, casually captured, generative models, captured images, approach combines recent</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present DreamBooth3D, an approach to personalize text-to-3D generative
models from as few as 3-6 casually captured images of a subject. Our approach
combines recent advances in personalizing text-to-image models (DreamBooth)
with text-to-3D generation (DreamFusion). We find that naively combining these
methods fails to yield satisfactory subject-specific 3D assets due to
personalized text-to-image models overfitting to the input viewpoints of the
subject. We overcome this through a 3-stage optimization strategy where we
jointly leverage the 3D consistency of neural radiance fields together with the
personalization capability of text-to-image models. Our method can produce
high-quality, subject-specific 3D assets with text-driven modifications such as
novel poses, colors and attributes that are not seen in any of the input images
of the subject.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：A Large-scale Study of Spatiotemporal Representation Learning with a New  Benchmark on Action Recognition</b></summary>
  <p><b>编号</b>：[12]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13505</p>
  <p><b>作者</b>：Andong Deng,  Taojiannan Yang,  Chen Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：specific area, provide a unified, facilitate the evolution, action recognition, unified protocol</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The goal of building a benchmark (suite of datasets) is to provide a unified
protocol for fair evaluation and thus facilitate the evolution of a specific
area. Nonetheless, we point out that existing protocols of action recognition
could yield partial evaluations due to several limitations. To comprehensively
probe the effectiveness of spatiotemporal representation learning, we introduce
BEAR, a new BEnchmark on video Action Recognition. BEAR is a collection of 18
video datasets grouped into 5 categories (anomaly, gesture, daily, sports, and
instructional), which covers a diverse set of real-world applications. With
BEAR, we thoroughly evaluate 6 common spatiotemporal models pre-trained by both
supervised and self-supervised learning. We also report transfer performance
via standard finetuning, few-shot finetuning, and unsupervised domain
adaptation. Our observation suggests that current state-of-the-art cannot
solidly guarantee high performance on datasets close to real-world
applications, and we hope BEAR can serve as a fair and challenging evaluation
benchmark to gain insights on building next-generation spatiotemporal learners.
Our dataset, code, and models are released at:
this https URL</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：ReBotNet: Fast Real-time Video Enhancement</b></summary>
  <p><b>编号</b>：[13]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13504</p>
  <p><b>作者</b>：Jeya Maria Jose Valanarasu,  Rahul Garg,  Andeep Toor,  Xin Tong,  Weijuan Xi,  Andreas Lugmayr,  Vishal M. Patel,  Anne Menini</p>
  <p><b>备注</b>：Project Website: this https URL</p>
  <p><b>关键词</b>：high computational load, real-time video enhancement, video restoration networks, computational load, high computational</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Most video restoration networks are slow, have high computational load, and
can't be used for real-time video enhancement. In this work, we design an
efficient and fast framework to perform real-time video enhancement for
practical use-cases like live video calls and video streams. Our proposed
method, called Recurrent Bottleneck Mixer Network (ReBotNet), employs a
dual-branch framework. The first branch learns spatio-temporal features by
tokenizing the input frames along the spatial and temporal dimensions using a
ConvNext-based encoder and processing these abstract tokens using a bottleneck
mixer. To further improve temporal consistency, the second branch employs a
mixer directly on tokens extracted from individual frames. A common decoder
then merges the features form the two branches to predict the enhanced frame.
In addition, we propose a recurrent training approach where the last frame's
prediction is leveraged to efficiently enhance the current frame while
improving temporal consistency. To evaluate our method, we curate two new
datasets that emulate real-world video call and streaming scenarios, and show
extensive results on multiple datasets where ReBotNet outperforms existing
approaches with lower computations, reduced memory requirements, and faster
inference time.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Chordal Averaging on Flag Manifolds and Its Applications</b></summary>
  <p><b>编号</b>：[14]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13501</p>
  <p><b>作者</b>：Nathan Mankovich,  Tolga Birdal</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：flag manifold, Stiefel manifold, provably-convergent algorithm, chordal metric, paper presents</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a new, provably-convergent algorithm for computing the
flag-mean and flag-median of a set of points on a flag manifold under the
chordal metric. The flag manifold is a mathematical space consisting of flags,
which are sequences of nested subspaces of a vector space that increase in
dimension. The flag manifold is a superset of a wide range of known matrix
groups, including Stiefel and Grassmanians, making it a general object that is
useful in a wide variety computer vision problems.
To tackle the challenge of computing first order flag statistics, we first
transform the problem into one that involves auxiliary variables constrained to
the Stiefel manifold. The Stiefel manifold is a space of orthogonal frames, and
leveraging the numerical stability and efficiency of Stiefel-manifold
optimization enables us to compute the flag-mean effectively. Through a series
of experiments, we show the competence of our method in Grassmann and rotation
averaging, as well as principal component analysis.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：TriPlaneNet: An Encoder for EG3D Inversion</b></summary>
  <p><b>编号</b>：[16]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13497</p>
  <p><b>作者</b>：Ananta R. Bhattarai,  Matthias Nießner,  Artem Sevastopolsky</p>
  <p><b>备注</b>：Video: this https URL Project page: this https URL</p>
  <p><b>关键词</b>：high-fidelity generative modeling, Recent progress, progress in NeRF-based, high-resolution and high-fidelity, modeling of human</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent progress in NeRF-based GANs has introduced a number of approaches for
high-resolution and high-fidelity generative modeling of human heads with a
possibility for novel view rendering. At the same time, one must solve an
inverse problem to be able to re-render or modify an existing image or video.
Despite the success of universal optimization-based methods for 2D GAN
inversion, those, applied to 3D GANs, may fail to produce 3D-consistent
renderings. Fast encoder-based techniques, such as those developed for
StyleGAN, may also be less appealing due to the lack of identity preservation.
In our work, we introduce a real-time method that bridges the gap between the
two approaches by directly utilizing the tri-plane representation introduced
for EG3D generative model. In particular, we build upon a feed-forward
convolutional encoder for the latent code and extend it with a
fully-convolutional predictor of tri-plane numerical offsets. As shown in our
work, the renderings are similar in quality to optimization-based techniques
and significantly outperform the baselines for novel view. As we empirically
prove, this is a consequence of directly operating in the tri-plane space, not
in the GAN parameter space, while making use of an encoder-based trainable
approach.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：The effectiveness of MAE pre-pretraining for billion-scale pretraining</b></summary>
  <p><b>编号</b>：[17]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13496</p>
  <p><b>作者</b>：Mannat Singh,  Quentin Duval,  Kalyan Vasudev Alwala,  Haoqi Fan,  Vaibhav Aggarwal,  Aaron Adcock,  Armand Joulin,  Piotr Dollár,  Christoph Feichtenhofer,  Ross Girshick,  Rohit Girdhar,  Ishan Misra</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：revisits the standard, paper revisits, computer vision, model, billions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper revisits the standard pretrain-then-finetune paradigm used in
computer vision for visual recognition tasks. Typically, state-of-the-art
foundation models are pretrained using large scale (weakly) supervised datasets
with billions of images. We introduce an additional pre-pretraining stage that
is simple and uses the self-supervised MAE technique to initialize the model.
While MAE has only been shown to scale with the size of models, we find that it
scales with the size of the training dataset as well. Thus, our MAE-based
pre-pretraining scales with both model and data size making it applicable for
training foundation models. Pre-pretraining consistently improves both the
model convergence and the downstream transfer performance across a range of
model scales (millions to billions of parameters), and dataset sizes (millions
to billions of images). We measure the effectiveness of pre-pretraining on 10
different visual recognition tasks spanning image classification, video
recognition, object detection, low-shot classification and zero-shot
recognition. Our largest model achieves new state-of-the-art results on
iNaturalist-18 (91.3%), 1-shot ImageNet-1k (62.1%), and zero-shot transfer on
Food-101 (96.0%). Our study reveals that model initialization plays a
significant role, even for web-scale pretraining with billions of images.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：ReVersion: Diffusion-Based Relation Inversion from Images</b></summary>
  <p><b>编号</b>：[18]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13495</p>
  <p><b>作者</b>：Ziqi Huang,  Tianxing Wu,  Yuming Jiang,  Kelvin C.K. Chan,  Ziwei Liu</p>
  <p><b>备注</b>：First two authors contributed equally. Project page: this https URL Code: this https URL</p>
  <p><b>关键词</b>：gain increasing popularity, models gain increasing, relation prompt, Diffusion models gain, Relation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diffusion models gain increasing popularity for their generative
capabilities. Recently, there have been surging needs to generate customized
images by inverting diffusion models from exemplar images. However, existing
inversion methods mainly focus on capturing object appearances. How to invert
object relations, another important pillar in the visual world, remains
unexplored. In this work, we propose ReVersion for the Relation Inversion task,
which aims to learn a specific relation (represented as "relation prompt") from
exemplar images. Specifically, we learn a relation prompt from a frozen
pre-trained text-to-image diffusion model. The learned relation prompt can then
be applied to generate relation-specific images with new objects, backgrounds,
and styles. Our key insight is the "preposition prior" - real-world relation
prompts can be sparsely activated upon a set of basis prepositional words.
Specifically, we propose a novel relation-steering contrastive learning scheme
to impose two critical properties of the relation prompt: 1) The relation
prompt should capture the interaction between objects, enforced by the
preposition prior. 2) The relation prompt should be disentangled away from
object appearances. We further devise relation-focal importance sampling to
emphasize high-level interactions over low-level appearances (e.g., texture,
color). To comprehensively evaluate this new task, we contribute ReVersion
Benchmark, which provides various exemplar images with diverse relations.
Extensive experiments validate the superiority of our approach over existing
methods across a wide range of visual relations.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations</b></summary>
  <p><b>编号</b>：[25]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13483</p>
  <p><b>作者</b>：Joy Hsu,  Jiayuan Mao,  Jiajun Wu</p>
  <p><b>备注</b>：In CVPR 2023</p>
  <p><b>关键词</b>：visually grounded dialogues, artificial intelligence tasks, embodied manipulation, wide range, range of artificial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Grounding object properties and relations in 3D scenes is a prerequisite for
a wide range of artificial intelligence tasks, such as visually grounded
dialogues and embodied manipulation. However, the variability of the 3D domain
induces two fundamental challenges: 1) the expense of labeling and 2) the
complexity of 3D grounded language. Hence, essential desiderata for models are
to be data-efficient, generalize to different data distributions and tasks with
unseen semantic forms, as well as ground complex language semantics (e.g.,
view-point anchoring and multi-object reference). To address these challenges,
we propose NS3D, a neuro-symbolic framework for 3D grounding. NS3D translates
language into programs with hierarchical structures by leveraging large
language-to-code models. Different functional modules in the programs are
implemented as neural networks. Notably, NS3D extends prior neuro-symbolic
visual reasoning methods by introducing functional modules that effectively
reason about high-arity relations (i.e., relations among more than two
objects), key in disambiguating objects in complex 3D scenes. Modular and
compositional architecture enables NS3D to achieve state-of-the-art results on
the ReferIt3D view-dependence task, a 3D referring expression comprehension
benchmark. Importantly, NS3D shows significantly improved performance on
settings of data-efficiency and generalization, and demonstrate zero-shot
transfer to an unseen 3D question-answering task.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：TactoFind: A Tactile Only System for Object Retrieval</b></summary>
  <p><b>编号</b>：[26]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13482</p>
  <p><b>作者</b>：Sameer Pai,  Tao Chen,  Megha Tippur,  Edward Adelson,  Abhishek Gupta,  Pulkit Agrawal</p>
  <p><b>备注</b>：Accepted in ICRA 2023</p>
  <p><b>关键词</b>：sensing is absent, move freely, study the problem, retrieval in scenarios, shapes are unknown</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the problem of object retrieval in scenarios where visual sensing is
absent, object shapes are unknown beforehand and objects can move freely, like
grabbing objects out of a drawer. Successful solutions require localizing free
objects, identifying specific object instances, and then grasping the
identified objects, only using touch feedback. Unlike vision, where cameras can
observe the entire scene, touch sensors are local and only observe parts of the
scene that are in contact with the manipulator. Moreover, information gathering
via touch sensors necessitates applying forces on the touched surface which may
disturb the scene itself. Reasoning with touch, therefore, requires careful
exploration and integration of information over time -- a challenge we tackle.
We present a system capable of using sparse tactile feedback from fingertip
touch sensors on a dexterous hand to localize, identify and grasp novel objects
without any visual feedback. Videos are available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Prior-free Category-level Pose Estimation with Implicit Space  Transformation</b></summary>
  <p><b>编号</b>：[27]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13479</p>
  <p><b>作者</b>：Jianhui Liu,  Yukang Chen,  Xiaoqing Ye,  Xiaojuan Qi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：pose estimation aims, pose estimation, specific category, estimation aims, aims to predict</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Category-level 6D pose estimation aims to predict the poses and sizes of
unseen objects from a specific category. Thanks to prior deformation, which
explicitly adapts a category-specific 3D prior (i.e., a 3D template) to a given
object instance, prior-based methods attained great success and have become a
major research stream. However, obtaining category-specific priors requires
collecting a large amount of 3D models, which is labor-consuming and often not
accessible in practice. This motivates us to investigate whether priors are
necessary to make prior-based methods effective. Our empirical study shows that
the 3D prior itself is not the credit to the high performance. The keypoint
actually is the explicit deformation process, which aligns camera and world
coordinates supervised by world-space 3D models (also called canonical space).
Inspired by these observation, we introduce a simple prior-free implicit space
transformation network, namely IST-Net, to transform camera-space features to
world-space counterparts and build correspondence between them in an implicit
manner without relying on 3D priors. Besides, we design camera- and world-space
enhancers to enrich the features with pose-sensitive information and
geometrical constraints, respectively. Albeit simple, IST-Net becomes the first
prior-free method that achieves state-of-the-art performance, with top
inference speed on the REAL275 dataset. Our code and models will be publicly
available.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：TransPoser: Transformer as an Optimizer for Joint Object Shape and Pose  Estimation</b></summary>
  <p><b>编号</b>：[28]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13477</p>
  <p><b>作者</b>：Yuta Yoshitake,  Mai Nishimura,  Shohei Nobuhara,  Ko Nishino</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：observed RGB-D images, sequentially observed RGB-D, Directional Distance Function, Deep Directional Distance, observed RGB-D</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a novel method for joint estimation of shape and pose of rigid
objects from their sequentially observed RGB-D images. In sharp contrast to
past approaches that rely on complex non-linear optimization, we propose to
formulate it as a neural optimization that learns to efficiently estimate the
shape and pose. We introduce Deep Directional Distance Function (DeepDDF), a
neural network that directly outputs the depth image of an object given the
camera viewpoint and viewing direction, for efficient error computation in 2D
image space. We formulate the joint estimation itself as a Transformer which we
refer to as TransPoser. We fully leverage the tokenization and multi-head
attention to sequentially process the growing set of observations and to
efficiently update the shape and pose with a learned momentum, respectively.
Experimental results on synthetic and real data show that DeepDDF achieves high
accuracy as a category-level object shape representation and TransPoser
achieves state-of-the-art accuracy efficiently for joint shape and pose
estimation.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Plotting Behind the Scenes: Towards Learnable Game Engines</b></summary>
  <p><b>编号</b>：[30]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13472</p>
  <p><b>作者</b>：Willi Menapace,  Aliaksandr Siarohin,  Stéphane Lathuilière,  Panos Achlioptas,  Vladislav Golyanik,  Elisa Ricci,  Sergey Tulyakov</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Game, Learnable Game Engine, computer graphics, powerful tools, tools in computer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Game engines are powerful tools in computer graphics. Their power comes at
the immense cost of their development. In this work, we present a framework to
train game-engine-like neural models, solely from monocular annotated videos.
The result-a Learnable Game Engine (LGE)-maintains states of the scene, objects
and agents in it, and enables rendering the environment from a controllable
viewpoint. Similarly to a game engine, it models the logic of the game and the
underlying rules of physics, to make it possible for a user to play the game by
specifying both high- and low-level action sequences. Most captivatingly, our
LGE unlocks the director's mode, where the game is played by plotting behind
the scenes, specifying high-level actions and goals for the agents in the form
of language and desired states. This requires learning "game AI", encapsulated
by our animation model, to navigate the scene using high-level constraints,
play against an adversary, devise the strategy to win a point. The key to
learning such game AI is the exploitation of a large and diverse text corpus,
collected in this work, describing detailed actions in a game and used to train
our animation model. To render the resulting state of the environment and its
agents, we use a compositional NeRF representation used in our synthesis model.
To foster future research, we present newly collected, annotated and calibrated
large-scale Tennis and Minecraft datasets. Our method significantly outperforms
existing neural video game simulators in terms of rendering quality. Besides,
our LGEs unlock applications beyond capabilities of the current state of the
art. Our framework, data, and models are available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Egocentric Audio-Visual Object Localization</b></summary>
  <p><b>编号</b>：[31]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13471</p>
  <p><b>作者</b>：Chao Huang,  Yapeng Tian,  Anurag Kumar,  Chenliang Xu</p>
  <p><b>备注</b>：Accepted by CVPR 2023</p>
  <p><b>关键词</b>：naturally perceive surrounding, perceive surrounding scenes, Humans naturally perceive, perceive surrounding, first-person view</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Humans naturally perceive surrounding scenes by unifying sound and sight in a
first-person view. Likewise, machines are advanced to approach human
intelligence by learning with multisensory inputs from an egocentric
perspective. In this paper, we explore the challenging egocentric audio-visual
object localization task and observe that 1) egomotion commonly exists in
first-person recordings, even within a short duration; 2) The out-of-view sound
components can be created while wearers shift their attention. To address the
first problem, we propose a geometry-aware temporal aggregation module to
handle the egomotion explicitly. The effect of egomotion is mitigated by
estimating the temporal geometry transformation and exploiting it to update
visual representations. Moreover, we propose a cascaded feature enhancement
module to tackle the second issue. It improves cross-modal localization
robustness by disentangling visually-indicated audio representation. During
training, we take advantage of the naturally available audio-visual temporal
synchronization as the ``free'' self-supervision to avoid costly labeling. We
also annotate and create the Epic Sounding Object dataset for evaluation
purposes. Extensive experiments show that our method achieves state-of-the-art
localization performance in egocentric videos and can be generalized to diverse
audio-visual scenes.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：CoBIT: A Contrastive Bi-directional Image-Text Generation Model</b></summary>
  <p><b>编号</b>：[36]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13455</p>
  <p><b>作者</b>：Haoxuan You,  Mandy Guo,  Zhecan Wang,  Kai-Wei Chang,  Jason Baldridge,  Jiahui Yu</p>
  <p><b>备注</b>：14 pages, 5 figures</p>
  <p><b>关键词</b>：pre-trained foundation models, field of vision, vision and language, language has witnessed, witnessed a proliferation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The field of vision and language has witnessed a proliferation of pre-trained
foundation models. Most existing methods are independently pre-trained with
contrastive objective like CLIP, image-to-text generative objective like PaLI,
or text-to-image generative objective like Parti. However, the three objectives
can be pre-trained on the same data, image-text pairs, and intuitively they
complement each other as contrasting provides global alignment capacity and
generation grants fine-grained understanding. In this work, we present a
Contrastive Bi-directional Image-Text generation model (CoBIT), which attempts
to unify the three pre-training objectives in one framework. Specifically,
CoBIT employs a novel unicoder-decoder structure, consisting of an image
unicoder, a text unicoder and a cross-modal decoder. The image/text unicoders
can switch between encoding and decoding in different tasks, enabling
flexibility and shared knowledge that benefits both image-to-text and
text-to-image generations. CoBIT achieves superior performance in image
understanding, image-text understanding (Retrieval, Captioning, VQA, SNLI-VE)
and text-based content creation, particularly in zero-shot scenarios. For
instance, 82.7% in zero-shot ImageNet classification, 9.37 FID score in
zero-shot text-to-image generation and 44.8 CIDEr in zero-shot captioning.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Set-the-Scene: Global-Local Training for Generating Controllable NeRF  Scenes</b></summary>
  <p><b>编号</b>：[39]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13450</p>
  <p><b>作者</b>：Dana Cohen-Bar,  Elad Richardson,  Gal Metzer,  Raja Giryes,  Daniel Cohen-Or</p>
  <p><b>备注</b>：project page at this https URL</p>
  <p><b>关键词</b>：text-guided image generation, breakthroughs in text-guided, text-guided image, image generation, generation have led</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent breakthroughs in text-guided image generation have led to remarkable
progress in the field of 3D synthesis from text. By optimizing neural radiance
fields (NeRF) directly from text, recent methods are able to produce remarkable
results. Yet, these methods are limited in their control of each object's
placement or appearance, as they represent the scene as a whole. This can be a
major issue in scenarios that require refining or manipulating objects in the
scene. To remedy this deficit, we propose a novel GlobalLocal training
framework for synthesizing a 3D scene using object proxies. A proxy represents
the object's placement in the generated scene and optionally defines its coarse
geometry. The key to our approach is to represent each object as an independent
NeRF. We alternate between optimizing each NeRF on its own and as part of the
full scene. Thus, a complete representation of each object can be learned,
while also creating a harmonious scene with style and lighting match. We show
that using proxies allows a wide variety of editing options, such as adjusting
the placement of each independent object, removing objects from a scene, or
refining an object. Our results show that Set-the-Scene offers a powerful
solution for scene synthesis and manipulation, filling a crucial gap in
controllable text-to-3D synthesis.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：CLIP for All Things Zero-Shot Sketch-Based Image Retrieval, Fine-Grained  or Not</b></summary>
  <p><b>编号</b>：[42]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13440</p>
  <p><b>作者</b>：Aneeshan Sain,  Ayan Kumar Bhunia,  Pinaki Nath Chowdhury,  Subhadeep Koley,  Tao Xiang,  Yi-Zhe Song</p>
  <p><b>备注</b>：Accepted in Computer Vision and Pattern Recognition (CVPR), 2023</p>
  <p><b>关键词</b>：based image retrieval, zero-shot sketch based, sketch based image, image retrieval, based image</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we leverage CLIP for zero-shot sketch based image retrieval
(ZS-SBIR). We are largely inspired by recent advances on foundation models and
the unparalleled generalisation ability they seem to offer, but for the first
time tailor it to benefit the sketch community. We put forward novel designs on
how best to achieve this synergy, for both the category setting and the
fine-grained setting ("all"). At the very core of our solution is a prompt
learning setup. First we show just via factoring in sketch-specific prompts, we
already have a category-level ZS-SBIR system that overshoots all prior arts, by
a large margin (24.8%) - a great testimony on studying the CLIP and ZS-SBIR
synergy. Moving onto the fine-grained setup is however trickier, and requires a
deeper dive into this synergy. For that, we come up with two specific designs
to tackle the fine-grained matching nature of the problem: (i) an additional
regularisation loss to ensure the relative separation between sketches and
photos is uniform across categories, which is not the case for the gold
standard standalone triplet loss, and (ii) a clever patch shuffling technique
to help establishing instance-level structural correspondences between
sketch-photo pairs. With these designs, we again observe significant
performance gains in the region of 26.9% over previous state-of-the-art. The
take-home message, if any, is the proposed CLIP and prompt learning paradigm
carries great promise in tackling other sketch-related tasks (not limited to
ZS-SBIR) where data scarcity remains a great challenge. Code and models will be
made available.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video  Generators</b></summary>
  <p><b>编号</b>：[43]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13439</p>
  <p><b>作者</b>：Levon Khachatryan,  Andranik Movsisyan,  Vahram Tadevosyan,  Roberto Henschel,  Zhangyang Wang,  Shant Navasardyan,  Humphrey Shi</p>
  <p><b>备注</b>：The project is available at: this https URL</p>
  <p><b>关键词</b>：computationally heavy training, large-scale video datasets, require large-scale video, Stable Diffusion, rely on computationally</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent text-to-video generation approaches rely on computationally heavy
training and require large-scale video datasets. In this paper, we introduce a
new task of zero-shot text-to-video generation and propose a low-cost approach
(without any training or optimization) by leveraging the power of existing
text-to-image synthesis methods (e.g., Stable Diffusion), making them suitable
for the video domain.
Our key modifications include (i) enriching the latent codes of the generated
frames with motion dynamics to keep the global scene and the background time
consistent; and (ii) reprogramming frame-level self-attention using a new
cross-frame attention of each frame on the first frame, to preserve the
context, appearance, and identity of the foreground object.
Experiments show that this leads to low overhead, yet high-quality and
remarkably consistent video generation. Moreover, our approach is not limited
to text-to-video synthesis but is also applicable to other tasks such as
conditional and content-specialized video generation, and Video
Instruct-Pix2Pix, i.e., instruction-guided video editing.
As experiments show, our method performs comparably or sometimes better than
recent approaches, despite not being trained on additional video data. Our code
will be open sourced at: this https URL .</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Patch-Mix Transformer for Unsupervised Domain Adaptation: A Game  Perspective</b></summary>
  <p><b>编号</b>：[44]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13434</p>
  <p><b>作者</b>：Jinjing Zhu,  Haotian Bai,  Lin Wang</p>
  <p><b>备注</b>：Accepted by CVPR 2023 (Highlight)</p>
  <p><b>关键词</b>：unsupervised domain adaptation, challenging unsupervised domain, vision transformer, recently made, challenging unsupervised</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Endeavors have been recently made to leverage the vision transformer (ViT)
for the challenging unsupervised domain adaptation (UDA) task. They typically
adopt the cross-attention in ViT for direct domain alignment. However, as the
performance of cross-attention highly relies on the quality of pseudo labels
for targeted samples, it becomes less effective when the domain gap becomes
large. We solve this problem from a game theory's perspective with the proposed
model dubbed as PMTrans, which bridges source and target domains with an
intermediate domain. Specifically, we propose a novel ViT-based module called
PatchMix that effectively builds up the intermediate domain, i.e., probability
distribution, by learning to sample patches from both domains based on the
game-theoretical models. This way, it learns to mix the patches from the source
and target domains to maximize the cross entropy (CE), while exploiting two
semi-supervised mixup losses in the feature and label spaces to minimize it. As
such, we interpret the process of UDA as a min-max CE game with three players,
including the feature extractor, classifier, and PatchMix, to find the Nash
Equilibria. Moreover, we leverage attention maps from ViT to re-weight the
label of each patch by its importance, making it possible to obtain more
domain-discriminative feature representations. We conduct extensive experiments
on four benchmark datasets, and the results show that PMTrans significantly
surpasses the ViT-based and CNN-based SoTA methods by +3.6% on Office-Home,
+1.4% on Office-31, and +17.7% on DomainNet, respectively.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Medical diffusion on a budget: textual inversion for medical image  generation</b></summary>
  <p><b>编号</b>：[45]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13430</p>
  <p><b>作者</b>：Bram de Wilde,  Anindo Saha,  Richard P.G. ten Broek,  Henkjan Huisman</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：gained immense popularity, immense popularity due, advancements in efficiency, gained immense, immense popularity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diffusion-based models for text-to-image generation have gained immense
popularity due to recent advancements in efficiency, accessibility, and
quality. Although it is becoming increasingly feasible to perform inference
with these systems using consumer-grade GPUs, training them from scratch still
requires access to large datasets and significant computational resources. In
the case of medical image generation, the availability of large, publicly
accessible datasets that include text reports is limited due to legal and
ethical concerns. While training a diffusion model on a private dataset may
address this issue, it is not always feasible for institutions lacking the
necessary computational resources. This work demonstrates that pre-trained
Stable Diffusion models, originally trained on natural images, can be adapted
to various medical imaging modalities by training text embeddings with textual
inversion. In this study, we conducted experiments using medical datasets
comprising only 100 samples from three medical modalities. Embeddings were
trained in a matter of hours, while still retaining diagnostic relevance in
image generation. Experiments were designed to achieve several objectives.
Firstly, we fine-tuned the training and inference processes of textual
inversion, revealing that larger embeddings and more examples are required.
Secondly, we validated our approach by demonstrating a 2\% increase in the
diagnostic accuracy (AUC) for detecting prostate cancer on MRI, which is a
challenging multi-modal imaging modality, from 0.78 to 0.80. Thirdly, we
performed simulations by interpolating between healthy and diseased states,
combining multiple pathologies, and inpainting to show embedding flexibility
and control of disease appearance. Finally, the embeddings trained in this
study are small (less than 1 MB), which facilitates easy sharing of medical
data with reduced privacy concerns.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Low-Light Image Enhancement by Learning Contrastive Representations in  Spatial and Frequency Domains</b></summary>
  <p><b>编号</b>：[50]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13412</p>
  <p><b>作者</b>：Yi Huang,  Xiaoguang Tu,  Gui Fu,  Tingting Liu,  Bokai Liu,  Ming Yang,  Ziliang Feng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：decrease image quality, low-light conditions tend, low-light conditions, poor visibility, downstream tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Images taken under low-light conditions tend to suffer from poor visibility,
which can decrease image quality and even reduce the performance of the
downstream tasks. It is hard for a CNN-based method to learn generalized
features that can recover normal images from the ones under various unknow
low-light conditions. In this paper, we propose to incorporate the contrastive
learning into an illumination correction network to learn abstract
representations to distinguish various low-light conditions in the
representation space, with the purpose of enhancing the generalizability of the
network. Considering that light conditions can change the frequency components
of the images, the representations are learned and compared in both spatial and
frequency domains to make full advantage of the contrastive learning. The
proposed method is evaluated on LOL and LOL-V2 datasets, the results show that
the proposed method achieves better qualitative and quantitative results
compared with other state-of-the-arts.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：SC-MIL: Supervised Contrastive Multiple Instance Learning for Imbalanced  Classification in Pathology</b></summary>
  <p><b>编号</b>：[52]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13405</p>
  <p><b>作者</b>：Dinkar Juyal,  Siddhant Shingi,  Syed Ashar Javed,  Harshith Padigela,  Chintan Shah,  Anand Sampat,  Archit Khosla,  John Abel,  Amaro Taylor-Weiner</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：gigapixel-sized images, predict biomarkers, biomarkers and risk-stratify, risk-stratify patients, patients from gigapixel-sized</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multiple Instance learning (MIL) models have been extensively used in
pathology to predict biomarkers and risk-stratify patients from gigapixel-sized
images. Machine learning problems in medical imaging often deal with rare
diseases, making it important for these models to work in a label-imbalanced
setting. Furthermore, these imbalances can occur in out-of-distribution (OOD)
datasets when the models are deployed in the real-world. We leverage the idea
that decoupling feature and classifier learning can lead to improved decision
boundaries for label imbalanced datasets. To this end, we investigate the
integration of supervised contrastive learning with multiple instance learning
(SC-MIL). Specifically, we propose a joint-training MIL framework in the
presence of label imbalance that progressively transitions from learning
bag-level representations to optimal classifier learning. We perform
experiments with different imbalance settings for two well-studied problems in
cancer pathology: subtyping of non-small cell lung cancer and subtyping of
renal cell carcinoma. SC-MIL provides large and consistent improvements over
other techniques on both in-distribution (ID) and OOD held-out sets across
multiple imbalanced settings.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Optimization and Optimizers for Adversarial Robustness</b></summary>
  <p><b>编号</b>：[54]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13401</p>
  <p><b>作者</b>：Hengyue Liang,  Buyun Liang,  Le Peng,  Ying Cui,  Tim Mitchell,  Ju Sun</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：nontrivial constrained optimization, deep learning models, entails solving nontrivial, solving nontrivial constrained, deep learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Empirical robustness evaluation (RE) of deep learning models against
adversarial perturbations entails solving nontrivial constrained optimization
problems. Existing numerical algorithms that are commonly used to solve them in
practice predominantly rely on projected gradient, and mostly handle
perturbations modeled by the $\ell_1$, $\ell_2$ and $\ell_\infty$ distances. In
this paper, we introduce a novel algorithmic framework that blends a
general-purpose constrained-optimization solver PyGRANSO with Constraint
Folding (PWCF), which can add more reliability and generality to the
state-of-the-art RE packages, e.g., AutoAttack. Regarding reliability, PWCF
provides solutions with stationarity measures and feasibility tests to assess
the solution quality. For generality, PWCF can handle perturbation models that
are typically inaccessible to the existing projected gradient methods; the main
requirement is the distance metric to be almost everywhere differentiable.
Taking advantage of PWCF and other existing numerical algorithms, we further
explore the distinct patterns in the solutions found for solving these
optimization problems using various combinations of losses, perturbation
models, and optimization algorithms. We then discuss the implications of these
patterns on the current robustness evaluation and adversarial training.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Multi-granularity Interaction Simulation for Unsupervised Interactive  Segmentation</b></summary>
  <p><b>编号</b>：[55]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13399</p>
  <p><b>作者</b>：Kehan Li,  Yian Zhao,  Zhennan Wang,  Zesen Cheng,  Peng Jin,  Xiangyang Ji,  Li Yuan,  Chang Liu,  Jie Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：medical image analysis, segmentation enables users, image analysis, image editing, medical image</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Interactive segmentation enables users to segment as needed by providing cues
of objects, which introduces human-computer interaction for many fields, such
as image editing and medical image analysis. Typically, massive and expansive
pixel-level annotations are spent to train deep models by object-oriented
interactions with manually labeled object masks. In this work, we reveal that
informative interactions can be made by simulation with semantic-consistent yet
diverse region exploration in an unsupervised paradigm. Concretely, we
introduce a Multi-granularity Interaction Simulation (MIS) approach to open up
a promising direction for unsupervised interactive segmentation. Drawing on the
high-quality dense features produced by recent self-supervised models, we
propose to gradually merge patches or regions with similar features to form
more extensive regions and thus, every merged region serves as a
semantic-meaningful multi-granularity proposal. By randomly sampling these
proposals and simulating possible interactions based on them, we provide
meaningful interaction at multiple granularities to teach the model to
understand interactions. Our MIS significantly outperforms non-deep learning
unsupervised methods and is even comparable with some previous deep-supervised
methods without any annotation.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：DDT: A Diffusion-Driven Transformer-based Framework for Human Mesh  Recovery from a Video</b></summary>
  <p><b>编号</b>：[56]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13397</p>
  <p><b>作者</b>：Ce Zheng,  Guo-Jun Qi,  Chen Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：rich human body, human-computer interaction, human body information, Human mesh recovery, virtual reality</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Human mesh recovery (HMR) provides rich human body information for various
real-world applications such as gaming, human-computer interaction, and virtual
reality. Compared to single image-based methods, video-based methods can
utilize temporal information to further improve performance by incorporating
human body motion priors. However, many-to-many approaches such as VIBE suffer
from motion smoothness and temporal inconsistency. While many-to-one approaches
such as TCMR and MPS-Net rely on the future frames, which is non-causal and
time inefficient during inference. To address these challenges, a novel
Diffusion-Driven Transformer-based framework (DDT) for video-based HMR is
presented. DDT is designed to decode specific motion patterns from the input
sequence, enhancing motion smoothness and temporal consistency. As a
many-to-many approach, the decoder of our DDT outputs the human mesh of all the
frames, making DDT more viable for real-world applications where time
efficiency is crucial and a causal model is desired. Extensive experiments are
conducted on the widely used datasets (Human3.6M, MPI-INF-3DHP, and 3DPW),
which demonstrated the effectiveness and efficiency of our DDT.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Zero-guidance Segmentation Using Zero Segment Labels</b></summary>
  <p><b>编号</b>：[57]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13396</p>
  <p><b>作者</b>：Pitchaporn Rewatbowornwong,  Nattanat Chatthee,  Ekapol Chuangsuwanich,  Supasorn Suwajanakorn</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：joint vision-language applications, exciting joint vision-language, arbitrary text query, vision-language applications, exciting joint</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>CLIP has enabled new and exciting joint vision-language applications, one of
which is open-vocabulary segmentation, which can locate any segment given an
arbitrary text query. In our research, we ask whether it is possible to
discover semantic segments without any user guidance in the form of text
queries or predefined classes, and label them using natural language
automatically? We propose a novel problem zero-guidance segmentation and the
first baseline that leverages two pre-trained generalist models, DINO and CLIP,
to solve this problem without any fine-tuning or segmentation dataset. The
general idea is to first segment an image into small over-segments, encode them
into CLIP's visual-language space, translate them into text labels, and merge
semantically similar segments together. The key challenge, however, is how to
encode a visual segment into a segment-specific embedding that balances global
and local context information, both useful for recognition. Our main
contribution is a novel attention-masking technique that balances the two
contexts by analyzing the attention layers inside CLIP. We also introduce
several metrics for the evaluation of this new task. With CLIP's innate
knowledge, our method can precisely locate the Mona Lisa painting among a
museum crowd. Project page: this https URL.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis</b></summary>
  <p><b>编号</b>：[59]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13391</p>
  <p><b>作者</b>：Chantal Pellegrini,  Matthias Keicher,  Ege Özsoy,  Petra Jiraskova,  Rickmer Braren,  Nassir Navab</p>
  <p><b>备注</b>：9 pages, 2 figures, 6 tables</p>
  <p><b>关键词</b>：diagnosis, resource to support, support clinical decision-making, clinical, Automated diagnosis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automated diagnosis prediction from medical images is a valuable resource to
support clinical decision-making. However, such systems usually need to be
trained on large amounts of annotated data, which often is scarce in the
medical domain. Zero-shot methods address this challenge by allowing a flexible
adaption to new settings with different clinical findings without relying on
labeled data. Further, to integrate automated diagnosis in the clinical
workflow, methods should be transparent and explainable, increasing medical
professionals' trust and facilitating correctness verification. In this work,
we introduce Xplainer, a novel framework for explainable zero-shot diagnosis in
the clinical setting. Xplainer adapts the classification-by-description
approach of contrastive vision-language models to the multi-label medical
diagnosis task. Specifically, instead of directly predicting a diagnosis, we
prompt the model to classify the existence of descriptive observations, which a
radiologist would look for on an X-Ray scan, and use the descriptor
probabilities to estimate the likelihood of a diagnosis. Our model is
explainable by design, as the final diagnosis prediction is directly based on
the prediction of the underlying descriptors. We evaluate Xplainer on two chest
X-ray datasets, CheXpert and ChestX-ray14, and demonstrate its effectiveness in
improving the performance and explainability of zero-shot diagnosis. Our
results suggest that Xplainer provides a more detailed understanding of the
decision-making process and can be a valuable tool for clinical diagnosis.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Plug-and-Play Regulators for Image-Text Matching</b></summary>
  <p><b>编号</b>：[70]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13371</p>
  <p><b>作者</b>：Haiwen Diao,  Ying Zhang,  Wei Liu,  Xiang Ruan,  Huchuan Lu</p>
  <p><b>备注</b>：13 pages, 9 figures, Accepted by TIP2023</p>
  <p><b>关键词</b>：shown great potential, Exploiting fine-grained correspondence, Exploiting fine-grained, image-text matching, shown great</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Exploiting fine-grained correspondence and visual-semantic alignments has
shown great potential in image-text matching. Generally, recent approaches
first employ a cross-modal attention unit to capture latent region-word
interactions, and then integrate all the alignments to obtain the final
similarity. However, most of them adopt one-time forward association or
aggregation strategies with complex architectures or additional information,
while ignoring the regulation ability of network feedback. In this paper, we
develop two simple but quite effective regulators which efficiently encode the
message output to automatically contextualize and aggregate cross-modal
representations. Specifically, we propose (i) a Recurrent Correspondence
Regulator (RCR) which facilitates the cross-modal attention unit progressively
with adaptive attention factors to capture more flexible correspondence, and
(ii) a Recurrent Aggregation Regulator (RAR) which adjusts the aggregation
weights repeatedly to increasingly emphasize important alignments and dilute
unimportant ones. Besides, it is interesting that RCR and RAR are
plug-and-play: both of them can be incorporated into many frameworks based on
cross-modal interaction to obtain significant benefits, and their cooperation
achieves further improvements. Extensive experiments on MSCOCO and Flickr30K
datasets validate that they can bring an impressive and consistent R@1 gain on
multiple models, confirming the general effectiveness and generalization
ability of the proposed methods. Code and pre-trained models are available at:
this https URL.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：POTTER: Pooling Attention Transformer for Efficient Human Mesh Recovery</b></summary>
  <p><b>编号</b>：[76]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13357</p>
  <p><b>作者</b>：Ce Zheng,  Xianpeng Liu,  Guo-Jun Qi,  Chen Chen</p>
  <p><b>备注</b>：CVPR 2023</p>
  <p><b>关键词</b>：achieved SOTA performance, human mesh recovery, accurate human mesh, human mesh, mesh recovery</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transformer architectures have achieved SOTA performance on the human mesh
recovery (HMR) from monocular images. However, the performance gain has come at
the cost of substantial memory and computational overhead. A lightweight and
efficient model to reconstruct accurate human mesh is needed for real-world
applications. In this paper, we propose a pure transformer architecture named
POoling aTtention TransformER (POTTER) for the HMR task from single images.
Observing that the conventional attention module is memory and computationally
expensive, we propose an efficient pooling attention module, which
significantly reduces the memory and computational cost without sacrificing
performance. Furthermore, we design a new transformer architecture by
integrating a High-Resolution (HR) stream for the HMR task. The high-resolution
local and global features from the HR stream can be utilized for recovering
more accurate human mesh. Our POTTER outperforms the SOTA method METRO by only
requiring 7% of total parameters and 14% of the Multiply-Accumulate Operations
on the Human3.6M (PA-MPJPE metric) and 3DPW (all three metrics) datasets. The
project webpage is this https URL.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Trajectory-Prediction with Vision: A Survey</b></summary>
  <p><b>编号</b>：[78]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13354</p>
  <p><b>作者</b>：Apoorv Singh</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：anticipate future trajectories, efficient route, plan a safe, safe and efficient, autonomous vehicle</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To plan a safe and efficient route, an autonomous vehicle should anticipate
future trajectories of other agents around it. Trajectory prediction is an
extremely challenging task which recently gained a lot of attention in the
autonomous vehicle research community. Trajectory-prediction forecasts future
state of all the dynamic agents in the scene given their current and past
states. A good prediction model can prevent collisions on the road, and hence
the ultimate goal for autonomous vehicles: Collision rate: collisions per
Million miles. The objective of this paper is to provide an overview of the
field trajectory-prediction. We categorize the relevant algorithms into
different classes so that researchers can follow through the trends in the
trajectory-prediction research field. Moreover we also touch upon the
background knowledge required to formulate a trajectory-prediction problem.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Increasing Textual Context Size Boosts Medical Image-Text Matching</b></summary>
  <p><b>编号</b>：[82]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13340</p>
  <p><b>作者</b>：Idan Glassberg,  Tom Hope</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：short technical report, technical report demonstrates, image-text matching tasks, short technical, technical report</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This short technical report demonstrates a simple technique that yields state
of the art results in medical image-text matching tasks. We analyze the use of
OpenAI's CLIP, a general image-text matching model, and observe that CLIP's
limited textual input size has negative impact on downstream performance in the
medical domain where encoding longer textual contexts is often required. We
thus train and release ClipMD, which is trained with a simple sliding window
technique to encode textual captions. ClipMD was tested on two medical
image-text datasets and compared with other image-text matching models. The
results show that ClipMD outperforms other models on both datasets by a large
margin. We make our code and pretrained model publicly available.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：DARE-GRAM : Unsupervised Domain Adaptation Regression by Aligning  Inverse Gram Matrices</b></summary>
  <p><b>编号</b>：[85]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13325</p>
  <p><b>作者</b>：Ismail Nejjar,  Qin Wang,  Olga Fink</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：labeled source dataset, unlabelled target dataset, Domain Adaptation Regression, Unsupervised Domain Adaptation, Domain Adaptation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Unsupervised Domain Adaptation Regression (DAR) aims to bridge the domain gap
between a labeled source dataset and an unlabelled target dataset for
regression problems. Recent works mostly focus on learning a deep feature
encoder by minimizing the discrepancy between source and target features. In
this work, we present a different perspective for the DAR problem by analyzing
the closed-form ordinary least square~(OLS) solution to the linear regressor in
the deep domain adaptation context. Rather than aligning the original feature
embedding space, we propose to align the inverse Gram matrix of the features,
which is motivated by its presence in the OLS solution and the Gram matrix's
ability to capture the feature correlations. Specifically, we propose a simple
yet effective DAR method which leverages the pseudo-inverse low-rank property
to align the scale and angle in a selected subspace generated by the
pseudo-inverse Gram matrix of the two domains. We evaluate our method on three
domain adaptation regression benchmarks. Experimental results demonstrate that
our method achieves state-of-the-art performance. Our code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Improving Generalization with Domain Convex Game</b></summary>
  <p><b>编号</b>：[95]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13297</p>
  <p><b>作者</b>：Fangrui Lv,  Jian Liang,  Shuang Li,  Jinming Zhang,  Di Liu</p>
  <p><b>备注</b>：accepted by CVPR 2023</p>
  <p><b>关键词</b>：deep neural networks, poor generalization capability, multiple source domains, alleviate the poor, capability of deep</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Domain generalization (DG) tends to alleviate the poor generalization
capability of deep neural networks by learning model with multiple source
domains. A classical solution to DG is domain augmentation, the common belief
of which is that diversifying source domains will be conducive to the
out-of-distribution generalization. However, these claims are understood
intuitively, rather than mathematically. Our explorations empirically reveal
that the correlation between model generalization and the diversity of domains
may be not strictly positive, which limits the effectiveness of domain
augmentation. This work therefore aim to guarantee and further enhance the
validity of this strand. To this end, we propose a new perspective on DG that
recasts it as a convex game between domains. We first encourage each
diversified domain to enhance model generalization by elaborately designing a
regularization term based on supermodularity. Meanwhile, a sample filter is
constructed to eliminate low-quality samples, thereby avoiding the impact of
potentially harmful information. Our framework presents a new avenue for the
formal analysis of DG, heuristic analysis and extensive experiments demonstrate
the rationality and effectiveness.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Considerations on the Evaluation of Biometric Quality Assessment  Algorithms</b></summary>
  <p><b>编号</b>：[96]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13294</p>
  <p><b>作者</b>：Torsten Schlett,  Christian Rathgeb,  Juan Tapia,  Christoph Busch</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Quality assessment, Quality assessment algorithms, assessment, assessment algorithms, Quality</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Quality assessment algorithms can be used to estimate the utility of a
biometric sample for the purpose of biometric recognition. "Error versus
Discard Characteristic" (EDC) plots, and "partial Area Under Curve" (pAUC)
values of curves therein, are generally used by researchers to evaluate the
predictive performance of such quality assessment algorithms. An EDC curve
depends on an error type such as the "False Non Match Rate" (FNMR), a quality
assessment algorithm, a biometric recognition system, a set of comparisons each
corresponding to a biometric sample pair, and a comparison score threshold
corresponding to a starting error. To compute an EDC curve, comparisons are
progressively discarded based on the associated samples' lowest quality scores,
and the error is computed for the remaining comparisons. Additionally, a
discard fraction limit or range must be selected to compute pAUC values, which
can then be used to quantitatively rank quality assessment algorithms.
This paper discusses and analyses various details for this kind of quality
assessment algorithm evaluation, including general EDC properties,
interpretability improvements for pAUC values based on a hard lower error limit
and a soft upper error limit, the use of relative instead of discrete rankings,
stepwise vs. linear curve interpolation, and normalisation of quality scores to
a [0, 100] integer range. We also analyse the stability of quantitative quality
assessment algorithm rankings based on pAUC values across varying pAUC discard
fraction limits and starting errors, concluding that higher pAUC discard
fraction limits should be preferred. The analyses are conducted both with
synthetic data and with real data for a face image quality assessment scenario,
with a focus on general modality-independent conclusions for EDC evaluations.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：LABRAD-OR: Lightweight Memory Scene Graphs for Accurate Bimodal  Reasoning in Dynamic Operating Rooms</b></summary>
  <p><b>编号</b>：[97]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13293</p>
  <p><b>作者</b>：Ege Özsoy,  Tobias Czempiel,  Felix Holm,  Chantal Pellegrini,  Nassir Navab</p>
  <p><b>备注</b>：11 pages, 3 figures</p>
  <p><b>关键词</b>：including ever-changing interactions, scene graphs, memory scene graphs, Modern surgeries, dynamic settings</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Modern surgeries are performed in complex and dynamic settings, including
ever-changing interactions between medical staff, patients, and equipment. The
holistic modeling of the operating room (OR) is, therefore, a challenging but
essential task, with the potential to optimize the performance of surgical
teams and aid in developing new surgical technologies to improve patient
outcomes. The holistic representation of surgical scenes as semantic scene
graphs (SGG), where entities are represented as nodes and relations between
them as edges, is a promising direction for fine-grained semantic OR
understanding. We propose, for the first time, the use of temporal information
for more accurate and consistent holistic OR modeling. Specifically, we
introduce memory scene graphs, where the scene graphs of previous time steps
act as the temporal representation guiding the current prediction. We design an
end-to-end architecture that intelligently fuses the temporal information of
our lightweight memory scene graphs with the visual information from point
clouds and images. We evaluate our method on the 4D-OR dataset and demonstrate
that integrating temporality leads to more accurate and consistent results
achieving an +5% increase and a new SOTA of 0.88 in macro F1. This work opens
the path for representing the entire surgery history with memory scene graphs
and improves the holistic understanding in the OR. Introducing scene graphs as
memory representations can offer a valuable tool for many temporal
understanding tasks.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：Unsupervised Deep Probabilistic Approach for Partial Point Cloud  Registration</b></summary>
  <p><b>编号</b>：[98]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13290</p>
  <p><b>作者</b>：Guofeng Mei,  Hao Tang,  Xiaoshui Huang,  Weijie Wang,  Juan Liu,  Jian Zhang,  Luc Van Gool,  Qiang Wu</p>
  <p><b>备注</b>：CVPR 2023</p>
  <p><b>关键词</b>：methods face challenges, registration methods face, point cloud registration, cloud registration methods, point clouds</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep point cloud registration methods face challenges to partial overlaps and
rely on labeled data. To address these issues, we propose UDPReg, an
unsupervised deep probabilistic registration framework for point clouds with
partial overlaps. Specifically, we first adopt a network to learn posterior
probability distributions of Gaussian mixture models (GMMs) from point clouds.
To handle partial point cloud registration, we apply the Sinkhorn algorithm to
predict the distribution-level correspondences under the constraint of the
mixing weights of GMMs. To enable unsupervised learning, we design three
distribution consistency-based losses: self-consistency, cross-consistency, and
local contrastive. The self-consistency loss is formulated by encouraging GMMs
in Euclidean and feature spaces to share identical posterior distributions. The
cross-consistency loss derives from the fact that the points of two partially
overlapping point clouds belonging to the same clusters share the cluster
centroids. The cross-consistency loss allows the network to flexibly learn a
transformation-invariant posterior distribution of two aligned point clouds.
The local contrastive loss facilitates the network to extract discriminative
local features. Our UDPReg achieves competitive performance on the
3DMatch/3DLoMatch and ModelNet/ModelLoNet benchmarks.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Visual-Language Prompt Tuning with Knowledge-guided Context Optimization</b></summary>
  <p><b>编号</b>：[100]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13283</p>
  <p><b>作者</b>：Hantao Yao,  Rui Zhang,  Changsheng Xu</p>
  <p><b>备注</b>：accepted by CVPR23</p>
  <p><b>关键词</b>：pre-trained visual-language model, task-related textual tokens, specific textual knowledge, learnable textual tokens, textual tokens</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Prompt tuning is an effective way to adapt the pre-trained visual-language
model (VLM) to the downstream task using task-related textual tokens.
Representative CoOp-based work combines the learnable textual tokens with the
class tokens to obtain specific textual knowledge. However, the specific
textual knowledge is the worse generalization to the unseen classes because it
forgets the essential general textual knowledge having a strong generalization
ability. To tackle this issue, we introduce a novel Knowledge-guided Context
Optimization (KgCoOp) to enhance the generalization ability of the learnable
prompt for unseen classes. The key insight of KgCoOp is that forgetting about
essential knowledge can be alleviated by reducing the discrepancy between the
learnable prompt and the hand-crafted prompt. Especially, KgCoOp minimizes the
discrepancy between the textual embeddings generated by learned prompts and the
hand-crafted prompts. Finally, adding the KgCoOp upon the contrastive loss can
make a discriminative prompt for both seen and unseen tasks. Extensive
evaluation of several benchmarks demonstrates that the proposed
Knowledge-guided Context Optimization is an efficient method for prompt tuning,
\emph{i.e.,} achieves better performance with less training time.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing  Field</b></summary>
  <p><b>编号</b>：[102]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13277</p>
  <p><b>作者</b>：Chong Bao,  Yinda Zhang,  Bangbang Yang,  Tianxing Fan,  Zesong Yang,  Hujun Bao,  Guofeng Zhang,  Zhaopeng Cui</p>
  <p><b>备注</b>：Accepted to CVPR 2023. Project Page: this https URL</p>
  <p><b>关键词</b>：http URL achieve, user-friendly tools, http URL, great success, editing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite the great success in 2D editing using user-friendly tools, such as
Photoshop, semantic strokes, or even text prompts, similar capabilities in 3D
areas are still limited, either relying on 3D modeling skills or allowing
editing within only a few this http URL this paper, we present a novel
semantic-driven NeRF editing approach, which enables users to edit a neural
radiance field with a single image, and faithfully delivers edited novel views
with high fidelity and multi-view this http URL achieve this goal, we propose
a prior-guided editing field to encode fine-grained geometric and texture
editing in 3D space, and develop a series of techniques to aid the editing
process, including cyclic constraints with a proxy mesh to facilitate geometric
supervision, a color compositing mechanism to stabilize semantic-driven texture
editing, and a feature-cluster-based regularization to preserve the irrelevant
content unchanged.Extensive experiments and editing examples on both real-world
and synthetic data demonstrate that our method achieves photo-realistic 3D
editing using only a single edited image, pushing the bound of semantic-driven
editing in 3D real-world scenes. Our project webpage:
this https URL.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：TAPS3D: Text-Guided 3D Textured Shape Generation from Pseudo Supervision</b></summary>
  <p><b>编号</b>：[103]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13273</p>
  <p><b>作者</b>：Jiacheng Wei,  Hao Wang,  Jiashi Feng,  Guosheng Lin,  Kim-Hui Yap</p>
  <p><b>备注</b>：Accepted to CVPR2023</p>
  <p><b>关键词</b>：open research task, textual descriptions, investigate an open, open research, research task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we investigate an open research task of generating
controllable 3D textured shapes from the given textual descriptions. Previous
works either require ground truth caption labeling or extensive optimization
time. To resolve these issues, we present a novel framework, TAPS3D, to train a
text-guided 3D shape generator with pseudo captions. Specifically, based on
rendered 2D images, we retrieve relevant words from the CLIP vocabulary and
construct pseudo captions using templates. Our constructed captions provide
high-level semantic supervision for generated 3D shapes. Further, in order to
produce fine-grained textures and increase geometry diversity, we propose to
adopt low-level image regularization to enable fake-rendered images to align
with the real ones. During the inference phase, our proposed model can generate
3D textured shapes from the given text without any additional optimization. We
conduct extensive experiments to analyze each of our proposed components and
show the efficacy of our framework in generating high-fidelity 3D textured and
text-relevant shapes.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Disguise without Disruption: Utility-Preserving Face De-Identification</b></summary>
  <p><b>编号</b>：[105]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13269</p>
  <p><b>作者</b>：Zikui Cai,  Zhongpai Gao,  Benjamin Planche,  Meng Zheng,  Terrence Chen,  M. Salman Asif,  Ziyan Wu</p>
  <p><b>备注</b>：paper + supplementary material</p>
  <p><b>关键词</b>：smart sensors, humanity is generating, increasing ubiquity, ubiquity of cameras, cameras and smart</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the increasing ubiquity of cameras and smart sensors, humanity is
generating data at an exponential rate. Access to this trove of information,
often covering yet-underrepresented use-cases (e.g., AI in medical settings)
could fuel a new generation of deep-learning tools. However, eager data
scientists should first provide satisfying guarantees w.r.t. the privacy of
individuals present in these untapped datasets. This is especially important
for images or videos depicting faces, as their biometric information is the
target of most identification methods. While a variety of solutions have been
proposed to de-identify such images, they often corrupt other non-identifying
facial attributes that would be relevant for downstream tasks. In this paper,
we propose Disguise, a novel algorithm to seamlessly de-identify facial images
while ensuring the usability of the altered data. Unlike prior arts, we ground
our solution in both differential privacy and ensemble-learning research
domains. Our method extracts and swaps depicted identities with fake ones,
synthesized via variational mechanisms to maximize obfuscation and
non-invertibility; while leveraging the supervision from a mixture-of-experts
to disentangle and preserve other utility attributes. We extensively evaluate
our method on multiple datasets, demonstrating higher de-identification rate
and superior consistency than prior art w.r.t. various downstream tasks.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：A Bag-of-Prototypes Representation for Dataset-Level Applications</b></summary>
  <p><b>编号</b>：[114]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13251</p>
  <p><b>作者</b>：Weijie Tu,  Weijian Deng,  Tom Gedeon,  Liang Zheng</p>
  <p><b>备注</b>：CVPR 2023 camera-ready</p>
  <p><b>关键词</b>：assessing training set, test set difficulty, training set suitability, work investigates dataset, training set</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This work investigates dataset vectorization for two dataset-level tasks:
assessing training set suitability and test set difficulty. The former measures
how suitable a training set is for a target domain, while the latter studies
how challenging a test set is for a learned model. Central to the two tasks is
measuring the underlying relationship between datasets. This needs a desirable
dataset vectorization scheme, which should preserve as much discriminative
dataset information as possible so that the distance between the resulting
dataset vectors can reflect dataset-to-dataset similarity. To this end, we
propose a bag-of-prototypes (BoP) dataset representation that extends the
image-level bag consisting of patch descriptors to dataset-level bag consisting
of semantic prototypes. Specifically, we develop a codebook consisting of K
prototypes clustered from a reference dataset. Given a dataset to be encoded,
we quantize each of its image features to a certain prototype in the codebook
and obtain a K-dimensional histogram. Without assuming access to dataset
labels, the BoP representation provides a rich characterization of the dataset
semantic distribution. Furthermore, BoP representations cooperate well with
Jensen-Shannon divergence for measuring dataset-to-dataset similarity. Although
very simple, BoP consistently shows its advantage over existing representations
on a series of benchmarks for two dataset-level tasks.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：CrOC: Cross-View Online Clustering for Dense Visual Representation  Learning</b></summary>
  <p><b>编号</b>：[117]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13245</p>
  <p><b>作者</b>：Thomas Stegmüller,  Tim Lebailly,  Behzad Bozorgtabar,  Tinne Tuytelaars,  Jean-Philippe Thiran</p>
  <p><b>备注</b>：Accepted at CVPR 2023, * denotes equal contribution</p>
  <p><b>关键词</b>：Learning dense visual, dense visual representations, Learning dense, scene-centric data, dense visual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Learning dense visual representations without labels is an arduous task and
more so from scene-centric data. We propose to tackle this challenging problem
by proposing a Cross-view consistency objective with an Online Clustering
mechanism (CrOC) to discover and segment the semantics of the views. In the
absence of hand-crafted priors, the resulting method is more generalizable and
does not require a cumbersome pre-processing step. More importantly, the
clustering algorithm conjointly operates on the features of both views, thereby
elegantly bypassing the issue of content not represented in both views and the
ambiguous matching of objects from one crop to the other. We demonstrate
excellent performance on linear and unsupervised segmentation transfer tasks on
various datasets and similarly for video object segmentation. Our code and
pre-trained models are publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：6D Object Pose Estimation from Approximate 3D Models for Orbital  Robotics</b></summary>
  <p><b>编号</b>：[118]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13241</p>
  <p><b>作者</b>：Maximilian Ulmer,  Maximilian Durner,  Martin Sundermeyer,  Manuel Stoiber,  Rudolph Triebel</p>
  <p><b>备注</b>：preprint</p>
  <p><b>关键词</b>：single images, model, model coordinates, pixel-wise coordinate error, pose</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a novel technique to estimate the 6D pose of objects from single
images where the 3D geometry of the object is only given approximately and not
as a precise 3D model. To achieve this, we employ a dense 2D-to-3D
correspondence predictor that regresses 3D model coordinates for every pixel.
In addition to the 3D coordinates, our model also estimates the pixel-wise
coordinate error to discard correspondences that are likely wrong. This allows
us to generate multiple 6D pose hypotheses of the object, which we then refine
iteratively using a highly efficient region-based approach. We also introduce a
novel pixel-wise posterior formulation by which we can estimate the probability
for each hypothesis and select the most likely one. As we show in experiments,
our approach is capable of dealing with extreme visual conditions including
overexposure, high contrast, or low signal-to-noise ratio. This makes it a
powerful technique for the particularly challenging task of estimating the pose
of tumbling satellites for in-orbit robotic applications. Our method achieves
state-of-the-art performance on the SPEED+ dataset and has won the SPEC2021
post-mortem competition.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Visually-Prompted Language Model for Fine-Grained Scene Graph Generation  in an Open World</b></summary>
  <p><b>编号</b>：[119]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13233</p>
  <p><b>作者</b>：Qifan Yu,  Juncheng Li,  Yu Wu,  Siliang Tang,  Wei Ji,  Yueting Zhuang</p>
  <p><b>备注</b>：21 pages, 16 figures</p>
  <p><b>关键词</b>：aims to extract, Scene Graph Generation, relationships in images, vision understanding, images for vision</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Scene Graph Generation (SGG) aims to extract <subject, predicate, object>
relationships in images for vision understanding. Although recent works have
made steady progress on SGG, they still suffer long-tail distribution issues
that tail-predicates are more costly to train and hard to distinguish due to a
small amount of annotated data compared to frequent predicates. Existing
re-balancing strategies try to haddle it via prior rules but are still confined
to pre-defined conditions, which are not scalable for various models and
datasets. In this paper, we propose a Cross-modal prediCate boosting (CaCao)
framework, where a visually-prompted language model is learned to generate
diverse fine-grained predicates in a low-resource way. The proposed CaCao can
be applied in a plug-and-play fashion and automatically strengthen existing SGG
to tackle the long-tailed problem. Based on that, we further introduce a novel
Entangled cross-modal prompt approach for open-world predicate scene graph
generation (Epic), where models can generalize to unseen predicates in a
zero-shot manner. Comprehensive experiments on three benchmark datasets show
that CaCao consistently boosts the performance of multiple scene graph
generation models in a model-agnostic way. Moreover, our Epic achieves
competitive performance on open-world predicate prediction.</subject,></p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Transforming Radiance Field with Lipschitz Network for Photorealistic 3D  Scene Stylization</b></summary>
  <p><b>编号</b>：[120]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13232</p>
  <p><b>作者</b>：Zicheng Zhang,  Yinglu Liu,  Congying Han,  Yingwei Pan,  Tiande Guo,  Ting Yao</p>
  <p><b>备注</b>：CVPR 2023, Highlight</p>
  <p><b>关键词</b>：Recent advances, synthesis have witnessed, witnessed the rise, Neural Radiance Fields, Lipschitz</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advances in 3D scene representation and novel view synthesis have
witnessed the rise of Neural Radiance Fields (NeRFs). Nevertheless, it is not
trivial to exploit NeRF for the photorealistic 3D scene stylization task, which
aims to generate visually consistent and photorealistic stylized scenes from
novel views. Simply coupling NeRF with photorealistic style transfer (PST) will
result in cross-view inconsistency and degradation of stylized view syntheses.
Through a thorough analysis, we demonstrate that this non-trivial task can be
simplified in a new light: When transforming the appearance representation of a
pre-trained NeRF with Lipschitz mapping, the consistency and photorealism
across source views will be seamlessly encoded into the syntheses. That
motivates us to build a concise and flexible learning framework namely LipRF,
which upgrades arbitrary 2D PST methods with Lipschitz mapping tailored for the
3D scene. Technically, LipRF first pre-trains a radiance field to reconstruct
the 3D scene, and then emulates the style on each view by 2D PST as the prior
to learn a Lipschitz network to stylize the pre-trained appearance. In view of
that Lipschitz condition highly impacts the expressivity of the neural network,
we devise an adaptive regularization to balance the reconstruction and
stylization. A gradual gradient aggregation strategy is further introduced to
optimize LipRF in a cost-efficient manner. We conduct extensive experiments to
show the high quality and robust performance of LipRF on both photorealistic 3D
stylization and object appearance editing.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Confidence-Aware and Self-Supervised Image Anomaly Localisation</b></summary>
  <p><b>编号</b>：[123]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13227</p>
  <p><b>作者</b>：Johanna P. Müller,  Matthew Baugh,  Jeremy Tan,  Mischa Dombrowski,  Bernhard Kainz</p>
  <p><b>备注</b>：Under Review</p>
  <p><b>关键词</b>：medical image analysis, Universal anomaly detection, challenging prob, lem in machine, remains a challenging</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Universal anomaly detection still remains a challenging prob- lem in machine
learning and medical image analysis. It is possible to learn an expected
distribution from a single class of normative samples, e.g., through epistemic
uncertainty estimates, auto-encoding models, or from synthetic anomalies in a
self-supervised way. The performance of self-supervised anomaly detection
approaches is still inferior compared to methods that use examples from known
unknown classes to shape the decision boundary. However, outlier exposure
methods often do not identify unknown unknowns. Here we discuss an improved
self-supervised single-class training strategy that supports the approximation
of proba- bilistic inference with loosen feature locality constraints. We show
that up-scaling of gradients with histogram-equalised images is beneficial for
recently proposed self-supervision tasks. Our method is integrated into several
out-of-distribution (OOD) detection models and we show evi- dence that our
method outperforms the state-of-the-art on various bench- mark datasets. Source
code will be publicly available by the time of the conference.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Exploring Structured Semantic Prior for Multi Label Recognition with  Incomplete Labels</b></summary>
  <p><b>编号</b>：[125]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13223</p>
  <p><b>作者</b>：Zixuan Ding,  Ao Wang,  Hui Chen,  Qiang Zhang,  Pengzhang Liu,  Yongjun Bao,  Weipeng Yan,  Jungong Han</p>
  <p><b>备注</b>：Accepted by IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2023</p>
  <p><b>关键词</b>：Multi-label recognition, structured semantic prior, semantic prior, Correspondence Prompt Network, Semantic Correspondence Prompt</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multi-label recognition (MLR) with incomplete labels is very challenging.
Recent works strive to explore the image-to-label correspondence in the
vision-language model, \ie, CLIP~\cite{radford2021clip}, to compensate for
insufficient annotations. In spite of promising performance, they generally
overlook the valuable prior about the label-to-label correspondence. In this
paper, we advocate remedying the deficiency of label supervision for the MLR
with incomplete labels by deriving a structured semantic prior about the
label-to-label correspondence via a semantic prior prompter. We then present a
novel Semantic Correspondence Prompt Network (SCPNet), which can thoroughly
explore the structured semantic prior. A Prior-Enhanced Self-Supervised
Learning method is further introduced to enhance the use of the prior.
Comprehensive experiments and analyses on several widely used benchmark
datasets show that our method significantly outperforms existing methods on all
datasets, well demonstrating the effectiveness and the superiority of our
method. Our code will be available at this https URL.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Explore the Power of Synthetic Data on Few-shot Object Detection</b></summary>
  <p><b>编号</b>：[126]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13221</p>
  <p><b>作者</b>：Shaobo Lin,  Kun Wang,  Xingyu Zeng,  Rui Zhao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：FSOD, aims to expand, synthetic, FSOD tasks, synthetic images</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Few-shot object detection (FSOD) aims to expand an object detector for novel
categories given only a few instances for training. The few training samples
restrict the performance of FSOD model. Recent text-to-image generation models
have shown promising results in generating high-quality images. How applicable
these synthetic images are for FSOD tasks remains under-explored. This work
extensively studies how synthetic images generated from state-of-the-art
text-to-image generators benefit FSOD tasks. We focus on two perspectives: (1)
How to use synthetic data for FSOD? (2) How to find representative samples from
the large-scale synthetic dataset? We design a copy-paste-based pipeline for
using synthetic data. Specifically, saliency object detection is applied to the
original generated image, and the minimum enclosing box is used for cropping
the main object based on the saliency map. After that, the cropped object is
randomly pasted on the image, which comes from the base dataset. We also study
the influence of the input text of text-to-image generator and the number of
synthetic images used. To construct a representative synthetic training
dataset, we maximize the diversity of the selected images via a sample-based
and cluster-based method. However, the severe problem of high false positives
(FP) ratio of novel categories in FSOD can not be solved by using synthetic
data. We propose integrating CLIP, a zero-shot recognition model, into the FSOD
pipeline, which can filter 90% of FP by defining a threshold for the similarity
score between the detected object and the text of the predicted category.
Extensive experiments on PASCAL VOC and MS COCO validate the effectiveness of
our method, in which performance gain is up to 21.9% compared to the few-shot
baseline.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：A Simple and Generic Framework for Feature Distillation via Channel-wise  Transformation</b></summary>
  <p><b>编号</b>：[131]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13212</p>
  <p><b>作者</b>：Ziwei Liu,  Yongtao Wang,  Xiaojie Chu</p>
  <p><b>备注</b>：13 pages</p>
  <p><b>关键词</b>：smaller student model, large teacher model, feature maps, feature misalignment issue, student model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Knowledge distillation is a popular technique for transferring the knowledge
from a large teacher model to a smaller student model by mimicking. However,
distillation by directly aligning the feature maps between teacher and student
may enforce overly strict constraints on the student thus degrade the
performance of the student model. To alleviate the above feature misalignment
issue, existing works mainly focus on spatially aligning the feature maps of
the teacher and the student, with pixel-wise transformation. In this paper, we
newly find that aligning the feature maps between teacher and student along the
channel-wise dimension is also effective for addressing the feature
misalignment issue. Specifically, we propose a learnable nonlinear channel-wise
transformation to align the features of the student and the teacher model.
Based on it, we further propose a simple and generic framework for feature
distillation, with only one hyper-parameter to balance the distillation loss
and the task specific loss. Extensive experimental results show that our method
achieves significant performance improvements in various computer vision tasks
including image classification (+3.28% top-1 accuracy for MobileNetV1 on
ImageNet-1K), object detection (+3.9% bbox mAP for ResNet50-based Faster-RCNN
on MS COCO), instance segmentation (+2.8% Mask mAP for ResNet50-based
Mask-RCNN), and semantic segmentation (+4.66% mIoU for ResNet18-based PSPNet in
semantic segmentation on Cityscapes), which demonstrates the effectiveness and
the versatility of the proposed method. The code will be made publicly
available.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Don't FREAK Out: A Frequency-Inspired Approach to Detecting Backdoor  Poisoned Samples in DNNs</b></summary>
  <p><b>编号</b>：[132]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13211</p>
  <p><b>作者</b>：Hasan Abed Al Kader Hammoud,  Adel Bibi,  Philip H.S. Torr,  Bernard Ghanem</p>
  <p><b>备注</b>：Accepted at CVPRW (The Art of Robustness)</p>
  <p><b>关键词</b>：Deep Neural, paper we investigate, Neural, sensitivity of Deep, Deep</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper we investigate the frequency sensitivity of Deep Neural
Networks (DNNs) when presented with clean samples versus poisoned samples. Our
analysis shows significant disparities in frequency sensitivity between these
two types of samples. Building on these findings, we propose FREAK, a
frequency-based poisoned sample detection algorithm that is simple yet
effective. Our experimental results demonstrate the efficacy of FREAK not only
against frequency backdoor attacks but also against some spatial attacks. Our
work is just the first step in leveraging these insights. We believe that our
analysis and proposed defense mechanism will provide a foundation for future
research and development of backdoor defenses.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Taking A Closer Look at Visual Relation: Unbiased Video Scene Graph  Generation with Decoupled Label Learning</b></summary>
  <p><b>编号</b>：[133]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13209</p>
  <p><b>作者</b>：Wenqing Wang,  Yawei Luo,  Zhiqing Chen,  Tao Jiang,  Lei Chen,  Yi Yang,  Jun Xiao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Current video-based scene, scene graph generation, video-based scene graph, inherent biased distribution, Current video-based</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Current video-based scene graph generation (VidSGG) methods have been found
to perform poorly on predicting predicates that are less represented due to the
inherent biased distribution in the training data. In this paper, we take a
closer look at the predicates and identify that most visual relations (e.g.
sit_above) involve both actional pattern (sit) and spatial pattern (above),
while the distribution bias is much less severe at the pattern level. Based on
this insight, we propose a decoupled label learning (DLL) paradigm to address
the intractable visual relation prediction from the pattern-level perspective.
Specifically, DLL decouples the predicate labels and adopts separate
classifiers to learn actional and spatial patterns respectively. The patterns
are then combined and mapped back to the predicate. Moreover, we propose a
knowledge-level label decoupling method to transfer non-target knowledge from
head predicates to tail predicates within the same pattern to calibrate the
distribution of tail classes. We validate the effectiveness of DLL on the
commonly used VidSGG benchmark, i.e. VidVRD. Extensive experiments demonstrate
that the DLL offers a remarkably simple but highly effective solution to the
long-tailed problem, achieving the state-of-the-art VidSGG performance.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：First Session Adaptation: A Strong Replay-Free Baseline for  Class-Incremental Learning</b></summary>
  <p><b>编号</b>：[135]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13199</p>
  <p><b>作者</b>：Aristeidis Panos,  Yuriko Kobe,  Daniel Olmeda Reino,  Rahaf Aljundi,  Richard E. Turner</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：image classification system, CIL, FSA, system is exposed, learning session</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In Class-Incremental Learning (CIL) an image classification system is exposed
to new classes in each learning session and must be updated incrementally.
Methods approaching this problem have updated both the classification head and
the feature extractor body at each session of CIL. In this work, we develop a
baseline method, First Session Adaptation (FSA), that sheds light on the
efficacy of existing CIL approaches and allows us to assess the relative
performance contributions from head and body adaption. FSA adapts a pre-trained
neural network body only on the first learning session and fixes it thereafter;
a head based on linear discriminant analysis (LDA), is then placed on top of
the adapted body, allowing exact updates through CIL. FSA is replay-free
i.e.~it does not memorize examples from previous sessions of continual
learning. To empirically motivate FSA, we first consider a diverse selection of
22 image-classification datasets, evaluating different heads and body
adaptation techniques in high/low-shot offline settings. We find that the LDA
head performs well and supports CIL out-of-the-box. We also find that
Featurewise Layer Modulation (FiLM) adapters are highly effective in the
few-shot setting, and full-body adaption in the high-shot setting. Second, we
empirically investigate various CIL settings including high-shot CIL and
few-shot CIL, including settings that have previously been used in the
literature. We show that FSA significantly improves over the state-of-the-art
in 15 of the 16 settings considered. FSA with FiLM adapters is especially
performant in the few-shot setting. These results indicate that current
approaches to continuous body adaptation are not working as expected. Finally,
we propose a measure that can be applied to a set of unlabelled inputs which is
predictive of the benefits of body adaptation.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Complementary Pseudo Multimodal Feature for Point Cloud Anomaly  Detection</b></summary>
  <p><b>编号</b>：[136]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13194</p>
  <p><b>作者</b>：Yunkang Cao,  Xiaohao Xu,  Weiming Shen</p>
  <p><b>备注</b>：Submitted to Pattern Recognition. Code is available on this https URL</p>
  <p><b>关键词</b>：promising research area, PCD anomaly detection, detection steadily emerges, anomaly detection steadily, anomaly detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Point cloud (PCD) anomaly detection steadily emerges as a promising research
area. This study aims to improve PCD anomaly detection performance by combining
handcrafted PCD descriptions with powerful pre-trained 2D neural networks. To
this end, this study proposes Complementary Pseudo Multimodal Feature (CPMF)
that incorporates local geometrical information in 3D modality using
handcrafted PCD descriptors and global semantic information in the generated
pseudo 2D modality using pre-trained 2D neural networks. For global semantics
extraction, CPMF projects the origin PCD into a pseudo 2D modality containing
multi-view images. These images are delivered to pre-trained 2D neural networks
for informative 2D modality feature extraction. The 3D and 2D modality features
are aggregated to obtain the CPMF for PCD anomaly detection. Extensive
experiments demonstrate the complementary capacity between 2D and 3D modality
features and the effectiveness of CPMF, with 95.15% image-level AU-ROC and
92.93% pixel-level PRO on the MVTec3D benchmark. Code is available on
this https URL.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：VADER: Video Alignment Differencing and Retrieval</b></summary>
  <p><b>编号</b>：[137]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13193</p>
  <p><b>作者</b>：Alexander Black,  Simon Jenni,  Tu Bui,  Md. Mehrab Tanjim,  Stefano Petrangeli,  Ritwik Sinha,  Viswanathan Swaminathan,  John Collomosse</p>
  <p><b>备注</b>：Submitted to ICCV2023</p>
  <p><b>关键词</b>：fight misinformation spread, change summarization method, propose VADER, summarization method, fight misinformation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose VADER, a spatio-temporal matching, alignment, and change
summarization method to help fight misinformation spread via manipulated
videos. VADER matches and coarsely aligns partial video fragments to candidate
videos using a robust visual descriptor and scalable search over adaptively
chunked video content. A transformer-based alignment module then refines the
temporal localization of the query fragment within the matched video. A
space-time comparator module identifies regions of manipulation between aligned
content, invariant to any changes due to any residual temporal misalignments or
artifacts arising from non-editorial changes of the content. Robustly matching
video to a trusted source enables conclusions to be drawn on video provenance,
enabling informed trust decisions on content encountered.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：Marching-Primitives: Shape Abstraction from Signed Distance Function</b></summary>
  <p><b>编号</b>：[140]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13190</p>
  <p><b>作者</b>：Weixiao Liu,  Yuwei Wu,  Sipu Ruan,  Gregory S. Chirikjian</p>
  <p><b>备注</b>：Accepted to CVPR2023 Highlight</p>
  <p><b>关键词</b>：Representing complex objects, Representing complex, computer vision, basic geometric primitives, complex objects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Representing complex objects with basic geometric primitives has long been a
topic in computer vision. Primitive-based representations have the merits of
compactness and computational efficiency in higher-level tasks such as physics
simulation, collision checking, and robotic manipulation. Unlike previous works
which extract polygonal meshes from a signed distance function (SDF), in this
paper, we present a novel method, named Marching-Primitives, to obtain a
primitive-based abstraction directly from an SDF. Our method grows geometric
primitives (such as superquadrics) iteratively by analyzing the connectivity of
voxels while marching at different levels of signed distance. For each valid
connected volume of interest, we march on the scope of voxels from which a
primitive is able to be extracted in a probabilistic sense and simultaneously
solve for the parameters of the primitive to capture the underlying local
geometry. We evaluate the performance of our method on both synthetic and
real-world datasets. The results show that the proposed method outperforms the
state-of-the-art in terms of accuracy, and is directly generalizable among
different categories and scales. The code is open-sourced at
this https URL.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：ScanERU: Interactive 3D Visual Grounding based on Embodied Reference  Understanding</b></summary>
  <p><b>编号</b>：[142]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13186</p>
  <p><b>作者</b>：Ziyang Lu,  Yunqiang Pei,  Guoqing Wang,  Yang Yang,  Zheng Wang,  Heng Tao Shen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：link natural language, natural language descriptions, point clouds, Aiming to link, link natural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Aiming to link natural language descriptions to specific regions in a 3D
scene represented as 3D point clouds, 3D visual grounding is a very fundamental
task for human-robot interaction. The recognition errors can significantly
impact the overall accuracy and then degrade the operation of AI systems.
Despite their effectiveness, existing methods suffer from the difficulty of low
recognition accuracy in cases of multiple adjacent objects with similar
this http URL address this issue, this work intuitively introduces the
human-robot interaction as a cue to facilitate the development of 3D visual
grounding. Specifically, a new task termed Embodied Reference Understanding
(ERU) is first designed for this concern. Then a new dataset called ScanERU is
constructed to evaluate the effectiveness of this idea. Different from existing
datasets, our ScanERU is the first to cover semi-synthetic scene integration
with textual, real-world visual, and synthetic gestural information.
Additionally, this paper formulates a heuristic framework based on attention
mechanisms and human body movements to enlighten the research of ERU.
Experimental results demonstrate the superiority of the proposed method,
especially in the recognition of multiple identical objects. Our codes and
dataset are ready to be available publicly.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：CMG-Net: An End-to-End Contact-Based Multi-Finger Dexterous Grasping  Network</b></summary>
  <p><b>编号</b>：[143]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13182</p>
  <p><b>作者</b>：Mingze Wei,  Yaomin Huang,  Zhiyuan Xu,  Ning Liu,  Zhengping Che,  Xinyu Zhang,  Chaomin Shen,  Feifei Feng,  Chun Shan,  Jian Tang</p>
  <p><b>备注</b>：The first two authors are with equal contributions. Paper accepted by ICRA 2023</p>
  <p><b>关键词</b>：representation, robotic hands, multi-finger robotic hands, grasping, paper</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we propose a novel representation for grasping using contacts
between multi-finger robotic hands and objects to be manipulated. This
representation significantly reduces the prediction dimensions and accelerates
the learning process. We present an effective end-to-end network, CMG-Net, for
grasping unknown objects in a cluttered environment by efficiently predicting
multi-finger grasp poses and hand configurations from a single-shot point
cloud. Moreover, we create a synthetic grasp dataset that consists of five
thousand cluttered scenes, 80 object categories, and 20 million annotations. We
perform a comprehensive empirical study and demonstrate the effectiveness of
our grasping representation and CMG-Net. Our work significantly outperforms the
state-of-the-art for three-finger robotic hands. We also demonstrate that the
model trained using synthetic data performs very well for real robots.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：Enhancement of theColor Image Compression Using a New Algorithm based on  Discrete Hermite Wavelet Transform</b></summary>
  <p><b>编号</b>：[145]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13175</p>
  <p><b>作者</b>：Hassan Mohamed Muhi-Aldeen,  Asma A. Abdulrahman,  Jabbar Abed Eleiwy,  Fouad S. Tahir,  Yurii Khlaponin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Internet has turned, small village, turned the entire, entire world, share millions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The Internet has turned the entire world into a small village;this is because
it has made it possible to share millions of images and videos. However,
sending and receiving a huge amount of data is considered to be a main
challenge. To address this issue, a new algorithm is required to reduce image
bits and represent the data in a compressed form. Nevertheless, image
compression is an important application for transferring large files and
images. This requires appropriate and efficient transfers in this field to
achieve the task and reach the best results. In this work, we propose a new
algorithm based on discrete Hermite wavelets transformation (DHWT) that shows
the efficiency and quality of the color images. By compressing the color image,
this method analyzes it and divides it into approximate coefficients and detail
coefficients after adding the wavelets into MATLAB. With Multi-Resolution
Analyses (MRA), the appropriate filter is derived, and the mathematical aspects
prove to be validated by testing a new filter and performing its operation.
After the decomposition of the rows and upon the process of the reconstruction,
taking the inverse of the filter and dealing with the columns of the matrix,
the original matrix is improved by measuring the parameters of the image to
achieve the best quality of the resulting image, such as the peak
signal-to-noise ratio (PSNR), compression ratio (CR), bits per pixel (BPP), and
mean square error (MSE).</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：3D-POP - An automated annotation approach to facilitate markerless 2D-3D  tracking of freely moving birds with marker-based motion capture</b></summary>
  <p><b>编号</b>：[146]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13174</p>
  <p><b>作者</b>：Hemal Naik,  Alex Hoi Hang Chan,  Junran Yang,  Mathilde Delacoux,  Iain D. Couzin,  Fumihiro Kano,  Máté Nagy</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Recent advances, advances in machine, machine learning, learning and computer, computer vision</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advances in machine learning and computer vision are revolutionizing
the field of animal behavior by enabling researchers to track the poses and
locations of freely moving animals without any marker attachment. However,
large datasets of annotated images of animals for markerless pose tracking,
especially high-resolution images taken from multiple angles with accurate 3D
annotations, are still scant. Here, we propose a method that uses a motion
capture (mo-cap) system to obtain a large amount of annotated data on animal
movement and posture (2D and 3D) in a semi-automatic manner. Our method is
novel in that it extracts the 3D positions of morphological keypoints (e.g
eyes, beak, tail) in reference to the positions of markers attached to the
animals. Using this method, we obtained, and offer here, a new dataset - 3D-POP
with approximately 300k annotated frames (4 million instances) in the form of
videos having groups of one to ten freely moving birds from 4 different camera
views in a 3.6m x 4.2m area. 3D-POP is the first dataset of flocking birds with
accurate keypoint annotations in 2D and 3D along with bounding box and
individual identities and will facilitate the development of solutions for
problems of 2D to 3D markerless pose, trajectory tracking, and identification
in birds.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：Take 5: Interpretable Image Classification with a Handful of Features</b></summary>
  <p><b>编号</b>：[150]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13166</p>
  <p><b>作者</b>：Thomas Norrenbrock,  Marco Rudolph,  Bodo Rosenhahn</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Deep Neural Networks, Deep Neural, Neural Networks, decision, human can follow</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep Neural Networks use thousands of mostly incomprehensible features to
identify a single class, a decision no human can follow. We propose an
interpretable sparse and low dimensional final decision layer in a deep neural
network with measurable aspects of interpretability and demonstrate it on
fine-grained image classification. We argue that a human can only understand
the decision of a machine learning model, if the features are interpretable and
only very few of them are used for a single decision. For that matter, the
final layer has to be sparse and, to make interpreting the features feasible,
low dimensional. We call a model with a Sparse Low-Dimensional Decision
SLDD-Model. We show that a SLDD-Model is easier to interpret locally and
globally than a dense high-dimensional decision layer while being able to
maintain competitive accuracy. Additionally, we propose a loss function that
improves a model's feature diversity and accuracy. Our more interpretable
SLDD-Model only uses 5 out of just 50 features per class, while maintaining 97%
to 100% of the accuracy on four common benchmark datasets compared to the
baseline model with 2048 features.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：Improvement of Color Image Analysis Using a New Hybrid Face Recognition  Algorithm based on Discrete Wavelets and Chebyshev Polynomials</b></summary>
  <p><b>编号</b>：[151]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13158</p>
  <p><b>作者</b>：Hassan Mohamed Muhi-Aldeen,  Maha Ammar Mustafa,  Asma A. Abdulrahman,  Jabbar Abed Eleiwy,  Fouad S. Tahir,  Yurii Khlaponin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Chebyshev Wavelets Transform, Wavelets Convolutional Neural, Chebeshev Wavelets Convolutional, Chebyshev Wavelets, Wavelets Transform</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This work is unique in the use of discrete wavelets that were built from or
derived from Chebyshev polynomials of the second and third kind, filter the
Discrete Second Chebyshev Wavelets Transform (DSCWT), and derive two effective
filters. The Filter Discrete Third Chebyshev Wavelets Transform (FDTCWT) is
used in the process of analyzing color images and removing noise and impurities
that accompany the image, as well as because of the large amount of data that
makes up the image as it is taken. These data are massive, making it difficult
to deal with each other during transmission. However to address this issue, the
image compression technique is used, with the image not losing information due
to the readings that were obtained, and the results were satisfactory. Mean
Square Error (MSE), Peak Signal Noise Ratio (PSNR), Bit Per Pixel (BPP), and
Compression Ratio (CR) Coronavirus is the initial treatment, while the
processing stage is done with network training for Convolutional Neural
Networks (CNN) with Discrete Second Chebeshev Wavelets Convolutional Neural
Network (DSCWCNN) and Discrete Third Chebeshev Wavelets Convolutional Neural
Network (DTCWCNN) to create an efficient algorithm for face recognition, and
the best results were achieved in accuracy and in the least amount of time. Two
samples of color images that were made or implemented were used. The proposed
theory was obtained with fast and good results; the results are evident shown
in the tables below.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：Calibrated Out-of-Distribution Detection with a Generic Representation</b></summary>
  <p><b>编号</b>：[154]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13148</p>
  <p><b>作者</b>：Tomas Vojir,  Jan Sochman,  Rahaf Aljundi,  Jiri Matas</p>
  <p><b>备注</b>：10 pages, submitted to conference</p>
  <p><b>关键词</b>：safety critical applications, essential building block, deploying vision models, critical applications, common issue</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Out-of-distribution detection is a common issue in deploying vision models in
practice and solving it is an essential building block in safety critical
applications. Existing OOD detection solutions focus on improving the OOD
robustness of a classification model trained exclusively on in-distribution
(ID) data. In this work, we take a different approach and propose to leverage
generic pre-trained representations. We first investigate the behaviour of
simple classifiers built on top of such representations and show striking
performance gains compared to the ID trained representations. We propose a
novel OOD method, called GROOD, that achieves excellent performance, predicated
by the use of a good generic representation. Only a trivial training process is
required for adapting GROOD to a particular problem. The method is simple,
general, efficient, calibrated and with only a few hyper-parameters. The method
achieves state-of-the-art performance on a number of OOD benchmarks, reaching
near perfect performance on several of them. The source code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：Generative Image Inpainting with Segmentation Confusion Adversarial  Training and Contrastive Learning</b></summary>
  <p><b>编号</b>：[159]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13133</p>
  <p><b>作者</b>：Zhiwen Zuo,  Lei Zhao,  Ailin Li,  Zhizhong Wang,  Zhanjie Zhang,  Jiafu Chen,  Wei Xing,  Dongming Lu</p>
  <p><b>备注</b>：Accepted to AAAI2023, Oral</p>
  <p><b>关键词</b>：adversarial training framework, confusion adversarial training, segmentation confusion adversarial, adversarial training, global adversarial training</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a new adversarial training framework for image inpainting
with segmentation confusion adversarial training (SCAT) and contrastive
learning. SCAT plays an adversarial game between an inpainting generator and a
segmentation network, which provides pixel-level local training signals and can
adapt to images with free-form holes. By combining SCAT with standard global
adversarial training, the new adversarial training framework exhibits the
following three advantages simultaneously: (1) the global consistency of the
repaired image, (2) the local fine texture details of the repaired image, and
(3) the flexibility of handling images with free-form holes. Moreover, we
propose the textural and semantic contrastive learning losses to stabilize and
improve our inpainting model's training by exploiting the feature
representation space of the discriminator, in which the inpainting images are
pulled closer to the ground truth images but pushed farther from the corrupted
images. The proposed contrastive losses better guide the repaired images to
move from the corrupted image data points to the real image data points in the
feature representation space, resulting in more realistic completed images. We
conduct extensive experiments on two benchmark datasets, demonstrating our
model's effectiveness and superiority both qualitatively and quantitatively.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：Masked Image Training for Generalizable Deep Image Denoising</b></summary>
  <p><b>编号</b>：[160]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13132</p>
  <p><b>作者</b>：Haoyu Chen,  Jinjin Gu,  Yihao Liu,  Salma Abdel Magid,  Chao Dong,  Qiong Wang,  Hanspeter Pfister,  Lei Zhu</p>
  <p><b>备注</b>：Accepted to CVPR 2023</p>
  <p><b>关键词</b>：devices inevitably introduce, inevitably introduce noise, devices inevitably, capturing and storing, inevitably introduce</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>When capturing and storing images, devices inevitably introduce noise.
Reducing this noise is a critical task called image denoising. Deep learning
has become the de facto method for image denoising, especially with the
emergence of Transformer-based models that have achieved notable
state-of-the-art results on various image tasks. However, deep learning-based
methods often suffer from a lack of generalization ability. For example, deep
models trained on Gaussian noise may perform poorly when tested on other noise
distributions. To address this issue, we present a novel approach to enhance
the generalization performance of denoising networks, known as masked training.
Our method involves masking random pixels of the input image and reconstructing
the missing information during training. We also mask out the features in the
self-attention layers to avoid the impact of training-testing inconsistency.
Our approach exhibits better generalization ability than other deep learning
models and is directly applicable to real-world scenarios. Additionally, our
interpretability analysis demonstrates the superiority of our method.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：Watch Out for the Confusing Faces: Detecting Face Swapping with the  Probability Distribution of Face Identification Models</b></summary>
  <p><b>编号</b>：[161]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13131</p>
  <p><b>作者</b>：Yuxuan Duan,  Xuhong Zhang,  Chuer Yu,  Zonghui Wang,  Shouling Ji,  Wenzhi Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：face swapping, surprising reality, raising concerns, FSD, face</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, face swapping has been developing rapidly and achieved a surprising
reality, raising concerns about fake content. As a countermeasure, various
detection approaches have been proposed and achieved promising performance.
However, most existing detectors struggle to maintain performance on unseen
face swapping methods and low-quality images. Apart from the generalization
problem, current detection approaches have been shown vulnerable to evasion
attacks crafted by detection-aware manipulators. Lack of robustness under
adversary scenarios leaves threats for applying face swapping detection in real
world. In this paper, we propose a novel face swapping detection approach based
on face identification probability distributions, coined as IdP_FSD, to improve
the generalization and robustness. IdP_FSD is specially designed for detecting
swapped faces whose identities belong to a finite set, which is meaningful in
real-world applications. Compared with previous general detection methods, we
make use of the available real faces with concerned identities and require no
fake samples for training. IdP_FSD exploits face swapping's common nature that
the identity of swapped face combines that of two faces involved in swapping.
We reflect this nature with the confusion of a face identification model and
measure the confusion with the maximum value of the output probability
distribution. What's more, to defend our detector under adversary scenarios, an
attention-based finetuning scheme is proposed for the face identification
models used in IdP_FSD. Extensive experiments show that the proposed IdP_FSD
not only achieves high detection performance on different benchmark datasets
and image qualities but also raises the bar for manipulators to evade the
detection.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：Task-Oriented Human-Object Interactions Generation with Implicit Neural  Representations</b></summary>
  <p><b>编号</b>：[162]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13129</p>
  <p><b>作者</b>：Quanzhou Li,  Jingbo Wang,  Chen Change Loy,  Bo Dai</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：vibrant research field, Digital human motion, human motion synthesis, motions, video games</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Digital human motion synthesis is a vibrant research field with applications
in movies, AR/VR, and video games. Whereas methods were proposed to generate
natural and realistic human motions, most only focus on modeling humans and
largely ignore object movements. Generating task-oriented human-object
interaction motions in simulation is challenging. For different intents of
using the objects, humans conduct various motions, which requires the human
first to approach the objects and then make them move consistently with the
human instead of staying still. Also, to deploy in downstream applications, the
synthesized motions are desired to be flexible in length, providing options to
personalize the predicted motions for various purposes. To this end, we propose
TOHO: Task-Oriented Human-Object Interactions Generation with Implicit Neural
Representations, which generates full human-object interaction motions to
conduct specific tasks, given only the task type, the object, and a starting
human status. TOHO generates human-object motions in three steps: 1) it first
estimates the keyframe poses of conducting a task given the task type and
object information; 2) then, it infills the keyframes and generates continuous
motions; 3) finally, it applies a compact closed-form object motion estimation
to generate the object motion. Our method generates continuous motions that are
parameterized only by the temporal coordinate, which allows for upsampling or
downsampling of the sequence to arbitrary frames and adjusting the motion
speeds by designing the temporal coordinate vector. We demonstrate the
effectiveness of our method, both qualitatively and quantitatively. This work
takes a step further toward general human-scene interaction simulation.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：MagicFusion: Boosting Text-to-Image Generation Performance by Fusing  Diffusion Models</b></summary>
  <p><b>编号</b>：[163]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13126</p>
  <p><b>作者</b>：Jing Zhao,  Heliang Zheng,  Chaoyue Wang,  Long Lan,  Wenjing Yang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：text-guided diffusion models, powerful text-guided diffusion, advent of open-source, open-source AI communities, communities has produced</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The advent of open-source AI communities has produced a cornucopia of
powerful text-guided diffusion models that are trained on various datasets.
While few explorations have been conducted on ensembling such models to combine
their strengths. In this work, we propose a simple yet effective method called
Saliency-aware Noise Blending (SNB) that can empower the fused text-guided
diffusion models to achieve more controllable generation. Specifically, we
experimentally find that the responses of classifier-free guidance are highly
related to the saliency of generated images. Thus we propose to trust different
models in their areas of expertise by blending the predicted noises of two
diffusion models in a saliency-aware manner. SNB is training-free and can be
completed within a DDIM sampling process. Additionally, it can automatically
align the semantics of two noise spaces without requiring additional
annotations such as masks. Extensive experiments show the impressive
effectiveness of SNB in various applications. Project page is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：Laplacian Segmentation Networks: Improved Epistemic Uncertainty from  Spatial Aleatoric Uncertainty</b></summary>
  <p><b>编号</b>：[165]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13123</p>
  <p><b>作者</b>：Kilian Zepf,  Selma Wanna,  Marco Miani,  Juston Moore,  Jes Frellsen,  Søren Hauberg,  Aasa Feragen,  Frederik Warburg</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：frequently encountered, scanner differences, Laplacian Segmentation Networks, Laplacian Segmentation, OOD</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Out of distribution (OOD) medical images are frequently encountered, e.g.
because of site- or scanner differences, or image corruption. OOD images come
with a risk of incorrect image segmentation, potentially negatively affecting
downstream diagnoses or treatment. To ensure robustness to such incorrect
segmentations, we propose Laplacian Segmentation Networks (LSN) that jointly
model epistemic (model) and aleatoric (data) uncertainty in image segmentation.
We capture data uncertainty with a spatially correlated logit distribution. For
model uncertainty, we propose the first Laplace approximation of the weight
posterior that scales to large neural networks with skip connections that have
high-dimensional outputs. Empirically, we demonstrate that modelling spatial
pixel correlation allows the Laplacian Segmentation Network to successfully
assign high epistemic uncertainty to out-of-distribution objects appearing
within images.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：Exploring Visual Prompts for Whole Slide Image Classification with  Multiple Instance Learning</b></summary>
  <p><b>编号</b>：[166]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13122</p>
  <p><b>作者</b>：Yi Lin,  Zhongchen Zhao,  Zhengjie ZHU,  Lisheng Wang,  Kwang-Ting Cheng,  Hao Chen</p>
  <p><b>备注</b>：Submitted to MICCAI 2023</p>
  <p><b>关键词</b>：Multiple instance learning, Multiple instance, MIL models, method, MIL</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multiple instance learning (MIL) has emerged as a popular method for
classifying histopathology whole slide images (WSIs). However, existing
approaches typically rely on pre-trained models from large natural image
datasets, such as ImageNet, to generate instance features, which can be
sub-optimal due to the significant differences between natural images and
histopathology images that lead to a domain shift. In this paper, we present a
novel, simple yet effective method for learning domain-specific knowledge
transformation from pre-trained models to histopathology images. Our approach
entails using a prompt component to assist the pre-trained model in discerning
differences between the pre-trained dataset and the target histopathology
dataset, resulting in improved performance of MIL models. We validate our
method on two publicly available datasets, Camelyon16 and TCGA-NSCLC. Extensive
experimental results demonstrate the significant performance improvement of our
method for different MIL models and backbones. Upon publication of this paper,
we will release the source code for our method.</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：DetOFA: Efficient Training of Once-for-All Networks for Object Detection  by Using Pre-trained Supernet and Path Filter</b></summary>
  <p><b>编号</b>：[167]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13121</p>
  <p><b>作者</b>：Yuiko Sakuma,  Masato Ishii,  Takuya Narihira</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：object detection task, address the challenge, object detection, small amount, detection task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We address the challenge of training a large supernet for the object
detection task, using a relatively small amount of training data. Specifically,
we propose an efficient supernet-based neural architecture search (NAS) method
that uses transfer learning and search space pruning. First, the supernet is
pre-trained on a classification task, for which large datasets are available.
Second, the search space defined by the supernet is pruned by removing
candidate models that are predicted to perform poorly. To effectively remove
the candidates over a wide range of resource constraints, we particularly
design a performance predictor, called path filter, which can accurately
predict the relative performance of the models that satisfy similar resource
constraints. Hence, supernet training is more focused on the best-performing
candidates. Our path filter handles prediction for paths with different
resource budgets. Compared to once-for-all, our proposed method reduces the
computational cost of the optimal network architecture by 30% and 63%, while
yielding better accuracy-floating point operations Pareto front (0.85 and 0.45
points of improvement on average precision for Pascal VOC and COCO,
respectively).</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：Adaptive Regularization for Class-Incremental Learning</b></summary>
  <p><b>编号</b>：[168]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13113</p>
  <p><b>作者</b>：Elif Ceren Gok,  Murat Onur Yildirim,  Mert Kilickaya,  Joaquin Vanschoren</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：observed class accuracy, previously observed class, Class-Incremental Learning updates, class accuracy, updates a deep</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Class-Incremental Learning updates a deep classifier with new categories
while maintaining the previously observed class accuracy. Regularizing the
neural network weights is a common method to prevent forgetting previously
learned classes while learning novel ones. However, existing regularizers use a
constant magnitude throughout the learning sessions, which may not reflect the
varying levels of difficulty of the tasks encountered during incremental
learning. This study investigates the necessity of adaptive regularization in
Class-Incremental Learning, which dynamically adjusts the regularization
strength according to the complexity of the task at hand. We propose a Bayesian
Optimization-based approach to automatically determine the optimal
regularization magnitude for each learning task. Our experiments on two
datasets via two regularizers demonstrate the importance of adaptive
regularization for achieving accurate and less forgetful visual incremental
learning.</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：Keypoint-Guided Optimal Transport</b></summary>
  <p><b>编号</b>：[170]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13102</p>
  <p><b>作者</b>：Xiang Gu,  Yucheng Yang,  Wei Zeng,  Jian Sun,  Zongben Xu</p>
  <p><b>备注</b>：Under review</p>
  <p><b>关键词</b>：Existing Optimal Transport, transport plan, proposed KPG-RL model, optimal transport plan, Optimal Transport</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing Optimal Transport (OT) methods mainly derive the optimal transport
plan/matching under the criterion of transport cost/distance minimization,
which may cause incorrect matching in some cases. In many applications,
annotating a few matched keypoints across domains is reasonable or even
effortless in annotation burden. It is valuable to investigate how to leverage
the annotated keypoints to guide the correct matching in OT. In this paper, we
propose a novel KeyPoint-Guided model by ReLation preservation (KPG-RL) that
searches for the optimal matching (i.e., transport plan) guided by the
keypoints in OT. To impose the keypoints in OT, first, we propose a mask-based
constraint of the transport plan that preserves the matching of keypoint pairs.
Second, we propose to preserve the relation of each data point to the keypoints
to guide the matching. The proposed KPG-RL model can be solved by Sinkhorn's
algorithm and is applicable even when distributions are supported in different
spaces. We further utilize the relation preservation constraint in the
Kantorovich Problem and Gromov-Wasserstein model to impose the guidance of
keypoints in them. Meanwhile, the proposed KPG-RL model is extended to the
partial OT setting. Moreover, we deduce the dual formulation of the KPG-RL
model, which is solved using deep learning techniques. Based on the learned
transport plan from dual KPG-RL, we propose a novel manifold barycentric
projection to transport source data to the target domain. As applications, we
apply the proposed KPG-RL model to the heterogeneous domain adaptation and
image-to-image translation. Experiments verified the effectiveness of the
proposed approach.</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：MMFormer: Multimodal Transformer Using Multiscale Self-Attention for  Remote Sensing Image Classification</b></summary>
  <p><b>编号</b>：[171]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13101</p>
  <p><b>作者</b>：Bo Zhang,  Zuheng Ming,  Wei Feng,  Yaqian Liu,  Liang He,  Kaixing Zhao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Remote Sensing, Detection and Ranging, Light Detection, Hyperspectral Image, benefit the complementary</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To benefit the complementary information between heterogeneous data, we
introduce a new Multimodal Transformer (MMFormer) for Remote Sensing (RS) image
classification using Hyperspectral Image (HSI) accompanied by another source of
data such as Light Detection and Ranging (LiDAR). Compared with traditional
Vision Transformer (ViT) lacking inductive biases of convolutions, we first
introduce convolutional layers to our MMFormer to tokenize patches from
multimodal data of HSI and LiDAR. Then we propose a Multi-scale Multi-head
Self-Attention (MSMHSA) module to address the problem of compatibility which
often limits to fuse HSI with high spectral resolution and LiDAR with
relatively low spatial resolution. The proposed MSMHSA module can incorporate
HSI to LiDAR data in a coarse-to-fine manner enabling us to learn a
fine-grained representation. Ex- tensive experiments on widely used benchmarks
(e.g., Trento and MUUFL) demonstrate the effectiveness and superiority of our
proposed MMFormer for RS image classification.</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：PointGame: Geometrically and Adaptively Masked Auto-Encoder on Point  Clouds</b></summary>
  <p><b>编号</b>：[172]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13100</p>
  <p><b>作者</b>：Yun Liu,  Xuefeng Yan,  Zhilei Chen,  Zhiqi Li,  Zeyong Wei,  Mingqiang Wei</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：attracting large attention, point cloud understanding, attracting large, large attention, cloud understanding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Self-supervised learning is attracting large attention in point cloud
understanding. However, exploring discriminative and transferable features
still remains challenging due to their nature of irregularity and sparsity. We
propose a geometrically and adaptively masked auto-encoder for self-supervised
learning on point clouds, termed \textit{PointGame}. PointGame contains two
core components: GATE and EAT. GATE stands for the geometrical and adaptive
token embedding module; it not only absorbs the conventional wisdom of
geometric descriptors that captures the surface shape effectively, but also
exploits adaptive saliency to focus on the salient part of a point cloud. EAT
stands for the external attention-based Transformer encoder with linear
computational complexity, which increases the efficiency of the whole pipeline.
Unlike cutting-edge unsupervised learning models, PointGame leverages geometric
descriptors to perceive surface shapes and adaptively mines discriminative
features from training data. PointGame showcases clear advantages over its
competitors on various downstream tasks under both global and local fine-tuning
strategies. The code and pre-trained models will be publicly available.</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：CP$^3$: Channel Pruning Plug-in for Point-based Networks</b></summary>
  <p><b>编号</b>：[174]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13097</p>
  <p><b>作者</b>：Yaomin Huang,  Ning Liu,  Zhengping Che,  Zhiyuan Xu,  Chaomin Shen,  Yaxin Peng,  Guixu Zhang,  Xinmei Liu,  Feifei Feng,  Jian Tang</p>
  <p><b>备注</b>：Yaomin Huang and Ning Liu are with equal contributions. This paper has been accepted by CVPR 2023</p>
  <p><b>关键词</b>：Channel pruning, channel pruning methods, pruning, CNN channel pruning, pruning methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Channel pruning can effectively reduce both computational cost and memory
footprint of the original network while keeping a comparable accuracy
performance. Though great success has been achieved in channel pruning for 2D
image-based convolutional networks (CNNs), existing works seldom extend the
channel pruning methods to 3D point-based neural networks (PNNs). Directly
implementing the 2D CNN channel pruning methods to PNNs undermine the
performance of PNNs because of the different representations of 2D images and
3D point clouds as well as the network architecture disparity. In this paper,
we proposed CP$^3$, which is a Channel Pruning Plug-in for Point-based network.
CP$^3$ is elaborately designed to leverage the characteristics of point clouds
and PNNs in order to enable 2D channel pruning methods for PNNs. Specifically,
it presents a coordinate-enhanced channel importance metric to reflect the
correlation between dimensional information and individual channel features,
and it recycles the discarded points in PNN's sampling process and reconsiders
their potentially-exclusive information to enhance the robustness of channel
pruning. Experiments on various PNN architectures show that CP$^3$ constantly
improves state-of-the-art 2D CNN pruning approaches on different point cloud
tasks. For instance, our compressed PointNeXt-S on ScanObjectNN achieves an
accuracy of 88.52% with a pruning rate of 57.8%, outperforming the baseline
pruning methods with an accuracy gain of 1.94%.</p>
  </details>
</details>
<details>
  <summary>83. <b>标题：Modeling Entities as Semantic Points for Visual Information Extraction  in the Wild</b></summary>
  <p><b>编号</b>：[175]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13095</p>
  <p><b>作者</b>：Zhibo Yang,  Rujiao Long,  Pengfei Wang,  Sibo Song,  Humen Zhong,  Wenqing Cheng,  Xiang Bai,  Cong Yao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：academia and industry, increasingly important, wide range, Information Extraction, Information</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, Visual Information Extraction (VIE) has been becoming increasingly
important in both the academia and industry, due to the wide range of
real-world applications. Previously, numerous works have been proposed to
tackle this problem. However, the benchmarks used to assess these methods are
relatively plain, i.e., scenarios with real-world complexity are not fully
represented in these benchmarks. As the first contribution of this work, we
curate and release a new dataset for VIE, in which the document images are much
more challenging in that they are taken from real applications, and
difficulties such as blur, partial occlusion, and printing shift are quite
common. All these factors may lead to failures in information extraction.
Therefore, as the second contribution, we explore an alternative approach to
precisely and robustly extract key information from document images under such
tough conditions. Specifically, in contrast to previous methods, which usually
either incorporate visual information into a multi-modal architecture or train
text spotting and information extraction in an end-to-end fashion, we
explicitly model entities as semantic points, i.e., center points of entities
are enriched with semantic information describing the attributes and
relationships of different entities, which could largely benefit entity
labeling and linking. Extensive experiments on standard benchmarks in this
field as well as the proposed dataset demonstrate that the proposed method can
achieve significantly enhanced performance on entity labeling and linking,
compared with previous state-of-the-art models. Dataset is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>84. <b>标题：Orthogonal Annotation Benefits Barely-supervised Medical Image  Segmentation</b></summary>
  <p><b>编号</b>：[179]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13090</p>
  <p><b>作者</b>：Heng Cai,  Shumeng Li,  Lei Qi,  Qian Yu,  Yinghuan Shi,  Yang Gao</p>
  <p><b>备注</b>：Accepted to CVPR 2023</p>
  <p><b>关键词</b>：Recent trends, medical image segmentation, semi-supervised medical image, semi-supervised learning, complementary views</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent trends in semi-supervised learning have significantly boosted the
performance of 3D semi-supervised medical image segmentation. Compared with 2D
images, 3D medical volumes involve information from different directions, e.g.,
transverse, sagittal, and coronal planes, so as to naturally provide
complementary views. These complementary views and the intrinsic similarity
among adjacent 3D slices inspire us to develop a novel annotation way and its
corresponding semi-supervised model for effective segmentation. Specifically,
we firstly propose the orthogonal annotation by only labeling two orthogonal
slices in a labeled volume, which significantly relieves the burden of
annotation. Then, we perform registration to obtain the initial pseudo labels
for sparsely labeled volumes. Subsequently, by introducing unlabeled volumes,
we propose a dual-network paradigm named Dense-Sparse Co-training (DeSCO) that
exploits dense pseudo labels in early stage and sparse labels in later stage
and meanwhile forces consistent output of two networks. Experimental results on
three benchmark datasets validated our effectiveness in performance and
efficiency in annotation. For example, with only 10 annotated slices, our
method reaches a Dice up to 86.93% on KiTS19 dataset.</p>
  </details>
</details>
<details>
  <summary>85. <b>标题：Box-Level Active Detection</b></summary>
  <p><b>编号</b>：[180]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13089</p>
  <p><b>作者</b>：Mengyao Lyu,  Jundong Zhou,  Hui Chen,  Yijie Huang,  Dongdong Yu,  Yaqian Li,  Yandong Guo,  Yuchen Guo,  Liuyu Xiang,  Guiguang Ding</p>
  <p><b>备注</b>：CVPR 2023 highlight</p>
  <p><b>关键词</b>：Active learning selects, learning selects informative, selects informative samples, proven efficient recently, learning selects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Active learning selects informative samples for annotation within budget,
which has proven efficient recently on object detection. However, the widely
used active detection benchmarks conduct image-level evaluation, which is
unrealistic in human workload estimation and biased towards crowded images.
Furthermore, existing methods still perform image-level annotation, but equally
scoring all targets within the same image incurs waste of budget and redundant
labels. Having revealed above problems and limitations, we introduce a
box-level active detection framework that controls a box-based budget per
cycle, prioritizes informative targets and avoids redundancy for fair
comparison and efficient application.
Under the proposed box-level setting, we devise a novel pipeline, namely
Complementary Pseudo Active Strategy (ComPAS). It exploits both human
annotations and the model intelligence in a complementary fashion: an efficient
input-end committee queries labels for informative objects only; meantime
well-learned targets are identified by the model and compensated with
pseudo-labels. ComPAS consistently outperforms 10 competitors under 4 settings
in a unified codebase. With supervision from labeled data only, it achieves
100% supervised performance of VOC0712 with merely 19% box annotations. On the
COCO dataset, it yields up to 4.3% mAP improvement over the second-best method.
ComPAS also supports training with the unlabeled pool, where it surpasses 90%
COCO supervised performance with 85% label reduction. Our source code is
publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>86. <b>标题：Robust Generalization against Photon-Limited Corruptions via Worst-Case  Sharpness Minimization</b></summary>
  <p><b>编号</b>：[181]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13087</p>
  <p><b>作者</b>：Zhuo Huang,  Miaoxi Zhu,  Xiaobo Xia,  Li Shen,  Jun Yu,  Chen Gong,  Bo Han,  Bo Du,  Tongliang Liu</p>
  <p><b>备注</b>：CVPR 2023</p>
  <p><b>关键词</b>：challenging data distributions, Robust generalization aims, aims to tackle, Robust generalization, worst-case</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Robust generalization aims to tackle the most challenging data distributions
which are rare in the training set and contain severe noises, i.e.,
photon-limited corruptions. Common solutions such as distributionally robust
optimization (DRO) focus on the worst-case empirical risk to ensure low
training error on the uncommon noisy distributions. However, due to the
over-parameterized model being optimized on scarce worst-case data, DRO fails
to produce a smooth loss landscape, thus struggling on generalizing well to the
test set. Therefore, instead of focusing on the worst-case risk minimization,
we propose SharpDRO by penalizing the sharpness of the worst-case distribution,
which measures the loss changes around the neighbor of learning parameters.
Through worst-case sharpness minimization, the proposed method successfully
produces a flat loss curve on the corrupted distributions, thus achieving
robust generalization. Moreover, by considering whether the distribution
annotation is available, we apply SharpDRO to two problem settings and design a
worst-case selection process for robust generalization. Theoretically, we show
that SharpDRO has a great convergence guarantee. Experimentally, we simulate
photon-limited corruptions using CIFAR10/100 and ImageNet30 datasets and show
that SharpDRO exhibits a strong generalization ability against severe
corruptions and exceeds well-known baseline methods with large performance
gains.</p>
  </details>
</details>
<details>
  <summary>87. <b>标题：Improving the Performance of Spiking Neural Networks on Event-based  Datasets with Knowledge Transfer</b></summary>
  <p><b>编号</b>：[184]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13077</p>
  <p><b>作者</b>：Xiang He,  Dongcheng Zhao,  Yang Li,  Guobin Shen,  Qingqun Kong,  Yi Zeng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Spiking neural networks, rich spatial-temporal dynamics, Spiking neural, neural networks, spatial-temporal dynamics</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Spiking neural networks (SNNs) have rich spatial-temporal dynamics, which are
suitable for processing neuromorphic, event-based data. However, event-based
datasets are usually less annotated than static datasets used in traditional
deep learning. Small data scale makes SNNs prone to overfitting and limits the
performance of the SNN. To enhance the generalizability of SNNs on event-based
datasets, we propose a knowledge-transfer framework that leverages static
images to assist in the training on neuromorphic datasets. Our method proposes
domain loss and semantic loss to exploit both domain-invariant and unique
features of these two domains, providing SNNs with more generalized knowledge
for subsequent targeted training on neuromorphic data. Specifically, domain
loss aligns the feature space and aims to capture common features between
static and event-based images, while semantic loss emphasizes that the
differences between samples from different categories should be as large as
possible. Experimental results demonstrate that our method outperforms existing
methods on all mainstream neuromorphic vision datasets. In particular, we
achieve significant performance improvement of 2.7\% and 9.8\% when using only
10\% training data of CIFAR10-DVS and N-Caltech 101 datasets, respectively.</p>
  </details>
</details>
<details>
  <summary>88. <b>标题：CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting  and Anchor Pre-Matching</b></summary>
  <p><b>编号</b>：[185]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13076</p>
  <p><b>作者</b>：Xiaoshi Wu,  Feng Zhu,  Rui Zhao,  Hongsheng Li</p>
  <p><b>备注</b>：11 pages, 4 figures. Accepted by CVPR 2023</p>
  <p><b>关键词</b>：base categories, detection task aiming, aiming at detecting, OVD, OVD benchmark</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Open-vocabulary detection (OVD) is an object detection task aiming at
detecting objects from novel categories beyond the base categories on which the
detector is trained. Recent OVD methods rely on large-scale visual-language
pre-trained models, such as CLIP, for recognizing novel objects. We identify
the two core obstacles that need to be tackled when incorporating these models
into detector training: (1) the distribution mismatch that happens when
applying a VL-model trained on whole images to region recognition tasks; (2)
the difficulty of localizing objects of unseen classes. To overcome these
obstacles, we propose CORA, a DETR-style framework that adapts CLIP for
Open-vocabulary detection by Region prompting and Anchor pre-matching. Region
prompting mitigates the whole-to-region distribution gap by prompting the
region features of the CLIP-based region classifier. Anchor pre-matching helps
learning generalizable object localization by a class-aware matching mechanism.
We evaluate CORA on the COCO OVD benchmark, where we achieve 41.7 AP50 on novel
classes, which outperforms the previous SOTA by 2.4 AP50 even without resorting
to extra training data. When extra training data is available, we train
CORA$^+$ on both ground-truth base-category annotations and additional pseudo
bounding box labels computed by CORA. CORA$^+$ achieves 43.1 AP50 on the COCO
OVD benchmark and 28.1 box APr on the LVIS OVD benchmark.</p>
  </details>
</details>
<details>
  <summary>89. <b>标题：PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360$^{\circ}$</b></summary>
  <p><b>编号</b>：[189]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13071</p>
  <p><b>作者</b>：Sizhe An,  Hongyi Xu,  Yichun Shi,  Guoxian Song,  Umit Ogras,  Linjie Luo</p>
  <p><b>备注</b>：CVPR 2023. Project Page:this https URL</p>
  <p><b>关键词</b>：computer graphics recently, gained increasing interests, computer vision, computer graphics, human head synthesis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Synthesis and reconstruction of 3D human head has gained increasing interests
in computer vision and computer graphics recently. Existing state-of-the-art 3D
generative adversarial networks (GANs) for 3D human head synthesis are either
limited to near-frontal views or hard to preserve 3D consistency in large view
angles. We propose PanoHead, the first 3D-aware generative model that enables
high-quality view-consistent image synthesis of full heads in $360^\circ$ with
diverse appearance and detailed geometry using only in-the-wild unstructured
images for training. At its core, we lift up the representation power of recent
3D GANs and bridge the data alignment gap when training from in-the-wild images
with widely distributed views. Specifically, we propose a novel two-stage
self-adaptive image alignment for robust 3D GAN training. We further introduce
a tri-grid neural volume representation that effectively addresses front-face
and back-head feature entanglement rooted in the widely-adopted tri-plane
formulation. Our method instills prior knowledge of 2D image segmentation in
adversarial learning of 3D neural scene structures, enabling compositable head
synthesis in diverse backgrounds. Benefiting from these designs, our method
significantly outperforms previous 3D GANs, generating high-quality 3D heads
with accurate geometry and diverse appearances, even with long wavy and afro
hairstyles, renderable from arbitrary poses. Furthermore, we show that our
system can reconstruct full 3D heads from single input images for personalized
realistic 3D avatars.</p>
  </details>
</details>
<details>
  <summary>90. <b>标题：Human Guided Ground-truth Generation for Realistic Image  Super-resolution</b></summary>
  <p><b>编号</b>：[190]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13069</p>
  <p><b>作者</b>：Du Chen,  Jie Liang,  Xindong Zhang,  Ming Liu,  Hui Zeng,  Lei Zhang</p>
  <p><b>备注</b>：10 pages. Already accpted by 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</p>
  <p><b>关键词</b>：generate the ground-truth, critical issue, issue for training, image, realistic image super-resolution</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>How to generate the ground-truth (GT) image is a critical issue for training
realistic image super-resolution (Real-ISR) models. Existing methods mostly
take a set of high-resolution (HR) images as GTs and apply various degradations
to simulate their low-resolution (LR) counterparts. Though great progress has
been achieved, such an LR-HR pair generation scheme has several limitations.
First, the perceptual quality of HR images may not be high enough, limiting the
quality of Real-ISR outputs. Second, existing schemes do not consider much
human perception in GT generation, and the trained models tend to produce
over-smoothed results or unpleasant artifacts. With the above considerations,
we propose a human guided GT generation scheme. We first elaborately train
multiple image enhancement models to improve the perceptual quality of HR
images, and enable one LR image having multiple HR counterparts. Human subjects
are then involved to annotate the high quality regions among the enhanced HR
images as GTs, and label the regions with unpleasant artifacts as negative
samples. A human guided GT image dataset with both positive and negative
samples is then constructed, and a loss function is proposed to train the
Real-ISR models. Experiments show that the Real-ISR models trained on our
dataset can produce perceptually more realistic results with less artifacts.
Dataset and codes can be found at this https URL</p>
  </details>
</details>
<details>
  <summary>91. <b>标题：SIEDOB: Semantic Image Editing by Disentangling Object and Background</b></summary>
  <p><b>编号</b>：[194]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13062</p>
  <p><b>作者</b>：Wuyang Luo,  Su Yang,  Xinjian Zhang,  Weishan Zhang</p>
  <p><b>备注</b>：CVPR 2023 highlight paper</p>
  <p><b>关键词</b>：Semantic image editing, textbf, segmentation map, editing provides users, flexible tool</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Semantic image editing provides users with a flexible tool to modify a given
image guided by a corresponding segmentation map. In this task, the features of
the foreground objects and the backgrounds are quite different. However, all
previous methods handle backgrounds and objects as a whole using a monolithic
model. Consequently, they remain limited in processing content-rich images and
suffer from generating unrealistic objects and texture-inconsistent
backgrounds. To address this issue, we propose a novel paradigm,
\textbf{S}emantic \textbf{I}mage \textbf{E}diting by \textbf{D}isentangling
\textbf{O}bject and \textbf{B}ackground (\textbf{SIEDOB}), the core idea of
which is to explicitly leverages several heterogeneous subnetworks for objects
and backgrounds. First, SIEDOB disassembles the edited input into background
regions and instance-level objects. Then, we feed them into the dedicated
generators. Finally, all synthesized parts are embedded in their original
locations and utilize a fusion network to obtain a harmonized result. Moreover,
to produce high-quality edited images, we propose some innovative designs,
including Semantic-Aware Self-Propagation Module, Boundary-Anchored Patch
Discriminator, and Style-Diversity Object Generator, and integrate them into
SIEDOB. We conduct extensive experiments on Cityscapes and ADE20K-Room datasets
and exhibit that our method remarkably outperforms the baselines, especially in
synthesizing realistic and diverse objects and texture-consistent backgrounds.</p>
  </details>
</details>
<details>
  <summary>92. <b>标题：DiffPattern: Layout Pattern Generation via Discrete Diffusion</b></summary>
  <p><b>编号</b>：[195]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13060</p>
  <p><b>作者</b>：Zixiao Wang,  Yunheng Shen,  Wenqian Zhao,  Yang Bai,  Guojin Chen,  Farzan Farnia,  Bei Yu</p>
  <p><b>备注</b>：DAC2023 Accepted</p>
  <p><b>关键词</b>：Deep generative models, generative models dominate, Deep generative, tool, DiffPattern</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep generative models dominate the existing literature in layout pattern
generation. However, leaving the guarantee of legality to an inexplicable
neural network could be problematic in several applications. In this paper, we
propose \tool{DiffPattern} to generate reliable layout patterns.
\tool{DiffPattern} introduces a novel diverse topology generation method via a
discrete diffusion model with compute-efficiently lossless layout pattern
representation. Then a white-box pattern assessment is utilized to generate
legal patterns given desired design rules. Our experiments on several benchmark
settings show that \tool{DiffPattern} significantly outperforms existing
baselines and is capable of synthesizing reliable layout patterns.</p>
  </details>
</details>
<details>
  <summary>93. <b>标题：Hierarchical Semantic Contrast for Scene-aware Video Anomaly Detection</b></summary>
  <p><b>编号</b>：[200]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13051</p>
  <p><b>作者</b>：Shengyang Sun,  Xiaojin Gong</p>
  <p><b>备注</b>：Accepted by CVPR 2023</p>
  <p><b>关键词</b>：video anomaly detection, Increasing scene-awareness, anomaly detection, key challenge, scene-aware VAD model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Increasing scene-awareness is a key challenge in video anomaly detection
(VAD). In this work, we propose a hierarchical semantic contrast (HSC) method
to learn a scene-aware VAD model from normal videos. We first incorporate
foreground object and background scene features with high-level semantics by
taking advantage of pre-trained video parsing models. Then, building upon the
autoencoder-based reconstruction framework, we introduce both scene-level and
object-level contrastive learning to enforce the encoded latent features to be
compact within the same semantic classes while being separable across different
classes. This hierarchical semantic contrast strategy helps to deal with the
diversity of normal patterns and also increases their discrimination ability.
Moreover, for the sake of tackling rare normal activities, we design a
skeleton-based motion augmentation to increase samples and refine the model
further. Extensive experiments on three public datasets and scene-dependent
mixture datasets validate the effectiveness of our proposed method.</p>
  </details>
</details>
<details>
  <summary>94. <b>标题：Top-Down Visual Attention from Analysis by Synthesis</b></summary>
  <p><b>编号</b>：[204]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13043</p>
  <p><b>作者</b>：Baifeng Shi,  Trevor Darrell,  Xin Wang</p>
  <p><b>备注</b>：CVPR2023 highlight; Project page: this https URL</p>
  <p><b>关键词</b>：Current attention algorithms, stimulus-driven and highlight, attention, top-down attention, top-down</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Current attention algorithms (e.g., self-attention) are stimulus-driven and
highlight all the salient objects in an image. However, intelligent agents like
humans often guide their attention based on the high-level task at hand,
focusing only on task-related objects. This ability of task-guided top-down
attention provides task-adaptive representation and helps the model generalize
to various tasks. In this paper, we consider top-down attention from a classic
Analysis-by-Synthesis (AbS) perspective of vision. Prior work indicates a
functional equivalence between visual attention and sparse reconstruction; we
show that an AbS visual system that optimizes a similar sparse reconstruction
objective modulated by a goal-directed top-down signal naturally simulates
top-down attention. We further propose Analysis-by-Synthesis Vision Transformer
(AbSViT), which is a top-down modulated ViT model that variationally
approximates AbS, and achieves controllable top-down attention. For real-world
applications, AbSViT consistently improves over baselines on Vision-Language
tasks such as VQA and zero-shot retrieval where language guides the top-down
attention. AbSViT can also serve as a general backbone, improving performance
on classification, semantic segmentation, and model robustness.</p>
  </details>
</details>
<details>
  <summary>95. <b>标题：Open-Vocabulary Object Detection using Pseudo Caption Labels</b></summary>
  <p><b>编号</b>：[206]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13040</p>
  <p><b>作者</b>：Han-Cheol Cho,  Won Young Jhoo,  Wooyoung Kang,  Byungseok Roh</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Recent open-vocabulary detection, detection methods aim, open-vocabulary detection methods, Recent open-vocabulary, image-text pairs</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent open-vocabulary detection methods aim to detect novel objects by
distilling knowledge from vision-language models (VLMs) trained on a vast
amount of image-text pairs. To improve the effectiveness of these methods,
researchers have utilized datasets with a large vocabulary that contains a
large number of object classes, under the assumption that such data will enable
models to extract comprehensive knowledge on the relationships between various
objects and better generalize to unseen object classes. In this study, we argue
that more fine-grained labels are necessary to extract richer knowledge about
novel objects, including object attributes and relationships, in addition to
their names. To address this challenge, we propose a simple and effective
method named Pseudo Caption Labeling (PCL), which utilizes an image captioning
model to generate captions that describe object instances from diverse
perspectives. The resulting pseudo caption labels offer dense samples for
knowledge distillation. On the LVIS benchmark, our best model trained on the
de-duplicated VisualGenome dataset achieves an AP of 34.5 and an APr of 30.6,
comparable to the state-of-the-art performance. PCL's simplicity and
flexibility are other notable features, as it is a straightforward
pre-processing technique that can be used with any image captioning model
without imposing any restrictions on model architecture or training process.</p>
  </details>
</details>
<details>
  <summary>96. <b>标题：Learning a Practical SDR-to-HDRTV Up-conversion using New Dataset and  Degradation Models</b></summary>
  <p><b>编号</b>：[211]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13031</p>
  <p><b>作者</b>：Cheng Guo,  Leidong Fan,  Ziyu Xue,  and Xiuhua Jiang</p>
  <p><b>备注</b>：Accepted by CVPR2023</p>
  <p><b>关键词</b>：high dynamic range-wide, users possess HDR-WCG, range-wide color gamut, dynamic range-wide color, standard dynamic range</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In media industry, the demand of SDR-to-HDRTV up-conversion arises when users
possess HDR-WCG (high dynamic range-wide color gamut) TVs while most
off-the-shelf footage is still in SDR (standard dynamic range). The research
community has started tackling this low-level vision task by learning-based
approaches. When applied to real SDR, yet, current methods tend to produce dim
and desaturated result, making nearly no improvement on viewing experience.
Different from other network-oriented methods, we attribute such deficiency to
training set (HDR-SDR pair). Consequently, we propose new HDRTV dataset (dubbed
HDRTV4K) and new HDR-to-SDR degradation models. Then, it's used to train a
luminance-segmented network (LSN) consisting of a global mapping trunk, and two
Transformer branches on bright and dark luminance range. We also update
assessment criteria by tailored metrics and subjective experiment. Finally,
ablation studies are conducted to prove the effectiveness. Our work is
available at: this https URL.</p>
  </details>
</details>
<details>
  <summary>97. <b>标题：ENVIDR: Implicit Differentiable Renderer with Neural Environment  Lighting</b></summary>
  <p><b>编号</b>：[215]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13022</p>
  <p><b>作者</b>：Ruofan Liang,  Huiting Chen,  Chunlin Li,  Fan Chen,  Selvakumar Panneer,  Nandita Vijaykumar</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：shown great potential, Recent advances, multiview images, shown great, great potential</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advances in neural rendering have shown great potential for
reconstructing scenes from multiview images. However, accurately representing
objects with glossy surfaces remains a challenge for existing methods. In this
work, we introduce ENVIDR, a rendering and modeling framework for high-quality
rendering and reconstruction of surfaces with challenging specular reflections.
To achieve this, we first propose a novel neural renderer with decomposed
rendering components to learn the interaction between surface and environment
lighting. This renderer is trained using existing physically based renderers
and is decoupled from actual scene representations. We then propose an
SDF-based neural surface model that leverages this learned neural renderer to
represent general scenes. Our model additionally synthesizes indirect
illuminations caused by inter-reflections from shiny surfaces by marching
surface-reflected rays. We demonstrate that our method outperforms state-of-art
methods on challenging shiny scenes, providing high-quality rendering of
specular reflections while also enabling material editing and scene relighting.</p>
  </details>
</details>
<details>
  <summary>98. <b>标题：MonoATT: Online Monocular 3D Object Detection with Adaptive Token  Transformer</b></summary>
  <p><b>编号</b>：[217]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13018</p>
  <p><b>作者</b>：Yunsong Zhou,  Hongzi Zhu,  Quan Liu,  Shan Chang,  Minyi Guo</p>
  <p><b>备注</b>：in the Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023</p>
  <p><b>关键词</b>：challenging task, Mobile monocular, tokens, MonoATT, Existing transformer-based offline</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Mobile monocular 3D object detection (Mono3D) (e.g., on a vehicle, a drone,
or a robot) is an important yet challenging task. Existing transformer-based
offline Mono3D models adopt grid-based vision tokens, which is suboptimal when
using coarse tokens due to the limited available computational power. In this
paper, we propose an online Mono3D framework, called MonoATT, which leverages a
novel vision transformer with heterogeneous tokens of varying shapes and sizes
to facilitate mobile Mono3D. The core idea of MonoATT is to adaptively assign
finer tokens to areas of more significance before utilizing a transformer to
enhance Mono3D. To this end, we first use prior knowledge to design a scoring
network for selecting the most important areas of the image, and then propose a
token clustering and merging network with an attention mechanism to gradually
merge tokens around the selected areas in multiple stages. Finally, a
pixel-level feature map is reconstructed from heterogeneous tokens before
employing a SOTA Mono3D detector as the underlying detection core. Experiment
results on the real-world KITTI dataset demonstrate that MonoATT can
effectively improve the Mono3D accuracy for both near and far objects and
guarantee low latency. MonoATT yields the best performance compared with the
state-of-the-art methods by a large margin and is ranked number one on the
KITTI 3D benchmark.</p>
  </details>
</details>
<details>
  <summary>99. <b>标题：Semantic Ray: Learning a Generalizable Semantic Field with  Cross-Reprojection Attention</b></summary>
  <p><b>编号</b>：[220]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13014</p>
  <p><b>作者</b>：Fangfu Liu,  Chubin Zhang,  Yu Zheng,  Yueqi Duan</p>
  <p><b>备注</b>：Accepted by CVPR 2023. Project page: this https URL</p>
  <p><b>关键词</b>：semantic radiance field, efficient and generalizable, radiance field, semantic, semantic radiance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we aim to learn a semantic radiance field from multiple scenes
that is accurate, efficient and generalizable. While most existing NeRFs target
at the tasks of neural scene rendering, image synthesis and multi-view
reconstruction, there are a few attempts such as Semantic-NeRF that explore to
learn high-level semantic understanding with the NeRF structure. However,
Semantic-NeRF simultaneously learns color and semantic label from a single ray
with multiple heads, where the single ray fails to provide rich semantic
information. As a result, Semantic NeRF relies on positional encoding and needs
to train one specific model for each scene. To address this, we propose
Semantic Ray (S-Ray) to fully exploit semantic information along the ray
direction from its multi-view reprojections. As directly performing dense
attention over multi-view reprojected rays would suffer from heavy
computational cost, we design a Cross-Reprojection Attention module with
consecutive intra-view radial and cross-view sparse attentions, which
decomposes contextual information along reprojected rays and cross multiple
views and then collects dense connections by stacking the modules. Experiments
show that our S-Ray is able to learn from multiple scenes, and it presents
strong generalization ability to adapt to unseen scenes.</p>
  </details>
</details>
<details>
  <summary>100. <b>标题：GesGPT: Speech Gesture Synthesis With Text Parsing from GPT</b></summary>
  <p><b>编号</b>：[221]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13013</p>
  <p><b>作者</b>：Nan Gao,  Zeyu Zhao,  Zhi Zeng,  Shuwu Zhang,  Dongdong Weng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：critical research area, gained significant attention, Large Language Models, research area, focusing on producing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Gesture synthesis has gained significant attention as a critical research
area, focusing on producing contextually appropriate and natural gestures
corresponding to speech or textual input. Although deep learning-based
approaches have achieved remarkable progress, they often overlook the rich
semantic information present in the text, leading to less expressive and
meaningful gestures. We propose GesGPT, a novel approach to gesture generation
that leverages the semantic analysis capabilities of Large Language Models
(LLMs), such as GPT. By capitalizing on the strengths of LLMs for text
analysis, we design prompts to extract gesture-related information from textual
input. Our method entails developing prompt principles that transform gesture
generation into an intention classification problem based on GPT, and utilizing
a curated gesture library and integration module to produce semantically rich
co-speech gestures. Experimental results demonstrate that GesGPT effectively
generates contextually appropriate and expressive gestures, offering a new
perspective on semantic co-speech gesture generation.</p>
  </details>
</details>
<details>
  <summary>101. <b>标题：Semantic Image Attack for Visual Model Diagnosis</b></summary>
  <p><b>编号</b>：[222]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13010</p>
  <p><b>作者</b>：Jinqi Luo,  Zhaoning Wang,  Chen Henry Wu,  Dong Huang,  Fernando De la Torre</p>
  <p><b>备注</b>：Initial version submitted to NeurIPS 2022</p>
  <p><b>关键词</b>：specific train, guarantee reliable, reliable or fair, SIA, model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In practice, metric analysis on a specific train and test dataset does not
guarantee reliable or fair ML models. This is partially due to the fact that
obtaining a balanced, diverse, and perfectly labeled dataset is typically
expensive, time-consuming, and error-prone. Rather than relying on a carefully
designed test set to assess ML models' failures, fairness, or robustness, this
paper proposes Semantic Image Attack (SIA), a method based on the adversarial
attack that provides semantic adversarial images to allow model diagnosis,
interpretability, and robustness. Traditional adversarial training is a popular
methodology for robustifying ML models against attacks. However, existing
adversarial methods do not combine the two aspects that enable the
interpretation and analysis of the model's flaws: semantic traceability and
perceptual quality. SIA combines the two features via iterative gradient ascent
on a predefined semantic attribute space and the image space. We illustrate the
validity of our approach in three scenarios for keypoint detection and
classification. (1) Model diagnosis: SIA generates a histogram of attributes
that highlights the semantic vulnerability of the ML model (i.e., attributes
that make the model fail). (2) Stronger attacks: SIA generates adversarial
examples with visually interpretable attributes that lead to higher attack
success rates than baseline methods. The adversarial training on SIA improves
the transferable robustness across different gradient-based attacks. (3)
Robustness to imbalanced datasets: we use SIA to augment the underrepresented
classes, which outperforms strong augmentation and re-balancing baselines.</p>
  </details>
</details>
<details>
  <summary>102. <b>标题：MELTR: Meta Loss Transformer for Learning to Fine-tune Video Foundation  Models</b></summary>
  <p><b>编号</b>：[223]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13009</p>
  <p><b>作者</b>：Dohwan Ko,  Joonmyung Choi,  Hyeong Kyu Choi,  Kyoung-Woon On,  Byungseok Roh,  Hyunwoo J. Kim</p>
  <p><b>备注</b>：Accepted paper at CVPR 2023</p>
  <p><b>关键词</b>：shown outstanding performance, Foundation models, capabilities across domains, shown outstanding, generalization capabilities</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Foundation models have shown outstanding performance and generalization
capabilities across domains. Since most studies on foundation models mainly
focus on the pretraining phase, a naive strategy to minimize a single
task-specific loss is adopted for fine-tuning. However, such fine-tuning
methods do not fully leverage other losses that are potentially beneficial for
the target task. Therefore, we propose MEta Loss TRansformer (MELTR), a plug-in
module that automatically and non-linearly combines various loss functions to
aid learning the target task via auxiliary learning. We formulate the auxiliary
learning as a bi-level optimization problem and present an efficient
optimization algorithm based on Approximate Implicit Differentiation (AID). For
evaluation, we apply our framework to various video foundation models (UniVL,
Violet and All-in-one), and show significant performance gain on all four
downstream tasks: text-to-video retrieval, video question answering, video
captioning, and multi-modal sentiment analysis. Our qualitative analyses
demonstrate that MELTR adequately `transforms' individual loss functions and
`melts' them into an effective unified loss. Code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>103. <b>标题：Controllable Inversion of Black-Box Face-Recognition Models via  Diffusion</b></summary>
  <p><b>编号</b>：[224]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13006</p>
  <p><b>作者</b>：Manuel Kansy,  Anton Raël,  Graziana Mignone,  Jacek Naruniec,  Christopher Schroers,  Markus Gross,  Romann M. Weber</p>
  <p><b>备注</b>：34 pages. Preprint. Under review</p>
  <p><b>关键词</b>：Face recognition models, recognition models embed, Face recognition, identity-specific facial features, low-dimensional identity vector</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Face recognition models embed a face image into a low-dimensional identity
vector containing abstract encodings of identity-specific facial features that
allow individuals to be distinguished from one another. We tackle the
challenging task of inverting the latent space of pre-trained face recognition
models without full model access (i.e. black-box setting). A variety of methods
have been proposed in literature for this task, but they have serious
shortcomings such as a lack of realistic outputs, long inference times, and
strong requirements for the data set and accessibility of the face recognition
model. Through an analysis of the black-box inversion problem, we show that the
conditional diffusion model loss naturally emerges and that we can effectively
sample from the inverse distribution even without an identity-specific loss.
Our method, named identity denoising diffusion probabilistic model (ID3PM),
leverages the stochastic nature of the denoising diffusion process to produce
high-quality, identity-preserving face images with various backgrounds,
lighting, poses, and expressions. We demonstrate state-of-the-art performance
in terms of identity preservation and diversity both qualitatively and
quantitatively. Our method is the first black-box face recognition model
inversion method that offers intuitive control over the generation process and
does not suffer from any of the common shortcomings from competing methods.</p>
  </details>
</details>
<details>
  <summary>104. <b>标题：From Knowledge Distillation to Self-Knowledge Distillation: A Unified  Approach with Normalized Loss and Customized Soft Labels</b></summary>
  <p><b>编号</b>：[225]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13005</p>
  <p><b>作者</b>：Zhendong Yang,  Ailing Zeng,  Zhe Li,  Tianke Zhang,  Chun Yuan,  Yu Li</p>
  <p><b>备注</b>：12 pages, 6 figures, 12 tables</p>
  <p><b>关键词</b>：soft labels, Universal Self-Knowledge Distillation, customized soft labels, soft non-target labels, teacher prediction logits</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Knowledge Distillation (KD) uses the teacher's prediction logits as soft
labels to guide the student, while self-KD does not need a real teacher to
require the soft labels. This work unifies the formulations of the two tasks by
decomposing and reorganizing the generic KD loss into a Normalized KD (NKD)
loss and customized soft labels for both target class (image's category) and
non-target classes named Universal Self-Knowledge Distillation (USKD). We
decompose the KD loss and find the non-target loss from it forces the student's
non-target logits to match the teacher's, but the sum of the two non-target
logits is different, preventing them from being identical. NKD normalizes the
non-target logits to equalize their sum. It can be generally used for KD and
self-KD to better use the soft labels for distillation loss. USKD generates
customized soft labels for both target and non-target classes without a
teacher. It smooths the target logit of the student as the soft target label
and uses the rank of the intermediate feature to generate the soft non-target
labels with Zipf's law. For KD with teachers, our NKD achieves state-of-the-art
performance on CIFAR-100 and ImageNet datasets, boosting the ImageNet Top-1
accuracy of ResNet18 from 69.90% to 71.96% with a ResNet-34 teacher. For
self-KD without teachers, USKD is the first self-KD method that can be
effectively applied to both CNN and ViT models with negligible additional time
and memory cost, resulting in new state-of-the-art results, such as 1.17% and
0.55% accuracy gains on ImageNet for MobileNet and DeiT-Tiny, respectively. Our
codes are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>105. <b>标题：Benchmarking the Reliability of Post-training Quantization: a Particular  Focus on Worst-case Performance</b></summary>
  <p><b>编号</b>：[227]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13003</p>
  <p><b>作者</b>：Zhihang Yuan,  Jiawei Liu,  Jiaxiang Wu,  Dawei Yang,  Qiang Wu,  Guangyu Sun,  Wenyu Liu,  Xinggang Wang,  Bingzhe Wu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：deep neural networks, compressing deep neural, Post-training quantization, PTQ methods, PTQ</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Post-training quantization (PTQ) is a popular method for compressing deep
neural networks (DNNs) without modifying their original architecture or
training procedures. Despite its effectiveness and convenience, the reliability
of PTQ methods in the presence of some extrem cases such as distribution shift
and data noise remains largely unexplored. This paper first investigates this
problem on various commonly-used PTQ methods. We aim to answer several research
questions related to the influence of calibration set distribution variations,
calibration paradigm selection, and data augmentation or sampling strategies on
PTQ reliability. A systematic evaluation process is conducted across a wide
range of tasks and commonly-used PTQ paradigms. The results show that most
existing PTQ methods are not reliable enough in term of the worst-case group
performance, highlighting the need for more robust methods. Our findings
provide insights for developing PTQ methods that can effectively handle
distribution shift scenarios and enable the deployment of quantized DNNs in
real-world applications.</p>
  </details>
</details>
<details>
  <summary>106. <b>标题：FER-former: Multi-modal Transformer for Facial Expression Recognition</b></summary>
  <p><b>编号</b>：[233]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12997</p>
  <p><b>作者</b>：Yande Li,  Mingjie Wang,  Minglun Gong,  Yonggang Lu,  Li Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Facial Expression Recognition, Expression Recognition, Virtual Reality, Facial Expression, interactions in Virtual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The ever-increasing demands for intuitive interactions in Virtual Reality has
triggered a boom in the realm of Facial Expression Recognition (FER). To
address the limitations in existing approaches (e.g., narrow receptive fields
and homogenous supervisory signals) and further cement the capacity of FER
tools, a novel multifarious supervision-steering Transformer for FER in the
wild is proposed in this paper. Referred as FER-former, our approach features
multi-granularity embedding integration, hybrid self-attention scheme, and
heterogeneous domain-steering supervision. In specific, to dig deep into the
merits of the combination of features provided by prevailing CNNs and
Transformers, a hybrid stem is designed to cascade two types of learning
paradigms simultaneously. Wherein, a FER-specific transformer mechanism is
devised to characterize conventional hard one-hot label-focusing and CLIP-based
text-oriented tokens in parallel for final classification. To ease the issue of
annotation ambiguity, a heterogeneous domains-steering supervision module is
proposed to make image features also have text-space semantic correlations by
supervising the similarity between image features and text features. On top of
the collaboration of multifarious token heads, diverse global receptive fields
with multi-modal semantic cues are captured, thereby delivering superb learning
capability. Extensive experiments on popular benchmarks demonstrate the
superiority of the proposed FER-former over the existing state-of-the-arts.</p>
  </details>
</details>
<details>
  <summary>107. <b>标题：Backdoor Defense via Adaptively Splitting Poisoned Dataset</b></summary>
  <p><b>编号</b>：[235]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12993</p>
  <p><b>作者</b>：Kuofeng Gao,  Yang Bai,  Jindong Gu,  Yong Yang,  Shu-Tao Xia</p>
  <p><b>备注</b>：Accepted by CVPR 2023</p>
  <p><b>关键词</b>：deep neural networks, neural networks, maliciously altered, studied to alleviate, alleviate the threat</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Backdoor defenses have been studied to alleviate the threat of deep neural
networks (DNNs) being backdoor attacked and thus maliciously altered. Since
DNNs usually adopt some external training data from an untrusted third party, a
robust backdoor defense strategy during the training stage is of importance. We
argue that the core of training-time defense is to select poisoned samples and
to handle them properly. In this work, we summarize the training-time defenses
from a unified framework as splitting the poisoned dataset into two data pools.
Under our framework, we propose an adaptively splitting dataset-based defense
(ASD). Concretely, we apply loss-guided split and meta-learning-inspired split
to dynamically update two data pools. With the split clean data pool and
polluted data pool, ASD successfully defends against backdoor attacks during
training. Extensive experiments on multiple benchmark datasets and DNN models
against six state-of-the-art backdoor attacks demonstrate the superiority of
our ASD. Our code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>108. <b>标题：A Survey of Historical Learning: Learning Models with Learning History</b></summary>
  <p><b>编号</b>：[236]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12992</p>
  <p><b>作者</b>：Xiang Li,  Ge Wu,  Lingfeng Yang,  Wenhai Wang,  Renjie Song,  Jian Yang</p>
  <p><b>备注</b>：Xiang Li and Ge Wu have equal contributions</p>
  <p><b>关键词</b>：learning, Historical Learning, Learning History, Historical, learning deep models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>New knowledge originates from the old. The various types of elements,
deposited in the training history, are a large amount of wealth for improving
learning deep models. In this survey, we comprehensively review and summarize
the topic--``Historical Learning: Learning Models with Learning History'',
which learns better neural models with the help of their learning history
during its optimization, from three detailed aspects: Historical Type (what),
Functional Part (where) and Storage Form (how). To our best knowledge, it is
the first survey that systematically studies the methodologies which make use
of various historical statistics when training deep neural networks. The
discussions with related topics like recurrent/memory networks, ensemble
learning, and reinforcement learning are demonstrated. We also expose future
challenges of this topic and encourage the community to pay attention to the
think of historical learning principles when designing algorithms. The paper
list related to historical learning is available at
\url{this https URL.}</p>
  </details>
</details>
<details>
  <summary>109. <b>标题：NVAutoNet: Fast and Accurate 360$^{\circ}$ 3D Perception For Self  Driving</b></summary>
  <p><b>编号</b>：[242]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12976</p>
  <p><b>作者</b>：Trung Pham,  Mehran Maghoumi,  Wanli Jiang,  Bala Siva Sashank Jujjavarapu,  Mehdi Sajjadi Xin Liu,  Hsuan-Chu Lin,  Bor-Jeng Chen,  Giang Truong,  Chao Fang,  Junghyun Kwon,  Minwoo Park</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：world is essential, Robust real-time perception, real-time perception, camera perception system, perception system</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Robust real-time perception of 3D world is essential to the autonomous
vehicle. We introduce an end-to-end surround camera perception system for
self-driving. Our perception system is a novel multi-task, multi-camera network
which takes a variable set of time-synced camera images as input and produces a
rich collection of 3D signals such as sizes, orientations, locations of
obstacles, parking spaces and free-spaces, etc. Our perception network is
modular and end-to-end: 1) the outputs can be consumed directly by downstream
modules without any post-processing such as clustering and fusion -- improving
speed of model deployment and in-car testing 2) the whole network training is
done in one single stage -- improving speed of model improvement and
iterations. The network is well designed to have high accuracy while running at
53 fps on NVIDIA Orin SoC (system-on-a-chip). The network is robust to sensor
mounting variations (within some tolerances) and can be quickly customized for
different vehicle types via efficient model fine-tuning thanks of its
capability of taking calibration parameters as additional inputs during
training and testing. Most importantly, our network has been successfully
deployed and being tested on real roads.</p>
  </details>
</details>
<details>
  <summary>110. <b>标题：Performance Analysis and Evaluation of Cloud Vision Emotion APIs</b></summary>
  <p><b>编号</b>：[243]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12974</p>
  <p><b>作者</b>：Salik Ram Khanal,  Prabin Sharma,  Hugo Fernandes,  João Barroso,  Vítor Manuel de Jesus Filipe</p>
  <p><b>备注</b>：10 pages, 6 figures</p>
  <p><b>关键词</b>：interact with computers, electronic devices, emerging practice, Facial expression, cloud-based vision application</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Facial expression is a way of communication that can be used to interact with
computers or other electronic devices and the recognition of emotion from faces
is an emerging practice with application in many fields. There are many
cloud-based vision application programming interfaces available that recognize
emotion from facial images and video. In this article, the performances of two
well-known APIs were compared using a public dataset of 980 images of facial
emotions. For these experiments, a client program was developed which iterates
over the image set, calls the cloud services, and caches the results of the
emotion detection for each image. The performance was evaluated in each class
of emotions using prediction accuracy. It has been found that the prediction
accuracy for each emotion varies according to the cloud service being used.
Similarly, each service provider presents a strong variation of performance
according to the class being analyzed, as can be seen with more detail in this
artilects.</p>
  </details>
</details>
<details>
  <summary>111. <b>标题：Efficient Meshy Neural Fields for Animatable Human Avatars</b></summary>
  <p><b>编号</b>：[247]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12965</p>
  <p><b>作者</b>：Xiaoke Huang,  Yiji Cheng,  Yansong Tang,  Xiu Li,  Jie Zhou,  Jiwen Lu</p>
  <p><b>备注</b>：25 pages, 21 figures. Project page: this https URL</p>
  <p><b>关键词</b>：active research topic, Efficiently digitizing high-fidelity, digitizing high-fidelity animatable, animatable human avatars, research topic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Efficiently digitizing high-fidelity animatable human avatars from videos is
a challenging and active research topic. Recent volume rendering-based neural
representations open a new way for human digitization with their friendly
usability and photo-realistic reconstruction quality. However, they are
inefficient for long optimization times and slow inference speed; their
implicit nature results in entangled geometry, materials, and dynamics of
humans, which are hard to edit afterward. Such drawbacks prevent their direct
applicability to downstream applications, especially the prominent
rasterization-based graphic ones. We present EMA, a method that Efficiently
learns Meshy neural fields to reconstruct animatable human Avatars. It jointly
optimizes explicit triangular canonical mesh, spatial-varying material, and
motion dynamics, via inverse rendering in an end-to-end fashion. Each above
component is derived from separate neural fields, relaxing the requirement of a
template, or rigging. The mesh representation is highly compatible with the
efficient rasterization-based renderer, thus our method only takes about an
hour of training and can render in real-time. Moreover, only minutes of
optimization is enough for plausible reconstruction results. The
disentanglement of meshes enables direct downstream applications. Extensive
experiments illustrate the very competitive performance and significant speed
boost against previous methods. We also showcase applications including novel
pose synthesis, material editing, and relighting. The project page:
this https URL.</p>
  </details>
</details>
<details>
  <summary>112. <b>标题：Continuous Indeterminate Probability Neural Network</b></summary>
  <p><b>编号</b>：[248]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12964</p>
  <p><b>作者</b>：Tao Yang</p>
  <p><b>备注</b>：8 pages</p>
  <p><b>关键词</b>：Continuous Indeterminate, paper introduces, latent random variables, Continuous, latent</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper introduces a general model called CIPNN - Continuous Indeterminate
Probability Neural Network, and this model is based on IPNN, which is used for
discrete latent random variables. Currently, posterior of continuous latent
variables is regarded as intractable, with the new theory proposed by IPNN this
problem can be solved. Our contributions are Four-fold. First, we derive the
analytical solution of the posterior calculation of continuous latent random
variables and propose a general classification model (CIPNN). Second, we
propose a general auto-encoder called CIPAE - Continuous Indeterminate
Probability Auto-Encoder, the decoder part is not a neural network and uses a
fully probabilistic inference model for the first time. Third, we propose a new
method to visualize the latent random variables, we use one of N dimensional
latent variables as a decoder to reconstruct the input image, which can work
even for classification tasks, in this way, we can see what each latent
variable has learned. Fourth, IPNN has shown great classification capability,
CIPNN has pushed this classification capability to infinity. Theoretical
advantages are reflected in experimental results.</p>
  </details>
</details>
<details>
  <summary>113. <b>标题：LightPainter: Interactive Portrait Relighting with Freehand Scribble</b></summary>
  <p><b>编号</b>：[254]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12950</p>
  <p><b>作者</b>：Yiqun Mei,  He Zhang,  Xuaner Zhang,  Jianming Zhang,  Zhixin Shu,  Yilin Wang,  Zijun Wei,  Shi Yan,  HyunJoon Jung,  Vishal M. Patel</p>
  <p><b>备注</b>：CVPR2023</p>
  <p><b>关键词</b>：desired lighting representation, achieved realistic results, environment map, realistic results, lighting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent portrait relighting methods have achieved realistic results of
portrait lighting effects given a desired lighting representation such as an
environment map. However, these methods are not intuitive for user interaction
and lack precise lighting control. We introduce LightPainter, a scribble-based
relighting system that allows users to interactively manipulate portrait
lighting effect with ease. This is achieved by two conditional neural networks,
a delighting module that recovers geometry and albedo optionally conditioned on
skin tone, and a scribble-based module for relighting. To train the relighting
module, we propose a novel scribble simulation procedure to mimic real user
scribbles, which allows our pipeline to be trained without any human
annotations. We demonstrate high-quality and flexible portrait lighting editing
capability with both quantitative and qualitative experiments. User study
comparisons with commercial lighting editing tools also demonstrate consistent
user preference for our method.</p>
  </details>
</details>
<details>
  <summary>114. <b>标题：FTSO: Effective NAS via First Topology Second Operator</b></summary>
  <p><b>编号</b>：[256]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12948</p>
  <p><b>作者</b>：Likang Wang,  Lei Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Existing one-shot neural, huge computational cost, one-shot neural architecture, neural architecture search, Existing one-shot</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing one-shot neural architecture search (NAS) methods have to conduct a
search over a giant super-net, which leads to the huge computational cost. To
reduce such cost, in this paper, we propose a method, called FTSO, to divide
the whole architecture search into two sub-steps. Specifically, in the first
step, we only search for the topology, and in the second step, we search for
the operators. FTSO not only reduces NAS's search time from days to 0.68
seconds, but also significantly improves the found architecture's accuracy. Our
extensive experiments on ImageNet show that within 18 seconds, FTSO can achieve
a 76.4% testing accuracy, 1.5% higher than the SOTA, PC-DARTS. In addition,
FTSO can reach a 97.77% testing accuracy, 0.27% higher than the SOTA, with
nearly 100% (99.8%) search time saved, when searching on CIFAR10.</p>
  </details>
</details>
<details>
  <summary>115. <b>标题：Underwater Camouflage Object Detection Dataset</b></summary>
  <p><b>编号</b>：[258]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12946</p>
  <p><b>作者</b>：Feng Dong,  Jinchao Zhu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：complex seabed scenes, camouflage object detection, Sonar,or UW-RS, UnderWater RGB, seabed scenes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We have made a dataset of camouflage object detection mainly for complex
seabed scenes, and named it UnderWater RGB&Sonar,or UW-RS for short. The UW-RS
dataset contains a total of 1972 image data. The dataset mainly consists of two
parts, namely underwater optical data part (UW-R dataset) and underwater sonar
data part (UW-S dataset).</p>
  </details>
</details>
<details>
  <summary>116. <b>标题：Cryptocurrency wallets: assessment and security</b></summary>
  <p><b>编号</b>：[261]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12940</p>
  <p><b>作者</b>：Ehsan Nowroozi,  Seyedsadra Seyedshoari,  Yassine Mekdad,  Erkay Savas,  Mauro Conti</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：digital wallets, Digital, wallets, software program, digital device</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Digital wallet as a software program or a digital device allows users to
conduct various transactions. Hot and cold digital wallets are considered as
two types of this wallet. Digital wallets need an online connection fall into
the first group, whereas digital wallets can operate without internet
connection belong to the second group. Prior to buying a digital wallet, it is
important to define for what purpose it will be utilized. The ease with which a
mobile phone transaction may be completed in a couple of seconds and the speed
with which transactions are executed are reflection of efficiency. One of the
most important elements of digital wallets is data organization. Digital
wallets are significantly less expensive than classic methods of transaction,
which entails various charges and fees. Constantly, demand for their usage is
growing due to speed, security, and the ability to conduct transactions between
two users without the need of a third party. As the popularity of digital
currency wallets grows, the number of security concerns impacting them
increases significantly. The current status of digital wallets on the market,
as well as the options for an efficient solution for obtaining and utilizing
digital wallets. Finally, the digital wallets' security and future improvement
prospects are discussed in this chapter.</p>
  </details>
</details>
<details>
  <summary>117. <b>标题：Real-World Community-in-the-Loop Smart Video Surveillance -- A Case  Study at a Community College</b></summary>
  <p><b>编号</b>：[265]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12934</p>
  <p><b>作者</b>：Shanle Yao,  Babak Rahimi Ardabili,  Armin Danesh Pazho,  Ghazal Alinezhad Noghre,  Christopher Neff,  Hamed Tabkhi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ensuring public safety, Smart Video surveillance, Video surveillance systems, important recently, recently for ensuring</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Smart Video surveillance systems have become important recently for ensuring
public safety and security, especially in smart cities. However, applying
real-time artificial intelligence technologies combined with low-latency
notification and alarming has made deploying these systems quite challenging.
This paper presents a case study for designing and deploying smart video
surveillance systems based on a real-world testbed at a community college. We
primarily focus on a smart camera-based system that can identify
suspicious/abnormal activities and alert the stakeholders and residents
immediately. The paper highlights and addresses different algorithmic and
system design challenges to guarantee real-time high-accuracy video analytics
processing in the testbed. It also presents an example of cloud system
infrastructure and a mobile application for real-time notification to keep
students, faculty/staff, and responsible security personnel in the loop. At the
same time, it covers the design decision to maintain communities' privacy and
ethical requirements as well as hardware configuration and setups. We evaluate
the system's performance using throughput and end-to-end latency. The
experiment results show that, on average, our system's end-to-end latency to
notify the end users in case of detecting suspicious objects is 5.3, 5.78, and
11.11 seconds when running 1, 4, and 8 cameras, respectively. On the other
hand, in case of detecting anomalous behaviors, the system could notify the end
users with 7.3, 7.63, and 20.78 seconds average latency. These results
demonstrate that the system effectively detects and notifies abnormal behaviors
and suspicious objects to the end users within a reasonable period. The system
can run eight cameras simultaneously at a 32.41 Frame Per Second (FPS) rate.</p>
  </details>
</details>
<details>
  <summary>118. <b>标题：Dense-Localizing Audio-Visual Events in Untrimmed Videos: A Large-Scale  Benchmark and Baseline</b></summary>
  <p><b>编号</b>：[266]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12930</p>
  <p><b>作者</b>：Tiantian Geng,  Teng Wang,  Jinming Duan,  Runmin Cong,  Feng Zheng</p>
  <p><b>备注</b>：Accepted by CVPR2023</p>
  <p><b>关键词</b>：handles manually trimmed, manually trimmed videos, audio-visual events, Existing audio-visual event, audio-visual event localization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing audio-visual event localization (AVE) handles manually trimmed
videos with only a single instance in each of them. However, this setting is
unrealistic as natural videos often contain numerous audio-visual events with
different categories. To better adapt to real-life applications, in this paper
we focus on the task of dense-localizing audio-visual events, which aims to
jointly localize and recognize all audio-visual events occurring in an
untrimmed video. The problem is challenging as it requires fine-grained
audio-visual scene and context understanding. To tackle this problem, we
introduce the first Untrimmed Audio-Visual (UnAV-100) dataset, which contains
10K untrimmed videos with over 30K audio-visual events. Each video has 2.8
audio-visual events on average, and the events are usually related to each
other and might co-occur as in real-life scenes. Next, we formulate the task
using a new learning-based framework, which is capable of fully integrating
audio and visual modalities to localize audio-visual events with various
lengths and capture dependencies between them in a single pass. Extensive
experiments demonstrate the effectiveness of our method as well as the
significance of multi-scale cross-modal perception and dependency modeling for
this task.</p>
  </details>
</details>
<details>
  <summary>119. <b>标题：Deep learning-based stereo camera multi-video synchronization</b></summary>
  <p><b>编号</b>：[271]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12916</p>
  <p><b>作者</b>：Nicolas Boizard,  Kevin El Haddad,  Thierry Ravet,  François Cresson,  Thierry Dutoit</p>
  <p><b>备注</b>：5 pages, 4 figures, Accepted at ICASSP 2023</p>
  <p><b>关键词</b>：Stereo vision, vision is essential, Stereo, applications, synchronization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Stereo vision is essential for many applications. Currently, the
synchronization of the streams coming from two cameras is done using mostly
hardware. A software-based synchronization method would reduce the cost, weight
and size of the entire system and allow for more flexibility when building such
systems. With this goal in mind, we present here a comparison of different deep
learning-based systems and prove that some are efficient and generalizable
enough for such a task. This study paves the way to a production ready
software-based video synchronization system.</p>
  </details>
</details>
<details>
  <summary>120. <b>标题：Self-distillation for surgical action recognition</b></summary>
  <p><b>编号</b>：[272]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12915</p>
  <p><b>作者</b>：Amine Yamlahi,  Thuy Nuong Tran,  Patrick Godau,  Melanie Schellenberg,  Dominik Michael,  Finn-Henri Smidt,  Jan-Hinrich Noelke,  Tim Adler,  Minu Dietlinde Tizabi,  Chinedu Nwoye,  Nicolas Padoy,  Lena Maier-Hein</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：contextaware decision support, Surgical scene understanding, operating room, scene understanding, key prerequisite</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Surgical scene understanding is a key prerequisite for contextaware decision
support in the operating room. While deep learning-based approaches have
already reached or even surpassed human performance in various fields, the task
of surgical action recognition remains a major challenge. With this
contribution, we are the first to investigate the concept of self-distillation
as a means of addressing class imbalance and potential label ambiguity in
surgical video analysis. Our proposed method is a heterogeneous ensemble of
three models that use Swin Transfomers as backbone and the concepts of
self-distillation and multi-task learning as core design choices. According to
ablation studies performed with the CholecT45 challenge data via
cross-validation, the biggest performance boost is achieved by the usage of
soft labels obtained by self-distillation. External validation of our method on
an independent test set was achieved by providing a Docker container of our
inference model to the challenge organizers. According to their analysis, our
method outperforms all other solutions submitted to the latest challenge in the
field. Our approach thus shows the potential of self-distillation for becoming
an important tool in medical image analysis applications.</p>
  </details>
</details>
<details>
  <summary>121. <b>标题：Scale space radon transform-based inertia axis and object central  symmetry estimation</b></summary>
  <p><b>编号</b>：[283]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12890</p>
  <p><b>作者</b>：Aicha Baya Goumeidane,  Djemel Ziou,  Nafaa Nacereddine</p>
  <p><b>备注</b>：This work has not been published</p>
  <p><b>关键词</b>：involving information obtained, Axes are involved, image content measurement, Inertia Axes, main axis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Inertia Axes are involved in many techniques for image content measurement
when involving information obtained from lines, angles, centroids... etc. We
investigate, here, the estimation of the main axis of inertia of an object in
the image. We identify the coincidence conditions of the Scale Space Radon
Transform (SSRT) maximum and the inertia main axis. We show, that by choosing
the appropriate scale parameter, it is possible to match the SSRT maximum and
the main axis of inertia location and orientation of the embedded object in the
image. Furthermore, an example of use case is presented where binary objects
central symmetry computation is derived by means of SSRT projections and the
axis of inertia orientation. To this end, some SSRT characteristics have been
highlighted and exploited. The experimentations show the SSRT-based main axis
of inertia computation effectiveness. Concerning the central symmetry, results
are very satisfying as experimentations carried out on randomly created images
dataset and existing datasets have permitted to divide successfully these
images bases into centrally symmetric and non-centrally symmetric objects.</p>
  </details>
</details>
<details>
  <summary>122. <b>标题：NeRF-GAN Distillation for Efficient 3D-Aware Generation with  Convolutions</b></summary>
  <p><b>编号</b>：[293]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12865</p>
  <p><b>作者</b>：Mohamad Shahbazi,  Evangelos Ntavelis,  Alessio Tonioni,  Edo Collins,  Danda Pani Paudel,  Martin Danelljan,  Luc Van Gool</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：generative models struggle, Neural Radiance Fields, Generative Adversarial Networks, generative models, convolutional generative models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pose-conditioned convolutional generative models struggle with high-quality
3D-consistent image generation from single-view datasets, due to their lack of
sufficient 3D priors. Recently, the integration of Neural Radiance Fields
(NeRFs) and generative models, such as Generative Adversarial Networks (GANs),
has transformed 3D-aware generation from single-view images. NeRF-GANs exploit
the strong inductive bias of 3D neural representations and volumetric rendering
at the cost of higher computational complexity. This study aims at revisiting
pose-conditioned 2D GANs for efficient 3D-aware generation at inference time by
distilling 3D knowledge from pretrained NeRF-GANS. We propose a simple and
effective method, based on re-using the well-disentangled latent space of a
pre-trained NeRF-GAN in a pose-conditioned convolutional network to directly
generate 3D-consistent images corresponding to the underlying 3D
representations. Experiments on several datasets demonstrate that the proposed
method obtains results comparable with volumetric rendering in terms of quality
and 3D consistency while benefiting from the superior computational advantage
of convolutional networks. The code will be available at:
this https URL</p>
  </details>
</details>
<details>
  <summary>123. <b>标题：LP-IOANet: Efficient High Resolution Document Shadow Removal</b></summary>
  <p><b>编号</b>：[294]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12862</p>
  <p><b>作者</b>：Konstantinos Georgiadis,  M. Kerim Yucel,  Evangelos Skartados,  Valia Dimaridou,  Anastasios Drosou,  Albert Saa-Garriga,  Bruno Manganelli</p>
  <p><b>备注</b>：ICASSP 2023</p>
  <p><b>关键词</b>：Document shadow removal, Output Attention Network, document enhancement pipelines, propose Laplacian Pyramid, improves visibility</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Document shadow removal is an integral task in document enhancement
pipelines, as it improves visibility, readability and thus the overall quality.
Assuming that the majority of practical document shadow removal scenarios
require real-time, accurate models that can produce high-resolution outputs
in-the-wild, we propose Laplacian Pyramid with Input/Output Attention Network
(LP-IOANet), a novel pipeline with a lightweight architecture and an upsampling
module. Furthermore, we propose three new datasets which cover a wide range of
lighting conditions, images, shadow shapes and viewpoints. Our results show
that we outperform the state-of-the-art by a 35% relative improvement in mean
average error (MAE), while running real-time in four times the resolution (of
the state-of-the-art method) on a mobile device.</p>
  </details>
</details>
<details>
  <summary>124. <b>标题：Test-time Defense against Adversarial Attacks: Detection and  Reconstruction of Adversarial Examples via Masked Autoencoder</b></summary>
  <p><b>编号</b>：[300]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12848</p>
  <p><b>作者</b>：Yun-Yun Tsai,  Ju-Chin Chao,  Albert Wen,  Zhaoyuan Yang,  Chengzhi Mao,  Tapan Shah,  Junfeng Yang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：test time, time, Existing defense methods, adversarial, adversarial attacks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing defense methods against adversarial attacks can be categorized into
training time and test time defenses. Training time defense, i.e., adversarial
training, requires a significant amount of extra time for training and is often
not able to be generalized to unseen attacks. On the other hand, test time
defense by test time weight adaptation requires access to perform gradient
descent on (part of) the model weights, which could be infeasible for models
with frozen weights. To address these challenges, we propose DRAM, a novel
defense method to Detect and Reconstruct multiple types of Adversarial attacks
via Masked autoencoder (MAE). We demonstrate how to use MAE losses to build a
KS-test to detect adversarial attacks. Moreover, the MAE losses can be used to
repair adversarial samples from unseen attack types. In this sense, DRAM
neither requires model weight updates in test time nor augments the training
set with more adversarial samples. Evaluating DRAM on the large-scale ImageNet
data, we achieve the best detection rate of 82% on average on eight types of
adversarial attacks compared with other detection baselines. For
reconstruction, DRAM improves the robust accuracy by 6% ~ 41% for Standard
ResNet50 and 3% ~ 8% for Robust ResNet50 compared with other self-supervision
tasks, such as rotation prediction and contrastive learning.</p>
  </details>
</details>
<details>
  <summary>125. <b>标题：Co-Speech Gesture Synthesis using Discrete Gesture Token Learning</b></summary>
  <p><b>编号</b>：[302]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12822</p>
  <p><b>作者</b>：Shuhong Lu,  Youngwoo Yoon,  Andrew Feng</p>
  <p><b>备注</b>：8 pages, 3 figures, 3 tables</p>
  <p><b>关键词</b>：creating believable motions, human users, Synthesizing realistic co-speech, unsolved problem, problem for creating</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Synthesizing realistic co-speech gestures is an important and yet unsolved
problem for creating believable motions that can drive a humanoid robot to
interact and communicate with human users. Such capability will improve the
impressions of the robots by human users and will find applications in
education, training, and medical services. One challenge in learning the
co-speech gesture model is that there may be multiple viable gesture motions
for the same speech utterance. The deterministic regression methods can not
resolve the conflicting samples and may produce over-smoothed or damped
motions. We proposed a two-stage model to address this uncertainty issue in
gesture synthesis by modeling the gesture segments as discrete latent codes.
Our method utilizes RQ-VAE in the first stage to learn a discrete codebook
consisting of gesture tokens from training data. In the second stage, a
two-level autoregressive transformer model is used to learn the prior
distribution of residual codes conditioned on input speech context. Since the
inference is formulated as token sampling, multiple gesture sequences could be
generated given the same speech input using top-k sampling. The quantitative
results and the user study showed the proposed method outperforms the previous
methods and is able to generate realistic and diverse gesture motions.</p>
  </details>
</details>
<details>
  <summary>126. <b>标题：IoT Device Identification Based on Network Communication Analysis Using  Deep Learning</b></summary>
  <p><b>编号</b>：[316]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12800</p>
  <p><b>作者</b>：Jaidip Kotak,  Yuval Elovici</p>
  <p><b>备注</b>：J Ambient Intell Human Comput (2022)</p>
  <p><b>关键词</b>：IoT devices, organization network, IoT, secure IoT devices, devices</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Attack vectors for adversaries have increased in organizations because of the
growing use of less secure IoT devices. The risk of attacks on an
organization's network has also increased due to the bring your own device
(BYOD) policy which permits employees to bring IoT devices onto the premises
and attach them to the organization's network. To tackle this threat and
protect their networks, organizations generally implement security policies in
which only white listed IoT devices are allowed on the organization's network.
To monitor compliance with such policies, it has become essential to
distinguish IoT devices permitted within an organization's network from non
white listed (unknown) IoT devices. In this research, deep learning is applied
to network communication for the automated identification of IoT devices
permitted on the network. In contrast to existing methods, the proposed
approach does not require complex feature engineering of the network
communication, because the 'communication behavior' of IoT devices is
represented as small images which are generated from the device's network
communication payload. The proposed approach is applicable for any IoT device,
regardless of the protocol used for communication. As our approach relies on
the network communication payload, it is also applicable for the IoT devices
behind a network address translation (NAT) enabled router. In this study, we
trained various classifiers on a publicly accessible dataset to identify IoT
devices in different scenarios, including the identification of known and
unknown IoT devices, achieving over 99% overall average detection accuracy.</p>
  </details>
</details>
<details>
  <summary>127. <b>标题：Time Series as Images: Vision Transformer for Irregularly Sampled Time  Series</b></summary>
  <p><b>编号</b>：[317]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12799</p>
  <p><b>作者</b>：Zekun Li,  Shiyang Li,  Xifeng Yan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：sampled time series, Irregularly sampled time, medical applications, time series, increasingly prevalent</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Irregularly sampled time series are becoming increasingly prevalent in
various domains, especially in medical applications. Although different
highly-customized methods have been proposed to tackle irregularity, how to
effectively model their complicated dynamics and high sparsity is still an open
problem. This paper studies the problem from a whole new perspective:
transforming irregularly sampled time series into line graph images and
adapting powerful vision transformers to perform time series classification in
the same way as image classification. Our approach largely simplifies algorithm
designs without assuming prior knowledge and can be potentially extended as a
general-purpose framework. Despite its simplicity, we show that it
substantially outperforms state-of-the-art specialized algorithms on several
popular healthcare and human activity datasets. Especially in the challenging
leave-sensors-out setting where a subset of variables is masked during testing,
the performance improvement is up to 54.0\% in absolute F1 score points. Our
code and data are available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>128. <b>标题：MSFA-Frequency-Aware Transformer for Hyperspectral Images Demosaicing</b></summary>
  <p><b>编号</b>：[329]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13404</p>
  <p><b>作者</b>：Haijin Zeng,  Kai Feng,  Shaoguang Huang,  Jiezhang Cao,  Yongyong Chen,  Hongyan Zhang,  Hiep Luong,  Wilfried Philips</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：multispectral filter arrays, Hyperspectral imaging systems, filter arrays, imaging systems, multispectral filter</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Hyperspectral imaging systems that use multispectral filter arrays (MSFA)
capture only one spectral component in each pixel. Hyperspectral demosaicing is
used to recover the non-measured components. While deep learning methods have
shown promise in this area, they still suffer from several challenges,
including limited modeling of non-local dependencies, lack of consideration of
the periodic MSFA pattern that could be linked to periodic artifacts, and
difficulty in recovering high-frequency details. To address these challenges,
this paper proposes a novel de-mosaicing framework, the MSFA-frequency-aware
Transformer network (FDM-Net). FDM-Net integrates a novel MSFA-frequency-aware
multi-head self-attention mechanism (MaFormer) and a filter-based Fourier
zero-padding method to reconstruct high pass components with greater difficulty
and low pass components with relative ease, separately. The advantage of
Maformer is that it can leverage the MSFA information and non-local
dependencies present in the data. Additionally, we introduce a joint spatial
and frequency loss to transfer MSFA information and enhance training on
frequency components that are hard to recover. Our experimental results
demonstrate that FDM-Net outperforms state-of-the-art methods with 6dB PSNR,
and reconstructs high-fidelity details successfully.</p>
  </details>
</details>
<details>
  <summary>129. <b>标题：Clinically Relevant Latent Space Embedding of Cancer Histopathology  Slides through Variational Autoencoder Based Image Compression</b></summary>
  <p><b>编号</b>：[332]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13332</p>
  <p><b>作者</b>：Mohammad Sadegh Nasr,  Amir Hajighasemi,  Paul Koomey,  Parisa Boodaghi Malidarreh,  Michael Robben,  Jillur Rahman Saurav,  Helen H. Shang,  Manfred Huber,  Jacob M. Luber</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Variational Autoencoder, previously reported state, based training approach, decompress cancer pathology, cancer pathology slides</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we introduce a Variational Autoencoder (VAE) based training
approach that can compress and decompress cancer pathology slides at a
compression ratio of 1:512, which is better than the previously reported state
of the art (SOTA) in the literature, while still maintaining accuracy in
clinical validation tasks. The compression approach was tested on more common
computer vision datasets such as CIFAR10, and we explore which image
characteristics enable this compression ratio on cancer imaging data but not
generic images. We generate and visualize embeddings from the compressed latent
space and demonstrate how they are useful for clinical interpretation of data,
and how in the future such latent embeddings can be used to accelerate search
of clinical imaging data.</p>
  </details>
</details>
<details>
  <summary>130. <b>标题：Improved Anisotropic Gaussian Filters</b></summary>
  <p><b>编号</b>：[335]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13278</p>
  <p><b>作者</b>：Alex Keilmann,  Michael Godehardt,  Ali Moghiseh,  Claudia Redenbach,  Katja Schladitz</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：anisotropic Gaussian filters, Elongated anisotropic Gaussian, anisotropic Gaussian, Gaussian filters, orientation estimation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Elongated anisotropic Gaussian filters are used for the orientation
estimation of fibers. In cases where computed tomography images are noisy,
roughly resolved, and of low contrast, they are the method of choice even if
being efficient only in virtual 2D slices. However, minor inaccuracies in the
anisotropic Gaussian filters can carry over to the orientation estimation.
Therefore, we propose a modified algorithm for 2D anisotropic Gaussian filters
and show that this improves their precision. Applied to synthetic images of
fiber bundles, it is more accurate and robust to noise. Finally, we demonstrate
the effectiveness of our approach by applying it to real-world images of sheet
molding compounds.</p>
  </details>
</details>
<details>
  <summary>131. <b>标题：A Confident Labelling Strategy Based on Deep Learning for Improving  Early Detection of Knee OsteoArthritis</b></summary>
  <p><b>编号</b>：[338]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13203</p>
  <p><b>作者</b>：Zhe Wang,  Aladine Chetouani,  Rachid Jennane</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：prevalent musculoskeletal disorder, Knee OsteoArthritis, mobility in seniors, prevalent musculoskeletal, musculoskeletal disorder</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Knee OsteoArthritis (KOA) is a prevalent musculoskeletal disorder that causes
decreased mobility in seniors. The diagnosis provided by physicians is
subjective, however, as it relies on personal experience and the
semi-quantitative Kellgren-Lawrence (KL) scoring system. KOA has been
successfully diagnosed by Computer-Aided Diagnostic (CAD) systems that use deep
learning techniques like Convolutional Neural Networks (CNN). In this paper, we
propose a novel Siamese-based network, and we introduce a new hybrid loss
strategy for the early detection of KOA. The model extends the classical
Siamese network by integrating a collection of Global Average Pooling (GAP)
layers for feature extraction at each level. Then, to improve the
classification performance, a novel training strategy that partitions each
training batch into low-, medium- and high-confidence subsets, and a specific
hybrid loss function are used for each new label attributed to each sample. The
final loss function is then derived by combining the latter loss functions with
optimized weights. Our test results demonstrate that our proposed approach
significantly improves the detection performance.</p>
  </details>
</details>
<details>
  <summary>132. <b>标题：A Permutable Hybrid Network for Volumetric Medical Image Segmentation</b></summary>
  <p><b>编号</b>：[340]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13111</p>
  <p><b>作者</b>：Yi Lin,  Xiao Fang,  Dong Zhang,  Kwang-Ting Cheng,  Hao Chen</p>
  <p><b>备注</b>：Submitted to MICCAI 2023</p>
  <p><b>关键词</b>：Vision Transformer, brought substantial advancements, medical image segmentation, advent of Vision, brought substantial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The advent of Vision Transformer (ViT) has brought substantial advancements
in 3D volumetric benchmarks, particularly in 3D medical image segmentation.
Concurrently, Multi-Layer Perceptron (MLP) networks have regained popularity
among researchers due to their comparable results to ViT, albeit with the
exclusion of the heavy self-attention module. This paper introduces a
permutable hybrid network for volumetric medical image segmentation, named
PHNet, which exploits the advantages of convolution neural network (CNN) and
MLP. PHNet addresses the intrinsic isotropy problem of 3D volumetric data by
utilizing both 2D and 3D CNN to extract local information. Besides, we propose
an efficient Multi-Layer Permute Perceptron module, named MLPP, which enhances
the original MLP by obtaining long-range dependence while retaining positional
information. Extensive experimental results validate that PHNet outperforms the
state-of-the-art methods on two public datasets, namely, COVID-19-20 and
Synapse. Moreover, the ablation study demonstrates the effectiveness of PHNet
in harnessing the strengths of both CNN and MLP. The code will be accessible to
the public upon acceptance.</p>
  </details>
</details>
<details>
  <summary>133. <b>标题：OCELOT: Overlapped Cell on Tissue Dataset for Histopathology</b></summary>
  <p><b>编号</b>：[341]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13110</p>
  <p><b>作者</b>：Jeongun Ryu,  Aaron Valero Puche,  JaeWoong Shin,  Seonwook Park,  Biagio Brattoli,  Jinhee Lee,  Wonkyung Jung,  Soo Ick Cho,  Kyunghyun Paeng,  Chan-Young Ock,  Donggeun Yoo,  Sérgio Pereira</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：extracting high-level medical, high-level medical information, Cell detection, Cell, extracting high-level</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cell detection is a fundamental task in computational pathology that can be
used for extracting high-level medical information from whole-slide images. For
accurate cell detection, pathologists often zoom out to understand the
tissue-level structures and zoom in to classify cells based on their morphology
and the surrounding context. However, there is a lack of efforts to reflect
such behaviors by pathologists in the cell detection models, mainly due to the
lack of datasets containing both cell and tissue annotations with overlapping
regions. To overcome this limitation, we propose and publicly release OCELOT, a
dataset purposely dedicated to the study of cell-tissue relationships for cell
detection in histopathology. OCELOT provides overlapping cell and tissue
annotations on images acquired from multiple organs. Within this setting, we
also propose multi-task learning approaches that benefit from learning both
cell and tissue tasks simultaneously. When compared against a model trained
only for the cell detection task, our proposed approaches improve cell
detection performance on 3 datasets: proposed OCELOT, public TIGER, and
internal CARP datasets. On the OCELOT test set in particular, we show up to
6.79 improvement in F1-score. We believe the contributions of this paper,
including the release of the OCELOT dataset at
this https URL are a crucial starting
point toward the important research direction of incorporating cell-tissue
relationships in computation pathology.</p>
  </details>
</details>
<details>
  <summary>134. <b>标题：Federated Uncertainty-Aware Aggregation for Fundus Diabetic Retinopathy  Staging</b></summary>
  <p><b>编号</b>：[344]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13033</p>
  <p><b>作者</b>：Meng Wang,  Lianyu Wang,  Xinxing Xu,  Ke Zou,  Yiming Qian,  Rick Siow Mong Goh,  Yong Liu,  Huazhu Fu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：shown promising performance, staging, diabetic retinopathy, shown promising, field of diabetic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning models have shown promising performance in the field of
diabetic retinopathy (DR) staging. However, collaboratively training a DR
staging model across multiple institutions remains a challenge due to non-iid
data, client reliability, and confidence evaluation of the prediction. To
address these issues, we propose a novel federated uncertainty-aware
aggregation paradigm (FedUAA), which considers the reliability of each client
and produces a confidence estimation for the DR staging. In our FedUAA, an
aggregated encoder is shared by all clients for learning a global
representation of fundus images, while a novel temperature-warmed uncertainty
head (TWEU) is utilized for each client for local personalized staging
criteria. Our TWEU employs an evidential deep layer to produce the uncertainty
score with the DR staging results for client reliability evaluation.
Furthermore, we developed a novel uncertainty-aware weighting module (UAW) to
dynamically adjust the weights of model aggregation based on the uncertainty
score distribution of each client. In our experiments, we collect five publicly
available datasets from different institutions to conduct a dataset for
federated DR staging to satisfy the real non-iid condition. The experimental
results demonstrate that our FedUAA achieves better DR staging performance with
higher reliability compared to other federated learning methods. Our proposed
FedUAA paradigm effectively addresses the challenges of collaboratively
training DR staging models across multiple institutions, and provides a robust
and reliable solution for the deployment of DR diagnosis models in real-world
clinical scenarios.</p>
  </details>
</details>
<details>
  <summary>135. <b>标题：Dermatologist-like explainable AI enhances trust and confidence in  diagnosing melanoma</b></summary>
  <p><b>编号</b>：[356]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12806</p>
  <p><b>作者</b>：Tirtha Chanda,  Katja Hauser,  Sarah Hobelsberger,  Tabea-Clara Bucher,  Carina Nogueira Garcia,  Christoph Wies,  Harald Kittler,  Philipp Tschandl,  Cristian Navarrete-Dechent,  Sebastian Podlipnik,  Emmanouil Chousakos,  Iva Crnaric,  Jovana Majstorovic,  Linda Alhajwan,  Tanya Foreman,  Sandra Peternel,  Sergei Sarap,  İrem Özdemir,  Raymond L. Barnhill,  Mar Llamas Velasco,  Gabriela Poch,  Sören Korsing,  Wiebke Sondermann,  Frank Friedrich Gellrich,  Markus V. Heppt,  Michael Erdmann,  Sebastian Haferkamp,  Konstantin Drexler,  Matthias Goebeler,  Bastian Schilling,  Jochen S. Utikal,  Kamran Ghoreschi,  Stefan Fröhling,  Eva Krieghoff-Henning,  Titus J. Brinker</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：poses severe obstacles, initial melanoma diagnosis, identify melanoma poses, melanoma poses severe, XAI</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Although artificial intelligence (AI) systems have been shown to improve the
accuracy of initial melanoma diagnosis, the lack of transparency in how these
systems identify melanoma poses severe obstacles to user acceptance.
Explainable artificial intelligence (XAI) methods can help to increase
transparency, but most XAI methods are unable to produce precisely located
domain-specific explanations, making the explanations difficult to interpret.
Moreover, the impact of XAI methods on dermatologists has not yet been
evaluated. Extending on two existing classifiers, we developed an XAI system
that produces text and region based explanations that are easily interpretable
by dermatologists alongside its differential diagnoses of melanomas and nevi.
To evaluate this system, we conducted a three-part reader study to assess its
impact on clinicians' diagnostic accuracy, confidence, and trust in the
XAI-support. We showed that our XAI's explanations were highly aligned with
clinicians' explanations and that both the clinicians' trust in the support
system and their confidence in their diagnoses were significantly increased
when using our XAI compared to using a conventional AI system. The clinicians'
diagnostic accuracy was numerically, albeit not significantly, increased. This
work demonstrates that clinicians are willing to adopt such an XAI system,
motivating their future use in the clinic.</p>
  </details>
</details>
<details>
  <summary>136. <b>标题：A Data Augmentation Method and the Embedding Mechanism for Detection and  Classification of Pulmonary Nodules on Small Samples</b></summary>
  <p><b>编号</b>：[357]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12801</p>
  <p><b>作者</b>：Yang Liu,  Yue-Jie Hou,  Chen-Xin Qin,  Xin-Hui Li,  Si-Jing Li,  Bin Wang,  Chi-Chun Zhou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：stages.omputer aided diagnosis, data augmentation method, early stages.omputer aided, augmentation method, embedding mechanism</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Detection of pulmonary nodules by CT is used for screening lung cancer in
early stages.omputer aided diagnosis (CAD) based on deep-learning method can
identify the suspected areas of pulmonary nodules in CT images, thus improving
the accuracy and efficiency of CT diagnosis. The accuracy and robustness of
deep learning models. Method:In this paper, we explore (1) the data
augmentation method based on the generation model and (2) the model structure
improvement method based on the embedding mechanism. Two strategies have been
introduced in this study: a new data augmentation method and a embedding
mechanism. In the augmentation method, a 3D pixel-level statistics algorithm is
proposed to generate pulmonary nodule and by combing the faked pulmonary nodule
and healthy lung, we generate new pulmonary nodule samples. The embedding
mechanism are designed to better understand the meaning of pixels of the
pulmonary nodule samples by introducing hidden variables. Result: The result of
the 3DVNET model with the augmentation method for pulmonary nodule detection
shows that the proposed data augmentation method outperforms the method based
on generative adversarial network (GAN) framework, training accuracy improved
by 1.5%, and with embedding mechanism for pulmonary nodules classification
shows that the embedding mechanism improves the accuracy and robustness for the
classification of pulmonary nodules obviously, the model training accuracy is
close to 1 and the model testing F1-score is 0.90.Conclusion:he proposed data
augmentation method and embedding mechanism are beneficial to improve the
accuracy and robustness of the model, and can be further applied in other
common diagnostic imaging tasks.</p>
  </details>
</details>
<h1>自然语言处理</h1>
<details>
  <summary>1. <b>标题：Learning and Verification of Task Structure in Instructional Videos</b></summary>
  <p><b>编号</b>：[1]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13519</p>
  <p><b>作者</b>：Medhini Narasimhan,  Licheng Yu,  Sean Bell,  Ning Zhang,  Trevor Darrell</p>
  <p><b>备注</b>：Wesbite at this https URL</p>
  <p><b>关键词</b>：multi-step task models, enormous number, diverse array, array of multi-step, instructional videos</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Given the enormous number of instructional videos available online, learning
a diverse array of multi-step task models from videos is an appealing goal. We
introduce a new pre-trained video model, VideoTaskformer, focused on
representing the semantics and structure of instructional videos. We pre-train
VideoTaskformer using a simple and effective objective: predicting weakly
supervised textual labels for steps that are randomly masked out from an
instructional video (masked step modeling). Compared to prior work which learns
step representations locally, our approach involves learning them globally,
leveraging video of the entire surrounding task as context. From these learned
representations, we can verify if an unseen video correctly executes a given
task, as well as forecast which steps are likely to be taken after a given
step. We introduce two new benchmarks for detecting mistakes in instructional
videos, to verify if there is an anomalous step and if steps are executed in
the right order. We also introduce a long-term forecasting benchmark, where the
goal is to predict long-range future steps from a given step. Our method
outperforms previous baselines on these tasks, and we believe the tasks will be
a valuable way for the community to measure the quality of step
representations. Additionally, we evaluate VideoTaskformer on 3 existing
benchmarks -- procedural activity recognition, step classification, and step
forecasting -- and demonstrate on each that our method outperforms existing
baselines and achieves new state-of-the-art performance.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Learning Semantic Text Similarity to rank Hypernyms of Financial Terms</b></summary>
  <p><b>编号</b>：[29]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13475</p>
  <p><b>作者</b>：Sohom Ghosh,  Ankush Chopra,  Sudip Kumar Naskar</p>
  <p><b>备注</b>：Our code base: this https URL</p>
  <p><b>关键词</b>：users access financial, access financial services, financial, paradigm shift, users access</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Over the years, there has been a paradigm shift in how users access financial
services. With the advancement of digitalization more users have been
preferring the online mode of performing financial activities. This has led to
the generation of a huge volume of financial content. Most investors prefer to
go through these contents before making decisions. Every industry has terms
that are specific to the domain it operates in. Banking and Financial Services
are not an exception to this. In order to fully comprehend these contents, one
needs to have a thorough understanding of the financial terms. Getting a basic
idea about a term becomes easy when it is explained with the help of the broad
category to which it belongs. This broad category is referred to as hypernym.
For example, "bond" is a hypernym of the financial term "alternative
debenture". In this paper, we propose a system capable of extracting and
ranking hypernyms for a given financial term. The system has been trained with
financial text corpora obtained from various sources like DBpedia [4],
Investopedia, Financial Industry Business Ontology (FIBO), prospectus and so
on. Embeddings of these terms have been extracted using FinBERT [3], FinISH [1]
and fine-tuned using SentenceBERT [54]. A novel approach has been used to
augment the training set with negative samples. It uses the hierarchy present
in FIBO. Finally, we benchmark the system performance with that of the existing
ones. We establish that it performs better than the existing ones and is also
scalable.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Extracting Physical Rehabilitation Exercise Information from Clinical  Notes: a Comparison of Rule-Based and Machine Learning Natural Language  Processing Techniques</b></summary>
  <p><b>编号</b>：[32]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13466</p>
  <p><b>作者</b>：Stephen W. Shaffran,  Fengyi Gao,  Parker E. Denny,  Bayan M. Aldhahwani,  Allyn Bove,  Shyam Visweswaran,  Yanshan Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：plays a crucial, crucial role, Physical rehabilitation plays, recovery process, post-stroke patients</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Physical rehabilitation plays a crucial role in the recovery process of
post-stroke patients. By personalizing therapies for patients leveraging
predictive modeling and electronic health records (EHRs), healthcare providers
can make the rehabilitation process more efficient. Before predictive modeling
can provide decision support for the assignment of treatment plans, automated
methods are necessary to extract physical rehabilitation exercise information
from unstructured EHRs. We introduce a rule-based natural language processing
algorithm to annotate therapeutic procedures for stroke patients and compare it
to several small machine learning models. We find that our algorithm
outperforms these models in extracting half of the concepts where sufficient
data is available, and individual exercise descriptions can be assigned binary
labels with an f-score of no less than 0.75 per concept. More research needs to
be done before these algorithms can be deployed on unlabeled documents, but
current progress gives promise to the potential of precision rehabilitation
research.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Deep RL with Hierarchical Action Exploration for Dialogue Generation</b></summary>
  <p><b>编号</b>：[33]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13465</p>
  <p><b>作者</b>：Itsugun Cho,  Ryota Takahashi,  Yusaku Yanase,  Hiroaki Saito</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：approximate dynamic programming, dynamic programming applied, natural language action, language action space, generation involves policy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Conventionally, since the natural language action space is astronomical,
approximate dynamic programming applied to dialogue generation involves policy
improvement with action sampling. However, such a practice is inefficient for
reinforcement learning (RL) because the eligible (high action value) responses
are very sparse, and the greedy policy sustained by the random sampling is
flabby. This paper shows that the performance of dialogue policy positively
correlated with sampling size by theoretical and experimental. We introduce a
novel dual-granularity Q-function to alleviate this limitation by exploring the
most promising response category to intervene in the sampling. It extracts the
actions following the grained hierarchy, which can achieve the optimum with
fewer policy iterations. Our approach learns in the way of offline RL from
multiple reward functions designed to recognize human emotional details.
Empirical studies demonstrate that our algorithm outperforms the baseline
methods. Further verification presents that ours can generate responses with
higher expected rewards and controllability.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：W2KPE: Keyphrase Extraction with Word-Word Relation</b></summary>
  <p><b>编号</b>：[34]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13463</p>
  <p><b>作者</b>：Wen Cheng,  Shichen Dong,  Wei Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：MUG Challenge Track, submission to ICASSP, MUG Challenge, Challenge Track, Named Entity Recognition</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper describes our submission to ICASSP 2023 MUG Challenge Track 4,
Keyphrase Extraction, which aims to extract keyphrases most relevant to the
conference theme from conference materials. We model the challenge as a
single-class Named Entity Recognition task and developed techniques for better
performance on the challenge: For the data preprocessing, we encode the split
keyphrases after word segmentation. In addition, we increase the amount of
input information that the model can accept at one time by fusing multiple
preprocessed sentences into one segment. We replace the loss function with the
multi-class focal loss to address the sparseness of keyphrases. Besides, we
score each appearance of keyphrases and add an extra output layer to fit the
score to rank keyphrases. Exhaustive evaluations are performed to find the best
combination of the word segmentation tool, the pre-trained embedding model, and
the corresponding hyperparameters. With these proposals, we scored 45.04 on the
final test set.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：CoBIT: A Contrastive Bi-directional Image-Text Generation Model</b></summary>
  <p><b>编号</b>：[36]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13455</p>
  <p><b>作者</b>：Haoxuan You,  Mandy Guo,  Zhecan Wang,  Kai-Wei Chang,  Jason Baldridge,  Jiahui Yu</p>
  <p><b>备注</b>：14 pages, 5 figures</p>
  <p><b>关键词</b>：pre-trained foundation models, field of vision, vision and language, language has witnessed, witnessed a proliferation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The field of vision and language has witnessed a proliferation of pre-trained
foundation models. Most existing methods are independently pre-trained with
contrastive objective like CLIP, image-to-text generative objective like PaLI,
or text-to-image generative objective like Parti. However, the three objectives
can be pre-trained on the same data, image-text pairs, and intuitively they
complement each other as contrasting provides global alignment capacity and
generation grants fine-grained understanding. In this work, we present a
Contrastive Bi-directional Image-Text generation model (CoBIT), which attempts
to unify the three pre-training objectives in one framework. Specifically,
CoBIT employs a novel unicoder-decoder structure, consisting of an image
unicoder, a text unicoder and a cross-modal decoder. The image/text unicoders
can switch between encoding and decoding in different tasks, enabling
flexibility and shared knowledge that benefits both image-to-text and
text-to-image generations. CoBIT achieves superior performance in image
understanding, image-text understanding (Retrieval, Captioning, VQA, SNLI-VE)
and text-based content creation, particularly in zero-shot scenarios. For
instance, 82.7% in zero-shot ImageNet classification, 9.37 FID score in
zero-shot text-to-image generation and 44.8 CIDEr in zero-shot captioning.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Development and validation of a natural language processing algorithm to  pseudonymize documents in the context of a clinical data warehouse</b></summary>
  <p><b>编号</b>：[38]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13451</p>
  <p><b>作者</b>：Xavier Tannier,  Perceval Wajsbürt,  Alice Calliger,  Basile Dura,  Alexandre Mouchet,  Martin Hilka,  Romain Bey</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ensuring patient privacy, Paris University Hospitals, Greater Paris University, research purposes, patient privacy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The objective of this study is to address the critical issue of
de-identification of clinical reports in order to allow access to data for
research purposes, while ensuring patient privacy. The study highlights the
difficulties faced in sharing tools and resources in this domain and presents
the experience of the Greater Paris University Hospitals (AP-HP) in
implementing a systematic pseudonymization of text documents from its Clinical
Data Warehouse. We annotated a corpus of clinical documents according to 12
types of identifying entities, and built a hybrid system, merging the results
of a deep learning model as well as manual rules. Our results show an overall
performance of 0.99 of F1-score. We discuss implementation choices and present
experiments to better understand the effort involved in such a task, including
dataset size, document types, language models, or rule addition. We share
guidelines and code under a 3-Clause BSD license.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Paraphrasing evades detectors of AI-generated text, but retrieval is an  effective defense</b></summary>
  <p><b>编号</b>：[51]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13408</p>
  <p><b>作者</b>：Kalpesh Krishna,  Yixiao Song,  Marzena Karpinska,  John Wieting,  Mohit Iyyer</p>
  <p><b>备注</b>：Preprint (27 pages). Code, models, data will be added to this https URL</p>
  <p><b>关键词</b>：fake content creation, malicious use cases, fake content, academic plagiarism, statistical irregularities</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To detect the deployment of large language models for malicious use cases
(e.g., fake content creation or academic plagiarism), several approaches have
recently been proposed for identifying AI-generated text via watermarks or
statistical irregularities. How robust are these detection algorithms to
paraphrases of AI-generated text? To stress test these detectors, we first
train an 11B parameter paraphrase generation model (DIPPER) that can paraphrase
paragraphs, optionally leveraging surrounding text (e.g., user-written prompts)
as context. DIPPER also uses scalar knobs to control the amount of lexical
diversity and reordering in the paraphrases. Paraphrasing text generated by
three large language models (including GPT3.5-davinci-003) with DIPPER
successfully evades several detectors, including watermarking, GPTZero,
DetectGPT, and OpenAI's text classifier. For example, DIPPER drops the
detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false
positive rate of 1%), without appreciably modifying the input semantics. To
increase the robustness of AI-generated text detection to paraphrase attacks,
we introduce a simple defense that relies on retrieving semantically-similar
generations and must be maintained by a language model API provider. Given a
candidate text, our algorithm searches a database of sequences previously
generated by the API, looking for sequences that match the candidate text
within a certain threshold. We empirically verify our defense using a database
of 15M generations from a fine-tuned T5-XXL model and find that it can detect
80% to 97% of paraphrased generations across different settings, while only
classifying 1% of human-written sequences as AI-generated. We will open source
our code, model and data for future research.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Compositional Zero-Shot Domain Transfer with Text-to-Text Models</b></summary>
  <p><b>编号</b>：[61]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13386</p>
  <p><b>作者</b>：Fangyu Liu,  Qianchu Liu,  Shruthi Bannur,  Fernando Pérez-García,  Naoto Usuyama,  Sheng Zhang,  Tristan Naumann,  Aditya Nori,  Hoifung Poon,  Javier Alvarez-Valle,  Ozan Oktay,  Stephanie L. Hyland</p>
  <p><b>备注</b>：Accepted at TACL, pre-MIT Press publication version. 16 pages, 4 figures</p>
  <p><b>关键词</b>：improving task performance, bottleneck for improving, performance in specialised, task, domain</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Label scarcity is a bottleneck for improving task performance in specialised
domains. We propose a novel compositional transfer learning framework (DoT5 -
domain compositional zero-shot T5) for zero-shot domain transfer. Without
access to in-domain labels, DoT5 jointly learns domain knowledge (from MLM of
unlabelled in-domain free text) and task knowledge (from task training on more
readily available general-domain data) in a multi-task manner. To improve the
transferability of task training, we design a strategy named NLGU: we
simultaneously train NLG for in-domain label-to-data generation which enables
data augmentation for self-finetuning and NLU for label prediction. We evaluate
DoT5 on the biomedical domain and the resource-lean subdomain of radiology,
focusing on NLI, text summarisation and embedding learning. DoT5 demonstrates
the effectiveness of compositional transfer learning through multi-task
learning. In particular, DoT5 outperforms the current SOTA in zero-shot
transfer by over 7 absolute points in accuracy on RadNLI. We validate DoT5 with
ablations and a case study demonstrating its ability to solve challenging NLI
examples requiring in-domain expertise.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Practical and Ethical Challenges of Large Language Models in Education:  A Systematic Literature Review</b></summary>
  <p><b>编号</b>：[65]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13379</p>
  <p><b>作者</b>：Lixiang Yan,  Lele Sha,  Linxuan Zhao,  Yuheng Li,  Roberto Martinez-Maldonado,  Guanliang Chen,  Xinyu Li,  Yueqiao Jin,  Dragan Gašević</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：analysing textual content, large language models, textual content, Educational technology innovations, based on large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Educational technology innovations that have been developed based on large
language models (LLMs) have shown the potential to automate the laborious
process of generating and analysing textual content. While various innovations
have been developed to automate a range of educational tasks (e.g., question
generation, feedback provision, and essay grading), there are concerns
regarding the practicality and ethicality of these innovations. Such concerns
may hinder future research and the adoption of LLMs-based innovations in
authentic educational contexts. To address this, we conducted a systematic
literature review of 118 peer-reviewed papers published since 2017 to pinpoint
the current state of research on using LLMs to automate and support educational
tasks. The practical and ethical challenges of LLMs-based innovations were also
identified by assessing their technological readiness, model performance,
replicability, system transparency, privacy, equality, and beneficence. The
findings were summarised into three recommendations for future studies,
including updating existing innovations with state-of-the-art models (e.g.,
GPT-3), embracing the initiative of open-sourcing models/systems, and adopting
a human-centred approach throughout the developmental process. These
recommendations could support future research to develop practical and ethical
innovations for supporting diverse educational tasks and benefiting students,
teachers, and institutions.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Capabilities of GPT-4 on Medical Challenge Problems</b></summary>
  <p><b>编号</b>：[67]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13375</p>
  <p><b>作者</b>：Harsha Nori,  Nicholas King,  Scott Mayer McKinney,  Dean Carignan,  Eric Horvitz</p>
  <p><b>备注</b>：33 pages, 15 figures</p>
  <p><b>关键词</b>：natural language understanding, demonstrated remarkable capabilities, Large language models, Large language, natural language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have demonstrated remarkable capabilities in
natural language understanding and generation across various domains, including
medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art
LLM, on medical competency examinations and benchmark datasets. GPT-4 is a
general-purpose model that is not specialized for medical problems through
training or engineered to solve clinical tasks. Our analysis covers two sets of
official practice materials for the USMLE, a three-step examination program
used to assess clinical competency and grant licensure in the United States. We
also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond
measuring model performance, experiments were conducted to investigate the
influence of test questions containing both text and images on model
performance, probe for memorization of content during training, and study
probability calibration, which is of critical importance in high-stakes
applications like medicine. Our results show that GPT-4, without any
specialized prompt crafting, exceeds the passing score on USMLE by over 20
points and outperforms earlier general-purpose models (GPT-3.5) as well as
models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned
version of Flan-PaLM 540B). In addition, GPT-4 is significantly better
calibrated than GPT-3.5, demonstrating a much-improved ability to predict the
likelihood that its answers are correct. We also explore the behavior of the
model qualitatively through a case study that shows the ability of GPT-4 to
explain medical reasoning, personalize explanations to students, and
interactively craft new counterfactual scenarios around a medical case.
Implications of the findings are discussed for potential uses of GPT-4 in
medical education, assessment, and clinical practice, with appropriate
attention to challenges of accuracy and safety.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Fine-tuning ClimateBert transformer with ClimaText for the disclosure  analysis of climate-related financial risks</b></summary>
  <p><b>编号</b>：[68]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13373</p>
  <p><b>作者</b>：Eduardo C. Garrido-Merchán,  Cristina González-Barthe,  María Coronado Vaca</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：climate-related financial risks, recent years, financial, climate-related financial, growing demand</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years there has been a growing demand from financial agents,
especially from particular and institutional investors, for companies to report
on climate-related financial risks. A vast amount of information, in text
format, can be expected to be disclosed in the short term by firms in order to
identify these types of risks in their financial and non financial reports,
particularly in response to the growing regulation that is being passed on the
matter. To this end, this paper applies state-of-the-art NLP techniques to
achieve the detection of climate change in text corpora. We use transfer
learning to fine-tune two transformer models, BERT and ClimateBert -a recently
published DistillRoBERTa-based model that has been specifically tailored for
climate text classification-. These two algorithms are based on the transformer
architecture which enables learning the contextual relationships between words
in a text. We carry out the fine-tuning process of both models on the novel
Clima-Text database, consisting of data collected from Wikipedia, 10K Files
Reports and web-based claims. Our text classification model obtained from the
ClimateBert fine-tuning process on ClimaText, outperforms the models created
with BERT and the current state-of-the-art transformer in this particular
problem. Our study is the first one to implement on the ClimaText database the
recently published ClimateBert algorithm. Based on our results, it can be said
that ClimateBert fine-tuned on ClimaText is an outstanding tool within the NLP
pre-trained transformer models that may and should be used by investors,
institutional agents and companies themselves to monitor the disclosure of
climate risk in financial reports. In addition, our transfer learning
methodology is cheap in computational terms, thus allowing any organization to
perform it.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：ChatGPT and a New Academic Reality: AI-Written Research Papers and the  Ethics of the Large Language Models in Scholarly Publishing</b></summary>
  <p><b>编号</b>：[71]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13367</p>
  <p><b>作者</b>：Brady Lund,  Ting Wang,  Nishith Reddy Mannuru,  Bing Nie,  Somipam Shimray,  Ziang Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：generative pre-trained transformer, text-based user requests, paper discusses OpenAIs, fulfill text-based user, discusses OpenAIs ChatGPT</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper discusses OpenAIs ChatGPT, a generative pre-trained transformer,
which uses natural language processing to fulfill text-based user requests
(i.e., a chatbot). The history and principles behind ChatGPT and similar models
are discussed. This technology is then discussed in relation to its potential
impact on academia and scholarly research and publishing. ChatGPT is seen as a
potential model for the automated preparation of essays and other types of
scholarly manuscripts. Potential ethical issues that could arise with the
emergence of large language models like GPT-3, the underlying technology behind
ChatGPT, and its usage by academics and researchers, are discussed and situated
within the context of broader advancements in artificial intelligence, machine
learning, and natural language processing for research and scholarly
publishing.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Requirement Formalisation using Natural Language Processing and Machine  Learning: A Systematic Review</b></summary>
  <p><b>编号</b>：[72]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13365</p>
  <p><b>作者</b>：Shekoufeh Kolahdouz-Rahimi,  Kevin Lano,  Chenghua Lin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：automatic Requirement Formalisation, software development methodologies, development methodologies attracts, methodologies attracts developers, Requirement Formalisation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Improvement of software development methodologies attracts developers to
automatic Requirement Formalisation (RF) in the Requirement Engineering (RE)
field. The potential advantages by applying Natural Language Processing (NLP)
and Machine Learning (ML) in reducing the ambiguity and incompleteness of
requirement written in natural languages is reported in different studies. The
goal of this paper is to survey and classify existing work on NLP and ML for
RF, identifying challenges in this domain and providing promising future
research directions. To achieve this, we conducted a systematic literature
review to outline the current state-of-the-art of NLP and ML techniques in RF
by selecting 257 papers from common used libraries. The search result is
filtered by defining inclusion and exclusion criteria and 47 relevant studies
between 2012 and 2022 are selected. We found that heuristic NLP approaches are
the most common NLP techniques used for automatic RF, primary operating on
structured and semi-structured data. This study also revealed that Deep
Learning (DL) technique are not widely used, instead classical ML techniques
are predominant in the surveyed studies. More importantly, we identified the
difficulty of comparing the performance of different approaches due to the lack
of standard benchmark cases for RF.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Reevaluating Data Partitioning for Emotion Detection in EmoWOZ</b></summary>
  <p><b>编号</b>：[73]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13364</p>
  <p><b>作者</b>：Moeen Mostafavi,  Michael D. Porter</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：paper focuses, emotion labels, emotion, emotion tags, extension of MultiWOZ</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper focuses on the EmoWoz dataset, an extension of MultiWOZ that
provides emotion labels for the dialogues. MultiWOZ was partitioned initially
for another purpose, resulting in a distributional shift when considering the
new purpose of emotion recognition. The emotion tags in EmoWoz are highly
imbalanced and unevenly distributed across the partitions, which causes
sub-optimal performance and poor comparison of models. We propose a stratified
sampling scheme based on emotion tags to address this issue, improve the
dataset's distribution, and reduce dataset shift. We also introduce a special
technique to handle conversation (sequential) data with many emotional tags.
Using our proposed sampling method, models built upon EmoWoz can perform
better, making it a more reliable resource for training conversational agents
with emotional intelligence. We recommend that future researchers use this new
partitioning to ensure consistent and accurate performance evaluations.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Towards the Scalable Evaluation of Cooperativeness in Language Models</b></summary>
  <p><b>编号</b>：[75]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13360</p>
  <p><b>作者</b>：Alan Chan,  Maxime Riché,  Jesse Clifton</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：conflict resolution, systems driven, driven by pre-trained, assist humans, humans in high-stakes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>It is likely that AI systems driven by pre-trained language models (PLMs)
will increasingly be used to assist humans in high-stakes interactions with
other agents, such as negotiation or conflict resolution. Consistent with the
goals of Cooperative AI \citep{dafoe_open_2020}, we wish to understand and
shape the multi-agent behaviors of PLMs in a pro-social manner. An important
first step is the evaluation of model behaviour across diverse cooperation
problems. Since desired behaviour in an interaction depends upon precise
game-theoretic structure, we focus on generating scenarios with particular
structures with both crowdworkers and a language model. Our work proceeds as
follows. First, we discuss key methodological issues in the generation of
scenarios corresponding to particular game-theoretic structures. Second, we
employ both crowdworkers and a language model to generate such scenarios. We
find that the quality of generations tends to be mediocre in both cases. We
additionally get both crowdworkers and a language model to judge whether given
scenarios align with their intended game-theoretic structure, finding mixed
results depending on the game. Third, we provide a dataset of scenario based on
our data generated. We provide both quantitative and qualitative evaluations of
UnifiedQA and GPT-3 on this dataset. We find that instruct-tuned models tend to
act in a way that could be perceived as cooperative when scaled up, while other
models seemed to have flat scaling trends.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Revealing Weaknesses of Vietnamese Language Models Through Unanswerable  Questions in Machine Reading Comprehension</b></summary>
  <p><b>编号</b>：[77]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13355</p>
  <p><b>作者</b>：Son Quoc Tran,  Phong Nguyen-Thuan Do,  Kiet Van Nguyen,  Ngan Luu-Thuy Nguyen</p>
  <p><b>备注</b>：Accepted at The 2023 EACL Student Research Workshop</p>
  <p><b>关键词</b>：Vietnamese Machine Reading, Machine Reading Comprehension, Machine Reading, multilinguality significantly restricts, Reading Comprehension</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Although the curse of multilinguality significantly restricts the language
abilities of multilingual models in monolingual settings, researchers now still
have to rely on multilingual models to develop state-of-the-art systems in
Vietnamese Machine Reading Comprehension. This difficulty in researching is
because of the limited number of high-quality works in developing Vietnamese
language models. In order to encourage more work in this research field, we
present a comprehensive analysis of language weaknesses and strengths of
current Vietnamese monolingual models using the downstream task of Machine
Reading Comprehension. From the analysis results, we suggest new directions for
developing Vietnamese language models. Besides this main contribution, we also
successfully reveal the existence of artifacts in Vietnamese Machine Reading
Comprehension benchmarks and suggest an urgent need for new high-quality
benchmarks to track the progress of Vietnamese Machine Reading Comprehension.
Moreover, we also introduced a minor but valuable modification to the process
of annotating unanswerable questions for Machine Reading Comprehension from
previous work. Our proposed modification helps improve the quality of
unanswerable questions to a higher level of difficulty for Machine Reading
Comprehension systems to solve.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly  Knowledge Graph</b></summary>
  <p><b>编号</b>：[80]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13351</p>
  <p><b>作者</b>：Debayan Banerjee,  Sushil Awale,  Ricardo Usbeck,  Chris Biemann</p>
  <p><b>备注</b>：12 pages ceur-ws 1 column accepted at International Bibliometric Information Retrieval Workshp @ ECIR 2023</p>
  <p><b>关键词</b>：scholarly knowledge graph, DBLP scholarly knowledge, knowledge graph, work we create, question answering dataset</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work we create a question answering dataset over the DBLP scholarly
knowledge graph (KG). DBLP is an on-line reference for bibliographic
information on major computer science publications that indexes over 4.4
million publications published by more than 2.2 million authors. Our dataset
consists of 10,000 question answer pairs with the corresponding SPARQL queries
which can be executed over the DBLP KG to fetch the correct answer. DBLP-QuAD
is the largest scholarly question answering dataset.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Leveraging Foundation Models for Clinical Text Analysis</b></summary>
  <p><b>编号</b>：[89]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13314</p>
  <p><b>作者</b>：Shaina Raza,  Syed Raza Bashir</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：health concern globally, significant public health, public health concern, extracting relevant information, concern globally</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Infectious diseases are a significant public health concern globally, and
extracting relevant information from scientific literature can facilitate the
development of effective prevention and treatment strategies. However, the
large amount of clinical data available presents a challenge for information
extraction. To address this challenge, this study proposes a natural language
processing (NLP) framework that uses a pre-trained transformer model fine-tuned
on task-specific data to extract key information related to infectious diseases
from free-text clinical data. The proposed framework includes three components:
a data layer for preparing datasets from clinical texts, a foundation model
layer for entity extraction, and an assessment layer for performance analysis.
The results of the evaluation indicate that the proposed method outperforms
standard methods, and leveraging prior knowledge through the pre-trained
transformer model makes it useful for investigating other infectious diseases
in the future.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：SwissBERT: The Multilingual Language Model for Switzerland</b></summary>
  <p><b>编号</b>：[90]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13310</p>
  <p><b>作者</b>：Jannis Vamvas,  Johannes Graën,  Rico Sennrich</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：processing Switzerland-related text, Switzerland-related text, processing Switzerland-related, model created specifically, created specifically</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present SwissBERT, a masked language model created specifically for
processing Switzerland-related text. SwissBERT is a pre-trained model that we
adapted to news articles written in the national languages of Switzerland --
German, French, Italian, and Romansh. We evaluate SwissBERT on natural language
understanding tasks related to Switzerland and find that it tends to outperform
previous models on these tasks, especially when processing contemporary news
and/or Romansh Grischun. Since SwissBERT uses language adapters, it may be
extended to Swiss German dialects in future work. The model and our open-source
code are publicly released at this https URL.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph  Question Answering</b></summary>
  <p><b>编号</b>：[99]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13284</p>
  <p><b>作者</b>：Debayan Banerjee,  Pranav Ajit Nair,  Ricardo Usbeck,  Chris Biemann</p>
  <p><b>备注</b>：16 pages single column format accepted at ESWC 2023 research track</p>
  <p><b>关键词</b>：Knowledge Graph Question, Graph Question Answering, Knowledge Graph, system named GETT-QA, Question Answering</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we present an end-to-end Knowledge Graph Question Answering
(KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text
pre-trained language model. The model takes a question in natural language as
input and produces a simpler form of the intended SPARQL query. In the simpler
form, the model does not directly produce entity and relation IDs. Instead, it
produces corresponding entity and relation labels. The labels are grounded to
KG entity and relation IDs in a subsequent step. To further improve the
results, we instruct the model to produce a truncated version of the KG
embedding for each entity. The truncated KG embedding enables a finer search
for disambiguation purposes. We find that T5 is able to learn the truncated KG
embeddings without any change of loss function, improving KGQA performance. As
a result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata
datasets on end-to-end KGQA over Wikidata.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Visual-Language Prompt Tuning with Knowledge-guided Context Optimization</b></summary>
  <p><b>编号</b>：[100]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13283</p>
  <p><b>作者</b>：Hantao Yao,  Rui Zhang,  Changsheng Xu</p>
  <p><b>备注</b>：accepted by CVPR23</p>
  <p><b>关键词</b>：pre-trained visual-language model, task-related textual tokens, specific textual knowledge, learnable textual tokens, textual tokens</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Prompt tuning is an effective way to adapt the pre-trained visual-language
model (VLM) to the downstream task using task-related textual tokens.
Representative CoOp-based work combines the learnable textual tokens with the
class tokens to obtain specific textual knowledge. However, the specific
textual knowledge is the worse generalization to the unseen classes because it
forgets the essential general textual knowledge having a strong generalization
ability. To tackle this issue, we introduce a novel Knowledge-guided Context
Optimization (KgCoOp) to enhance the generalization ability of the learnable
prompt for unseen classes. The key insight of KgCoOp is that forgetting about
essential knowledge can be alleviated by reducing the discrepancy between the
learnable prompt and the hand-crafted prompt. Especially, KgCoOp minimizes the
discrepancy between the textual embeddings generated by learned prompts and the
hand-crafted prompts. Finally, adding the KgCoOp upon the contrastive loss can
make a discriminative prompt for both seen and unseen tasks. Extensive
evaluation of several benchmarks demonstrates that the proposed
Knowledge-guided Context Optimization is an efficient method for prompt tuning,
\emph{i.e.,} achieves better performance with less training time.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Parameter-Efficient Sparse Retrievers and Rerankers using Adapters</b></summary>
  <p><b>编号</b>：[127]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13220</p>
  <p><b>作者</b>：Vaishali Pal,  Carlos Lassance,  Hervé Déjean,  Stéphane Clinchant</p>
  <p><b>备注</b>：accepted at ECIR'23</p>
  <p><b>关键词</b>：Parameter-Efficient transfer learning, Natural Language Processing, transfer learning, Language Processing, Adapters</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Parameter-Efficient transfer learning with Adapters have been studied in
Natural Language Processing (NLP) as an alternative to full fine-tuning.
Adapters are memory-efficient and scale well with downstream tasks by training
small bottle-neck layers added between transformer layers while keeping the
large pretrained language model (PLMs) frozen. In spite of showing promising
results in NLP, these methods are under-explored in Information Retrieval.
While previous studies have only experimented with dense retriever or in a
cross lingual retrieval scenario, in this paper we aim to complete the picture
on the use of adapters in IR. First, we study adapters for SPLADE, a sparse
retriever, for which adapters not only retain the efficiency and effectiveness
otherwise achieved by finetuning, but are memory-efficient and orders of
magnitude lighter to train. We observe that Adapters-SPLADE not only optimizes
just 2\% of training parameters, but outperforms fully fine-tuned counterpart
and existing parameter-efficient dense IR models on IR benchmark datasets.
Secondly, we address domain adaptation of neural retrieval thanks to adapters
on cross-domain BEIR datasets and TripClick. Finally, we also consider
knowledge sharing between rerankers and first stage rankers. Overall, our study
complete the examination of adapters for neural IR</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Fairness-guided Few-shot Prompting for Large Language Models</b></summary>
  <p><b>编号</b>：[128]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13217</p>
  <p><b>作者</b>：Huan Ma,  Changqing Zhang,  Yatao Bian,  Lemao Liu,  Zhirui Zhang,  Peilin Zhao,  Shu Zhang,  Huazhu Fu,  Qinghua Hu,  Bingzhe Wu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：demonstrated surprising ability, in-context learning, solve numerous downstream, Large language models, Large language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models have demonstrated surprising ability to perform
in-context learning, i.e., these models can be directly applied to solve
numerous downstream tasks by conditioning on a prompt constructed by a few
input-output examples. However, prior research has shown that in-context
learning can suffer from high instability due to variations in training
examples, example order, and prompt formats. Therefore, the construction of an
appropriate prompt is essential for improving the performance of in-context
learning. In this paper, we revisit this problem from the view of predictive
bias. Specifically, we introduce a metric to evaluate the predictive bias of a
fixed prompt against labels or a given attributes. Then we empirically show
that prompts with higher bias always lead to unsatisfactory predictive quality.
Based on this observation, we propose a novel search strategy based on the
greedy search to identify the near-optimal prompt for improving the performance
of in-context learning. We perform comprehensive experiments with
state-of-the-art mainstream models such as GPT-3 on various downstream tasks.
Our results indicate that our method can enhance the model's in-context
learning performance in an effective and interpretable manner.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：A Simple Explanation for the Phase Transition in Large Language Models  with List Decoding</b></summary>
  <p><b>编号</b>：[169]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13112</p>
  <p><b>作者</b>：Cheng-Shang Chang</p>
  <p><b>备注</b>：5 pages, 1 figure</p>
  <p><b>关键词</b>：exhibit emergent abilities, recent experimental results, large language models, experimental results show, exhibit emergent</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Various recent experimental results show that large language models (LLM)
exhibit emergent abilities that are not present in small models. System
performance is greatly improved after passing a certain critical threshold of
scale. In this letter, we provide a simple explanation for such a phase
transition phenomenon. For this, we model an LLM as a sequence-to-sequence
random function. Instead of using instant generation at each step, we use a
list decoder that keeps a list of candidate sequences at each step and defers
the generation of the output sequence at the end. We show that there is a
critical threshold such that the expected number of erroneous candidate
sequences remains bounded when an LLM is below the threshold, and it grows
exponentially when an LLM is above the threshold. Such a threshold is related
to the basic reproduction number in a contagious disease.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain  Batch and Proxy Gradient Transfer</b></summary>
  <p><b>编号</b>：[173]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13099</p>
  <p><b>作者</b>：Hyukhun Koh,  Haesung Pyun,  Nakyeong Yang,  Kyomin Jung</p>
  <p><b>备注</b>：8 pages, 3 figures, ACL 2023 workshop (DSTC)</p>
  <p><b>关键词</b>：Task Oriented Dialogue, Task Oriented, Proxy Gradient Transfer, Oriented Dialogue, Multi Domain Batch</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In Task Oriented Dialogue (TOD) system, detecting and inducing new intents
are two main challenges to apply the system in the real world. In this paper,
we suggest the semantic multi-view model to resolve these two challenges: (1)
SBERT for General Embedding (GE), (2) Multi Domain Batch (MDB) for dialogue
domain knowledge, and (3) Proxy Gradient Transfer (PGT) for cluster-specialized
semantic. MDB feeds diverse dialogue datasets to the model at once to tackle
the multi-domain problem by learning the multiple domain knowledge. We
introduce a novel method PGT, which employs the Siamese network to fine-tune
the model with a clustering method directly.Our model can learn how to cluster
dialogue utterances by using PGT. Experimental results demonstrate that our
multi-view model with MDB and PGT significantly improves the Open Intent
Induction performance compared to baseline systems.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Beyond Universal Transformer: block reusing with adaptor in Transformer  for automatic speech recognit</b></summary>
  <p><b>编号</b>：[188]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13072</p>
  <p><b>作者</b>：Haoyu Tang,  Zhaoyi Liu,  Chang Zeng,  Xinfeng Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：recently made significant, made significant achievements, ASR system, ASR, recently made</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transformer-based models have recently made significant achievements in the
application of end-to-end (E2E) automatic speech recognition (ASR). It is
possible to deploy the E2E ASR system on smart devices with the help of
Transformer-based models. While these models still have the disadvantage of
requiring a large number of model parameters. To overcome the drawback of
universal Transformer models for the application of ASR on edge devices, we
propose a solution that can reuse the block in Transformer models for the
occasion of the small footprint ASR system, which meets the objective of
accommodating resource limitations without compromising recognition accuracy.
Specifically, we design a novel block-reusing strategy for speech Transformer
(BRST) to enhance the effectiveness of parameters and propose an adapter module
(ADM) that can produce a compact and adaptable model with only a few additional
trainable parameters accompanying each reusing block. We conducted an
experiment with the proposed method on the public AISHELL-1 corpus, and the
results show that the proposed approach achieves the character error rate (CER)
of 9.3%/6.63% with only 7.6M/8.3M parameters without and with the ADM,
respectively. In addition, we also make a deeper analysis to show the effect of
ADM in the general block-reusing method.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Retrieval-Augmented Classification with Decoupled Representation</b></summary>
  <p><b>编号</b>：[191]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13065</p>
  <p><b>作者</b>：Xinnian Liang,  Shuangzhi Wu,  Hui Huang,  Jiaqi Bai,  Chao Bian,  Zhoujun Li</p>
  <p><b>备注</b>：preprint</p>
  <p><b>关键词</b>：shown marvelous improvements, Pretrained language, Pretrained language models, shown marvelous, marvelous improvements</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pretrained language models (PLMs) have shown marvelous improvements across
various NLP tasks. Most Chinese PLMs simply treat an input text as a sequence
of characters, and completely ignore word information. Although Whole Word
Masking can alleviate this, the semantics in words is still not well
represented. In this paper, we revisit the segmentation granularity of Chinese
PLMs. We propose a mixed-granularity Chinese BERT (MigBERT) by considering both
characters and words. To achieve this, we design objective functions for
learning both character and word-level representations. We conduct extensive
experiments on various Chinese NLP tasks to evaluate existing PLMs as well as
the proposed MigBERT. Experimental results show that MigBERT achieves new SOTA
performance on all these tasks. Further analysis demonstrates that words are
semantically richer than characters. More interestingly, we show that MigBERT
also works with Japanese. Our code has been released
here~\footnote{\url{this https URL}} and you can download
our model here~\footnote{\url{this https URL}}.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：SPeC: A Soft Prompt-Based Calibration on Mitigating Performance  Variability in Clinical Notes Summarization</b></summary>
  <p><b>编号</b>：[208]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13035</p>
  <p><b>作者</b>：Yu-Neng Chuang,  Ruixiang Tang,  Xiaoqian Jiang,  Xia Hu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：encompassing medical histories, Electronic health records, store an extensive, extensive array, Electronic health</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Electronic health records (EHRs) store an extensive array of patient
information, encompassing medical histories, diagnoses, treatments, and test
outcomes. These records are crucial for enabling healthcare providers to make
well-informed decisions regarding patient care. Summarizing clinical notes
further assists healthcare professionals in pinpointing potential health risks
and making better-informed decisions. This process contributes to reducing
errors and enhancing patient outcomes by ensuring providers have access to the
most pertinent and current patient data. Recent research has shown that
incorporating prompts with large language models (LLMs) substantially boosts
the efficacy of summarization tasks. However, we show that this approach also
leads to increased output variance, resulting in notably divergent outputs even
when prompts share similar meanings. To tackle this challenge, we introduce a
model-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft
prompts to diminish variance while preserving the advantages of prompt-based
summarization. Experimental findings on multiple clinical note tasks and LLMs
indicate that our method not only bolsters performance but also effectively
curbs variance for various LLMs, providing a more uniform and dependable
solution for summarizing vital medical information.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：GesGPT: Speech Gesture Synthesis With Text Parsing from GPT</b></summary>
  <p><b>编号</b>：[221]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13013</p>
  <p><b>作者</b>：Nan Gao,  Zeyu Zhao,  Zhi Zeng,  Shuwu Zhang,  Dongdong Weng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：critical research area, gained significant attention, Large Language Models, research area, focusing on producing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Gesture synthesis has gained significant attention as a critical research
area, focusing on producing contextually appropriate and natural gestures
corresponding to speech or textual input. Although deep learning-based
approaches have achieved remarkable progress, they often overlook the rich
semantic information present in the text, leading to less expressive and
meaningful gestures. We propose GesGPT, a novel approach to gesture generation
that leverages the semantic analysis capabilities of Large Language Models
(LLMs), such as GPT. By capitalizing on the strengths of LLMs for text
analysis, we design prompts to extract gesture-related information from textual
input. Our method entails developing prompt principles that transform gesture
generation into an intention classification problem based on GPT, and utilizing
a curated gesture library and integration module to produce semantically rich
co-speech gestures. Experimental results demonstrate that GesGPT effectively
generates contextually appropriate and expressive gestures, offering a new
perspective on semantic co-speech gesture generation.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Is ChatGPT A Good Keyphrase Generator? A Preliminary Study</b></summary>
  <p><b>编号</b>：[229]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13001</p>
  <p><b>作者</b>：Mingyang Song,  Haiyun Jiang,  Shuming Shi,  Songfang Yao,  Shilong Lu,  Yi Feng,  Huafeng Liu,  Liping Jing</p>
  <p><b>备注</b>：Technical Report, 7 pages</p>
  <p><b>关键词</b>：computational linguistics community, recently garnered significant, garnered significant attention, linguistics community, keyphrase generation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The emergence of ChatGPT has recently garnered significant attention from the
computational linguistics community. To demonstrate its capabilities as a
keyphrase generator, we conduct a preliminary evaluation of ChatGPT for the
keyphrase generation task. We evaluate its performance in various aspects,
including keyphrase generation prompts, keyphrase generation diversity,
multi-domain keyphrase generation, and long document understanding. Our
evaluation is based on six benchmark datasets, and we adopt the prompt
suggested by OpenAI while extending it to six candidate prompts. We find that
ChatGPT performs exceptionally well on all six candidate prompts, with minor
performance differences observed across the datasets. Based on our findings, we
conclude that ChatGPT has great potential for keyphrase generation. Moreover,
we discover that ChatGPT still faces challenges when it comes to generating
absent keyphrases. Meanwhile, in the final section, we also present some
limitations and future expansions of this report.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Analyzing the Generalizability of Deep Contextualized Language  Representations For Text Classification</b></summary>
  <p><b>编号</b>：[264]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12936</p>
  <p><b>作者</b>：Berfu Buyukoz</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：study evaluates, evaluates the robustness, supervised learning, contextual language representations, deep contextual language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This study evaluates the robustness of two state-of-the-art deep contextual
language representations, ELMo and DistilBERT, on supervised learning of binary
protest news classification and sentiment analysis of product reviews. A
"cross-context" setting is enabled using test sets that are distinct from the
training data. Specifically, in the news classification task, the models are
developed on local news from India and tested on the local news from China. In
the sentiment analysis task, the models are trained on movie reviews and tested
on customer reviews. This comparison is aimed at exploring the limits of the
representative power of today's Natural Language Processing systems on the path
to the systems that are generalizable to real-life scenarios. The models are
fine-tuned and fed into a Feed-Forward Neural Network and a Bidirectional Long
Short Term Memory network. Multinomial Naive Bayes and Linear Support Vector
Machine are used as traditional baselines. The results show that, in binary
text classification, DistilBERT is significantly better than ELMo on
generalizing to the cross-context setting. ELMo is observed to be significantly
more robust to the cross-context test data than both baselines. On the other
hand, the baselines performed comparably well to ELMo when the training and
test data are subsets of the same corpus (no cross-context). DistilBERT is also
found to be 30% smaller and 83% faster than ELMo. The results suggest that
DistilBERT can transfer generic semantic knowledge to other domains better than
ELMo. DistilBERT is also favorable in incorporating into real-life systems for
it requires a smaller computational training budget. When generalization is not
the utmost preference and test domain is similar to the training domain, the
traditional ML algorithms can still be considered as more economic alternatives
to deep language representations.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Towards Understanding the Generalization of Medical Text-to-SQL Models  and Datasets</b></summary>
  <p><b>编号</b>：[278]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12898</p>
  <p><b>作者</b>：Richard Tarbell,  Kim-Kwang Raymond Choo,  Glenn Dietrich,  Anthony Rios</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Electronic medical records, stored in relational, relational databases, Electronic medical, medical records</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Electronic medical records (EMRs) are stored in relational databases. It can
be challenging to access the required information if the user is unfamiliar
with the database schema or general database fundamentals. Hence, researchers
have explored text-to-SQL generation methods that provide healthcare
professionals direct access to EMR data without needing a database expert.
However, currently available datasets have been essentially "solved" with
state-of-the-art models achieving accuracy greater than or near 90%. In this
paper, we show that there is still a long way to go before solving text-to-SQL
generation in the medical domain. To show this, we create new splits of the
existing medical text-to-SQL dataset MIMICSQL that better measure the
generalizability of the resulting models. We evaluate state-of-the-art language
models on our new split showing substantial drops in performance with accuracy
dropping from up to 92% to 28%, thus showing substantial room for improvement.
Moreover, we introduce a novel data augmentation approach to improve the
generalizability of the language models. Overall, this paper is the first step
towards developing more robust text-to-SQL models in the medical
domain.\footnote{The dataset and code will be released upon acceptance.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：A Small-Scale Switch Transformer and NLP-based Model for Clinical  Narratives Classification</b></summary>
  <p><b>编号</b>：[281]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12892</p>
  <p><b>作者</b>：Thanh-Dung Le,  Philippe Jouvet,  Rita Noumeir</p>
  <p><b>备注</b>：Submitted to IEEE Journal of Biomedical and Health Informatics</p>
  <p><b>关键词</b>：natural language processing, Switch Transformer, language processing tasks, Transformer-based models, small French clinical</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, Transformer-based models such as the Switch Transformer have
achieved remarkable results in natural language processing tasks. However,
these models are often too complex and require extensive pre-training, which
limits their effectiveness for small clinical text classification tasks with
limited data. In this study, we propose a simplified Switch Transformer
framework and train it from scratch on a small French clinical text
classification dataset at CHU Sainte-Justine hospital. Our results demonstrate
that the simplified small-scale Transformer models outperform pre-trained
BERT-based models, including DistillBERT, CamemBERT, FlauBERT, and FrALBERT.
Additionally, using a mixture of expert mechanisms from the Switch Transformer
helps capture diverse patterns; hence, the proposed approach achieves better
results than a conventional Transformer with the self-attention mechanism.
Finally, our proposed framework achieves an accuracy of 87\%, precision at
87\%, and recall at 85\%, compared to the third-best pre-trained BERT-based
model, FlauBERT, which achieved an accuracy of 84\%, precision at 84\%, and
recall at 84\%. However, Switch Transformers have limitations, including a
generalization gap and sharp minima. We compare it with a multi-layer
perceptron neural network for small French clinical narratives classification
and show that the latter outperforms all other models.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：JaCoText: A Pretrained Model for Java Code-Text Generation</b></summary>
  <p><b>编号</b>：[292]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12869</p>
  <p><b>作者</b>：Jessica López Espejel,  Mahaman Sanoussi Yahaya Alassan,  Walid Dahhane,  El Hassane Ettifouri</p>
  <p><b>备注</b>：International Conference on Code Generation and Implementation Volume: 17</p>
  <p><b>关键词</b>：shown high performance, natural language, shown high, language, language generation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pretrained transformer-based models have shown high performance in natural
language generation task. However, a new wave of interest has surged: automatic
programming language generation. This task consists of translating natural
language instructions to a programming code. Despite the fact that well-known
pretrained models on language generation have achieved good performance in
learning programming languages, effort is still needed in automatic code
generation. In this paper, we introduce JaCoText, a model based on Transformers
neural network. It aims to generate java source code from natural language
text. JaCoText leverages advantages of both natural language and code
generation models. More specifically, we study some findings from the state of
the art and use them to (1) initialize our model from powerful pretrained
models, (2) explore additional pretraining on our java dataset, (3) carry out
experiments combining the unimodal and bimodal data in the training, and (4)
scale the input and output length during the fine-tuning of the model.
Conducted experiments on CONCODE dataset show that JaCoText achieves new
state-of-the-art results.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Salient Span Masking for Temporal Understanding</b></summary>
  <p><b>编号</b>：[295]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12860</p>
  <p><b>作者</b>：Jeremy R. Cole,  Aditi Chaudhary,  Bhuwan Dhingra,  Partha Talukdar</p>
  <p><b>备注</b>：5 pages 1 figure, to appear in EACL 2023</p>
  <p><b>关键词</b>：closed-book question answering, Salient Span Masking, Temporal Span Masking, question answering performance, improve closed-book question</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Salient Span Masking (SSM) has shown itself to be an effective strategy to
improve closed-book question answering performance. SSM extends general masked
language model pretraining by creating additional unsupervised training
sentences that mask a single entity or date span, thus oversampling factual
information. Despite the success of this paradigm, the span types and sampling
strategies are relatively arbitrary and not widely studied for other tasks.
Thus, we investigate SSM from the perspective of temporal tasks, where learning
a good representation of various temporal expressions is important. To that
end, we introduce Temporal Span Masking (TSM) intermediate training. First, we
find that SSM alone improves the downstream performance on three temporal tasks
by an avg. +5.8 points. Further, we are able to achieve additional improvements
(avg. +0.29 points) by adding the TSM task. These comprise the new best
reported results on the targeted tasks. Our analysis suggests that the
effectiveness of SSM stems from the sentences chosen in the training data
rather than the mask choice: sentences with entities frequently also contain
temporal expressions. Nonetheless, the additional targeted spans of TSM can
still improve performance, especially in a zero-shot context.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：From Wide to Deep: Dimension Lifting Network for Parameter-efficient  Knowledge Graph Embedding</b></summary>
  <p><b>编号</b>：[306]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12816</p>
  <p><b>作者</b>：Borui Cai,  Yong Xiang,  Longxiang Gao,  Di Wu,  He Zhang,  Jiong Jin,  Tom Luan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：entity representations, KGE methods, KGE, Conventional KGE methods, downstream tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Knowledge graph embedding (KGE) that maps entities and relations into vector
representations is essential for downstream tasks. Conventional KGE methods
require relatively high-dimensional entity representations to preserve the
structural information of knowledge graph, but lead to oversized model
parameters. Recent methods reduce model parameters by adopting low-dimensional
entity representations, while developing techniques (e.g., knowledge
distillation) to compensate for the reduced dimension. However, such operations
produce degraded model accuracy and limited reduction of model parameters.
Specifically, we view the concatenation of all entity representations as an
embedding layer, and then conventional KGE methods that adopt high-dimensional
entity representations equal to enlarging the width of the embedding layer to
gain expressiveness. To achieve parameter efficiency without sacrificing
accuracy, we instead increase the depth and propose a deeper embedding network
for entity representations, i.e., a narrow embedding layer and a multi-layer
dimension lifting network (LiftNet). Experiments on three public datasets show
that the proposed method (implemented based on TransE and DistMult) with
4-dimensional entity representations achieves more accurate link prediction
results than counterpart parameter-efficient KGE methods and strong KGE
baselines, including TransE and DistMult with 512-dimensional entity
representations.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Are LLMs the Master of All Trades? : Exploring Domain-Agnostic Reasoning  Skills of LLMs</b></summary>
  <p><b>编号</b>：[309]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12810</p>
  <p><b>作者</b>：Shrivats Agrawal</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Machine Learning communities, Machine Learning, highly contested topic, Learning communities, topic in Machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The potential of large language models (LLMs) to reason like humans has been
a highly contested topic in Machine Learning communities. However, the
reasoning abilities of humans are multifaceted and can be seen in various
forms, including analogical, spatial and moral reasoning, among others. This
fact raises the question whether LLMs can perform equally well across all these
different domains. This research work aims to investigate the performance of
LLMs on different reasoning tasks by conducting experiments that directly use
or draw inspirations from existing datasets on analogical and spatial
reasoning. Additionally, to evaluate the ability of LLMs to reason like human,
their performance is evaluted on more open-ended, natural language questions.
My findings indicate that LLMs excel at analogical and moral reasoning, yet
struggle to perform as proficiently on spatial reasoning tasks. I believe these
experiments are crucial for informing the future development of LLMs,
particularly in contexts that require diverse reasoning proficiencies. By
shedding light on the reasoning abilities of LLMs, this study aims to push
forward our understanding of how they can better emulate the cognitive
abilities of humans.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：PACO: Provocation Involving Action, Culture, and Oppression</b></summary>
  <p><b>编号</b>：[310]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12808</p>
  <p><b>作者</b>：Vaibhav Garg,  Ganning Xu,  Munindar P. Singh</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：group based, religious groups, Indian Muslims, India, people identify</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In India, people identify with a particular group based on certain attributes
such as religion. The same religious groups are often provoked against each
other. Previous studies show the role of provocation in increasing tensions
between India's two prominent religious groups: Hindus and Muslims. With the
advent of the Internet, such provocation also surfaced on social media
platforms such as WhatsApp.
By leveraging an existing dataset of Indian WhatsApp posts, we identified
three categories of provoking sentences against Indian Muslims. Further, we
labeled 7,000 sentences for three provocation categories and called this
dataset PACO. We leveraged PACO to train a model that can identify provoking
sentences from a WhatsApp post. Our best model is fine-tuned RoBERTa and
achieved a 0.851 average AUC score over five-fold cross-validation.
Automatically identifying provoking sentences could stop provoking text from
reaching out to the masses, and can prevent possible discrimination or violence
against the target religious group.
Further, we studied the provocative speech through a pragmatic lens, by
identifying the dialog acts and impoliteness super-strategies used against the
religious group.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Features matching using natural language processing</b></summary>
  <p><b>编号</b>：[313]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12804</p>
  <p><b>作者</b>：Muhammad Danial Khilji</p>
  <p><b>备注</b>：10 pages, 7 figures, International Conference on NLP & AI (NLPAI 2023)</p>
  <p><b>关键词</b>：pretrained Natural Language, basic step, Natural Language, step in matching, matching</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The feature matching is a basic step in matching different datasets. This
article proposes shows a new hybrid model of a pretrained Natural Language
Processing (NLP) based model called BERT used in parallel with a statistical
model based on Jaccard similarity to measure the similarity between list of
features from two different datasets. This reduces the time required to search
for correlations or manually match each feature from one dataset to another.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：An Analysis of Abstractive Text Summarization Using Pre-trained Models</b></summary>
  <p><b>编号</b>：[320]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12796</p>
  <p><b>作者</b>：Tohida Rehman,  Suchandan Das,  Debarshi Kumar Sanyal,  Samiran Chattopadhyay</p>
  <p><b>备注</b>：11 Pages, 6 Figures, 3 Tables</p>
  <p><b>关键词</b>：Bing to find, People nowadays, find information, Yahoo, Internet</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>People nowadays use search engines like Google, Yahoo, and Bing to find
information on the Internet. Due to explosion in data, it is helpful for users
if they are provided relevant summaries of the search results rather than just
links to webpages. Text summarization has become a vital approach to help
consumers swiftly grasp vast amounts of this http URL this paper, different
pre-trained models for text summarization are evaluated on different datasets.
Specifically, we have used three different pre-trained models, namely,
google/pegasus-cnn-dailymail, T5-base, facebook/bart-large-cnn. We have
considered three different datasets, namely, CNN-dailymail, SAMSum and BillSum
to get the output from the above three models. The pre-trained models are
compared over these different datasets, each of 2000 examples, through ROUGH
and BLEU metrics.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Named Entity Recognition Based Automatic Generation of Research  Highlights</b></summary>
  <p><b>编号</b>：[321]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12795</p>
  <p><b>作者</b>：Tohida Rehman,  Debarshi Kumar Sanyal,  Prasenjit Majumder,  Samiran Chattopadhyay</p>
  <p><b>备注</b>：7 Pages, 3 Figures, 2 Tables</p>
  <p><b>关键词</b>：traditionally prefaced, highlights, paper, scientific paper, named entity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A scientific paper is traditionally prefaced by an abstract that summarizes
the paper. Recently, research highlights that focus on the main findings of the
paper have emerged as a complementary summary in addition to an abstract.
However, highlights are not yet as common as abstracts, and are absent in many
papers. In this paper, we aim to automatically generate research highlights
using different sections of a research paper as input. We investigate whether
the use of named entity recognition on the input improves the quality of the
generated highlights. In particular, we have used two deep learning-based
models: the first is a pointer-generator network, and the second augments the
first model with coverage mechanism. We then augment each of the above models
with named entity recognition features. The proposed method can be used to
produce highlights for papers with missing highlights. Our experiments show
that adding named entity information improves the performance of the deep
learning-based summarizers in terms of ROUGE, METEOR and BERTScore measures.</p>
  </details>
</details>
<h1>机器学习</h1>
<details>
  <summary>1. <b>标题：Learning and Verification of Task Structure in Instructional Videos</b></summary>
  <p><b>编号</b>：[1]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13519</p>
  <p><b>作者</b>：Medhini Narasimhan,  Licheng Yu,  Sean Bell,  Ning Zhang,  Trevor Darrell</p>
  <p><b>备注</b>：Wesbite at this https URL</p>
  <p><b>关键词</b>：multi-step task models, enormous number, diverse array, array of multi-step, instructional videos</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Given the enormous number of instructional videos available online, learning
a diverse array of multi-step task models from videos is an appealing goal. We
introduce a new pre-trained video model, VideoTaskformer, focused on
representing the semantics and structure of instructional videos. We pre-train
VideoTaskformer using a simple and effective objective: predicting weakly
supervised textual labels for steps that are randomly masked out from an
instructional video (masked step modeling). Compared to prior work which learns
step representations locally, our approach involves learning them globally,
leveraging video of the entire surrounding task as context. From these learned
representations, we can verify if an unseen video correctly executes a given
task, as well as forecast which steps are likely to be taken after a given
step. We introduce two new benchmarks for detecting mistakes in instructional
videos, to verify if there is an anomalous step and if steps are executed in
the right order. We also introduce a long-term forecasting benchmark, where the
goal is to predict long-range future steps from a given step. Our method
outperforms previous baselines on these tasks, and we believe the tasks will be
a valuable way for the community to measure the quality of step
representations. Additionally, we evaluate VideoTaskformer on 3 existing
benchmarks -- procedural activity recognition, step classification, and step
forecasting -- and demonstrate on each that our method outperforms existing
baselines and achieves new state-of-the-art performance.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Three ways to improve feature alignment for open vocabulary detection</b></summary>
  <p><b>编号</b>：[2]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13518</p>
  <p><b>作者</b>：Relja Arandjelović,  Alex Andonian,  Arthur Mensch,  Olivier J. Hénaff,  Jean-Baptiste Alayrac,  Andrew Zisserman</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：open vocabulary detection, zero-shot open vocabulary, vision-text feature alignment, core problem, open vocabulary</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The core problem in zero-shot open vocabulary detection is how to align
visual and text features, so that the detector performs well on unseen classes.
Previous approaches train the feature pyramid and detection head from scratch,
which breaks the vision-text feature alignment established during pretraining,
and struggles to prevent the language model from forgetting unseen classes.
We propose three methods to alleviate these issues. Firstly, a simple scheme
is used to augment the text embeddings which prevents overfitting to a small
number of classes seen during training, while simultaneously saving memory and
computation. Secondly, the feature pyramid network and the detection head are
modified to include trainable gated shortcuts, which encourages vision-text
feature alignment and guarantees it at the start of detection training.
Finally, a self-training approach is used to leverage a larger corpus of
image-text pairs thus improving detection performance on classes with no human
annotated bounding boxes.
Our three methods are evaluated on the zero-shot version of the LVIS
benchmark, each of them showing clear and significant benefits. Our final
network achieves the new stateof-the-art on the mAP-all metric and demonstrates
competitive performance for mAP-rare, as well as superior transfer to COCO and
Objects365.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Ablating Concepts in Text-to-Image Diffusion Models</b></summary>
  <p><b>编号</b>：[3]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13516</p>
  <p><b>作者</b>：Nupur Kumari,  Bingliang Zhang,  Sheng-Yu Wang,  Eli Shechtman,  Richard Zhang,  Jun-Yan Zhu</p>
  <p><b>备注</b>：project website: this https URL</p>
  <p><b>关键词</b>：powerful compositional ability, generate high-fidelity images, compositional ability, generate high-fidelity, powerful compositional</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large-scale text-to-image diffusion models can generate high-fidelity images
with powerful compositional ability. However, these models are typically
trained on an enormous amount of Internet data, often containing copyrighted
material, licensed images, and personal photos. Furthermore, they have been
found to replicate the style of various living artists or memorize exact
training samples. How can we remove such copyrighted concepts or images without
retraining the model from scratch? To achieve this goal, we propose an
efficient method of ablating concepts in the pretrained model, i.e., preventing
the generation of a target concept. Our algorithm learns to match the image
distribution for a target style, instance, or text prompt we wish to ablate to
the distribution corresponding to an anchor concept. This prevents the model
from generating target concepts given its text condition. Extensive experiments
show that our method can successfully prevent the generation of the ablated
concept while preserving closely related concepts in the model.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Persistent Nature: A Generative Model of Unbounded 3D Worlds</b></summary>
  <p><b>编号</b>：[4]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13515</p>
  <p><b>作者</b>：Lucy Chai,  Richard Tucker,  Zhengqi Li,  Phillip Isola,  Noah Snavely</p>
  <p><b>备注</b>：CVPR camera ready version, project page: this https URL</p>
  <p><b>关键词</b>：realistic image quality, increasingly realistic image, limited camera motions, image quality, realistic image</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite increasingly realistic image quality, recent 3D image generative
models often operate on 3D volumes of fixed extent with limited camera motions.
We investigate the task of unconditionally synthesizing unbounded nature
scenes, enabling arbitrarily large camera motion while maintaining a persistent
3D world model. Our scene representation consists of an extendable, planar
scene layout grid, which can be rendered from arbitrary camera poses via a 3D
decoder and volume rendering, and a panoramic skydome. Based on this
representation, we learn a generative world model solely from single-view
internet photos. Our method enables simulating long flights through 3D
landscapes, while maintaining global scene consistency--for instance, returning
to the starting point yields the same view of the scene. Our approach enables
scene extrapolation beyond the fixed bounds of current 3D generative models,
while also supporting a persistent, camera-independent world representation
that stands in contrast to auto-regressive 3D prediction models. Our project
page: this https URL.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Neural Preset for Color Style Transfer</b></summary>
  <p><b>编号</b>：[7]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13511</p>
  <p><b>作者</b>：Zhanghan Ke,  Yuhao Liu,  Lei Zhu,  Nanxuan Zhao,  Rynson W.H. Lau</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：vast memory requirement, Neural Preset technique, Neural Preset, technique to address, address the limitations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we present a Neural Preset technique to address the
limitations of existing color style transfer methods, including visual
artifacts, vast memory requirement, and slow style switching speed. Our method
is based on two core designs. First, we propose Deterministic Neural Color
Mapping (DNCM) to consistently operate on each pixel via an image-adaptive
color mapping matrix, avoiding artifacts and supporting high-resolution inputs
with a small memory footprint. Second, we develop a two-stage pipeline by
dividing the task into color normalization and stylization, which allows
efficient style switching by extracting color styles as presets and reusing
them on normalized input images. Due to the unavailability of pairwise
datasets, we describe how to train Neural Preset via a self-supervised
strategy. Various advantages of Neural Preset over existing methods are
demonstrated through comprehensive evaluations. Besides, we show that our
trained model can naturally support multiple applications without fine-tuning,
including low-light image enhancement, underwater image correction, image
dehazing, and image harmonization.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：The Quantization Model of Neural Scaling</b></summary>
  <p><b>编号</b>：[11]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13506</p>
  <p><b>作者</b>：Eric J. Michaud,  Ziming Liu,  Uzay Girit,  Max Tegmark</p>
  <p><b>备注</b>：24 pages, 15 figures</p>
  <p><b>关键词</b>：Quantization Hypothesis, observed power law, power law dropoff, data size, power law</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose the $\textit{Quantization Model}$ of neural scaling laws,
explaining both the observed power law dropoff of loss with model and data
size, and also the sudden emergence of new capabilities with scale. We derive
this model from what we call the $\textit{Quantization Hypothesis}$, where
learned network capabilities are quantized into discrete chunks
($\textit{quanta}$). We show that when quanta are learned in order of
decreasing use frequency, then a power law in use frequencies explains observed
power law scaling of loss. We validate this prediction on toy datasets, then
study how scaling curves decompose for large language models. Using language
model internals, we auto-discover diverse model capabilities (quanta) and find
tentative evidence that the distribution over corresponding subproblems in the
prediction of natural text is compatible with the power law predicted from the
neural scaling exponent as predicted from our theory.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Chordal Averaging on Flag Manifolds and Its Applications</b></summary>
  <p><b>编号</b>：[14]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13501</p>
  <p><b>作者</b>：Nathan Mankovich,  Tolga Birdal</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：flag manifold, Stiefel manifold, provably-convergent algorithm, chordal metric, paper presents</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a new, provably-convergent algorithm for computing the
flag-mean and flag-median of a set of points on a flag manifold under the
chordal metric. The flag manifold is a mathematical space consisting of flags,
which are sequences of nested subspaces of a vector space that increase in
dimension. The flag manifold is a superset of a wide range of known matrix
groups, including Stiefel and Grassmanians, making it a general object that is
useful in a wide variety computer vision problems.
To tackle the challenge of computing first order flag statistics, we first
transform the problem into one that involves auxiliary variables constrained to
the Stiefel manifold. The Stiefel manifold is a space of orthogonal frames, and
leveraging the numerical stability and efficiency of Stiefel-manifold
optimization enables us to compute the flag-mean effectively. Through a series
of experiments, we show the competence of our method in Grassmann and rotation
averaging, as well as principal component analysis.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：A Closer Look at Model Adaptation using Feature Distortion and  Simplicity Bias</b></summary>
  <p><b>编号</b>：[15]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13500</p>
  <p><b>作者</b>：Puja Trivedi,  Danai Koutra,  Jayaraman J. Thiagarajan</p>
  <p><b>备注</b>：Accepted to ICLR 2023 as notable-25% (spotlight)</p>
  <p><b>关键词</b>：effective transfer learning, transfer learning, expressivity of pretrained, pretrained models, models have increased</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Advances in the expressivity of pretrained models have increased interest in
the design of adaptation protocols which enable safe and effective transfer
learning. Going beyond conventional linear probing (LP) and fine tuning (FT)
strategies, protocols that can effectively control feature distortion, i.e.,
the failure to update features orthogonal to the in-distribution, have been
found to achieve improved out-of-distribution generalization (OOD). In order to
limit this distortion, the LP+FT protocol, which first learns a linear probe
and then uses this initialization for subsequent FT, was proposed. However, in
this paper, we find when adaptation protocols (LP, FT, LP+FT) are also
evaluated on a variety of safety objectives (e.g., calibration, robustness,
etc.), a complementary perspective to feature distortion is helpful to explain
protocol behavior. To this end, we study the susceptibility of protocols to
simplicity bias (SB), i.e. the well-known propensity of deep neural networks to
rely upon simple features, as SB has recently been shown to underlie several
problems in robust generalization. Using a synthetic dataset, we demonstrate
the susceptibility of existing protocols to SB. Given the strong effectiveness
of LP+FT, we then propose modified linear probes that help mitigate SB, and
lead to better initializations for subsequent FT. We verify the effectiveness
of the proposed LP+FT variants for decreasing SB in a controlled setting, and
their ability to improve OOD generalization and safety on three adaptation
datasets.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：The effectiveness of MAE pre-pretraining for billion-scale pretraining</b></summary>
  <p><b>编号</b>：[17]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13496</p>
  <p><b>作者</b>：Mannat Singh,  Quentin Duval,  Kalyan Vasudev Alwala,  Haoqi Fan,  Vaibhav Aggarwal,  Aaron Adcock,  Armand Joulin,  Piotr Dollár,  Christoph Feichtenhofer,  Ross Girshick,  Rohit Girdhar,  Ishan Misra</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：revisits the standard, paper revisits, computer vision, model, billions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper revisits the standard pretrain-then-finetune paradigm used in
computer vision for visual recognition tasks. Typically, state-of-the-art
foundation models are pretrained using large scale (weakly) supervised datasets
with billions of images. We introduce an additional pre-pretraining stage that
is simple and uses the self-supervised MAE technique to initialize the model.
While MAE has only been shown to scale with the size of models, we find that it
scales with the size of the training dataset as well. Thus, our MAE-based
pre-pretraining scales with both model and data size making it applicable for
training foundation models. Pre-pretraining consistently improves both the
model convergence and the downstream transfer performance across a range of
model scales (millions to billions of parameters), and dataset sizes (millions
to billions of images). We measure the effectiveness of pre-pretraining on 10
different visual recognition tasks spanning image classification, video
recognition, object detection, low-shot classification and zero-shot
recognition. Our largest model achieves new state-of-the-art results on
iNaturalist-18 (91.3%), 1-shot ImageNet-1k (62.1%), and zero-shot transfer on
Food-101 (96.0%). Our study reveals that model initialization plays a
significant role, even for web-scale pretraining with billions of images.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Boosting Reinforcement Learning and Planning with Demonstrations: A  Survey</b></summary>
  <p><b>编号</b>：[22]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13489</p>
  <p><b>作者</b>：Tongzhou Mu,  Hao Su</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：tremendous success recently, complex environments, tremendous success, impractical or inefficient, inefficient in complex</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Although reinforcement learning has seen tremendous success recently, this
kind of trial-and-error learning can be impractical or inefficient in complex
environments. The use of demonstrations, on the other hand, enables agents to
benefit from expert knowledge rather than having to discover the best action to
take through exploration. In this survey, we discuss the advantages of using
demonstrations in sequential decision making, various ways to apply
demonstrations in learning-based decision making paradigms (for example,
reinforcement learning and planning in the learned models), and how to collect
the demonstrations in various scenarios. Additionally, we exemplify a practical
pipeline for generating and utilizing demonstrations in the recently proposed
ManiSkill robot learning benchmark.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：TactoFind: A Tactile Only System for Object Retrieval</b></summary>
  <p><b>编号</b>：[26]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13482</p>
  <p><b>作者</b>：Sameer Pai,  Tao Chen,  Megha Tippur,  Edward Adelson,  Abhishek Gupta,  Pulkit Agrawal</p>
  <p><b>备注</b>：Accepted in ICRA 2023</p>
  <p><b>关键词</b>：sensing is absent, move freely, study the problem, retrieval in scenarios, shapes are unknown</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the problem of object retrieval in scenarios where visual sensing is
absent, object shapes are unknown beforehand and objects can move freely, like
grabbing objects out of a drawer. Successful solutions require localizing free
objects, identifying specific object instances, and then grasping the
identified objects, only using touch feedback. Unlike vision, where cameras can
observe the entire scene, touch sensors are local and only observe parts of the
scene that are in contact with the manipulator. Moreover, information gathering
via touch sensors necessitates applying forces on the touched surface which may
disturb the scene itself. Reasoning with touch, therefore, requires careful
exploration and integration of information over time -- a challenge we tackle.
We present a system capable of using sparse tactile feedback from fingertip
touch sensors on a dexterous hand to localize, identify and grasp novel objects
without any visual feedback. Videos are available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Optimization Dynamics of Equivariant and Augmented Neural Networks</b></summary>
  <p><b>编号</b>：[35]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13458</p>
  <p><b>作者</b>：Axel Flinth,  Fredrik Ohlsson</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：symmetric data, investigate the optimization, optimization of multilayer, multilayer perceptrons, perceptrons on symmetric</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We investigate the optimization of multilayer perceptrons on symmetric data.
We compare the strategy of constraining the architecture to be equivariant to
that of using augmentation. We show that, under natural assumptions on the loss
and non-linearities, the sets of equivariant stationary points are identical
for the two strategies, and that the set of equivariant layers is invariant
under the gradient flow for augmented models. Finally, we show that stationary
points may be unstable for augmented training although they are stable for the
equivariant models</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Human Behavior in the Time of COVID-19: Learning from Big Data</b></summary>
  <p><b>编号</b>：[37]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13452</p>
  <p><b>作者</b>：Hanjia Lyu,  Arsal Imtiaz,  Yufei Zhao,  Jiebo Luo</p>
  <p><b>备注</b>：Accepted for publication in the Horizons in Big Data 2022 article collection of Frontiers in Big Data</p>
  <p><b>关键词</b>：World Health Organization, Health Organization, World Health, million confirmed cases, human behavior</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Since the World Health Organization (WHO) characterized COVID-19 as a
pandemic in March 2020, there have been over 600 million confirmed cases of
COVID-19 and more than six million deaths as of October 2022. The relationship
between the COVID-19 pandemic and human behavior is complicated. On one hand,
human behavior is found to shape the spread of the disease. On the other hand,
the pandemic has impacted and even changed human behavior in almost every
aspect. To provide a holistic understanding of the complex interplay between
human behavior and the COVID-19 pandemic, researchers have been employing big
data techniques such as natural language processing, computer vision, audio
signal processing, frequent pattern mining, and machine learning. In this
study, we present an overview of the existing studies on using big data
techniques to study human behavior in the time of the COVID-19 pandemic. In
particular, we categorize these studies into three groups - using big data to
measure, model, and leverage human behavior, respectively. The related tasks,
data, and methods are summarized accordingly. To provide more insights into how
to fight the COVID-19 pandemic and future global catastrophes, we further
discuss challenges and potential opportunities.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Set-the-Scene: Global-Local Training for Generating Controllable NeRF  Scenes</b></summary>
  <p><b>编号</b>：[39]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13450</p>
  <p><b>作者</b>：Dana Cohen-Bar,  Elad Richardson,  Gal Metzer,  Raja Giryes,  Daniel Cohen-Or</p>
  <p><b>备注</b>：project page at this https URL</p>
  <p><b>关键词</b>：text-guided image generation, breakthroughs in text-guided, text-guided image, image generation, generation have led</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent breakthroughs in text-guided image generation have led to remarkable
progress in the field of 3D synthesis from text. By optimizing neural radiance
fields (NeRF) directly from text, recent methods are able to produce remarkable
results. Yet, these methods are limited in their control of each object's
placement or appearance, as they represent the scene as a whole. This can be a
major issue in scenarios that require refining or manipulating objects in the
scene. To remedy this deficit, we propose a novel GlobalLocal training
framework for synthesizing a 3D scene using object proxies. A proxy represents
the object's placement in the generated scene and optionally defines its coarse
geometry. The key to our approach is to represent each object as an independent
NeRF. We alternate between optimizing each NeRF on its own and as part of the
full scene. Thus, a complete representation of each object can be learned,
while also creating a harmonious scene with style and lighting match. We show
that using proxies allows a wide variety of editing options, such as adjusting
the placement of each independent object, removing objects from a scene, or
refining an object. Our results show that Set-the-Scene offers a powerful
solution for scene synthesis and manipulation, filling a crucial gap in
controllable text-to-3D synthesis.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：GiveMeLabeledIssues: An Open Source Issue Recommendation System</b></summary>
  <p><b>编号</b>：[47]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13418</p>
  <p><b>作者</b>：Joseph Vargovich,  Fabio Santos,  Jacob Penney,  Marco A. Gerosa,  Igor Steinmacher</p>
  <p><b>备注</b>：MSR Data and Tool Showcase 2023</p>
  <p><b>关键词</b>：Open Source Software, Source Software, Open Source, navigate an Open, project issue-tracking system</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Developers often struggle to navigate an Open Source Software (OSS) project's
issue-tracking system and find a suitable task. Proper issue labeling can aid
task selection, but current tools are limited to classifying the issues
according to their type (e.g., bug, question, good first issue, feature, etc.).
In contrast, this paper presents a tool (GiveMeLabeledIssues) that mines
project repositories and labels issues based on the skills required to solve
them. We leverage the domain of the APIs involved in the solution (e.g., User
Interface (UI), Test, Databases (DB), etc.) as a proxy for the required skills.
GiveMeLabeledIssues facilitates matching developers' skills to tasks, reducing
the burden on project maintainers. The tool obtained a precision of 83.9% when
predicting the API domains involved in the issues. The replication package
contains instructions on executing the tool and including new projects. A demo
video is available at this https URL</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Paraphrasing evades detectors of AI-generated text, but retrieval is an  effective defense</b></summary>
  <p><b>编号</b>：[51]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13408</p>
  <p><b>作者</b>：Kalpesh Krishna,  Yixiao Song,  Marzena Karpinska,  John Wieting,  Mohit Iyyer</p>
  <p><b>备注</b>：Preprint (27 pages). Code, models, data will be added to this https URL</p>
  <p><b>关键词</b>：fake content creation, malicious use cases, fake content, academic plagiarism, statistical irregularities</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To detect the deployment of large language models for malicious use cases
(e.g., fake content creation or academic plagiarism), several approaches have
recently been proposed for identifying AI-generated text via watermarks or
statistical irregularities. How robust are these detection algorithms to
paraphrases of AI-generated text? To stress test these detectors, we first
train an 11B parameter paraphrase generation model (DIPPER) that can paraphrase
paragraphs, optionally leveraging surrounding text (e.g., user-written prompts)
as context. DIPPER also uses scalar knobs to control the amount of lexical
diversity and reordering in the paraphrases. Paraphrasing text generated by
three large language models (including GPT3.5-davinci-003) with DIPPER
successfully evades several detectors, including watermarking, GPTZero,
DetectGPT, and OpenAI's text classifier. For example, DIPPER drops the
detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false
positive rate of 1%), without appreciably modifying the input semantics. To
increase the robustness of AI-generated text detection to paraphrase attacks,
we introduce a simple defense that relies on retrieving semantically-similar
generations and must be maintained by a language model API provider. Given a
candidate text, our algorithm searches a database of sequences previously
generated by the API, looking for sequences that match the candidate text
within a certain threshold. We empirically verify our defense using a database
of 15M generations from a fine-tuned T5-XXL model and find that it can detect
80% to 97% of paraphrased generations across different settings, while only
classifying 1% of human-written sequences as AI-generated. We will open source
our code, model and data for future research.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：SC-MIL: Supervised Contrastive Multiple Instance Learning for Imbalanced  Classification in Pathology</b></summary>
  <p><b>编号</b>：[52]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13405</p>
  <p><b>作者</b>：Dinkar Juyal,  Siddhant Shingi,  Syed Ashar Javed,  Harshith Padigela,  Chintan Shah,  Anand Sampat,  Archit Khosla,  John Abel,  Amaro Taylor-Weiner</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：gigapixel-sized images, predict biomarkers, biomarkers and risk-stratify, risk-stratify patients, patients from gigapixel-sized</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multiple Instance learning (MIL) models have been extensively used in
pathology to predict biomarkers and risk-stratify patients from gigapixel-sized
images. Machine learning problems in medical imaging often deal with rare
diseases, making it important for these models to work in a label-imbalanced
setting. Furthermore, these imbalances can occur in out-of-distribution (OOD)
datasets when the models are deployed in the real-world. We leverage the idea
that decoupling feature and classifier learning can lead to improved decision
boundaries for label imbalanced datasets. To this end, we investigate the
integration of supervised contrastive learning with multiple instance learning
(SC-MIL). Specifically, we propose a joint-training MIL framework in the
presence of label imbalance that progressively transitions from learning
bag-level representations to optimal classifier learning. We perform
experiments with different imbalance settings for two well-studied problems in
cancer pathology: subtyping of non-small cell lung cancer and subtyping of
renal cell carcinoma. SC-MIL provides large and consistent improvements over
other techniques on both in-distribution (ID) and OOD held-out sets across
multiple imbalanced settings.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Optimization and Optimizers for Adversarial Robustness</b></summary>
  <p><b>编号</b>：[54]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13401</p>
  <p><b>作者</b>：Hengyue Liang,  Buyun Liang,  Le Peng,  Ying Cui,  Tim Mitchell,  Ju Sun</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：nontrivial constrained optimization, deep learning models, entails solving nontrivial, solving nontrivial constrained, deep learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Empirical robustness evaluation (RE) of deep learning models against
adversarial perturbations entails solving nontrivial constrained optimization
problems. Existing numerical algorithms that are commonly used to solve them in
practice predominantly rely on projected gradient, and mostly handle
perturbations modeled by the $\ell_1$, $\ell_2$ and $\ell_\infty$ distances. In
this paper, we introduce a novel algorithmic framework that blends a
general-purpose constrained-optimization solver PyGRANSO with Constraint
Folding (PWCF), which can add more reliability and generality to the
state-of-the-art RE packages, e.g., AutoAttack. Regarding reliability, PWCF
provides solutions with stationarity measures and feasibility tests to assess
the solution quality. For generality, PWCF can handle perturbation models that
are typically inaccessible to the existing projected gradient methods; the main
requirement is the distance metric to be almost everywhere differentiable.
Taking advantage of PWCF and other existing numerical algorithms, we further
explore the distinct patterns in the solutions found for solving these
optimization problems using various combinations of losses, perturbation
models, and optimization algorithms. We then discuss the implications of these
patterns on the current robustness evaluation and adversarial training.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis</b></summary>
  <p><b>编号</b>：[59]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13391</p>
  <p><b>作者</b>：Chantal Pellegrini,  Matthias Keicher,  Ege Özsoy,  Petra Jiraskova,  Rickmer Braren,  Nassir Navab</p>
  <p><b>备注</b>：9 pages, 2 figures, 6 tables</p>
  <p><b>关键词</b>：diagnosis, resource to support, support clinical decision-making, clinical, Automated diagnosis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automated diagnosis prediction from medical images is a valuable resource to
support clinical decision-making. However, such systems usually need to be
trained on large amounts of annotated data, which often is scarce in the
medical domain. Zero-shot methods address this challenge by allowing a flexible
adaption to new settings with different clinical findings without relying on
labeled data. Further, to integrate automated diagnosis in the clinical
workflow, methods should be transparent and explainable, increasing medical
professionals' trust and facilitating correctness verification. In this work,
we introduce Xplainer, a novel framework for explainable zero-shot diagnosis in
the clinical setting. Xplainer adapts the classification-by-description
approach of contrastive vision-language models to the multi-label medical
diagnosis task. Specifically, instead of directly predicting a diagnosis, we
prompt the model to classify the existence of descriptive observations, which a
radiologist would look for on an X-Ray scan, and use the descriptor
probabilities to estimate the likelihood of a diagnosis. Our model is
explainable by design, as the final diagnosis prediction is directly based on
the prediction of the underlying descriptors. We evaluate Xplainer on two chest
X-ray datasets, CheXpert and ChestX-ray14, and demonstrate its effectiveness in
improving the performance and explainability of zero-shot diagnosis. Our
results suggest that Xplainer provides a more detailed understanding of the
decision-making process and can be a valuable tool for clinical diagnosis.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Compositional Zero-Shot Domain Transfer with Text-to-Text Models</b></summary>
  <p><b>编号</b>：[61]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13386</p>
  <p><b>作者</b>：Fangyu Liu,  Qianchu Liu,  Shruthi Bannur,  Fernando Pérez-García,  Naoto Usuyama,  Sheng Zhang,  Tristan Naumann,  Aditya Nori,  Hoifung Poon,  Javier Alvarez-Valle,  Ozan Oktay,  Stephanie L. Hyland</p>
  <p><b>备注</b>：Accepted at TACL, pre-MIT Press publication version. 16 pages, 4 figures</p>
  <p><b>关键词</b>：improving task performance, bottleneck for improving, performance in specialised, task, domain</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Label scarcity is a bottleneck for improving task performance in specialised
domains. We propose a novel compositional transfer learning framework (DoT5 -
domain compositional zero-shot T5) for zero-shot domain transfer. Without
access to in-domain labels, DoT5 jointly learns domain knowledge (from MLM of
unlabelled in-domain free text) and task knowledge (from task training on more
readily available general-domain data) in a multi-task manner. To improve the
transferability of task training, we design a strategy named NLGU: we
simultaneously train NLG for in-domain label-to-data generation which enables
data augmentation for self-finetuning and NLU for label prediction. We evaluate
DoT5 on the biomedical domain and the resource-lean subdomain of radiology,
focusing on NLI, text summarisation and embedding learning. DoT5 demonstrates
the effectiveness of compositional transfer learning through multi-task
learning. In particular, DoT5 outperforms the current SOTA in zero-shot
transfer by over 7 absolute points in accuracy on RadNLI. We validate DoT5 with
ablations and a case study demonstrating its ability to solve challenging NLI
examples requiring in-domain expertise.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Adversarial Robustness of Learning-based Static Malware Classifiers</b></summary>
  <p><b>编号</b>：[69]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13372</p>
  <p><b>作者</b>：Shoumik Saha,  Wenxiao Wang,  Yigitcan Kaya,  Soheil Feizi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ongoing arms race, arms race, anti-virus systems, authors and anti-virus, arms race increases</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Malware detection has long been a stage for an ongoing arms race between
malware authors and anti-virus systems. Solutions that utilize machine learning
(ML) gain traction as the scale of this arms race increases. This trend,
however, makes performing attacks directly on ML an attractive prospect for
adversaries. We study this arms race from both perspectives in the context of
MalConv, a popular convolutional neural network-based malware classifier that
operates on raw bytes of files. First, we show that MalConv is vulnerable to
adversarial patch attacks: appending a byte-level patch to malware files
bypasses detection 94.3% of the time. Moreover, we develop a universal
adversarial patch (UAP) attack where a single patch can drop the detection rate
in constant time of any malware file that contains it by 80%. These patches are
effective even being relatively small with respect to the original file size --
between 2%-8%. As a countermeasure, we then perform window ablation that allows
us to apply de-randomized smoothing, a modern certified defense to patch
attacks in vision tasks, to raw files. The resulting `smoothed-MalConv' can
detect over 80% of malware that contains the universal patch and provides
certified robustness up to 66%, outlining a promising step towards robust
malware detection. To our knowledge, we are the first to apply universal
adversarial patch attack and certified defense using ablations on byte level in
the malware field.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Requirement Formalisation using Natural Language Processing and Machine  Learning: A Systematic Review</b></summary>
  <p><b>编号</b>：[72]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13365</p>
  <p><b>作者</b>：Shekoufeh Kolahdouz-Rahimi,  Kevin Lano,  Chenghua Lin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：automatic Requirement Formalisation, software development methodologies, development methodologies attracts, methodologies attracts developers, Requirement Formalisation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Improvement of software development methodologies attracts developers to
automatic Requirement Formalisation (RF) in the Requirement Engineering (RE)
field. The potential advantages by applying Natural Language Processing (NLP)
and Machine Learning (ML) in reducing the ambiguity and incompleteness of
requirement written in natural languages is reported in different studies. The
goal of this paper is to survey and classify existing work on NLP and ML for
RF, identifying challenges in this domain and providing promising future
research directions. To achieve this, we conducted a systematic literature
review to outline the current state-of-the-art of NLP and ML techniques in RF
by selecting 257 papers from common used libraries. The search result is
filtered by defining inclusion and exclusion criteria and 47 relevant studies
between 2012 and 2022 are selected. We found that heuristic NLP approaches are
the most common NLP techniques used for automatic RF, primary operating on
structured and semi-structured data. This study also revealed that Deep
Learning (DL) technique are not widely used, instead classical ML techniques
are predominant in the surveyed studies. More importantly, we identified the
difficulty of comparing the performance of different approaches due to the lack
of standard benchmark cases for RF.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Reevaluating Data Partitioning for Emotion Detection in EmoWOZ</b></summary>
  <p><b>编号</b>：[73]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13364</p>
  <p><b>作者</b>：Moeen Mostafavi,  Michael D. Porter</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：paper focuses, emotion labels, emotion, emotion tags, extension of MultiWOZ</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper focuses on the EmoWoz dataset, an extension of MultiWOZ that
provides emotion labels for the dialogues. MultiWOZ was partitioned initially
for another purpose, resulting in a distributional shift when considering the
new purpose of emotion recognition. The emotion tags in EmoWoz are highly
imbalanced and unevenly distributed across the partitions, which causes
sub-optimal performance and poor comparison of models. We propose a stratified
sampling scheme based on emotion tags to address this issue, improve the
dataset's distribution, and reduce dataset shift. We also introduce a special
technique to handle conversation (sequential) data with many emotional tags.
Using our proposed sampling method, models built upon EmoWoz can perform
better, making it a more reliable resource for training conversational agents
with emotional intelligence. We recommend that future researchers use this new
partitioning to ensure consistent and accurate performance evaluations.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：FS-Real: Towards Real-World Cross-Device Federated Learning</b></summary>
  <p><b>编号</b>：[74]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13363</p>
  <p><b>作者</b>：Daoyuan Chen,  Dawei Gao,  Yuexiang Xie,  Xuchen Pan,  Zitao Li,  Yaliang Li,  Bolin Ding,  Jingren Zhou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：train high-quality models, attracts increasing attention, Federated Learning, heterogeneous devices, aims to train</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Federated Learning (FL) aims to train high-quality models in collaboration
with distributed clients while not uploading their local data, which attracts
increasing attention in both academia and industry. However, there is still a
considerable gap between the flourishing FL research and real-world scenarios,
mainly caused by the characteristics of heterogeneous devices and its scales.
Most existing works conduct evaluations with homogeneous devices, which are
mismatched with the diversity and variability of heterogeneous devices in
real-world scenarios. Moreover, it is challenging to conduct research and
development at scale with heterogeneous devices due to limited resources and
complex software stacks. These two key factors are important yet underexplored
in FL research as they directly impact the FL training dynamics and final
performance, making the effectiveness and usability of FL algorithms unclear.
To bridge the gap, in this paper, we propose an efficient and scalable
prototyping system for real-world cross-device FL, FS-Real. It supports
heterogeneous device runtime, contains parallelism and robustness enhanced FL
server, and provides implementations and extensibility for advanced FL utility
features such as personalization, communication compression and asynchronous
aggregation. To demonstrate the usability and efficiency of FS-Real, we conduct
extensive experiments with various device distributions, quantify and analyze
the effect of the heterogeneous device and various scales, and further provide
insights and open discussions about real-world FL scenarios. Our system is
released to help to pave the way for further real-world FL research and broad
applications involving diverse devices and scales.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Towards the Scalable Evaluation of Cooperativeness in Language Models</b></summary>
  <p><b>编号</b>：[75]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13360</p>
  <p><b>作者</b>：Alan Chan,  Maxime Riché,  Jesse Clifton</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：conflict resolution, systems driven, driven by pre-trained, assist humans, humans in high-stakes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>It is likely that AI systems driven by pre-trained language models (PLMs)
will increasingly be used to assist humans in high-stakes interactions with
other agents, such as negotiation or conflict resolution. Consistent with the
goals of Cooperative AI \citep{dafoe_open_2020}, we wish to understand and
shape the multi-agent behaviors of PLMs in a pro-social manner. An important
first step is the evaluation of model behaviour across diverse cooperation
problems. Since desired behaviour in an interaction depends upon precise
game-theoretic structure, we focus on generating scenarios with particular
structures with both crowdworkers and a language model. Our work proceeds as
follows. First, we discuss key methodological issues in the generation of
scenarios corresponding to particular game-theoretic structures. Second, we
employ both crowdworkers and a language model to generate such scenarios. We
find that the quality of generations tends to be mediocre in both cases. We
additionally get both crowdworkers and a language model to judge whether given
scenarios align with their intended game-theoretic structure, finding mixed
results depending on the game. Third, we provide a dataset of scenario based on
our data generated. We provide both quantitative and qualitative evaluations of
UnifiedQA and GPT-3 on this dataset. We find that instruct-tuned models tend to
act in a way that could be perceived as cooperative when scaled up, while other
models seemed to have flat scaling trends.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Increasing Textual Context Size Boosts Medical Image-Text Matching</b></summary>
  <p><b>编号</b>：[82]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13340</p>
  <p><b>作者</b>：Idan Glassberg,  Tom Hope</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：short technical report, technical report demonstrates, image-text matching tasks, short technical, technical report</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This short technical report demonstrates a simple technique that yields state
of the art results in medical image-text matching tasks. We analyze the use of
OpenAI's CLIP, a general image-text matching model, and observe that CLIP's
limited textual input size has negative impact on downstream performance in the
medical domain where encoding longer textual contexts is often required. We
thus train and release ClipMD, which is trained with a simple sliding window
technique to encode textual captions. ClipMD was tested on two medical
image-text datasets and compared with other image-text matching models. The
results show that ClipMD outperforms other models on both datasets by a large
margin. We make our code and pretrained model publicly available.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Audio Diffusion Model for Speech Synthesis: A Survey on Text To Speech  and Speech Enhancement in Generative AI</b></summary>
  <p><b>编号</b>：[83]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13336</p>
  <p><b>作者</b>：Chenshuang Zhang,  Chaoning Zhang,  Sheng Zheng,  Mengchun Zhang,  Maryam Qamar,  Sung-Ho Bae,  In So Kweon</p>
  <p><b>备注</b>：18 pages</p>
  <p><b>关键词</b>：demonstrated impressive performance, interesting direction, diffusion model, demonstrated impressive, impressive performance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generative AI has demonstrated impressive performance in various fields,
among which speech synthesis is an interesting direction. With the diffusion
model as the most popular generative model, numerous works have attempted two
active tasks: text to speech and speech enhancement. This work conducts a
survey on audio diffusion model, which is complementary to existing surveys
that either lack the recent progress of diffusion-based speech synthesis or
highlight an overall picture of applying diffusion model in multiple fields.
Specifically, this work first briefly introduces the background of audio and
diffusion model. As for the text-to-speech task, we divide the methods into
three categories based on the stage where diffusion model is adopted: acoustic
model, vocoder and end-to-end framework. Moreover, we categorize various speech
enhancement tasks by either certain signals are removed or added into the input
speech. Comparisons of experimental results and discussions are also covered in
this survey.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Decentralized Adversarial Training over Graphs</b></summary>
  <p><b>编号</b>：[84]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13326</p>
  <p><b>作者</b>：Ying Cao,  Elsa Rizk,  Stefan Vlaski,  Ali H. Sayed</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2303.01936</p>
  <p><b>关键词</b>：attracting considerable attention, recent years, vulnerability of machine, attracting considerable, considerable attention</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The vulnerability of machine learning models to adversarial attacks has been
attracting considerable attention in recent years. Most existing studies focus
on the behavior of stand-alone single-agent learners. In comparison, this work
studies adversarial training over graphs, where individual agents are subjected
to perturbations of varied strength levels across space. It is expected that
interactions by linked agents, and the heterogeneity of the attack models that
are possible over the graph, can help enhance robustness in view of the
coordination power of the group. Using a min-max formulation of diffusion
learning, we develop a decentralized adversarial training framework for
multi-agent systems. We analyze the convergence properties of the proposed
scheme for both convex and non-convex environments, and illustrate the enhanced
robustness to adversarial attacks.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Reckoning with the Disagreement Problem: Explanation Consensus as a  Training Objective</b></summary>
  <p><b>编号</b>：[94]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13299</p>
  <p><b>作者</b>：Avi Schwarzschild,  Max Cembalest,  Karthik Rao,  Keegan Hines,  John Dickerson</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：neural networks increasingly, networks increasingly make, increasingly make critical, make critical decisions, high-stakes settings</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As neural networks increasingly make critical decisions in high-stakes
settings, monitoring and explaining their behavior in an understandable and
trustworthy manner is a necessity. One commonly used type of explainer is post
hoc feature attribution, a family of methods for giving each feature in an
input a score corresponding to its influence on a model's output. A major
limitation of this family of explainers in practice is that they can disagree
on which features are more important than others. Our contribution in this
paper is a method of training models with this disagreement problem in mind. We
do this by introducing a Post hoc Explainer Agreement Regularization (PEAR)
loss term alongside the standard term corresponding to accuracy, an additional
term that measures the difference in feature attribution between a pair of
explainers. We observe on three datasets that we can train a model with this
loss term to improve explanation consensus on unseen data, and see improved
consensus between explainers other than those used in the loss term. We examine
the trade-off between improved consensus and model performance. And finally, we
study the influence our method has on feature attribution explanations.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Improving Generalization with Domain Convex Game</b></summary>
  <p><b>编号</b>：[95]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13297</p>
  <p><b>作者</b>：Fangrui Lv,  Jian Liang,  Shuang Li,  Jinming Zhang,  Di Liu</p>
  <p><b>备注</b>：accepted by CVPR 2023</p>
  <p><b>关键词</b>：deep neural networks, poor generalization capability, multiple source domains, alleviate the poor, capability of deep</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Domain generalization (DG) tends to alleviate the poor generalization
capability of deep neural networks by learning model with multiple source
domains. A classical solution to DG is domain augmentation, the common belief
of which is that diversifying source domains will be conducive to the
out-of-distribution generalization. However, these claims are understood
intuitively, rather than mathematically. Our explorations empirically reveal
that the correlation between model generalization and the diversity of domains
may be not strictly positive, which limits the effectiveness of domain
augmentation. This work therefore aim to guarantee and further enhance the
validity of this strand. To this end, we propose a new perspective on DG that
recasts it as a convex game between domains. We first encourage each
diversified domain to enhance model generalization by elaborately designing a
regularization term based on supermodularity. Meanwhile, a sample filter is
constructed to eliminate low-quality samples, thereby avoiding the impact of
potentially harmful information. Our framework presents a new avenue for the
formal analysis of DG, heuristic analysis and extensive experiments demonstrate
the rationality and effectiveness.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Enriching Neural Network Training Dataset to Improve Worst-Case  Performance Guarantees</b></summary>
  <p><b>编号</b>：[122]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13228</p>
  <p><b>作者</b>：Rahul Nellikkath,  Spyros Chatzivasileiadis</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2212.10930</p>
  <p><b>关键词</b>：approximate non-linear relationships, Machine learning algorithms, Machine learning, non-linear relationships, valuable tool</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine learning algorithms, especially Neural Networks (NNs), are a valuable
tool used to approximate non-linear relationships, like the AC-Optimal Power
Flow (AC-OPF), with considerable accuracy -- and achieving a speedup of several
orders of magnitude when deployed for use. Often in power systems literature,
the NNs are trained with a fixed dataset generated prior to the training
process. In this paper, we show that adapting the NN training dataset during
training can improve the NN performance and substantially reduce its worst-case
violations. This paper proposes an algorithm that identifies and enriches the
training dataset with critical datapoints that reduce the worst-case violations
and deliver a neural network with improved worst-case performance guarantees.
We demonstrate the performance of our algorithm in four test power systems,
ranging from 39-buses to 162-buses.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Don't FREAK Out: A Frequency-Inspired Approach to Detecting Backdoor  Poisoned Samples in DNNs</b></summary>
  <p><b>编号</b>：[132]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13211</p>
  <p><b>作者</b>：Hasan Abed Al Kader Hammoud,  Adel Bibi,  Philip H.S. Torr,  Bernard Ghanem</p>
  <p><b>备注</b>：Accepted at CVPRW (The Art of Robustness)</p>
  <p><b>关键词</b>：Deep Neural, paper we investigate, Neural, sensitivity of Deep, Deep</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper we investigate the frequency sensitivity of Deep Neural
Networks (DNNs) when presented with clean samples versus poisoned samples. Our
analysis shows significant disparities in frequency sensitivity between these
two types of samples. Building on these findings, we propose FREAK, a
frequency-based poisoned sample detection algorithm that is simple yet
effective. Our experimental results demonstrate the efficacy of FREAK not only
against frequency backdoor attacks but also against some spatial attacks. Our
work is just the first step in leveraging these insights. We believe that our
analysis and proposed defense mechanism will provide a foundation for future
research and development of backdoor defenses.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：It is all Connected: A New Graph Formulation for Spatio-Temporal  Forecasting</b></summary>
  <p><b>编号</b>：[144]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13177</p>
  <p><b>作者</b>：Lars Ødegaard Bentsen,  Narada Dilp Warakagoda,  Roy Stenbro,  Paal Engelstad</p>
  <p><b>备注</b>：Pre-print</p>
  <p><b>关键词</b>：make informed decisions, modern society, ever-increasing number, number of sensors, sensors in modern</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With an ever-increasing number of sensors in modern society, spatio-temporal
time series forecasting has become a de facto tool to make informed decisions
about the future. Most spatio-temporal forecasting models typically comprise
distinct components that learn spatial and temporal dependencies. A common
methodology employs some Graph Neural Network (GNN) to capture relations
between spatial locations, while another network, such as a recurrent neural
network (RNN), learns temporal correlations. By representing every recorded
sample as its own node in a graph, rather than all measurements for a
particular location as a single node, temporal and spatial information is
encoded in a similar manner. In this setting, GNNs can now directly learn both
temporal and spatial dependencies, jointly, while also alleviating the need for
additional temporal networks. Furthermore, the framework does not require
aligned measurements along the temporal dimension, meaning that it also
naturally facilitates irregular time series, different sampling frequencies or
missing data, without the need for data imputation. To evaluate the proposed
methodology, we consider wind speed forecasting as a case study, where our
proposed framework outperformed other spatio-temporal models using GNNs with
either Transformer or LSTM networks as temporal update functions.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Take 5: Interpretable Image Classification with a Handful of Features</b></summary>
  <p><b>编号</b>：[150]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13166</p>
  <p><b>作者</b>：Thomas Norrenbrock,  Marco Rudolph,  Bodo Rosenhahn</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Deep Neural Networks, Deep Neural, Neural Networks, decision, human can follow</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep Neural Networks use thousands of mostly incomprehensible features to
identify a single class, a decision no human can follow. We propose an
interpretable sparse and low dimensional final decision layer in a deep neural
network with measurable aspects of interpretability and demonstrate it on
fine-grained image classification. We argue that a human can only understand
the decision of a machine learning model, if the features are interpretable and
only very few of them are used for a single decision. For that matter, the
final layer has to be sparse and, to make interpreting the features feasible,
low dimensional. We call a model with a Sparse Low-Dimensional Decision
SLDD-Model. We show that a SLDD-Model is easier to interpret locally and
globally than a dense high-dimensional decision layer while being able to
maintain competitive accuracy. Additionally, we propose a loss function that
improves a model's feature diversity and accuracy. Our more interpretable
SLDD-Model only uses 5 out of just 50 features per class, while maintaining 97%
to 100% of the accuracy on four common benchmark datasets compared to the
baseline model with 2048 features.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Adiabatic replay for continual learning</b></summary>
  <p><b>编号</b>：[152]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13157</p>
  <p><b>作者</b>：Alexander Krawczyk,  Alexander Gepperth</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：avoid catastrophic forgetting, Conventional replay-based approaches, catastrophic forgetting, approaches to continual, order to avoid</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Conventional replay-based approaches to continual learning (CL) require, for
each learning phase with new data, the replay of samples representing all of
the previously learned knowledge in order to avoid catastrophic forgetting.
Since the amount of learned knowledge grows over time in CL problems,
generative replay spends an increasing amount of time just re-learning what is
already known. In this proof-of-concept study, we propose a replay-based CL
strategy that we term adiabatic replay (AR), which derives its efficiency from
the (reasonable) assumption that each new learning phase is adiabatic, i.e.,
represents only a small addition to existing knowledge. Each new learning phase
triggers a sampling process that selectively replays, from the body of existing
knowledge, just such samples that are similar to the new data, in contrast to
replaying all of it. Complete replay is not required since AR represents the
data distribution by GMMs, which are capable of selectively updating their
internal representation only where data statistics have changed. As long as
additions are adiabatic, the amount of to-be-replayed samples need not to
depend on the amount of previously acquired knowledge at all. We verify
experimentally that AR is superior to state-of-the-art deep generative replay
using VAEs.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：FedGH: Heterogeneous Federated Learning with Generalized Global Header</b></summary>
  <p><b>编号</b>：[157]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13137</p>
  <p><b>作者</b>：Liping Yi,  Gang Wang,  Xiaoguang Liu,  Zhuan Shi,  Han Yu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：emerging machine learning, machine learning paradigm, Global prediction Header, machine learning, learning paradigm</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Federated learning (FL) is an emerging machine learning paradigm that allows
multiple parties to train a shared model collaboratively in a
privacy-preserving manner. Existing horizontal FL methods generally assume that
the FL server and clients hold the same model structure. However, due to system
heterogeneity and the need for personalization, enabling clients to hold models
with diverse structures has become an important direction. Existing
model-heterogeneous FL approaches often require publicly available datasets and
incur high communication and/or computational costs, which limit their
performances. To address these limitations, we propose the Federated Global
prediction Header (FedGH) approach. It is a communication and
computation-efficient model-heterogeneous FL framework which trains a shared
generalized global prediction header with representations extracted by
heterogeneous extractors for clients' models at the FL server. The trained
generalized global prediction header learns from different clients. The
acquired global knowledge is then transferred to clients to substitute each
client's local prediction header. We derive the non-convex convergence rate of
FedGH. Extensive experiments on two real-world datasets demonstrate that FedGH
achieves significantly more advantageous performance in both model-homogeneous
and -heterogeneous FL scenarios compared to seven state-of-the-art personalized
FL models, beating the best-performing baseline by up to 8.87% (for
model-homogeneous FL) and 1.83% (for model-heterogeneous FL) in terms of
average test accuracy, while saving up to 85.53% of communication overhead.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Laplacian Segmentation Networks: Improved Epistemic Uncertainty from  Spatial Aleatoric Uncertainty</b></summary>
  <p><b>编号</b>：[165]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13123</p>
  <p><b>作者</b>：Kilian Zepf,  Selma Wanna,  Marco Miani,  Juston Moore,  Jes Frellsen,  Søren Hauberg,  Aasa Feragen,  Frederik Warburg</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：frequently encountered, scanner differences, Laplacian Segmentation Networks, Laplacian Segmentation, OOD</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Out of distribution (OOD) medical images are frequently encountered, e.g.
because of site- or scanner differences, or image corruption. OOD images come
with a risk of incorrect image segmentation, potentially negatively affecting
downstream diagnoses or treatment. To ensure robustness to such incorrect
segmentations, we propose Laplacian Segmentation Networks (LSN) that jointly
model epistemic (model) and aleatoric (data) uncertainty in image segmentation.
We capture data uncertainty with a spatially correlated logit distribution. For
model uncertainty, we propose the first Laplace approximation of the weight
posterior that scales to large neural networks with skip connections that have
high-dimensional outputs. Empirically, we demonstrate that modelling spatial
pixel correlation allows the Laplacian Segmentation Network to successfully
assign high epistemic uncertainty to out-of-distribution objects appearing
within images.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Adaptive Regularization for Class-Incremental Learning</b></summary>
  <p><b>编号</b>：[168]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13113</p>
  <p><b>作者</b>：Elif Ceren Gok,  Murat Onur Yildirim,  Mert Kilickaya,  Joaquin Vanschoren</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：observed class accuracy, previously observed class, Class-Incremental Learning updates, class accuracy, updates a deep</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Class-Incremental Learning updates a deep classifier with new categories
while maintaining the previously observed class accuracy. Regularizing the
neural network weights is a common method to prevent forgetting previously
learned classes while learning novel ones. However, existing regularizers use a
constant magnitude throughout the learning sessions, which may not reflect the
varying levels of difficulty of the tasks encountered during incremental
learning. This study investigates the necessity of adaptive regularization in
Class-Incremental Learning, which dynamically adjusts the regularization
strength according to the complexity of the task at hand. We propose a Bayesian
Optimization-based approach to automatically determine the optimal
regularization magnitude for each learning task. Our experiments on two
datasets via two regularizers demonstrate the importance of adaptive
regularization for achieving accurate and less forgetful visual incremental
learning.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：Keypoint-Guided Optimal Transport</b></summary>
  <p><b>编号</b>：[170]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13102</p>
  <p><b>作者</b>：Xiang Gu,  Yucheng Yang,  Wei Zeng,  Jian Sun,  Zongben Xu</p>
  <p><b>备注</b>：Under review</p>
  <p><b>关键词</b>：Existing Optimal Transport, transport plan, proposed KPG-RL model, optimal transport plan, Optimal Transport</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing Optimal Transport (OT) methods mainly derive the optimal transport
plan/matching under the criterion of transport cost/distance minimization,
which may cause incorrect matching in some cases. In many applications,
annotating a few matched keypoints across domains is reasonable or even
effortless in annotation burden. It is valuable to investigate how to leverage
the annotated keypoints to guide the correct matching in OT. In this paper, we
propose a novel KeyPoint-Guided model by ReLation preservation (KPG-RL) that
searches for the optimal matching (i.e., transport plan) guided by the
keypoints in OT. To impose the keypoints in OT, first, we propose a mask-based
constraint of the transport plan that preserves the matching of keypoint pairs.
Second, we propose to preserve the relation of each data point to the keypoints
to guide the matching. The proposed KPG-RL model can be solved by Sinkhorn's
algorithm and is applicable even when distributions are supported in different
spaces. We further utilize the relation preservation constraint in the
Kantorovich Problem and Gromov-Wasserstein model to impose the guidance of
keypoints in them. Meanwhile, the proposed KPG-RL model is extended to the
partial OT setting. Moreover, we deduce the dual formulation of the KPG-RL
model, which is solved using deep learning techniques. Based on the learned
transport plan from dual KPG-RL, we propose a novel manifold barycentric
projection to transport source data to the target domain. As applications, we
apply the proposed KPG-RL model to the heterogeneous domain adaptation and
image-to-image translation. Experiments verified the effectiveness of the
proposed approach.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：The Probabilistic Stability of Stochastic Gradient Descent</b></summary>
  <p><b>编号</b>：[176]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13093</p>
  <p><b>作者</b>：Liu Ziyin,  Botao Li,  Tomer Galanti,  Masahito Ueda</p>
  <p><b>备注</b>：preprint</p>
  <p><b>关键词</b>：SGD, stochastic gradient descent, deep learning theory, stability, learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A fundamental open problem in deep learning theory is how to define and
understand the stability of stochastic gradient descent (SGD) close to a fixed
point. Conventional literature relies on the convergence of statistical
moments, esp., the variance, of the parameters to quantify the stability. We
revisit the definition of stability for SGD and use the \textit{convergence in
probability} condition to define the \textit{probabilistic stability} of SGD.
The proposed stability directly answers a fundamental question in deep learning
theory: how SGD selects a meaningful solution for a neural network from an
enormous number of solutions that may overfit badly. To achieve this, we show
that only under the lens of probabilistic stability does SGD exhibit rich and
practically relevant phases of learning, such as the phases of the complete
loss of stability, incorrect learning, convergence to low-rank saddles, and
correct learning. When applied to a neural network, these phase diagrams imply
that SGD prefers low-rank saddles when the underlying gradient is noisy,
thereby improving the learning performance. This result is in sharp contrast to
the conventional wisdom that SGD prefers flatter minima to sharp ones, which we
find insufficient to explain the experimental data. We also prove that the
probabilistic stability of SGD can be quantified by the Lyapunov exponents of
the SGD dynamics, which can easily be measured in practice. Our work
potentially opens a new venue for addressing the fundamental question of how
the learning algorithm affects the learning outcome in deep learning.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Box-Level Active Detection</b></summary>
  <p><b>编号</b>：[180]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13089</p>
  <p><b>作者</b>：Mengyao Lyu,  Jundong Zhou,  Hui Chen,  Yijie Huang,  Dongdong Yu,  Yaqian Li,  Yandong Guo,  Yuchen Guo,  Liuyu Xiang,  Guiguang Ding</p>
  <p><b>备注</b>：CVPR 2023 highlight</p>
  <p><b>关键词</b>：Active learning selects, learning selects informative, selects informative samples, proven efficient recently, learning selects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Active learning selects informative samples for annotation within budget,
which has proven efficient recently on object detection. However, the widely
used active detection benchmarks conduct image-level evaluation, which is
unrealistic in human workload estimation and biased towards crowded images.
Furthermore, existing methods still perform image-level annotation, but equally
scoring all targets within the same image incurs waste of budget and redundant
labels. Having revealed above problems and limitations, we introduce a
box-level active detection framework that controls a box-based budget per
cycle, prioritizes informative targets and avoids redundancy for fair
comparison and efficient application.
Under the proposed box-level setting, we devise a novel pipeline, namely
Complementary Pseudo Active Strategy (ComPAS). It exploits both human
annotations and the model intelligence in a complementary fashion: an efficient
input-end committee queries labels for informative objects only; meantime
well-learned targets are identified by the model and compensated with
pseudo-labels. ComPAS consistently outperforms 10 competitors under 4 settings
in a unified codebase. With supervision from labeled data only, it achieves
100% supervised performance of VOC0712 with merely 19% box annotations. On the
COCO dataset, it yields up to 4.3% mAP improvement over the second-best method.
ComPAS also supports training with the unlabeled pool, where it surpasses 90%
COCO supervised performance with 85% label reduction. Our source code is
publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：MSAT: Biologically Inspired Multi-Stage Adaptive Threshold for  Conversion of Spiking Neural Networks</b></summary>
  <p><b>编号</b>：[182]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13080</p>
  <p><b>作者</b>：Xiang He,  Yang Li,  Dongcheng Zhao,  Qingqun Kong,  Yi Zeng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Spiking Neural Networks, Artificial Neural Networks, Neural Networks, low power consumption, power consumption due</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Spiking Neural Networks (SNNs) can do inference with low power consumption
due to their spike sparsity. ANN-SNN conversion is an efficient way to achieve
deep SNNs by converting well-trained Artificial Neural Networks (ANNs).
However, the existing methods commonly use constant threshold for conversion,
which prevents neurons from rapidly delivering spikes to deeper layers and
causes high time delay. In addition, the same response for different inputs may
result in information loss during the information transmission. Inspired by the
biological model mechanism, we propose a multi-stage adaptive threshold (MSAT).
Specifically, for each neuron, the dynamic threshold varies with firing history
and input properties and is positively correlated with the average membrane
potential and negatively correlated with the rate of depolarization. The
self-adaptation to membrane potential and input allows a timely adjustment of
the threshold to fire spike faster and transmit more information. Moreover, we
analyze the Spikes of Inactivated Neurons error which is pervasive in early
time steps and propose spike confidence accordingly as a measurement of
confidence about the neurons that correctly deliver spikes. We use such spike
confidence in early time steps to determine whether to elicit spike to
alleviate this error. Combined with the proposed method, we examine the
performance on non-trivial datasets CIFAR-10, CIFAR-100, and ImageNet. We also
conduct sentiment classification and speech recognition experiments on the IDBM
and Google speech commands datasets respectively. Experiments show
near-lossless and lower latency ANN-SNN conversion. To the best of our
knowledge, this is the first time to build a biologically inspired multi-stage
adaptive threshold for converted SNN, with comparable performance to
state-of-the-art methods while improving energy efficiency.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：Reimagining Application User Interface (UI) Design using Deep Learning  Methods: Challenges and Opportunities</b></summary>
  <p><b>编号</b>：[197]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13055</p>
  <p><b>作者</b>：Subtain Malik,  Muhammad Tariq Saeed,  Marya Jabeen Zia,  Shahzad Rasool,  Liaquat Ali Khan,  Mian Ilyas Ahmed</p>
  <p><b>备注</b>：A review paper on studies of UI design techniques and deep learning</p>
  <p><b>关键词</b>：user interface design, user interface, deep learning methods, neural networks, design user interface</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we present a review of the recent work in deep learning
methods for user interface design. The survey encompasses well known deep
learning techniques (deep neural networks, convolutional neural networks,
recurrent neural networks, autoencoders, and generative adversarial networks)
and datasets widely used to design user interface applications. We highlight
important problems and emerging research frontiers in this field. We believe
that the use of deep learning for user interface design automation tasks could
be one of the high potential fields for the advancement of the software
development industry.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Towards Better Dynamic Graph Learning: New Architecture and Unified  Library</b></summary>
  <p><b>编号</b>：[202]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13047</p>
  <p><b>作者</b>：Le Yu,  Leilei Sun,  Bowen Du,  Weifeng Lv</p>
  <p><b>备注</b>：24 pages</p>
  <p><b>关键词</b>：historical first-hop interactions, dynamic graph learning, nodes' historical first-hop, Transformer-based architecture, first-hop interactions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose DyGFormer, a new Transformer-based architecture for dynamic graph
learning that solely learns from the sequences of nodes' historical first-hop
interactions. DyGFormer incorporates two distinct designs: a neighbor
co-occurrence encoding scheme that explores the correlations of the source node
and destination node based on their sequences; a patching technique that
divides each sequence into multiple patches and feeds them to Transformer,
allowing the model to effectively and efficiently benefit from longer
histories. We also introduce DyGLib, a unified library with standard training
pipelines, extensible coding interfaces, and comprehensive evaluating protocols
to promote reproducible, scalable, and credible dynamic graph learning
research. By performing extensive experiments on thirteen datasets from various
domains for transductive/inductive dynamic link prediction and dynamic node
classification tasks, we observe that: DyGFormer achieves state-of-the-art
performance on most of the datasets, demonstrating the effectiveness of
capturing nodes' correlations and long-term temporal dependencies; the results
of baselines vary across different datasets and some findings are inconsistent
with previous reports, which may be caused by their diverse pipelines and
problematic implementations. We hope our work can provide new insights and
facilitate the development of the dynamic graph learning field. All the
resources including datasets, data loaders, algorithms, and executing scripts
are publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：SPeC: A Soft Prompt-Based Calibration on Mitigating Performance  Variability in Clinical Notes Summarization</b></summary>
  <p><b>编号</b>：[208]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13035</p>
  <p><b>作者</b>：Yu-Neng Chuang,  Ruixiang Tang,  Xiaoqian Jiang,  Xia Hu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：encompassing medical histories, Electronic health records, store an extensive, extensive array, Electronic health</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Electronic health records (EHRs) store an extensive array of patient
information, encompassing medical histories, diagnoses, treatments, and test
outcomes. These records are crucial for enabling healthcare providers to make
well-informed decisions regarding patient care. Summarizing clinical notes
further assists healthcare professionals in pinpointing potential health risks
and making better-informed decisions. This process contributes to reducing
errors and enhancing patient outcomes by ensuring providers have access to the
most pertinent and current patient data. Recent research has shown that
incorporating prompts with large language models (LLMs) substantially boosts
the efficacy of summarization tasks. However, we show that this approach also
leads to increased output variance, resulting in notably divergent outputs even
when prompts share similar meanings. To tackle this challenge, we introduce a
model-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft
prompts to diminish variance while preserving the advantages of prompt-based
summarization. Experimental findings on multiple clinical note tasks and LLMs
indicate that our method not only bolsters performance but also effectively
curbs variance for various LLMs, providing a more uniform and dependable
solution for summarizing vital medical information.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Preference-Aware Constrained Multi-Objective Bayesian Optimization</b></summary>
  <p><b>编号</b>：[209]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13034</p>
  <p><b>作者</b>：Alaleh Ahmadianshalchi,  Syrine Belakaria,  Janardhan Rao Doppa</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2110.06980</p>
  <p><b>关键词</b>：black-box objective functions, feasible input designs, paper addresses, functions with practitioner-specified, engineering design problems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper addresses the problem of constrained multi-objective optimization
over black-box objective functions with practitioner-specified preferences over
the objectives when a large fraction of the input space is infeasible (i.e.,
violates constraints). This problem arises in many engineering design problems
including analog circuits and electric power system design. Our overall goal is
to approximate the optimal Pareto set over the small fraction of feasible input
designs. The key challenges include the huge size of the design space, multiple
objectives and large number of constraints, and the small fraction of feasible
input designs which can be identified only after performing expensive
simulations. We propose a novel and efficient preference-aware constrained
multi-objective Bayesian optimization approach referred to as PAC-MOO to
address these challenges. The key idea is to learn surrogate models for both
output objectives and constraints, and select the candidate input for
evaluation in each iteration that maximizes the information gained about the
optimal constrained Pareto front while factoring in the preferences over
objectives. Our experiments on two real-world analog circuit design
optimization problems demonstrate the efficacy of PAC-MOO over prior methods.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Self-Supervised Clustering of Multivariate Time-Series Data for  Identifying TBI Physiological States</b></summary>
  <p><b>编号</b>：[214]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13024</p>
  <p><b>作者</b>：Hamid Ghaderi,  Brandon Foreman,  Amin Nayebi,  Sindhu Tipirneni,  Chandan K. Reddy,  Vignesh Subbian</p>
  <p><b>备注</b>：10 pages, 7 figures, 2 tables</p>
  <p><b>关键词</b>：Traumatic Brain Injury, Brain Injury, Traumatic Brain, Determining clinically relevant, multivariate time series</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Determining clinically relevant physiological states from multivariate time
series data with missing values is essential for providing appropriate
treatment for acute conditions such as Traumatic Brain Injury (TBI),
respiratory failure, and heart failure. Utilizing non-temporal clustering or
data imputation and aggregation techniques may lead to loss of valuable
information and biased analyses. In our study, we apply the SLAC-Time
algorithm, an innovative self-supervision-based approach that maintains data
integrity by avoiding imputation or aggregation, offering a more useful
representation of acute patient states. By using SLAC-Time to cluster data in a
large research dataset, we identified three distinct TBI physiological states
and their specific feature profiles. We employed various clustering evaluation
metrics and incorporated input from a clinical domain expert to validate and
interpret the identified physiological states. Further, we discovered how
specific clinical events and interventions can influence patient states and
state transitions.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：ENVIDR: Implicit Differentiable Renderer with Neural Environment  Lighting</b></summary>
  <p><b>编号</b>：[215]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13022</p>
  <p><b>作者</b>：Ruofan Liang,  Huiting Chen,  Chunlin Li,  Fan Chen,  Selvakumar Panneer,  Nandita Vijaykumar</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：shown great potential, Recent advances, multiview images, shown great, great potential</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advances in neural rendering have shown great potential for
reconstructing scenes from multiview images. However, accurately representing
objects with glossy surfaces remains a challenge for existing methods. In this
work, we introduce ENVIDR, a rendering and modeling framework for high-quality
rendering and reconstruction of surfaces with challenging specular reflections.
To achieve this, we first propose a novel neural renderer with decomposed
rendering components to learn the interaction between surface and environment
lighting. This renderer is trained using existing physically based renderers
and is decoupled from actual scene representations. We then propose an
SDF-based neural surface model that leverages this learned neural renderer to
represent general scenes. Our model additionally synthesizes indirect
illuminations caused by inter-reflections from shiny surfaces by marching
surface-reflected rays. We demonstrate that our method outperforms state-of-art
methods on challenging shiny scenes, providing high-quality rendering of
specular reflections while also enabling material editing and scene relighting.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Failure-tolerant Distributed Learning for Anomaly Detection in Wireless  Networks</b></summary>
  <p><b>编号</b>：[219]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13015</p>
  <p><b>作者</b>：Marc Katzef,  Andrew C. Cullen,  Tansu Alpcan,  Christopher Leckie,  Justin Kopacz</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：lack thereof, anomaly detection, cripple distributed systems, distributed, potentially cripple distributed</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The analysis of distributed techniques is often focused upon their
efficiency, without considering their robustness (or lack thereof). Such a
consideration is particularly important when devices or central servers can
fail, which can potentially cripple distributed systems. When such failures
arise in wireless communications networks, important services that they
use/provide (like anomaly detection) can be left inoperable and can result in a
cascade of security problems. In this paper, we present a novel method to
address these risks by combining both flat- and star-topologies, combining the
performance and reliability benefits of both. We refer to this method as
"Tol-FL", due to its increased failure-tolerance as compared to the technique
of Federated Learning. Our approach both limits device failure risks while
outperforming prior methods by up to 8% in terms of anomaly detection AUROC in
a range of realistic settings that consider client as well as server failure,
all while reducing communication costs. This performance demonstrates that
Tol-FL is a highly suitable method for distributed model training for anomaly
detection, especially in the domain of wireless networks.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Semantic Image Attack for Visual Model Diagnosis</b></summary>
  <p><b>编号</b>：[222]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13010</p>
  <p><b>作者</b>：Jinqi Luo,  Zhaoning Wang,  Chen Henry Wu,  Dong Huang,  Fernando De la Torre</p>
  <p><b>备注</b>：Initial version submitted to NeurIPS 2022</p>
  <p><b>关键词</b>：specific train, guarantee reliable, reliable or fair, SIA, model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In practice, metric analysis on a specific train and test dataset does not
guarantee reliable or fair ML models. This is partially due to the fact that
obtaining a balanced, diverse, and perfectly labeled dataset is typically
expensive, time-consuming, and error-prone. Rather than relying on a carefully
designed test set to assess ML models' failures, fairness, or robustness, this
paper proposes Semantic Image Attack (SIA), a method based on the adversarial
attack that provides semantic adversarial images to allow model diagnosis,
interpretability, and robustness. Traditional adversarial training is a popular
methodology for robustifying ML models against attacks. However, existing
adversarial methods do not combine the two aspects that enable the
interpretation and analysis of the model's flaws: semantic traceability and
perceptual quality. SIA combines the two features via iterative gradient ascent
on a predefined semantic attribute space and the image space. We illustrate the
validity of our approach in three scenarios for keypoint detection and
classification. (1) Model diagnosis: SIA generates a histogram of attributes
that highlights the semantic vulnerability of the ML model (i.e., attributes
that make the model fail). (2) Stronger attacks: SIA generates adversarial
examples with visually interpretable attributes that lead to higher attack
success rates than baseline methods. The adversarial training on SIA improves
the transferable robustness across different gradient-based attacks. (3)
Robustness to imbalanced datasets: we use SIA to augment the underrepresented
classes, which outperforms strong augmentation and re-balancing baselines.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Controllable Inversion of Black-Box Face-Recognition Models via  Diffusion</b></summary>
  <p><b>编号</b>：[224]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13006</p>
  <p><b>作者</b>：Manuel Kansy,  Anton Raël,  Graziana Mignone,  Jacek Naruniec,  Christopher Schroers,  Markus Gross,  Romann M. Weber</p>
  <p><b>备注</b>：34 pages. Preprint. Under review</p>
  <p><b>关键词</b>：Face recognition models, recognition models embed, Face recognition, identity-specific facial features, low-dimensional identity vector</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Face recognition models embed a face image into a low-dimensional identity
vector containing abstract encodings of identity-specific facial features that
allow individuals to be distinguished from one another. We tackle the
challenging task of inverting the latent space of pre-trained face recognition
models without full model access (i.e. black-box setting). A variety of methods
have been proposed in literature for this task, but they have serious
shortcomings such as a lack of realistic outputs, long inference times, and
strong requirements for the data set and accessibility of the face recognition
model. Through an analysis of the black-box inversion problem, we show that the
conditional diffusion model loss naturally emerges and that we can effectively
sample from the inverse distribution even without an identity-specific loss.
Our method, named identity denoising diffusion probabilistic model (ID3PM),
leverages the stochastic nature of the denoising diffusion process to produce
high-quality, identity-preserving face images with various backgrounds,
lighting, poses, and expressions. We demonstrate state-of-the-art performance
in terms of identity preservation and diversity both qualitatively and
quantitatively. Our method is the first black-box face recognition model
inversion method that offers intuitive control over the generation process and
does not suffer from any of the common shortcomings from competing methods.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Adversarially Contrastive Estimation of Conditional Neural Processes</b></summary>
  <p><b>编号</b>：[226]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13004</p>
  <p><b>作者</b>：Zesheng Ye,  Jing Du,  Lina Yao</p>
  <p><b>备注</b>：29 pages</p>
  <p><b>关键词</b>：Conditional Neural Processes, Neural Processes, Conditional Neural, exact conditional likelihoods, exact conditional</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Conditional Neural Processes~(CNPs) formulate distributions over functions
and generate function observations with exact conditional likelihoods. CNPs,
however, have limited expressivity for high-dimensional observations, since
their predictive distribution is factorized into a product of unconstrained
(typically) Gaussian outputs. Previously, this could be handled using latent
variables or autoregressive likelihood, but at the expense of intractable
training and quadratically increased complexity. Instead, we propose
calibrating CNPs with an adversarial training scheme besides regular maximum
likelihood estimates. Specifically, we train an energy-based model (EBM) with
noise contrastive estimation, which enforces EBM to identify true observations
from the generations of CNP. In this way, CNP must generate predictions closer
to the ground-truth to fool EBM, instead of merely optimizing with respect to
the fixed-form likelihood. From generative function reconstruction to
downstream regression and classification tasks, we demonstrate that our method
fits mainstream CNP members, showing effectiveness when unconstrained Gaussian
likelihood is defined, requiring minimal computation overhead while preserving
foundation properties of CNPs.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Benchmarking the Reliability of Post-training Quantization: a Particular  Focus on Worst-case Performance</b></summary>
  <p><b>编号</b>：[227]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13003</p>
  <p><b>作者</b>：Zhihang Yuan,  Jiawei Liu,  Jiaxiang Wu,  Dawei Yang,  Qiang Wu,  Guangyu Sun,  Wenyu Liu,  Xinggang Wang,  Bingzhe Wu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：deep neural networks, compressing deep neural, Post-training quantization, PTQ methods, PTQ</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Post-training quantization (PTQ) is a popular method for compressing deep
neural networks (DNNs) without modifying their original architecture or
training procedures. Despite its effectiveness and convenience, the reliability
of PTQ methods in the presence of some extrem cases such as distribution shift
and data noise remains largely unexplored. This paper first investigates this
problem on various commonly-used PTQ methods. We aim to answer several research
questions related to the influence of calibration set distribution variations,
calibration paradigm selection, and data augmentation or sampling strategies on
PTQ reliability. A systematic evaluation process is conducted across a wide
range of tasks and commonly-used PTQ paradigms. The results show that most
existing PTQ methods are not reliable enough in term of the worst-case group
performance, highlighting the need for more robust methods. Our findings
provide insights for developing PTQ methods that can effectively handle
distribution shift scenarios and enable the deployment of quantized DNNs in
real-world applications.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Planning Goals for Exploration</b></summary>
  <p><b>编号</b>：[228]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13002</p>
  <p><b>作者</b>：Edward S. Hu,  Richard Chang,  Oleh Rybkin,  Dinesh Jayaraman</p>
  <p><b>备注</b>：Camera Ready version for ICLR2023 Spotlight</p>
  <p><b>关键词</b>：accomplish diverse tasks, accomplish diverse, diverse tasks, PEG, Planning Exploratory Goals</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Dropped into an unknown environment, what should an agent do to quickly learn
about the environment and how to accomplish diverse tasks within it? We address
this question within the goal-conditioned reinforcement learning paradigm, by
identifying how the agent should set its goals at training time to maximize
exploration. We propose "Planning Exploratory Goals" (PEG), a method that sets
goals for each training episode to directly optimize an intrinsic exploration
reward. PEG first chooses goal commands such that the agent's goal-conditioned
policy, at its current level of training, will end up in states with high
exploration potential. It then launches an exploration policy starting at those
promising states. To enable this direct optimization, PEG learns world models
and adapts sampling-based planning algorithms to "plan goal commands". In
challenging simulated robotics environments including a multi-legged ant robot
in a maze, and a robot arm on a cluttered tabletop, PEG exploration enables
more efficient and effective training of goal-conditioned policies relative to
baselines and ablations. Our ant successfully navigates a long maze, and the
robot arm successfully builds a stack of three blocks upon command. Website:
this https URL</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Automated Federated Learning in Mobile Edge Networks -- Fast Adaptation  and Convergence</b></summary>
  <p><b>编号</b>：[231]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12999</p>
  <p><b>作者</b>：Chaoqun You,  Kun Guo,  Gang Feng,  Peng Yang,  Tony Q. S. Quek</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：mobile edge networks, train machine learning, distributed manner, mobile edge, edge networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Federated Learning (FL) can be used in mobile edge networks to train machine
learning models in a distributed manner. Recently, FL has been interpreted
within a Model-Agnostic Meta-Learning (MAML) framework, which brings FL
significant advantages in fast adaptation and convergence over heterogeneous
datasets. However, existing research simply combines MAML and FL without
explicitly addressing how much benefit MAML brings to FL and how to maximize
such benefit over mobile edge networks. In this paper, we quantify the benefit
from two aspects: optimizing FL hyperparameters (i.e., sampled data size and
the number of communication rounds) and resource allocation (i.e., transmit
power) in mobile edge networks. Specifically, we formulate the MAML-based FL
design as an overall learning time minimization problem, under the constraints
of model accuracy and energy consumption. Facilitated by the convergence
analysis of MAML-based FL, we decompose the formulated problem and then solve
it using analytical solutions and the coordinate descent method. With the
obtained FL hyperparameters and resource allocation, we design a MAML-based FL
algorithm, called Automated Federated Learning (AutoFL), that is able to
conduct fast adaptation and convergence. Extensive experimental results verify
that AutoFL outperforms other benchmark algorithms regarding the learning time
and convergence performance.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：A Survey of Historical Learning: Learning Models with Learning History</b></summary>
  <p><b>编号</b>：[236]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12992</p>
  <p><b>作者</b>：Xiang Li,  Ge Wu,  Lingfeng Yang,  Wenhai Wang,  Renjie Song,  Jian Yang</p>
  <p><b>备注</b>：Xiang Li and Ge Wu have equal contributions</p>
  <p><b>关键词</b>：learning, Historical Learning, Learning History, Historical, learning deep models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>New knowledge originates from the old. The various types of elements,
deposited in the training history, are a large amount of wealth for improving
learning deep models. In this survey, we comprehensively review and summarize
the topic--``Historical Learning: Learning Models with Learning History'',
which learns better neural models with the help of their learning history
during its optimization, from three detailed aspects: Historical Type (what),
Functional Part (where) and Storage Form (how). To our best knowledge, it is
the first survey that systematically studies the methodologies which make use
of various historical statistics when training deep neural networks. The
discussions with related topics like recurrent/memory networks, ensemble
learning, and reinforcement learning are demonstrated. We also expose future
challenges of this topic and encourage the community to pay attention to the
think of historical learning principles when designing algorithms. The paper
list related to historical learning is available at
\url{this https URL.}</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Fault Prognosis of Turbofan Engines: Eventual Failure Prediction and  Remaining Useful Life Estimation</b></summary>
  <p><b>编号</b>：[240]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12982</p>
  <p><b>作者</b>：Joseph Cohen,  Xun Huan,  Jun Ni</p>
  <p><b>备注</b>：Preprint with 10 pages, 5 figures. Submitted to International Journal of Prognostics and Health Management (IJPHM)</p>
  <p><b>关键词</b>：industrial big data, PHM Data Challenge, minimize inventory, era of industrial, industrial big</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the era of industrial big data, prognostics and health management is
essential to improve the prediction of future failures to minimize inventory,
maintenance, and human costs. Used for the 2021 PHM Data Challenge, the new
Commercial Modular Aero-Propulsion System Simulation dataset from NASA is an
open-source benchmark containing simulated turbofan engine units flown under
realistic flight conditions. Deep learning approaches implemented previously
for this application attempt to predict the remaining useful life of the engine
units, but have not utilized labeled failure mode information, impeding
practical usage and explainability. To address these limitations, a new
prognostics approach is formulated with a customized loss function to
simultaneously predict the current health state, the eventual failing
component(s), and the remaining useful life. The proposed method incorporates
principal component analysis to orthogonalize statistical time-domain features,
which are inputs into supervised regressors such as random forests, extreme
random forests, XGBoost, and artificial neural networks. The highest performing
algorithm, ANN-Flux, achieves AUROC and AUPR scores exceeding 0.95 for each
classification. In addition, ANN-Flux reduces the remaining useful life RMSE by
38% for the same test split of the dataset compared to past work, with
significantly less computational cost.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Connected Superlevel Set in (Deep) Reinforcement Learning and its  Application to Minimax Theorems</b></summary>
  <p><b>编号</b>：[241]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12981</p>
  <p><b>作者</b>：Sihan Zeng,  Thinh T. Doan,  Justin Romberg</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：improve the understanding, policy optimization problems, policy parameter, robust reinforcement learning, reinforcement learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The aim of this paper is to improve the understanding of the optimization
landscape for policy optimization problems in reinforcement learning.
Specifically, we show that the superlevel set of the objective function with
respect to the policy parameter is always a connected set both in the tabular
setting and under policies represented by a class of neural networks. In
addition, we show that the optimization objective as a function of the policy
parameter and reward satisfies a stronger "equiconnectedness" property. To our
best knowledge, these are novel and previously unknown discoveries.
We present an application of the connectedness of these superlevel sets to
the derivation of minimax theorems for robust reinforcement learning. We show
that any minimax optimization program which is convex on one side and is
equiconnected on the other side observes the minimax equality (i.e. has a Nash
equilibrium). We find that this exact structure is exhibited by an interesting
robust reinforcement learning problem under an adversarial reward attack, and
the validity of its minimax equality immediately follows. This is the first
time such a result is established in the literature.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Continuous Indeterminate Probability Neural Network</b></summary>
  <p><b>编号</b>：[248]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12964</p>
  <p><b>作者</b>：Tao Yang</p>
  <p><b>备注</b>：8 pages</p>
  <p><b>关键词</b>：Continuous Indeterminate, paper introduces, latent random variables, Continuous, latent</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper introduces a general model called CIPNN - Continuous Indeterminate
Probability Neural Network, and this model is based on IPNN, which is used for
discrete latent random variables. Currently, posterior of continuous latent
variables is regarded as intractable, with the new theory proposed by IPNN this
problem can be solved. Our contributions are Four-fold. First, we derive the
analytical solution of the posterior calculation of continuous latent random
variables and propose a general classification model (CIPNN). Second, we
propose a general auto-encoder called CIPAE - Continuous Indeterminate
Probability Auto-Encoder, the decoder part is not a neural network and uses a
fully probabilistic inference model for the first time. Third, we propose a new
method to visualize the latent random variables, we use one of N dimensional
latent variables as a decoder to reconstruct the input image, which can work
even for classification tasks, in this way, we can see what each latent
variable has learned. Fourth, IPNN has shown great classification capability,
CIPNN has pushed this classification capability to infinity. Theoretical
advantages are reflected in experimental results.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Forecast-Aware Model Driven LSTM</b></summary>
  <p><b>编号</b>：[249]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12963</p>
  <p><b>作者</b>：Sophia Hamer,  Jennifer Sleeman,  Ivanka Stajner</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：air quality events, extreme air quality, air quality, air quality forecasting, Poor air quality</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Poor air quality can have a significant impact on human health. The National
Oceanic and Atmospheric Administration (NOAA) air quality forecasting guidance
is challenged by the increasing presence of extreme air quality events due to
extreme weather events such as wild fires and heatwaves. These extreme air
quality events further affect human health. Traditional methods used to correct
model bias make assumptions about linearity and the underlying distribution.
Extreme air quality events tend to occur without a strong signal leading up to
the event and this behavior tends to cause existing methods to either under or
over compensate for the bias. Deep learning holds promise for air quality
forecasting in the presence of extreme air quality events due to its ability to
generalize and learn nonlinear problems. However, in the presence of these
anomalous air quality events, standard deep network approaches that use a
single network for generalizing to future forecasts, may not always provide the
best performance even with a full feature-set including geography and
meteorology. In this work we describe a method that combines unsupervised
learning and a forecast-aware bi-directional LSTM network to perform bias
correction for operational air quality forecasting using AirNow station data
for ozone and PM2.5 in the continental US. Using an unsupervised clustering
method trained on station geographical features such as latitude and longitude,
urbanization, and elevation, the learned clusters direct training by
partitioning the training data for the LSTM networks. LSTMs are forecast-aware
and implemented using a unique way to perform learning forward and backwards in
time across forecasting days. When comparing the RMSE of the forecast model to
the RMSE of the bias corrected model, the bias corrected model shows
significant improvement (27\% lower RMSE for ozone) over the base forecast.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：The Shaky Foundations of Clinical Foundation Models: A Survey of Large  Language Models and Foundation Models for EMRs</b></summary>
  <p><b>编号</b>：[250]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12961</p>
  <p><b>作者</b>：Michael Wornow,  Yizhe Xu,  Rahul Thapa,  Birju Patel,  Ethan Steinberg,  Scott Fleming,  Michael A. Pfeffer,  Jason Fries,  Nigam H. Shah</p>
  <p><b>备注</b>：16 pages, 4 figures, submitted to NPJ Digital Medicine</p>
  <p><b>关键词</b>：electronic medical records, spurred significant interest, improve patient care, building similar models, medical records</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The successes of foundation models such as ChatGPT and AlphaFold have spurred
significant interest in building similar models for electronic medical records
(EMRs) to improve patient care and hospital operations. However, recent hype
has obscured critical gaps in our understanding of these models' capabilities.
We review over 80 foundation models trained on non-imaging EMR data (i.e.
clinical text and/or structured data) and create a taxonomy delineating their
architectures, training data, and potential use cases. We find that most models
are trained on small, narrowly-scoped clinical datasets (e.g. MIMIC-III) or
broad, public biomedical corpora (e.g. PubMed) and are evaluated on tasks that
do not provide meaningful insights on their usefulness to health systems. In
light of these findings, we propose an improved evaluation framework for
measuring the benefits of clinical foundation models that is more closely
grounded to metrics that matter in healthcare.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：Variantional autoencoder with decremental information bottleneck for  disentanglement</b></summary>
  <p><b>编号</b>：[251]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12959</p>
  <p><b>作者</b>：Jiantao Wu,  Shentong Mo,  Muhammad Awais,  Sara Atito,  Xingshen Zhang,  Lin Wang,  Xiang Yang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：disentanglement, major challenge, Information Bottleneck, Information, latent</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>One major challenge of disentanglement learning with variational autoencoders
is the trade-off between disentanglement and reconstruction fidelity. Previous
incremental methods with only on latent space cannot optimize these two targets
simultaneously, so they expand the Information Bottleneck while training to
{optimize from disentanglement to reconstruction. However, a large bottleneck
will lose the constraint of disentanglement, causing the information diffusion
problem. To tackle this issue, we present a novel decremental variational
autoencoder with disentanglement-invariant transformations to optimize multiple
objectives in different layers, termed DeVAE, for balancing disentanglement and
reconstruction fidelity by decreasing the information bottleneck of diverse
latent spaces gradually. Benefiting from the multiple latent spaces, DeVAE
allows simultaneous optimization of multiple objectives to optimize
reconstruction while keeping the constraint of disentanglement, avoiding
information diffusion. DeVAE is also compatible with large models with
high-dimension latent space. Experimental results on dSprites and Shapes3D that
DeVAE achieves \fix{R2q6}{a good balance between disentanglement and
reconstruction.DeVAE shows high tolerant of hyperparameters and on
high-dimensional latent spaces.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Reinforcement Learning with Exogenous States and Rewards</b></summary>
  <p><b>编号</b>：[252]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12957</p>
  <p><b>作者</b>：George Trimponias,  Thomas G. Dietterich</p>
  <p><b>备注</b>：Greatly extends the initial work reported in 1806.01584</p>
  <p><b>关键词</b>：Markov Decision Process, Markov Reward Process, injecting uncontrolled variation, endogenous Markov Decision, Exogenous state variables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Exogenous state variables and rewards can slow reinforcement learning by
injecting uncontrolled variation into the reward signal. This paper formalizes
exogenous state variables and rewards and shows that if the reward function
decomposes additively into endogenous and exogenous components, the MDP can be
decomposed into an exogenous Markov Reward Process (based on the exogenous
reward) and an endogenous Markov Decision Process (optimizing the endogenous
reward). Any optimal policy for the endogenous MDP is also an optimal policy
for the original MDP, but because the endogenous reward typically has reduced
variance, the endogenous MDP is easier to solve. We study settings where the
decomposition of the state space into exogenous and endogenous state spaces is
not given but must be discovered. The paper introduces and proves correctness
of algorithms for discovering the exogenous and endogenous subspaces of the
state space when they are mixed through linear combination. These algorithms
can be applied during reinforcement learning to discover the exogenous space,
remove the exogenous reward, and focus reinforcement learning on the endogenous
MDP. Experiments on a variety of challenging synthetic MDPs show that these
methods, applied online, discover large exogenous state spaces and produce
substantial speedups in reinforcement learning.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：TSI-GAN: Unsupervised Time Series Anomaly Detection using Convolutional  Cycle-Consistent Generative Adversarial Networks</b></summary>
  <p><b>编号</b>：[253]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12952</p>
  <p><b>作者</b>：Shyam Sundar Saravanan,  Tie Luo,  Mao Van Ngo</p>
  <p><b>备注</b>：To appear in the Proceedings of PAKDD 2023 (27th Pacific-Asia Conference on Knowledge Discovery and Data Mining)</p>
  <p><b>关键词</b>：credit card frauds, autonomous driving, medical diagnosis, complex temporal patterns, credit card</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Anomaly detection is widely used in network intrusion detection, autonomous
driving, medical diagnosis, credit card frauds, etc. However, several key
challenges remain open, such as lack of ground truth labels, presence of
complex temporal patterns, and generalizing over different datasets. This paper
proposes TSI-GAN, an unsupervised anomaly detection model for time-series that
can learn complex temporal patterns automatically and generalize well, i.e., no
need for choosing dataset-specific parameters, making statistical assumptions
about underlying data, or changing model architectures. To achieve these goals,
we convert each input time-series into a sequence of 2D images using two
encoding techniques with the intent of capturing temporal patterns and various
types of deviance. Moreover, we design a reconstructive GAN that uses
convolutional layers in an encoder-decoder network and employs
cycle-consistency loss during training to ensure that inverse mappings are
accurate as well. In addition, we also instrument a Hodrick-Prescott filter in
post-processing to mitigate false positives. We evaluate TSI-GAN using 250
well-curated and harder-than-usual datasets and compare with 8 state-of-the-art
baseline methods. The results demonstrate the superiority of TSI-GAN to all the
baselines, offering an overall performance improvement of 13% and 31% over the
second-best performer MERLIN and the third-best performer LSTM-AE,
respectively.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：FTSO: Effective NAS via First Topology Second Operator</b></summary>
  <p><b>编号</b>：[256]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12948</p>
  <p><b>作者</b>：Likang Wang,  Lei Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Existing one-shot neural, huge computational cost, one-shot neural architecture, neural architecture search, Existing one-shot</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing one-shot neural architecture search (NAS) methods have to conduct a
search over a giant super-net, which leads to the huge computational cost. To
reduce such cost, in this paper, we propose a method, called FTSO, to divide
the whole architecture search into two sub-steps. Specifically, in the first
step, we only search for the topology, and in the second step, we search for
the operators. FTSO not only reduces NAS's search time from days to 0.68
seconds, but also significantly improves the found architecture's accuracy. Our
extensive experiments on ImageNet show that within 18 seconds, FTSO can achieve
a 76.4% testing accuracy, 1.5% higher than the SOTA, PC-DARTS. In addition,
FTSO can reach a 97.77% testing accuracy, 0.27% higher than the SOTA, with
nearly 100% (99.8%) search time saved, when searching on CIFAR10.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：Deep Attention Recognition for Attack Identification in 5G UAV  scenarios: Novel Architecture and End-to-End Evaluation</b></summary>
  <p><b>编号</b>：[257]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12947</p>
  <p><b>作者</b>：Joseanne Viana,  Hamed Farkhari,  Pedro Sebastiao,  Luis Miguel Campos,  Katerina Koutlia,  Biljana Bojovic,  Sandra Lagen,  Rui Dinis</p>
  <p><b>备注</b>：17 pages, 11 figures</p>
  <p><b>关键词</b>：decrease UAV control, unmanned aerial vehicle, robust security features, security features inherent, UAV control communication</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite the robust security features inherent in the 5G framework, attackers
will still discover ways to disrupt 5G unmanned aerial vehicle (UAV) operations
and decrease UAV control communication performance in Air-to-Ground (A2G)
links. Operating under the assumption that the 5G UAV communications
infrastructure will never be entirely secure, we propose Deep Attention
Recognition (DAtR) as a solution to identify attacks based on a small deep
network embedded in authenticated UAVs. Our proposed solution uses two
observable parameters: the Signal-to-Interference-plus-Noise Ratio (SINR) and
the Reference Signal Received Power (RSSI) to recognize attacks under
Line-of-Sight (LoS), Non-Line-of-Sight (NLoS), and a probabilistic combination
of the two conditions. In the tested scenarios, a number of attackers are
located in random positions, while their power is varied in each simulation.
Moreover, terrestrial users are included in the network to impose additional
complexity on attack detection. To improve the systems overall performance in
the attack scenarios, we propose complementing the deep network decision with
two mechanisms based on data manipulation and majority voting techniques. We
compare several performance parameters in our proposed Deep Network. For
example, the impact of Long Short-Term-Memory (LSTM) and Attention layers in
terms of their overall accuracy, the window size effect, and test the accuracy
when only partial data is available in the training process. Finally, we
benchmark our deep network with six widely used classifiers regarding
classification accuracy. Our algorithms accuracy exceeds 4% compared with the
eXtreme Gradient Boosting (XGB) classifier in LoS condition and around 3% in
the short distance NLoS condition. Considering the proposed deep network, all
other classifiers present lower accuracy than XGB.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：Leveraging Multi-time Hamilton-Jacobi PDEs for Certain Scientific  Machine Learning Problems</b></summary>
  <p><b>编号</b>：[267]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12928</p>
  <p><b>作者</b>：Paula Chen,  Tingwei Meng,  Zongren Zou,  Jérôme Darbon,  George Em Karniadakis</p>
  <p><b>备注</b>：26 pages in total, 23 pages for the main text, 9 figures</p>
  <p><b>关键词</b>：Hamilton-Jacobi partial differential, Hamilton-Jacobi partial, range of fields, imaging sciences, including optimal control</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Hamilton-Jacobi partial differential equations (HJ PDEs) have deep
connections with a wide range of fields, including optimal control,
differential games, and imaging sciences. By considering the time variable to
be a higher dimensional quantity, HJ PDEs can be extended to the multi-time
case. In this paper, we establish a novel theoretical connection between
specific optimization problems arising in machine learning and the multi-time
Hopf formula, which corresponds to a representation of the solution to certain
multi-time HJ PDEs. Through this connection, we increase the interpretability
of the training process of certain machine learning applications by showing
that when we solve these learning problems, we also solve a multi-time HJ PDE
and, by extension, its corresponding optimal control problem. As a first
exploration of this connection, we develop the relation between the regularized
linear regression problem and the Linear Quadratic Regulator (LQR). We then
leverage our theoretical connection to adapt standard LQR solvers (namely,
those based on the Riccati ordinary differential equations) to design new
training approaches for machine learning. Finally, we provide some numerical
examples that demonstrate the versatility and possible computational advantages
of our Riccati-based approach in the context of continual learning,
post-training calibration, transfer learning, and sparse dynamics
identification.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：Revisiting the Fragility of Influence Functions</b></summary>
  <p><b>编号</b>：[268]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12922</p>
  <p><b>作者</b>：Jacob R. Epifano,  Ravi P. Ramachandran,  Aaron J. Masino,  Ghulam Rasool</p>
  <p><b>备注</b>：11 pages, 5 figures, accepted to Neural Networks</p>
  <p><b>关键词</b>：deep learning models, influence functions, explain the predictions, predictions of deep, deep learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the last few years, many works have tried to explain the predictions of
deep learning models. Few methods, however, have been proposed to verify the
accuracy or faithfulness of these explanations. Recently, influence functions,
which is a method that approximates the effect that leave-one-out training has
on the loss function, has been shown to be fragile. The proposed reason for
their fragility remains unclear. Although previous work suggests the use of
regularization to increase robustness, this does not hold in all cases. In this
work, we seek to investigate the experiments performed in the prior work in an
effort to understand the underlying mechanisms of influence function fragility.
First, we verify influence functions using procedures from the literature under
conditions where the convexity assumptions of influence functions are met.
Then, we relax these assumptions and study the effects of non-convexity by
using deeper models and more complex datasets. Here, we analyze the key metrics
and procedures that are used to validate influence functions. Our results
indicate that the validation procedures may cause the observed fragility.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：Stability is Stable: Connections between Replicability, Privacy, and  Adaptive Generalization</b></summary>
  <p><b>编号</b>：[269]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12921</p>
  <p><b>作者</b>：Mark Bun,  Marco Gaboardi,  Max Hopkins,  Russell Impagliazzo,  Rex Lei,  Toniann Pitassi,  Jessica Sorrell,  Satchit Sivakumar</p>
  <p><b>备注</b>：STOC 2023</p>
  <p><b>关键词</b>：introduced in Impagliazzo, replicable algorithms, Impagliazzo, algorithms, replicable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The notion of replicable algorithms was introduced in Impagliazzo et al.
[STOC '22] to describe randomized algorithms that are stable under the
resampling of their inputs. More precisely, a replicable algorithm gives the
same output with high probability when its randomness is fixed and it is run on
a new i.i.d. sample drawn from the same distribution. Using replicable
algorithms for data analysis can facilitate the verification of published
results by ensuring that the results of an analysis will be the same with high
probability, even when that analysis is performed on a new data set.
In this work, we establish new connections and separations between
replicability and standard notions of algorithmic stability. In particular, we
give sample-efficient algorithmic reductions between perfect generalization,
approximate differential privacy, and replicability for a broad class of
statistical problems. Conversely, we show any such equivalence must break down
computationally: there exist statistical problems that are easy under
differential privacy, but that cannot be solved replicably without breaking
public-key cryptography. Furthermore, these results are tight: our reductions
are statistically optimal, and we show that any computational separation
between DP and replicability must imply the existence of one-way functions.
Our statistical reductions give a new algorithmic framework for translating
between notions of stability, which we instantiate to answer several open
questions in replicability and privacy. This includes giving sample-efficient
replicable algorithms for various PAC learning, distribution estimation, and
distribution testing problems, algorithmic amplification of $\delta$ in
approximate DP, conversions from item-level to user-level privacy, and the
existence of private agnostic-to-realizable learning reductions under
structured distributions.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：Self-distillation for surgical action recognition</b></summary>
  <p><b>编号</b>：[272]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12915</p>
  <p><b>作者</b>：Amine Yamlahi,  Thuy Nuong Tran,  Patrick Godau,  Melanie Schellenberg,  Dominik Michael,  Finn-Henri Smidt,  Jan-Hinrich Noelke,  Tim Adler,  Minu Dietlinde Tizabi,  Chinedu Nwoye,  Nicolas Padoy,  Lena Maier-Hein</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：contextaware decision support, Surgical scene understanding, operating room, scene understanding, key prerequisite</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Surgical scene understanding is a key prerequisite for contextaware decision
support in the operating room. While deep learning-based approaches have
already reached or even surpassed human performance in various fields, the task
of surgical action recognition remains a major challenge. With this
contribution, we are the first to investigate the concept of self-distillation
as a means of addressing class imbalance and potential label ambiguity in
surgical video analysis. Our proposed method is a heterogeneous ensemble of
three models that use Swin Transfomers as backbone and the concepts of
self-distillation and multi-task learning as core design choices. According to
ablation studies performed with the CholecT45 challenge data via
cross-validation, the biggest performance boost is achieved by the usage of
soft labels obtained by self-distillation. External validation of our method on
an independent test set was achieved by providing a Docker container of our
inference model to the challenge organizers. According to their analysis, our
method outperforms all other solutions submitted to the latest challenge in the
field. Our approach thus shows the potential of self-distillation for becoming
an important tool in medical image analysis applications.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：TRON: Transformer Neural Network Acceleration with Non-Coherent Silicon  Photonics</b></summary>
  <p><b>编号</b>：[273]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12914</p>
  <p><b>作者</b>：Salma Afifi,  Febin Sunny,  Mahdi Nikdast,  Sudeep Pasricha</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：natural language processing, solutions for natural, language processing, NLP, rapidly being integrated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transformer neural networks are rapidly being integrated into
state-of-the-art solutions for natural language processing (NLP) and computer
vision. However, the complex structure of these models creates challenges for
accelerating their execution on conventional electronic platforms. We propose
the first silicon photonic hardware neural network accelerator called TRON for
transformer-based models such as BERT, and Vision Transformers. Our analysis
demonstrates that TRON exhibits at least 14x better throughput and 8x better
energy efficiency, in comparison to state-of-the-art transformer accelerators.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：Cross-Layer Design for AI Acceleration with Non-Coherent Optical  Computing</b></summary>
  <p><b>编号</b>：[275]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12910</p>
  <p><b>作者</b>：Febin Sunny,  Mahdi Nikdast,  Sudeep Pasricha</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：graph convolutional networks, neural networks require, networks require massive, deep neural networks, require massive computational</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Emerging AI applications such as ChatGPT, graph convolutional networks, and
other deep neural networks require massive computational resources for training
and inference. Contemporary computing platforms such as CPUs, GPUs, and TPUs
are struggling to keep up with the demands of these AI applications.
Non-coherent optical computing represents a promising approach for light-speed
acceleration of AI workloads. In this paper, we show how cross-layer design can
overcome challenges in non-coherent optical computing platforms. We describe
approaches for optical device engineering, tuning circuit enhancements, and
architectural innovations to adapt optical computing to a variety of AI
workloads. We also discuss techniques for hardware/software co-design that can
intelligently map and adapt AI software to improve its performance on
non-coherent optical computing platforms.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：Feature Reduction Method Comparison Towards Explainability and  Efficiency in Cybersecurity Intrusion Detection Systems</b></summary>
  <p><b>编号</b>：[282]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12891</p>
  <p><b>作者</b>：Adam M. Lehavi,  Seongtae Kim</p>
  <p><b>备注</b>：Published in 2022 21st IEEE International Conference on Machine Learning and Applications. 8 pages. 5 figures</p>
  <p><b>关键词</b>：intrusion detection systems, prevent attacks based, realm of cybersecurity, intrusion detection, detection systems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the realm of cybersecurity, intrusion detection systems (IDS) detect and
prevent attacks based on collected computer and network data. In recent
research, IDS models have been constructed using machine learning (ML) and deep
learning (DL) methods such as Random Forest (RF) and deep neural networks
(DNN). Feature selection (FS) can be used to construct faster, more
interpretable, and more accurate models. We look at three different FS
techniques; RF information gain (RF-IG), correlation feature selection using
the Bat Algorithm (CFS-BA), and CFS using the Aquila Optimizer (CFS-AO). Our
results show CFS-BA to be the most efficient of the FS methods, building in 55%
of the time of the best RF-IG model while achieving 99.99% of its accuracy.
This reinforces prior contributions attesting to CFS-BA's accuracy while
building upon the relationship between subset size, CFS score, and RF-IG score
in final results.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：A dynamic risk score for early prediction of cardiogenic shock using  machine learning</b></summary>
  <p><b>编号</b>：[285]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12888</p>
  <p><b>作者</b>：Yuxuan Hu,  Albert Lui,  Mark Goldstein,  Mukund Sudarshan,  Andrea Tinsay,  Cindy Tsui,  Samuel Maidman,  John Medamana,  Neil Jethani,  Aaalad Puli,  Vuthy Nguy,  Yindalon Aphinyanaphongs,  Nicholas Kiefer,  Nathaniel Smilowitz,  James Horowitz,  Tania Ahuja,  Glenn Fishman,  Judith Hochman,  Stuart Katz,  Samuel Bernard,  Rajesh Ranganath</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：major cardiovascular diseases, cardiogenic shock, major cardiovascular, cardiovascular diseases, diseases that affect</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Myocardial infarction and heart failure are major cardiovascular diseases
that affect millions of people in the US. The morbidity and mortality are
highest among patients who develop cardiogenic shock. Early recognition of
cardiogenic shock is critical. Prompt implementation of treatment measures can
prevent the deleterious spiral of ischemia, low blood pressure, and reduced
cardiac output due to cardiogenic shock. However, early identification of
cardiogenic shock has been challenging due to human providers' inability to
process the enormous amount of data in the cardiac intensive care unit (ICU)
and lack of an effective risk stratification tool. We developed a deep
learning-based risk stratification tool, called CShock, for patients admitted
into the cardiac ICU with acute decompensated heart failure and/or myocardial
infarction to predict onset of cardiogenic shock. To develop and validate
CShock, we annotated cardiac ICU datasets with physician adjudicated outcomes.
CShock achieved an area under the receiver operator characteristic curve
(AUROC) of 0.820, which substantially outperformed CardShock (AUROC 0.519), a
well-established risk score for cardiogenic shock prognosis. CShock was
externally validated in an independent patient cohort and achieved an AUROC of
0.800, demonstrating its generalizability in other cardiac ICUs.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：Robust Consensus in Ranking Data Analysis: Definitions, Properties and  Computational Issues</b></summary>
  <p><b>编号</b>：[287]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12878</p>
  <p><b>作者</b>：Morgane Goibert,  Clément Calauzènes,  Ekhine Irurozki,  Stéphan Clémençon</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：partly contaminated data, statistical learning techniques, ranking data analysis, learning techniques, presence of partly</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As the issue of robustness in AI systems becomes vital, statistical learning
techniques that are reliable even in presence of partly contaminated data have
to be developed. Preference data, in the form of (complete) rankings in the
simplest situations, are no exception and the demand for appropriate concepts
and tools is all the more pressing given that technologies fed by or producing
this type of data (e.g. search engines, recommending systems) are now massively
deployed. However, the lack of vector space structure for the set of rankings
(i.e. the symmetric group $\mathfrak{S}_n$) and the complex nature of
statistics considered in ranking data analysis make the formulation of
robustness objectives in this domain challenging. In this paper, we introduce
notions of robustness, together with dedicated statistical methods, for
Consensus Ranking the flagship problem in ranking data analysis, aiming at
summarizing a probability distribution on $\mathfrak{S}_n$ by a median ranking.
Precisely, we propose specific extensions of the popular concept of breakdown
point, tailored to consensus ranking, and address the related computational
issues. Beyond the theoretical contributions, the relevance of the approach
proposed is supported by an experimental study.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：Human Uncertainty in Concept-Based AI Systems</b></summary>
  <p><b>编号</b>：[290]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12872</p>
  <p><b>作者</b>：Katherine M. Collins,  Matthew Barker,  Mateo Espinosa Zarlenga,  Naveen Raman,  Umang Bhatt,  Mateja Jamnik,  Ilia Sucholutsky,  Adrian Weller,  Krishnamurthy Dvijotham</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：safety-critical settings, loop may abate, clinician working, humans, mitigating risks arising</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Placing a human in the loop may abate the risks of deploying AI systems in
safety-critical settings (e.g., a clinician working with a medical AI system).
However, mitigating risks arising from human error and uncertainty within such
human-AI interactions is an important and understudied issue. In this work, we
study human uncertainty in the context of concept-based models, a family of AI
systems that enable human feedback via concept interventions where an expert
intervenes on human-interpretable concepts relevant to the task. Prior work in
this space often assumes that humans are oracles who are always certain and
correct. Yet, real-world decision-making by humans is prone to occasional
mistakes and uncertainty. We study how existing concept-based models deal with
uncertain interventions from humans using two novel datasets: UMNIST, a visual
dataset with controlled simulated uncertainty based on the MNIST dataset, and
CUB-S, a relabeling of the popular CUB concept dataset with rich,
densely-annotated soft labels from humans. We show that training with uncertain
concept labels may help mitigate weaknesses of concept-based systems when
handling uncertain interventions. These results allow us to identify several
open challenges, which we argue can be tackled through future multidisciplinary
research on building interactive uncertainty-aware systems. To facilitate
further research, we release a new elicitation platform, UElic, to collect
uncertain feedback from humans in collaborative prediction tasks.</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：NeRF-GAN Distillation for Efficient 3D-Aware Generation with  Convolutions</b></summary>
  <p><b>编号</b>：[293]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12865</p>
  <p><b>作者</b>：Mohamad Shahbazi,  Evangelos Ntavelis,  Alessio Tonioni,  Edo Collins,  Danda Pani Paudel,  Martin Danelljan,  Luc Van Gool</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：generative models struggle, Neural Radiance Fields, Generative Adversarial Networks, generative models, convolutional generative models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pose-conditioned convolutional generative models struggle with high-quality
3D-consistent image generation from single-view datasets, due to their lack of
sufficient 3D priors. Recently, the integration of Neural Radiance Fields
(NeRFs) and generative models, such as Generative Adversarial Networks (GANs),
has transformed 3D-aware generation from single-view images. NeRF-GANs exploit
the strong inductive bias of 3D neural representations and volumetric rendering
at the cost of higher computational complexity. This study aims at revisiting
pose-conditioned 2D GANs for efficient 3D-aware generation at inference time by
distilling 3D knowledge from pretrained NeRF-GANS. We propose a simple and
effective method, based on re-using the well-disentangled latent space of a
pre-trained NeRF-GAN in a pose-conditioned convolutional network to directly
generate 3D-consistent images corresponding to the underlying 3D
representations. Experiments on several datasets demonstrate that the proposed
method obtains results comparable with volumetric rendering in terms of quality
and 3D consistency while benefiting from the superior computational advantage
of convolutional networks. The code will be available at:
this https URL</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：Anti-symmetric Barron functions and their approximation with sums of  determinants</b></summary>
  <p><b>编号</b>：[296]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12856</p>
  <p><b>作者</b>：Nilin Abrahamsen,  Lin Lin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Barron space, identical particles, Barron space consists, fundamental problem, permutations of identical</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A fundamental problem in quantum physics is to encode functions that are
completely anti-symmetric under permutations of identical particles. The Barron
space consists of high-dimensional functions that can be parameterized by
infinite neural networks with one hidden layer. By explicitly encoding the
anti-symmetric structure, we prove that the anti-symmetric functions which
belong to the Barron space can be efficiently approximated with sums of
determinants. This yields a factorial improvement in complexity compared to the
standard representation in the Barron space and provides a theoretical
explanation for the effectiveness of determinant-based architectures in
ab-initio quantum chemistry.</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：Three iterations of $(1-d)$-WL test distinguish non isometric clouds of  $d$-dimensional points</b></summary>
  <p><b>编号</b>：[297]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12853</p>
  <p><b>作者</b>：Valentino Delle Rose,  Alexander Kozachinskiy,  Cristóbal Rojas,  Mircea Petrache,  Pablo Barceló</p>
  <p><b>备注</b>：14 pages</p>
  <p><b>关键词</b>：fundamental iterative algorithm, fundamental iterative, iterative algorithm, algorithm for checking, checking isomorphism</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The Weisfeiler--Lehman (WL) test is a fundamental iterative algorithm for
checking isomorphism of graphs. It has also been observed that it underlies the
design of several graph neural network architectures, whose capabilities and
performance can be understood in terms of the expressive power of this test.
Motivated by recent developments in machine learning applications to datasets
involving three-dimensional objects, we study when the WL test is {\em
complete} for clouds of euclidean points represented by complete distance
graphs, i.e., when it can distinguish, up to isometry, any arbitrary such
cloud.
Our main result states that the $(d-1)$-dimensional WL test is complete for
point clouds in $d$-dimensional Euclidean space, for any $d\ge 2$, and that
only three iterations of the test suffice. Our result is tight for $d = 2, 3$.
We also observe that the $d$-dimensional WL test only requires one iteration to
achieve completeness.</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：Test-time Defense against Adversarial Attacks: Detection and  Reconstruction of Adversarial Examples via Masked Autoencoder</b></summary>
  <p><b>编号</b>：[300]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12848</p>
  <p><b>作者</b>：Yun-Yun Tsai,  Ju-Chin Chao,  Albert Wen,  Zhaoyuan Yang,  Chengzhi Mao,  Tapan Shah,  Junfeng Yang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：test time, time, Existing defense methods, adversarial, adversarial attacks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing defense methods against adversarial attacks can be categorized into
training time and test time defenses. Training time defense, i.e., adversarial
training, requires a significant amount of extra time for training and is often
not able to be generalized to unseen attacks. On the other hand, test time
defense by test time weight adaptation requires access to perform gradient
descent on (part of) the model weights, which could be infeasible for models
with frozen weights. To address these challenges, we propose DRAM, a novel
defense method to Detect and Reconstruct multiple types of Adversarial attacks
via Masked autoencoder (MAE). We demonstrate how to use MAE losses to build a
KS-test to detect adversarial attacks. Moreover, the MAE losses can be used to
repair adversarial samples from unseen attack types. In this sense, DRAM
neither requires model weight updates in test time nor augments the training
set with more adversarial samples. Evaluating DRAM on the large-scale ImageNet
data, we achieve the best detection rate of 82% on average on eight types of
adversarial attacks compared with other detection baselines. For
reconstruction, DRAM improves the robust accuracy by 6% ~ 41% for Standard
ResNet50 and 3% ~ 8% for Robust ResNet50 compared with other self-supervision
tasks, such as rotation prediction and contrastive learning.</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：Co-Speech Gesture Synthesis using Discrete Gesture Token Learning</b></summary>
  <p><b>编号</b>：[302]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12822</p>
  <p><b>作者</b>：Shuhong Lu,  Youngwoo Yoon,  Andrew Feng</p>
  <p><b>备注</b>：8 pages, 3 figures, 3 tables</p>
  <p><b>关键词</b>：creating believable motions, human users, Synthesizing realistic co-speech, unsolved problem, problem for creating</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Synthesizing realistic co-speech gestures is an important and yet unsolved
problem for creating believable motions that can drive a humanoid robot to
interact and communicate with human users. Such capability will improve the
impressions of the robots by human users and will find applications in
education, training, and medical services. One challenge in learning the
co-speech gesture model is that there may be multiple viable gesture motions
for the same speech utterance. The deterministic regression methods can not
resolve the conflicting samples and may produce over-smoothed or damped
motions. We proposed a two-stage model to address this uncertainty issue in
gesture synthesis by modeling the gesture segments as discrete latent codes.
Our method utilizes RQ-VAE in the first stage to learn a discrete codebook
consisting of gesture tokens from training data. In the second stage, a
two-level autoregressive transformer model is used to learn the prior
distribution of residual codes conditioned on input speech context. Since the
inference is formulated as token sampling, multiple gesture sequences could be
generated given the same speech input using top-k sampling. The quantitative
results and the user study showed the proposed method outperforms the previous
methods and is able to generate realistic and diverse gesture motions.</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：Towards A Visual Programming Tool to Create Deep Learning Models</b></summary>
  <p><b>编号</b>：[303]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12821</p>
  <p><b>作者</b>：Tommaso Calò,  Luigi De Russis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Deep Learning, computer science, Learning, programming languages, high-level programming languages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep Learning (DL) developers come from different backgrounds, e.g.,
medicine, genomics, finance, and computer science. To create a DL model, they
must learn and use high-level programming languages (e.g., Python), thus
needing to handle related setups and solve programming errors. This paper
presents DeepBlocks, a visual programming tool that allows DL developers to
design, train, and evaluate models without relying on specific programming
languages. DeepBlocks works by building on the typical model structure: a
sequence of learnable functions whose arrangement defines the specific
characteristics of the model. We derived DeepBlocks' design goals from a
5-participants formative interview, and we validated the first implementation
of the tool through a typical use case. Results are promising and show that
developers could visually design complex DL architectures.</p>
  </details>
</details>
<details>
  <summary>83. <b>标题：An Empirical Analysis of the Shift and Scale Parameters in BatchNorm</b></summary>
  <p><b>编号</b>：[304]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12818</p>
  <p><b>作者</b>：Yashna Peerthum,  Mark Stamp</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Convolutional Neural Networks, deep neural networks, neural networks, Convolutional Neural, deep neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Batch Normalization (BatchNorm) is a technique that improves the training of
deep neural networks, especially Convolutional Neural Networks (CNN). It has
been empirically demonstrated that BatchNorm increases performance, stability,
and accuracy, although the reasons for such improvements are unclear. BatchNorm
includes a normalization step as well as trainable shift and scale parameters.
In this paper, we empirically examine the relative contribution to the success
of BatchNorm of the normalization step, as compared to the re-parameterization
via shifting and scaling. To conduct our experiments, we implement two new
optimizers in PyTorch, namely, a version of BatchNorm that we refer to as
AffineLayer, which includes the re-parameterization step without normalization,
and a version with just the normalization step, that we call BatchNorm-minus.
We compare the performance of our AffineLayer and BatchNorm-minus
implementations to standard BatchNorm, and we also compare these to the case
where no batch normalization is used. We experiment with four ResNet
architectures (ResNet18, ResNet34, ResNet50, and ResNet101) over a standard
image dataset and multiple batch sizes. Among other findings, we provide
empirical evidence that the success of BatchNorm may derive primarily from
improved weight initialization.</p>
  </details>
</details>
<details>
  <summary>84. <b>标题：From Wide to Deep: Dimension Lifting Network for Parameter-efficient  Knowledge Graph Embedding</b></summary>
  <p><b>编号</b>：[306]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12816</p>
  <p><b>作者</b>：Borui Cai,  Yong Xiang,  Longxiang Gao,  Di Wu,  He Zhang,  Jiong Jin,  Tom Luan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：entity representations, KGE methods, KGE, Conventional KGE methods, downstream tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Knowledge graph embedding (KGE) that maps entities and relations into vector
representations is essential for downstream tasks. Conventional KGE methods
require relatively high-dimensional entity representations to preserve the
structural information of knowledge graph, but lead to oversized model
parameters. Recent methods reduce model parameters by adopting low-dimensional
entity representations, while developing techniques (e.g., knowledge
distillation) to compensate for the reduced dimension. However, such operations
produce degraded model accuracy and limited reduction of model parameters.
Specifically, we view the concatenation of all entity representations as an
embedding layer, and then conventional KGE methods that adopt high-dimensional
entity representations equal to enlarging the width of the embedding layer to
gain expressiveness. To achieve parameter efficiency without sacrificing
accuracy, we instead increase the depth and propose a deeper embedding network
for entity representations, i.e., a narrow embedding layer and a multi-layer
dimension lifting network (LiftNet). Experiments on three public datasets show
that the proposed method (implemented based on TransE and DistMult) with
4-dimensional entity representations achieves more accurate link prediction
results than counterpart parameter-efficient KGE methods and strong KGE
baselines, including TransE and DistMult with 512-dimensional entity
representations.</p>
  </details>
</details>
<details>
  <summary>85. <b>标题：A Comparison of Graph Neural Networks for Malware Classification</b></summary>
  <p><b>编号</b>：[307]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12812</p>
  <p><b>作者</b>：Vrinda Malhotra,  Katerina Potika,  Mark Stamp</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Managing the threat, requires accurate detection, threat posed, malware requires accurate, GNN models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Managing the threat posed by malware requires accurate detection and
classification techniques. Traditional detection strategies, such as signature
scanning, rely on manual analysis of malware to extract relevant features,
which is labor intensive and requires expert knowledge. Function call graphs
consist of a set of program functions and their inter-procedural calls,
providing a rich source of information that can be leveraged to classify
malware without the labor intensive feature extraction step of traditional
techniques. In this research, we treat malware classification as a graph
classification problem. Based on Local Degree Profile features, we train a wide
range of Graph Neural Network (GNN) architectures to generate embeddings which
we then classify. We find that our best GNN models outperform previous
comparable research involving the well-known MalNet-Tiny Android malware
dataset. In addition, our GNN models do not suffer from the overfitting issues
that commonly afflict non-GNN techniques, although GNN models require longer
training times.</p>
  </details>
</details>
<details>
  <summary>86. <b>标题：SignCRF: Scalable Channel-agnostic Data-driven Radio Authentication  System</b></summary>
  <p><b>编号</b>：[308]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12811</p>
  <p><b>作者</b>：Amani Al-shawabka,  Philip Pietraski,  Sudhir B Pattar,  Pedram Johari,  Tommaso Melodia</p>
  <p><b>备注</b>：11 pages, 13 figures, 3 tables</p>
  <p><b>关键词</b>：Deep Learning, Radio Frequency Fingerprinting, Frequency Fingerprinting, Radio Frequency, Fingerprinting through Deep</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Radio Frequency Fingerprinting through Deep Learning (RFFDL) is a data-driven
IoT authentication technique that leverages the unique hardware-level
manufacturing imperfections associated with a particular device to recognize
(fingerprint) the device based on variations introduced in the transmitted
waveform. The proposed SignCRF is a scalable, channel-agnostic, data-driven
radio authentication platform with unmatched precision in fingerprinting
wireless devices based on their unique manufacturing impairments and
independent of the dynamic channel irregularities caused by mobility. SignCRF
consists of (i) a baseline classifier finely trained to authenticate devices
with high accuracy and at scale; (ii) an environment translator carefully
designed and trained to remove the dynamic channel impact from RF signals while
maintaining the radio's specific signature; (iii) a Max-Rule module that
selects the highest precision authentication technique between the baseline
classifier and the environment translator per radio. We design, train, and
validate the performance of SignCRF for multiple technologies in dynamic
environments and at scale (100 LoRa and 20 WiFi devices). We demonstrate that
SignCRF significantly improves the RFFDL performance by achieving as high as 5x
and 8x improvement in correct authentication of WiFi and LoRa devices when
compared to the state-of-the-art, respectively.</p>
  </details>
</details>
<details>
  <summary>87. <b>标题：Granular-ball Optimization Algorithm</b></summary>
  <p><b>编号</b>：[311]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12807</p>
  <p><b>作者</b>：Shuyin Xia,  Jiancu Chen,  Bin Hou,  Guoyin Wang</p>
  <p><b>备注</b>：10 pages, 22 figures</p>
  <p><b>关键词</b>：finest granularity, designed based, intelligent optimization algorithms, optimization algorithms, optimization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The existing intelligent optimization algorithms are designed based on the
finest granularity, i.e., a point. This leads to weak global search ability and
inefficiency. To address this problem, we proposed a novel multi-granularity
optimization algorithm, namely granular-ball optimization algorithm (GBO), by
introducing granular-ball computing. GBO uses many granular-balls to cover the
solution space. Quite a lot of small and fine-grained granular-balls are used
to depict the important parts, and a little number of large and coarse-grained
granular-balls are used to depict the inessential parts. Fine multi-granularity
data description ability results in a higher global search capability and
faster convergence speed. In comparison with the most popular and
state-of-the-art algorithms, the experiments on twenty benchmark functions
demonstrate its better performance. The faster speed, higher approximation
ability of optimal solution, no hyper-parameters, and simpler design of GBO
make it an all-around replacement of most of the existing popular intelligent
optimization algorithms.</p>
  </details>
</details>
<details>
  <summary>88. <b>标题：Features matching using natural language processing</b></summary>
  <p><b>编号</b>：[313]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12804</p>
  <p><b>作者</b>：Muhammad Danial Khilji</p>
  <p><b>备注</b>：10 pages, 7 figures, International Conference on NLP & AI (NLPAI 2023)</p>
  <p><b>关键词</b>：pretrained Natural Language, basic step, Natural Language, step in matching, matching</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The feature matching is a basic step in matching different datasets. This
article proposes shows a new hybrid model of a pretrained Natural Language
Processing (NLP) based model called BERT used in parallel with a statistical
model based on Jaccard similarity to measure the similarity between list of
features from two different datasets. This reduces the time required to search
for correlations or manually match each feature from one dataset to another.</p>
  </details>
</details>
<details>
  <summary>89. <b>标题：Distributed Learning Meets 6G: A Communication and Computing Perspective</b></summary>
  <p><b>编号</b>：[315]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12802</p>
  <p><b>作者</b>：Shashank Jere,  Yifei Song,  Yang Yi,  Lingjia Liu</p>
  <p><b>备注</b>：This article has been accepted to IEEE Wireless Communications Magazine (WCM) under the Special Issue "AI-Powered Telco Network Automation: 5G Evolution and 6G"</p>
  <p><b>关键词</b>：telecommunication network paradigms, exploring Distributed, evolving telecommunication network, ever-improving computing capabilities, capabilities and storage</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the ever-improving computing capabilities and storage capacities of
mobile devices in line with evolving telecommunication network paradigms, there
has been an explosion of research interest towards exploring Distributed
Learning (DL) frameworks to realize stringent key performance indicators (KPIs)
that are expected in next-generation/6G cellular networks. In conjunction with
Edge Computing, Federated Learning (FL) has emerged as the DL architecture of
choice in prominent wireless applications. This article lays an outline of how
DL in general and FL-based strategies specifically can contribute towards
realizing a part of the 6G vision and strike a balance between communication
and computing constraints. As a practical use case, we apply Multi-Agent
Reinforcement Learning (MARL) within the FL framework to the Dynamic Spectrum
Access (DSA) problem and present preliminary evaluation results. Top
contemporary challenges in applying DL approaches to 6G networks are also
highlighted.</p>
  </details>
</details>
<details>
  <summary>90. <b>标题：IoT Device Identification Based on Network Communication Analysis Using  Deep Learning</b></summary>
  <p><b>编号</b>：[316]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12800</p>
  <p><b>作者</b>：Jaidip Kotak,  Yuval Elovici</p>
  <p><b>备注</b>：J Ambient Intell Human Comput (2022)</p>
  <p><b>关键词</b>：IoT devices, organization network, IoT, secure IoT devices, devices</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Attack vectors for adversaries have increased in organizations because of the
growing use of less secure IoT devices. The risk of attacks on an
organization's network has also increased due to the bring your own device
(BYOD) policy which permits employees to bring IoT devices onto the premises
and attach them to the organization's network. To tackle this threat and
protect their networks, organizations generally implement security policies in
which only white listed IoT devices are allowed on the organization's network.
To monitor compliance with such policies, it has become essential to
distinguish IoT devices permitted within an organization's network from non
white listed (unknown) IoT devices. In this research, deep learning is applied
to network communication for the automated identification of IoT devices
permitted on the network. In contrast to existing methods, the proposed
approach does not require complex feature engineering of the network
communication, because the 'communication behavior' of IoT devices is
represented as small images which are generated from the device's network
communication payload. The proposed approach is applicable for any IoT device,
regardless of the protocol used for communication. As our approach relies on
the network communication payload, it is also applicable for the IoT devices
behind a network address translation (NAT) enabled router. In this study, we
trained various classifiers on a publicly accessible dataset to identify IoT
devices in different scenarios, including the identification of known and
unknown IoT devices, achieving over 99% overall average detection accuracy.</p>
  </details>
</details>
<details>
  <summary>91. <b>标题：Time Series as Images: Vision Transformer for Irregularly Sampled Time  Series</b></summary>
  <p><b>编号</b>：[317]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12799</p>
  <p><b>作者</b>：Zekun Li,  Shiyang Li,  Xifeng Yan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：sampled time series, Irregularly sampled time, medical applications, time series, increasingly prevalent</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Irregularly sampled time series are becoming increasingly prevalent in
various domains, especially in medical applications. Although different
highly-customized methods have been proposed to tackle irregularity, how to
effectively model their complicated dynamics and high sparsity is still an open
problem. This paper studies the problem from a whole new perspective:
transforming irregularly sampled time series into line graph images and
adapting powerful vision transformers to perform time series classification in
the same way as image classification. Our approach largely simplifies algorithm
designs without assuming prior knowledge and can be potentially extended as a
general-purpose framework. Despite its simplicity, we show that it
substantially outperforms state-of-the-art specialized algorithms on several
popular healthcare and human activity datasets. Especially in the challenging
leave-sensors-out setting where a subset of variables is masked during testing,
the performance improvement is up to 54.0\% in absolute F1 score points. Our
code and data are available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>92. <b>标题：Interpersonal Distance Tracking with mmWave Radar and IMUs</b></summary>
  <p><b>编号</b>：[318]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12798</p>
  <p><b>作者</b>：Yimin Dai,  Xian Shuai,  Rui Tan,  Guoliang Xing</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：real-time social distancing, social distancing management, track interpersonal distances, contagious diseases, essential for real-time</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Tracking interpersonal distances is essential for real-time social distancing
management and {\em ex-post} contact tracing to prevent spreads of contagious
diseases. Bluetooth neighbor discovery has been employed for such purposes in
combating COVID-19, but does not provide satisfactory spatiotemporal
resolutions. This paper presents ImmTrack, a system that uses a millimeter wave
radar and exploits the inertial measurement data from user-carried smartphones
or wearables to track interpersonal distances. By matching the movement traces
reconstructed from the radar and inertial data, the pseudo identities of the
inertial data can be transferred to the radar sensing results in the global
coordinate system. The re-identified, radar-sensed movement trajectories are
then used to track interpersonal distances. In a broader sense, ImmTrack is the
first system that fuses data from millimeter wave radar and inertial
measurement units for simultaneous user tracking and re-identification.
Evaluation with up to 27 people in various indoor/outdoor environments shows
ImmTrack's decimeters-seconds spatiotemporal accuracy in contact tracing, which
is similar to that of the privacy-intrusive camera surveillance and
significantly outperforms the Bluetooth neighbor discovery approach.</p>
  </details>
</details>
<details>
  <summary>93. <b>标题：An algorithmic framework for the optimization of deep neural networks  architectures and hyperparameters</b></summary>
  <p><b>编号</b>：[319]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12797</p>
  <p><b>作者</b>：Julie Keisler (EDF R&D OSIRIS, EDF R&D, CRIStAL),  El-Ghazali Talbi (CRIStAL),  Sandra Claudel (EDF R&D OSIRIS, EDF R&D),  Gilles Cabriel (EDF R&D OSIRIS, EDF R&D)</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：automatically generate efficient, generate efficient deep, efficient deep neural, deep neural networks, automatically generate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we propose an algorithmic framework to automatically generate
efficient deep neural networks and optimize their associated hyperparameters.
The framework is based on evolving directed acyclic graphs (DAGs), defining a
more flexible search space than the existing ones in the literature. It allows
mixtures of different classical operations: convolutions, recurrences and dense
layers, but also more newfangled operations such as self-attention. Based on
this search space we propose neighbourhood and evolution search operators to
optimize both the architecture and hyper-parameters of our networks. These
search operators can be used with any metaheuristic capable of handling mixed
search spaces. We tested our algorithmic framework with an evolutionary
algorithm on a time series prediction benchmark. The results demonstrate that
our framework was able to find models outperforming the established baseline on
numerous datasets.</p>
  </details>
</details>
<details>
  <summary>94. <b>标题：An Analysis of Abstractive Text Summarization Using Pre-trained Models</b></summary>
  <p><b>编号</b>：[320]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12796</p>
  <p><b>作者</b>：Tohida Rehman,  Suchandan Das,  Debarshi Kumar Sanyal,  Samiran Chattopadhyay</p>
  <p><b>备注</b>：11 Pages, 6 Figures, 3 Tables</p>
  <p><b>关键词</b>：Bing to find, People nowadays, find information, Yahoo, Internet</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>People nowadays use search engines like Google, Yahoo, and Bing to find
information on the Internet. Due to explosion in data, it is helpful for users
if they are provided relevant summaries of the search results rather than just
links to webpages. Text summarization has become a vital approach to help
consumers swiftly grasp vast amounts of this http URL this paper, different
pre-trained models for text summarization are evaluated on different datasets.
Specifically, we have used three different pre-trained models, namely,
google/pegasus-cnn-dailymail, T5-base, facebook/bart-large-cnn. We have
considered three different datasets, namely, CNN-dailymail, SAMSum and BillSum
to get the output from the above three models. The pre-trained models are
compared over these different datasets, each of 2000 examples, through ROUGH
and BLEU metrics.</p>
  </details>
</details>
<details>
  <summary>95. <b>标题：Named Entity Recognition Based Automatic Generation of Research  Highlights</b></summary>
  <p><b>编号</b>：[321]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12795</p>
  <p><b>作者</b>：Tohida Rehman,  Debarshi Kumar Sanyal,  Prasenjit Majumder,  Samiran Chattopadhyay</p>
  <p><b>备注</b>：7 Pages, 3 Figures, 2 Tables</p>
  <p><b>关键词</b>：traditionally prefaced, highlights, paper, scientific paper, named entity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A scientific paper is traditionally prefaced by an abstract that summarizes
the paper. Recently, research highlights that focus on the main findings of the
paper have emerged as a complementary summary in addition to an abstract.
However, highlights are not yet as common as abstracts, and are absent in many
papers. In this paper, we aim to automatically generate research highlights
using different sections of a research paper as input. We investigate whether
the use of named entity recognition on the input improves the quality of the
generated highlights. In particular, we have used two deep learning-based
models: the first is a pointer-generator network, and the second augments the
first model with coverage mechanism. We then augment each of the above models
with named entity recognition features. The proposed method can be used to
produce highlights for papers with missing highlights. Our experiments show
that adding named entity information improves the performance of the deep
learning-based summarizers in terms of ROUGE, METEOR and BERTScore measures.</p>
  </details>
</details>
<details>
  <summary>96. <b>标题：Generalization with quantum geometry for learning unitaries</b></summary>
  <p><b>编号</b>：[323]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13462</p>
  <p><b>作者</b>：Tobias Haug,  M.S. Kim</p>
  <p><b>备注</b>：16 pages, 13 figures</p>
  <p><b>关键词</b>：make accurate predictions, make accurate, accurate predictions, quantum Fisher information, data quantum Fisher</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generalization is the ability of quantum machine learning models to make
accurate predictions on new data by learning from training data. Here, we
introduce the data quantum Fisher information metric (DQFIM) to determine when
a model can generalize. For variational learning of unitaries, the DQFIM
quantifies the amount of circuit parameters and training data needed to
successfully train and generalize. We apply the DQFIM to explain when a
constant number of training states and polynomial number of parameters are
sufficient for generalization. Further, we can improve generalization by
removing symmetries from training data. Finally, we show that
out-of-distribution generalization, where training and testing data are drawn
from different data distributions, can be better than using the same
distribution. Our work opens up new approaches to improve generalization in
quantum machine learning.</p>
  </details>
</details>
<details>
  <summary>97. <b>标题：Adaptive Endpointing with Deep Contextual Multi-armed Bandits</b></summary>
  <p><b>编号</b>：[328]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13407</p>
  <p><b>作者</b>：Do June Min,  Andreas Stolcke,  Anirudh Raju,  Colin Vaz,  Di He,  Venkatesh Ravichandran,  Viet Anh Trinh</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：supervised framework, incorporate feedback, feedback and improve, online setting, Current endpointing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Current endpointing (EP) solutions learn in a supervised framework, which
does not allow the model to incorporate feedback and improve in an online
setting. Also, it is a common practice to utilize costly grid-search to find
the best configuration for an endpointing model. In this paper, we aim to
provide a solution for adaptive endpointing by proposing an efficient method
for choosing an optimal endpointing configuration given utterance-level audio
features in an online setting, while avoiding hyperparameter grid-search. Our
method does not require ground truth labels, and only uses online learning from
reward signals without requiring annotated labels. Specifically, we propose a
deep contextual multi-armed bandit-based approach, which combines the
representational power of neural networks with the action exploration behavior
of Thompson modeling algorithms. We compare our approach to several baselines,
and show that our deep bandit models also succeed in reducing early cutoff
errors while maintaining low latency.</p>
  </details>
</details>
<details>
  <summary>98. <b>标题：Deep Generative Multi-Agent Imitation Model as a Computational Benchmark  for Evaluating Human Performance in Complex Interactive Tasks: A Case Study  in Football</b></summary>
  <p><b>编号</b>：[333]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13323</p>
  <p><b>作者</b>：Chaoyi Gu,  Varuna De Silva</p>
  <p><b>备注</b>：8 pages, 10 figures</p>
  <p><b>关键词</b>：engineering and sports, evaluating human performance, performance, human performance, Recurrent Neural Network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Evaluating the performance of human is a common need across many
applications, such as in engineering and sports. When evaluating human
performance in completing complex and interactive tasks, the most common way is
to use a metric having been proved efficient for that context, or to use
subjective measurement techniques. However, this can be an error prone and
unreliable process since static metrics cannot capture all the complex contexts
associated with such tasks and biases exist in subjective measurement. The
objective of our research is to create data-driven AI agents as computational
benchmarks to evaluate human performance in solving difficult tasks involving
multiple humans and contextual factors. We demonstrate this within the context
of football performance analysis. We train a generative model based on
Conditional Variational Recurrent Neural Network (VRNN) Model on a large player
and ball tracking dataset. The trained model is used to imitate the
interactions between two teams and predict the performance from each team. Then
the trained Conditional VRNN Model is used as a benchmark to evaluate team
performance. The experimental results on Premier League football dataset
demonstrates the usefulness of our method to existing state-of-the-art static
metric used in football analytics.</p>
  </details>
</details>
<details>
  <summary>99. <b>标题：RLOR: A Flexible Framework of Deep Reinforcement Learning for Operation  Research</b></summary>
  <p><b>编号</b>：[339]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13117</p>
  <p><b>作者</b>：Ching Pui Wan,  Tung Li,  Jason Min Wang</p>
  <p><b>备注</b>：21 pages</p>
  <p><b>关键词</b>：solving large combinatorial, operation research, Reinforcement learning, large combinatorial optimization, operation research problems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reinforcement learning has been applied in operation research and has shown
promise in solving large combinatorial optimization problems. However, existing
works focus on developing neural network architectures for certain problems.
These works lack the flexibility to incorporate recent advances in
reinforcement learning, as well as the flexibility of customizing model
architectures for operation research problems. In this work, we analyze the
end-to-end autoregressive models for vehicle routing problems and show that
these models can benefit from the recent advances in reinforcement learning
with a careful re-implementation of the model architecture. In particular, we
re-implemented the Attention Model and trained it with Proximal Policy
Optimization (PPO) in CleanRL, showing at least 8 times speed up in training
time. We hereby introduce RLOR, a flexible framework for Deep Reinforcement
Learning for Operation Research. We believe that a flexible framework is key to
developing deep reinforcement learning models for operation research problems.
The code of our work is publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>100. <b>标题：Predicting the Initial Conditions of the Universe using Deep Learning</b></summary>
  <p><b>编号</b>：[342]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13056</p>
  <p><b>作者</b>：Vaibhav Jindal,  Drew Jamieson,  Albert Liang,  Aarti Singh,  Shirley Ho</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：vast input space, initial conditions, alternate modeling tool, N-body simulations, computationally expensive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Finding the initial conditions that led to the current state of the universe
is challenging because it involves searching over a vast input space of initial
conditions, along with modeling their evolution via tools such as N-body
simulations which are computationally expensive. Deep learning has emerged as
an alternate modeling tool that can learn the mapping between the linear input
of an N-body simulation and the final nonlinear displacements at redshift zero,
which can significantly accelerate the forward modeling. However, this does not
help reduce the search space for initial conditions. In this paper, we
demonstrate for the first time that a deep learning model can be trained for
the reverse mapping. We train a V-Net based convolutional neural network, which
outputs the linear displacement of an N-body system, given the current time
nonlinear displacement and the cosmological parameters of the system. We
demonstrate that this neural network accurately recovers the initial linear
displacement field over a wide range of scales ($<1$-$2\%$ error up to nearly $k="1\" \mathrm{mpc}^{-1}\,h$), despite the ill-defined nature of inverse problem at smaller scales. specifically, scales are dominated by nonlinear effects which makes backward dynamics much more susceptible numerical and computational errors leading highly divergent trajectories a one-to-many mapping. results our method motivate that neural network based models can act as good approximators initial linear states their predictions serve starting points for sampling-based methods infer universe.< p>
  </1$-$2\%$></p></details>
</details>
<details>
  <summary>101. <b>标题：Cube-Based 3D Denoising Diffusion Probabilistic Model for Cone Beam  Computed Tomography Reconstruction with Incomplete Data</b></summary>
  <p><b>编号</b>：[353]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12861</p>
  <p><b>作者</b>：Wenjun Xia,  Chuang Niu,  Wenxiang Cong,  Ge Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Deep learning, computed tomography, extensively researched, field of computed, CBCT</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning (DL) has been extensively researched in the field of computed
tomography (CT) reconstruction with incomplete data, particularly in
sparse-view CT reconstruction. However, applying DL to sparse-view cone beam CT
(CBCT) remains challenging. Many models learn the mapping from sparse-view CT
images to ground truth but struggle to achieve satisfactory performance in
terms of global artifact removal. Incorporating sinogram data and utilizing
dual-domain information can enhance anti-artifact performance, but this
requires storing the entire sinogram in memory. This presents a memory issue
for high-resolution CBCT sinograms, limiting further research and application.
In this paper, we propose a cube-based 3D denoising diffusion probabilistic
model (DDPM) for CBCT reconstruction using down-sampled data. A DDPM network,
trained on cubes extracted from paired fully sampled sinograms and down-sampled
sinograms, is employed to inpaint down-sampled sinograms. Our method divides
the entire sinogram into overlapping cubes and processes these cubes in
parallel using multiple GPUs, overcoming memory limitations. Experimental
results demonstrate that our approach effectively suppresses few-view artifacts
while preserving textural details faithfully.</p>
  </details>
</details>
<details>
  <summary>102. <b>标题：The power and limitations of learning quantum dynamics incoherently</b></summary>
  <p><b>编号</b>：[354]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12834</p>
  <p><b>作者</b>：Sofiene Jerbi,  Joe Gibbs,  Manuel S. Rudolph,  Matthias C. Caro,  Patrick J. Coles,  Hsin-Yuan Huang,  Zoë Holmes</p>
  <p><b>备注</b>：6+9 pages, 7 figures</p>
  <p><b>关键词</b>：study quantum systems, important tool, tool to study, quantum systems, Quantum process learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Quantum process learning is emerging as an important tool to study quantum
systems. While studied extensively in coherent frameworks, where the target and
model system can share quantum information, less attention has been paid to
whether the dynamics of quantum systems can be learned without the system and
target directly interacting. Such incoherent frameworks are practically
appealing since they open up methods of transpiling quantum processes between
the different physical platforms without the need for technically challenging
hybrid entanglement schemes. Here we provide bounds on the sample complexity of
learning unitary processes incoherently by analyzing the number of measurements
that are required to emulate well-established coherent learning strategies. We
prove that if arbitrary measurements are allowed, then any efficiently
representable unitary can be efficiently learned within the incoherent
framework; however, when restricted to shallow-depth measurements only
low-entangling unitaries can be learned. We demonstrate our incoherent learning
algorithm for low entangling unitaries by successfully learning a 16-qubit
unitary on \texttt{ibmq\_kolkata}, and further demonstrate the scalabilty of
our proposed algorithm through extensive numerical experiments.</p>
  </details>
</details>
<details>
  <summary>103. <b>标题：Fixed points of arbitrarily deep 1-dimensional neural networks</b></summary>
  <p><b>编号</b>：[355]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12814</p>
  <p><b>作者</b>：Andrew Cook,  Andy Hammerlindl,  Warwick Tucker</p>
  <p><b>备注</b>：9 pages, 3 figures</p>
  <p><b>关键词</b>：closed under composition, logistic sigmoid, fixed points, logistic sigmoid function, mathbb</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we introduce a new class of functions on $\mathbb{R}$ that is
closed under composition, and contains the logistic sigmoid function. We use
this class to show that any 1-dimensional neural network of arbitrary depth
with logistic sigmoid activation functions has at most three fixed points.
While such neural networks are far from real world applications, we are able to
completely understand their fixed points, providing a foundation to the much
needed connection between application and theory of deep neural networks.</p>
  </details>
</details>
<details>
  <summary>104. <b>标题：Dermatologist-like explainable AI enhances trust and confidence in  diagnosing melanoma</b></summary>
  <p><b>编号</b>：[356]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12806</p>
  <p><b>作者</b>：Tirtha Chanda,  Katja Hauser,  Sarah Hobelsberger,  Tabea-Clara Bucher,  Carina Nogueira Garcia,  Christoph Wies,  Harald Kittler,  Philipp Tschandl,  Cristian Navarrete-Dechent,  Sebastian Podlipnik,  Emmanouil Chousakos,  Iva Crnaric,  Jovana Majstorovic,  Linda Alhajwan,  Tanya Foreman,  Sandra Peternel,  Sergei Sarap,  İrem Özdemir,  Raymond L. Barnhill,  Mar Llamas Velasco,  Gabriela Poch,  Sören Korsing,  Wiebke Sondermann,  Frank Friedrich Gellrich,  Markus V. Heppt,  Michael Erdmann,  Sebastian Haferkamp,  Konstantin Drexler,  Matthias Goebeler,  Bastian Schilling,  Jochen S. Utikal,  Kamran Ghoreschi,  Stefan Fröhling,  Eva Krieghoff-Henning,  Titus J. Brinker</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：poses severe obstacles, initial melanoma diagnosis, identify melanoma poses, melanoma poses severe, XAI</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Although artificial intelligence (AI) systems have been shown to improve the
accuracy of initial melanoma diagnosis, the lack of transparency in how these
systems identify melanoma poses severe obstacles to user acceptance.
Explainable artificial intelligence (XAI) methods can help to increase
transparency, but most XAI methods are unable to produce precisely located
domain-specific explanations, making the explanations difficult to interpret.
Moreover, the impact of XAI methods on dermatologists has not yet been
evaluated. Extending on two existing classifiers, we developed an XAI system
that produces text and region based explanations that are easily interpretable
by dermatologists alongside its differential diagnoses of melanomas and nevi.
To evaluate this system, we conducted a three-part reader study to assess its
impact on clinicians' diagnostic accuracy, confidence, and trust in the
XAI-support. We showed that our XAI's explanations were highly aligned with
clinicians' explanations and that both the clinicians' trust in the support
system and their confidence in their diagnoses were significantly increased
when using our XAI compared to using a conventional AI system. The clinicians'
diagnostic accuracy was numerically, albeit not significantly, increased. This
work demonstrates that clinicians are willing to adopt such an XAI system,
motivating their future use in the clinic.</p>
  </details>
</details>
<details>
  <summary>105. <b>标题：A Data Augmentation Method and the Embedding Mechanism for Detection and  Classification of Pulmonary Nodules on Small Samples</b></summary>
  <p><b>编号</b>：[357]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12801</p>
  <p><b>作者</b>：Yang Liu,  Yue-Jie Hou,  Chen-Xin Qin,  Xin-Hui Li,  Si-Jing Li,  Bin Wang,  Chi-Chun Zhou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：stages.omputer aided diagnosis, data augmentation method, early stages.omputer aided, augmentation method, embedding mechanism</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Detection of pulmonary nodules by CT is used for screening lung cancer in
early stages.omputer aided diagnosis (CAD) based on deep-learning method can
identify the suspected areas of pulmonary nodules in CT images, thus improving
the accuracy and efficiency of CT diagnosis. The accuracy and robustness of
deep learning models. Method:In this paper, we explore (1) the data
augmentation method based on the generation model and (2) the model structure
improvement method based on the embedding mechanism. Two strategies have been
introduced in this study: a new data augmentation method and a embedding
mechanism. In the augmentation method, a 3D pixel-level statistics algorithm is
proposed to generate pulmonary nodule and by combing the faked pulmonary nodule
and healthy lung, we generate new pulmonary nodule samples. The embedding
mechanism are designed to better understand the meaning of pixels of the
pulmonary nodule samples by introducing hidden variables. Result: The result of
the 3DVNET model with the augmentation method for pulmonary nodule detection
shows that the proposed data augmentation method outperforms the method based
on generative adversarial network (GAN) framework, training accuracy improved
by 1.5%, and with embedding mechanism for pulmonary nodules classification
shows that the embedding mechanism improves the accuracy and robustness for the
classification of pulmonary nodules obviously, the model training accuracy is
close to 1 and the model testing F1-score is 0.90.Conclusion:he proposed data
augmentation method and embedding mechanism are beneficial to improve the
accuracy and robustness of the model, and can be further applied in other
common diagnostic imaging tasks.</p>
  </details>
</details>
<h1>人工智能</h1>
<details>
  <summary>1. <b>标题：Learning and Verification of Task Structure in Instructional Videos</b></summary>
  <p><b>编号</b>：[1]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13519</p>
  <p><b>作者</b>：Medhini Narasimhan,  Licheng Yu,  Sean Bell,  Ning Zhang,  Trevor Darrell</p>
  <p><b>备注</b>：Wesbite at this https URL</p>
  <p><b>关键词</b>：multi-step task models, enormous number, diverse array, array of multi-step, instructional videos</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Given the enormous number of instructional videos available online, learning
a diverse array of multi-step task models from videos is an appealing goal. We
introduce a new pre-trained video model, VideoTaskformer, focused on
representing the semantics and structure of instructional videos. We pre-train
VideoTaskformer using a simple and effective objective: predicting weakly
supervised textual labels for steps that are randomly masked out from an
instructional video (masked step modeling). Compared to prior work which learns
step representations locally, our approach involves learning them globally,
leveraging video of the entire surrounding task as context. From these learned
representations, we can verify if an unseen video correctly executes a given
task, as well as forecast which steps are likely to be taken after a given
step. We introduce two new benchmarks for detecting mistakes in instructional
videos, to verify if there is an anomalous step and if steps are executed in
the right order. We also introduce a long-term forecasting benchmark, where the
goal is to predict long-range future steps from a given step. Our method
outperforms previous baselines on these tasks, and we believe the tasks will be
a valuable way for the community to measure the quality of step
representations. Additionally, we evaluate VideoTaskformer on 3 existing
benchmarks -- procedural activity recognition, step classification, and step
forecasting -- and demonstrate on each that our method outperforms existing
baselines and achieves new state-of-the-art performance.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Three ways to improve feature alignment for open vocabulary detection</b></summary>
  <p><b>编号</b>：[2]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13518</p>
  <p><b>作者</b>：Relja Arandjelović,  Alex Andonian,  Arthur Mensch,  Olivier J. Hénaff,  Jean-Baptiste Alayrac,  Andrew Zisserman</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：open vocabulary detection, zero-shot open vocabulary, vision-text feature alignment, core problem, open vocabulary</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The core problem in zero-shot open vocabulary detection is how to align
visual and text features, so that the detector performs well on unseen classes.
Previous approaches train the feature pyramid and detection head from scratch,
which breaks the vision-text feature alignment established during pretraining,
and struggles to prevent the language model from forgetting unseen classes.
We propose three methods to alleviate these issues. Firstly, a simple scheme
is used to augment the text embeddings which prevents overfitting to a small
number of classes seen during training, while simultaneously saving memory and
computation. Secondly, the feature pyramid network and the detection head are
modified to include trainable gated shortcuts, which encourages vision-text
feature alignment and guarantees it at the start of detection training.
Finally, a self-training approach is used to leverage a larger corpus of
image-text pairs thus improving detection performance on classes with no human
annotated bounding boxes.
Our three methods are evaluated on the zero-shot version of the LVIS
benchmark, each of them showing clear and significant benefits. Our final
network achieves the new stateof-the-art on the mAP-all metric and demonstrates
competitive performance for mAP-rare, as well as superior transfer to COCO and
Objects365.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Towards Solving Fuzzy Tasks with Human Feedback: A Retrospective of the  MineRL BASALT 2022 Competition</b></summary>
  <p><b>编号</b>：[6]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13512</p>
  <p><b>作者</b>：Stephanie Milani,  Anssi Kanervisto,  Karolis Ramanauskas,  Sander Schulhoff,  Brandon Houghton,  Sharada Mohanty,  Byron Galbraith,  Ke Chen,  Yan Song,  Tianze Zhou,  Bingquan Yu,  He Liu,  Kai Guan,  Yujing Hu,  Tangjie Lv,  Federico Malato,  Florian Leopold,  Amogh Raut,  Ville Hautamäki,  Andrew Melnik,  Shu Ishida,  João F. Henriques,  Robert Klassert,  Walter Laurito,  Ellen Novoseller,  Vinicius G. Goecks,  Nicholas Waytowich,  David Watkins,  Josh Miller,  Rohin Shah</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：fine-tuning foundation models, MineRL BASALT Competition, fine-tuning foundation, facilitate research, foundation models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To facilitate research in the direction of fine-tuning foundation models from
human feedback, we held the MineRL BASALT Competition on Fine-Tuning from Human
Feedback at NeurIPS 2022. The BASALT challenge asks teams to compete to develop
algorithms to solve tasks with hard-to-specify reward functions in Minecraft.
Through this competition, we aimed to promote the development of algorithms
that use human feedback as channels to learn the desired behavior. We describe
the competition and provide an overview of the top solutions. We conclude by
discussing the impact of the competition and future directions for improvement.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Neural Preset for Color Style Transfer</b></summary>
  <p><b>编号</b>：[7]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13511</p>
  <p><b>作者</b>：Zhanghan Ke,  Yuhao Liu,  Lei Zhu,  Nanxuan Zhao,  Rynson W.H. Lau</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：vast memory requirement, Neural Preset technique, Neural Preset, technique to address, address the limitations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we present a Neural Preset technique to address the
limitations of existing color style transfer methods, including visual
artifacts, vast memory requirement, and slow style switching speed. Our method
is based on two core designs. First, we propose Deterministic Neural Color
Mapping (DNCM) to consistently operate on each pixel via an image-adaptive
color mapping matrix, avoiding artifacts and supporting high-resolution inputs
with a small memory footprint. Second, we develop a two-stage pipeline by
dividing the task into color normalization and stylization, which allows
efficient style switching by extracting color styles as presets and reusing
them on normalized input images. Due to the unavailability of pairwise
datasets, we describe how to train Neural Preset via a self-supervised
strategy. Various advantages of Neural Preset over existing methods are
demonstrated through comprehensive evaluations. Besides, we show that our
trained model can naturally support multiple applications without fine-tuning,
including low-light image enhancement, underwater image correction, image
dehazing, and image harmonization.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：DreamBooth3D: Subject-Driven Text-to-3D Generation</b></summary>
  <p><b>编号</b>：[10]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13508</p>
  <p><b>作者</b>：Amit Raj,  Srinivas Kaza,  Ben Poole,  Michael Niemeyer,  Nataniel Ruiz,  Ben Mildenhall,  Shiran Zada,  Kfir Aberman,  Michael Rubinstein,  Jonathan Barron,  Yuanzhen Li,  Varun Jampani</p>
  <p><b>备注</b>：Project page at this https URL Video Summary at this https URL</p>
  <p><b>关键词</b>：casually captured images, casually captured, generative models, captured images, approach combines recent</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present DreamBooth3D, an approach to personalize text-to-3D generative
models from as few as 3-6 casually captured images of a subject. Our approach
combines recent advances in personalizing text-to-image models (DreamBooth)
with text-to-3D generation (DreamFusion). We find that naively combining these
methods fails to yield satisfactory subject-specific 3D assets due to
personalized text-to-image models overfitting to the input viewpoints of the
subject. We overcome this through a 3-stage optimization strategy where we
jointly leverage the 3D consistency of neural radiance fields together with the
personalization capability of text-to-image models. Our method can produce
high-quality, subject-specific 3D assets with text-driven modifications such as
novel poses, colors and attributes that are not seen in any of the input images
of the subject.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：TriPlaneNet: An Encoder for EG3D Inversion</b></summary>
  <p><b>编号</b>：[16]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13497</p>
  <p><b>作者</b>：Ananta R. Bhattarai,  Matthias Nießner,  Artem Sevastopolsky</p>
  <p><b>备注</b>：Video: this https URL Project page: this https URL</p>
  <p><b>关键词</b>：high-fidelity generative modeling, Recent progress, progress in NeRF-based, high-resolution and high-fidelity, modeling of human</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent progress in NeRF-based GANs has introduced a number of approaches for
high-resolution and high-fidelity generative modeling of human heads with a
possibility for novel view rendering. At the same time, one must solve an
inverse problem to be able to re-render or modify an existing image or video.
Despite the success of universal optimization-based methods for 2D GAN
inversion, those, applied to 3D GANs, may fail to produce 3D-consistent
renderings. Fast encoder-based techniques, such as those developed for
StyleGAN, may also be less appealing due to the lack of identity preservation.
In our work, we introduce a real-time method that bridges the gap between the
two approaches by directly utilizing the tri-plane representation introduced
for EG3D generative model. In particular, we build upon a feed-forward
convolutional encoder for the latent code and extend it with a
fully-convolutional predictor of tri-plane numerical offsets. As shown in our
work, the renderings are similar in quality to optimization-based techniques
and significantly outperform the baselines for novel view. As we empirically
prove, this is a consequence of directly operating in the tri-plane space, not
in the GAN parameter space, while making use of an encoder-based trainable
approach.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：The effectiveness of MAE pre-pretraining for billion-scale pretraining</b></summary>
  <p><b>编号</b>：[17]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13496</p>
  <p><b>作者</b>：Mannat Singh,  Quentin Duval,  Kalyan Vasudev Alwala,  Haoqi Fan,  Vaibhav Aggarwal,  Aaron Adcock,  Armand Joulin,  Piotr Dollár,  Christoph Feichtenhofer,  Ross Girshick,  Rohit Girdhar,  Ishan Misra</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：revisits the standard, paper revisits, computer vision, model, billions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper revisits the standard pretrain-then-finetune paradigm used in
computer vision for visual recognition tasks. Typically, state-of-the-art
foundation models are pretrained using large scale (weakly) supervised datasets
with billions of images. We introduce an additional pre-pretraining stage that
is simple and uses the self-supervised MAE technique to initialize the model.
While MAE has only been shown to scale with the size of models, we find that it
scales with the size of the training dataset as well. Thus, our MAE-based
pre-pretraining scales with both model and data size making it applicable for
training foundation models. Pre-pretraining consistently improves both the
model convergence and the downstream transfer performance across a range of
model scales (millions to billions of parameters), and dataset sizes (millions
to billions of images). We measure the effectiveness of pre-pretraining on 10
different visual recognition tasks spanning image classification, video
recognition, object detection, low-shot classification and zero-shot
recognition. Our largest model achieves new state-of-the-art results on
iNaturalist-18 (91.3%), 1-shot ImageNet-1k (62.1%), and zero-shot transfer on
Food-101 (96.0%). Our study reveals that model initialization plays a
significant role, even for web-scale pretraining with billions of images.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Attention! Dynamic Epistemic Logic Models of (In)attentive Agents</b></summary>
  <p><b>编号</b>：[19]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13494</p>
  <p><b>作者</b>：Gaia Belardinelli,  Thomas Bolander</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：crucial cognitive ability, information we observe, crucial cognitive, cognitive ability, ability that limits</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Attention is the crucial cognitive ability that limits and selects what
information we observe. Previous work by Bolander et al. (2016) proposes a
model of attention based on dynamic epistemic logic (DEL) where agents are
either fully attentive or not attentive at all. While introducing the realistic
feature that inattentive agents believe nothing happens, the model does not
represent the most essential aspect of attention: its selectivity. Here, we
propose a generalization that allows for paying attention to subsets of atomic
formulas. We introduce the corresponding logic for propositional attention, and
show its axiomatization to be sound and complete. We then extend the framework
to account for inattentive agents that, instead of assuming nothing happens,
may default to a specific truth-value of what they failed to attend to (a sort
of prior concerning the unattended atoms). This feature allows for a more
cognitively plausible representation of the inattentional blindness phenomenon,
where agents end up with false beliefs due to their failure to attend to
conspicuous but unexpected events. Both versions of the model define
attention-based learning through appropriate DEL event models based on a few
and clear edge principles. While the size of such event models grow
exponentially both with the number of agents and the number of atoms, we
introduce a new logical language for describing event models syntactically and
show that using this language our event models can be represented linearly in
the number of agents and atoms. Furthermore, representing our event models
using this language is achieved by a straightforward formalisation of the
aforementioned edge principles.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Boosting Reinforcement Learning and Planning with Demonstrations: A  Survey</b></summary>
  <p><b>编号</b>：[22]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13489</p>
  <p><b>作者</b>：Tongzhou Mu,  Hao Su</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：tremendous success recently, complex environments, tremendous success, impractical or inefficient, inefficient in complex</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Although reinforcement learning has seen tremendous success recently, this
kind of trial-and-error learning can be impractical or inefficient in complex
environments. The use of demonstrations, on the other hand, enables agents to
benefit from expert knowledge rather than having to discover the best action to
take through exploration. In this survey, we discuss the advantages of using
demonstrations in sequential decision making, various ways to apply
demonstrations in learning-based decision making paradigms (for example,
reinforcement learning and planning in the learned models), and how to collect
the demonstrations in various scenarios. Additionally, we exemplify a practical
pipeline for generating and utilizing demonstrations in the recently proposed
ManiSkill robot learning benchmark.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations</b></summary>
  <p><b>编号</b>：[25]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13483</p>
  <p><b>作者</b>：Joy Hsu,  Jiayuan Mao,  Jiajun Wu</p>
  <p><b>备注</b>：In CVPR 2023</p>
  <p><b>关键词</b>：visually grounded dialogues, artificial intelligence tasks, embodied manipulation, wide range, range of artificial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Grounding object properties and relations in 3D scenes is a prerequisite for
a wide range of artificial intelligence tasks, such as visually grounded
dialogues and embodied manipulation. However, the variability of the 3D domain
induces two fundamental challenges: 1) the expense of labeling and 2) the
complexity of 3D grounded language. Hence, essential desiderata for models are
to be data-efficient, generalize to different data distributions and tasks with
unseen semantic forms, as well as ground complex language semantics (e.g.,
view-point anchoring and multi-object reference). To address these challenges,
we propose NS3D, a neuro-symbolic framework for 3D grounding. NS3D translates
language into programs with hierarchical structures by leveraging large
language-to-code models. Different functional modules in the programs are
implemented as neural networks. Notably, NS3D extends prior neuro-symbolic
visual reasoning methods by introducing functional modules that effectively
reason about high-arity relations (i.e., relations among more than two
objects), key in disambiguating objects in complex 3D scenes. Modular and
compositional architecture enables NS3D to achieve state-of-the-art results on
the ReferIt3D view-dependence task, a 3D referring expression comprehension
benchmark. Importantly, NS3D shows significantly improved performance on
settings of data-efficiency and generalization, and demonstrate zero-shot
transfer to an unseen 3D question-answering task.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：TactoFind: A Tactile Only System for Object Retrieval</b></summary>
  <p><b>编号</b>：[26]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13482</p>
  <p><b>作者</b>：Sameer Pai,  Tao Chen,  Megha Tippur,  Edward Adelson,  Abhishek Gupta,  Pulkit Agrawal</p>
  <p><b>备注</b>：Accepted in ICRA 2023</p>
  <p><b>关键词</b>：sensing is absent, move freely, study the problem, retrieval in scenarios, shapes are unknown</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the problem of object retrieval in scenarios where visual sensing is
absent, object shapes are unknown beforehand and objects can move freely, like
grabbing objects out of a drawer. Successful solutions require localizing free
objects, identifying specific object instances, and then grasping the
identified objects, only using touch feedback. Unlike vision, where cameras can
observe the entire scene, touch sensors are local and only observe parts of the
scene that are in contact with the manipulator. Moreover, information gathering
via touch sensors necessitates applying forces on the touched surface which may
disturb the scene itself. Reasoning with touch, therefore, requires careful
exploration and integration of information over time -- a challenge we tackle.
We present a system capable of using sparse tactile feedback from fingertip
touch sensors on a dexterous hand to localize, identify and grasp novel objects
without any visual feedback. Videos are available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Plotting Behind the Scenes: Towards Learnable Game Engines</b></summary>
  <p><b>编号</b>：[30]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13472</p>
  <p><b>作者</b>：Willi Menapace,  Aliaksandr Siarohin,  Stéphane Lathuilière,  Panos Achlioptas,  Vladislav Golyanik,  Elisa Ricci,  Sergey Tulyakov</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Game, Learnable Game Engine, computer graphics, powerful tools, tools in computer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Game engines are powerful tools in computer graphics. Their power comes at
the immense cost of their development. In this work, we present a framework to
train game-engine-like neural models, solely from monocular annotated videos.
The result-a Learnable Game Engine (LGE)-maintains states of the scene, objects
and agents in it, and enables rendering the environment from a controllable
viewpoint. Similarly to a game engine, it models the logic of the game and the
underlying rules of physics, to make it possible for a user to play the game by
specifying both high- and low-level action sequences. Most captivatingly, our
LGE unlocks the director's mode, where the game is played by plotting behind
the scenes, specifying high-level actions and goals for the agents in the form
of language and desired states. This requires learning "game AI", encapsulated
by our animation model, to navigate the scene using high-level constraints,
play against an adversary, devise the strategy to win a point. The key to
learning such game AI is the exploitation of a large and diverse text corpus,
collected in this work, describing detailed actions in a game and used to train
our animation model. To render the resulting state of the environment and its
agents, we use a compositional NeRF representation used in our synthesis model.
To foster future research, we present newly collected, annotated and calibrated
large-scale Tennis and Minecraft datasets. Our method significantly outperforms
existing neural video game simulators in terms of rendering quality. Besides,
our LGEs unlock applications beyond capabilities of the current state of the
art. Our framework, data, and models are available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Extracting Physical Rehabilitation Exercise Information from Clinical  Notes: a Comparison of Rule-Based and Machine Learning Natural Language  Processing Techniques</b></summary>
  <p><b>编号</b>：[32]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13466</p>
  <p><b>作者</b>：Stephen W. Shaffran,  Fengyi Gao,  Parker E. Denny,  Bayan M. Aldhahwani,  Allyn Bove,  Shyam Visweswaran,  Yanshan Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：plays a crucial, crucial role, Physical rehabilitation plays, recovery process, post-stroke patients</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Physical rehabilitation plays a crucial role in the recovery process of
post-stroke patients. By personalizing therapies for patients leveraging
predictive modeling and electronic health records (EHRs), healthcare providers
can make the rehabilitation process more efficient. Before predictive modeling
can provide decision support for the assignment of treatment plans, automated
methods are necessary to extract physical rehabilitation exercise information
from unstructured EHRs. We introduce a rule-based natural language processing
algorithm to annotate therapeutic procedures for stroke patients and compare it
to several small machine learning models. We find that our algorithm
outperforms these models in extracting half of the concepts where sufficient
data is available, and individual exercise descriptions can be assigned binary
labels with an f-score of no less than 0.75 per concept. More research needs to
be done before these algorithms can be deployed on unlabeled documents, but
current progress gives promise to the potential of precision rehabilitation
research.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Deep RL with Hierarchical Action Exploration for Dialogue Generation</b></summary>
  <p><b>编号</b>：[33]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13465</p>
  <p><b>作者</b>：Itsugun Cho,  Ryota Takahashi,  Yusaku Yanase,  Hiroaki Saito</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：approximate dynamic programming, dynamic programming applied, natural language action, language action space, generation involves policy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Conventionally, since the natural language action space is astronomical,
approximate dynamic programming applied to dialogue generation involves policy
improvement with action sampling. However, such a practice is inefficient for
reinforcement learning (RL) because the eligible (high action value) responses
are very sparse, and the greedy policy sustained by the random sampling is
flabby. This paper shows that the performance of dialogue policy positively
correlated with sampling size by theoretical and experimental. We introduce a
novel dual-granularity Q-function to alleviate this limitation by exploring the
most promising response category to intervene in the sampling. It extracts the
actions following the grained hierarchy, which can achieve the optimum with
fewer policy iterations. Our approach learns in the way of offline RL from
multiple reward functions designed to recognize human emotional details.
Empirical studies demonstrate that our algorithm outperforms the baseline
methods. Further verification presents that ours can generate responses with
higher expected rewards and controllability.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：DDT: A Diffusion-Driven Transformer-based Framework for Human Mesh  Recovery from a Video</b></summary>
  <p><b>编号</b>：[56]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13397</p>
  <p><b>作者</b>：Ce Zheng,  Guo-Jun Qi,  Chen Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：rich human body, human-computer interaction, human body information, Human mesh recovery, virtual reality</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Human mesh recovery (HMR) provides rich human body information for various
real-world applications such as gaming, human-computer interaction, and virtual
reality. Compared to single image-based methods, video-based methods can
utilize temporal information to further improve performance by incorporating
human body motion priors. However, many-to-many approaches such as VIBE suffer
from motion smoothness and temporal inconsistency. While many-to-one approaches
such as TCMR and MPS-Net rely on the future frames, which is non-causal and
time inefficient during inference. To address these challenges, a novel
Diffusion-Driven Transformer-based framework (DDT) for video-based HMR is
presented. DDT is designed to decode specific motion patterns from the input
sequence, enhancing motion smoothness and temporal consistency. As a
many-to-many approach, the decoder of our DDT outputs the human mesh of all the
frames, making DDT more viable for real-world applications where time
efficiency is crucial and a causal model is desired. Extensive experiments are
conducted on the widely used datasets (Human3.6M, MPI-INF-3DHP, and 3DPW),
which demonstrated the effectiveness and efficiency of our DDT.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Planning for Manipulation among Movable Objects: Deciding Which Objects  Go Where, in What Order, and How</b></summary>
  <p><b>编号</b>：[62]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13385</p>
  <p><b>作者</b>：Dhruv Saxena,  Maxim Likhachev</p>
  <p><b>备注</b>：Accepted for publication at the International Conference on Automated Planning and Scheduling (ICAPS), 2023</p>
  <p><b>关键词</b>：style robot manipulation, robot manipulation tasks, lean or topple, cluttered and confined, Multi-Agent Pathfinding MAPF</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We are interested in pick-and-place style robot manipulation tasks in
cluttered and confined 3D workspaces among movable objects that may be
rearranged by the robot and may slide, tilt, lean or topple. A recently
proposed algorithm, M4M, determines which objects need to be moved and where by
solving a Multi-Agent Pathfinding MAPF abstraction of this problem. It then
utilises a nonprehensile push planner to compute actions for how the robot
might realise these rearrangements and a rigid body physics simulator to check
whether the actions satisfy physics constraints encoded in the problem.
However, M4M greedily commits to valid pushes found during planning, and does
not reason about orderings over pushes if multiple objects need to be
rearranged. Furthermore, M4M does not reason about other possible MAPF
solutions that lead to different rearrangements and pushes. In this paper, we
extend M4M and present Enhanced-M4M (E-M4M) -- a systematic graph search-based
solver that searches over orderings of pushes for movable objects that need to
be rearranged and different possible rearrangements of the scene. We introduce
several algorithmic optimisations to circumvent the increased computational
complexity, discuss the space of problems solvable by E-M4M and show that
experimentally, both on the real robot and in simulation, it significantly
outperforms the original M4M algorithm, as well as other state-of-the-art
alternatives when dealing with complex scenes.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Practical and Ethical Challenges of Large Language Models in Education:  A Systematic Literature Review</b></summary>
  <p><b>编号</b>：[65]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13379</p>
  <p><b>作者</b>：Lixiang Yan,  Lele Sha,  Linxuan Zhao,  Yuheng Li,  Roberto Martinez-Maldonado,  Guanliang Chen,  Xinyu Li,  Yueqiao Jin,  Dragan Gašević</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：analysing textual content, large language models, textual content, Educational technology innovations, based on large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Educational technology innovations that have been developed based on large
language models (LLMs) have shown the potential to automate the laborious
process of generating and analysing textual content. While various innovations
have been developed to automate a range of educational tasks (e.g., question
generation, feedback provision, and essay grading), there are concerns
regarding the practicality and ethicality of these innovations. Such concerns
may hinder future research and the adoption of LLMs-based innovations in
authentic educational contexts. To address this, we conducted a systematic
literature review of 118 peer-reviewed papers published since 2017 to pinpoint
the current state of research on using LLMs to automate and support educational
tasks. The practical and ethical challenges of LLMs-based innovations were also
identified by assessing their technological readiness, model performance,
replicability, system transparency, privacy, equality, and beneficence. The
findings were summarised into three recommendations for future studies,
including updating existing innovations with state-of-the-art models (e.g.,
GPT-3), embracing the initiative of open-sourcing models/systems, and adopting
a human-centred approach throughout the developmental process. These
recommendations could support future research to develop practical and ethical
innovations for supporting diverse educational tasks and benefiting students,
teachers, and institutions.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Capabilities of GPT-4 on Medical Challenge Problems</b></summary>
  <p><b>编号</b>：[67]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13375</p>
  <p><b>作者</b>：Harsha Nori,  Nicholas King,  Scott Mayer McKinney,  Dean Carignan,  Eric Horvitz</p>
  <p><b>备注</b>：33 pages, 15 figures</p>
  <p><b>关键词</b>：natural language understanding, demonstrated remarkable capabilities, Large language models, Large language, natural language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have demonstrated remarkable capabilities in
natural language understanding and generation across various domains, including
medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art
LLM, on medical competency examinations and benchmark datasets. GPT-4 is a
general-purpose model that is not specialized for medical problems through
training or engineered to solve clinical tasks. Our analysis covers two sets of
official practice materials for the USMLE, a three-step examination program
used to assess clinical competency and grant licensure in the United States. We
also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond
measuring model performance, experiments were conducted to investigate the
influence of test questions containing both text and images on model
performance, probe for memorization of content during training, and study
probability calibration, which is of critical importance in high-stakes
applications like medicine. Our results show that GPT-4, without any
specialized prompt crafting, exceeds the passing score on USMLE by over 20
points and outperforms earlier general-purpose models (GPT-3.5) as well as
models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned
version of Flan-PaLM 540B). In addition, GPT-4 is significantly better
calibrated than GPT-3.5, demonstrating a much-improved ability to predict the
likelihood that its answers are correct. We also explore the behavior of the
model qualitatively through a case study that shows the ability of GPT-4 to
explain medical reasoning, personalize explanations to students, and
interactively craft new counterfactual scenarios around a medical case.
Implications of the findings are discussed for potential uses of GPT-4 in
medical education, assessment, and clinical practice, with appropriate
attention to challenges of accuracy and safety.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Fine-tuning ClimateBert transformer with ClimaText for the disclosure  analysis of climate-related financial risks</b></summary>
  <p><b>编号</b>：[68]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13373</p>
  <p><b>作者</b>：Eduardo C. Garrido-Merchán,  Cristina González-Barthe,  María Coronado Vaca</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：climate-related financial risks, recent years, financial, climate-related financial, growing demand</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years there has been a growing demand from financial agents,
especially from particular and institutional investors, for companies to report
on climate-related financial risks. A vast amount of information, in text
format, can be expected to be disclosed in the short term by firms in order to
identify these types of risks in their financial and non financial reports,
particularly in response to the growing regulation that is being passed on the
matter. To this end, this paper applies state-of-the-art NLP techniques to
achieve the detection of climate change in text corpora. We use transfer
learning to fine-tune two transformer models, BERT and ClimateBert -a recently
published DistillRoBERTa-based model that has been specifically tailored for
climate text classification-. These two algorithms are based on the transformer
architecture which enables learning the contextual relationships between words
in a text. We carry out the fine-tuning process of both models on the novel
Clima-Text database, consisting of data collected from Wikipedia, 10K Files
Reports and web-based claims. Our text classification model obtained from the
ClimateBert fine-tuning process on ClimaText, outperforms the models created
with BERT and the current state-of-the-art transformer in this particular
problem. Our study is the first one to implement on the ClimaText database the
recently published ClimateBert algorithm. Based on our results, it can be said
that ClimateBert fine-tuned on ClimaText is an outstanding tool within the NLP
pre-trained transformer models that may and should be used by investors,
institutional agents and companies themselves to monitor the disclosure of
climate risk in financial reports. In addition, our transfer learning
methodology is cheap in computational terms, thus allowing any organization to
perform it.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Requirement Formalisation using Natural Language Processing and Machine  Learning: A Systematic Review</b></summary>
  <p><b>编号</b>：[72]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13365</p>
  <p><b>作者</b>：Shekoufeh Kolahdouz-Rahimi,  Kevin Lano,  Chenghua Lin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：automatic Requirement Formalisation, software development methodologies, development methodologies attracts, methodologies attracts developers, Requirement Formalisation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Improvement of software development methodologies attracts developers to
automatic Requirement Formalisation (RF) in the Requirement Engineering (RE)
field. The potential advantages by applying Natural Language Processing (NLP)
and Machine Learning (ML) in reducing the ambiguity and incompleteness of
requirement written in natural languages is reported in different studies. The
goal of this paper is to survey and classify existing work on NLP and ML for
RF, identifying challenges in this domain and providing promising future
research directions. To achieve this, we conducted a systematic literature
review to outline the current state-of-the-art of NLP and ML techniques in RF
by selecting 257 papers from common used libraries. The search result is
filtered by defining inclusion and exclusion criteria and 47 relevant studies
between 2012 and 2022 are selected. We found that heuristic NLP approaches are
the most common NLP techniques used for automatic RF, primary operating on
structured and semi-structured data. This study also revealed that Deep
Learning (DL) technique are not widely used, instead classical ML techniques
are predominant in the surveyed studies. More importantly, we identified the
difficulty of comparing the performance of different approaches due to the lack
of standard benchmark cases for RF.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：POTTER: Pooling Attention Transformer for Efficient Human Mesh Recovery</b></summary>
  <p><b>编号</b>：[76]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13357</p>
  <p><b>作者</b>：Ce Zheng,  Xianpeng Liu,  Guo-Jun Qi,  Chen Chen</p>
  <p><b>备注</b>：CVPR 2023</p>
  <p><b>关键词</b>：achieved SOTA performance, human mesh recovery, accurate human mesh, human mesh, mesh recovery</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transformer architectures have achieved SOTA performance on the human mesh
recovery (HMR) from monocular images. However, the performance gain has come at
the cost of substantial memory and computational overhead. A lightweight and
efficient model to reconstruct accurate human mesh is needed for real-world
applications. In this paper, we propose a pure transformer architecture named
POoling aTtention TransformER (POTTER) for the HMR task from single images.
Observing that the conventional attention module is memory and computationally
expensive, we propose an efficient pooling attention module, which
significantly reduces the memory and computational cost without sacrificing
performance. Furthermore, we design a new transformer architecture by
integrating a High-Resolution (HR) stream for the HMR task. The high-resolution
local and global features from the HR stream can be utilized for recovering
more accurate human mesh. Our POTTER outperforms the SOTA method METRO by only
requiring 7% of total parameters and 14% of the Multiply-Accumulate Operations
on the Human3.6M (PA-MPJPE metric) and 3DPW (all three metrics) datasets. The
project webpage is this https URL.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Revealing Weaknesses of Vietnamese Language Models Through Unanswerable  Questions in Machine Reading Comprehension</b></summary>
  <p><b>编号</b>：[77]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13355</p>
  <p><b>作者</b>：Son Quoc Tran,  Phong Nguyen-Thuan Do,  Kiet Van Nguyen,  Ngan Luu-Thuy Nguyen</p>
  <p><b>备注</b>：Accepted at The 2023 EACL Student Research Workshop</p>
  <p><b>关键词</b>：Vietnamese Machine Reading, Machine Reading Comprehension, Machine Reading, multilinguality significantly restricts, Reading Comprehension</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Although the curse of multilinguality significantly restricts the language
abilities of multilingual models in monolingual settings, researchers now still
have to rely on multilingual models to develop state-of-the-art systems in
Vietnamese Machine Reading Comprehension. This difficulty in researching is
because of the limited number of high-quality works in developing Vietnamese
language models. In order to encourage more work in this research field, we
present a comprehensive analysis of language weaknesses and strengths of
current Vietnamese monolingual models using the downstream task of Machine
Reading Comprehension. From the analysis results, we suggest new directions for
developing Vietnamese language models. Besides this main contribution, we also
successfully reveal the existence of artifacts in Vietnamese Machine Reading
Comprehension benchmarks and suggest an urgent need for new high-quality
benchmarks to track the progress of Vietnamese Machine Reading Comprehension.
Moreover, we also introduced a minor but valuable modification to the process
of annotating unanswerable questions for Machine Reading Comprehension from
previous work. Our proposed modification helps improve the quality of
unanswerable questions to a higher level of difficulty for Machine Reading
Comprehension systems to solve.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Planning for Complex Non-prehensile Manipulation Among Movable Objects  by Interleaving Multi-Agent Pathfinding and Physics-Based Simulation</b></summary>
  <p><b>编号</b>：[79]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13352</p>
  <p><b>作者</b>：Dhruv Mauria Saxena,  Maxim Likhachev</p>
  <p><b>备注</b>：Accepted for publication at the IEEE International Conference on Robotics and Automation (ICRA), 2023</p>
  <p><b>关键词</b>：heavy clutter require, clutter require robots, heavy clutter, clutter require, potential contacts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Real-world manipulation problems in heavy clutter require robots to reason
about potential contacts with objects in the environment. We focus on
pick-and-place style tasks to retrieve a target object from a shelf where some
`movable' objects must be rearranged in order to solve the task. In particular,
our motivation is to allow the robot to reason over and consider non-prehensile
rearrangement actions that lead to complex robot-object and object-object
interactions where multiple objects might be moved by the robot simultaneously,
and objects might tilt, lean on each other, or topple. To support this, we
query a physics-based simulator to forward simulate these interaction dynamics
which makes action evaluation during planning computationally very expensive.
To make the planner tractable, we establish a connection between the domain of
Manipulation Among Movable Objects and Multi-Agent Pathfinding that lets us
decompose the problem into two phases our M4M algorithm iterates over. First we
solve a multi-agent planning problem that reasons about the configurations of
movable objects but does not forward simulate a physics model. Next, an arm
motion planning problem is solved that uses a physics-based simulator but does
not search over possible configurations of movable objects. We run simulated
and real-world experiments with the PR2 robot and compare against relevant
baseline algorithms. Our results highlight that M4M generates complex 3D
interactions, and solves at least twice as many problems as the baselines with
competitive performance.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Audio Diffusion Model for Speech Synthesis: A Survey on Text To Speech  and Speech Enhancement in Generative AI</b></summary>
  <p><b>编号</b>：[83]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13336</p>
  <p><b>作者</b>：Chenshuang Zhang,  Chaoning Zhang,  Sheng Zheng,  Mengchun Zhang,  Maryam Qamar,  Sung-Ho Bae,  In So Kweon</p>
  <p><b>备注</b>：18 pages</p>
  <p><b>关键词</b>：demonstrated impressive performance, interesting direction, diffusion model, demonstrated impressive, impressive performance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generative AI has demonstrated impressive performance in various fields,
among which speech synthesis is an interesting direction. With the diffusion
model as the most popular generative model, numerous works have attempted two
active tasks: text to speech and speech enhancement. This work conducts a
survey on audio diffusion model, which is complementary to existing surveys
that either lack the recent progress of diffusion-based speech synthesis or
highlight an overall picture of applying diffusion model in multiple fields.
Specifically, this work first briefly introduces the background of audio and
diffusion model. As for the text-to-speech task, we divide the methods into
three categories based on the stage where diffusion model is adopted: acoustic
model, vocoder and end-to-end framework. Moreover, we categorize various speech
enhancement tasks by either certain signals are removed or added into the input
speech. Comparisons of experimental results and discussions are also covered in
this survey.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Decentralized Adversarial Training over Graphs</b></summary>
  <p><b>编号</b>：[84]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13326</p>
  <p><b>作者</b>：Ying Cao,  Elsa Rizk,  Stefan Vlaski,  Ali H. Sayed</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2303.01936</p>
  <p><b>关键词</b>：attracting considerable attention, recent years, vulnerability of machine, attracting considerable, considerable attention</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The vulnerability of machine learning models to adversarial attacks has been
attracting considerable attention in recent years. Most existing studies focus
on the behavior of stand-alone single-agent learners. In comparison, this work
studies adversarial training over graphs, where individual agents are subjected
to perturbations of varied strength levels across space. It is expected that
interactions by linked agents, and the heterogeneity of the attack models that
are possible over the graph, can help enhance robustness in view of the
coordination power of the group. Using a min-max formulation of diffusion
learning, we develop a decentralized adversarial training framework for
multi-agent systems. We analyze the convergence properties of the proposed
scheme for both convex and non-convex environments, and illustrate the enhanced
robustness to adversarial attacks.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：DARE-GRAM : Unsupervised Domain Adaptation Regression by Aligning  Inverse Gram Matrices</b></summary>
  <p><b>编号</b>：[85]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13325</p>
  <p><b>作者</b>：Ismail Nejjar,  Qin Wang,  Olga Fink</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：labeled source dataset, unlabelled target dataset, Domain Adaptation Regression, Unsupervised Domain Adaptation, Domain Adaptation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Unsupervised Domain Adaptation Regression (DAR) aims to bridge the domain gap
between a labeled source dataset and an unlabelled target dataset for
regression problems. Recent works mostly focus on learning a deep feature
encoder by minimizing the discrepancy between source and target features. In
this work, we present a different perspective for the DAR problem by analyzing
the closed-form ordinary least square~(OLS) solution to the linear regressor in
the deep domain adaptation context. Rather than aligning the original feature
embedding space, we propose to align the inverse Gram matrix of the features,
which is motivated by its presence in the OLS solution and the Gram matrix's
ability to capture the feature correlations. Specifically, we propose a simple
yet effective DAR method which leverages the pseudo-inverse low-rank property
to align the scale and angle in a selected subspace generated by the
pseudo-inverse Gram matrix of the two domains. We evaluate our method on three
domain adaptation regression benchmarks. Experimental results demonstrate that
our method achieves state-of-the-art performance. Our code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：QDP: Learning to Sequentially Optimise Quasi-Static and Dynamic  Manipulation Primitives for Robotic Cloth Manipulation</b></summary>
  <p><b>编号</b>：[86]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13320</p>
  <p><b>作者</b>：David Blanco-Mulero,  Gokhan Alcan,  Fares J. Abu-Dakka,  Ville Kyrki</p>
  <p><b>备注</b>：8 pages, 7 figures. Supplementary material available at this https URL</p>
  <p><b>关键词</b>：Pre-defined manipulation primitives, dynamic manipulation primitives, manipulation primitives, primitives, Pre-defined manipulation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pre-defined manipulation primitives are widely used for cloth manipulation.
However, cloth properties such as its stiffness or density can highly impact
the performance of these primitives. Although existing solutions have tackled
the parameterisation of pick and place locations, the effect of factors such as
the velocity or trajectory of quasi-static and dynamic manipulation primitives
has been neglected. Choosing appropriate values for these parameters is crucial
to cope with the range of materials present in house-hold cloth objects. To
address this challenge, we introduce the Quasi-Dynamic Parameterisable (QDP)
method, which optimises parameters such as the motion velocity in addition to
the pick and place positions of quasi-static and dynamic manipulation
primitives. In this work, we leverage the framework of Sequential Reinforcement
Learning to decouple sequentially the parameters that compose the primitives.
To evaluate the effectiveness of the method we focus on the task of cloth
unfolding with a robotic arm in simulation and real-world experiments. Our
results in simulation show that by deciding the optimal parameters for the
primitives the performance can improve by 20% compared to sub-optimal ones.
Real-world results demonstrate the advantage of modifying the velocity and
height of manipulation primitives for cloths with different mass, stiffness,
shape and size. Supplementary material, videos, and code, can be found at
this https URL.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Leveraging Foundation Models for Clinical Text Analysis</b></summary>
  <p><b>编号</b>：[89]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13314</p>
  <p><b>作者</b>：Shaina Raza,  Syed Raza Bashir</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：health concern globally, significant public health, public health concern, extracting relevant information, concern globally</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Infectious diseases are a significant public health concern globally, and
extracting relevant information from scientific literature can facilitate the
development of effective prevention and treatment strategies. However, the
large amount of clinical data available presents a challenge for information
extraction. To address this challenge, this study proposes a natural language
processing (NLP) framework that uses a pre-trained transformer model fine-tuned
on task-specific data to extract key information related to infectious diseases
from free-text clinical data. The proposed framework includes three components:
a data layer for preparing datasets from clinical texts, a foundation model
layer for entity extraction, and an assessment layer for performance analysis.
The results of the evaluation indicate that the proposed method outperforms
standard methods, and leveraging prior knowledge through the pre-trained
transformer model makes it useful for investigating other infectious diseases
in the future.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Innovation Slowdown: Decelerating Concept Creation and Declining  Originality in New Technological Concepts</b></summary>
  <p><b>编号</b>：[93]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13300</p>
  <p><b>作者</b>：Serhad Sarica,  Jianxi Luo</p>
  <p><b>备注</b>：submitted to Design Science</p>
  <p><b>关键词</b>：design reuses, lead to exponential, exponential growth, technological concepts, newly created concepts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The creation of new technological concepts through design reuses,
recombination, and synthesis of prior concepts to create new ones may lead to
exponential growth of the concept space over time. However, our statistical
analysis of a large-scale technology semantic network consisting of over four
million concepts from patent texts found evidence of a persistent deceleration
in the pace of concept creation and a decline in the originality of newly
created concepts. These trends may be attributed to the limitations of human
intelligence in innovating beyond an expanding space of prior art. To sustain
innovation, we recommend the development and implementation of creative
artificial intelligence that can augment various aspects of the innovation
process, including learning, creation, and evaluation.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Reckoning with the Disagreement Problem: Explanation Consensus as a  Training Objective</b></summary>
  <p><b>编号</b>：[94]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13299</p>
  <p><b>作者</b>：Avi Schwarzschild,  Max Cembalest,  Karthik Rao,  Keegan Hines,  John Dickerson</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：neural networks increasingly, networks increasingly make, increasingly make critical, make critical decisions, high-stakes settings</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As neural networks increasingly make critical decisions in high-stakes
settings, monitoring and explaining their behavior in an understandable and
trustworthy manner is a necessity. One commonly used type of explainer is post
hoc feature attribution, a family of methods for giving each feature in an
input a score corresponding to its influence on a model's output. A major
limitation of this family of explainers in practice is that they can disagree
on which features are more important than others. Our contribution in this
paper is a method of training models with this disagreement problem in mind. We
do this by introducing a Post hoc Explainer Agreement Regularization (PEAR)
loss term alongside the standard term corresponding to accuracy, an additional
term that measures the difference in feature attribution between a pair of
explainers. We observe on three datasets that we can train a model with this
loss term to improve explanation consensus on unseen data, and see improved
consensus between explainers other than those used in the loss term. We examine
the trade-off between improved consensus and model performance. And finally, we
study the influence our method has on feature attribution explanations.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Unsupervised Deep Probabilistic Approach for Partial Point Cloud  Registration</b></summary>
  <p><b>编号</b>：[98]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13290</p>
  <p><b>作者</b>：Guofeng Mei,  Hao Tang,  Xiaoshui Huang,  Weijie Wang,  Juan Liu,  Jian Zhang,  Luc Van Gool,  Qiang Wu</p>
  <p><b>备注</b>：CVPR 2023</p>
  <p><b>关键词</b>：methods face challenges, registration methods face, point cloud registration, cloud registration methods, point clouds</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep point cloud registration methods face challenges to partial overlaps and
rely on labeled data. To address these issues, we propose UDPReg, an
unsupervised deep probabilistic registration framework for point clouds with
partial overlaps. Specifically, we first adopt a network to learn posterior
probability distributions of Gaussian mixture models (GMMs) from point clouds.
To handle partial point cloud registration, we apply the Sinkhorn algorithm to
predict the distribution-level correspondences under the constraint of the
mixing weights of GMMs. To enable unsupervised learning, we design three
distribution consistency-based losses: self-consistency, cross-consistency, and
local contrastive. The self-consistency loss is formulated by encouraging GMMs
in Euclidean and feature spaces to share identical posterior distributions. The
cross-consistency loss derives from the fact that the points of two partially
overlapping point clouds belonging to the same clusters share the cluster
centroids. The cross-consistency loss allows the network to flexibly learn a
transformation-invariant posterior distribution of two aligned point clouds.
The local contrastive loss facilitates the network to extract discriminative
local features. Our UDPReg achieves competitive performance on the
3DMatch/3DLoMatch and ModelNet/ModelLoNet benchmarks.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Frame-Level Multi-Label Playing Technique Detection Using Multi-Scale  Network and Self-Attention Mechanism</b></summary>
  <p><b>编号</b>：[104]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13272</p>
  <p><b>作者</b>：Dichucheng Li,  Mingjin Che,  Wenwu Meng,  Yulun Wu,  Yi Yu,  Fan Xia,  Wei Li</p>
  <p><b>备注</b>：Accepted to ICASSP 2023</p>
  <p><b>关键词</b>：Instrument playing technique, playing technique, musical presentation, key element, element of musical</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Instrument playing technique (IPT) is a key element of musical presentation.
However, most of the existing works for IPT detection only concern monophonic
music signals, yet little has been done to detect IPTs in polyphonic
instrumental solo pieces with overlapping IPTs or mixed IPTs. In this paper, we
formulate it as a frame-level multi-label classification problem and apply it
to Guzheng, a Chinese plucked string instrument. We create a new dataset,
Guzheng\_Tech99, containing Guzheng recordings and onset, offset, pitch, IPT
annotations of each note. Because different IPTs vary a lot in their lengths,
we propose a new method to solve this problem using multi-scale network and
self-attention. The multi-scale network extracts features from different
scales, and the self-attention mechanism applied to the feature maps at the
coarsest scale further enhances the long-range feature extraction. Our approach
outperforms existing works by a large margin, indicating its effectiveness in
IPT detection.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Transforming Radiance Field with Lipschitz Network for Photorealistic 3D  Scene Stylization</b></summary>
  <p><b>编号</b>：[120]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13232</p>
  <p><b>作者</b>：Zicheng Zhang,  Yinglu Liu,  Congying Han,  Yingwei Pan,  Tiande Guo,  Ting Yao</p>
  <p><b>备注</b>：CVPR 2023, Highlight</p>
  <p><b>关键词</b>：Recent advances, synthesis have witnessed, witnessed the rise, Neural Radiance Fields, Lipschitz</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advances in 3D scene representation and novel view synthesis have
witnessed the rise of Neural Radiance Fields (NeRFs). Nevertheless, it is not
trivial to exploit NeRF for the photorealistic 3D scene stylization task, which
aims to generate visually consistent and photorealistic stylized scenes from
novel views. Simply coupling NeRF with photorealistic style transfer (PST) will
result in cross-view inconsistency and degradation of stylized view syntheses.
Through a thorough analysis, we demonstrate that this non-trivial task can be
simplified in a new light: When transforming the appearance representation of a
pre-trained NeRF with Lipschitz mapping, the consistency and photorealism
across source views will be seamlessly encoded into the syntheses. That
motivates us to build a concise and flexible learning framework namely LipRF,
which upgrades arbitrary 2D PST methods with Lipschitz mapping tailored for the
3D scene. Technically, LipRF first pre-trains a radiance field to reconstruct
the 3D scene, and then emulates the style on each view by 2D PST as the prior
to learn a Lipschitz network to stylize the pre-trained appearance. In view of
that Lipschitz condition highly impacts the expressivity of the neural network,
we devise an adaptive regularization to balance the reconstruction and
stylization. A gradual gradient aggregation strategy is further introduced to
optimize LipRF in a cost-efficient manner. We conduct extensive experiments to
show the high quality and robust performance of LipRF on both photorealistic 3D
stylization and object appearance editing.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Enriching Neural Network Training Dataset to Improve Worst-Case  Performance Guarantees</b></summary>
  <p><b>编号</b>：[122]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13228</p>
  <p><b>作者</b>：Rahul Nellikkath,  Spyros Chatzivasileiadis</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2212.10930</p>
  <p><b>关键词</b>：approximate non-linear relationships, Machine learning algorithms, Machine learning, non-linear relationships, valuable tool</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine learning algorithms, especially Neural Networks (NNs), are a valuable
tool used to approximate non-linear relationships, like the AC-Optimal Power
Flow (AC-OPF), with considerable accuracy -- and achieving a speedup of several
orders of magnitude when deployed for use. Often in power systems literature,
the NNs are trained with a fixed dataset generated prior to the training
process. In this paper, we show that adapting the NN training dataset during
training can improve the NN performance and substantially reduce its worst-case
violations. This paper proposes an algorithm that identifies and enriches the
training dataset with critical datapoints that reduce the worst-case violations
and deliver a neural network with improved worst-case performance guarantees.
We demonstrate the performance of our algorithm in four test power systems,
ranging from 39-buses to 162-buses.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Fairness-guided Few-shot Prompting for Large Language Models</b></summary>
  <p><b>编号</b>：[128]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13217</p>
  <p><b>作者</b>：Huan Ma,  Changqing Zhang,  Yatao Bian,  Lemao Liu,  Zhirui Zhang,  Peilin Zhao,  Shu Zhang,  Huazhu Fu,  Qinghua Hu,  Bingzhe Wu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：demonstrated surprising ability, in-context learning, solve numerous downstream, Large language models, Large language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models have demonstrated surprising ability to perform
in-context learning, i.e., these models can be directly applied to solve
numerous downstream tasks by conditioning on a prompt constructed by a few
input-output examples. However, prior research has shown that in-context
learning can suffer from high instability due to variations in training
examples, example order, and prompt formats. Therefore, the construction of an
appropriate prompt is essential for improving the performance of in-context
learning. In this paper, we revisit this problem from the view of predictive
bias. Specifically, we introduce a metric to evaluate the predictive bias of a
fixed prompt against labels or a given attributes. Then we empirically show
that prompts with higher bias always lead to unsatisfactory predictive quality.
Based on this observation, we propose a novel search strategy based on the
greedy search to identify the near-optimal prompt for improving the performance
of in-context learning. We perform comprehensive experiments with
state-of-the-art mainstream models such as GPT-3 on various downstream tasks.
Our results indicate that our method can enhance the model's in-context
learning performance in an effective and interpretable manner.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：A Case Study on AI Engineering Practices: Developing an Autonomous Stock  Trading System</b></summary>
  <p><b>编号</b>：[129]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13216</p>
  <p><b>作者</b>：Marcel Grote,  Justus Bogner</p>
  <p><b>备注</b>：Accepted for publication at the International Conference on AI Engineering (CAIN) 2023</p>
  <p><b>关键词</b>：solve complex problems, artificial intelligence, complex problems, solve complex, development</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Today, many systems use artificial intelligence (AI) to solve complex
problems. While this often increases system effectiveness, developing a
production-ready AI-based system is a difficult task. Thus, solid AI
engineering practices are required to ensure the quality of the resulting
system and to improve the development process. While several practices have
already been proposed for the development of AI-based systems, detailed
practical experiences of applying these practices are rare.
In this paper, we aim to address this gap by collecting such experiences
during a case study, namely the development of an autonomous stock trading
system that uses machine learning functionality to invest in stocks. We
selected 10 AI engineering practices from the literature and systematically
applied them during development, with the goal to collect evidence about their
applicability and effectiveness. Using structured field notes, we documented
our experiences. Furthermore, we also used field notes to document challenges
that occurred during the development, and the solutions we applied to overcome
them. Afterwards, we analyzed the collected field notes, and evaluated how each
practice improved the development. Lastly, we compared our evidence with
existing literature.
Most applied practices improved our system, albeit to varying extent, and we
were able to overcome all major challenges. The qualitative results provide
detailed accounts about 10 AI engineering practices, as well as challenges and
solutions associated with such a project. Our experiences therefore enrich the
emerging body of evidence in this field, which may be especially helpful for
practitioner teams new to AI engineering.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Complementary Pseudo Multimodal Feature for Point Cloud Anomaly  Detection</b></summary>
  <p><b>编号</b>：[136]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13194</p>
  <p><b>作者</b>：Yunkang Cao,  Xiaohao Xu,  Weiming Shen</p>
  <p><b>备注</b>：Submitted to Pattern Recognition. Code is available on this https URL</p>
  <p><b>关键词</b>：promising research area, PCD anomaly detection, detection steadily emerges, anomaly detection steadily, anomaly detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Point cloud (PCD) anomaly detection steadily emerges as a promising research
area. This study aims to improve PCD anomaly detection performance by combining
handcrafted PCD descriptions with powerful pre-trained 2D neural networks. To
this end, this study proposes Complementary Pseudo Multimodal Feature (CPMF)
that incorporates local geometrical information in 3D modality using
handcrafted PCD descriptors and global semantic information in the generated
pseudo 2D modality using pre-trained 2D neural networks. For global semantics
extraction, CPMF projects the origin PCD into a pseudo 2D modality containing
multi-view images. These images are delivered to pre-trained 2D neural networks
for informative 2D modality feature extraction. The 3D and 2D modality features
are aggregated to obtain the CPMF for PCD anomaly detection. Extensive
experiments demonstrate the complementary capacity between 2D and 3D modality
features and the effectiveness of CPMF, with 95.15% image-level AU-ROC and
92.93% pixel-level PRO on the MVTec3D benchmark. Code is available on
this https URL.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Mechanism Design for Ad Auctions with Display Prices</b></summary>
  <p><b>编号</b>：[138]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13192</p>
  <p><b>作者</b>：Bin Li,  Yahui Lei</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：display prices, ads are displayed, direct comparison, comparison among similar, prices</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In many applications, ads are displayed together with the prices, so as to
provide a direct comparison among similar products or services. The
price-displaying feature not only influences the consumers' decisions, but also
affects the advertisers' bidding behaviors. In this paper, we study ad auctions
with display prices from the perspective of mechanism design, in which
advertisers are asked to submit both the costs and prices of their products. We
provide a characterization for all incentive compatible auctions with display
prices, and use it to design auctions under two scenarios. In the former
scenario, the display prices are assumed to be exogenously determined. For this
setting, we derive the welfare-maximizing and revenue-maximizing auctions for
any realization of the price profile. In the latter, advertisers are allowed to
strategize display prices in their own interests. We investigate two families
of allocation policies within the scenario and identify the equilibrium prices
accordingly. Our results reveal that the display prices do affect the design of
ad auctions and the platform can leverage such information to optimize the
performance of ad delivery.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：Extended High Utility Pattern Mining: An Answer Set Programming Based  Framework and Applications</b></summary>
  <p><b>编号</b>：[139]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13191</p>
  <p><b>作者</b>：Francesco Cauteruccio,  Giorgio Terracina</p>
  <p><b>备注</b>：Under consideration in Theory and Practice of Logic Programming (TPLP)</p>
  <p><b>关键词</b>：Utility Pattern Mining, Answer Set Programming, High Utility Pattern, important challenge, challenge in data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Detecting sets of relevant patterns from a given dataset is an important
challenge in data mining. The relevance of a pattern, also called utility in
the literature, is a subjective measure and can be actually assessed from very
different points of view. Rule-based languages like Answer Set Programming
(ASP) seem well suited for specifying user-provided criteria to assess pattern
utility in a form of constraints; moreover, declarativity of ASP allows for a
very easy switch between several criteria in order to analyze the dataset from
different points of view. In this paper, we make steps toward extending the
notion of High Utility Pattern Mining (HUPM); in particular we introduce a new
framework that allows for new classes of utility criteria not considered in the
previous literature. We also show how recent extensions of ASP with external
functions can support a fast and effective encoding and testing of the new
framework. To demonstrate the potential of the proposed framework, we exploit
it as a building block for the definition of an innovative method for
predicting ICU admission for COVID-19 patients. Finally, an extensive
experimental activity demonstrates both from a quantitative and a qualitative
point of view the effectiveness of the proposed approach. Under consideration
in Theory and Practice of Logic Programming (TPLP)</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：CMG-Net: An End-to-End Contact-Based Multi-Finger Dexterous Grasping  Network</b></summary>
  <p><b>编号</b>：[143]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13182</p>
  <p><b>作者</b>：Mingze Wei,  Yaomin Huang,  Zhiyuan Xu,  Ning Liu,  Zhengping Che,  Xinyu Zhang,  Chaomin Shen,  Feifei Feng,  Chun Shan,  Jian Tang</p>
  <p><b>备注</b>：The first two authors are with equal contributions. Paper accepted by ICRA 2023</p>
  <p><b>关键词</b>：representation, robotic hands, multi-finger robotic hands, grasping, paper</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we propose a novel representation for grasping using contacts
between multi-finger robotic hands and objects to be manipulated. This
representation significantly reduces the prediction dimensions and accelerates
the learning process. We present an effective end-to-end network, CMG-Net, for
grasping unknown objects in a cluttered environment by efficiently predicting
multi-finger grasp poses and hand configurations from a single-shot point
cloud. Moreover, we create a synthetic grasp dataset that consists of five
thousand cluttered scenes, 80 object categories, and 20 million annotations. We
perform a comprehensive empirical study and demonstrate the effectiveness of
our grasping representation and CMG-Net. Our work significantly outperforms the
state-of-the-art for three-finger robotic hands. We also demonstrate that the
model trained using synthetic data performs very well for real robots.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Design Patterns for AI-based Systems: A Multivocal Literature Review and  Pattern Repository</b></summary>
  <p><b>编号</b>：[147]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13173</p>
  <p><b>作者</b>：Lukas Heiland,  Marius Hauser,  Justus Bogner</p>
  <p><b>备注</b>：Accepted for publication at the International Conference on AI Engineering (CAIN) 2023</p>
  <p><b>关键词</b>：artificial intelligence components, considerable attention recently, gained considerable attention, AI-based systems, patterns</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Systems with artificial intelligence components, so-called AI-based systems,
have gained considerable attention recently. However, many organizations have
issues with achieving production readiness with such systems. As a means to
improve certain software quality attributes and to address frequently occurring
problems, design patterns represent proven solution blueprints. While new
patterns for AI-based systems are emerging, existing patterns have also been
adapted to this new context.
The goal of this study is to provide an overview of design patterns for
AI-based systems, both new and adapted ones. We want to collect and categorize
patterns, and make them accessible for researchers and practitioners. To this
end, we first performed a multivocal literature review (MLR) to collect design
patterns used with AI-based systems. We then integrated the created pattern
collection into a web-based pattern repository to make the patterns browsable
and easy to find.
As a result, we selected 51 resources (35 white and 16 gray ones), from which
we extracted 70 unique patterns used for AI-based systems. Among these are 34
new patterns and 36 traditional ones that have been adapted to this context.
Popular pattern categories include "architecture" (25 patterns), "deployment"
(16), "implementation" (9), or "security & safety" (9). While some patterns
with four or more mentions already seem established, the majority of patterns
have only been mentioned once or twice (51 patterns). Our results in this
emerging field can be used by researchers as a foundation for follow-up studies
and by practitioners to discover relevant patterns for informing the design of
AI-based systems.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：An elementary belief function logic</b></summary>
  <p><b>编号</b>：[149]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13168</p>
  <p><b>作者</b>：Didier Dubois,  Lluis Godo,  Henri Prade</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Non-additive uncertainty theories, imprecise probabilities share, lower probabilities extend, typically possibility theory, necessity measures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Non-additive uncertainty theories, typically possibility theory, belief
functions and imprecise probabilities share a common feature with modal logic:
the duality properties between possibility and necessity measures, belief and
plausibility functions as well as between upper and lower probabilities extend
the duality between possibility and necessity modalities to the graded
environment. It has been shown that the all-or-nothing version of possibility
theory can be exactly captured by a minimal epistemic logic (MEL) that uses a
very small fragment of the KD modal logic, without resorting to relational
semantics. Besides, the case of belief functions has been studied
independently, and a belief function logic has been obtained by extending the
modal logic S5 to graded modalities using Łukasiewicz logic, albeit using
relational semantics. This paper shows that a simpler belief function logic can
be devised by adding Łukasiewicz logic on top of MEL. It allows for a more
natural semantics in terms of Shafer basic probability assignments.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：Adiabatic replay for continual learning</b></summary>
  <p><b>编号</b>：[152]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13157</p>
  <p><b>作者</b>：Alexander Krawczyk,  Alexander Gepperth</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：avoid catastrophic forgetting, Conventional replay-based approaches, catastrophic forgetting, approaches to continual, order to avoid</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Conventional replay-based approaches to continual learning (CL) require, for
each learning phase with new data, the replay of samples representing all of
the previously learned knowledge in order to avoid catastrophic forgetting.
Since the amount of learned knowledge grows over time in CL problems,
generative replay spends an increasing amount of time just re-learning what is
already known. In this proof-of-concept study, we propose a replay-based CL
strategy that we term adiabatic replay (AR), which derives its efficiency from
the (reasonable) assumption that each new learning phase is adiabatic, i.e.,
represents only a small addition to existing knowledge. Each new learning phase
triggers a sampling process that selectively replays, from the body of existing
knowledge, just such samples that are similar to the new data, in contrast to
replaying all of it. Complete replay is not required since AR represents the
data distribution by GMMs, which are capable of selectively updating their
internal representation only where data statistics have changed. As long as
additions are adiabatic, the amount of to-be-replayed samples need not to
depend on the amount of previously acquired knowledge at all. We verify
experimentally that AR is superior to state-of-the-art deep generative replay
using VAEs.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Defining Quality Requirements for a Trustworthy AI Wildflower Monitoring  Platform</b></summary>
  <p><b>编号</b>：[153]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13151</p>
  <p><b>作者</b>：Petra Heck,  Gerard Schouten</p>
  <p><b>备注</b>：Preprint - Paper accepted for CAIN23 - 2nd international conference on AI Engineering</p>
  <p><b>关键词</b>：trained machine learning, machine learning model, trained machine, machine learning, solution to evolve</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>For an AI solution to evolve from a trained machine learning model into a
production-ready AI system, many more things need to be considered than just
the performance of the machine learning model. A production-ready AI system
needs to be trustworthy, i.e. of high quality. But how to determine this in
practice? For traditional software, ISO25000 and its predecessors have since
long time been used to define and measure quality characteristics. Recently,
quality models for AI systems, based on ISO25000, have been introduced. This
paper applies one such quality model to a real-life case study: a deep learning
platform for monitoring wildflowers. The paper presents three realistic
scenarios sketching what it means to respectively use, extend and incrementally
improve the deep learning platform for wildflower identification and counting.
Next, it is shown how the quality model can be used as a structured dictionary
to define quality requirements for data, model and software. Future work
remains to extend the quality model with metrics, tools and best practices to
aid AI engineering practitioners in implementing trustworthy AI systems.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：MagicFusion: Boosting Text-to-Image Generation Performance by Fusing  Diffusion Models</b></summary>
  <p><b>编号</b>：[163]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13126</p>
  <p><b>作者</b>：Jing Zhao,  Heliang Zheng,  Chaoyue Wang,  Long Lan,  Wenjing Yang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：text-guided diffusion models, powerful text-guided diffusion, advent of open-source, open-source AI communities, communities has produced</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The advent of open-source AI communities has produced a cornucopia of
powerful text-guided diffusion models that are trained on various datasets.
While few explorations have been conducted on ensembling such models to combine
their strengths. In this work, we propose a simple yet effective method called
Saliency-aware Noise Blending (SNB) that can empower the fused text-guided
diffusion models to achieve more controllable generation. Specifically, we
experimentally find that the responses of classifier-free guidance are highly
related to the saliency of generated images. Thus we propose to trust different
models in their areas of expertise by blending the predicted noises of two
diffusion models in a saliency-aware manner. SNB is training-free and can be
completed within a DDIM sampling process. Additionally, it can automatically
align the semantics of two noise spaces without requiring additional
annotations such as masks. Extensive experiments show the impressive
effectiveness of SNB in various applications. Project page is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：A Simple Explanation for the Phase Transition in Large Language Models  with List Decoding</b></summary>
  <p><b>编号</b>：[169]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13112</p>
  <p><b>作者</b>：Cheng-Shang Chang</p>
  <p><b>备注</b>：5 pages, 1 figure</p>
  <p><b>关键词</b>：exhibit emergent abilities, recent experimental results, large language models, experimental results show, exhibit emergent</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Various recent experimental results show that large language models (LLM)
exhibit emergent abilities that are not present in small models. System
performance is greatly improved after passing a certain critical threshold of
scale. In this letter, we provide a simple explanation for such a phase
transition phenomenon. For this, we model an LLM as a sequence-to-sequence
random function. Instead of using instant generation at each step, we use a
list decoder that keeps a list of candidate sequences at each step and defers
the generation of the output sequence at the end. We show that there is a
critical threshold such that the expected number of erroneous candidate
sequences remains bounded when an LLM is below the threshold, and it grows
exponentially when an LLM is above the threshold. Such a threshold is related
to the basic reproduction number in a contagious disease.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain  Batch and Proxy Gradient Transfer</b></summary>
  <p><b>编号</b>：[173]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13099</p>
  <p><b>作者</b>：Hyukhun Koh,  Haesung Pyun,  Nakyeong Yang,  Kyomin Jung</p>
  <p><b>备注</b>：8 pages, 3 figures, ACL 2023 workshop (DSTC)</p>
  <p><b>关键词</b>：Task Oriented Dialogue, Task Oriented, Proxy Gradient Transfer, Oriented Dialogue, Multi Domain Batch</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In Task Oriented Dialogue (TOD) system, detecting and inducing new intents
are two main challenges to apply the system in the real world. In this paper,
we suggest the semantic multi-view model to resolve these two challenges: (1)
SBERT for General Embedding (GE), (2) Multi Domain Batch (MDB) for dialogue
domain knowledge, and (3) Proxy Gradient Transfer (PGT) for cluster-specialized
semantic. MDB feeds diverse dialogue datasets to the model at once to tackle
the multi-domain problem by learning the multiple domain knowledge. We
introduce a novel method PGT, which employs the Siamese network to fine-tune
the model with a clustering method directly.Our model can learn how to cluster
dialogue utterances by using PGT. Experimental results demonstrate that our
multi-view model with MDB and PGT significantly improves the Open Intent
Induction performance compared to baseline systems.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：CP$^3$: Channel Pruning Plug-in for Point-based Networks</b></summary>
  <p><b>编号</b>：[174]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13097</p>
  <p><b>作者</b>：Yaomin Huang,  Ning Liu,  Zhengping Che,  Zhiyuan Xu,  Chaomin Shen,  Yaxin Peng,  Guixu Zhang,  Xinmei Liu,  Feifei Feng,  Jian Tang</p>
  <p><b>备注</b>：Yaomin Huang and Ning Liu are with equal contributions. This paper has been accepted by CVPR 2023</p>
  <p><b>关键词</b>：Channel pruning, channel pruning methods, pruning, CNN channel pruning, pruning methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Channel pruning can effectively reduce both computational cost and memory
footprint of the original network while keeping a comparable accuracy
performance. Though great success has been achieved in channel pruning for 2D
image-based convolutional networks (CNNs), existing works seldom extend the
channel pruning methods to 3D point-based neural networks (PNNs). Directly
implementing the 2D CNN channel pruning methods to PNNs undermine the
performance of PNNs because of the different representations of 2D images and
3D point clouds as well as the network architecture disparity. In this paper,
we proposed CP$^3$, which is a Channel Pruning Plug-in for Point-based network.
CP$^3$ is elaborately designed to leverage the characteristics of point clouds
and PNNs in order to enable 2D channel pruning methods for PNNs. Specifically,
it presents a coordinate-enhanced channel importance metric to reflect the
correlation between dimensional information and individual channel features,
and it recycles the discarded points in PNN's sampling process and reconsiders
their potentially-exclusive information to enhance the robustness of channel
pruning. Experiments on various PNN architectures show that CP$^3$ constantly
improves state-of-the-art 2D CNN pruning approaches on different point cloud
tasks. For instance, our compressed PointNeXt-S on ScanObjectNN achieves an
accuracy of 88.52% with a pruning rate of 57.8%, outperforming the baseline
pruning methods with an accuracy gain of 1.94%.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting  and Anchor Pre-Matching</b></summary>
  <p><b>编号</b>：[185]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13076</p>
  <p><b>作者</b>：Xiaoshi Wu,  Feng Zhu,  Rui Zhao,  Hongsheng Li</p>
  <p><b>备注</b>：11 pages, 4 figures. Accepted by CVPR 2023</p>
  <p><b>关键词</b>：base categories, detection task aiming, aiming at detecting, OVD, OVD benchmark</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Open-vocabulary detection (OVD) is an object detection task aiming at
detecting objects from novel categories beyond the base categories on which the
detector is trained. Recent OVD methods rely on large-scale visual-language
pre-trained models, such as CLIP, for recognizing novel objects. We identify
the two core obstacles that need to be tackled when incorporating these models
into detector training: (1) the distribution mismatch that happens when
applying a VL-model trained on whole images to region recognition tasks; (2)
the difficulty of localizing objects of unseen classes. To overcome these
obstacles, we propose CORA, a DETR-style framework that adapts CLIP for
Open-vocabulary detection by Region prompting and Anchor pre-matching. Region
prompting mitigates the whole-to-region distribution gap by prompting the
region features of the CLIP-based region classifier. Anchor pre-matching helps
learning generalizable object localization by a class-aware matching mechanism.
We evaluate CORA on the COCO OVD benchmark, where we achieve 41.7 AP50 on novel
classes, which outperforms the previous SOTA by 2.4 AP50 even without resorting
to extra training data. When extra training data is available, we train
CORA$^+$ on both ground-truth base-category annotations and additional pseudo
bounding box labels computed by CORA. CORA$^+$ achieves 43.1 AP50 on the COCO
OVD benchmark and 28.1 box APr on the LVIS OVD benchmark.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Open-Vocabulary Object Detection using Pseudo Caption Labels</b></summary>
  <p><b>编号</b>：[206]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13040</p>
  <p><b>作者</b>：Han-Cheol Cho,  Won Young Jhoo,  Wooyoung Kang,  Byungseok Roh</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Recent open-vocabulary detection, detection methods aim, open-vocabulary detection methods, Recent open-vocabulary, image-text pairs</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent open-vocabulary detection methods aim to detect novel objects by
distilling knowledge from vision-language models (VLMs) trained on a vast
amount of image-text pairs. To improve the effectiveness of these methods,
researchers have utilized datasets with a large vocabulary that contains a
large number of object classes, under the assumption that such data will enable
models to extract comprehensive knowledge on the relationships between various
objects and better generalize to unseen object classes. In this study, we argue
that more fine-grained labels are necessary to extract richer knowledge about
novel objects, including object attributes and relationships, in addition to
their names. To address this challenge, we propose a simple and effective
method named Pseudo Caption Labeling (PCL), which utilizes an image captioning
model to generate captions that describe object instances from diverse
perspectives. The resulting pseudo caption labels offer dense samples for
knowledge distillation. On the LVIS benchmark, our best model trained on the
de-duplicated VisualGenome dataset achieves an AP of 34.5 and an APr of 30.6,
comparable to the state-of-the-art performance. PCL's simplicity and
flexibility are other notable features, as it is a straightforward
pre-processing technique that can be used with any image captioning model
without imposing any restrictions on model architecture or training process.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：SPeC: A Soft Prompt-Based Calibration on Mitigating Performance  Variability in Clinical Notes Summarization</b></summary>
  <p><b>编号</b>：[208]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13035</p>
  <p><b>作者</b>：Yu-Neng Chuang,  Ruixiang Tang,  Xiaoqian Jiang,  Xia Hu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：encompassing medical histories, Electronic health records, store an extensive, extensive array, Electronic health</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Electronic health records (EHRs) store an extensive array of patient
information, encompassing medical histories, diagnoses, treatments, and test
outcomes. These records are crucial for enabling healthcare providers to make
well-informed decisions regarding patient care. Summarizing clinical notes
further assists healthcare professionals in pinpointing potential health risks
and making better-informed decisions. This process contributes to reducing
errors and enhancing patient outcomes by ensuring providers have access to the
most pertinent and current patient data. Recent research has shown that
incorporating prompts with large language models (LLMs) substantially boosts
the efficacy of summarization tasks. However, we show that this approach also
leads to increased output variance, resulting in notably divergent outputs even
when prompts share similar meanings. To tackle this challenge, we introduce a
model-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft
prompts to diminish variance while preserving the advantages of prompt-based
summarization. Experimental findings on multiple clinical note tasks and LLMs
indicate that our method not only bolsters performance but also effectively
curbs variance for various LLMs, providing a more uniform and dependable
solution for summarizing vital medical information.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Preference-Aware Constrained Multi-Objective Bayesian Optimization</b></summary>
  <p><b>编号</b>：[209]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13034</p>
  <p><b>作者</b>：Alaleh Ahmadianshalchi,  Syrine Belakaria,  Janardhan Rao Doppa</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2110.06980</p>
  <p><b>关键词</b>：black-box objective functions, feasible input designs, paper addresses, functions with practitioner-specified, engineering design problems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper addresses the problem of constrained multi-objective optimization
over black-box objective functions with practitioner-specified preferences over
the objectives when a large fraction of the input space is infeasible (i.e.,
violates constraints). This problem arises in many engineering design problems
including analog circuits and electric power system design. Our overall goal is
to approximate the optimal Pareto set over the small fraction of feasible input
designs. The key challenges include the huge size of the design space, multiple
objectives and large number of constraints, and the small fraction of feasible
input designs which can be identified only after performing expensive
simulations. We propose a novel and efficient preference-aware constrained
multi-objective Bayesian optimization approach referred to as PAC-MOO to
address these challenges. The key idea is to learn surrogate models for both
output objectives and constraints, and select the candidate input for
evaluation in each iteration that maximizes the information gained about the
optimal constrained Pareto front while factoring in the preferences over
objectives. Our experiments on two real-world analog circuit design
optimization problems demonstrate the efficacy of PAC-MOO over prior methods.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Failure-tolerant Distributed Learning for Anomaly Detection in Wireless  Networks</b></summary>
  <p><b>编号</b>：[219]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13015</p>
  <p><b>作者</b>：Marc Katzef,  Andrew C. Cullen,  Tansu Alpcan,  Christopher Leckie,  Justin Kopacz</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：lack thereof, anomaly detection, cripple distributed systems, distributed, potentially cripple distributed</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The analysis of distributed techniques is often focused upon their
efficiency, without considering their robustness (or lack thereof). Such a
consideration is particularly important when devices or central servers can
fail, which can potentially cripple distributed systems. When such failures
arise in wireless communications networks, important services that they
use/provide (like anomaly detection) can be left inoperable and can result in a
cascade of security problems. In this paper, we present a novel method to
address these risks by combining both flat- and star-topologies, combining the
performance and reliability benefits of both. We refer to this method as
"Tol-FL", due to its increased failure-tolerance as compared to the technique
of Federated Learning. Our approach both limits device failure risks while
outperforming prior methods by up to 8% in terms of anomaly detection AUROC in
a range of realistic settings that consider client as well as server failure,
all while reducing communication costs. This performance demonstrates that
Tol-FL is a highly suitable method for distributed model training for anomaly
detection, especially in the domain of wireless networks.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Semantic Image Attack for Visual Model Diagnosis</b></summary>
  <p><b>编号</b>：[222]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13010</p>
  <p><b>作者</b>：Jinqi Luo,  Zhaoning Wang,  Chen Henry Wu,  Dong Huang,  Fernando De la Torre</p>
  <p><b>备注</b>：Initial version submitted to NeurIPS 2022</p>
  <p><b>关键词</b>：specific train, guarantee reliable, reliable or fair, SIA, model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In practice, metric analysis on a specific train and test dataset does not
guarantee reliable or fair ML models. This is partially due to the fact that
obtaining a balanced, diverse, and perfectly labeled dataset is typically
expensive, time-consuming, and error-prone. Rather than relying on a carefully
designed test set to assess ML models' failures, fairness, or robustness, this
paper proposes Semantic Image Attack (SIA), a method based on the adversarial
attack that provides semantic adversarial images to allow model diagnosis,
interpretability, and robustness. Traditional adversarial training is a popular
methodology for robustifying ML models against attacks. However, existing
adversarial methods do not combine the two aspects that enable the
interpretation and analysis of the model's flaws: semantic traceability and
perceptual quality. SIA combines the two features via iterative gradient ascent
on a predefined semantic attribute space and the image space. We illustrate the
validity of our approach in three scenarios for keypoint detection and
classification. (1) Model diagnosis: SIA generates a histogram of attributes
that highlights the semantic vulnerability of the ML model (i.e., attributes
that make the model fail). (2) Stronger attacks: SIA generates adversarial
examples with visually interpretable attributes that lead to higher attack
success rates than baseline methods. The adversarial training on SIA improves
the transferable robustness across different gradient-based attacks. (3)
Robustness to imbalanced datasets: we use SIA to augment the underrepresented
classes, which outperforms strong augmentation and re-balancing baselines.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Planning Goals for Exploration</b></summary>
  <p><b>编号</b>：[228]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.13002</p>
  <p><b>作者</b>：Edward S. Hu,  Richard Chang,  Oleh Rybkin,  Dinesh Jayaraman</p>
  <p><b>备注</b>：Camera Ready version for ICLR2023 Spotlight</p>
  <p><b>关键词</b>：accomplish diverse tasks, accomplish diverse, diverse tasks, PEG, Planning Exploratory Goals</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Dropped into an unknown environment, what should an agent do to quickly learn
about the environment and how to accomplish diverse tasks within it? We address
this question within the goal-conditioned reinforcement learning paradigm, by
identifying how the agent should set its goals at training time to maximize
exploration. We propose "Planning Exploratory Goals" (PEG), a method that sets
goals for each training episode to directly optimize an intrinsic exploration
reward. PEG first chooses goal commands such that the agent's goal-conditioned
policy, at its current level of training, will end up in states with high
exploration potential. It then launches an exploration policy starting at those
promising states. To enable this direct optimization, PEG learns world models
and adapts sampling-based planning algorithms to "plan goal commands". In
challenging simulated robotics environments including a multi-legged ant robot
in a maze, and a robot arm on a cluttered tabletop, PEG exploration enables
more efficient and effective training of goal-conditioned policies relative to
baselines and ablations. Our ant successfully navigates a long maze, and the
robot arm successfully builds a stack of three blocks upon command. Website:
this https URL</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：A Survey of Historical Learning: Learning Models with Learning History</b></summary>
  <p><b>编号</b>：[236]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12992</p>
  <p><b>作者</b>：Xiang Li,  Ge Wu,  Lingfeng Yang,  Wenhai Wang,  Renjie Song,  Jian Yang</p>
  <p><b>备注</b>：Xiang Li and Ge Wu have equal contributions</p>
  <p><b>关键词</b>：learning, Historical Learning, Learning History, Historical, learning deep models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>New knowledge originates from the old. The various types of elements,
deposited in the training history, are a large amount of wealth for improving
learning deep models. In this survey, we comprehensively review and summarize
the topic--``Historical Learning: Learning Models with Learning History'',
which learns better neural models with the help of their learning history
during its optimization, from three detailed aspects: Historical Type (what),
Functional Part (where) and Storage Form (how). To our best knowledge, it is
the first survey that systematically studies the methodologies which make use
of various historical statistics when training deep neural networks. The
discussions with related topics like recurrent/memory networks, ensemble
learning, and reinforcement learning are demonstrated. We also expose future
challenges of this topic and encourage the community to pay attention to the
think of historical learning principles when designing algorithms. The paper
list related to historical learning is available at
\url{this https URL.}</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Uncertainty Calibration for Counterfactual Propensity Estimation in  Recommendation</b></summary>
  <p><b>编号</b>：[244]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12973</p>
  <p><b>作者</b>：Wenbo Hu,  Xin Sun,  Qiang liu,  Shu Wu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：missing due, selection biases, large portion, missing, uncertainty calibration</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recommendation systems, a large portion of the ratings are missing due to
the selection biases, which is known as Missing Not At Random. The
counterfactual inverse propensity scoring (IPS) was used to weight the
imputation error of every observed rating. Although effective in multiple
scenarios, we argue that the performance of IPS estimation is limited due to
the uncertainty miscalibration of propensity estimation. In this paper, we
propose the uncertainty calibration for the propensity estimation in
recommendation systems with multiple representative uncertainty calibration
techniques. Theoretical analysis on the bias and generalization bound shows the
superiority of the calibrated IPS estimator over the uncalibrated one.
Experimental results on the coat and yahoo datasets shows that the uncertainty
calibration is improved and hence brings the better recommendation results.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Continuous Indeterminate Probability Neural Network</b></summary>
  <p><b>编号</b>：[248]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12964</p>
  <p><b>作者</b>：Tao Yang</p>
  <p><b>备注</b>：8 pages</p>
  <p><b>关键词</b>：Continuous Indeterminate, paper introduces, latent random variables, Continuous, latent</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper introduces a general model called CIPNN - Continuous Indeterminate
Probability Neural Network, and this model is based on IPNN, which is used for
discrete latent random variables. Currently, posterior of continuous latent
variables is regarded as intractable, with the new theory proposed by IPNN this
problem can be solved. Our contributions are Four-fold. First, we derive the
analytical solution of the posterior calculation of continuous latent random
variables and propose a general classification model (CIPNN). Second, we
propose a general auto-encoder called CIPAE - Continuous Indeterminate
Probability Auto-Encoder, the decoder part is not a neural network and uses a
fully probabilistic inference model for the first time. Third, we propose a new
method to visualize the latent random variables, we use one of N dimensional
latent variables as a decoder to reconstruct the input image, which can work
even for classification tasks, in this way, we can see what each latent
variable has learned. Fourth, IPNN has shown great classification capability,
CIPNN has pushed this classification capability to infinity. Theoretical
advantages are reflected in experimental results.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：The Shaky Foundations of Clinical Foundation Models: A Survey of Large  Language Models and Foundation Models for EMRs</b></summary>
  <p><b>编号</b>：[250]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12961</p>
  <p><b>作者</b>：Michael Wornow,  Yizhe Xu,  Rahul Thapa,  Birju Patel,  Ethan Steinberg,  Scott Fleming,  Michael A. Pfeffer,  Jason Fries,  Nigam H. Shah</p>
  <p><b>备注</b>：16 pages, 4 figures, submitted to NPJ Digital Medicine</p>
  <p><b>关键词</b>：electronic medical records, spurred significant interest, improve patient care, building similar models, medical records</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The successes of foundation models such as ChatGPT and AlphaFold have spurred
significant interest in building similar models for electronic medical records
(EMRs) to improve patient care and hospital operations. However, recent hype
has obscured critical gaps in our understanding of these models' capabilities.
We review over 80 foundation models trained on non-imaging EMR data (i.e.
clinical text and/or structured data) and create a taxonomy delineating their
architectures, training data, and potential use cases. We find that most models
are trained on small, narrowly-scoped clinical datasets (e.g. MIMIC-III) or
broad, public biomedical corpora (e.g. PubMed) and are evaluated on tasks that
do not provide meaningful insights on their usefulness to health systems. In
light of these findings, we propose an improved evaluation framework for
measuring the benefits of clinical foundation models that is more closely
grounded to metrics that matter in healthcare.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Variantional autoencoder with decremental information bottleneck for  disentanglement</b></summary>
  <p><b>编号</b>：[251]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12959</p>
  <p><b>作者</b>：Jiantao Wu,  Shentong Mo,  Muhammad Awais,  Sara Atito,  Xingshen Zhang,  Lin Wang,  Xiang Yang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：disentanglement, major challenge, Information Bottleneck, Information, latent</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>One major challenge of disentanglement learning with variational autoencoders
is the trade-off between disentanglement and reconstruction fidelity. Previous
incremental methods with only on latent space cannot optimize these two targets
simultaneously, so they expand the Information Bottleneck while training to
{optimize from disentanglement to reconstruction. However, a large bottleneck
will lose the constraint of disentanglement, causing the information diffusion
problem. To tackle this issue, we present a novel decremental variational
autoencoder with disentanglement-invariant transformations to optimize multiple
objectives in different layers, termed DeVAE, for balancing disentanglement and
reconstruction fidelity by decreasing the information bottleneck of diverse
latent spaces gradually. Benefiting from the multiple latent spaces, DeVAE
allows simultaneous optimization of multiple objectives to optimize
reconstruction while keeping the constraint of disentanglement, avoiding
information diffusion. DeVAE is also compatible with large models with
high-dimension latent space. Experimental results on dSprites and Shapes3D that
DeVAE achieves \fix{R2q6}{a good balance between disentanglement and
reconstruction.DeVAE shows high tolerant of hyperparameters and on
high-dimensional latent spaces.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：Reinforcement Learning with Exogenous States and Rewards</b></summary>
  <p><b>编号</b>：[252]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12957</p>
  <p><b>作者</b>：George Trimponias,  Thomas G. Dietterich</p>
  <p><b>备注</b>：Greatly extends the initial work reported in 1806.01584</p>
  <p><b>关键词</b>：Markov Decision Process, Markov Reward Process, injecting uncontrolled variation, endogenous Markov Decision, Exogenous state variables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Exogenous state variables and rewards can slow reinforcement learning by
injecting uncontrolled variation into the reward signal. This paper formalizes
exogenous state variables and rewards and shows that if the reward function
decomposes additively into endogenous and exogenous components, the MDP can be
decomposed into an exogenous Markov Reward Process (based on the exogenous
reward) and an endogenous Markov Decision Process (optimizing the endogenous
reward). Any optimal policy for the endogenous MDP is also an optimal policy
for the original MDP, but because the endogenous reward typically has reduced
variance, the endogenous MDP is easier to solve. We study settings where the
decomposition of the state space into exogenous and endogenous state spaces is
not given but must be discovered. The paper introduces and proves correctness
of algorithms for discovering the exogenous and endogenous subspaces of the
state space when they are mixed through linear combination. These algorithms
can be applied during reinforcement learning to discover the exogenous space,
remove the exogenous reward, and focus reinforcement learning on the endogenous
MDP. Experiments on a variety of challenging synthetic MDPs show that these
methods, applied online, discover large exogenous state spaces and produce
substantial speedups in reinforcement learning.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：Use of Federated Learning and Blockchain towards Securing Financial  Services</b></summary>
  <p><b>编号</b>：[259]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12944</p>
  <p><b>作者</b>：Pushpita Chatterjee,  Debashis Das,  Danda B Rawat</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：financial services, financial, services, cyber-attacks pose, axiomatic threat</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent days, the proliferation of several existing and new cyber-attacks
pose an axiomatic threat to the stability of financial services. It is hard to
predict the nature of attacks that can trigger a serious financial crisis. The
unprecedented digital transformation to financial services has been accelerated
during the COVID-19 pandemic and it is still ongoing. Attackers are taking
advantage of this transformation and pose a new global threat to financial
stability and integrity. Many large organizations are switching from
centralized finance (CeFi) to decentralized finance (DeFi) because
decentralized finance has many advantages. Blockchain can bring big and
far-reaching effects on the trustworthiness, safety, accessibility,
cost-effectiveness, and openness of the financial sector. The present paper
gives an in-depth look at how blockchain and federated learning (FL) are used
in financial services. It starts with an overview of recent developments in
both use cases. This paper explores and discusses existing financial service
vulnerabilities, potential threats, and consequent risks. So, we explain the
problems that can be fixed in financial services and how blockchain and FL
could help solve them. These problems include data protection, storage
optimization, and making more money in financial services. We looked at many
blockchain-enabled FL methods and came up with some possible solutions that
could be used in financial services to solve several challenges like
cost-effectiveness, automation, and security control. Finally, we point out
some future directions at the end of this study.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：A Survey on Explainable Artificial Intelligence for Network  Cybersecurity</b></summary>
  <p><b>编号</b>：[260]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12942</p>
  <p><b>作者</b>：Gaith Rjoub,  Jamal Bentahar,  Omar Abdel Wahab,  Rabeb Mizouni,  Alyssa Song,  Robin Cohen,  Hadi Otrok,  Azzam Mourad</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：critical applications, artificial intelligence, black-box nature, XAI, intelligence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The black-box nature of artificial intelligence (AI) models has been the
source of many concerns in their use for critical applications. Explainable
Artificial Intelligence (XAI) is a rapidly growing research field that aims to
create machine learning models that can provide clear and interpretable
explanations for their decisions and actions. In the field of network
cybersecurity, XAI has the potential to revolutionize the way we approach
network security by enabling us to better understand the behavior of cyber
threats and to design more effective defenses. In this survey, we review the
state of the art in XAI for cybersecurity in network systems and explore the
various approaches that have been proposed to address this important problem.
The review follows a systematic classification of network-driven cybersecurity
threats and issues. We discuss the challenges and limitations of current XAI
methods in the context of cybersecurity and outline promising directions for
future research.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：Real-World Community-in-the-Loop Smart Video Surveillance -- A Case  Study at a Community College</b></summary>
  <p><b>编号</b>：[265]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12934</p>
  <p><b>作者</b>：Shanle Yao,  Babak Rahimi Ardabili,  Armin Danesh Pazho,  Ghazal Alinezhad Noghre,  Christopher Neff,  Hamed Tabkhi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ensuring public safety, Smart Video surveillance, Video surveillance systems, important recently, recently for ensuring</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Smart Video surveillance systems have become important recently for ensuring
public safety and security, especially in smart cities. However, applying
real-time artificial intelligence technologies combined with low-latency
notification and alarming has made deploying these systems quite challenging.
This paper presents a case study for designing and deploying smart video
surveillance systems based on a real-world testbed at a community college. We
primarily focus on a smart camera-based system that can identify
suspicious/abnormal activities and alert the stakeholders and residents
immediately. The paper highlights and addresses different algorithmic and
system design challenges to guarantee real-time high-accuracy video analytics
processing in the testbed. It also presents an example of cloud system
infrastructure and a mobile application for real-time notification to keep
students, faculty/staff, and responsible security personnel in the loop. At the
same time, it covers the design decision to maintain communities' privacy and
ethical requirements as well as hardware configuration and setups. We evaluate
the system's performance using throughput and end-to-end latency. The
experiment results show that, on average, our system's end-to-end latency to
notify the end users in case of detecting suspicious objects is 5.3, 5.78, and
11.11 seconds when running 1, 4, and 8 cameras, respectively. On the other
hand, in case of detecting anomalous behaviors, the system could notify the end
users with 7.3, 7.63, and 20.78 seconds average latency. These results
demonstrate that the system effectively detects and notifies abnormal behaviors
and suspicious objects to the end users within a reasonable period. The system
can run eight cameras simultaneously at a 32.41 Frame Per Second (FPS) rate.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：Revisiting the Fragility of Influence Functions</b></summary>
  <p><b>编号</b>：[268]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12922</p>
  <p><b>作者</b>：Jacob R. Epifano,  Ravi P. Ramachandran,  Aaron J. Masino,  Ghulam Rasool</p>
  <p><b>备注</b>：11 pages, 5 figures, accepted to Neural Networks</p>
  <p><b>关键词</b>：deep learning models, influence functions, explain the predictions, predictions of deep, deep learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the last few years, many works have tried to explain the predictions of
deep learning models. Few methods, however, have been proposed to verify the
accuracy or faithfulness of these explanations. Recently, influence functions,
which is a method that approximates the effect that leave-one-out training has
on the loss function, has been shown to be fragile. The proposed reason for
their fragility remains unclear. Although previous work suggests the use of
regularization to increase robustness, this does not hold in all cases. In this
work, we seek to investigate the experiments performed in the prior work in an
effort to understand the underlying mechanisms of influence function fragility.
First, we verify influence functions using procedures from the literature under
conditions where the convexity assumptions of influence functions are met.
Then, we relax these assumptions and study the effects of non-convexity by
using deeper models and more complex datasets. Here, we analyze the key metrics
and procedures that are used to validate influence functions. Our results
indicate that the validation procedures may cause the observed fragility.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：Deep learning-based stereo camera multi-video synchronization</b></summary>
  <p><b>编号</b>：[271]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12916</p>
  <p><b>作者</b>：Nicolas Boizard,  Kevin El Haddad,  Thierry Ravet,  François Cresson,  Thierry Dutoit</p>
  <p><b>备注</b>：5 pages, 4 figures, Accepted at ICASSP 2023</p>
  <p><b>关键词</b>：Stereo vision, vision is essential, Stereo, applications, synchronization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Stereo vision is essential for many applications. Currently, the
synchronization of the streams coming from two cameras is done using mostly
hardware. A software-based synchronization method would reduce the cost, weight
and size of the entire system and allow for more flexibility when building such
systems. With this goal in mind, we present here a comparison of different deep
learning-based systems and prove that some are efficient and generalizable
enough for such a task. This study paves the way to a production ready
software-based video synchronization system.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：Cross-Layer Design for AI Acceleration with Non-Coherent Optical  Computing</b></summary>
  <p><b>编号</b>：[275]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12910</p>
  <p><b>作者</b>：Febin Sunny,  Mahdi Nikdast,  Sudeep Pasricha</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：graph convolutional networks, neural networks require, networks require massive, deep neural networks, require massive computational</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Emerging AI applications such as ChatGPT, graph convolutional networks, and
other deep neural networks require massive computational resources for training
and inference. Contemporary computing platforms such as CPUs, GPUs, and TPUs
are struggling to keep up with the demands of these AI applications.
Non-coherent optical computing represents a promising approach for light-speed
acceleration of AI workloads. In this paper, we show how cross-layer design can
overcome challenges in non-coherent optical computing platforms. We describe
approaches for optical device engineering, tuning circuit enhancements, and
architectural innovations to adapt optical computing to a variety of AI
workloads. We also discuss techniques for hardware/software co-design that can
intelligently map and adapt AI software to improve its performance on
non-coherent optical computing platforms.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：Feature Reduction Method Comparison Towards Explainability and  Efficiency in Cybersecurity Intrusion Detection Systems</b></summary>
  <p><b>编号</b>：[282]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12891</p>
  <p><b>作者</b>：Adam M. Lehavi,  Seongtae Kim</p>
  <p><b>备注</b>：Published in 2022 21st IEEE International Conference on Machine Learning and Applications. 8 pages. 5 figures</p>
  <p><b>关键词</b>：intrusion detection systems, prevent attacks based, realm of cybersecurity, intrusion detection, detection systems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the realm of cybersecurity, intrusion detection systems (IDS) detect and
prevent attacks based on collected computer and network data. In recent
research, IDS models have been constructed using machine learning (ML) and deep
learning (DL) methods such as Random Forest (RF) and deep neural networks
(DNN). Feature selection (FS) can be used to construct faster, more
interpretable, and more accurate models. We look at three different FS
techniques; RF information gain (RF-IG), correlation feature selection using
the Bat Algorithm (CFS-BA), and CFS using the Aquila Optimizer (CFS-AO). Our
results show CFS-BA to be the most efficient of the FS methods, building in 55%
of the time of the best RF-IG model while achieving 99.99% of its accuracy.
This reinforces prior contributions attesting to CFS-BA's accuracy while
building upon the relationship between subset size, CFS score, and RF-IG score
in final results.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：A dynamic risk score for early prediction of cardiogenic shock using  machine learning</b></summary>
  <p><b>编号</b>：[285]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12888</p>
  <p><b>作者</b>：Yuxuan Hu,  Albert Lui,  Mark Goldstein,  Mukund Sudarshan,  Andrea Tinsay,  Cindy Tsui,  Samuel Maidman,  John Medamana,  Neil Jethani,  Aaalad Puli,  Vuthy Nguy,  Yindalon Aphinyanaphongs,  Nicholas Kiefer,  Nathaniel Smilowitz,  James Horowitz,  Tania Ahuja,  Glenn Fishman,  Judith Hochman,  Stuart Katz,  Samuel Bernard,  Rajesh Ranganath</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：major cardiovascular diseases, cardiogenic shock, major cardiovascular, cardiovascular diseases, diseases that affect</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Myocardial infarction and heart failure are major cardiovascular diseases
that affect millions of people in the US. The morbidity and mortality are
highest among patients who develop cardiogenic shock. Early recognition of
cardiogenic shock is critical. Prompt implementation of treatment measures can
prevent the deleterious spiral of ischemia, low blood pressure, and reduced
cardiac output due to cardiogenic shock. However, early identification of
cardiogenic shock has been challenging due to human providers' inability to
process the enormous amount of data in the cardiac intensive care unit (ICU)
and lack of an effective risk stratification tool. We developed a deep
learning-based risk stratification tool, called CShock, for patients admitted
into the cardiac ICU with acute decompensated heart failure and/or myocardial
infarction to predict onset of cardiogenic shock. To develop and validate
CShock, we annotated cardiac ICU datasets with physician adjudicated outcomes.
CShock achieved an area under the receiver operator characteristic curve
(AUROC) of 0.820, which substantially outperformed CardShock (AUROC 0.519), a
well-established risk score for cardiogenic shock prognosis. CShock was
externally validated in an independent patient cohort and achieved an AUROC of
0.800, demonstrating its generalizability in other cardiac ICUs.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：Human Uncertainty in Concept-Based AI Systems</b></summary>
  <p><b>编号</b>：[290]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12872</p>
  <p><b>作者</b>：Katherine M. Collins,  Matthew Barker,  Mateo Espinosa Zarlenga,  Naveen Raman,  Umang Bhatt,  Mateja Jamnik,  Ilia Sucholutsky,  Adrian Weller,  Krishnamurthy Dvijotham</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：safety-critical settings, loop may abate, clinician working, humans, mitigating risks arising</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Placing a human in the loop may abate the risks of deploying AI systems in
safety-critical settings (e.g., a clinician working with a medical AI system).
However, mitigating risks arising from human error and uncertainty within such
human-AI interactions is an important and understudied issue. In this work, we
study human uncertainty in the context of concept-based models, a family of AI
systems that enable human feedback via concept interventions where an expert
intervenes on human-interpretable concepts relevant to the task. Prior work in
this space often assumes that humans are oracles who are always certain and
correct. Yet, real-world decision-making by humans is prone to occasional
mistakes and uncertainty. We study how existing concept-based models deal with
uncertain interventions from humans using two novel datasets: UMNIST, a visual
dataset with controlled simulated uncertainty based on the MNIST dataset, and
CUB-S, a relabeling of the popular CUB concept dataset with rich,
densely-annotated soft labels from humans. We show that training with uncertain
concept labels may help mitigate weaknesses of concept-based systems when
handling uncertain interventions. These results allow us to identify several
open challenges, which we argue can be tackled through future multidisciplinary
research on building interactive uncertainty-aware systems. To facilitate
further research, we release a new elicitation platform, UElic, to collect
uncertain feedback from humans in collaborative prediction tasks.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：Salient Span Masking for Temporal Understanding</b></summary>
  <p><b>编号</b>：[295]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12860</p>
  <p><b>作者</b>：Jeremy R. Cole,  Aditi Chaudhary,  Bhuwan Dhingra,  Partha Talukdar</p>
  <p><b>备注</b>：5 pages 1 figure, to appear in EACL 2023</p>
  <p><b>关键词</b>：closed-book question answering, Salient Span Masking, Temporal Span Masking, question answering performance, improve closed-book question</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Salient Span Masking (SSM) has shown itself to be an effective strategy to
improve closed-book question answering performance. SSM extends general masked
language model pretraining by creating additional unsupervised training
sentences that mask a single entity or date span, thus oversampling factual
information. Despite the success of this paradigm, the span types and sampling
strategies are relatively arbitrary and not widely studied for other tasks.
Thus, we investigate SSM from the perspective of temporal tasks, where learning
a good representation of various temporal expressions is important. To that
end, we introduce Temporal Span Masking (TSM) intermediate training. First, we
find that SSM alone improves the downstream performance on three temporal tasks
by an avg. +5.8 points. Further, we are able to achieve additional improvements
(avg. +0.29 points) by adding the TSM task. These comprise the new best
reported results on the targeted tasks. Our analysis suggests that the
effectiveness of SSM stems from the sentences chosen in the training data
rather than the mask choice: sentences with entities frequently also contain
temporal expressions. Nonetheless, the additional targeted spans of TSM can
still improve performance, especially in a zero-shot context.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：Semi-Oblivious Chase Termination for Linear Existential Rules: An  Experimental Study</b></summary>
  <p><b>编号</b>：[298]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12851</p>
  <p><b>作者</b>：Marco Calautti,  Mostafa Milani,  Andreas Pieris</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：fundamental algorithmic tool, fundamental algorithmic, algorithmic tool, existential rules, chase</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The chase procedure is a fundamental algorithmic tool in databases that
allows us to reason with constraints, such as existential rules, with a
plethora of applications. It takes as input a database and a set of
constraints, and iteratively completes the database as dictated by the
constraints. A key challenge, though, is the fact that it may not terminate,
which leads to the problem of checking whether it terminates given a database
and a set of constraints. In this work, we focus on the semi-oblivious version
of the chase, which is well-suited for practical implementations, and linear
existential rules, a central class of constraints with several applications. In
this setting, there is a mature body of theoretical work that provides
syntactic characterizations of when the chase terminates, algorithms for
checking chase termination, precise complexity results, and worst-case optimal
bounds on the size of the result of the chase (whenever is finite). Our main
objective is to experimentally evaluate the existing chase termination
algorithms with the aim of understanding which input parameters affect their
performance, clarifying whether they can be used in practice, and revealing
their performance limitations.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：Data-Driven Leader-following Consensus for Nonlinear Multi-Agent Systems  against Composite Attacks: A Twins Layer Approach</b></summary>
  <p><b>编号</b>：[301]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12823</p>
  <p><b>作者</b>：Xin Gong,  Jintao Peng,  Dong Yang,  Zhan Shu,  Tingwen Huang,  Yukang Cui</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Denial of Service, including Denial, nonlinear multi-agent systems, digital twin layer, Digital Twin technology</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper studies the leader-following consensuses of uncertain and
nonlinear multi-agent systems against composite attacks (CAs), including Denial
of Service (DoS) attacks and actuation attacks (AAs). A double-layer control
framework is formulated, where a digital twin layer (TL) is added beside the
traditional cyber-physical layer (CPL), inspired by the recent Digital Twin
technology. Consequently, the resilient control task against CAs can be divided
into two parts: One is distributed estimation against DoS attacks on the TL and
the other is resilient decentralized tracking control against actuation attacks
on the CPL. %The data-driven scheme is used to deal with both model
non-linearity and model uncertainty, in which only the input and output data of
the system are employed throughout the whole control process. First, a
distributed observer based on switching estimation law against DoS is designed
on TL. Second, a distributed model free adaptive control (DMFAC) protocol based
on attack compensation against AAs is designed on CPL. Moreover, the uniformly
ultimately bounded convergence of consensus error of the proposed double-layer
DMFAC algorithm is strictly proved. Finally, the simulation verifies the
effectiveness of the resilient double-layer control scheme.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：Co-Speech Gesture Synthesis using Discrete Gesture Token Learning</b></summary>
  <p><b>编号</b>：[302]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12822</p>
  <p><b>作者</b>：Shuhong Lu,  Youngwoo Yoon,  Andrew Feng</p>
  <p><b>备注</b>：8 pages, 3 figures, 3 tables</p>
  <p><b>关键词</b>：creating believable motions, human users, Synthesizing realistic co-speech, unsolved problem, problem for creating</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Synthesizing realistic co-speech gestures is an important and yet unsolved
problem for creating believable motions that can drive a humanoid robot to
interact and communicate with human users. Such capability will improve the
impressions of the robots by human users and will find applications in
education, training, and medical services. One challenge in learning the
co-speech gesture model is that there may be multiple viable gesture motions
for the same speech utterance. The deterministic regression methods can not
resolve the conflicting samples and may produce over-smoothed or damped
motions. We proposed a two-stage model to address this uncertainty issue in
gesture synthesis by modeling the gesture segments as discrete latent codes.
Our method utilizes RQ-VAE in the first stage to learn a discrete codebook
consisting of gesture tokens from training data. In the second stage, a
two-level autoregressive transformer model is used to learn the prior
distribution of residual codes conditioned on input speech context. Since the
inference is formulated as token sampling, multiple gesture sequences could be
generated given the same speech input using top-k sampling. The quantitative
results and the user study showed the proposed method outperforms the previous
methods and is able to generate realistic and diverse gesture motions.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：Towards A Visual Programming Tool to Create Deep Learning Models</b></summary>
  <p><b>编号</b>：[303]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12821</p>
  <p><b>作者</b>：Tommaso Calò,  Luigi De Russis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Deep Learning, computer science, Learning, programming languages, high-level programming languages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep Learning (DL) developers come from different backgrounds, e.g.,
medicine, genomics, finance, and computer science. To create a DL model, they
must learn and use high-level programming languages (e.g., Python), thus
needing to handle related setups and solve programming errors. This paper
presents DeepBlocks, a visual programming tool that allows DL developers to
design, train, and evaluate models without relying on specific programming
languages. DeepBlocks works by building on the typical model structure: a
sequence of learnable functions whose arrangement defines the specific
characteristics of the model. We derived DeepBlocks' design goals from a
5-participants formative interview, and we validated the first implementation
of the tool through a typical use case. Results are promising and show that
developers could visually design complex DL architectures.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：From Wide to Deep: Dimension Lifting Network for Parameter-efficient  Knowledge Graph Embedding</b></summary>
  <p><b>编号</b>：[306]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12816</p>
  <p><b>作者</b>：Borui Cai,  Yong Xiang,  Longxiang Gao,  Di Wu,  He Zhang,  Jiong Jin,  Tom Luan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：entity representations, KGE methods, KGE, Conventional KGE methods, downstream tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Knowledge graph embedding (KGE) that maps entities and relations into vector
representations is essential for downstream tasks. Conventional KGE methods
require relatively high-dimensional entity representations to preserve the
structural information of knowledge graph, but lead to oversized model
parameters. Recent methods reduce model parameters by adopting low-dimensional
entity representations, while developing techniques (e.g., knowledge
distillation) to compensate for the reduced dimension. However, such operations
produce degraded model accuracy and limited reduction of model parameters.
Specifically, we view the concatenation of all entity representations as an
embedding layer, and then conventional KGE methods that adopt high-dimensional
entity representations equal to enlarging the width of the embedding layer to
gain expressiveness. To achieve parameter efficiency without sacrificing
accuracy, we instead increase the depth and propose a deeper embedding network
for entity representations, i.e., a narrow embedding layer and a multi-layer
dimension lifting network (LiftNet). Experiments on three public datasets show
that the proposed method (implemented based on TransE and DistMult) with
4-dimensional entity representations achieves more accurate link prediction
results than counterpart parameter-efficient KGE methods and strong KGE
baselines, including TransE and DistMult with 512-dimensional entity
representations.</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：SignCRF: Scalable Channel-agnostic Data-driven Radio Authentication  System</b></summary>
  <p><b>编号</b>：[308]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12811</p>
  <p><b>作者</b>：Amani Al-shawabka,  Philip Pietraski,  Sudhir B Pattar,  Pedram Johari,  Tommaso Melodia</p>
  <p><b>备注</b>：11 pages, 13 figures, 3 tables</p>
  <p><b>关键词</b>：Deep Learning, Radio Frequency Fingerprinting, Frequency Fingerprinting, Radio Frequency, Fingerprinting through Deep</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Radio Frequency Fingerprinting through Deep Learning (RFFDL) is a data-driven
IoT authentication technique that leverages the unique hardware-level
manufacturing imperfections associated with a particular device to recognize
(fingerprint) the device based on variations introduced in the transmitted
waveform. The proposed SignCRF is a scalable, channel-agnostic, data-driven
radio authentication platform with unmatched precision in fingerprinting
wireless devices based on their unique manufacturing impairments and
independent of the dynamic channel irregularities caused by mobility. SignCRF
consists of (i) a baseline classifier finely trained to authenticate devices
with high accuracy and at scale; (ii) an environment translator carefully
designed and trained to remove the dynamic channel impact from RF signals while
maintaining the radio's specific signature; (iii) a Max-Rule module that
selects the highest precision authentication technique between the baseline
classifier and the environment translator per radio. We design, train, and
validate the performance of SignCRF for multiple technologies in dynamic
environments and at scale (100 LoRa and 20 WiFi devices). We demonstrate that
SignCRF significantly improves the RFFDL performance by achieving as high as 5x
and 8x improvement in correct authentication of WiFi and LoRa devices when
compared to the state-of-the-art, respectively.</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：PACO: Provocation Involving Action, Culture, and Oppression</b></summary>
  <p><b>编号</b>：[310]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12808</p>
  <p><b>作者</b>：Vaibhav Garg,  Ganning Xu,  Munindar P. Singh</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：group based, religious groups, Indian Muslims, India, people identify</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In India, people identify with a particular group based on certain attributes
such as religion. The same religious groups are often provoked against each
other. Previous studies show the role of provocation in increasing tensions
between India's two prominent religious groups: Hindus and Muslims. With the
advent of the Internet, such provocation also surfaced on social media
platforms such as WhatsApp.
By leveraging an existing dataset of Indian WhatsApp posts, we identified
three categories of provoking sentences against Indian Muslims. Further, we
labeled 7,000 sentences for three provocation categories and called this
dataset PACO. We leveraged PACO to train a model that can identify provoking
sentences from a WhatsApp post. Our best model is fine-tuned RoBERTa and
achieved a 0.851 average AUC score over five-fold cross-validation.
Automatically identifying provoking sentences could stop provoking text from
reaching out to the masses, and can prevent possible discrimination or violence
against the target religious group.
Further, we studied the provocative speech through a pragmatic lens, by
identifying the dialog acts and impoliteness super-strategies used against the
religious group.</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：Granular-ball Optimization Algorithm</b></summary>
  <p><b>编号</b>：[311]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12807</p>
  <p><b>作者</b>：Shuyin Xia,  Jiancu Chen,  Bin Hou,  Guoyin Wang</p>
  <p><b>备注</b>：10 pages, 22 figures</p>
  <p><b>关键词</b>：finest granularity, designed based, intelligent optimization algorithms, optimization algorithms, optimization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The existing intelligent optimization algorithms are designed based on the
finest granularity, i.e., a point. This leads to weak global search ability and
inefficiency. To address this problem, we proposed a novel multi-granularity
optimization algorithm, namely granular-ball optimization algorithm (GBO), by
introducing granular-ball computing. GBO uses many granular-balls to cover the
solution space. Quite a lot of small and fine-grained granular-balls are used
to depict the important parts, and a little number of large and coarse-grained
granular-balls are used to depict the inessential parts. Fine multi-granularity
data description ability results in a higher global search capability and
faster convergence speed. In comparison with the most popular and
state-of-the-art algorithms, the experiments on twenty benchmark functions
demonstrate its better performance. The faster speed, higher approximation
ability of optimal solution, no hyper-parameters, and simpler design of GBO
make it an all-around replacement of most of the existing popular intelligent
optimization algorithms.</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：Evolving Populations of Diverse RL Agents with MAP-Elites</b></summary>
  <p><b>编号</b>：[314]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12803</p>
  <p><b>作者</b>：Thomas Pierrot,  Arthur Flajolet</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：powerful alternative optimization, alternative optimization paradigm, Quality Diversity, flagship algorithm MAP-ELITES, mutations and crossovers</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Quality Diversity (QD) has emerged as a powerful alternative optimization
paradigm that aims at generating large and diverse collections of solutions,
notably with its flagship algorithm MAP-ELITES (ME) which evolves solutions
through mutations and crossovers. While very effective for some unstructured
problems, early ME implementations relied exclusively on random search to
evolve the population of solutions, rendering them notoriously
sample-inefficient for high-dimensional problems, such as when evolving neural
networks. Follow-up works considered exploiting gradient information to guide
the search in order to address these shortcomings through techniques borrowed
from either Black-Box Optimization (BBO) or Reinforcement Learning (RL). While
mixing RL techniques with ME unlocked state-of-the-art performance for robotics
control problems that require a good amount of exploration, it also plagued
these ME variants with limitations common among RL algorithms that ME was free
of, such as hyperparameter sensitivity, high stochasticity as well as training
instability, including when the population size increases as some components
are shared across the population in recent approaches. Furthermore, existing
approaches mixing ME with RL tend to be tied to a specific RL algorithm, which
effectively prevents their use on problems where the corresponding RL algorithm
fails. To address these shortcomings, we introduce a flexible framework that
allows the use of any RL algorithm and alleviates the aforementioned
limitations by evolving populations of agents (whose definition include
hyperparameters and all learnable parameters) instead of just policies. We
demonstrate the benefits brought about by our framework through extensive
numerical experiments on a number of robotics control problems, some of which
with deceptive rewards, taken from the QD-RL literature.</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：IoT Device Identification Based on Network Communication Analysis Using  Deep Learning</b></summary>
  <p><b>编号</b>：[316]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12800</p>
  <p><b>作者</b>：Jaidip Kotak,  Yuval Elovici</p>
  <p><b>备注</b>：J Ambient Intell Human Comput (2022)</p>
  <p><b>关键词</b>：IoT devices, organization network, IoT, secure IoT devices, devices</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Attack vectors for adversaries have increased in organizations because of the
growing use of less secure IoT devices. The risk of attacks on an
organization's network has also increased due to the bring your own device
(BYOD) policy which permits employees to bring IoT devices onto the premises
and attach them to the organization's network. To tackle this threat and
protect their networks, organizations generally implement security policies in
which only white listed IoT devices are allowed on the organization's network.
To monitor compliance with such policies, it has become essential to
distinguish IoT devices permitted within an organization's network from non
white listed (unknown) IoT devices. In this research, deep learning is applied
to network communication for the automated identification of IoT devices
permitted on the network. In contrast to existing methods, the proposed
approach does not require complex feature engineering of the network
communication, because the 'communication behavior' of IoT devices is
represented as small images which are generated from the device's network
communication payload. The proposed approach is applicable for any IoT device,
regardless of the protocol used for communication. As our approach relies on
the network communication payload, it is also applicable for the IoT devices
behind a network address translation (NAT) enabled router. In this study, we
trained various classifiers on a publicly accessible dataset to identify IoT
devices in different scenarios, including the identification of known and
unknown IoT devices, achieving over 99% overall average detection accuracy.</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：Time Series as Images: Vision Transformer for Irregularly Sampled Time  Series</b></summary>
  <p><b>编号</b>：[317]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12799</p>
  <p><b>作者</b>：Zekun Li,  Shiyang Li,  Xifeng Yan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：sampled time series, Irregularly sampled time, medical applications, time series, increasingly prevalent</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Irregularly sampled time series are becoming increasingly prevalent in
various domains, especially in medical applications. Although different
highly-customized methods have been proposed to tackle irregularity, how to
effectively model their complicated dynamics and high sparsity is still an open
problem. This paper studies the problem from a whole new perspective:
transforming irregularly sampled time series into line graph images and
adapting powerful vision transformers to perform time series classification in
the same way as image classification. Our approach largely simplifies algorithm
designs without assuming prior knowledge and can be potentially extended as a
general-purpose framework. Despite its simplicity, we show that it
substantially outperforms state-of-the-art specialized algorithms on several
popular healthcare and human activity datasets. Especially in the challenging
leave-sensors-out setting where a subset of variables is masked during testing,
the performance improvement is up to 54.0\% in absolute F1 score points. Our
code and data are available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>83. <b>标题：An algorithmic framework for the optimization of deep neural networks  architectures and hyperparameters</b></summary>
  <p><b>编号</b>：[319]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12797</p>
  <p><b>作者</b>：Julie Keisler (EDF R&D OSIRIS, EDF R&D, CRIStAL),  El-Ghazali Talbi (CRIStAL),  Sandra Claudel (EDF R&D OSIRIS, EDF R&D),  Gilles Cabriel (EDF R&D OSIRIS, EDF R&D)</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：automatically generate efficient, generate efficient deep, efficient deep neural, deep neural networks, automatically generate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we propose an algorithmic framework to automatically generate
efficient deep neural networks and optimize their associated hyperparameters.
The framework is based on evolving directed acyclic graphs (DAGs), defining a
more flexible search space than the existing ones in the literature. It allows
mixtures of different classical operations: convolutions, recurrences and dense
layers, but also more newfangled operations such as self-attention. Based on
this search space we propose neighbourhood and evolution search operators to
optimize both the architecture and hyper-parameters of our networks. These
search operators can be used with any metaheuristic capable of handling mixed
search spaces. We tested our algorithmic framework with an evolutionary
algorithm on a time series prediction benchmark. The results demonstrate that
our framework was able to find models outperforming the established baseline on
numerous datasets.</p>
  </details>
</details>
<details>
  <summary>84. <b>标题：An Analysis of Abstractive Text Summarization Using Pre-trained Models</b></summary>
  <p><b>编号</b>：[320]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12796</p>
  <p><b>作者</b>：Tohida Rehman,  Suchandan Das,  Debarshi Kumar Sanyal,  Samiran Chattopadhyay</p>
  <p><b>备注</b>：11 Pages, 6 Figures, 3 Tables</p>
  <p><b>关键词</b>：Bing to find, People nowadays, find information, Yahoo, Internet</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>People nowadays use search engines like Google, Yahoo, and Bing to find
information on the Internet. Due to explosion in data, it is helpful for users
if they are provided relevant summaries of the search results rather than just
links to webpages. Text summarization has become a vital approach to help
consumers swiftly grasp vast amounts of this http URL this paper, different
pre-trained models for text summarization are evaluated on different datasets.
Specifically, we have used three different pre-trained models, namely,
google/pegasus-cnn-dailymail, T5-base, facebook/bart-large-cnn. We have
considered three different datasets, namely, CNN-dailymail, SAMSum and BillSum
to get the output from the above three models. The pre-trained models are
compared over these different datasets, each of 2000 examples, through ROUGH
and BLEU metrics.</p>
  </details>
</details>
<details>
  <summary>85. <b>标题：Named Entity Recognition Based Automatic Generation of Research  Highlights</b></summary>
  <p><b>编号</b>：[321]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12795</p>
  <p><b>作者</b>：Tohida Rehman,  Debarshi Kumar Sanyal,  Prasenjit Majumder,  Samiran Chattopadhyay</p>
  <p><b>备注</b>：7 Pages, 3 Figures, 2 Tables</p>
  <p><b>关键词</b>：traditionally prefaced, highlights, paper, scientific paper, named entity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A scientific paper is traditionally prefaced by an abstract that summarizes
the paper. Recently, research highlights that focus on the main findings of the
paper have emerged as a complementary summary in addition to an abstract.
However, highlights are not yet as common as abstracts, and are absent in many
papers. In this paper, we aim to automatically generate research highlights
using different sections of a research paper as input. We investigate whether
the use of named entity recognition on the input improves the quality of the
generated highlights. In particular, we have used two deep learning-based
models: the first is a pointer-generator network, and the second augments the
first model with coverage mechanism. We then augment each of the above models
with named entity recognition features. The proposed method can be used to
produce highlights for papers with missing highlights. Our experiments show
that adding named entity information improves the performance of the deep
learning-based summarizers in terms of ROUGE, METEOR and BERTScore measures.</p>
  </details>
</details>
<details>
  <summary>86. <b>标题：The power and limitations of learning quantum dynamics incoherently</b></summary>
  <p><b>编号</b>：[354]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2303.12834</p>
  <p><b>作者</b>：Sofiene Jerbi,  Joe Gibbs,  Manuel S. Rudolph,  Matthias C. Caro,  Patrick J. Coles,  Hsin-Yuan Huang,  Zoë Holmes</p>
  <p><b>备注</b>：6+9 pages, 7 figures</p>
  <p><b>关键词</b>：study quantum systems, important tool, tool to study, quantum systems, Quantum process learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Quantum process learning is emerging as an important tool to study quantum
systems. While studied extensively in coherent frameworks, where the target and
model system can share quantum information, less attention has been paid to
whether the dynamics of quantum systems can be learned without the system and
target directly interacting. Such incoherent frameworks are practically
appealing since they open up methods of transpiling quantum processes between
the different physical platforms without the need for technically challenging
hybrid entanglement schemes. Here we provide bounds on the sample complexity of
learning unitary processes incoherently by analyzing the number of measurements
that are required to emulate well-established coherent learning strategies. We
prove that if arbitrary measurements are allowed, then any efficiently
representable unitary can be efficiently learned within the incoherent
framework; however, when restricted to shallow-depth measurements only
low-entangling unitaries can be learned. We demonstrate our incoherent learning
algorithm for low entangling unitaries by successfully learning a 16-qubit
unitary on \texttt{ibmq\_kolkata}, and further demonstrate the scalabilty of
our proposed algorithm through extensive numerical experiments.</p>
  </details>
</details>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">徐耀彬</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://louishsu.xyz/2023/03/24/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">http://louishsu.xyz/2023/03/24/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://louishsu.xyz" target="_blank">LOUIS' BLOG</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2023/03/11/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.html"><img class="next-cover" src="https://img0.baidu.com/it/u=3005164807,267475947&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG?w=804&amp;h=500" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">强化学习</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">徐耀彬</div><div class="author-info__description">专注于自然语言处理前沿技术与应用价值！</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">14</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/isLouisHsu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/isLouisHsu" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:is.louishsu@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">记录和分享一些学习和开源内容，若有问题可通过邮箱is.louishsu@foxmail.com联系，欢迎交流！！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">统计</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">计算机视觉</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">自然语言处理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">机器学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">人工智能</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/03/24/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2023-03-24)"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv每日速递(2023-03-24)"/></a><div class="content"><a class="title" href="/2023/03/24/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2023-03-24)">Arxiv每日速递(2023-03-24)</a><time datetime="2023-03-24T00:39:44.495Z" title="发表于 2023-03-24 08:39:44">2023-03-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/11/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.html" title="强化学习"><img src="https://img0.baidu.com/it/u=3005164807,267475947&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG?w=804&amp;h=500" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="强化学习"/></a><div class="content"><a class="title" href="/2023/03/11/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.html" title="强化学习">强化学习</a><time datetime="2023-03-11T13:45:45.000Z" title="发表于 2023-03-11 21:45:45">2023-03-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/11/26/%E5%8D%87%E7%BA%A7%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%85%A8%E6%94%BB%E7%95%A5.html" title="升级深度学习开发环境全攻略"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="升级深度学习开发环境全攻略"/></a><div class="content"><a class="title" href="/2022/11/26/%E5%8D%87%E7%BA%A7%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%85%A8%E6%94%BB%E7%95%A5.html" title="升级深度学习开发环境全攻略">升级深度学习开发环境全攻略</a><time datetime="2022-11-26T15:29:06.000Z" title="发表于 2022-11-26 23:29:06">2022-11-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/11/17/2022%E5%85%A8%E7%90%83%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0%E5%A4%A7%E8%B5%9B(GAIIC2022)%EF%BC%9A%E5%95%86%E5%93%81%E6%A0%87%E9%A2%98%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB(%E4%BA%8C%E7%AD%89%E5%A5%96).html" title="2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖)"><img src="https://cdn.kesci.com/upload/image/r7j60un866.png?imageView2/2/w/2500/h/2500" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖)"/></a><div class="content"><a class="title" href="/2022/11/17/2022%E5%85%A8%E7%90%83%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0%E5%A4%A7%E8%B5%9B(GAIIC2022)%EF%BC%9A%E5%95%86%E5%93%81%E6%A0%87%E9%A2%98%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB(%E4%BA%8C%E7%AD%89%E5%A5%96).html" title="2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖)">2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖)</a><time datetime="2022-11-17T14:29:06.000Z" title="发表于 2022-11-17 22:29:06">2022-11-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/10/22/%E4%B8%AD%E5%9B%BD%E6%B3%95%E5%BE%8B%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E8%AF%84%E6%B5%8B(CAIL2021)%EF%BC%9A%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96(Rank2).html" title="中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)"><img src="http://cail.cipsc.org.cn/img/index_mainpic.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)"/></a><div class="content"><a class="title" href="/2021/10/22/%E4%B8%AD%E5%9B%BD%E6%B3%95%E5%BE%8B%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E8%AF%84%E6%B5%8B(CAIL2021)%EF%BC%9A%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96(Rank2).html" title="中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)">中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)</a><time datetime="2021-10-22T14:29:06.000Z" title="发表于 2021-10-22 22:29:06">2021-10-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2023 By 徐耀彬</div><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender"title="主题采用butterfly"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr"title="本站使用JsDelivr为静态资源提供CDN加速"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub"title="本站项目由Gtihub托管"alt="img"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"alt="img"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></br></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    let initData = {
      el: '#twikoo-wrap',
      envId: 'blog-',
      region: ''
    }

    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    twikoo.init(initData)
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'blog-',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/机器学习/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐱 机器学习 (2)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/自然语言处理/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 自然语言处理 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/竞赛相关/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 竞赛相关 (3)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/阅读笔记/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 阅读笔记 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="http://louishsu.xyz/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style>
  <script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/hexo-github-calendar@1.21/hexo_githubcalendar.js"></script>
  <script data-pjax>
        function GithubCalendarConfig(){
            var git_githubapiurl ="https://python-github-calendar-api.vercel.app/api?isLouisHsu";
            var git_color =['#ebedf0', '#fdcdec', '#fc9bd9', '#fa6ac5', '#f838b2', '#f5089f', '#c4067e', '#92055e', '#540336', '#48022f', '#30021f'];
            var git_user ="isLouisHsu";
            var parent_div_git = document.getElementById('recent-posts');
            var git_div_html = '<div class="recent-post-item" style="width:100%;height:auto;padding:10px;"><div id="github_loading" style="width:10%;height:100%;margin:0 auto;display: block"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"  viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animateTransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animateTransform></path></svg></div><div id="github_container"></div></div>';
            if(parent_div_git && location.pathname =='/'){
                console.log('已挂载github calendar')
                // parent_div_git.innerHTML=git_div_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",git_div_html) // 有报错，但不影响使用(支持pjax跳转)
            };
            GithubCalendar(git_githubapiurl,git_color,git_user)
        }
        if(document.getElementById('recent-posts')){
            GithubCalendarConfig()
        }
    </script>
    <style>#github_container{min-height:280px}@media screen and (max-width:650px) {#github_container{background-image:;min-height:0px}}</style>
    <style></style><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>