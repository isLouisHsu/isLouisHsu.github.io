<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Arxiv每日速递(2023-05-22) | LOUIS' BLOG</title><meta name="author" content="徐耀彬"><meta name="copyright" content="徐耀彬"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。 统计 今日共更新369篇论文，其中：  70篇计算机视觉（cs.CV） 108篇自然语言处理（cs.CL） 120篇机器学习（cs.LG） 99篇人工智能（cs.AI）  计算机视觉    1. 标题：Chupa: Carving 3D Clothed Humans fro">
<meta property="og:type" content="article">
<meta property="og:title" content="Arxiv每日速递(2023-05-22)">
<meta property="og:url" content="http://louishsu.xyz/2023/05/22/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">
<meta property="og:site_name" content="LOUIS&#39; BLOG">
<meta property="og:description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。 统计 今日共更新369篇论文，其中：  70篇计算机视觉（cs.CV） 108篇自然语言处理（cs.CL） 120篇机器学习（cs.LG） 99篇人工智能（cs.AI）  计算机视觉    1. 标题：Chupa: Carving 3D Clothed Humans fro">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png">
<meta property="article:published_time" content="2023-05-22T00:42:30.587Z">
<meta property="article:modified_time" content="2023-05-22T00:43:59.656Z">
<meta property="article:author" content="徐耀彬">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://louishsu.xyz/2023/05/22/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-05-22 08:43:59'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/background.css"><script src="https://cdn.jsdelivr.net/npm/echarts@4.7.0/dist/echarts.min.js"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">19</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-brands fa-app-store"></i><span> 利器</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://code.visualstudio.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> VSCode：微软旗下的跨平台代码编辑软件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mobaxterm.mobatek.net/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> MobaXterm：超好用的全能远程终端</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.zotero.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zotero：便于收集、组织、引用、共享的文献管理工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/CopyTranslator/CopyTranslator"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> CopyTranslator：“复制即翻译”的外文辅助阅读翻译解决方案</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ditto-cp.sourceforge.io/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Ditto：强大的Windows剪贴板增强工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/indiff/qttabbar"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> QtTabBar：在Windows资源管理器中使用多标签功能扩展工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/sentialx/multrin"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Multrin：“窗口合并”辅助小工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/Wox-launcher/Wox"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Wox &amp; Everything：基于名称快速定位文件和文件夹的搜索工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/NickeManarin/ScreenToGif"><i class="fa-fw fa-regular fa-star"></i><span> ScreenToGif：快速录制屏幕指定区域并保存为动图文件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mathpix.com/"><i class="fa-fw fa-regular fa-star"></i><span> Mathpix Snipping：识别数学公式并转换成LaTeX</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">LOUIS' BLOG</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-brands fa-app-store"></i><span> 利器</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://code.visualstudio.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> VSCode：微软旗下的跨平台代码编辑软件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mobaxterm.mobatek.net/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> MobaXterm：超好用的全能远程终端</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.zotero.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zotero：便于收集、组织、引用、共享的文献管理工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/CopyTranslator/CopyTranslator"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> CopyTranslator：“复制即翻译”的外文辅助阅读翻译解决方案</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ditto-cp.sourceforge.io/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Ditto：强大的Windows剪贴板增强工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/indiff/qttabbar"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> QtTabBar：在Windows资源管理器中使用多标签功能扩展工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/sentialx/multrin"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Multrin：“窗口合并”辅助小工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/Wox-launcher/Wox"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Wox &amp; Everything：基于名称快速定位文件和文件夹的搜索工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/NickeManarin/ScreenToGif"><i class="fa-fw fa-regular fa-star"></i><span> ScreenToGif：快速录制屏幕指定区域并保存为动图文件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mathpix.com/"><i class="fa-fw fa-regular fa-star"></i><span> Mathpix Snipping：识别数学公式并转换成LaTeX</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Arxiv每日速递(2023-05-22)<a class="post-edit-link" href="https://github.com/isLouisHsu/blog/tree/master/source_posts/Arxiv每日速递.md" title="编辑" target="_blank"><i class="fas fa-pencil-square"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-05-22T00:42:30.587Z" title="发表于 2023-05-22 08:42:30">2023-05-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-05-22T00:43:59.656Z" title="更新于 2023-05-22 08:43:59">2023-05-22</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">阅读笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">98k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>586分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2023/05/22/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。</p>
<h1>统计</h1>
<p>今日共更新369篇论文，其中：</p>
<ul>
<li><a href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89">70篇计算机视觉（cs.CV）</a></li>
<li><a href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86">108篇自然语言处理（cs.CL）</a></li>
<li><a href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">120篇机器学习（cs.LG）</a></li>
<li><a href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD">99篇人工智能（cs.AI）</a></li>
</ul>
<h1>计算机视觉</h1>
<details>
  <summary>1. <b>标题：Chupa: Carving 3D Clothed Humans from Skinned Shape Priors using 2D  Diffusion Probabilistic Models</b></summary>
  <p><b>编号</b>：[1]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11870</p>
  <p><b>作者</b>：Byungjun Kim,  Patrick Kwon,  Kwangho Lee,  Myunggi Lee,  Sookwan Han,  Daesik Kim,  Hanbyul Joo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：normal maps, generation pipeline, generation, normal, human</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a 3D generation pipeline that uses diffusion models to generate
realistic human digital avatars. Due to the wide variety of human identities,
poses, and stochastic details, the generation of 3D human meshes has been a
challenging problem. To address this, we decompose the problem into 2D normal
map generation and normal map-based 3D reconstruction. Specifically, we first
simultaneously generate realistic normal maps for the front and backside of a
clothed human, dubbed dual normal maps, using a pose-conditional diffusion
model. For 3D reconstruction, we ``carve'' the prior SMPL-X mesh to a detailed
3D mesh according to the normal maps through mesh optimization. To further
enhance the high-frequency details, we present a diffusion resampling scheme on
both body and facial regions, thus encouraging the generation of realistic
digital avatars. We also seamlessly incorporate a recent text-to-image
diffusion model to support text-based human identity control. Our method,
namely, Chupa, is capable of generating realistic 3D clothed humans with better
perceptual quality and identity variety.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Video Killed the HD-Map: Predicting Driving Behavior Directly From Drone  Images</b></summary>
  <p><b>编号</b>：[10]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11856</p>
  <p><b>作者</b>：Yunpeng Liu,  Vasileios Lioutas,  Jonathan Wilder Lavington,  Matthew Niedoba,  Justice Sefas,  Setareh Dabiri,  Dylan Green,  Xiaoxuan Liang,  Berend Zwartsenberg,  Adam Ścibior,  Frank Wood</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：increasingly realistic simulations, realistic simulations, development of algorithms, demonstrations has led, led to increasingly</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The development of algorithms that learn behavioral driving models using
human demonstrations has led to increasingly realistic simulations. In general,
such models learn to jointly predict trajectories for all controlled agents by
exploiting road context information such as drivable lanes obtained from
manually annotated high-definition (HD) maps. Recent studies show that these
models can greatly benefit from increasing the amount of human data available
for training. However, the manual annotation of HD maps which is necessary for
every new location puts a bottleneck on efficiently scaling up human traffic
datasets. We propose a drone birdview image-based map (DBM) representation that
requires minimal annotation and provides rich road context information. We
evaluate multi-agent trajectory prediction using the DBM by incorporating it
into a differentiable driving simulator as an image-texture-based
differentiable rendering module. Our results demonstrate competitive
multi-agent trajectory prediction performance when using our DBM representation
as compared to models trained with rasterized HD maps.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Any-to-Any Generation via Composable Diffusion</b></summary>
  <p><b>编号</b>：[13]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11846</p>
  <p><b>作者</b>：Zineng Tang,  Ziyi Yang,  Chenguang Zhu,  Michael Zeng,  Mohit Bansal</p>
  <p><b>备注</b>：Project Page: this https URL</p>
  <p><b>关键词</b>：generative model capable, model capable, capable of generating, modalities, generative model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present Composable Diffusion (CoDi), a novel generative model capable of
generating any combination of output modalities, such as language, image,
video, or audio, from any combination of input modalities. Unlike existing
generative AI systems, CoDi can generate multiple modalities in parallel and
its input is not limited to a subset of modalities like text or image. Despite
the absence of training datasets for many combinations of modalities, we
propose to align modalities in both the input and output space. This allows
CoDi to freely condition on any input combination and generate any group of
modalities, even if they are not present in the training data. CoDi employs a
novel composable generation strategy which involves building a shared
multimodal space by bridging alignment in the diffusion process, enabling the
synchronized generation of intertwined modalities, such as temporally aligned
video and audio. Highly customizable and flexible, CoDi achieves strong
joint-modality generation quality, and outperforms or is on par with the
unimodal state-of-the-art for single-modality synthesis. The project page with
demonstrations and code is at this https URL</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：RxnScribe: A Sequence Generation Model for Reaction Diagram Parsing</b></summary>
  <p><b>编号</b>：[14]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11845</p>
  <p><b>作者</b>：Yujie Qian,  Jiang Guo,  Zhengkai Tu,  Connor W. Coley,  Regina Barzilay</p>
  <p><b>备注</b>：To be published in the Journal of Chemical Information and Modeling</p>
  <p><b>关键词</b>：extracting reaction schemes, chemistry literature, Reaction, extracting reaction, reaction schemes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reaction diagram parsing is the task of extracting reaction schemes from a
diagram in the chemistry literature. The reaction diagrams can be arbitrarily
complex, thus robustly parsing them into structured data is an open challenge.
In this paper, we present RxnScribe, a machine learning model for parsing
reaction diagrams of varying styles. We formulate this structured prediction
task with a sequence generation approach, which condenses the traditional
pipeline into an end-to-end model. We train RxnScribe on a dataset of 1,378
diagrams and evaluate it with cross validation, achieving an 80.0% soft match
F1 score, with significant improvements over previous models. Our code and data
are publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：MaGIC: Multi-modality Guided Image Completion</b></summary>
  <p><b>编号</b>：[25]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11818</p>
  <p><b>作者</b>：Yongsheng Yu,  Hao Wang,  Tiejian Luo,  Heng Fan,  Libo Zhang</p>
  <p><b>备注</b>：11 pages, 6 figures</p>
  <p><b>关键词</b>：large missing regions, missing regions due, image completion, vanilla image completion, Guided Image Completion</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The vanilla image completion approaches are sensitive to the large missing
regions due to limited available reference information for plausible
generation. To mitigate this, existing methods incorporate the extra cue as a
guidance for image completion. Despite improvements, these approaches are often
restricted to employing a single modality (e.g., segmentation or sketch maps),
which lacks scalability in leveraging multi-modality for more plausible
completion. In this paper, we propose a novel, simple yet effective method for
Multi-modal Guided Image Completion, dubbed MaGIC, which not only supports a
wide range of single modality as the guidance (e.g., text, canny edge, sketch,
segmentation, reference image, depth, and pose), but also adapts to arbitrarily
customized combination of these modalities (i.e., arbitrary multi-modality) for
image completion. For building MaGIC, we first introduce a modality-specific
conditional U-Net (MCU-Net) that injects single-modal signal into a U-Net
denoiser for single-modal guided image completion. Then, we devise a consistent
modality blending (CMB) method to leverage modality signals encoded in multiple
learned MCU-Nets through gradient guidance in latent space. Our CMB is
training-free, and hence avoids the cumbersome joint re-training of different
modalities, which is the secret of MaGIC to achieve exceptional flexibility in
accommodating new modalities for completion. Experiments show the superiority
of MaGIC over state-of-arts and its generalization to various completion tasks
including in/out-painting and local editing. Our project with code and models
is available at this http URL.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：A One-Class Classifier for the Detection of GAN Manipulated  Multi-Spectral Satellite Images</b></summary>
  <p><b>编号</b>：[33]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11795</p>
  <p><b>作者</b>：Lydia Abady,  Giovanna Maria Dimitri,  Mauro Barni</p>
  <p><b>备注</b>：17 pages, 4 figures</p>
  <p><b>关键词</b>：highly realistic image, realistic image quality, image quality achieved, highly realistic, quality achieved</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The highly realistic image quality achieved by current image generative
models has many academic and industrial applications. To limit the use of such
models to benign applications, though, it is necessary that tools to
conclusively detect whether an image has been generated synthetically or not
are developed. For this reason, several detectors have been developed providing
excellent performance in computer vision applications, however, they can not be
applied as they are to multispectral satellite images, and hence new models
must be trained. In general, two-class classifiers can achieve very good
detection accuracies, however they are not able to generalise to image domains
and generative models architectures different than those used during training.
For this reason, in this paper, we propose a one-class classifier based on
Vector Quantized Variational Autoencoder 2 (VQ-VAE 2) features to overcome the
limitations of two-class classifiers. First, we emphasize the generalization
problem that binary classifiers suffer from by training and testing an
EfficientNet-B4 architecture on multiple multispectral datasets. Then we show
that, since the VQ-VAE 2 based classifier is trained only on pristine images,
it is able to detect images belonging to different domains and generated by
architectures that have not been used during training. Last, we compare the two
classifiers head-to-head on the same generated datasets, highlighting the
superiori generalization capabilities of the VQ-VAE 2-based detector.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Neural Foundations of Mental Simulation: Future Prediction of Latent  Representations on Dynamic Scenes</b></summary>
  <p><b>编号</b>：[43]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11772</p>
  <p><b>作者</b>：Aran Nayebi,  Rishi Rajalingham,  Mehrdad Jazayeri,  Guangyu Robert Yang</p>
  <p><b>备注</b>：17 pages, 6 figures</p>
  <p><b>关键词</b>：underlying dynamical trajectories, plausible future states, physical world, objects and events, consequences of actions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Humans and animals have a rich and flexible understanding of the physical
world, which enables them to infer the underlying dynamical trajectories of
objects and events, plausible future states, and use that to plan and
anticipate the consequences of actions. However, the neural mechanisms
underlying these computations are unclear. We combine a goal-driven modeling
approach with dense neurophysiological data and high-throughput human
behavioral readouts to directly impinge on this question. Specifically, we
construct and evaluate several classes of sensory-cognitive networks to predict
the future state of rich, ethologically-relevant environments, ranging from
self-supervised end-to-end models with pixel-wise or object-centric objectives,
to models that future predict in the latent space of purely static image-based
or dynamic video-based pretrained foundation models. We find strong
differentiation across these model classes in their ability to predict neural
and behavioral data both within and across diverse environments. In particular,
we find that neural responses are currently best predicted by models trained to
predict the future state of their environment in the latent space of pretrained
foundation models optimized for dynamic scenes in a self-supervised manner.
Notably, models that future predict in the latent space of video foundation
models that are optimized to support a diverse range of sensorimotor tasks,
reasonably match both human behavioral error patterns and neural dynamics
across all environmental scenarios that we were able to test. Overall, these
findings suggest that the neural mechanisms and behaviors of primate mental
simulation are thus far most consistent with being optimized to future predict
on dynamic, reusable visual representations that are useful for embodied AI
more generally.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Enhancing Vision-Language Pre-Training with Jointly Learned Questioner  and Dense Captioner</b></summary>
  <p><b>编号</b>：[44]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11769</p>
  <p><b>作者</b>：Zikang Liu,  Sihan Chen,  Longteng Guo,  Handong Li,  Xingjian He,  Jing Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：visual question answering, demonstrated significant success, including image captioning, VQA and dense, Large pre-trained multimodal</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large pre-trained multimodal models have demonstrated significant success in
a range of downstream tasks, including image captioning, image-text retrieval,
visual question answering (VQA), etc. However, many of these methods rely on
image-text pairs collected from the web as pre-training data and unfortunately
overlook the need for fine-grained feature alignment between vision and
language modalities, which requires detailed understanding of images and
language expressions. While integrating VQA and dense captioning (DC) into
pre-training can address this issue, acquiring image-question-answer as well as
image-location-caption triplets is challenging and time-consuming.
Additionally, publicly available datasets for VQA and dense captioning are
typically limited in scale due to manual data collection and labeling efforts.
In this paper, we propose a novel method called Joint QA and DC GEneration
(JADE), which utilizes a pre-trained multimodal model and easily-crawled
image-text pairs to automatically generate and filter large-scale VQA and dense
captioning datasets. We apply this method to the Conceptual Caption (CC3M)
dataset to generate a new dataset called CC3M-QA-DC. Experiments show that when
used for pre-training in a multi-task manner, CC3M-QA-DC can improve the
performance with various backbones on various downstream tasks. Furthermore,
our generated CC3M-QA-DC can be combined with larger image-text datasets (e.g.,
CC15M) and achieve competitive results compared with models using much more
data. Code and dataset will be released.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Generating Visual Spatial Description via Holistic 3D Scene  Understanding</b></summary>
  <p><b>编号</b>：[45]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11768</p>
  <p><b>作者</b>：Yu Zhao,  Hao Fei,  Wei Ji,  Jianguo Wei,  Meishan Zhang,  Min Zhang,  Tat-Seng Chua</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Visual spatial description, aims to generate, Existing VSD work, VSD, target objects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Visual spatial description (VSD) aims to generate texts that describe the
spatial relations of the given objects within images. Existing VSD work merely
models the 2D geometrical vision features, thus inevitably falling prey to the
problem of skewed spatial understanding of target objects. In this work, we
investigate the incorporation of 3D scene features for VSD. With an external 3D
scene extractor, we obtain the 3D objects and scene features for input images,
based on which we construct a target object-centered 3D spatial scene graph
(Go3D-S2G), such that we model the spatial semantics of target objects within
the holistic 3D scenes. Besides, we propose a scene subgraph selecting
mechanism, sampling topologically-diverse subgraphs from Go3D-S2G, where the
diverse local structure features are navigated to yield spatially-diversified
text generation. Experimental results on two VSD datasets demonstrate that our
framework outperforms the baselines significantly, especially improving on the
cases with complex visual spatial relations. Meanwhile, our method can produce
more spatially-diversified generation. Code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Survey of Automatic Plankton Image Recognition: Challenges, Existing  Solutions and Future Perspectives</b></summary>
  <p><b>编号</b>：[56]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11739</p>
  <p><b>作者</b>：Tuomas Eerola,  Daniel Batrakhanov,  Nastaran Vatankhah Barazandeh,  Kaisa Kraft,  Lumi Haraguchi,  Lasse Lensu,  Sanna Suikkanen,  Jukka Seppälä,  Timo Tamminen,  Heikki Kälviäinen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：plankton recognition, Planktonic organisms, plankton, ecosystems and respond, respond quickly</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Planktonic organisms are key components of aquatic ecosystems and respond
quickly to changes in the environment, therefore their monitoring is vital to
understand the changes in the environment. Yet, monitoring plankton at
appropriate scales still remains a challenge, limiting our understanding of
functioning of aquatic systems and their response to changes. Modern plankton
imaging instruments can be utilized to sample at high frequencies, enabling
novel possibilities to study plankton populations. However, manual analysis of
the data is costly, time consuming and expert based, making such approach
unsuitable for large-scale application and urging for automatic solutions. The
key problem related to the utilization of plankton datasets through image
analysis is plankton recognition. Despite the large amount of research done,
automatic methods have not been widely adopted for operational use. In this
paper, a comprehensive survey on existing solutions for automatic plankton
recognition is presented. First, we identify the most notable challenges that
that make the development of plankton recognition systems difficult. Then, we
provide a detailed description of solutions for these challenges proposed in
plankton recognition literature. Finally, we propose a workflow to identify the
specific challenges in new datasets and the recommended approaches to address
them. For many of the challenges, applicable solutions exist. However,
important challenges remain unsolved: 1) the domain shift between the datasets
hindering the development of a general plankton recognition system that would
work across different imaging instruments, 2) the difficulty to identify and
process the images of previously unseen classes, and 3) the uncertainty in
expert annotations that affects the training of the machine learning models for
recognition. These challenges should be addressed in the future research.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Long-tailed Visual Recognition via Gaussian Clouded Logit Adjustment</b></summary>
  <p><b>编号</b>：[59]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11733</p>
  <p><b>作者</b>：Mengke Li,  Yiu-ming Cheung,  Yang Lu</p>
  <p><b>备注</b>：Published as a conference paper at CVPR 2022. arXiv admin note: text overlap with arXiv:2305.10648</p>
  <p><b>关键词</b>：deep neural networks, achieved great success, Long-tailed data, neural networks, big challenge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Long-tailed data is still a big challenge for deep neural networks, even
though they have achieved great success on balanced data. We observe that
vanilla training on long-tailed data with cross-entropy loss makes the
instance-rich head classes severely squeeze the spatial distribution of the
tail classes, which leads to difficulty in classifying tail class samples.
Furthermore, the original cross-entropy loss can only propagate gradient
short-lively because the gradient in softmax form rapidly approaches zero as
the logit difference increases. This phenomenon is called softmax saturation.
It is unfavorable for training on balanced data, but can be utilized to adjust
the validity of the samples in long-tailed data, thereby solving the distorted
embedding space of long-tailed problems. To this end, this paper proposes the
Gaussian clouded logit adjustment by Gaussian perturbation of different class
logits with varied amplitude. We define the amplitude of perturbation as cloud
size and set relatively large cloud sizes to tail classes. The large cloud size
can reduce the softmax saturation and thereby making tail class samples more
active as well as enlarging the embedding space. To alleviate the bias in a
classifier, we therefore propose the class-based effective number sampling
strategy with classifier re-training. Extensive experiments on benchmark
datasets validate the superior performance of the proposed method. Source code
is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：ViDaS Video Depth-aware Saliency Network</b></summary>
  <p><b>编号</b>：[61]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11729</p>
  <p><b>作者</b>：Ioanna Diamanti,  Antigoni Tsiami,  Petros Koutras,  Petros Maragos</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：fully convolutional Video, introduce ViDaS, fully convolutional, Saliency, RGB frames</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce ViDaS, a two-stream, fully convolutional Video, Depth-Aware
Saliency network to address the problem of attention modeling ``in-the-wild",
via saliency prediction in videos. Contrary to existing visual saliency
approaches using only RGB frames as input, our network employs also depth as an
additional modality. The network consists of two visual streams, one for the
RGB frames, and one for the depth frames. Both streams follow an
encoder-decoder approach and are fused to obtain a final saliency map. The
network is trained end-to-end and is evaluated in a variety of different
databases with eye-tracking data, containing a wide range of video content.
Although the publicly available datasets do not contain depth, we estimate it
using three different state-of-the-art methods, to enable comparisons and a
deeper insight. Our method outperforms in most cases state-of-the-art models
and our RGB-only variant, which indicates that depth can be beneficial to
accurately estimating saliency in videos displayed on a 2D screen. Depth has
been widely used to assist salient object detection problems, where it has been
proven to be very beneficial. Our problem though differs significantly from
salient object detection, since it is not restricted to specific salient
objects, but predicts human attention in a more general aspect. These two
problems not only have different objectives, but also different ground truth
data and evaluation metrics. To our best knowledge, this is the first
competitive deep learning video saliency estimation approach that combines both
RGB and Depth features to address the general problem of saliency estimation
``in-the-wild". The code will be publicly released.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Information Screening whilst Exploiting! Multimodal Relation Extraction  with Feature Denoising and Multimodal Topic Modeling</b></summary>
  <p><b>编号</b>：[65]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11719</p>
  <p><b>作者</b>：Shengqiong Wu,  Hao Fei,  Yixin Cao,  Lidong Bing,  Tat-Seng Chua</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：multimodal relation extraction, Existing research, relation extraction, faces two co-existing, co-existing challenges</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing research on multimodal relation extraction (MRE) faces two
co-existing challenges, internal-information over-utilization and
external-information under-exploitation. To combat that, we propose a novel
framework that simultaneously implements the idea of internal-information
screening and external-information exploiting. First, we represent the
fine-grained semantic structures of the input image and text with the visual
and textual scene graphs, which are further fused into a unified cross-modal
graph (CMG). Based on CMG, we perform structure refinement with the guidance of
the graph information bottleneck principle, actively denoising the
less-informative features. Next, we perform topic modeling over the input image
and text, incorporating latent multimodal topic features to enrich the
contexts. On the benchmark MRE dataset, our system outperforms the current best
model significantly. With further in-depth analyses, we reveal the great
potential of our method for the MRE task. Our codes are open at
this https URL.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Towards Accurate Image Coding: Improved Autoregressive Image Generation  with Dynamic Vector Quantization</b></summary>
  <p><b>编号</b>：[66]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11718</p>
  <p><b>作者</b>：Mengqi Huang,  Zhendong Mao,  Zhuowei Chen,  Yongdong Zhang</p>
  <p><b>备注</b>：CVPR 2023</p>
  <p><b>关键词</b>：Existing vector quantization, autoregressive models follow, learned codebook, Existing vector, vector quantization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing vector quantization (VQ) based autoregressive models follow a
two-stage generation paradigm that first learns a codebook to encode images as
discrete codes, and then completes generation based on the learned codebook.
However, they encode fixed-size image regions into fixed-length codes and
ignore their naturally different information densities, which results in
insufficiency in important regions and redundancy in unimportant ones, and
finally degrades the generation quality and speed. Moreover, the fixed-length
coding leads to an unnatural raster-scan autoregressive generation. To address
the problem, we propose a novel two-stage framework: (1) Dynamic-Quantization
VAE (DQ-VAE) which encodes image regions into variable-length codes based on
their information densities for an accurate and compact code representation.
(2) DQ-Transformer which thereby generates images autoregressively from
coarse-grained (smooth regions with fewer codes) to fine-grained (details
regions with more codes) by modeling the position and content of codes in each
granularity alternately, through a novel stacked-transformer architecture and
shared-content, non-shared position input layers designs. Comprehensive
experiments on various generation tasks validate our superiorities in both
effectiveness and efficiency. Code will be released at
this https URL.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Efficient and Deterministic Search Strategy Based on Residual  Projections for Point Cloud Registration</b></summary>
  <p><b>编号</b>：[68]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11716</p>
  <p><b>作者</b>：Xinyi Li,  Yinlong Liu,  Hu Cao,  Xueli Liu,  Feihu Zhang,  Alois Knoll</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：cloud registration paradigm, typical point cloud, point cloud registration, Estimating the rigid, scans through putative</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Estimating the rigid transformation between two LiDAR scans through putative
3D correspondences is a typical point cloud registration paradigm. Current 3D
feature matching approaches commonly lead to numerous outlier correspondences,
making outlier-robust registration techniques indispensable. Many recent
studies have adopted the branch and bound (BnB) optimization framework to solve
the correspondence-based point cloud registration problem globally and
deterministically. Nonetheless, BnB-based methods are time-consuming to search
the entire 6-dimensional parameter space, since their computational complexity
is exponential to the dimension of the solution domain. In order to enhance
algorithm efficiency, existing works attempt to decouple the 6 degrees of
freedom (DOF) original problem into two 3-DOF sub-problems, thereby reducing
the dimension of the parameter space. In contrast, our proposed approach
introduces a novel pose decoupling strategy based on residual projections,
effectively decomposing the raw problem into three 2-DOF rotation search
sub-problems. Subsequently, we employ a novel BnB-based search method to solve
these sub-problems, achieving efficient and deterministic registration.
Furthermore, our method can be adapted to address the challenging problem of
simultaneous pose and correspondence registration (SPCR). Through extensive
experiments conducted on synthetic and real-world datasets, we demonstrate that
our proposed method outperforms state-of-the-art methods in terms of
efficiency, while simultaneously ensuring robustness.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：S-JEA: Stacked Joint Embedding Architectures for Self-Supervised Visual  Representation Learning</b></summary>
  <p><b>编号</b>：[71]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11701</p>
  <p><b>作者</b>：Alžběta Manová,  Aiden Durrant,  Georgios Leontidis</p>
  <p><b>备注</b>：9 pages, 4 figures, 3 tables</p>
  <p><b>关键词</b>：demonstrate high empirical, high empirical success, learning image representations, Self-Supervised Learning, learning image</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The recent emergence of Self-Supervised Learning (SSL) as a fundamental
paradigm for learning image representations has, and continues to, demonstrate
high empirical success in a variety of tasks. However, most SSL approaches fail
to learn embeddings that capture hierarchical semantic concepts that are
separable and interpretable. In this work, we aim to learn highly separable
semantic hierarchical representations by stacking Joint Embedding Architectures
(JEA) where higher-level JEAs are input with representations of lower-level
JEA. This results in a representation space that exhibits distinct
sub-categories of semantic concepts (e.g., model and colour of vehicles) in
higher-level JEAs. We empirically show that representations from stacked JEA
perform on a similar level as traditional JEA with comparative parameter counts
and visualise the representation spaces to validate the semantic hierarchies.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Surgical-VQLA: Transformer with Gated Vision-Language Embedding for  Visual Question Localized-Answering in Robotic Surgery</b></summary>
  <p><b>编号</b>：[75]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11692</p>
  <p><b>作者</b>：Long Bai,  Mobarakol Islam,  Lalithkumar Seenivasan,  Hongliang Ren</p>
  <p><b>备注</b>：To appear in IEEE ICRA 2023. Code and data availability: this https URL</p>
  <p><b>关键词</b>：junior residents, availability of computer-aided, computer-aided simulators, residents still heavily, heavily rely</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite the availability of computer-aided simulators and recorded videos of
surgical procedures, junior residents still heavily rely on experts to answer
their queries. However, expert surgeons are often overloaded with clinical and
academic workloads and limit their time in answering. For this purpose, we
develop a surgical question-answering system to facilitate robot-assisted
surgical scene and activity understanding from recorded videos. Most of the
existing VQA methods require an object detector and regions based feature
extractor to extract visual features and fuse them with the embedded text of
the question for answer generation. However, (1) surgical object detection
model is scarce due to smaller datasets and lack of bounding box annotation;
(2) current fusion strategy of heterogeneous modalities like text and image is
naive; (3) the localized answering is missing, which is crucial in complex
surgical scenarios. In this paper, we propose Visual Question
Localized-Answering in Robotic Surgery (Surgical-VQLA) to localize the specific
surgical area during the answer prediction. To deal with the fusion of the
heterogeneous modalities, we design gated vision-language embedding (GVLE) to
build input patches for the Language Vision Transformer (LViT) to predict the
answer. To get localization, we add the detection head in parallel with the
prediction head of the LViT. We also integrate GIoU loss to boost localization
performance by preserving the accuracy of the question-answering model. We
annotate two datasets of VQLA by utilizing publicly available surgical videos
from MICCAI challenges EndoVis-17 and 18. Our validation results suggest that
Surgical-VQLA can better understand the surgical scene and localize the
specific area related to the question-answering. GVLE presents an efficient
language-vision embedding technique by showing superior performance over the
existing benchmarks.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Learning Global-aware Kernel for Image Harmonization</b></summary>
  <p><b>编号</b>：[80]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11676</p>
  <p><b>作者</b>：Xintian Shen,  Jiangning Zhang,  Jun Chen,  Shipeng Bai,  Yue Han,  Yabiao Wang,  Chengjie Wang,  Yong Liu</p>
  <p><b>备注</b>：10 pages, 10 figures</p>
  <p><b>关键词</b>：visual inconsistency problem, aims to solve, solve the visual, visual inconsistency, inconsistency problem</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image harmonization aims to solve the visual inconsistency problem in
composited images by adaptively adjusting the foreground pixels with the
background as references. Existing methods employ local color transformation or
region matching between foreground and background, which neglects powerful
proximity prior and independently distinguishes fore-/back-ground as a whole
part for harmonization. As a result, they still show a limited performance
across varied foreground objects and scenes. To address this issue, we propose
a novel Global-aware Kernel Network (GKNet) to harmonize local regions with
comprehensive consideration of long-distance background references.
Specifically, GKNet includes two parts, \ie, harmony kernel prediction and
harmony kernel modulation branches. The former includes a Long-distance
Reference Extractor (LRE) to obtain long-distance context and Kernel Prediction
Blocks (KPB) to predict multi-level harmony kernels by fusing global
information with local features. To achieve this goal, a novel Selective
Correlation Fusion (SCF) module is proposed to better select relevant
long-distance background references for local harmonization. The latter employs
the predicted kernels to harmonize foreground regions with both local and
global awareness. Abundant experiments demonstrate the superiority of our
method for image harmonization over state-of-the-art methods, \eg, achieving
39.53dB PSNR that surpasses the best counterpart by +0.78dB $\uparrow$;
decreasing fMSE/MSE by 11.5\%$\downarrow$/6.7\%$\downarrow$ compared with the
SoTA method. Code will be available at
\href{this https URL}{here}.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Cinematic Mindscapes: High-quality Video Reconstruction from Brain  Activity</b></summary>
  <p><b>编号</b>：[81]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11675</p>
  <p><b>作者</b>：Zijiao Chen,  Jiaxin Qing,  Juan Helen Zhou</p>
  <p><b>备注</b>：15 pages, 11 figures, submitted to anonymous conference</p>
  <p><b>关键词</b>：Reconstructing human vision, cognitive process, human vision, understand our cognitive, Reconstructing human</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reconstructing human vision from brain activities has been an appealing task
that helps to understand our cognitive process. Even though recent research has
seen great success in reconstructing static images from non-invasive brain
recordings, work on recovering continuous visual experiences in the form of
videos is limited. In this work, we propose Mind-Video that learns
spatiotemporal information from continuous fMRI data of the cerebral cortex
progressively through masked brain modeling, multimodal contrastive learning
with spatiotemporal attention, and co-training with an augmented Stable
Diffusion model that incorporates network temporal inflation. We show that
high-quality videos of arbitrary frame rates can be reconstructed with
Mind-Video using adversarial guidance. The recovered videos were evaluated with
various semantic and pixel-level metrics. We achieved an average accuracy of
85% in semantic classification tasks and 0.19 in structural similarity index
(SSIM), outperforming the previous state-of-the-art by 45%. We also show that
our model is biologically plausible and interpretable, reflecting established
physiological processes.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Few-shot 3D Shape Generation</b></summary>
  <p><b>编号</b>：[86]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11664</p>
  <p><b>作者</b>：Jingyuan Zhu,  Huimin Ma,  Jiansheng Chen,  Jian Yuan</p>
  <p><b>备注</b>：23 pages, 15 figures</p>
  <p><b>关键词</b>：virtual reality, wide variety, variety of applications, data, shape generation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Realistic and diverse 3D shape generation is helpful for a wide variety of
applications such as virtual reality, gaming, and animation. Modern generative
models, such as GANs and diffusion models, learn from large-scale datasets and
generate new samples following similar data distributions. However, when
training data is limited, deep neural generative networks overfit and tend to
replicate training samples. Prior works focus on few-shot image generation to
produce high-quality and diverse results using a few target images.
Unfortunately, abundant 3D shape data is typically hard to obtain as well. In
this work, we make the first attempt to realize few-shot 3D shape generation by
adapting generative models pre-trained on large source domains to target
domains using limited data. To relieve overfitting and keep considerable
diversity, we propose to maintain the probability distributions of the pairwise
relative distances between adapted samples at feature-level and shape-level
during domain adaptation. Our approach only needs the silhouettes of few-shot
target samples as training data to learn target geometry distributions and
achieve generated shapes with diverse topology and textures. Moreover, we
introduce several metrics to evaluate the quality and diversity of few-shot 3D
shape generation. The effectiveness of our approach is demonstrated
qualitatively and quantitatively under a series of few-shot 3D shape adaptation
setups.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：DAP: A Dynamic Adversarial Patch for Evading Person Detectors</b></summary>
  <p><b>编号</b>：[110]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11618</p>
  <p><b>作者</b>：Amira Guesmi,  Ruitian Ding,  Muhammad Abdullah Hanif,  Ihsen Alouani,  Muhammad Shafique</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：generating naturalistic adversarial, naturalistic adversarial patches, Dynamic Adversarial Patch, generating naturalistic, proposed approach generates</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we present a novel approach for generating naturalistic
adversarial patches without using GANs. Our proposed approach generates a
Dynamic Adversarial Patch (DAP) that looks naturalistic while maintaining high
attack efficiency and robustness in real-world scenarios. To achieve this, we
redefine the optimization problem by introducing a new objective function,
where a similarity metric is used to construct a similarity loss. This guides
the patch to follow predefined patterns while maximizing the victim model's
loss function. Our technique is based on directly modifying the pixel values in
the patch which gives higher flexibility and larger space to incorporate
multiple transformations compared to the GAN-based techniques. Furthermore,
most clothing-based physical attacks assume static objects and ignore the
possible transformations caused by non-rigid deformation due to changes in a
person's pose. To address this limitation, we incorporate a ``Creases
Transformation'' (CT) block, i.e., a preprocessing block following an
Expectation Over Transformation (EOT) block used to generate a large variation
of transformed patches incorporated in the training process to increase its
robustness to different possible real-world distortions (e.g., creases in the
clothing, rotation, re-scaling, random noise, brightness and contrast
variations, etc.). We demonstrate that the presence of different real-world
variations in clothing and object poses (i.e., above-mentioned distortions)
lead to a drop in the performance of state-of-the-art attacks. For instance,
these techniques can merely achieve 20\% in the physical world and 30.8\% in
the digital world while our attack provides superior success rate of up to 65\%
and 84.56\%, respectively when attacking the YOLOv3tiny detector deployed in
smart cameras at the edge.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD  Detection, Calibration, and Accuracy</b></summary>
  <p><b>编号</b>：[111]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11616</p>
  <p><b>作者</b>：Stanislav Dereka,  Ivan Karpukhin,  Sergey Kolesnikov</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：effectiveness remains limited, remains limited due, Deep ensembles achieved, effectiveness remains, remains limited</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep ensembles achieved state-of-the-art results in classification and
out-of-distribution (OOD) detection; however, their effectiveness remains
limited due to the homogeneity of learned patterns within the ensemble. To
overcome this challenge, our study introduces a novel approach that promotes
diversity among ensemble members by leveraging saliency maps. By incorporating
saliency map diversification, our method outperforms conventional ensemble
techniques in multiple classification and OOD detection tasks, while also
improving calibration. Experiments on well-established OpenOOD benchmarks
highlight the potential of our method in practical applications.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Towards Better Gradient Consistency for Neural Signed Distance Functions  via Level Set Alignment</b></summary>
  <p><b>编号</b>：[116]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11601</p>
  <p><b>作者</b>：Baorui Ma,  Junsheng Zhou,  Yu-Shen Liu,  Zhizhong Han</p>
  <p><b>备注</b>：To appear at CVPR2023. Project page: this https URL</p>
  <p><b>关键词</b>：shown remarkable capability, signed distance functions, Neural signed distance, level set, level sets</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural signed distance functions (SDFs) have shown remarkable capability in
representing geometry with details. However, without signed distance
supervision, it is still a challenge to infer SDFs from point clouds or
multi-view images using neural networks. In this paper, we claim that gradient
consistency in the field, indicated by the parallelism of level sets, is the
key factor affecting the inference accuracy. Hence, we propose a level set
alignment loss to evaluate the parallelism of level sets, which can be
minimized to achieve better gradient consistency. Our novelty lies in that we
can align all level sets to the zero level set by constraining gradients at
queries and their projections on the zero level set in an adaptive way. Our
insight is to propagate the zero level set to everywhere in the field through
consistent gradients to eliminate uncertainty in the field that is caused by
the discreteness of 3D point clouds or the lack of observations from multi-view
images. Our proposed loss is a general term which can be used upon different
methods to infer SDFs from 3D point clouds and multi-view images. Our numerical
and visual comparisons demonstrate that our loss can significantly improve the
accuracy of SDFs inferred from point clouds or multi-view images under various
benchmarks. Code and data are available at
this https URL .</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields</b></summary>
  <p><b>编号</b>：[124]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11588</p>
  <p><b>作者</b>：Jingbo Zhang,  Xiaoyu Li,  Ziyu Wan,  Can Wang,  Jing Liao</p>
  <p><b>备注</b>：Homepage: this https URL</p>
  <p><b>关键词</b>：film industry, video gaming, widely applicable, applicable to video, metaverse applications</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Text-driven 3D scene generation is widely applicable to video gaming, film
industry, and metaverse applications that have a large demand for 3D scenes.
However, existing text-to-3D generation methods are limited to producing 3D
objects with simple geometries and dreamlike styles that lack realism. In this
work, we present Text2NeRF, which is able to generate a wide range of 3D scenes
with complicated geometric structures and high-fidelity textures purely from a
text prompt. To this end, we adopt NeRF as the 3D representation and leverage a
pre-trained text-to-image diffusion model to constrain the 3D reconstruction of
the NeRF to reflect the scene description. Specifically, we employ the
diffusion model to infer the text-related image as the content prior and use a
monocular depth estimation method to offer the geometric prior. Both content
and geometric priors are utilized to update the NeRF model. To guarantee
textured and geometric consistency between different views, we introduce a
progressive scene inpainting and updating strategy for novel view synthesis of
the scene. Our method requires no additional training data but only a natural
language description of the scene as the input. Extensive experiments
demonstrate that our Text2NeRF outperforms existing methods in producing
photo-realistic, multi-view consistent, and diverse 3D scenes from a variety of
natural language prompts.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：What You Hear Is What You See: Audio Quality Metrics From Image Quality  Metrics</b></summary>
  <p><b>编号</b>：[128]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11582</p>
  <p><b>作者</b>：Tashi Namgyal,  Alexander Hepburn,  Raul Santos-Rodriguez,  Valero Laparra,  Jesus Malo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：image perceptual metrics, evaluating audio signals, feasibility of utilizing, image perceptual, investigate the feasibility</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this study, we investigate the feasibility of utilizing state-of-the-art
image perceptual metrics for evaluating audio signals by representing them as
spectrograms. The encouraging outcome of the proposed approach is based on the
similarity between the neural mechanisms in the auditory and visual pathways.
Furthermore, we customise one of the metrics which has a psychoacoustically
plausible architecture to account for the peculiarities of sound signals. We
evaluate the effectiveness of our proposed metric and several baseline metrics
using a music dataset, with promising results in terms of the correlation
between the metrics and the perceived quality of audio as rated by human
evaluators.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：A Unified Prompt-Guided In-Context Inpainting Framework for  Reference-based Image Manipulations</b></summary>
  <p><b>编号</b>：[132]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11577</p>
  <p><b>作者</b>：Chenjie Cao,  Qiaole Dong,  Yikai Wang,  Yunuo Cai,  Yanwei Fu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：generating high-fidelity images, consistent text prompts, yielded impressive results, generative models, Recent advancements</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advancements in Text-to-Image (T2I) generative models have yielded
impressive results in generating high-fidelity images based on consistent text
prompts. However, there is a growing interest in exploring the potential of
these models for more diverse reference-based image manipulation tasks that
require spatial understanding and visual context. Previous approaches have
achieved this by incorporating additional control modules or fine-tuning the
generative models specifically for each task until convergence. In this paper,
we propose a different perspective. We conjecture that current large-scale T2I
generative models already possess the capability to perform these tasks but are
not fully activated within the standard generation process. To unlock these
capabilities, we introduce a unified Prompt-Guided In-Context inpainting (PGIC)
framework, which leverages large-scale T2I models to re-formulate and solve
reference-guided image manipulations. In the PGIC framework, the reference and
masked target are stitched together as a new input for the generative models,
enabling the filling of masked regions as producing final results. Furthermore,
we demonstrate that the self-attention modules in T2I models are well-suited
for establishing spatial correlations and efficiently addressing challenging
reference-guided manipulations. These large T2I models can be effectively
driven by task-specific prompts with minimal training cost or even with frozen
backbones. We synthetically evaluate the effectiveness of the proposed PGIC
framework across various tasks, including reference-guided image inpainting,
faithful inpainting, outpainting, local super-resolution, and novel view
synthesis. Our results show that PGIC achieves significantly better performance
while requiring less computation compared to other fine-tuning based
approaches.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：StereoVAE: A lightweight stereo matching system through embedded GPUs</b></summary>
  <p><b>编号</b>：[135]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11566</p>
  <p><b>作者</b>：Qiong Chang,  Xiang Li,  Xin Xu,  Xin Liu,  Yun Li,  Miyazaki Jun</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：present a lightweight, stereo matching, matching, lightweight system, matching accuracy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a lightweight system for stereo matching through embedded GPUs. It
breaks the trade-off between accuracy and processing speed in stereo matching,
enabling our embedded system to further improve the matching accuracy while
ensuring real-time processing. The main idea of our method is to construct a
tiny neural network based on variational auto-encoder (VAE) to upsample and
refinement a small size of coarse disparity map, which is first generated by a
traditional matching method. The proposed hybrid structure cannot only bring
the advantage of traditional methods in terms of computational complexity, but
also ensure the matching accuracy under the impact of neural network. Extensive
experiments on the KITTI 2015 benchmark demonstrate that our tiny system
exhibits high robustness in improving the accuracy of the coarse disparity maps
generated by different algorithms, while also running in real-time on embedded
GPUs.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Brain Captioning: Decoding human brain activity into images and text</b></summary>
  <p><b>编号</b>：[137]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11560</p>
  <p><b>作者</b>：Matteo Ferrante,  Furkan Ozcelik,  Tommaso Boccato,  Rufin VanRullen,  Nicola Toschi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：intricate neural mechanisms, relying on intricate, interpret these stimuli, visual information, processes an immense</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Every day, the human brain processes an immense volume of visual information,
relying on intricate neural mechanisms to perceive and interpret these stimuli.
Recent breakthroughs in functional magnetic resonance imaging (fMRI) have
enabled scientists to extract visual information from human brain activity
patterns. In this study, we present an innovative method for decoding brain
activity into meaningful images and captions, with a specific focus on brain
captioning due to its enhanced flexibility as compared to brain decoding into
images. Our approach takes advantage of cutting-edge image captioning models
and incorporates a unique image reconstruction pipeline that utilizes latent
diffusion models and depth estimation. We utilized the Natural Scenes Dataset,
a comprehensive fMRI dataset from eight subjects who viewed images from the
COCO dataset. We employed the Generative Image-to-text Transformer (GIT) as our
backbone for captioning and propose a new image reconstruction pipeline based
on latent diffusion models. The method involves training regularized linear
regression models between brain activity and extracted features. Additionally,
we incorporated depth maps from the ControlNet model to further guide the
reconstruction process. We evaluate our methods using quantitative metrics for
both generated captions and images. Our brain captioning approach outperforms
existing methods, while our image reconstruction pipeline generates plausible
images with improved spatial relationships. In conclusion, we demonstrate
significant progress in brain decoding, showcasing the enormous potential of
integrating vision and language to better understand human cognition. Our
approach provides a flexible platform for future research, with potential
applications in various fields, including neural art, style transfer, and
portable devices.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Efficient Cross-Lingual Transfer for Chinese Stable Diffusion with  Images as Pivots</b></summary>
  <p><b>编号</b>：[148]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11540</p>
  <p><b>作者</b>：Jinyi Hu,  Xu Han,  Xiaoyuan Yi,  Yutong Chen,  Wenhao Li,  Zhiyuan Liu,  Maosong Sun</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：made impressive progress, English Stable Diffusion, Stable Diffusion, made impressive, impressive progress</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diffusion models have made impressive progress in text-to-image synthesis.
However, training such large-scale models (e.g. Stable Diffusion), from scratch
requires high computational costs and massive high-quality text-image pairs,
which becomes unaffordable in other languages. To handle this challenge, we
propose IAP, a simple but effective method to transfer English Stable Diffusion
into Chinese. IAP optimizes only a separate Chinese text encoder with all other
parameters fixed to align Chinese semantics space to the English one in CLIP.
To achieve this, we innovatively treat images as pivots and minimize the
distance of attentive features produced from cross-attention between images and
each language respectively. In this way, IAP establishes connections of
Chinese, English and visual semantics in CLIP's embedding space efficiently,
advancing the quality of the generated image with direct Chinese prompts.
Experimental results show that our method outperforms several strong Chinese
diffusion models with only 5%~10% training data.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Embracing Limited and Imperfect Data: A Review on Plant Stress  Recognition Using Deep Learning</b></summary>
  <p><b>编号</b>：[152]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11533</p>
  <p><b>作者</b>：Mingle Xu,  Hyongsuk Kim,  Jucheng Yang,  Alvaro Fuentes,  Yao Meng,  Sook Yoon,  Taehyun Kim,  Dong Sun Park</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：witnessed significant improvements, recognition has witnessed, witnessed significant, significant improvements, improvements in recent</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Plant stress recognition has witnessed significant improvements in recent
years with the advent of deep learning. A large-scale and annotated training
dataset is required to achieve decent performance; however, collecting it is
frequently difficult and expensive. Therefore, deploying current deep
learning-based methods in real-world applications may suffer primarily from
limited and imperfect data. Embracing them is a promising strategy that has not
received sufficient attention. From this perspective, a systematic survey was
conducted in this study, with the ultimate objective of monitoring plant growth
by implementing deep learning, which frees humans and potentially reduces the
resultant losses from plant stress. We believe that our paper has highlighted
the importance of embracing this limited and imperfect data and enhanced its
relevant understanding.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：DSFNet: Dual Space Fusion Network for Occlusion-Robust 3D Dense Face  Alignment</b></summary>
  <p><b>编号</b>：[157]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11522</p>
  <p><b>作者</b>：Heyuan Li,  Bo Wang,  Yu Cheng,  Mohan Kankanhalli,  Robby T. Tan</p>
  <p><b>备注</b>：Accepted into CVPR'23</p>
  <p><b>关键词</b>：view angles limits, Sensitivity to severe, existing monocular, model space, limits the usage</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sensitivity to severe occlusion and large view angles limits the usage
scenarios of the existing monocular 3D dense face alignment methods. The
state-of-the-art 3DMM-based method, directly regresses the model's
coefficients, underutilizing the low-level 2D spatial and semantic information,
which can actually offer cues for face shape and orientation. In this work, we
demonstrate how modeling 3D facial geometry in image and model space jointly
can solve the occlusion and view angle problems. Instead of predicting the
whole face directly, we regress image space features in the visible facial
region by dense prediction first. Subsequently, we predict our model's
coefficients based on the regressed feature of the visible regions, leveraging
the prior knowledge of whole face geometry from the morphable models to
complete the invisible regions. We further propose a fusion network that
combines the advantages of both the image and model space predictions to
achieve high robustness and accuracy in unconstrained scenarios. Thanks to the
proposed fusion module, our method is robust not only to occlusion and large
pitch and roll view angles, which is the benefit of our image space approach,
but also to noise and large yaw angles, which is the benefit of our model space
method. Comprehensive evaluations demonstrate the superior performance of our
method compared with the state-of-the-art methods. On the 3D dense face
alignment task, we achieve 3.80% NME on the AFLW2000-3D dataset, which
outperforms the state-of-the-art method by 5.5%. Code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Late-Constraint Diffusion Guidance for Controllable Image Synthesis</b></summary>
  <p><b>编号</b>：[158]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11520</p>
  <p><b>作者</b>：Chang Liu,  Dong Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：demonstrated impressive capability, synthesizing photorealistic images, Diffusion models, demonstrated impressive, impressive capability</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diffusion models, either with or without text condition, have demonstrated
impressive capability in synthesizing photorealistic images given a few or even
no words. These models may not fully satisfy user need, as normal users or
artists intend to control the synthesized images with specific guidance, like
overall layout, color, structure, object shape, and so on. To adapt diffusion
models for controllable image synthesis, several methods have been proposed to
incorporate the required conditions as regularization upon the intermediate
features of the diffusion denoising network. These methods, known as
early-constraint ones in this paper, have difficulties in handling multiple
conditions with a single solution. They intend to train separate models for
each specific condition, which require much training cost and result in
non-generalizable solutions. To address these difficulties, we propose a new
approach namely late-constraint: we leave the diffusion networks unchanged, but
constrain its output to be aligned with the required conditions. Specifically,
we train a lightweight condition adapter to establish the correlation between
external conditions and internal representations of diffusion models. During
the iterative denoising process, the conditional guidance is sent into
corresponding condition adapter to manipulate the sampling process with the
established correlation. We further equip the introduced late-constraint
strategy with a timestep resampling method and an early stopping technique,
which boost the quality of synthesized image meanwhile complying with the
guidance. Our method outperforms the existing early-constraint methods and
generalizes better to unseen condition.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：When SAM Meets Shadow Detection</b></summary>
  <p><b>编号</b>：[162]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11513</p>
  <p><b>作者</b>：Leiping Jie,  Hui Zhang</p>
  <p><b>备注</b>：Technical Report</p>
  <p><b>关键词</b>：attracted significant attention, recently attracted significant, promptable generic object, significant attention, promptable generic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As a promptable generic object segmentation model, segment anything model
(SAM) has recently attracted significant attention, and also demonstrates its
powerful performance. Nevertheless, it still meets its Waterloo when
encountering several tasks, e.g., medical image segmentation, camouflaged
object detection, etc. In this report, we try SAM on an unexplored popular
task: shadow detection. Specifically, four benchmarks were chosen and evaluated
with widely used metrics. The experimental results show that the performance
for shadow detection using SAM is not satisfactory, especially when comparing
with the elaborate models. Code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：TreePrompt: Learning to Compose Tree Prompts for Explainable Visual  Grounding</b></summary>
  <p><b>编号</b>：[172]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11497</p>
  <p><b>作者</b>：Chenchi Zhang,  Jun Xiao,  Lei Chen,  Jian Shao,  Long Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：achieved great success, large pretrained vision-language, pretrained vision-language models, downstream tasks, visual grounding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Prompt tuning has achieved great success in transferring the knowledge from
large pretrained vision-language models into downstream tasks, and has
dominated the performance on visual grounding (VG). However, almost all
existing prompt tuning paradigms suffer from poor interpretability. In this
paper, we argue that their poor interpretability is attributed to the holistic
prompt generation and inference process. By "holistic", we mean that they
usually directly learn a set of vectors as the prompt (i.e., prompt
generation), and use the learned global prompt to augment the textual input for
the VG model (i.e., prompt inference). To this end, we propose a new prompt
construction paradigm with explicit explainable ability, named TreePrompt.
Specifically, we first deconstruct a complex sentence into a tree, that is
consistent with human reasoning. Then, following the syntax tree, we compose a
structured prompt in a bottom-up manner. Thanks to this step-by-step prompt
construction process, each intermediate prompt (i.e., tree node) permits us to
understand the reasoning process. Extensive ablations on various backbones and
benchmarks consistently demonstrate the effectiveness and interpretability of
our TreePrompt.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：LLM Itself Can Read and Generate CXR Images</b></summary>
  <p><b>编号</b>：[174]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11490</p>
  <p><b>作者</b>：Suhyeon Lee,  Won Jun Kim,  Jong Chul Ye</p>
  <p><b>备注</b>：12 pages, 4 figures</p>
  <p><b>关键词</b>：recent remarkable development, large language models, recent remarkable, remarkable development, development of large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Building on the recent remarkable development of large language models
(LLMs), active attempts are being made to extend the utility of LLMs to
multimodal tasks. There have been previous efforts to link language and visual
information, and attempts to add visual capabilities to LLMs are ongoing as
well. However, existing attempts use LLMs only as image decoders and no attempt
has been made to generate images in the same line as the natural language. By
adopting a VQ-GAN framework in which latent representations of images are
treated as a kind of text tokens, we present a novel method to fine-tune a
pre-trained LLM to read and generate images like text without any structural
changes, extra training objectives, or the need for training an ad-hoc network
while still preserving the of the instruction-following capability of the LLM.
We apply this framework to chest X-ray (CXR) image and report generation tasks
as it is a domain in which translation of complex information between visual
and language domains is important. The code will soon be made publicly
available.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：AttriCLIP: A Non-Incremental Learner for Incremental Knowledge Learning</b></summary>
  <p><b>编号</b>：[176]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11488</p>
  <p><b>作者</b>：Runqi Wang,  Xiaoyue Duan,  Guoliang Kang,  Jianzhuang Liu,  Shaohui Lin,  Songcen Xu,  Jinhu Lv,  Baochang Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：aims to enable, incrementally learn knowledge, Continual learning aims, sequentially arrived, incrementally learn</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Continual learning aims to enable a model to incrementally learn knowledge
from sequentially arrived data. Previous works adopt the conventional
classification architecture, which consists of a feature extractor and a
classifier. The feature extractor is shared across sequentially arrived tasks
or classes, but one specific group of weights of the classifier corresponding
to one new class should be incrementally expanded. Consequently, the parameters
of a continual learner gradually increase. Moreover, as the classifier contains
all historical arrived classes, a certain size of the memory is usually
required to store rehearsal data to mitigate classifier bias and catastrophic
forgetting. In this paper, we propose a non-incremental learner, named
AttriCLIP, to incrementally extract knowledge of new classes or tasks.
Specifically, AttriCLIP is built upon the pre-trained visual-language model
CLIP. Its image encoder and text encoder are fixed to extract features from
both images and text. Text consists of a category name and a fixed number of
learnable parameters which are selected from our designed attribute word bank
and serve as attributes. As we compute the visual and textual similarity for
classification, AttriCLIP is a non-incremental learner. The attribute prompts,
which encode the common knowledge useful for classification, can effectively
mitigate the catastrophic forgetting and avoid constructing a replay memory. We
evaluate our AttriCLIP and compare it with CLIP-based and previous
state-of-the-art continual learning methods in realistic settings with
domain-shift and long-sequence learning. The results show that our method
performs favorably against previous state-of-the-arts. The implementation code
can be available at this https URL.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：PointGPT: Auto-regressively Generative Pre-training from Point Clouds</b></summary>
  <p><b>编号</b>：[177]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11487</p>
  <p><b>作者</b>：Guangyan Chen,  Meiling Wang,  Yi Yang,  Kai Yu,  Li Yuan,  Yufeng Yue</p>
  <p><b>备注</b>：9 pages, 2 figures</p>
  <p><b>关键词</b>：demonstrated remarkable effectiveness, Large language models, generative pre-training transformer, Large language, generative pre-training</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) based on the generative pre-training transformer
(GPT) have demonstrated remarkable effectiveness across a diverse range of
downstream tasks. Inspired by the advancements of the GPT, we present PointGPT,
a novel approach that extends the concept of GPT to point clouds, addressing
the challenges associated with disorder properties, low information density,
and task gaps. Specifically, a point cloud auto-regressive generation task is
proposed to pre-train transformer models. Our method partitions the input point
cloud into multiple point patches and arranges them in an ordered sequence
based on their spatial proximity. Then, an extractor-generator based
transformer decoder, with a dual masking strategy, learns latent
representations conditioned on the preceding point patches, aiming to predict
the next one in an auto-regressive manner. Our scalable approach allows for
learning high-capacity models that generalize well, achieving state-of-the-art
performance on various downstream tasks. In particular, our approach achieves
classification accuracies of 94.9% on the ModelNet40 dataset and 93.4% on the
ScanObjectNN dataset, outperforming all other transformer models. Furthermore,
our method also attains new state-of-the-art accuracies on all four few-shot
learning benchmarks.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：CM-MaskSD: Cross-Modality Masked Self-Distillation for Referring Image  Segmentation</b></summary>
  <p><b>编号</b>：[181]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11481</p>
  <p><b>作者</b>：Wenxuan Wang,  Jing Liu,  Xingjian He,  Yisi Zhang,  Chen Chen,  Jiachen Shen,  Yan Zhang,  Jiangyun Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：natural language expression, fundamental vision-language task, Referring image segmentation, language expression, intends to segment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Referring image segmentation (RIS) is a fundamental vision-language task that
intends to segment a desired object from an image based on a given natural
language expression. Due to the essentially distinct data properties between
image and text, most of existing methods either introduce complex designs
towards fine-grained vision-language alignment or lack required dense
alignment, resulting in scalability issues or mis-segmentation problems such as
over- or under-segmentation. To achieve effective and efficient fine-grained
feature alignment in the RIS task, we explore the potential of masked
multimodal modeling coupled with self-distillation and propose a novel
cross-modality masked self-distillation framework named CM-MaskSD, in which our
method inherits the transferred knowledge of image-text semantic alignment from
CLIP model to realize fine-grained patch-word feature alignment for better
segmentation accuracy. Moreover, our CM-MaskSD framework can considerably boost
model performance in a nearly parameter-free manner, since it shares weights
between the main segmentation branch and the introduced masked
self-distillation branches, and solely introduces negligible parameters for
coordinating the multimodal features. Comprehensive experiments on three
benchmark datasets (i.e. RefCOCO, RefCOCO+, G-Ref) for the RIS task
convincingly demonstrate the superiority of our proposed framework over
previous state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：RAMiT: Reciprocal Attention Mixing Transformer for Lightweight Image  Restoration</b></summary>
  <p><b>编号</b>：[185]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11474</p>
  <p><b>作者</b>：Haram Choi,  Cheolwoong Na,  Jihyeon Oh,  Seungjae Lee,  Jinseop Kim,  Subeen Choe,  Jeongmin Lee,  Taehoon Kim,  Jihoon Yang</p>
  <p><b>备注</b>：Technical report. 9 pages for main contents + 14 pages for appendix + 6 pages for references</p>
  <p><b>关键词</b>：Attention Mixing Transformer, Reciprocal Attention Mixing, image restoration, recent works, works have made</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Although many recent works have made advancements in the image restoration
(IR) field, they often suffer from an excessive number of parameters. Another
issue is that most Transformer-based IR methods focus only on either local or
global features, leading to limited receptive fields or deficient parameter
issues. To address these problems, we propose a lightweight IR network,
Reciprocal Attention Mixing Transformer (RAMiT). It employs our proposed
dimensional reciprocal attention mixing Transformer (D-RAMiT) blocks, which
compute bi-dimensional (spatial and channel) self-attentions in parallel with
different numbers of multi-heads. The bi-dimensional attentions help each other
to complement their counterpart's drawbacks and are then mixed. Additionally,
we introduce a hierarchical reciprocal attention mixing (H-RAMi) layer that
compensates for pixel-level information losses and utilizes semantic
information while maintaining an efficient hierarchical structure. Furthermore,
we revisit and modify MobileNet V1 and V2 to attach efficient convolutions to
our proposed components. The experimental results demonstrate that RAMiT
achieves state-of-the-art performance on multiple lightweight IR tasks,
including super-resolution, color denoising, grayscale denoising, low-light
enhancement, and deraining. Codes will be available soon.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Overcoming Topology Agnosticism: Enhancing Skeleton-Based Action  Recognition through Redefined Skeletal Topology Awareness</b></summary>
  <p><b>编号</b>：[188]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11468</p>
  <p><b>作者</b>：Yuxuan Zhou,  Zhi-Qi Cheng,  Jun-Yan He,  Bin Luo,  Yifeng Geng,  Xuansong Xie,  Margret Keuper</p>
  <p><b>备注</b>：this https URL</p>
  <p><b>关键词</b>：Graph Convolutional Networks, Convolutional Networks, adjacency matrix, graph adjacency matrix, Graph Convolutional</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Graph Convolutional Networks (GCNs) have long defined the state-of-the-art in
skeleton-based action recognition, leveraging their ability to unravel the
complex dynamics of human joint topology through the graph's adjacency matrix.
However, an inherent flaw has come to light in these cutting-edge models: they
tend to optimize the adjacency matrix jointly with the model weights. This
process, while seemingly efficient, causes a gradual decay of bone connectivity
data, culminating in a model indifferent to the very topology it sought to map.
As a remedy, we propose a threefold strategy: (1) We forge an innovative
pathway that encodes bone connectivity by harnessing the power of graph
distances. This approach preserves the vital topological nuances often lost in
conventional GCNs. (2) We highlight an oft-overlooked feature - the temporal
mean of a skeletal sequence, which, despite its modest guise, carries highly
action-specific information. (3) Our investigation revealed strong variations
in joint-to-joint relationships across different actions. This finding exposes
the limitations of a single adjacency matrix in capturing the variations of
relational configurations emblematic of human movement, which we remedy by
proposing an efficient refinement to Graph Convolutions (GC) - the BlockGC.
This evolution slashes parameters by a substantial margin (above 40%), while
elevating performance beyond original GCNs. Our full model, the BlockGCN,
establishes new standards in skeleton-based action recognition for small model
sizes. Its high accuracy, notably on the large-scale NTU RGB+D 120 dataset,
stand as compelling proof of the efficacy of BlockGCN. The source code and
model can be found at this https URL.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Learning Sequence Descriptor based on Spatiotemporal Attention for  Visual Place Recognition</b></summary>
  <p><b>编号</b>：[189]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11467</p>
  <p><b>作者</b>：Fenglin Zhang,  Junqiao Zhao,  Yingfeng Cai,  Gengxuan Tian,  Wenjie Mu,  Chen Ye</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Sequence-based visual place, visual place recognition, Sequence-based visual, place recognition, aims to match</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sequence-based visual place recognition (sVPR) aims to match frame sequences
with frames stored in a reference map for localization. Existing methods
include sequence matching and sequence descriptor-based retrieval. The former
is based on the assumption of constant velocity, which is difficult to hold in
real scenarios and does not get rid of the intrinsic single frame descriptor
mismatch. The latter solves this problem by extracting a descriptor for the
whole sequence, but current sequence descriptors are only constructed by
feature aggregation of multi-frames, with no temporal information interaction.
In this paper, we propose a sequential descriptor extraction method to fuse
spatiotemporal information effectively and generate discriminative descriptors.
Specifically, similar features on the same frame focu on each other and learn
space structure, and the same local regions of different frames learn local
feature changes over time. And we use sliding windows to control the temporal
self-attention range and adpot relative position encoding to construct the
positional relationships between different features, which allows our
descriptor to capture the inherent dynamics in the frame sequence and local
feature motion.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：ReDirTrans: Latent-to-Latent Translation for Gaze and Head Redirection</b></summary>
  <p><b>编号</b>：[199]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11452</p>
  <p><b>作者</b>：Shiwei Jin,  Zhen Wang,  Lei Wang,  Ning Bi,  Truong Nguyen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：require large amounts, methods require large, accurate gaze annotations, require large, large amounts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Learning-based gaze estimation methods require large amounts of training data
with accurate gaze annotations. Facing such demanding requirements of gaze data
collection and annotation, several image synthesis methods were proposed, which
successfully redirected gaze directions precisely given the assigned
conditions. However, these methods focused on changing gaze directions of the
images that only include eyes or restricted ranges of faces with low resolution
(less than $128\times128$) to largely reduce interference from other attributes
such as hairs, which limits application scenarios. To cope with this
limitation, we proposed a portable network, called ReDirTrans, achieving
latent-to-latent translation for redirecting gaze directions and head
orientations in an interpretable manner. ReDirTrans projects input latent
vectors into aimed-attribute embeddings only and redirects these embeddings
with assigned pitch and yaw values. Then both the initial and edited embeddings
are projected back (deprojected) to the initial latent space as residuals to
modify the input latent vectors by subtraction and addition, representing old
status removal and new status addition. The projection of aimed attributes only
and subtraction-addition operations for status replacement essentially mitigate
impacts on other attributes and the distribution of latent vectors. Thus, by
combining ReDirTrans with a pretrained fixed e4e-StyleGAN pair, we created
ReDirTrans-GAN, which enables accurately redirecting gaze in full-face images
with $1024\times1024$ resolution while preserving other attributes such as
identity, expression, and hairstyle. Furthermore, we presented improvements for
the downstream learning-based gaze estimation task, using redirected samples as
dataset augmentation.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：SurgMAE: Masked Autoencoders for Long Surgical Video Analysis</b></summary>
  <p><b>编号</b>：[200]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11451</p>
  <p><b>作者</b>：Muhammad Abdullah Jamal,  Omid Mohareri</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：automatically detect clinical, enable workflow efficiency, workflow efficiency tools, deep learning models, detect clinical</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>There has been a growing interest in using deep learning models for
processing long surgical videos, in order to automatically detect
clinical/operational activities and extract metrics that can enable workflow
efficiency tools and applications. However, training such models require vast
amounts of labeled data which is costly and not scalable. Recently,
self-supervised learning has been explored in computer vision community to
reduce the burden of the annotation cost. Masked autoencoders (MAE) got the
attention in self-supervised paradigm for Vision Transformers (ViTs) by
predicting the randomly masked regions given the visible patches of an image or
a video clip, and have shown superior performance on benchmark datasets.
However, the application of MAE in surgical data remains unexplored. In this
paper, we first investigate whether MAE can learn transferrable representations
in surgical video domain. We propose SurgMAE, which is a novel architecture
with a masking strategy based on sampling high spatio-temporal tokens for MAE.
We provide an empirical study of SurgMAE on two large scale long surgical video
datasets, and find that our method outperforms several baselines in low data
regime. We conduct extensive ablation studies to show the efficacy of our
approach and also demonstrate it's superior performance on UCF-101 to prove
it's generalizability in non-surgical datasets as well.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Equivariant Multi-Modality Image Fusion</b></summary>
  <p><b>编号</b>：[204]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11443</p>
  <p><b>作者</b>：Zixiang Zhao,  Haowen Bai,  Jiangshe Zhang,  Yulun Zhang,  Kai Zhang,  Shuang Xu,  Dongdong Chen,  Radu Timofte,  Luc Van Gool</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：retain complementary features, sensors or modalities, allowing the fused, texture details, combine information</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multi-modality image fusion is a technique used to combine information from
different sensors or modalities, allowing the fused image to retain
complementary features from each modality, such as functional highlights and
texture details. However, effectively training such fusion models is difficult
due to the lack of ground truth fusion data. To address this issue, we propose
the Equivariant Multi-Modality imAge fusion (EMMA) paradigm for end-to-end
self-supervised learning. Our approach is based on the prior knowledge that
natural images are equivariant to specific transformations. Thus, we introduce
a novel training framework that includes a fusion module and a learnable
pseudo-sensing module, which allow the network training to follow the
principles of physical sensing and imaging process, and meanwhile satisfy the
equivariant prior for natural images. Our extensive experiments demonstrate
that our method produces high-quality fusion results for both infrared-visible
and medical images, while facilitating downstream multi-modal segmentation and
detection tasks. The code will be released.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Few-Shot Learning with Visual Distribution Calibration and Cross-Modal  Distribution Alignment</b></summary>
  <p><b>编号</b>：[207]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11439</p>
  <p><b>作者</b>：Runqi Wang,  Hao Zheng,  Xiaoyue Duan,  Jianzhuang Liu,  Yuning Lu,  Tian Wang,  Songcen Xu,  Baochang Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Pre-trained vision-language models, language feature distributions, models have inspired, inspired much research, feature distributions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pre-trained vision-language models have inspired much research on few-shot
learning. However, with only a few training images, there exist two crucial
problems: (1) the visual feature distributions are easily distracted by
class-irrelevant information in images, and (2) the alignment between the
visual and language feature distributions is difficult. To deal with the
distraction problem, we propose a Selective Attack module, which consists of
trainable adapters that generate spatial attention maps of images to guide the
attacks on class-irrelevant image areas. By messing up these areas, the
critical features are captured and the visual distributions of image features
are calibrated. To better align the visual and language feature distributions
that describe the same object class, we propose a cross-modal distribution
alignment module, in which we introduce a vision-language prototype for each
class to align the distributions, and adopt the Earth Mover's Distance (EMD) to
optimize the prototypes. For efficient computation, the upper bound of EMD is
derived. In addition, we propose an augmentation strategy to increase the
diversity of the images and the text prompts, which can reduce overfitting to
the few-shot training images. Extensive experiments on 11 datasets demonstrate
that our method consistently outperforms prior arts in few-shot learning. The
implementation code will be available at this https URL.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：RGB-D And Thermal Sensor Fusion: A Systematic Literature Review</b></summary>
  <p><b>编号</b>：[213]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11427</p>
  <p><b>作者</b>：Martin Brenner,  Napoleon H. Reyes,  Teo Susnjak,  Andre L.C. Barczak</p>
  <p><b>备注</b>：33 pages, 20 figures</p>
  <p><b>关键词</b>：diverse spectral ranges, computer vision field, multimodal data fusion, spectral ranges, computer vision</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the last decade, the computer vision field has seen significant progress
in multimodal data fusion and learning, where multiple sensors, including
depth, infrared, and visual, are used to capture the environment across diverse
spectral ranges. Despite these advancements, there has been no systematic and
comprehensive evaluation of fusing RGB-D and thermal modalities to date. While
autonomous driving using LiDAR, radar, RGB, and other sensors has garnered
substantial research interest, along with the fusion of RGB and depth
modalities, the integration of thermal cameras and, specifically, the fusion of
RGB-D and thermal data, has received comparatively less attention. This might
be partly due to the limited number of publicly available datasets for such
applications. This paper provides a comprehensive review of both,
state-of-the-art and traditional methods used in fusing RGB-D and thermal
camera data for various applications, such as site inspection, human tracking,
fault detection, and others. The reviewed literature has been categorised into
technical areas, such as 3D reconstruction, segmentation, object detection,
available datasets, and other related topics. Following a brief introduction
and an overview of the methodology, the study delves into calibration and
registration techniques, then examines thermal visualisation and 3D
reconstruction, before discussing the application of classic feature-based
techniques as well as modern deep learning approaches. The paper concludes with
a discourse on current limitations and potential future research directions. It
is hoped that this survey will serve as a valuable reference for researchers
looking to familiarise themselves with the latest advancements and contribute
to the RGB-DT research field.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：PastNet: Introducing Physical Inductive Biases for Spatio-temporal Video  Prediction</b></summary>
  <p><b>编号</b>：[217]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11421</p>
  <p><b>作者</b>：Hao Wu,  Wei Xion,  Fan Xu,  Xiao Luo,  Chong Chen,  Xian-Sheng Hua,  Haixin Wang</p>
  <p><b>备注</b>：11</p>
  <p><b>关键词</b>：historical data streams, future videos based, involves generating future, video prediction, data streams</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we investigate the challenge of spatio-temporal video
prediction, which involves generating future videos based on historical data
streams. Existing approaches typically utilize external information such as
semantic maps to enhance video prediction, which often neglect the inherent
physical knowledge embedded within videos. Furthermore, their high
computational demands could impede their applications for high-resolution
videos. To address these constraints, we introduce a novel approach called
Physics-assisted Spatio-temporal Network (PastNet) for generating high-quality
video predictions. The core of our PastNet lies in incorporating a spectral
convolution operator in the Fourier domain, which efficiently introduces
inductive biases from the underlying physical laws. Additionally, we employ a
memory bank with the estimated intrinsic dimensionality to discretize local
features during the processing of complex spatio-temporal signals, thereby
reducing computational costs and facilitating efficient high-resolution video
prediction. Extensive experiments on various widely-used datasets demonstrate
the effectiveness and efficiency of the proposed PastNet compared with
state-of-the-art methods, particularly in high-resolution scenarios.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：JetSeg: Efficient Real-Time Semantic Segmentation Model for Low-Power  GPU-Embedded Systems</b></summary>
  <p><b>编号</b>：[219]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11419</p>
  <p><b>作者</b>：Miguel Lopez-Montiel,  Daniel Alejandro Lopez,  Oscar Montiel</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Real-time semantic segmentation, requires high-accuracy models, semantic segmentation, challenging task, task that requires</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Real-time semantic segmentation is a challenging task that requires
high-accuracy models with low-inference times. Implementing these models on
embedded systems is limited by hardware capability and memory usage, which
produces bottlenecks. We propose an efficient model for real-time semantic
segmentation called JetSeg, consisting of an encoder called JetNet, and an
improved RegSeg decoder. The JetNet is designed for GPU-Embedded Systems and
includes two main components: a new light-weight efficient block called
JetBlock, that reduces the number of parameters minimizing memory usage and
inference time without sacrificing accuracy; a new strategy that involves the
combination of asymmetric and non-asymmetric convolutions with
depthwise-dilated convolutions called JetConv, a channel shuffle operation,
light-weight activation functions, and a convenient number of group
convolutions for embedded systems, and an innovative loss function named
JetLoss, which integrates the Precision, Recall, and IoUB losses to improve
semantic segmentation and reduce computational complexity. Experiments
demonstrate that JetSeg is much faster on workstation devices and more suitable
for Low-Power GPU-Embedded Systems than existing state-of-the-art models for
real-time semantic segmentation. Our approach outperforms state-of-the-art
real-time encoder-decoder models by reducing 46.70M parameters and 5.14%
GFLOPs, which makes JetSeg up to 2x faster on the NVIDIA Titan RTX GPU and the
Jetson Xavier than other models. The JetSeg code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Efficient Mixed Transformer for Single Image Super-Resolution</b></summary>
  <p><b>编号</b>：[228]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11403</p>
  <p><b>作者</b>：Ling Zheng,  Jinchen Zhu,  Jinpeng Shi,  Shizhuang Weng</p>
  <p><b>备注</b>：Super-resolution, Long-range attention, Transformer, Locality</p>
  <p><b>关键词</b>：single image super-resolution, Transformer-based methods, SISR, Transformer-based, achieved impressive results</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, Transformer-based methods have achieved impressive results in
single image super-resolution (SISR). However, the lack of locality mechanism
and high complexity limit their application in the field of super-resolution
(SR). To solve these problems, we propose a new method, Efficient Mixed
Transformer (EMT) in this study. Specifically, we propose the Mixed Transformer
Block (MTB), consisting of multiple consecutive transformer layers, in some of
which the Pixel Mixer (PM) is used to replace the Self-Attention (SA). PM can
enhance the local knowledge aggregation with pixel shifting operations. At the
same time, no additional complexity is introduced as PM has no parameters and
floating-point operations. Moreover, we employ striped window for SA (SWSA) to
gain an efficient global dependency modelling by utilizing image anisotropy.
Experimental results show that EMT outperforms the existing methods on
benchmark dataset and achieved state-of-the-art performance. The Code is
available at https://github. com/Fried-Rice-Lab/EMT.git.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Remembering What Is Important: A Factorised Multi-Head Retrieval and  Auxiliary Memory Stabilisation Scheme for Human Motion Prediction</b></summary>
  <p><b>编号</b>：[231]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11394</p>
  <p><b>作者</b>：Tharindu Fernando,  Harshala Gammulle,  Sridha Sridharan,  Simon Denman,  Clinton Fookes</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Humans exhibit complex, exhibit complex motions, Humans exhibit, exhibit complex, vary depending</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Humans exhibit complex motions that vary depending on the task that they are
performing, the interactions they engage in, as well as subject-specific
preferences. Therefore, forecasting future poses based on the history of the
previous motions is a challenging task. This paper presents an innovative
auxiliary-memory-powered deep neural network framework for the improved
modelling of historical knowledge. Specifically, we disentangle
subject-specific, task-specific, and other auxiliary information from the
observed pose sequences and utilise these factorised features to query the
memory. A novel Multi-Head knowledge retrieval scheme leverages these
factorised feature embeddings to perform multiple querying operations over the
historical observations captured within the auxiliary memory. Moreover, our
proposed dynamic masking strategy makes this feature disentanglement process
dynamic. Two novel loss functions are introduced to encourage diversity within
the auxiliary memory while ensuring the stability of the memory contents, such
that it can locate and store salient information that can aid the long-term
prediction of future motion, irrespective of data imbalances or the diversity
of the input data distribution. With extensive experiments conducted on two
public benchmarks, Human3.6M and CMU-Mocap, we demonstrate that these design
choices collectively allow the proposed approach to outperform the current
state-of-the-art methods by significant margins: $>$ 17\% on the Human3.6M
dataset and $>$ 9\% on the CMU-Mocap dataset.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Fast-StrucTexT: An Efficient Hourglass Transformer with Modality-guided  Dynamic Token Merge for Document Understanding</b></summary>
  <p><b>编号</b>：[232]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11392</p>
  <p><b>作者</b>：Mingliang Zhai,  Yulin Li,  Xiameng Qin,  Chen Yi,  Qunyi Xie,  Chengquan Zhang,  Kun Yao,  Yuwei Wu,  Yunde Jia</p>
  <p><b>备注</b>：IJCAI 2023</p>
  <p><b>关键词</b>：quadratic computational complexity, computational complexity dependency, sequence length, Transformers achieve promising, high effectiveness</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transformers achieve promising performance in document understanding because
of their high effectiveness and still suffer from quadratic computational
complexity dependency on the sequence length. General efficient transformers
are challenging to be directly adapted to model document. They are unable to
handle the layout representation in documents, e.g. word, line and paragraph,
on different granularity levels and seem hard to achieve a good trade-off
between efficiency and performance. To tackle the concerns, we propose
Fast-StrucTexT, an efficient multi-modal framework based on the StrucTexT
algorithm with an hourglass transformer architecture, for visual document
understanding. Specifically, we design a modality-guided dynamic token merging
block to make the model learn multi-granularity representation and prunes
redundant tokens. Additionally, we present a multi-modal interaction module
called Symmetry Cross Attention (SCA) to consider multi-modal fusion and
efficiently guide the token mergence. The SCA allows one modality input as
query to calculate cross attention with another modality in a dual phase.
Extensive experiments on FUNSD, SROIE, and CORD datasets demonstrate that our
model achieves the state-of-the-art performance and almost 1.9X faster
inference time than the state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Deep Image Compression Using Scene Text Quality Assessment</b></summary>
  <p><b>编号</b>：[244]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11373</p>
  <p><b>作者</b>：Shohei Uchigasaki,  Tomo Miyazaki,  Shinichiro Omachi</p>
  <p><b>备注</b>：Accepted by Pattern Recognition, 2023</p>
  <p><b>关键词</b>：Internet communication engineering, technology for Internet, Internet communication, communication engineering, fundamental technology</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image compression is a fundamental technology for Internet communication
engineering. However, a high compression rate with general methods may degrade
images, resulting in unreadable texts. In this paper, we propose an image
compression method for maintaining text quality. We developed a scene text
image quality assessment model to assess text quality in compressed images. The
assessment model iteratively searches for the best-compressed image holding
high-quality text. Objective and subjective results showed that the proposed
method was superior to existing methods. Furthermore, the proposed assessment
model outperformed other deep-learning regression models.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Smart Pressure e-Mat for Human Sleeping Posture and Dynamic Activity  Recognition</b></summary>
  <p><b>编号</b>：[245]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11367</p>
  <p><b>作者</b>：Liangqi Yuan,  Yuan Wei,  Jia Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：early childhood education, emphasis on healthcare, early childhood, childhood education, non-invasive measurement</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the emphasis on healthcare, early childhood education, and fitness,
non-invasive measurement and recognition methods have received more attention.
Pressure sensing has been extensively studied due to its advantages of simple
structure, easy access, visualization application, and harmlessness. This paper
introduces a smart pressure e-mat (SPeM) system based on a piezoresistive
material Velostat for human monitoring applications, including sleeping
postures, sports, and yoga recognition. After a subsystem scans e-mat readings
and processes the signal, it generates a pressure image stream. Deep neural
networks (DNNs) are used to fit and train the pressure image stream and
recognize the corresponding human behavior. Four sleeping postures and five
dynamic activities inspired by Nintendo Switch Ring Fit Adventure (RFA) are
used as a preliminary validation of the proposed SPeM system. The SPeM system
achieves high accuracies on both applications, which demonstrates the high
accuracy and generalization ability of the models. Compared with other pressure
sensor-based systems, SPeM possesses more flexible applications and commercial
application prospects, with reliable, robust, and repeatable properties.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Enhancing Transformer Backbone for Egocentric Video Action Segmentation</b></summary>
  <p><b>编号</b>：[247]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11365</p>
  <p><b>作者</b>：Sakib Reza,  Balaji Sundareshan,  Mohsen Moghaddam,  Octavia Camps</p>
  <p><b>备注</b>：Joint 3rd Ego4D and 11th EPIC Workshop on Egocentric Vision at CVPR 2023</p>
  <p><b>关键词</b>：human behavior analysis, action segmentation models, temporal action segmentation, action segmentation, Egocentric temporal action</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Egocentric temporal action segmentation in videos is a crucial task in
computer vision with applications in various fields such as mixed reality,
human behavior analysis, and robotics. Although recent research has utilized
advanced visual-language frameworks, transformers remain the backbone of action
segmentation models. Therefore, it is necessary to improve transformers to
enhance the robustness of action segmentation models. In this work, we propose
two novel ideas to enhance the state-of-the-art transformer for action
segmentation. First, we introduce a dual dilated attention mechanism to
adaptively capture hierarchical representations in both local-to-global and
global-to-local contexts. Second, we incorporate cross-connections between the
encoder and decoder blocks to prevent the loss of local context by the decoder.
Additionally, we utilize state-of-the-art visual-language representation
learning techniques to extract richer and more compact features for our
transformer. Our proposed approach outperforms other state-of-the-art methods
on the Georgia Tech Egocentric Activities (GTEA) and HOI4D Office Tools
datasets, and we validate our introduced components with ablation studies. The
source code and supplementary materials are publicly available on
this https URL.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Data Redaction from Conditional Generative Models</b></summary>
  <p><b>编号</b>：[255]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11351</p>
  <p><b>作者</b>：Zhifeng Kong,  Kamalika Chaudhuri</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：produce undesirable samples, Deep generative models, harmful content, Deep generative, produce undesirable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep generative models are known to produce undesirable samples such as
harmful content. Traditional mitigation methods include re-training from
scratch, filtering, or editing; however, these are either computationally
expensive or can be circumvented by third parties. In this paper, we take a
different approach and study how to post-edit an already-trained conditional
generative model so that it redacts certain conditionals that will, with high
probability, lead to undesirable content. This is done by distilling the
conditioning network in the models, giving a solution that is effective,
efficient, controllable, and universal for a class of deep generative models.
We conduct experiments on redacting prompts in text-to-image models and
redacting voices in text-to-speech models. Our method is computationally light,
leads to better redaction quality and robustness than baseline methods while
still retaining high generation quality.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：Quantifying the robustness of deep multispectral segmentation models  against natural perturbations and data poisoning</b></summary>
  <p><b>编号</b>：[258]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11347</p>
  <p><b>作者</b>：Elise Bishoff,  Charles Godfrey,  Myles McKay,  Eleanor Byler</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：traditional RGB channels, including additional spectral, additional spectral bands, natural perturbations, spectral bands</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In overhead image segmentation tasks, including additional spectral bands
beyond the traditional RGB channels can improve model performance. However, it
is still unclear how incorporating this additional data impacts model
robustness to adversarial attacks and natural perturbations. For adversarial
robustness, the additional information could improve the model's ability to
distinguish malicious inputs, or simply provide new attack avenues and
vulnerabilities. For natural perturbations, the additional information could
better inform model decisions and weaken perturbation effects or have no
significant influence at all. In this work, we seek to characterize the
performance and robustness of a multispectral (RGB and near infrared) image
segmentation model subjected to adversarial attacks and natural perturbations.
While existing adversarial and natural robustness research has focused
primarily on digital perturbations, we prioritize on creating realistic
perturbations designed with physical world conditions in mind. For adversarial
robustness, we focus on data poisoning attacks whereas for natural robustness,
we focus on extending ImageNet-C common corruptions for fog and snow that
coherently and self-consistently perturbs the input data. Overall, we find both
RGB and multispectral models are vulnerable to data poisoning attacks
regardless of input or fusion architectures and that while physically
realizable natural perturbations still degrade model performance, the impact
differs based on fusion architecture and input data.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Coordinated Transformer with Position \& Sample-aware Central Loss for  Anatomical Landmark Detection</b></summary>
  <p><b>编号</b>：[264]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11338</p>
  <p><b>作者</b>：Qikui Zhu,  Yihui Bi,  Danxin Wang,  Xiangpeng Chu,  Jie Chen,  Yanqing Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Heatmap-based anatomical landmark, central loss, Heatmap-based anatomical, global spatial structure, computational inability challenge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Heatmap-based anatomical landmark detection is still facing two unresolved
challenges: 1) inability to accurately evaluate the distribution of heatmap; 2)
inability to effectively exploit global spatial structure information. To
address the computational inability challenge, we propose a novel
position-aware and sample-aware central loss. Specifically, our central loss
can absorb position information, enabling accurate evaluation of the heatmap
distribution. More advanced is that our central loss is sample-aware, which can
adaptively distinguish easy and hard samples and make the model more focused on
hard samples while solving the challenge of extreme imbalance between landmarks
and non-landmarks. To address the challenge of ignoring structure information,
a Coordinated Transformer, called CoorTransformer, is proposed, which
establishes long-range dependencies under the guidance of landmark coordination
information, making the attention more focused on the sparse landmarks while
taking advantage of global spatial structure. Furthermore, CoorTransformer can
speed up convergence, effectively avoiding the defect that Transformers have
difficulty converging in sparse representation learning. Using the advanced
CoorTransformer and central loss, we propose a generalized detection model that
can handle various scenarios, inherently exploiting the underlying relationship
between landmarks and incorporating rich structural knowledge around the target
landmarks. We analyzed and evaluated CoorTransformer and central loss on three
challenging landmark detection tasks. The experimental results show that our
CoorTransformer outperforms state-of-the-art methods, and the central loss
significantly improves the performance of the model with p-values< 0.05.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：RoomDreamer: Text-Driven 3D Indoor Scene Synthesis with Coherent  Geometry and Texture</b></summary>
  <p><b>编号</b>：[265]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11337</p>
  <p><b>作者</b>：Liangchen Song,  Liangliang Cao,  Hongyu Xu,  Kai Kang,  Feng Tang,  Junsong Yuan,  Yang Zhao</p>
  <p><b>备注</b>：Video results: this https URL</p>
  <p><b>关键词</b>：meshes produced leave, Geometry Guided Diffusion, capturing are widely, meshes produced, produced leave</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The techniques for 3D indoor scene capturing are widely used, but the meshes
produced leave much to be desired. In this paper, we propose "RoomDreamer",
which leverages powerful natural language to synthesize a new room with a
different style. Unlike existing image synthesis methods, our work addresses
the challenge of synthesizing both geometry and texture aligned to the input
scene structure and prompt simultaneously. The key insight is that a scene
should be treated as a whole, taking into account both scene texture and
geometry. The proposed framework consists of two significant components:
Geometry Guided Diffusion and Mesh Optimization. Geometry Guided Diffusion for
3D Scene guarantees the consistency of the scene style by applying the 2D prior
to the entire scene simultaneously. Mesh Optimization improves the geometry and
texture jointly and eliminates the artifacts in the scanned scene. To validate
the proposed method, real indoor scenes scanned with smartphones are used for
extensive experiments, through which the effectiveness of our method is
demonstrated.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：MALM: Mask Augmentation based Local Matching for Food-Recipe Retrieval</b></summary>
  <p><b>编号</b>：[269]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11327</p>
  <p><b>作者</b>：Bhanu Prakash Voutharoja,  Peng Wang,  Lei Wang,  Vivienne Guan</p>
  <p><b>备注</b>：Under review. Link to the dataset repo - this https URL</p>
  <p><b>关键词</b>：significant practical, representations, local matching, food item, matching</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image-to-recipe retrieval is a challenging vision-to-language task of
significant practical value. The main challenge of the task lies in the
ultra-high redundancy in the long recipe and the large variation reflected in
both food item combination and food item appearance. A de-facto idea to address
this task is to learn a shared feature embedding space in which a food image is
aligned better to its paired recipe than other recipes. However, such
supervised global matching is prone to supervision collapse, i.e., only partial
information that is necessary for distinguishing training pairs can be
identified, while other information that is potentially useful in
generalization could be lost. To mitigate such a problem, we propose a
mask-augmentation-based local matching network (MALM), where an image-text
matching module and a masked self-distillation module benefit each other
mutually to learn generalizable cross-modality representations. On one hand, we
perform local matching between the tokenized representations of image and text
to locate fine-grained cross-modality correspondence explicitly. We involve
representations of masked image patches in this process to alleviate
overfitting resulting from local matching especially when some food items are
underrepresented. On the other hand, predicting the hidden representations of
the masked patches through self-distillation helps to learn general-purpose
image representations that are expected to generalize better. And the
multi-task nature of the model enables the representations of masked patches to
be text-aware and thus facilitates the lost information reconstruction.
Experimental results on Recipe1M dataset show our method can clearly outperform
state-of-the-art (SOTA) methods. Our code will be available at
this https URL</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：JoIN: Joint GANs Inversion for Intrinsic Image Decomposition</b></summary>
  <p><b>编号</b>：[272]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11321</p>
  <p><b>作者</b>：Viraj Shah,  Svetlana Lazebnik,  Julien Philip</p>
  <p><b>备注</b>：Project webpage is available at this https URL</p>
  <p><b>关键词</b>：Generative Adversarial Networks, Intrinsic Image Decomposition, Adversarial Networks, Generative Adversarial, bank of Generative</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we propose to solve ill-posed inverse imaging problems using a
bank of Generative Adversarial Networks (GAN) as a prior and apply our method
to the case of Intrinsic Image Decomposition for faces and materials. Our
method builds on the demonstrated success of GANs to capture complex image
distributions. At the core of our approach is the idea that the latent space of
a GAN is a well-suited optimization domain to solve inverse problems. Given an
input image, we propose to jointly inverse the latent codes of a set of GANs
and combine their outputs to reproduce the input. Contrary to most GAN
inversion methods which are limited to inverting only a single GAN, we
demonstrate that it is possible to maintain distribution priors while inverting
several GANs jointly. We show that our approach is modular, allowing various
forward imaging models, that it can successfully decompose both synthetic and
real images, and provides additional advantages such as leveraging properties
of GAN latent space for image relighting.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models</b></summary>
  <p><b>编号</b>：[294]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11281</p>
  <p><b>作者</b>：Ziyi Wu,  Jingyu Hu,  Wuyue Lu,  Igor Gilitschenski,  Animesh Garg</p>
  <p><b>备注</b>：Project page: this https URL . An earlier version of this work appeared at the ICLR 2023 Workshop on Neurosymbolic Generative Models: this https URL</p>
  <p><b>关键词</b>：enable systematic generalization, providing structured representations, providing structured, systematic generalization, Object-centric learning aims</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Object-centric learning aims to represent visual data with a set of object
entities (a.k.a. slots), providing structured representations that enable
systematic generalization. Leveraging advanced architectures like Transformers,
recent approaches have made significant progress in unsupervised object
discovery. In addition, slot-based representations hold great potential for
generative modeling, such as controllable image generation and object
manipulation in image editing. However, current slot-based methods often
produce blurry images and distorted objects, exhibiting poor generative
modeling capabilities. In this paper, we focus on improving slot-to-image
decoding, a crucial aspect for high-quality visual generation. We introduce
SlotDiffusion -- an object-centric Latent Diffusion Model (LDM) designed for
both image and video data. Thanks to the powerful modeling capacity of LDMs,
SlotDiffusion surpasses previous slot models in unsupervised object
segmentation and visual generation across six datasets. Furthermore, our
learned object features can be utilized by existing object-centric dynamics
models, improving video prediction quality and downstream temporal reasoning
tasks. Finally, we demonstrate the scalability of SlotDiffusion to
unconstrained real-world datasets such as PASCAL VOC and COCO, when integrated
with self-supervised pre-trained image encoders.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：Towards Collaborative Plan Acquisition through Theory of Mind Modeling  in Situated Dialogue</b></summary>
  <p><b>编号</b>：[297]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11271</p>
  <p><b>作者</b>：Cristian-Paul Bara,  Ziqiao Ma,  Yingzhuo Yu,  Julie Shah,  Joyce Chai</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：incomplete initial plans, incomplete initial, tasks, partner, knowledge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Collaborative tasks often begin with partial task knowledge and incomplete
initial plans from each partner. To complete these tasks, agents need to engage
in situated communication with their partners and coordinate their partial
plans towards a complete plan to achieve a joint task goal. While such
collaboration seems effortless in a human-human team, it is highly challenging
for human-AI collaboration. To address this limitation, this paper takes a step
towards collaborative plan acquisition, where humans and agents strive to learn
and communicate with each other to acquire a complete plan for joint tasks.
Specifically, we formulate a novel problem for agents to predict the missing
task knowledge for themselves and for their partners based on rich perceptual
and dialogue history. We extend a situated dialogue benchmark for symmetric
collaborative tasks in a 3D blocks world and investigate computational
strategies for plan acquisition. Our empirical results suggest that predicting
the partner's missing knowledge is a more viable approach than predicting one's
own. We show that explicit modeling of the partner's dialogue moves and mental
states produces improved and more stable results than without. These results
provide insight for future AI agents that can predict what knowledge their
partner is missing and, therefore, can proactively communicate such information
to help their partner acquire such missing knowledge toward a common
understanding of joint tasks.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Multi-Focus Image Fusion Based on Spatial Frequency(SF) and Consistency  Verification(CV) in DCT Domain</b></summary>
  <p><b>编号</b>：[299]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11265</p>
  <p><b>作者</b>：Krishnendu K. S.</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Visual Sensor Networks, Wireless Visual Sensor, Sensor Networks, image, output image</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multi-focus is a technique of focusing on different aspects of a particular
object or scene. Wireless Visual Sensor Networks (WVSN) use multi-focus image
fusion, which combines two or more images to create a more accurate output
image that describes the scene better than any individual input image. WVSN has
various applications, including video surveillance, monitoring, and tracking.
Therefore, a high-level analysis of these networks can benefit Biometrics. This
paper introduces an algorithm that utilizes discrete cosine transform (DCT)
standards to fuse multi-focus images in WVSNs. The spatial frequency (SF) of
the corresponding blocks from the source images determines the fusion
criterion. The blocks with higher spatial frequencies make up the DCT
presentation of the fused image, and the Consistency Verification (CV)
procedure is used to enhance the output image quality. The proposed fusion
method was tested on multiple pairs of multi-focus images coded on JPEG
standard to evaluate the fusion performance, and the results indicate that it
improves the visual quality of the output image and outperforms other DCT-based
techniques.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：PDP: Parameter-free Differentiable Pruning is All You Need</b></summary>
  <p><b>编号</b>：[319]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11203</p>
  <p><b>作者</b>：Minsik Cho,  Saurabh Adya,  Devang Naik</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：PDP, DNN accelerators, pruning, DNN, DNN pruning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>DNN pruning is a popular way to reduce the size of a model, improve the
inference latency, and minimize the power consumption on DNN accelerators.
However, existing approaches might be too complex, expensive or ineffective to
apply to a variety of vision/language tasks, DNN architectures and to honor
structured pruning constraints. In this paper, we propose an efficient yet
effective train-time pruning scheme, Parameter-free Differentiable Pruning
(PDP), which offers state-of-the-art qualities in model size, accuracy, and
training cost. PDP uses a dynamic function of weights during training to
generate soft pruning masks for the weights in a parameter-free manner for a
given pruning target. While differentiable, the simplicity and efficiency of
PDP make it universal enough to deliver state-of-the-art
random/structured/channel pruning results on various vision and natural
language tasks. For example, for MobileNet-v1, PDP can achieve 68.2% top-1
ImageNet1k accuracy at 86.6% sparsity, which is 1.7% higher accuracy than those
from the state-of-the-art algorithms. Also, PDP yields over 83.1% accuracy on
Multi-Genre Natural Language Inference with 90% sparsity for BERT, while the
next best from the existing techniques shows 81.5% accuracy. In addition, PDP
can be applied to structured pruning, such as N:M pruning and channel pruning.
For 1:4 structured pruning of ResNet18, PDP improved the top-1 ImageNet1k
accuracy by over 3.6% over the state-of-the-art. For channel pruning of
ResNet50, PDP reduced the top-1 ImageNet1k accuracy by 0.6% from the
state-of-the-art.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：Towards Generalizable Data Protection With Transferable Unlearnable  Examples</b></summary>
  <p><b>编号</b>：[325]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11191</p>
  <p><b>作者</b>：Bin Fang,  Bo Li,  Shuang Wu,  Tianyi Zheng,  Shouhong Ding,  Ran Yi,  Lizhuang Ma</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2305.10691</p>
  <p><b>关键词</b>：making a profound, profound impact, Artificial Intelligence, data, Intelligence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Artificial Intelligence (AI) is making a profound impact in almost every
domain. One of the crucial factors contributing to this success has been the
access to an abundance of high-quality data for constructing machine learning
models. Lately, as the role of data in artificial intelligence has been
significantly magnified, concerns have arisen regarding the secure utilization
of data, particularly in the context of unauthorized data usage. To mitigate
data exploitation, data unlearning have been introduced to render data
unexploitable. However, current unlearnable examples lack the generalization
required for wide applicability. In this paper, we present a novel,
generalizable data protection method by generating transferable unlearnable
examples. To the best of our knowledge, this is the first solution that
examines data privacy from the perspective of data distribution. Through
extensive experimentation, we substantiate the enhanced generalizable
protection capabilities of our proposed method.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：Vanishing Activations: A Symptom of Deep Capsule Networks</b></summary>
  <p><b>编号</b>：[330]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11178</p>
  <p><b>作者</b>：Miles Everett,  Mingjun Zhong,  Georgios Leontidis</p>
  <p><b>备注</b>：9 pages, 7 figures</p>
  <p><b>关键词</b>：Neural Networks utilizing, original Capsule Network, Capsule Networks, Networks utilizing vector, visual concepts evolve</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Capsule Networks, an extension to Neural Networks utilizing vector or matrix
representations instead of scalars, were initially developed to create a
dynamic parse tree where visual concepts evolve from parts to complete objects.
Early implementations of Capsule Networks achieved and maintain
state-of-the-art results on various datasets. However, recent studies have
revealed shortcomings in the original Capsule Network architecture, notably its
failure to construct a parse tree and its susceptibility to vanishing gradients
when deployed in deeper networks. This paper extends the investigation to a
range of leading Capsule Network architectures, demonstrating that these issues
are not confined to the original design. We argue that the majority of Capsule
Network research has produced architectures that, while modestly divergent from
the original Capsule Network, still retain a fundamentally similar structure.
We posit that this inherent design similarity might be impeding the scalability
of Capsule Networks. Our study contributes to the broader discussion on
improving the robustness and scalability of Capsule Networks.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：Towards More Transparent and Accurate Cancer Diagnosis with an  Unsupervised CAE Approach</b></summary>
  <p><b>编号</b>：[340]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11728</p>
  <p><b>作者</b>：Zahra Tabatabaei,  Adrian Colomer,  Javier Oliver Moll,  Valery Naranjo</p>
  <p><b>备注</b>：this paper is under review in Scientific reports</p>
  <p><b>关键词</b>：Medical Image Retrieval, Digital pathology, pathology has revolutionized, Content-Based Medical Image, revolutionized cancer diagnosis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Digital pathology has revolutionized cancer diagnosis by leveraging
Content-Based Medical Image Retrieval (CBMIR) for analyzing histopathological
Whole Slide Images (WSIs). CBMIR enables searching for similar content,
enhancing diagnostic reliability and accuracy. In 2020, breast and prostate
cancer constituted 11.7% and 14.1% of cases, respectively, as reported by the
Global Cancer Observatory (GCO). The proposed Unsupervised CBMIR (UCBMIR)
replicates the traditional cancer diagnosis workflow, offering a dependable
method to support pathologists in WSI-based diagnostic conclusions. This
approach alleviates pathologists' workload, potentially enhancing diagnostic
efficiency. To address the challenge of the lack of labeled histopathological
images in CBMIR, a customized unsupervised Convolutional Auto Encoder (CAE) was
developed, extracting 200 features per image for the search engine component.
UCBMIR was evaluated using widely-used numerical techniques in CBMIR, alongside
visual evaluation and comparison with a classifier. The validation involved
three distinct datasets, with an external evaluation demonstrating its
effectiveness. UCBMIR outperformed previous studies, achieving a top 5 recall
of 99% and 80% on BreaKHis and SICAPv2, respectively, using the first
evaluation technique. Precision rates of 91% and 70% were achieved for BreaKHis
and SICAPv2, respectively, using the second evaluation technique. Furthermore,
UCBMIR demonstrated the capability to identify various patterns in patches,
achieving an 81% accuracy in the top 5 when tested on an external image from
Arvaniti.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：A quality assurance framework for real-time monitoring of deep learning  segmentation models in radiotherapy</b></summary>
  <p><b>编号</b>：[341]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11715</p>
  <p><b>作者</b>：Xiyao Jin,  Yao Hao,  Jessica Hilliard,  Zhehao Zhang,  Maria A. Thomas,  Hua Li,  Abhinav K. Jha,  Geoffrey D. Hugo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ground truth contours, safely deploy deep, deploy deep learning, deep learning models, quality assurance framework</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To safely deploy deep learning models in the clinic, a quality assurance
framework is needed for routine or continuous monitoring of input-domain shift
and the models' performance without ground truth contours. In this work,
cardiac substructure segmentation was used as an example task to establish a QA
framework. A benchmark dataset consisting of Computed Tomography (CT) images
along with manual cardiac delineations of 241 patients were collected,
including one 'common' image domain and five 'uncommon' domains. Segmentation
models were tested on the benchmark dataset for an initial evaluation of model
capacity and limitations. An image domain shift detector was developed by
utilizing a trained Denoising autoencoder (DAE) and two hand-engineered
features. Another Variational Autoencoder (VAE) was also trained to estimate
the shape quality of the auto-segmentation results. Using the extracted
features from the image/segmentation pair as inputs, a regression model was
trained to predict the per-patient segmentation accuracy, measured by Dice
coefficient similarity (DSC). The framework was tested across 19 segmentation
models to evaluate the generalizability of the entire framework.
As results, the predicted DSC of regression models achieved a mean absolute
error (MAE) ranging from 0.036 to 0.046 with an averaged MAE of 0.041. When
tested on the benchmark dataset, the performances of all segmentation models
were not significantly affected by scanning parameters: FOV, slice thickness
and reconstructions kernels. For input images with Poisson noise, CNN-based
segmentation models demonstrated a decreased DSC ranging from 0.07 to 0.41,
while the transformer-based model was not significantly affected.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：Sim-to-Real Segmentation in Robot-assisted Transoral Tracheal Intubation</b></summary>
  <p><b>编号</b>：[342]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11686</p>
  <p><b>作者</b>：Guankun Wang,  Tian-Ao Ren,  Jiewen Lai,  Long Bai,  Hongliang Ren</p>
  <p><b>备注</b>：Extended abstract in IEEE ICRA 2023 Workshop (New Evolutions in Surgical Robotics: Embracing Multimodal Imaging Guidance, Intelligence, and Bio-inspired Mechanisms)</p>
  <p><b>关键词</b>：distinguish anatomical features, Robotic-assisted tracheal intubation, tracheal intubation requires, Robotic-assisted tracheal, requires the robot</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Robotic-assisted tracheal intubation requires the robot to distinguish
anatomical features like an experienced physician using deep-learning
techniques. However, real datasets of oropharyngeal organs are limited due to
patient privacy issues, making it challenging to train deep-learning models for
accurate image segmentation. We hereby consider generating a new data modality
through a virtual environment to assist the training process. Specifically,
this work introduces a virtual dataset generated by the Simulation Open
Framework Architecture (SOFA) framework to overcome the limited availability of
actual endoscopic images. We also propose a domain adaptive Sim-to-Real method
for oropharyngeal organ image segmentation, which employs an image blending
strategy called IoU-Ranking Blend (IRB) and style-transfer techniques to
address discrepancies between datasets. Experimental results demonstrate the
superior performance of the proposed approach with domain adaptive models,
improving segmentation accuracy and training stability. In the practical
application, the trained segmentation model holds great promise for
robot-assisted intubation surgery and intelligent surgical navigation.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：JOINEDTrans: Prior Guided Multi-task Transformer for Joint Optic  Disc/Cup Segmentation and Fovea Detection</b></summary>
  <p><b>编号</b>：[352]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11504</p>
  <p><b>作者</b>：Huaqing He,  Li Lin,  Zhiyuan Cai,  Pujin Cheng,  Xiaoying Tang</p>
  <p><b>备注</b>：11 pages, 6 figures</p>
  <p><b>关键词</b>：Deep learning-based image, analyzing retinal landmarks, optic disc, optic cup, learning-based image segmentation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning-based image segmentation and detection models have largely
improved the efficiency of analyzing retinal landmarks such as optic disc (OD),
optic cup (OC), and fovea. However, factors including ophthalmic
disease-related lesions and low image quality issues may severely complicate
automatic OD/OC segmentation and fovea detection. Most existing works treat the
identification of each landmark as a single task, and take into account no
prior information. To address these issues, we propose a prior guided
multi-task transformer framework for joint OD/OC segmentation and fovea
detection, named JOINEDTrans. JOINEDTrans effectively combines various spatial
features of the fundus images, relieving the structural distortions induced by
lesions and other imaging issues. It contains a segmentation branch and a
detection branch. To be noted, we employ an encoder pretrained in a vessel
segmentation task to effectively exploit the positional relationship among
vessel, OD/OC, and fovea, successfully incorporating spatial prior into the
proposed JOINEDTrans framework. There are a coarse stage and a fine stage in
JOINEDTrans. In the coarse stage, OD/OC coarse segmentation and fovea heatmap
localization are obtained through a joint segmentation and detection module. In
the fine stage, we crop regions of interest for subsequent refinement and use
predictions obtained in the coarse stage to provide additional information for
better performance and faster convergence. Experimental results demonstrate
that JOINEDTrans outperforms existing state-of-the-art methods on the publicly
available GAMMA, REFUGE, and PALM fundus image datasets. We make our code
available at this https URL</p>
  </details>
</details>
<h1>自然语言处理</h1>
<details>
  <summary>1. <b>标题：Scaling laws for language encoding models in fMRI</b></summary>
  <p><b>编号</b>：[5]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11863</p>
  <p><b>作者</b>：Richard Antonello,  Aditya Vaidya,  Alexander G. Huth</p>
  <p><b>备注</b>：Under review</p>
  <p><b>关键词</b>：transformer-based unidirectional language, predicting brain responses, unidirectional language models, Representations from transformer-based, transformer-based unidirectional</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Representations from transformer-based unidirectional language models are
known to be effective at predicting brain responses to natural language.
However, most studies comparing language models to brains have used GPT-2 or
similarly sized language models. Here we tested whether larger open-source
models such as those from the OPT and LLaMA families are better at predicting
brain responses recorded using fMRI. Mirroring scaling results from other
contexts, we found that brain prediction performance scales log-linearly with
model size from 125M to 30B parameter models, with ~15% increased encoding
performance as measured by correlation with a held-out test set across 3
subjects. Similar log-linear behavior was observed when scaling the size of the
fMRI training set. We also characterized scaling for acoustic encoding models
that use HuBERT, WavLM, and Whisper, and we found comparable improvements with
model size. A noise ceiling analysis of these large, high-performance encoding
models showed that performance is nearing the theoretical maximum for brain
areas such as the precuneus and higher auditory cortex. These results suggest
that increasing scale in both models and data will yield incredibly effective
models of language processing in the brain, enabling better scientific
understanding as well as applications such as decoding.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Reducing Sequence Length by Predicting Edit Operations with Large  Language Models</b></summary>
  <p><b>编号</b>：[6]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11862</p>
  <p><b>作者</b>：Masahiro Kaneko,  Naoaki Okazaki</p>
  <p><b>备注</b>：Work in progress</p>
  <p><b>关键词</b>：Large Language Models, Language Models, gained significant attention, Large Language, demonstrated remarkable performance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) have demonstrated remarkable performance in
various tasks and gained significant attention. LLMs are also used for local
sequence transduction tasks, including grammatical error correction (GEC) and
formality style transfer, where most tokens in a source text are kept
unchanged. However, it is inefficient to generate all target tokens because a
prediction error of a target token may cause a catastrophe in predicting
subsequent tokens and because the computational cost grows quadratically with
the target sequence length. This paper proposes to predict a set of edit
operations for the source text for local sequence transduction tasks.
Representing an edit operation with a span of the source text and changed
tokens, we can reduce the length of the target sequence and thus the
computational cost for inference. We apply instruction tuning for LLMs on the
supervision data of edit operations. Experiments show that the proposed method
achieves comparable performance to the baseline in four tasks, paraphrasing,
formality style transfer, GEC, and text simplification, despite reducing the
length of the target text by as small as 21\%. Furthermore, we report that the
instruction tuning with the proposed method achieved the state-of-the-art
performance in the four tasks.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning  with LLMs</b></summary>
  <p><b>编号</b>：[7]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11860</p>
  <p><b>作者</b>：Pranjal Aggarwal,  Aman Madaan,  Yiming Yang,  Mausam</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, language models, frequent solution, improving the correctness, large language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A popular approach for improving the correctness of output from large
language models (LLMs) is Self-Consistency - poll the LLM multiple times and
output the most frequent solution. Existing Self-Consistency techniques always
draw a constant number of samples per question, where a better approach will be
to non-uniformly distribute the available budget based on the amount of
agreement in the samples drawn so far. In response, we introduce
Adaptive-Consistency, a cost-efficient, model-agnostic technique that
dynamically adjusts the number of samples per question using a lightweight
stopping criterion. Our experiments over 13 datasets and two LLMs demonstrate
that Adaptive-Consistency reduces sample budget by up to 6.0 times with an
average accuracy drop of less than 0.1%.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Complex Claim Verification with Evidence Retrieved in the Wild</b></summary>
  <p><b>编号</b>：[8]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11859</p>
  <p><b>作者</b>：Jifan Chen,  Grace Kim,  Aniruddh Sriram,  Greg Durrett,  Eunsol Choi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：automatic fact-checking, core part, part of automatic, Evidence, retrieval</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Evidence retrieval is a core part of automatic fact-checking. Prior work
makes simplifying assumptions in retrieval that depart from real-world use
cases: either no access to evidence, access to evidence curated by a human
fact-checker, or access to evidence available long after the claim has been
made. In this work, we present the first fully automated pipeline to check
real-world claims by retrieving raw evidence from the web. We restrict our
retriever to only search documents available prior to the claim's making,
modeling the realistic scenario where an emerging claim needs to be checked.
Our pipeline includes five components: claim decomposition, raw document
retrieval, fine-grained evidence retrieval, claim-focused summarization, and
veracity judgment. We conduct experiments on complex political claims in the
ClaimDecomp dataset and show that the aggregated evidence produced by our
pipeline improves veracity judgments. Human evaluation finds the evidence
summary produced by our system is reliable (it does not hallucinate
information) and relevant to answering key questions about a claim, suggesting
that it can assist fact-checkers even when it cannot surface a complete
evidence set.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain,  and Cross-domain Settings</b></summary>
  <p><b>编号</b>：[12]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11853</p>
  <p><b>作者</b>：Shuaichen Chang,  Eric Fosler-Lussier</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large language models, demonstrated remarkable capability, Large language, language models, in-context learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) with in-context learning have demonstrated
remarkable capability in the text-to-SQL task. Previous research has prompted
LLMs with various demonstration-retrieval strategies and intermediate reasoning
steps to enhance the performance of LLMs. However, those works often employ
varied strategies when constructing the prompt text for text-to-SQL inputs,
such as databases and demonstration examples. This leads to a lack of
comparability in both the prompt constructions and their primary contributions.
Furthermore, selecting an effective prompt construction has emerged as a
persistent problem for future research. To address this limitation, we
comprehensively investigate the impact of prompt constructions across various
settings and provide insights for future work.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Any-to-Any Generation via Composable Diffusion</b></summary>
  <p><b>编号</b>：[13]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11846</p>
  <p><b>作者</b>：Zineng Tang,  Ziyi Yang,  Chenguang Zhu,  Michael Zeng,  Mohit Bansal</p>
  <p><b>备注</b>：Project Page: this https URL</p>
  <p><b>关键词</b>：generative model capable, model capable, capable of generating, modalities, generative model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present Composable Diffusion (CoDi), a novel generative model capable of
generating any combination of output modalities, such as language, image,
video, or audio, from any combination of input modalities. Unlike existing
generative AI systems, CoDi can generate multiple modalities in parallel and
its input is not limited to a subset of modalities like text or image. Despite
the absence of training datasets for many combinations of modalities, we
propose to align modalities in both the input and output space. This allows
CoDi to freely condition on any input combination and generate any group of
modalities, even if they are not present in the training data. CoDi employs a
novel composable generation strategy which involves building a shared
multimodal space by bridging alignment in the diffusion process, enabling the
synchronized generation of intertwined modalities, such as temporally aligned
video and audio. Highly customizable and flexible, CoDi achieves strong
joint-modality generation quality, and outperforms or is on par with the
unimodal state-of-the-art for single-modality synthesis. The project page with
demonstrations and code is at this https URL</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：RxnScribe: A Sequence Generation Model for Reaction Diagram Parsing</b></summary>
  <p><b>编号</b>：[14]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11845</p>
  <p><b>作者</b>：Yujie Qian,  Jiang Guo,  Zhengkai Tu,  Connor W. Coley,  Regina Barzilay</p>
  <p><b>备注</b>：To be published in the Journal of Chemical Information and Modeling</p>
  <p><b>关键词</b>：extracting reaction schemes, chemistry literature, Reaction, extracting reaction, reaction schemes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reaction diagram parsing is the task of extracting reaction schemes from a
diagram in the chemistry literature. The reaction diagrams can be arbitrarily
complex, thus robustly parsing them into structured data is an open challenge.
In this paper, we present RxnScribe, a machine learning model for parsing
reaction diagrams of varying styles. We formulate this structured prediction
task with a sequence generation approach, which condenses the traditional
pipeline into an end-to-end model. We train RxnScribe on a dataset of 1,378
diagrams and evaluate it with cross validation, achieving an 80.0% soft match
F1 score, with significant improvements over previous models. Our code and data
are publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：How Does Generative Retrieval Scale to Millions of Passages?</b></summary>
  <p><b>编号</b>：[16]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11841</p>
  <p><b>作者</b>：Ronak Pradeep,  Kai Hui,  Jai Gupta,  Adam D. Lelkes,  Honglei Zhuang,  Jimmy Lin,  Donald Metzler,  Vinh Q. Tran</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Differentiable Search Index, Search Index, Differentiable Search, forgoing external indices, single Transformer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Popularized by the Differentiable Search Index, the emerging paradigm of
generative retrieval re-frames the classic information retrieval problem into a
sequence-to-sequence modeling task, forgoing external indices and encoding an
entire document corpus within a single Transformer. Although many different
approaches have been proposed to improve the effectiveness of generative
retrieval, they have only been evaluated on document corpora on the order of
100k in size. We conduct the first empirical study of generative retrieval
techniques across various corpus scales, ultimately scaling up to the entire MS
MARCO passage ranking task with a corpus of 8.8M passages and evaluating model
sizes up to 11B parameters. We uncover several findings about scaling
generative retrieval to millions of passages; notably, the central importance
of using synthetic queries as document representations during indexing, the
ineffectiveness of existing proposed architecture modifications when accounting
for compute cost, and the limits of naively scaling model parameters with
respect to retrieval performance. While we find that generative retrieval is
competitive with state-of-the-art dual encoders on small corpora, scaling to
millions of passages remains an important and unsolved challenge. We believe
these findings will be valuable for the community to clarify the current state
of generative retrieval, highlight the unique challenges, and inspire new
research directions.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage  Leveraging Generative Models</b></summary>
  <p><b>编号</b>：[17]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11840</p>
  <p><b>作者</b>：Akshita Jha,  Aida Davani,  Chandan K. Reddy,  Shachi Dave,  Vinodkumar Prabhakaran,  Sunipa Dev</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：people in NLP, mitigate social stereotypes, NLP models, crucial to detect, detect and mitigate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Stereotype benchmark datasets are crucial to detect and mitigate social
stereotypes about groups of people in NLP models. However, existing datasets
are limited in size and coverage, and are largely restricted to stereotypes
prevalent in the Western society. This is especially problematic as language
technologies gain hold across the globe. To address this gap, we present
SeeGULL, a broad-coverage stereotype dataset, built by utilizing generative
capabilities of large language models such as PaLM, and GPT-3, and leveraging a
globally diverse rater pool to validate the prevalence of those stereotypes in
society. SeeGULL is in English, and contains stereotypes about identity groups
spanning 178 countries across 8 different geo-political regions across 6
continents, as well as state-level identities within the US and India. We also
include fine-grained offensiveness scores for different stereotypes and
demonstrate their global disparities. Furthermore, we include comparative
annotations about the same groups by annotators living in the region vs. those
that are based in North America, and demonstrate that within-region stereotypes
about groups differ from those prevalent in North America. CONTENT WARNING:
This paper contains stereotype examples that may be offensive.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Appraising the Potential Uses and Harms of LLMs for Medical Systematic  Reviews</b></summary>
  <p><b>编号</b>：[22]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11828</p>
  <p><b>作者</b>：Hye Sun Yun,  Iain J. Marshall,  Thomas Trikalinos,  Byron C. Wallace</p>
  <p><b>备注</b>：33 pages, 3 figures, 7 tables</p>
  <p><b>关键词</b>：informing clinical decision, clinical decision making, crucial for informing, informing clinical, clinical decision</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Medical systematic reviews are crucial for informing clinical decision making
and healthcare policy. But producing such reviews is onerous and
time-consuming. Thus, high-quality evidence synopses are not available for many
questions and may be outdated even when they are available. Large language
models (LLMs) are now capable of generating long-form texts, suggesting the
tantalizing possibility of automatically generating literature reviews on
demand. However, LLMs sometimes generate inaccurate (and potentially
misleading) texts by hallucinating or omitting important information. In the
healthcare context, this may render LLMs unusable at best and dangerous at
worst. Most discussion surrounding the benefits and risks of LLMs have been
divorced from specific applications. In this work, we seek to qualitatively
characterize the potential utility and risks of LLMs for assisting in
production of medical evidence reviews. We conducted 16 semi-structured
interviews with international experts in systematic reviews, grounding
discussion in the context of generating evidence reviews. Domain experts
indicated that LLMs could aid writing reviews, as a tool for drafting or
creating plain language summaries, generating templates or suggestions,
distilling information, crosschecking, and synthesizing or interpreting text
inputs. But they also identified issues with model outputs and expressed
concerns about potential downstream harms of confidently composed but
inaccurate LLM outputs which might mislead. Other anticipated potential
downstream harms included lessened accountability and proliferation of
automatically generated reviews that might be of low quality. Informed by this
qualitative analysis, we identify criteria for rigorous evaluation of
biomedical LLMs aligned with domain expert views.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：STOAT: Structured Data to Analytical Text With Controls</b></summary>
  <p><b>编号</b>：[23]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11826</p>
  <p><b>作者</b>：Deepanway Ghosal,  Preksha Nema,  Aravindan Raghuveer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：made tremendous progress, Recent language models, text generation, Recent language, made tremendous</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent language models have made tremendous progress in the structured data
to text generation task. However, these models still give sub-optimal
performance where logical inference is required to generate the descriptions.
In this work, we specifically focus on analytical text generation from
structured data such as tables. Building on the taxonomy proposed in (Gupta et
al., 2020) we focus on controllable table to text generation for the following
reasoning categories: numerical reasoning, commonsense reasoning, temporal
reasoning, table knowledge, and entity knowledge. We propose STOAT model, which
is table and reasoning aware, with vector-quantization to infuse the given
reasoning categories in the output. We observe that our model provides 10.19%,
1.13% improvement on the PARENT metric in iToTTo and Infotabs for the
analytical sentence task. We also found that our model generates 15.3% more
faithful and analytical descriptions as compared to the baseline models in
human evaluation. We curate and release two reasoning category annotated
table-to-interesting text generation datasets based on the ToTTo (Parikh et
al., 2020) and InfoTabs datasets (Gupta et al.,2020).</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Pseudo-Label Training and Model Inertia in Neural Machine Translation</b></summary>
  <p><b>编号</b>：[29]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11808</p>
  <p><b>作者</b>：Benjamin Hsu,  Anna Currey,  Xing Niu,  Maria Nădejde,  Georgiana Dinu</p>
  <p><b>备注</b>：accepted ICLR 2023</p>
  <p><b>关键词</b>：neural machine translation, machine learning applications, over-parameterized deep neural, deep neural models, machine translation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Like many other machine learning applications, neural machine translation
(NMT) benefits from over-parameterized deep neural models. However, these
models have been observed to be brittle: NMT model predictions are sensitive to
small input changes and can show significant variation across re-training or
incremental model updates. This work studies a frequently used method in NMT,
pseudo-label training (PLT), which is common to the related techniques of
forward-translation (or self-training) and sequence-level knowledge
distillation. While the effect of PLT on quality is well-documented, we
highlight a lesser-known effect: PLT can enhance a model's stability to model
updates and input perturbations, a set of properties we call model inertia. We
study inertia effects under different training settings and we identify
distribution simplification as a mechanism behind the observed results.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：The Inside Story: Towards Better Understanding of Machine Translation  Neural Evaluation Metrics</b></summary>
  <p><b>编号</b>：[31]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11806</p>
  <p><b>作者</b>：Ricardo Rei,  Nuno M. Guerreiro,  Marcos Treviso,  Luisa Coheur,  Alon Lavie,  André F.T. Martins</p>
  <p><b>备注</b>：Accepted at ACL 2023</p>
  <p><b>关键词</b>：exhibit significant improvements, machine translation evaluation, traditional metrics based, exhibit significant, human judgments</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural metrics for machine translation evaluation, such as COMET, exhibit
significant improvements in their correlation with human judgments, as compared
to traditional metrics based on lexical overlap, such as BLEU. Yet, neural
metrics are, to a great extent, "black boxes" returning a single sentence-level
score without transparency about the decision-making process. In this work, we
develop and compare several neural explainability methods and demonstrate their
effectiveness for interpreting state-of-the-art fine-tuned neural metrics. Our
study reveals that these metrics leverage token-level information that can be
directly attributed to translation errors, as assessed through comparison of
token-level neural saliency maps with Multidimensional Quality Metrics (MQM)
annotations and with synthetically-generated critical translation errors. To
ease future research, we release our code at:
this https URL.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Chain-of-thought prompting for responding to in-depth dialogue questions  with LLM</b></summary>
  <p><b>编号</b>：[34]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11792</p>
  <p><b>作者</b>：Hongru Wang,  Rui Wang,  Fei Mi,  Zezhong Wang,  Ruifeng Xu,  Kam-Fai Wong</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：textit, user status, provide insight, current status, user</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The way and content in which users ask questions can provide insight into
their current status, including their personality, emotions, and psychology.
Instead of directly prompting the large language models (LLMs), we explore how
chain-of-thought prompting helps in this scenario to perform reasoning and
planning according to user status, aiming to provide a more personalized and
engaging experience for the user query. To this end, we first construct a
benchmark of 6 dialogue or question-answering datasets in both English and
Chinese, covering 3 different aspects of user status (\textit{including}
\textit{personality}, \textit{emotion}, and \textit{psychology}). Then we
prompt the LLMs to generate the response regarding the user status as
intermediate reasoning processing. We propose a novel demonstration selection
strategy using the semantic similarity of intermediate reasoning instead of
test queries. To evaluate the effectiveness and robustness of our approach, we
conduct extensive experiments with 7 LLMs under zero-shot and one-shot
settings. The experimental results show that our approach consistently
outperforms standard prompting in terms of both \textit{helpfulness} and
\textit{acceptness} across all datasets, regardless of the LLMs used. The code
and dataset can be found at
\url{this https URL\_CoT.git}.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Enhancing Few-shot NER with Prompt Ordering based Data Augmentation</b></summary>
  <p><b>编号</b>：[35]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11791</p>
  <p><b>作者</b>：Huiming Wang,  Liying Cheng,  Wenxuan Zhang,  De Wen Soh,  Lidong Bing</p>
  <p><b>备注</b>：7 pages, 2 figures</p>
  <p><b>关键词</b>：pre-trained language models, named entity recognition, including few-shot named, low-resource settings, pre-trained language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, data augmentation (DA) methods have been proven to be effective for
pre-trained language models (PLMs) in low-resource settings, including few-shot
named entity recognition (NER). However, conventional NER DA methods are mostly
aimed at sequence labeling models, i.e., token-level classification, and few
are compatible with unified autoregressive generation frameworks, which can
handle a wider range of NER tasks, such as nested NER. Furthermore, these
generation frameworks have a strong assumption that the entities will appear in
the target sequence with the same left-to-right order as the source sequence.
In this paper, we claim that there is no need to keep this strict order, and
more diversified but reasonable target entity sequences can be provided during
the training stage as a novel DA method. Nevertheless, a naive mixture of
augmented data can confuse the model since one source sequence will then be
paired with different target sequences. Therefore, we propose a simple but
effective Prompt Ordering based Data Augmentation (PODA) method to improve the
training of unified autoregressive generation frameworks under few-shot NER
scenarios. Experimental results on three public NER datasets and further
analyses demonstrate the effectiveness of our approach.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Prompting with Pseudo-Code Instructions</b></summary>
  <p><b>编号</b>：[36]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11790</p>
  <p><b>作者</b>：Mayank Mishra,  Prince Kumar,  Riyaz Bhat,  Rudra Murthy V,  Danish Contractor,  Srikanth Tamilselvam</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, natural language, recently emerged, popular method, method of harnessing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Prompting with natural language instructions has recently emerged as a
popular method of harnessing the capabilities of large language models. Given
the inherent ambiguity present in natural language, it is intuitive to consider
the possible advantages of prompting with less ambiguous prompt styles, such as
the use of pseudo-code.
In this paper we explore if prompting via pseudo-code instructions helps
improve the performance of pre-trained language models. We manually create a
dataset of pseudo-code prompts for 132 different tasks spanning classification,
QA and generative language tasks, sourced from the Super-NaturalInstructions
dataset. Using these prompts along with their counterparts in natural language,
we study their performance on two LLM families - BLOOM and CodeGen. Our
experiments show that using pseudo-code instructions leads to better results,
with an average increase (absolute) of 7-16 points in F1 scores for
classification tasks and an improvement (relative) of 12-38% in aggregate ROUGE
scores across all tasks. We include detailed ablation studies which indicate
that code comments, docstrings, and the structural clues encoded in pseudo-code
all contribute towards the improvement in performance.
To the best of our knowledge our work is the first to demonstrate how
pseudo-code prompts can be helpful in improving the performance of pre-trained
LMs.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Solving NLP Problems through Human-System Collaboration: A  Discussion-based Approach</b></summary>
  <p><b>编号</b>：[37]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11789</p>
  <p><b>作者</b>：Masahiro Kaneko,  Graham Neubig,  Naoaki Okazaki</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：solve common problems, solve common, common problems, agreeing or disagreeing, Humans work</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Humans work together to solve common problems by having discussions,
explaining, and agreeing or disagreeing with each other. Similarly, if a system
can have discussions with humans when solving tasks, it can improve the
system's performance and reliability. In previous research on explainability,
it has only been possible for the system to make predictions and for humans to
ask questions about them rather than having a mutual exchange of opinions. This
research aims to create a dataset and computational framework for systems that
discuss and refine their predictions through dialogue. Through experiments, we
show that the proposed system can have beneficial discussions with humans
improving the accuracy by up to 25 points in the natural language inference
task.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：DMDD: A Large-Scale Dataset for Dataset Mentions Detection</b></summary>
  <p><b>编号</b>：[41]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11779</p>
  <p><b>作者</b>：Huitong Pan,  Qi Zhang,  Eduard Dragut,  Cornelia Caragea,  Longin Jan Latecki</p>
  <p><b>备注</b>：Pre-MIT Press publication version. Submitted to TACL</p>
  <p><b>关键词</b>：dataset mention detection, automatic information extraction, dataset mention, mention detection, identify research opportunities</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The recognition of dataset names is a critical task for automatic information
extraction in scientific literature, enabling researchers to understand and
identify research opportunities. However, existing corpora for dataset mention
detection are limited in size and naming diversity. In this paper, we introduce
the Dataset Mentions Detection Dataset (DMDD), the largest publicly available
corpus for this task. DMDD consists of the DMDD main corpus, comprising 31,219
scientific articles with over 449,000 dataset mentions weakly annotated in the
format of in-text spans, and an evaluation set, which comprises of 450
scientific articles manually annotated for evaluation purposes. We use DMDD to
establish baseline performance for dataset mention detection and linking. By
analyzing the performance of various models on DMDD, we are able to identify
open problems in dataset mention detection. We invite the community to use our
dataset as a challenge to develop novel dataset mention detection models.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Cross-Lingual Supervision improves Large Language Models Pre-training</b></summary>
  <p><b>编号</b>：[42]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11778</p>
  <p><b>作者</b>：Andrea Schioppa,  Xavier Garcia,  Orhan Firat</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：recent rapid progress, pre-training Large Language, Machine Translation Systems, self-supervised language modeling, Large Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The recent rapid progress in pre-training Large Language Models has relied on
using self-supervised language modeling objectives like next token prediction
or span corruption. On the other hand, Machine Translation Systems are mostly
trained using cross-lingual supervision that requires aligned data between
source and target languages. We demonstrate that pre-training Large Language
Models on a mixture of a self-supervised Language Modeling objective and the
supervised Machine Translation objective, therefore including cross-lingual
parallel data during pre-training, yields models with better in-context
learning abilities. As pre-training is a very resource-intensive process and a
grid search on the best mixing ratio between the two objectives is
prohibitively expensive, we propose a simple yet effective strategy to learn it
during pre-training.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Enhancing Vision-Language Pre-Training with Jointly Learned Questioner  and Dense Captioner</b></summary>
  <p><b>编号</b>：[44]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11769</p>
  <p><b>作者</b>：Zikang Liu,  Sihan Chen,  Longteng Guo,  Handong Li,  Xingjian He,  Jing Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：visual question answering, demonstrated significant success, including image captioning, VQA and dense, Large pre-trained multimodal</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large pre-trained multimodal models have demonstrated significant success in
a range of downstream tasks, including image captioning, image-text retrieval,
visual question answering (VQA), etc. However, many of these methods rely on
image-text pairs collected from the web as pre-training data and unfortunately
overlook the need for fine-grained feature alignment between vision and
language modalities, which requires detailed understanding of images and
language expressions. While integrating VQA and dense captioning (DC) into
pre-training can address this issue, acquiring image-question-answer as well as
image-location-caption triplets is challenging and time-consuming.
Additionally, publicly available datasets for VQA and dense captioning are
typically limited in scale due to manual data collection and labeling efforts.
In this paper, we propose a novel method called Joint QA and DC GEneration
(JADE), which utilizes a pre-trained multimodal model and easily-crawled
image-text pairs to automatically generate and filter large-scale VQA and dense
captioning datasets. We apply this method to the Conceptual Caption (CC3M)
dataset to generate a new dataset called CC3M-QA-DC. Experiments show that when
used for pre-training in a multi-task manner, CC3M-QA-DC can improve the
performance with various backbones on various downstream tasks. Furthermore,
our generated CC3M-QA-DC can be combined with larger image-text datasets (e.g.,
CC15M) and achieve competitive results compared with models using much more
data. Code and dataset will be released.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Generating Visual Spatial Description via Holistic 3D Scene  Understanding</b></summary>
  <p><b>编号</b>：[45]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11768</p>
  <p><b>作者</b>：Yu Zhao,  Hao Fei,  Wei Ji,  Jianguo Wei,  Meishan Zhang,  Min Zhang,  Tat-Seng Chua</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Visual spatial description, aims to generate, Existing VSD work, VSD, target objects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Visual spatial description (VSD) aims to generate texts that describe the
spatial relations of the given objects within images. Existing VSD work merely
models the 2D geometrical vision features, thus inevitably falling prey to the
problem of skewed spatial understanding of target objects. In this work, we
investigate the incorporation of 3D scene features for VSD. With an external 3D
scene extractor, we obtain the 3D objects and scene features for input images,
based on which we construct a target object-centered 3D spatial scene graph
(Go3D-S2G), such that we model the spatial semantics of target objects within
the holistic 3D scenes. Besides, we propose a scene subgraph selecting
mechanism, sampling topologically-diverse subgraphs from Go3D-S2G, where the
diverse local structure features are navigated to yield spatially-diversified
text generation. Experimental results on two VSD datasets demonstrate that our
framework outperforms the baselines significantly, especially improving on the
cases with complex visual spatial relations. Meanwhile, our method can produce
more spatially-diversified generation. Code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：ReSeTOX: Re-learning attention weights for toxicity mitigation in  machine translation</b></summary>
  <p><b>编号</b>：[47]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11761</p>
  <p><b>作者</b>：Javier García Gilabert,  Carlos Escolano,  Marta R. Costa-Jussà</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：proposed method, addresses the issue, Neural Machine Translation, REdo SEarch, generating translation outputs</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Our proposed method, ReSeTOX (REdo SEarch if TOXic), addresses the issue of
Neural Machine Translation (NMT) generating translation outputs that contain
toxic words not present in the input. The objective is to mitigate the
introduction of toxic language without the need for re-training. In the case of
identified added toxicity during the inference process, ReSeTOX dynamically
adjusts the key-value self-attention weights and re-evaluates the beam search
hypotheses. Experimental results demonstrate that ReSeTOX achieves a remarkable
57% reduction in added toxicity while maintaining an average translation
quality of 99.5% across 164 languages.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Controlling the Extraction of Memorized Data from Large Language Models  via Prompt-Tuning</b></summary>
  <p><b>编号</b>：[48]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11759</p>
  <p><b>作者</b>：Mustafa Safa Ozdayi,  Charith Peris,  Jack FitzGerald,  Christophe Dupuy,  Jimit Majmudar,  Haidar Khan,  Rahil Parikh,  Rahul Gupta</p>
  <p><b>备注</b>：5 pages, 3 Figures, ACL 2023</p>
  <p><b>关键词</b>：Large Language Models, memorize significant portions, Large Language, Language Models, memorize significant</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) are known to memorize significant portions of
their training data. Parts of this memorized content have been shown to be
extractable by simply querying the model, which poses a privacy risk. We
present a novel approach which uses prompt-tuning to control the extraction
rates of memorized content in LLMs. We present two prompt training strategies
to increase and decrease extraction rates, which correspond to an attack and a
defense, respectively. We demonstrate the effectiveness of our techniques by
using models from the GPT-Neo family on a public benchmark. For the 1.3B
parameter GPT-Neo model, our attack yields a 9.3 percentage point increase in
extraction rate compared to our baseline. Our defense can be tuned to achieve
different privacy-utility trade-offs by a user-specified hyperparameter. We
achieve an extraction rate reduction of up to 97.7% relative to our baseline,
with a perplexity increase of 16.9%.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large  Language Models</b></summary>
  <p><b>编号</b>：[51]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11747</p>
  <p><b>作者</b>：Junyi Li,  Xiaoxue Cheng,  Wayne Xin Zhao,  Jian-Yun Nie,  Ji-Rong Wen</p>
  <p><b>备注</b>：Working in progress</p>
  <p><b>关键词</b>：Large language models, language models, Large language, generate hallucinations, hallucinations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs), such as ChatGPT, are prone to generate
hallucinations, \ie content that conflicts with the source or cannot be
verified by the factual knowledge. To understand what types of content and to
which extent LLMs are apt to hallucinate, we introduce the Hallucination
Evaluation for Large Language Models (HELMA) benchmark, a large collection of
generated and human-annotated hallucinated samples for evaluating the
performance of LLMs in recognizing and alleviating hallucination. To generate
these samples, we propose a ChatGPT-based two-step framework, \ie
sampling-then-filtering. Specifically, we first adopt two different sampling
methods to generate hallucinated samples based on instructions, and then use an
example-enhanced filtering method to select the best one. Furthermore, we also
hire some human labelers to annotate the hallucinations in ChatGPT responses.
The empirical results suggest that ChatGPT has some probabilities to generate
hallucinations and existing LLMs face great challenges in recognizing the
hallucinations in text. In addition, the performance can be improved by
providing external knowledge or adding reasoning steps. Our benchmark can be
accessed at this https URL.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：HalOmi: A Manually Annotated Benchmark for Multilingual Hallucination  and Omission Detection in Machine Translation</b></summary>
  <p><b>编号</b>：[52]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11746</p>
  <p><b>作者</b>：David Dale,  Elena Voita,  Janice Lam,  Prangthip Hansanti,  Christophe Ropers,  Elahe Kalbassi,  Cynthia Gao,  Loïc Barrault,  Marta R. Costa-jussà</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：information completely unrelated, completely unrelated, information completely, machine translation, input information</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Hallucinations in machine translation are translations that contain
information completely unrelated to the input. Omissions are translations that
do not include some of the input information. While both cases tend to be
catastrophic errors undermining user trust, annotated data with these types of
pathologies is extremely scarce and is limited to a few high-resource
languages. In this work, we release an annotated dataset for the hallucination
and omission phenomena covering 18 translation directions with varying resource
levels and scripts. Our annotation covers different levels of partial and full
hallucinations as well as omissions both at the sentence and at the word level.
Additionally, we revisit previous methods for hallucination and omission
detection, show that conclusions made based on a single language pair largely
do not hold for a large-scale evaluation, and establish new solid baselines.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Inference-time Re-ranker Relevance Feedback for Neural Information  Retrieval</b></summary>
  <p><b>编号</b>：[53]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11744</p>
  <p><b>作者</b>：Revanth Gangi Reddy,  Pradeep Dasigi,  Md Arafat Sultan,  Arman Cohan,  Avirup Sil,  Heng Ji,  Hannaneh Hajishirzi</p>
  <p><b>备注</b>：Preprint</p>
  <p><b>关键词</b>：powerful cross-encoder model, Neural information retrieval, Neural information, bi-encoder network, network first retrieves</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural information retrieval often adopts a retrieve-and-rerank framework: a
bi-encoder network first retrieves K (e.g., 100) candidates that are then
re-ranked using a more powerful cross-encoder model to rank the better
candidates higher. The re-ranker generally produces better candidate scores
than the retriever, but is limited to seeing only the top K retrieved
candidates, thus providing no improvements in retrieval performance as measured
by Recall@K. In this work, we leverage the re-ranker to also improve retrieval
by providing inference-time relevance feedback to the retriever. Concretely, we
update the retriever's query representation for a test instance using a
lightweight inference-time distillation of the re-ranker's prediction for that
instance. The distillation loss is designed to bring the retriever's candidate
scores closer to those of the re-ranker. A second retrieval step is then
performed with the updated query vector. We empirically show that our approach,
which can serve arbitrary retrieve-and-rerank pipelines, significantly improves
retrieval recall in multiple domains, languages, and modalities.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：CRITIC: Large Language Models Can Self-Correct with Tool-Interactive  Critiquing</b></summary>
  <p><b>编号</b>：[57]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11738</p>
  <p><b>作者</b>：Zhibin Gou,  Zhihong Shao,  Yeyun Gong,  Yelong Shen,  Yujiu Yang,  Nan Duan,  Weizhu Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, Recent developments, developments in large, large language, language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent developments in large language models (LLMs) have been impressive.
However, these models sometimes show inconsistencies and problematic behavior,
such as hallucinating facts, generating flawed code, or creating offensive and
toxic content. Unlike these models, humans typically utilize external tools to
cross-check and refine their initial content, like using a search engine for
fact-checking, or a code interpreter for debugging. Inspired by this
observation, we introduce a framework called CRITIC that allows LLMs, which are
essentially "black boxes" to validate and progressively amend their own outputs
in a manner similar to human interaction with tools. More specifically,
starting with an initial output, CRITIC interacts with appropriate tools to
evaluate certain aspects of the text, and then revises the output based on the
feedback obtained during this validation process. Comprehensive evaluations
involving free-form question answering, mathematical program synthesis, and
toxicity reduction demonstrate that CRITIC consistently enhances the
performance of LLMs. Meanwhile, our research highlights the crucial importance
of external feedback in promoting the ongoing self-improvement of LLMs.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Persian Typographical Error Type Detection using Many-to-Many Deep  Neural Networks on Algorithmically-Generated Misspellings</b></summary>
  <p><b>编号</b>：[60]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11731</p>
  <p><b>作者</b>：Mohammad Dehghani,  Heshaam Faili</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：text created daily, Digital technologies, technologies have led, created daily, text created</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Digital technologies have led to an influx of text created daily in a variety
of languages, styles, and formats. A great deal of the popularity of
spell-checking systems can be attributed to this phenomenon since they are
crucial to polishing the digitally conceived text. In this study, we tackle
Typographical Error Type Detection in Persian, which has been relatively
understudied. In this paper, we present a public dataset named FarsTypo,
containing 3.4 million chronologically ordered and part-of-speech tagged words
of diverse topics and linguistic styles. An algorithm for applying
Persian-specific errors is developed and applied to a scalable size of these
words, forming a parallel dataset of correct and incorrect words. Using
FarsTypo, we establish a firm baseline and compare different methodologies
using various architectures. In addition, we present a novel Many-to-Many Deep
Sequential Neural Network to perform token classification using both word and
character embeddings in combination with bidirectional LSTM layers to detect
typographical errors across 51 classes. We compare our approach with
highly-advanced industrial systems that, unlike this study, have been developed
utilizing a variety of resources. The results of our final method were
competitive in that we achieved an accuracy of 97.62%, a precision of 98.83%, a
recall of 98.61%, and outperformed the rest in terms of speed.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：S$^3$HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid  Question Answering</b></summary>
  <p><b>编号</b>：[64]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11725</p>
  <p><b>作者</b>：Fangyu Lei,  Xiang Li,  Yifan Wei,  Shizhu He,  Yiming Huang,  Jun Zhao,  Kang Liu</p>
  <p><b>备注</b>：ACL 2023</p>
  <p><b>关键词</b>：Answering multi-hop questions, text and table, Answering multi-hop, multi-hop questions, challenging task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Answering multi-hop questions over hybrid factual knowledge from the given
text and table (TextTableQA) is a challenging task. Existing models mainly
adopt a retriever-reader framework, which have several deficiencies, such as
noisy labeling in training retriever, insufficient utilization of heterogeneous
information over text and table, and deficient ability for different reasoning
operations. In this paper, we propose a three-stage TextTableQA framework
S3HQA, which comprises of retriever, selector, and reasoner. We use a retriever
with refinement training to solve the noisy labeling problem. Then, a hybrid
selector considers the linked relationships between heterogeneous data to
select the most relevant factual knowledge. For the final stage, instead of
adapting a reading comprehension module like in previous methods, we employ a
generation-based reasoner to obtain answers. This includes two approaches: a
row-wise generator and an LLM prompting generator~(first time used in this
task). The experimental results demonstrate that our method achieves
competitive results in the few-shot setting. When trained on the full dataset,
our approach outperforms all baseline methods, ranking first on the HybridQA
leaderboard.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Information Screening whilst Exploiting! Multimodal Relation Extraction  with Feature Denoising and Multimodal Topic Modeling</b></summary>
  <p><b>编号</b>：[65]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11719</p>
  <p><b>作者</b>：Shengqiong Wu,  Hao Fei,  Yixin Cao,  Lidong Bing,  Tat-Seng Chua</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：multimodal relation extraction, Existing research, relation extraction, faces two co-existing, co-existing challenges</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing research on multimodal relation extraction (MRE) faces two
co-existing challenges, internal-information over-utilization and
external-information under-exploitation. To combat that, we propose a novel
framework that simultaneously implements the idea of internal-information
screening and external-information exploiting. First, we represent the
fine-grained semantic structures of the input image and text with the visual
and textual scene graphs, which are further fused into a unified cross-modal
graph (CMG). Based on CMG, we perform structure refinement with the guidance of
the graph information bottleneck principle, actively denoising the
less-informative features. Next, we perform topic modeling over the input image
and text, incorporating latent multimodal topic features to enrich the
contexts. On the benchmark MRE dataset, our system outperforms the current best
model significantly. With further in-depth analyses, we reveal the great
potential of our method for the MRE task. Our codes are open at
this https URL.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：What Comes Next? Evaluating Uncertainty in Neural Text Generators  Against Human Production Variability</b></summary>
  <p><b>编号</b>：[70]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11707</p>
  <p><b>作者</b>：Mario Giulianelli,  Joris Baan,  Wilker Aziz,  Raquel Fernández,  Barbara Plank</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Natural Language Generation, Natural Language, Language Generation, multiple communicative goals, put into words</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In Natural Language Generation (NLG) tasks, for any input, multiple
communicative goals are plausible, and any goal can be put into words, or
produced, in multiple ways. We characterise the extent to which human
production varies lexically, syntactically, and semantically across four NLG
tasks, connecting human production variability to aleatoric or data
uncertainty. We then inspect the space of output strings shaped by a generation
system's predicted probability distribution and decoding algorithm to probe its
uncertainty. For each test input, we measure the generator's calibration to
human production variability. Following this instance-level approach, we
analyse NLG models and decoding strategies, demonstrating that probing a
generator with multiple samples and, when possible, multiple references,
provides the level of detail necessary to gain understanding of a model's
representation of uncertainty.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set  Operations</b></summary>
  <p><b>编号</b>：[74]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11694</p>
  <p><b>作者</b>：Chaitanya Malaviya,  Peter Shaw,  Ming-Wei Chang,  Kenton Lee,  Kristina Toutanova</p>
  <p><b>备注</b>：ACL 2023; Dataset available at this https URL</p>
  <p><b>关键词</b>：Formulating selective information, Formulating selective, set operations, operations, queries</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Formulating selective information needs results in queries that implicitly
specify set operations, such as intersection, union, and difference. For
instance, one might search for "shorebirds that are not sandpipers" or
"science-fiction films shot in England". To study the ability of retrieval
systems to meet such information needs, we construct QUEST, a dataset of 3357
natural language queries with implicit set operations, that map to a set of
entities corresponding to Wikipedia documents. The dataset challenges models to
match multiple constraints mentioned in queries with corresponding evidence in
documents and correctly perform various set operations. The dataset is
constructed semi-automatically using Wikipedia category names. Queries are
automatically composed from individual categories, then paraphrased and further
validated for naturalness and fluency by crowdworkers. Crowdworkers also assess
the relevance of entities based on their documents and highlight attribution of
query constraints to spans of document text. We analyze several modern
retrieval systems, finding that they often struggle on such queries. Queries
involving negation and conjunction are particularly challenging and systems are
further challenged with combinations of these operations.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Surgical-VQLA: Transformer with Gated Vision-Language Embedding for  Visual Question Localized-Answering in Robotic Surgery</b></summary>
  <p><b>编号</b>：[75]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11692</p>
  <p><b>作者</b>：Long Bai,  Mobarakol Islam,  Lalithkumar Seenivasan,  Hongliang Ren</p>
  <p><b>备注</b>：To appear in IEEE ICRA 2023. Code and data availability: this https URL</p>
  <p><b>关键词</b>：junior residents, availability of computer-aided, computer-aided simulators, residents still heavily, heavily rely</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite the availability of computer-aided simulators and recorded videos of
surgical procedures, junior residents still heavily rely on experts to answer
their queries. However, expert surgeons are often overloaded with clinical and
academic workloads and limit their time in answering. For this purpose, we
develop a surgical question-answering system to facilitate robot-assisted
surgical scene and activity understanding from recorded videos. Most of the
existing VQA methods require an object detector and regions based feature
extractor to extract visual features and fuse them with the embedded text of
the question for answer generation. However, (1) surgical object detection
model is scarce due to smaller datasets and lack of bounding box annotation;
(2) current fusion strategy of heterogeneous modalities like text and image is
naive; (3) the localized answering is missing, which is crucial in complex
surgical scenarios. In this paper, we propose Visual Question
Localized-Answering in Robotic Surgery (Surgical-VQLA) to localize the specific
surgical area during the answer prediction. To deal with the fusion of the
heterogeneous modalities, we design gated vision-language embedding (GVLE) to
build input patches for the Language Vision Transformer (LViT) to predict the
answer. To get localization, we add the detection head in parallel with the
prediction head of the LViT. We also integrate GIoU loss to boost localization
performance by preserving the accuracy of the question-answering model. We
annotate two datasets of VQLA by utilizing publicly available surgical videos
from MICCAI challenges EndoVis-17 and 18. Our validation results suggest that
Surgical-VQLA can better understand the surgical scene and localize the
specific area related to the question-answering. GVLE presents an efficient
language-vision embedding technique by showing superior performance over the
existing benchmarks.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Sensing of inspiration events from speech: comparison of deep learning  and linguistic methods</b></summary>
  <p><b>编号</b>：[77]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11683</p>
  <p><b>作者</b>：Aki Härmä,  Ulf Grossekathöfer,  Okke Ouweltjes,  Venkata Srikanth Nallanthighal</p>
  <p><b>备注</b>：8 pages</p>
  <p><b>关键词</b>：respiratory health parameters, Respiratory chest belt, chest belt sensor, respiratory belt sensor, belt sensor</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Respiratory chest belt sensor can be used to measure the respiratory rate and
other respiratory health parameters. Virtual Respiratory Belt, VRB, algorithms
estimate the belt sensor waveform from speech audio. In this paper we compare
the detection of inspiration events (IE) from respiratory belt sensor data
using a novel neural VRB algorithm and the detections based on time-aligned
linguistic content. The results show the superiority of the VRB method over
word pause detection or grammatical content segmentation. The comparison of the
methods show that both read and spontaneous speech content has a significant
amount of ungrammatical breathing, that is, breathing events that are not
aligned with grammatically appropriate places in language. This study gives new
insights into the development of VRB methods and adds to the general
understanding of speech breathing behavior. Moreover, a new VRB method, VRBOLA,
for the reconstruction of the continuous breathing waveform is demonstrated.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Bias Beyond English: Counterfactual Tests for Bias in Sentiment Analysis  in Four Languages</b></summary>
  <p><b>编号</b>：[83]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11673</p>
  <p><b>作者</b>：Seraphina Goldfarb-Tarrant,  Adam Lopez,  Roi Blanco,  Diego Marcheggiani</p>
  <p><b>备注</b>：5 pages, accepted to Findings of ACL 2023</p>
  <p><b>关键词</b>：Sentiment analysis, products and hundreds, languages, Gender and racial, well-studied in English</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sentiment analysis (SA) systems are used in many products and hundreds of
languages. Gender and racial biases are well-studied in English SA systems, but
understudied in other languages, with few resources for such studies. To remedy
this, we build a counterfactual evaluation corpus for gender and racial/migrant
bias in four languages. We demonstrate its usefulness by answering a simple but
important question that an engineer might need to answer when deploying a
system: What biases do systems import from pre-trained models when compared to
a baseline with no pre-training? Our evaluation corpus, by virtue of being
counterfactual, not only reveals which models have less bias, but also
pinpoints changes in model bias behaviour, which enables more targeted
mitigation strategies. We release our code and evaluation corpora to facilitate
future research.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Algorithmic failure as a humanities methodology: machine learning's  mispredictions identify rich cases for qualitative analysis</b></summary>
  <p><b>编号</b>：[87]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11663</p>
  <p><b>作者</b>：Jill Walker Rettberg</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：machine learning, methodology proposed, machine learning algorithm, machine vision technologies, machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This commentary tests a methodology proposed by Munk et al. (2022) for using
failed predictions in machine learning as a method to identify ambiguous and
rich cases for qualitative analysis. Using a dataset describing actions
performed by fictional characters interacting with machine vision technologies
in 500 artworks, movies, novels and videogames, I trained a simple machine
learning algorithm (using the kNN algorithm in R) to predict whether or not an
action was active or passive using only information about the fictional
characters. Predictable actions were generally unemotional and unambiguous
activities where machine vision technologies were treated as simple tools.
Unpredictable actions, that is, actions that the algorithm could not correctly
predict, were more ambivalent and emotionally loaded, with more complex power
relationships between characters and technologies. The results thus support
Munk et al.'s theory that failed predictions can be productively used to
identify rich cases for qualitative analysis. This test goes beyond simply
replicating Munk et al.'s results by demonstrating that the method can be
applied to a broader humanities domain, and that it does not require complex
neural networks but can also work with a simpler machine learning algorithm.
Further research is needed to develop an understanding of what kinds of data
the method is useful for and which kinds of machine learning are most
generative. To support this, the R code required to produce the results is
included so the test can be replicated. The code can also be reused or adapted
to test the method on other datasets.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Evaluating task understanding through multilingual consistency: A  ChatGPT case study</b></summary>
  <p><b>编号</b>：[88]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11662</p>
  <p><b>作者</b>：Xenia Ohmer,  Elia Bruni,  Dieuwke Hupkes</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：creating future-proof evaluation, future-proof evaluation sets, creating future-proof, staggering pace, capabilities of large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>At the staggering pace with which the capabilities of large language models
(LLMs) are increasing, creating future-proof evaluation sets to assess their
understanding becomes more and more challenging. In this paper, we propose a
novel paradigm for evaluating LLMs which leverages the idea that correct world
understanding should be consistent across different (Fregean) senses of the
same meaning. Accordingly, we measure understanding not in terms of correctness
but by evaluating consistency across multiple senses that are generated by the
model itself. We showcase our approach by instantiating a test where the
different senses are different languages, hence using multilingual
self-consistency as a litmus test for the model's understanding and
simultaneously addressing the important topic of multilingualism. Taking one of
the latest versions of ChatGPT as our object of study, we evaluate multilingual
consistency for two different tasks across three different languages. We show
that its multilingual consistency is still lacking, and that its task and world
understanding are thus not language-independent. As our approach does not
require any static evaluation corpora in languages other than English, it can
easily and cheaply be extended to different languages and tasks and could
become an integral part of future benchmarking efforts.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：LLM-Pruner: On the Structural Pruning of Large Language Models</b></summary>
  <p><b>编号</b>：[105]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11627</p>
  <p><b>作者</b>：Xinyin Ma,  Gongfan Fang,  Xinchao Wang</p>
  <p><b>备注</b>：Technical Report; Work In Progress</p>
  <p><b>关键词</b>：shown remarkable capabilities, Large language models, shown remarkable, Large language, language understanding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have shown remarkable capabilities in language
understanding and generation. However, such impressive capability typically
comes with a substantial model size, which presents significant challenges in
both the deployment, inference, and training stages. With LLM being a
general-purpose task solver, we explore its compression in a task-agnostic
manner, which aims to preserve the multi-task solving and language generation
ability of the original LLM. One challenge to achieving this is the enormous
size of the training corpus of LLM, which makes both data transfer and model
post-training over-burdensome. Thus, we tackle the compression of LLMs within
the bound of two constraints: being task-agnostic and minimizing the reliance
on the original training dataset. Our method, named LLM-Pruner, adopts
structural pruning that selectively removes non-critical coupled structures
based on gradient information, maximally preserving the majority of the LLM's
functionality. To this end, the performance of pruned models can be efficiently
recovered through tuning techniques, LoRA, in merely 3 hours, requiring only
50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna,
and ChatGLM, and demonstrate that the compressed models still exhibit
satisfactory capabilities in zero-shot classification and generation. The code
is available at: this https URL</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：CCT-Code: Cross-Consistency Training for Multilingual Clone Detection  and Code Search</b></summary>
  <p><b>编号</b>：[106]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11626</p>
  <p><b>作者</b>：Nikita Sorokin,  Dmitry Abulkhanov,  Sergey Nikolenko,  Valentin Malykh</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：multilingual clone detection, well-known tasks important, clone detection, information retrieval problems, clone detection benchmark</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider the clone detection and information retrieval problems for source
code, well-known tasks important for any programming language. Although it is
also an important and interesting problem to find code snippets that operate
identically but are written in different programming languages, to the best of
our knowledge multilingual clone detection has not been studied in literature.
In this work, we formulate the multilingual clone detection problem and present
XCD, a new benchmark dataset produced from the CodeForces submissions dataset.
Moreover, we present a novel training procedure, called cross-consistency
training (CCT), that we apply to train language models on source code in
different programming languages. The resulting CCT-LM model, initialized with
GraphCodeBERT and fine-tuned with CCT, achieves new state of the art,
outperforming existing approaches on the POJ-104 clone detection benchmark with
95.67\% MAP and AdvTest code search benchmark with 47.18\% MRR; it also shows
the best results on the newly created multilingual clone detection benchmark
XCD across all programming languages.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Searching by Code: a New SearchBySnippet Dataset and SnippeR Retrieval  Model for Searching by Code Snippets</b></summary>
  <p><b>编号</b>：[107]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11625</p>
  <p><b>作者</b>：Ivan Sedykh,  Dmitry Abulkhanov,  Nikita Sorokin,  Sergey Nikolenko,  Valentin Malykh</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：recent years, developments in recent, Code, SearchBySnippet dataset, important task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Code search is an important task that has seen many developments in recent
years. However, previous attempts have mostly considered the problem of
searching for code by a text query. We argue that using a code snippet (and
possibly an associated traceback) as a query and looking for answers with
bugfixing instructions and code samples is a natural use case that is not
covered by existing approaches. Moreover, existing datasets use comments
extracted from code rather than full-text descriptions as text, making them
unsuitable for this use case. We present a new SearchBySnippet dataset
implementing the search-by-code use case based on StackOverflow data; it turns
out that in this setting, existing architectures fall short of the simplest
BM25 baseline even after fine-tuning. We present a new single encoder model
SnippeR that outperforms several strong baselines on the SearchBySnippet
dataset with a result of 0.451 Recall@10; we propose the SearchBySnippet
dataset and SnippeR as a new important benchmark for code search evaluation.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Attributable and Scalable Opinion Summarization</b></summary>
  <p><b>编号</b>：[114]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11603</p>
  <p><b>作者</b>：Tom Hosking,  Hao Tang,  Mirella Lapata</p>
  <p><b>备注</b>：ACL 2023</p>
  <p><b>关键词</b>：common opinions based, unsupervised opinion summarization, identifies common opinions, hierarchical discrete latent, unsupervised opinion</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a method for unsupervised opinion summarization that encodes
sentences from customer reviews into a hierarchical discrete latent space, then
identifies common opinions based on the frequency of their encodings. We are
able to generate both abstractive summaries by decoding these frequent
encodings, and extractive summaries by selecting the sentences assigned to the
same frequent encodings. Our method is attributable, because the model
identifies sentences used to generate the summary as part of the summarization
process. It scales easily to many hundreds of input reviews, because
aggregation is performed in the latent space rather than over long sequences of
tokens. We also demonstrate that our appraoch enables a degree of control,
generating aspect-specific summaries by restricting the model to parts of the
encoding space that correspond to desired aspects (e.g., location or food).
Automatic and human evaluation on two datasets from different domains
demonstrates that our method generates summaries that are more informative than
prior work and better grounded in the input reviews.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Introspective Tips: Large Language Model for In-Context Decision Making</b></summary>
  <p><b>编号</b>：[117]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11598</p>
  <p><b>作者</b>：Liting Chen,  Lu Wang,  Hang Dong,  Yali Du,  Jie Yan,  Fangkai Yang,  Shuang Li,  Pu Zhao,  Si Qin,  Saravan Rajmohan,  Qingwei Lin,  Dongmei Zhang</p>
  <p><b>备注</b>：22 pages, 4 figures</p>
  <p><b>关键词</b>：large language models, natural language processing, influenced natural language, demonstrating exceptional results, substantially influenced natural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The emergence of large language models (LLMs) has substantially influenced
natural language processing, demonstrating exceptional results across various
tasks. In this study, we employ ``Introspective Tips" to facilitate LLMs in
self-optimizing their decision-making. By introspectively examining
trajectories, LLM refines its policy by generating succinct and valuable tips.
Our method enhances the agent's performance in both few-shot and zero-shot
learning situations by considering three essential scenarios: learning from the
agent's past experiences, integrating expert demonstrations, and generalizing
across diverse games. Importantly, we accomplish these improvements without
fine-tuning the LLM parameters; rather, we adjust the prompt to generalize
insights from the three aforementioned situations. Our framework not only
supports but also emphasizes the advantage of employing LLM in in-contxt
decision-making. Experiments involving over 100 games in TextWorld illustrate
the superior performance of our approach.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：Mitigating Backdoor Poisoning Attacks through the Lens of Spurious  Correlation</b></summary>
  <p><b>编号</b>：[119]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11596</p>
  <p><b>作者</b>：Xuanli He,  Qiongkai Xu,  Jun Wang,  Benjamin Rubinstein,  Trevor Cohn</p>
  <p><b>备注</b>：14 pages, 4 figures</p>
  <p><b>关键词</b>：Modern NLP models, compromise model behaviour, large untrusted datasets, Modern NLP, NLP models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Modern NLP models are often trained over large untrusted datasets, raising
the potential for a malicious adversary to compromise model behaviour. For
instance, backdoors can be implanted through crafting training instances with a
specific textual trigger and a target label. This paper posits that backdoor
poisoning attacks exhibit spurious correlation between simple text features and
classification labels, and accordingly, proposes methods for mitigating
spurious correlation as means of defence. Our empirical study reveals that the
malicious triggers are highly correlated to their target labels; therefore such
correlations are extremely distinguishable compared to those scores of benign
features, and can be used to filter out potentially problematic instances.
Compared with several existing defences, our defence method significantly
reduces attack success rates across backdoor attacks, and in the case of
insertion based attacks, our method provides a near-perfect defence.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Diving into the Inter-Consistency of Large Language Models: An  Insightful Analysis through Debate</b></summary>
  <p><b>编号</b>：[120]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11595</p>
  <p><b>作者</b>：Kai Xiong,  Xiao Ding,  Yixin Cao,  Ting Liu,  Bing Qin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large language models, natural language processing, demonstrated impressive zero-shot, few-shot commonsense reasoning, Large language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have demonstrated impressive zero-shot or
few-shot commonsense reasoning performance on various natural language
processing (NLP) tasks. However, despite their strong commonsense reasoning
abilities, LLMs still exhibit various kinds of inconsistency problems. While
previous researches mainly focused on the self-consistency within a single LLM,
we propose to explore the inter-consistency issue between two or more LLMs,
which is critical for diverse and precise decision-making processes. Since the
LLMs possess human-like intelligence after instruction tuning and reinforcement
learning with human feedback (RLHF), we design a formal debate framework to
delve into the inter-consistency problem among LLMs with three-stage debate:
fair debate, mismatched debate, and roundtable debate. Through extensive
experiments on 7 commonsense reasoning datasets, LLMs not only become more
inter-consistent by compromising and refuting but also achieve higher
performance and stronger interpretability. Furthermore, we find a much stronger
LLM would be dominant in mismatched debates, while it will be easily misled by
relatively weaker LLMs in a more complex debate scenario such as roundtable
debate.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：IKDSumm: Incorporating Key-phrases into BERT for extractive Disaster  Tweet Summarization</b></summary>
  <p><b>编号</b>：[121]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11592</p>
  <p><b>作者</b>：Piyush Kumar Garg,  Roshni Chakraborty,  Srishti Gupta,  Sourav Kumar Dandapat</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Online social media, social media platforms, Online social, media platforms, social media</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Online social media platforms, such as Twitter, are one of the most valuable
sources of information during disaster events. Therefore, humanitarian
organizations, government agencies, and volunteers rely on a summary of this
information, i.e., tweets, for effective disaster management. Although there
are several existing supervised and unsupervised approaches for automated tweet
summary approaches, these approaches either require extensive labeled
information or do not incorporate specific domain knowledge of disasters.
Additionally, the most recent approaches to disaster summarization have
proposed BERT-based models to enhance the summary quality. However, for further
improved performance, we introduce the utilization of domain-specific knowledge
without any human efforts to understand the importance (salience) of a tweet
which further aids in summary creation and improves summary quality. In this
paper, we propose a disaster-specific tweet summarization framework, IKDSumm,
which initially identifies the crucial and important information from each
tweet related to a disaster through key-phrases of that tweet. We identify
these key-phrases by utilizing the domain knowledge (using existing ontology)
of disasters without any human intervention. Further, we utilize these
key-phrases to automatically generate a summary of the tweets. Therefore, given
tweets related to a disaster, IKDSumm ensures fulfillment of the summarization
key objectives, such as information coverage, relevance, and diversity in
summary without any human intervention. We evaluate the performance of IKDSumm
with 8 state-of-the-art techniques on 12 disaster datasets. The evaluation
results show that IKDSumm outperforms existing techniques by approximately
2-79% in terms of ROUGE-N F1-score.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Speech-Text Dialog Pre-training for Spoken Dialog Understanding with  Explicit Cross-Modal Alignment</b></summary>
  <p><b>编号</b>：[131]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11579</p>
  <p><b>作者</b>：Tianshu Yu,  Haoyu Gao,  Ting-En Lin,  Min Yang,  Yuchuan Wu,  Wentao Ma,  Chao Wang,  Fei Huang,  Yongbin Li</p>
  <p><b>备注</b>：Accepted at ACL 2023 main conference</p>
  <p><b>关键词</b>：shown remarkable success, natural language processing, language processing tasks, speech-text, pre-training</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, speech-text pre-training methods have shown remarkable success in
many speech and natural language processing tasks. However, most previous
pre-trained models are usually tailored for one or two specific tasks, but fail
to conquer a wide range of speech-text tasks. In addition, existing speech-text
pre-training methods fail to explore the contextual information within a
dialogue to enrich utterance representations. In this paper, we propose
Speech-text dialog Pre-training for spoken dialog understanding with ExpliCiT
cRoss-Modal Alignment (SPECTRA), which is the first-ever speech-text dialog
pre-training model. Concretely, to consider the temporality of speech modality,
we design a novel temporal position prediction task to capture the speech-text
alignment. This pre-training task aims to predict the start and end time of
each textual word in the corresponding speech waveform. In addition, to learn
the characteristics of spoken dialogs, we generalize a response selection task
from textual dialog pre-training to speech-text dialog pre-training scenarios.
Experimental results on four different downstream speech-text tasks demonstrate
the superiority of SPECTRA in learning speech-text alignment and multi-turn
dialog context.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Decouple knowledge from paramters for plug-and-play language modeling</b></summary>
  <p><b>编号</b>：[136]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11564</p>
  <p><b>作者</b>：Xin Cheng,  Yankai Lin,  Xiuying Chen,  Dongyan Zhao,  Rui Yan</p>
  <p><b>备注</b>：ACL2023 Findings</p>
  <p><b>关键词</b>：made impressive results, NLP tasks, Pre-trained language models, knowledge, Pre-trained language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pre-trained language models(PLM) have made impressive results in various NLP
tasks. It has been revealed that one of the key factors to their success is the
parameters of these models implicitly learn all kinds of knowledge during
pre-training. However, encoding knowledge implicitly in the model parameters
has two fundamental drawbacks. First, the knowledge is neither editable nor
scalable once the model is trained, which is especially problematic in that
knowledge is consistently evolving. Second, it lacks interpretability and
prevents humans from understanding which knowledge PLM requires for a certain
problem. In this paper, we introduce PlugLM, a pre-training model with
differentiable plug-in memory(DPM). The key intuition is to decouple the
knowledge storage from model parameters with an editable and scalable key-value
memory and leverage knowledge in an explainable manner by knowledge retrieval
in the DPM. To justify this design choice, we conduct evaluations in three
settings including: (1) domain adaptation. PlugLM obtains 3.95 F1 improvements
across four domains on average without any in-domain pre-training. (2)
knowledge update. PlugLM could absorb new knowledge in a training-free way
after pre-training is done. (3) in-task knowledge learning. PlugLM could be
further improved by incorporating training samples into DPM with knowledge
prompting.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via  Tool Embeddings</b></summary>
  <p><b>编号</b>：[140]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11554</p>
  <p><b>作者</b>：Shibo Hao,  Tianyang Liu,  Zhen Wang,  Zhiting Hu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Augmenting large language, large language models, solving complex problems, language models, tools</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Augmenting large language models (LLMs) with external tools has emerged as a
promising approach to solving complex problems. However, traditional methods,
which finetune LLMs with tool demonstration data, can be both costly and
restricted to a predefined set of tools. Recent in-context learning paradigm
alleviates these issues, but the limited context length only allows for a few
shots of demonstrations, leading to suboptimal understandings of the tools.
Moreover, when there are numerous tools to choose from, in-context learning
could completely fail to work. In this paper, we propose an alternative
approach, $\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our
approach represents each $\underline{tool}$ as a to$\underline{ken}$
($\textit{toolken}$) and learns an embedding for it, enabling tool calls in the
same way as generating a regular word token. Once a toolken is triggered, the
LLM is prompted to complete arguments for the tool to execute. ToolkenGPT
offers the flexibility to plug in an arbitrary number of tools by expanding the
set of toolkens on the fly. In addition, it improves tool use by allowing
extensive demonstration data for learning the toolken embeddings. In diverse
domains, including numerical reasoning, knowledge-based question answering, and
embodied plan generation, our approach effectively augments LLMs with tools and
substantially outperforms various latest baselines. ToolkenGPT demonstrates the
promising ability to use relevant tools from a large tool set in complex
scenarios.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Unsupervised Scientific Abstract Segmentation with Normalized Mutual  Information</b></summary>
  <p><b>编号</b>：[141]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11553</p>
  <p><b>作者</b>：Yingqiang Gao,  Jessica Lam,  Nianlong Gu,  Richard H.R. Hahnloser</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：scientific papers consist, Normalized Mutual Information, papers consist, NMI, abstracts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The abstracts of scientific papers consist of premises and conclusions.
Structured abstracts explicitly highlight the conclusion sentences, whereas
non-structured abstracts may have conclusion sentences at uncertain positions.
This implicit nature of conclusion positions makes the automatic segmentation
of scientific abstracts into premises and conclusions a challenging task. In
this work, we empirically explore using Normalized Mutual Information (NMI) for
abstract segmentation. We consider each abstract as a recurrent cycle of
sentences and place segmentation boundaries by greedily optimizing the NMI
score between premises and conclusions. On non-structured abstracts, our
proposed unsupervised approach GreedyCAS achieves the best performance across
all evaluation metrics; on structured abstracts, GreedyCAS outperforms all
baseline methods measured by $P_k$. The strong correlation of NMI to our
evaluation metrics reveals the effectiveness of NMI for abstract segmentation.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Viewing Knowledge Transfer in Multilingual Machine Translation Through a  Representational Lens</b></summary>
  <p><b>编号</b>：[143]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11550</p>
  <p><b>作者</b>：David Stap,  Vlad Niculae,  Christof Monz</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：multilingual neural machine, measuring knowledge transfer, neural machine translation, Representational Transfer Potential, sufficient metric</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We argue that translation quality alone is not a sufficient metric for
measuring knowledge transfer in multilingual neural machine translation. To
support this claim, we introduce Representational Transfer Potential (RTP),
which measures representational similarities between languages. We show that
RTP can measure both positive and negative transfer (interference), and find
that RTP is strongly correlated with changes in translation quality, indicating
that transfer does occur. Furthermore, we investigate data and language
characteristics that are relevant for transfer, and find that multi-parallel
overlap is an important yet under-explored feature. Based on this, we develop a
novel training scheme, which uses an auxiliary similarity loss that encourages
representations to be more invariant across languages by taking advantage of
multi-parallel data. We show that our method yields increased translation
quality for low- and mid-resource languages across multiple data and model
setups.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Constructing Word-Context-Coupled Space Aligned with Associative  Knowledge Relations for Interpretable Language Modeling</b></summary>
  <p><b>编号</b>：[146]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11543</p>
  <p><b>作者</b>：Fanyu Wang,  Zhenping Xie</p>
  <p><b>备注</b>：Accepted at ACL 2023, Findings</p>
  <p><b>关键词</b>：natural language processing, achieved excellent performance, current natural language, pre-trained language model, pre-trained language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As the foundation of current natural language processing methods, pre-trained
language model has achieved excellent performance. However, the black-box
structure of the deep neural network in pre-trained language models seriously
limits the interpretability of the language modeling process. After revisiting
the coupled requirement of deep neural representation and semantics logic of
language modeling, a Word-Context-Coupled Space (W2CSpace) is proposed by
introducing the alignment processing between uninterpretable neural
representation and interpretable statistical logic. Moreover, a clustering
process is also designed to connect the word- and context-level semantics.
Specifically, an associative knowledge network (AKN), considered interpretable
statistical logic, is introduced in the alignment process for word-level
semantics. Furthermore, the context-relative distance is employed as the
semantic feature for the downstream classifier, which is greatly different from
the current uninterpretable semantic representations of pre-trained models. Our
experiments for performance evaluation and interpretable analysis are executed
on several types of datasets, including SIGHAN, Weibo, and ChnSenti. Wherein a
novel evaluation strategy for the interpretability of machine learning models
is first proposed. According to the experimental results, our language model
can achieve better performance and highly credible interpretable ability
compared to related state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Empower Large Language Model to Perform Better on Industrial  Domain-Specific Question Answering</b></summary>
  <p><b>编号</b>：[147]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11541</p>
  <p><b>作者</b>：Zezhong Wang,  Fangkai Yang,  Pu Zhao,  Lu Wang,  Jue Zhang,  Mohit Garg,  Qingwei Lin,  Dongmei Zhang</p>
  <p><b>备注</b>：13 pages, 1 figure</p>
  <p><b>关键词</b>：Large Language Model, achieved remarkable results, Large Language, industrial domain-specific scenarios, real industrial domain-specific</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Model (LLM) has gained popularity and achieved remarkable
results in open-domain tasks, but its performance in real industrial
domain-specific scenarios is average since there is no specific knowledge in
it. This issue has attracted widespread attention, but there are few relevant
benchmarks available. In this paper, we provide a benchmark Question Answering
(QA) dataset named MSQA, which is about Microsoft products and IT technical
problems encountered by customers. This dataset contains industry
cloud-specific QA knowledge, which is not available for general LLM, so it is
well suited for evaluating methods aimed at improving domain-specific
capabilities of LLM. In addition, we propose a new model interaction paradigm
that can empower LLM to achieve better performance on domain-specific tasks
where it is not proficient. Extensive experiments demonstrate that the approach
following our model fusion framework outperforms the commonly used LLM with
retrieval methods.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Efficient Cross-Lingual Transfer for Chinese Stable Diffusion with  Images as Pivots</b></summary>
  <p><b>编号</b>：[148]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11540</p>
  <p><b>作者</b>：Jinyi Hu,  Xu Han,  Xiaoyuan Yi,  Yutong Chen,  Wenhao Li,  Zhiyuan Liu,  Maosong Sun</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：made impressive progress, English Stable Diffusion, Stable Diffusion, made impressive, impressive progress</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diffusion models have made impressive progress in text-to-image synthesis.
However, training such large-scale models (e.g. Stable Diffusion), from scratch
requires high computational costs and massive high-quality text-image pairs,
which becomes unaffordable in other languages. To handle this challenge, we
propose IAP, a simple but effective method to transfer English Stable Diffusion
into Chinese. IAP optimizes only a separate Chinese text encoder with all other
parameters fixed to align Chinese semantics space to the English one in CLIP.
To achieve this, we innovatively treat images as pivots and minimize the
distance of attentive features produced from cross-attention between images and
each language respectively. In this way, IAP establishes connections of
Chinese, English and visual semantics in CLIP's embedding space efficiently,
advancing the quality of the generated image with direct Chinese prompts.
Experimental results show that our method outperforms several strong Chinese
diffusion models with only 5%~10% training data.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：PORTRAIT: a hybrid aPproach tO cReate extractive ground-TRuth summAry  for dIsaster evenT</b></summary>
  <p><b>编号</b>：[150]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11536</p>
  <p><b>作者</b>：Piyush Kumar Garg,  Roshni Chakraborty,  Sourav Kumar Dandapat</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：social media platforms, Disaster summarization approaches, important information posted, ground-truth summary, ground-truth</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Disaster summarization approaches provide an overview of the important
information posted during disaster events on social media platforms, such as,
Twitter. However, the type of information posted significantly varies across
disasters depending on several factors like the location, type, severity, etc.
Verification of the effectiveness of disaster summarization approaches still
suffer due to the lack of availability of good spectrum of datasets along with
the ground-truth summary. Existing approaches for ground-truth summary
generation (ground-truth for extractive summarization) relies on the wisdom and
intuition of the annotators. Annotators are provided with a complete set of
input tweets from which a subset of tweets is selected by the annotators for
the summary. This process requires immense human effort and significant time.
Additionally, this intuition-based selection of the tweets might lead to a high
variance in summaries generated across annotators. Therefore, to handle these
challenges, we propose a hybrid (semi-automated) approach (PORTRAIT) where we
partly automate the ground-truth summary generation procedure. This approach
reduces the effort and time of the annotators while ensuring the quality of the
created ground-truth summary. We validate the effectiveness of PORTRAIT on 5
disaster events through quantitative and qualitative comparisons of
ground-truth summaries generated by existing intuitive approaches, a
semi-automated approach, and PORTRAIT. We prepare and release the ground-truth
summaries for 5 disaster events which consist of both natural and man-made
disaster events belonging to 4 different countries. Finally, we provide a study
about the performance of various state-of-the-art summarization approaches on
the ground-truth summaries generated by PORTRAIT using ROUGE-N F1-scores.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：A Sequence-to-Sequence Approach for Arabic Pronoun Resolution</b></summary>
  <p><b>编号</b>：[154]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11529</p>
  <p><b>作者</b>：Hanan S. Murayshid,  Hafida Benhidour,  Said Kerrache</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：BERT pre-trained Language, natural language processing, pre-trained Language Model, advanced natural language, BERT pre-trained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper proposes a sequence-to-sequence learning approach for Arabic
pronoun resolution, which explores the effectiveness of using advanced natural
language processing (NLP) techniques, specifically Bi-LSTM and the BERT
pre-trained Language Model, in solving the pronoun resolution problem in
Arabic. The proposed approach is evaluated on the AnATAr dataset, and its
performance is compared to several baseline models, including traditional
machine learning models and handcrafted feature-based models. Our results
demonstrate that the proposed model outperforms the baseline models, which
include KNN, logistic regression, and SVM, across all metrics. In addition, we
explore the effectiveness of various modifications to the model, including
concatenating the anaphor text beside the paragraph text as input, adding a
mask to focus on candidate scores, and filtering candidates based on gender and
number agreement with the anaphor. Our results show that these modifications
significantly improve the model's performance, achieving up to 81% on MRR and
71% for F1 score while also demonstrating higher precision, recall, and
accuracy. These findings suggest that the proposed model is an effective
approach to Arabic pronoun resolution and highlights the potential benefits of
leveraging advanced NLP neural models.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：InstructIE: A Chinese Instruction-based Information Extraction Dataset</b></summary>
  <p><b>编号</b>：[155]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11527</p>
  <p><b>作者</b>：Honghao Gui,  Jintian Zhang,  Hongbin Ye,  Ningyu Zhang</p>
  <p><b>备注</b>：Work in progress</p>
  <p><b>关键词</b>：Information Extraction, task dubbed Instruction-based, Extraction, dubbed Instruction-based, extract information</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce a new Information Extraction (IE) task dubbed Instruction-based
IE, which aims to ask the system to follow specific instructions or guidelines
to extract information. To facilitate research in this area, we construct a
dataset called InstructIE, consisting of 270,000 weakly supervised data from
Chinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We
further evaluate the performance of various baseline models on the InstructIE
dataset. The results reveal that although current models exhibit promising
performance, there is still room for improvement. Furthermore, we conduct a
comprehensive case study analysis, underlining the challenges inherent in the
Instruction-based IE task. Code and dataset are available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：DiffuSIA: A Spiral Interaction Architecture for Encoder-Decoder Text  Diffusion</b></summary>
  <p><b>编号</b>：[159]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11517</p>
  <p><b>作者</b>：Chao-Hong Tan,  Jia-Chen Gu,  Zhen-Hua Ling</p>
  <p><b>备注</b>：Work in Progress</p>
  <p><b>关键词</b>：attracted increasing attention, recently attracted increasing, deep generative models, generative models, increasing attention</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diffusion models have emerged as the new state-of-the-art family of deep
generative models, and their promising potentials for text generation have
recently attracted increasing attention. Existing studies mostly adopt a single
encoder architecture with partially noising processes for conditional text
generation, but its degree of flexibility for conditional modeling is limited.
In fact, the encoder-decoder architecture is naturally more flexible for its
detachable encoder and decoder modules, which is extensible to multilingual and
multimodal generation tasks for conditions and target texts. However, the
encoding process of conditional texts lacks the understanding of target texts.
To this end, a spiral interaction architecture for encoder-decoder text
diffusion (DiffuSIA) is proposed. Concretely, the conditional information from
encoder is designed to be captured by the diffusion decoder, while the target
information from decoder is designed to be captured by the conditional encoder.
These two types of information flow run through multilayer interaction spirally
for deep fusion and understanding. DiffuSIA is evaluated on four text
generation tasks, including paraphrase, text simplification, question
generation, and open-domain dialogue generation. Experimental results show that
DiffuSIA achieves competitive performance among previous methods on all four
tasks, demonstrating the effectiveness and generalization ability of the
proposed method.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Contextualized Word Vector-based Methods for Discovering Semantic  Differences with No Training nor Word Alignment</b></summary>
  <p><b>编号</b>：[160]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11516</p>
  <p><b>作者</b>：Ryo Nagata,  Hiroya Takamura,  Naoki Otani,  Yoshifumi Kawasaki</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：contextualized word vectors, discovering semantic differences, word vectors, propose methods, word</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we propose methods for discovering semantic differences in
words appearing in two corpora based on the norms of contextualized word
vectors. The key idea is that the coverage of meanings is reflected in the norm
of its mean word vector. The proposed methods do not require the assumptions
concerning words and corpora for comparison that the previous methods do. All
they require are to compute the mean vector of contextualized word vectors and
its norm for each word type. Nevertheless, they are (i) robust for the skew in
corpus size; (ii) capable of detecting semantic differences in infrequent
words; and (iii) effective in pinpointing word instances that have a meaning
missing in one of the two corpora for comparison. We show these advantages for
native and non-native English corpora and also for historical corpora.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Plug-and-Play Medical Dialogue System</b></summary>
  <p><b>编号</b>：[166]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11508</p>
  <p><b>作者</b>：Chengfeng Dou,  Zhi Jin,  Wenping Jiao,  Haiyan Zhao,  Zhenwei Tao,  Yongqiang Zhao</p>
  <p><b>备注</b>：9 pages, 3 figures, Possible submission to Emnlp or AAAI</p>
  <p><b>关键词</b>：provide accurate answers, necessitating specific domain, specific domain knowledge, necessitating specific, Large Language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Medical dialogue systems aim to provide accurate answers to patients,
necessitating specific domain knowledge. Recent advancements in Large Language
Models (LLMs) have demonstrated their exceptional capabilities in the medical
Q&A domain, indicating a rich understanding of common sense. However, LLMs are
insufficient for direct diagnosis due to the absence of diagnostic strategies.
The conventional approach to address this challenge involves expensive
fine-tuning of LLMs. Alternatively, a more appealing solution is the
development of a plugin that empowers LLMs to perform medical conversation
tasks. Drawing inspiration from in-context learning, we propose PlugMed, a
Plug-and-Play Medical Dialogue System that facilitates appropriate dialogue
actions by LLMs through two modules: the prompt generation (PG) module and the
response ranking (RR) module. The PG module is designed to capture dialogue
information from both global and local perspectives. It selects suitable
prompts by assessing their similarity to the entire dialogue history and recent
utterances grouped by patient symptoms, respectively. Additionally, the RR
module incorporates fine-tuned SLMs as response filters and selects appropriate
responses generated by LLMs. Moreover, we devise a novel evaluation method
based on intent and medical entities matching to assess the efficacy of
dialogue strategies in medical conversations more effectively. Experimental
evaluations conducted on three unlabeled medical dialogue datasets, including
both automatic and manual assessments, demonstrate that our model surpasses the
strong fine-tuning baselines.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：A Topic-aware Summarization Framework with Different Modal Side  Information</b></summary>
  <p><b>编号</b>：[168]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11503</p>
  <p><b>作者</b>：Xiuying Chen,  Mingzhe Li,  Shen Gao,  Xin Cheng,  Qiang Yang,  Qishen Zhang,  Xin Gao,  Xiangliang Zhang</p>
  <p><b>备注</b>：SIGIR 2023, 10 pages</p>
  <p><b>关键词</b>：side information, Automatic summarization plays, exponential document growth, information, http URL</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automatic summarization plays an important role in the exponential document
growth on the Web. On content websites such as this http URL and this http URL, there
often exist various kinds of side information along with the main document for
attention attraction and easier understanding, such as videos, images, and
queries. Such information can be used for better summarization, as they often
explicitly or implicitly mention the essence of the article. However, most of
the existing side-aware summarization methods are designed to incorporate
either single-modal or multi-modal side information, and cannot effectively
adapt to each other. In this paper, we propose a general summarization
framework, which can flexibly incorporate various modalities of side
information. The main challenges in designing a flexible summarization model
with side information include: (1) the side information can be in textual or
visual format, and the model needs to align and unify it with the document into
the same semantic space, (2) the side inputs can contain information from
various aspects, and the model should recognize the aspects useful for
summarization. To address these two challenges, we first propose a unified
topic encoder, which jointly discovers latent topics from the document and
various kinds of side information. The learned topics flexibly bridge and guide
the information flow between multiple inputs in a graph encoder through a
topic-aware interaction. We secondly propose a triplet contrastive learning
mechanism to align the single-modal or multi-modal information into a unified
semantic space, where the summary quality is enhanced by better understanding
the document and side information. Results show that our model significantly
surpasses strong baselines on three public single-modal or multi-modal
benchmark summarization datasets.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：From Alignment to Entailment: A Unified Textual Entailment Framework for  Entity Alignment</b></summary>
  <p><b>编号</b>：[169]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11501</p>
  <p><b>作者</b>：Yu Zhao,  Yike Wu,  Xiangrui Cai,  Ying Zhang,  Haiwei Zhang,  Xiaojie Yuan</p>
  <p><b>备注</b>：Accepted by ACL 2023 Findings</p>
  <p><b>关键词</b>：Entity Alignment, aims to find, find the equivalent, entities, Alignment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Entity Alignment (EA) aims to find the equivalent entities between two
Knowledge Graphs (KGs). Existing methods usually encode the triples of entities
as embeddings and learn to align the embeddings, which prevents the direct
interaction between the original information of the cross-KG entities.
Moreover, they encode the relational triples and attribute triples of an entity
in heterogeneous embedding spaces, which prevents them from helping each other.
In this paper, we transform both triples into unified textual sequences, and
model the EA task as a bi-directional textual entailment task between the
sequences of cross-KG entities. Specifically, we feed the sequences of two
entities simultaneously into a pre-trained language model (PLM) and propose two
kinds of PLM-based entity aligners that model the entailment probability
between sequences as the similarity between entities. Our approach captures the
unified correlation pattern of two kinds of information between entities, and
explicitly models the fine-grained interaction between original entity
information. The experiments on five cross-lingual EA datasets show that our
approach outperforms the state-of-the-art EA methods and enables the mutual
enhancement of the heterogeneous information. Codes are available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by  Reversing Chain-of-Thought</b></summary>
  <p><b>编号</b>：[170]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11499</p>
  <p><b>作者</b>：Tianci Xue,  Ziqi Wang,  Zhenhailong Wang,  Chi Han,  Pengfei Yu,  Heng Ji</p>
  <p><b>备注</b>：24 pages, 21 figures</p>
  <p><b>关键词</b>：Large language Models, achieved promising performance, language Models, Large language, tasks by incorporating</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language Models (LLMs) have achieved promising performance on
arithmetic reasoning tasks by incorporating step-by-step chain-of-thought (CoT)
prompting. However, LLMs face challenges in maintaining factual consistency
during reasoning, exhibiting tendencies to condition overlooking, question
misinterpretation, and condition hallucination over given problems. Existing
methods use coarse-grained feedback (e.g., whether the answer is correct) to
improve factual consistency. In this work, we propose RCoT (Reversing
Chain-of-Thought), a novel method to improve LLMs' reasoning abilities by
automatically detecting and rectifying factual inconsistency in LLMs' generated
solutions. To detect factual inconsistency, RCoT first asks LLMs to reconstruct
the problem based on generated solutions. Then fine-grained comparisons between
the original problem and the reconstructed problem expose the factual
inconsistency in the original solutions. To rectify the solution, RCoT
formulates detected factual inconsistency into fine-grained feedback to guide
LLMs in revising solutions. Experimental results demonstrate consistent
improvements of RCoT over standard CoT across seven arithmetic datasets.
Moreover, we find that manually written fine-grained feedback can dramatically
improve LLMs' reasoning abilities (e.g., ChatGPT reaches 94.6% accuracy on
GSM8K), encouraging the community to further explore the fine-grained feedback
generation methods.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Recouple Event Field via Probabilistic Bias for Event Extraction</b></summary>
  <p><b>编号</b>：[171]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11498</p>
  <p><b>作者</b>：Xingyu Bai,  Taiqiang Wu,  Han Guo,  Zhe Zhao,  Xuefeng Yang,  Jiayi Li,  Weijie Liu,  Qi Ju,  Weigang Guo,  Yujiu Yang</p>
  <p><b>备注</b>：Published in: ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</p>
  <p><b>关键词</b>：pre-trained language models, aiming to identify, classify event triggers, identify and classify, benefited from pre-trained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Event Extraction (EE), aiming to identify and classify event triggers and
arguments from event mentions, has benefited from pre-trained language models
(PLMs). However, existing PLM-based methods ignore the information of
trigger/argument fields, which is crucial for understanding event schemas. To
this end, we propose a Probabilistic reCoupling model enhanced Event extraction
framework (ProCE). Specifically, we first model the syntactic-related event
fields as probabilistic biases, to clarify the event fields from ambiguous
entanglement. Furthermore, considering multiple occurrences of the same
triggers/arguments in EE, we explore probabilistic interaction strategies among
multiple fields of the same triggers/arguments, to recouple the corresponding
clarified distributions and capture more latent information fields. Experiments
on EE datasets demonstrate the effectiveness and generalization of our proposed
approach.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：TreePrompt: Learning to Compose Tree Prompts for Explainable Visual  Grounding</b></summary>
  <p><b>编号</b>：[172]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11497</p>
  <p><b>作者</b>：Chenchi Zhang,  Jun Xiao,  Lei Chen,  Jian Shao,  Long Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：achieved great success, large pretrained vision-language, pretrained vision-language models, downstream tasks, visual grounding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Prompt tuning has achieved great success in transferring the knowledge from
large pretrained vision-language models into downstream tasks, and has
dominated the performance on visual grounding (VG). However, almost all
existing prompt tuning paradigms suffer from poor interpretability. In this
paper, we argue that their poor interpretability is attributed to the holistic
prompt generation and inference process. By "holistic", we mean that they
usually directly learn a set of vectors as the prompt (i.e., prompt
generation), and use the learned global prompt to augment the textual input for
the VG model (i.e., prompt inference). To this end, we propose a new prompt
construction paradigm with explicit explainable ability, named TreePrompt.
Specifically, we first deconstruct a complex sentence into a tree, that is
consistent with human reasoning. Then, following the syntax tree, we compose a
structured prompt in a bottom-up manner. Thanks to this step-by-step prompt
construction process, each intermediate prompt (i.e., tree node) permits us to
understand the reasoning process. Extensive ablations on various backbones and
benchmarks consistently demonstrate the effectiveness and interpretability of
our TreePrompt.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：LLM Itself Can Read and Generate CXR Images</b></summary>
  <p><b>编号</b>：[174]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11490</p>
  <p><b>作者</b>：Suhyeon Lee,  Won Jun Kim,  Jong Chul Ye</p>
  <p><b>备注</b>：12 pages, 4 figures</p>
  <p><b>关键词</b>：recent remarkable development, large language models, recent remarkable, remarkable development, development of large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Building on the recent remarkable development of large language models
(LLMs), active attempts are being made to extend the utility of LLMs to
multimodal tasks. There have been previous efforts to link language and visual
information, and attempts to add visual capabilities to LLMs are ongoing as
well. However, existing attempts use LLMs only as image decoders and no attempt
has been made to generate images in the same line as the natural language. By
adopting a VQ-GAN framework in which latent representations of images are
treated as a kind of text tokens, we present a novel method to fine-tune a
pre-trained LLM to read and generate images like text without any structural
changes, extra training objectives, or the need for training an ad-hoc network
while still preserving the of the instruction-following capability of the LLM.
We apply this framework to chest X-ray (CXR) image and report generation tasks
as it is a domain in which translation of complex information between visual
and language domains is important. The code will soon be made publicly
available.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：Enhancing Personalized Dialogue Generation with Contrastive Latent  Variables: Combining Sparse and Dense Persona</b></summary>
  <p><b>编号</b>：[180]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11482</p>
  <p><b>作者</b>：Yihong Tang,  Bo Wang,  Miao Fang,  Dongming Zhao,  Kun Huang,  Ruifang He,  Yuexian Hou</p>
  <p><b>备注</b>：ACL 2023</p>
  <p><b>关键词</b>：personalized dialogue explores, generation and personality, explores the consistent, consistent relationship, persona</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The personalized dialogue explores the consistent relationship between
dialogue generation and personality. Existing personalized dialogue agents
model persona profiles from three resources: sparse or dense persona
descriptions and dialogue histories. However, sparse structured persona
attributes are explicit but uninformative, dense persona texts contain rich
persona descriptions with much noise, and dialogue history query is both noisy
and uninformative for persona modeling. In this work, we combine the advantages
of the three resources to obtain a richer and more accurate persona. We design
a Contrastive Latent Variable-based model (CLV) that clusters the dense persona
descriptions into sparse categories, which are combined with the history query
to generate personalized responses. Experimental results on Chinese and English
datasets demonstrate our model's superiority in personalization.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：CCGen: Explainable Complementary Concept Generation in E-Commerce</b></summary>
  <p><b>编号</b>：[182]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11480</p>
  <p><b>作者</b>：Jie Huang,  Yifan Gao,  Zheng Li,  Jingfeng Yang,  Yangqiu Song,  Chao Zhang,  Zining Zhu,  Haoming Jiang,  Kevin Chen-Chuan Chang,  Bing Yin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Digital Cameras, Camera Lenses, Camera Cases, Complementary Concept Generation, Concept Generation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose and study Complementary Concept Generation (CCGen): given a
concept of interest, e.g., "Digital Cameras", generating a list of
complementary concepts, e.g., 1) Camera Lenses 2) Batteries 3) Camera Cases 4)
Memory Cards 5) Battery Chargers. CCGen is beneficial for various applications
like query suggestion and item recommendation, especially in the e-commerce
domain. To solve CCGen, we propose to train language models to generate ranked
lists of concepts with a two-step training strategy. We also teach the models
to generate explanations by incorporating explanations distilled from large
teacher models. Extensive experiments and analysis demonstrate that our model
can generate high-quality concepts complementary to the input concept while
producing explanations to justify the predictions.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：Graphologue: Exploring Large Language Model Responses with Interactive  Diagrams</b></summary>
  <p><b>编号</b>：[186]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11473</p>
  <p><b>作者</b>：Peiling Jiang,  Jude Rayan,  Steven P. Dow,  Haijun Xia</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large language models, unprecedented intelligence exhibited, Large language, language models, diverse applications</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have recently soared in popularity due to their
ease of access and the unprecedented intelligence exhibited on diverse
applications. However, LLMs like ChatGPT present significant limitations in
supporting complex information tasks due to the insufficient affordances of the
text-based medium and linear conversational structure. Through a formative
study with ten participants, we found that LLM interfaces often present
long-winded responses, making it difficult for people to quickly comprehend and
interact flexibly with various pieces of information, particularly during more
complex tasks. We present Graphologue, an interactive system that converts
text-based responses from LLMs into graphical diagrams to facilitate
information-seeking and question-answering tasks. Graphologue employs novel
prompting strategies and interface designs to extract entities and
relationships from LLM responses and constructs node-link diagrams in
real-time. Further, users can interact with the diagrams to flexibly adjust the
graphical presentation and to submit context-specific prompts to obtain more
information. Utilizing diagrams, Graphologue enables graphical, non-linear
dialogues between humans and LLMs, facilitating information exploration,
organization, and comprehension.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：Extending Memory for Language Modelling</b></summary>
  <p><b>编号</b>：[193]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11462</p>
  <p><b>作者</b>：Anupiya Nugaliyadde</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：made major advances, language, Breakthroughs in deep, long, memory</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Breakthroughs in deep learning and memory networks have made major advances
in natural language understanding. Language is sequential and information
carried through the sequence can be captured through memory networks. Learning
the sequence is one of the key aspects in learning the language. However,
memory networks are not capable of holding infinitely long sequences in their
memories and are limited by various constraints such as the vanishing or
exploding gradient problem. Therefore, natural language understanding models
are affected when presented with long sequential text. We introduce Long Term
Memory network (LTM) to learn from infinitely long sequences. LTM gives
priority to the current inputs to allow it to have a high impact. Language
modeling is an important factor in natural language understanding. LTM was
tested in language modeling, which requires long term memory. LTM is tested on
Penn Tree bank dataset, Google Billion Word dataset and WikiText-2 dataset. We
compare LTM with other language models which require long term memory.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：Self-Agreement: A Framework for Fine-tuning Language Models to Find  Agreement among Diverse Opinions</b></summary>
  <p><b>编号</b>：[195]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11460</p>
  <p><b>作者</b>：Shiyao Ding,  Takayuki Ito</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：multiagent systems, challenging topic, topic in multiagent, agreement, opinions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Finding an agreement among diverse opinions is a challenging topic in
multiagent systems. Recently, large language models (LLMs) have shown great
potential in addressing this challenge due to their remarkable capabilities in
comprehending human opinions and generating human-like text. However, they
typically rely on extensive human-annotated data. In this paper, we propose
Self-Agreement, a novel framework for fine-tuning LLMs to autonomously find
agreement using data generated by LLM itself. Specifically, our approach
employs the generative pre-trained transformer-3 (GPT-3) to generate multiple
opinions for each question in a question dataset and create several agreement
candidates among these opinions. Then, a bidirectional encoder representations
from transformers (BERT)-based model evaluates the agreement score of each
agreement candidate and selects the one with the highest agreement score. This
process yields a dataset of question-opinion-agreements, which we use to
fine-tune a pre-trained LLM for discovering agreements among diverse opinions.
Remarkably, a pre-trained LLM fine-tuned by our Self-Agreement framework
achieves comparable performance to GPT-3 with only 1/25 of its parameters,
showcasing its ability to identify agreement among various opinions without the
need for human-annotated data.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：Shattering the Agent-Environment Interface for Fine-Tuning Inclusive  Language Models</b></summary>
  <p><b>编号</b>：[198]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11455</p>
  <p><b>作者</b>：Wanqiao Xu,  Shi Dong,  Dilip Arumugam,  Benjamin Van Roy</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：emulate human feedback, human feedback, autoregressive language models, language model, emulate human</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A centerpiece of the ever-popular reinforcement learning from human feedback
(RLHF) approach to fine-tuning autoregressive language models is the explicit
training of a reward model to emulate human feedback, distinct from the
language model itself. This reward model is then coupled with policy-gradient
methods to dramatically improve the alignment between language model outputs
and desired responses. In this work, we adopt a novel perspective wherein a
pre-trained language model is itself simultaneously a policy, reward function,
and transition function. An immediate consequence of this is that reward
learning and language model fine-tuning can be performed jointly and directly,
without requiring any further downstream policy optimization. While this
perspective does indeed break the traditional agent-environment interface, we
nevertheless maintain that there can be enormous statistical benefits afforded
by bringing to bear traditional algorithmic concepts from reinforcement
learning. Our experiments demonstrate one concrete instance of this through
efficient exploration based on the representation and resolution of epistemic
uncertainty. In order to illustrate these ideas in a transparent manner, we
restrict attention to a simple didactic data generating process and leave for
future work extension to systems of practical scale.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer  with Fine-tuning Slow and Fast</b></summary>
  <p><b>编号</b>：[202]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11449</p>
  <p><b>作者</b>：Yiduo Guo,  Yaobo Liang,  Dongyan Zhao,  Bing Liu,  Duan Nan</p>
  <p><b>备注</b>：Accepted by ACL2023 (Long paper)</p>
  <p><b>关键词</b>：multilingual pre-trained language, pre-trained language model, language model fine-tuned, Existing research, non-source languages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing research has shown that a multilingual pre-trained language model
fine-tuned with one (source) language also performs well on downstream tasks
for non-source languages, even though no fine-tuning is done on these
languages. However, there is a clear gap between the performance of the source
language and that of the non-source languages. This paper analyzes the
fine-tuning process, discovers when the performance gap changes and identifies
which network weights affect the overall performance most. Additionally, the
paper seeks to answer to what extent the gap can be reduced by reducing
forgetting. Based on the analysis results, a method named Fine-tuning slow and
fast with four training policies is proposed to address these issues.
Experimental results show the proposed method outperforms baselines by a clear
margin.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：Arukikata Travelogue Dataset</b></summary>
  <p><b>编号</b>：[203]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11444</p>
  <p><b>作者</b>：Hiroki Ouchi,  Hiroyuki Shindo,  Shoko Wakamiya,  Yuki Matsuda,  Naoya Inoue,  Shohei Higashiyama,  Satoshi Nakamura,  Taro Watanabe</p>
  <p><b>备注</b>：The application website for Arukikata Travelogue Dataset: this https URL</p>
  <p><b>关键词</b>：constructed Arukikata Travelogue, Arukikata Travelogue Dataset, constructed Arukikata, Arukikata Travelogue, released it free</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We have constructed Arukikata Travelogue Dataset and released it free of
charge for academic research. This dataset is a Japanese text dataset with a
total of over 31 million words, comprising 4,672 Japanese domestic travelogues
and 9,607 overseas travelogues. Before providing our dataset, there was a
scarcity of widely available travelogue data for research purposes, and each
researcher had to prepare their own data. This hinders the replication of
existing studies and fair comparative analysis of experimental results. Our
dataset enables any researchers to conduct investigation on the same data and
to ensure transparency and reproducibility in research. In this paper, we
describe the academic significance, characteristics, and prospects of our
dataset.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：Zero-Shot Text Classification via Self-Supervised Tuning</b></summary>
  <p><b>编号</b>：[205]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11442</p>
  <p><b>作者</b>：Chaoqun Liu,  Wenxuan Zhang,  Guizhen Chen,  Xiaobao Wu,  Anh Tuan Luu,  Chip Hong Chang,  Lidong Bing</p>
  <p><b>备注</b>：Accepted to the Findings of ACL 2023</p>
  <p><b>关键词</b>：zero-shot text classification, large-scale annotated data, text classification tasks, text classification, Existing solutions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing solutions to zero-shot text classification either conduct prompting
with pre-trained language models, which is sensitive to the choices of
templates, or rely on large-scale annotated data of relevant tasks for
meta-tuning. In this work, we propose a new paradigm based on self-supervised
learning to solve zero-shot text classification tasks by tuning the language
models with unlabeled data, called self-supervised tuning. By exploring the
inherent structure of free texts, we propose a new learning objective called
first sentence prediction to bridge the gap between unlabeled data and text
classification tasks. After tuning the model to learn to predict the first
sentence in a paragraph based on the rest, the model is able to conduct
zero-shot inference on unseen tasks such as topic classification and sentiment
analysis. Experimental results show that our model outperforms the
state-of-the-art baselines on 7 out of 10 tasks. Moreover, the analysis reveals
that our model is less sensitive to the prompt design. Our code and pre-trained
models are publicly available at this https URL .</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：Phonetic and Prosody-aware Self-supervised Learning Approach for  Non-native Fluency Scoring</b></summary>
  <p><b>编号</b>：[208]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11438</p>
  <p><b>作者</b>：Kaiqi Fu,  Shaojun Gao,  Shuju Shi,  Xiaohai Tian,  Wei Li,  Zejun Ma</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：evaluated by analyzing, analyzing a range, prosodic features, phonetic and prosodic, map fluency-related features</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Speech fluency/disfluency can be evaluated by analyzing a range of phonetic
and prosodic features. Deep neural networks are commonly trained to map
fluency-related features into the human scores. However, the effectiveness of
deep learning-based models is constrained by the limited amount of labeled
training samples. To address this, we introduce a self-supervised learning
(SSL) approach that takes into account phonetic and prosody awareness for
fluency scoring. Specifically, we first pre-train the model using a
reconstruction loss function, by masking phones and their durations jointly on
a large amount of unlabeled speech and text prompts. We then fine-tune the
pre-trained model using human-annotated scoring data. Our experimental results,
conducted on datasets such as Speechocean762 and our non-native datasets, show
that our proposed method outperforms the baseline systems in terms of Pearson
correlation coefficients (PCC). Moreover, we also conduct an ablation study to
better understand the contribution of phonetic and prosody factors during the
pre-training stage.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks</b></summary>
  <p><b>编号</b>：[211]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11430</p>
  <p><b>作者</b>：Shubhra Kanti Karmaker Santu,  Dongji Feng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：traditional conversational settings, shown great success, performing ill-defined complex, conversational settings, largely under-studied</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While LLMs have shown great success in understanding and generating text in
traditional conversational settings, their potential for performing ill-defined
complex tasks is largely under-studied. Indeed, we are yet to conduct
comprehensive benchmarking studies with multiple LLMs that are exclusively
focused on a complex task. However, conducting such benchmarking studies is
challenging because of the large variations in LLMs' performance when different
prompt types/styles are used and different degrees of detail are provided in
the prompts. To address this issue, the paper proposes a general taxonomy that
can be used to design prompts with specific properties in order to perform a
wide range of complex tasks. This taxonomy will allow future benchmarking
studies to report the specific categories of prompts used as part of the study,
enabling meaningful comparisons across different studies. Also, by establishing
a common standard through this taxonomy, researchers will be able to draw more
accurate conclusions about LLMs' performance on a specific complex task.</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：Post Hoc Explanations of Language Models Can Improve Language Models</b></summary>
  <p><b>编号</b>：[214]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11426</p>
  <p><b>作者</b>：Satyapriya,  Krishna,  Jiaqi Ma,  Dylan Slack,  Asma Ghandeharioun,  Sameer Singh,  Himabindu Lakkaraju</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：demonstrated remarkable capabilities, performing complex tasks, Post Hoc Explanations, Large Language Models, Amplifying Model Performance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) have demonstrated remarkable capabilities in
performing complex tasks. Moreover, recent research has shown that
incorporating human-annotated rationales (e.g., Chain-of- Thought prompting)
during in-context learning can significantly enhance the performance of these
models, particularly on tasks that require reasoning capabilities. However,
incorporating such rationales poses challenges in terms of scalability as this
requires a high degree of human involvement. In this work, we present a novel
framework, Amplifying Model Performance by Leveraging In-Context Learning with
Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges
by automating the process of rationale generation. To this end, we leverage
post hoc explanation methods which output attribution scores (explanations)
capturing the influence of each of the input features on model predictions.
More specifically, we construct automated natural language rationales that
embed insights from post hoc explanations to provide corrective signals to
LLMs. Extensive experimentation with real-world datasets demonstrates that our
framework, AMPLIFY, leads to prediction accuracy improvements of about 10-25%
over a wide range of tasks, including those where prior approaches which rely
on human-annotated rationales such as Chain-of-Thought prompting fall short.
Our work makes one of the first attempts at highlighting the potential of post
hoc explanations as valuable tools for enhancing the effectiveness of LLMs.
Furthermore, we conduct additional empirical analyses and ablation studies to
demonstrate the impact of each of the components of AMPLIFY, which, in turn,
lead to critical insights for refining in-context learning.</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：DUB: Discrete Unit Back-translation for Speech Translation</b></summary>
  <p><b>编号</b>：[224]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11411</p>
  <p><b>作者</b>：Dong Zhang,  Rong Ye,  Tom Ko,  Mingxuan Wang,  Yaqian Zhou</p>
  <p><b>备注</b>：Accepted to Findings of ACL 2023</p>
  <p><b>关键词</b>：machine translation, translation, discrete units, unsupervised discrete units, discrete units yields</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>How can speech-to-text translation (ST) perform as well as machine
translation (MT)? The key point is to bridge the modality gap between speech
and text so that useful MT techniques can be applied to ST. Recently, the
approach of representing speech with unsupervised discrete units yields a new
way to ease the modality problem. This motivates us to propose Discrete Unit
Back-translation (DUB) to answer two questions: (1) Is it better to represent
speech with discrete units than with continuous features in direct ST? (2) How
much benefit can useful MT techniques bring to ST? With DUB, the
back-translation technique can successfully be applied on direct ST and obtains
an average boost of 5.5 BLEU on MuST-C En-De/Fr/Es. In the low-resource
language scenario, our method achieves comparable performance to existing
methods that rely on large-scale external data. Code and models are available
at this https URL.</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：AlignAtt: Using Attention-based Audio-Translation Alignments as a Guide  for Simultaneous Speech Translation</b></summary>
  <p><b>编号</b>：[225]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11408</p>
  <p><b>作者</b>：Sara Papi,  Marco Turchi,  Matteo Negri</p>
  <p><b>备注</b>：Accepted at Interspeech 2023</p>
  <p><b>关键词</b>：machine translation-related tasks, natural language processing, including its effectiveness, core mechanism, mechanism of today</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Attention is the core mechanism of today's most used architectures for
natural language processing and has been analyzed from many perspectives,
including its effectiveness for machine translation-related tasks. Among these
studies, attention resulted to be a useful source of information to get
insights about word alignment also when the input text is substituted with
audio segments, as in the case of the speech translation (ST) task. In this
paper, we propose AlignAtt, a novel policy for simultaneous ST (SimulST) that
exploits the attention information to generate source-target alignments that
guide the model during inference. Through experiments on the 8 language pairs
of MuST-C v1.0, we show that AlignAtt outperforms previous state-of-the-art
SimulST policies applied to offline-trained models with gains in terms of BLEU
of 2 points and latency reductions ranging from 0.5s to 0.8s across the 8
languages.</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：Comfort Foods and Community Connectedness: Investigating Diet Change  during COVID-19 Using YouTube Videos on Twitter</b></summary>
  <p><b>编号</b>：[230]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11398</p>
  <p><b>作者</b>：Yelena Mejova,  Lydia Manikonda</p>
  <p><b>备注</b>：To be published in The International AAAI Conference on Web and Social Media (ICWSM) 2023</p>
  <p><b>关键词</b>：important health-related behaviors, impacting important health-related, millions of people, health-related behaviors, drastically changed</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Unprecedented lockdowns at the start of the COVID-19 pandemic have
drastically changed the routines of millions of people, potentially impacting
important health-related behaviors. In this study, we use YouTube videos
embedded in tweets about diet, exercise and fitness posted before and during
COVID-19 to investigate the influence of the pandemic lockdowns on diet and
nutrition. In particular, we examine the nutritional profile of the foods
mentioned in the transcript, description and title of each video in terms of
six macronutrients (protein, energy, fat, sodium, sugar, and saturated fat).
These macronutrient values were further linked to demographics to assess if
there are specific effects on those potentially having insufficient access to
healthy sources of food. Interrupted time series analysis revealed a
considerable shift in the aggregated macronutrient scores before and during
COVID-19. In particular, whereas areas with lower incomes showed decrease in
energy, fat, and saturated fat, those with higher percentage of African
Americans showed an elevation in sodium. Word2Vec word similarities and odds
ratio analysis suggested a shift from popular diets and lifestyle bloggers
before the lockdowns to the interest in a variety of healthy foods, communal
sharing of quick and easy recipes, as well as a new emphasis on comfort foods.
To the best of our knowledge, this work is novel in terms of linking attention
signals in tweets, content of videos, their nutrients profile, and aggregate
demographics of the users. The insights made possible by this combination of
resources are important for monitoring the secondary health effects of social
distancing, and informing social programs designed to alleviate these effects.</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：Fast-StrucTexT: An Efficient Hourglass Transformer with Modality-guided  Dynamic Token Merge for Document Understanding</b></summary>
  <p><b>编号</b>：[232]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11392</p>
  <p><b>作者</b>：Mingliang Zhai,  Yulin Li,  Xiameng Qin,  Chen Yi,  Qunyi Xie,  Chengquan Zhang,  Kun Yao,  Yuwei Wu,  Yunde Jia</p>
  <p><b>备注</b>：IJCAI 2023</p>
  <p><b>关键词</b>：quadratic computational complexity, computational complexity dependency, sequence length, Transformers achieve promising, high effectiveness</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transformers achieve promising performance in document understanding because
of their high effectiveness and still suffer from quadratic computational
complexity dependency on the sequence length. General efficient transformers
are challenging to be directly adapted to model document. They are unable to
handle the layout representation in documents, e.g. word, line and paragraph,
on different granularity levels and seem hard to achieve a good trade-off
between efficiency and performance. To tackle the concerns, we propose
Fast-StrucTexT, an efficient multi-modal framework based on the StrucTexT
algorithm with an hourglass transformer architecture, for visual document
understanding. Specifically, we design a modality-guided dynamic token merging
block to make the model learn multi-granularity representation and prunes
redundant tokens. Additionally, we present a multi-modal interaction module
called Symmetry Cross Attention (SCA) to consider multi-modal fusion and
efficiently guide the token mergence. The SCA allows one modality input as
query to calculate cross attention with another modality in a dual phase.
Extensive experiments on FUNSD, SROIE, and CORD datasets demonstrate that our
model achieves the state-of-the-art performance and almost 1.9X faster
inference time than the state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：Characterizing tradeoffs between teaching via language and  demonstrations in multi-agent systems</b></summary>
  <p><b>编号</b>：[243]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11374</p>
  <p><b>作者</b>：Dhara Yu,  Noah D. Goodman,  Jesse Mu</p>
  <p><b>备注</b>：7 pages, 6 figures, to appear in Proceedings of the 45th Annual Conference of the Cognitive Science Society</p>
  <p><b>关键词</b>：language, demonstration, effective, task, world</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Humans teach others about the world through language and demonstration. When
might one of these modalities be more effective than the other? In this work,
we study the factors that modulate the effectiveness of language vs.
demonstration using multi-agent systems to model human communication.
Specifically, we train neural network agents to teach via language or
demonstration in a grounded communication task, manipulating 1) the inherent
difficulty of the task and 2) the competence of the teacher. We find that
teaching by demonstration is more effective in the simplest settings, but
language is more effective as task difficulty increases, due to its ability to
generalize more effectively to unseen scenarios. Overall, these results provide
converging evidence for a tradeoff between language and demonstration as
teaching modalities in humans, and make the novel predictions that
demonstration may be optimal for easy tasks, while language enables
generalization in more challenging settings.</p>
  </details>
</details>
<details>
  <summary>83. <b>标题：AutoTrial: Prompting Language Models for Clinical Trial Design</b></summary>
  <p><b>编号</b>：[246]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11366</p>
  <p><b>作者</b>：Zifeng Wang,  Cao Xiao,  Jimeng Sun</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：drug development, critical for drug, eligibility criteria, Clinical, Clinical trials</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Clinical trials are critical for drug development. Constructing the
appropriate eligibility criteria (i.e., the inclusion/exclusion criteria for
patient recruitment) is essential for the trial's success. Proper design of
clinical trial protocols should consider similar precedent trials and their
eligibility criteria to ensure sufficient patient coverage. In this paper, we
present a method named AutoTrial to aid the design of clinical eligibility
criteria using language models. It allows (1) controllable generation under
instructions via a hybrid of discrete and neural prompting, (2) scalable
knowledge incorporation via in-context learning, and (3) explicit reasoning
chains to provide rationales for understanding the outputs. Experiments on over
70K clinical trials verify that AutoTrial generates high-quality criteria texts
that are fluent and coherent and with high accuracy in capturing the relevant
clinical concepts to the target trial. It is noteworthy that our method, with a
much smaller parameter size, gains around 60\% winning rate against the GPT-3.5
baselines via human evaluations.</p>
  </details>
</details>
<details>
  <summary>84. <b>标题：Visualizing Linguistic Diversity of Text Datasets Synthesized by Large  Language Models</b></summary>
  <p><b>编号</b>：[248]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11364</p>
  <p><b>作者</b>：Emily Reif,  Minsuk Kahng,  Savvas Petridis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large language models, Large language, language models, generate smaller, prompting for benchmarking</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) can be used to generate smaller, more refined
datasets via few-shot prompting for benchmarking, fine-tuning or other use
cases. However, understanding and evaluating these datasets is difficult, and
the failure modes of LLM-generated data are still not well understood.
Specifically, the data can be repetitive in surprising ways, not only
semantically but also syntactically and lexically. We present LinguisticLens, a
novel inter-active visualization tool for making sense of and analyzing
syntactic diversity of LLM-generated datasets. LinguisticLens clusters text
along syntactic, lexical, and semantic axes. It supports hierarchical
visualization of a text dataset, allowing users to quickly scan for an overview
and inspect individual examples. The live demo is available at
this http URL.</p>
  </details>
</details>
<details>
  <summary>85. <b>标题：MD3: The Multi-Dialect Dataset of Dialogues</b></summary>
  <p><b>编号</b>：[254]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11355</p>
  <p><b>作者</b>：Jacob Eisenstein,  Vinodkumar Prabhakaran,  Clara Rivera,  Dorottya Demszky,  Devyani Sharma</p>
  <p><b>备注</b>：InterSpeech 2023</p>
  <p><b>关键词</b>：speech representing English, representing English, conversational speech representing, United States, conversational speech</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce a new dataset of conversational speech representing English from
India, Nigeria, and the United States. The Multi-Dialect Dataset of Dialogues
(MD3) strikes a new balance between open-ended conversational speech and
task-oriented dialogue by prompting participants to perform a series of short
information-sharing tasks. This facilitates quantitative cross-dialectal
comparison, while avoiding the imposition of a restrictive task structure that
might inhibit the expression of dialect features. Preliminary analysis of the
dataset reveals significant differences in syntax and in the use of discourse
markers. The dataset, which will be made publicly available with the
publication of this paper, includes more than 20 hours of audio and more than
200,000 orthographically-transcribed tokens.</p>
  </details>
</details>
<details>
  <summary>86. <b>标题：Data Redaction from Conditional Generative Models</b></summary>
  <p><b>编号</b>：[255]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11351</p>
  <p><b>作者</b>：Zhifeng Kong,  Kamalika Chaudhuri</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：produce undesirable samples, Deep generative models, harmful content, Deep generative, produce undesirable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep generative models are known to produce undesirable samples such as
harmful content. Traditional mitigation methods include re-training from
scratch, filtering, or editing; however, these are either computationally
expensive or can be circumvented by third parties. In this paper, we take a
different approach and study how to post-edit an already-trained conditional
generative model so that it redacts certain conditionals that will, with high
probability, lead to undesirable content. This is done by distilling the
conditioning network in the models, giving a solution that is effective,
efficient, controllable, and universal for a class of deep generative models.
We conduct experiments on redacting prompts in text-to-image models and
redacting voices in text-to-speech models. Our method is computationally light,
leads to better redaction quality and robustness than baseline methods while
still retaining high generation quality.</p>
  </details>
</details>
<details>
  <summary>87. <b>标题：Unsupervised Domain-agnostic Fake News Detection using Multi-modal Weak  Signals</b></summary>
  <p><b>编号</b>：[256]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11349</p>
  <p><b>作者</b>：Amila Silva,  Ling Luo,  Shanika Karunasekera,  Christopher Leckie</p>
  <p><b>备注</b>：15 pages</p>
  <p><b>关键词</b>：fake news detection, fake, emergence of social, social media, main platforms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The emergence of social media as one of the main platforms for people to
access news has enabled the wide dissemination of fake news. This has motivated
numerous studies on automating fake news detection. Although there have been
limited attempts at unsupervised fake news detection, their performance suffers
due to not exploiting the knowledge from various modalities related to news
records and due to the presence of various latent biases in the existing news
datasets. To address these limitations, this work proposes an effective
framework for unsupervised fake news detection, which first embeds the
knowledge available in four modalities in news records and then proposes a
novel noise-robust self-supervised learning technique to identify the veracity
of news records from the multi-modal embeddings. Also, we propose a novel
technique to construct news datasets minimizing the latent biases in existing
news datasets. Following the proposed approach for dataset construction, we
produce a Large-scale Unlabelled News Dataset consisting 419,351 news articles
related to COVID-19, acronymed as LUND-COVID. We trained the proposed
unsupervised framework using LUND-COVID to exploit the potential of large
datasets, and evaluate it using a set of existing labelled datasets. Our
results show that the proposed unsupervised framework largely outperforms
existing unsupervised baselines for different tasks such as multi-modal fake
news detection, fake news early detection and few-shot fake news detection,
while yielding notable improvements for unseen domains during training.</p>
  </details>
</details>
<details>
  <summary>88. <b>标题：In the Name of Fairness: Assessing the Bias in Clinical Record  De-identification</b></summary>
  <p><b>编号</b>：[257]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11348</p>
  <p><b>作者</b>：Yuxin Xiao,  Shulammite Lim,  Tom Joseph Pollard,  Marzyeh Ghassemi</p>
  <p><b>备注</b>：Accepted by FAccT 2023</p>
  <p><b>关键词</b>：electronic health records, protected health information, clinical data requires, Data sharing, health records</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Data sharing is crucial for open science and reproducible research, but the
legal sharing of clinical data requires the removal of protected health
information from electronic health records. This process, known as
de-identification, is often achieved through the use of machine learning
algorithms by many commercial and open-source systems. While these systems have
shown compelling results on average, the variation in their performance across
different demographic groups has not been thoroughly examined. In this work, we
investigate the bias of de-identification systems on names in clinical notes
via a large-scale empirical analysis. To achieve this, we create 16 name sets
that vary along four demographic dimensions: gender, race, name popularity, and
the decade of popularity. We insert these names into 100 manually curated
clinical templates and evaluate the performance of nine public and private
de-identification methods. Our findings reveal that there are statistically
significant performance gaps along a majority of the demographic dimensions in
most methods. We further illustrate that de-identification quality is affected
by polysemy in names, gender context, and clinical note characteristics. To
mitigate the identified gaps, we propose a simple and method-agnostic solution
by fine-tuning de-identification methods with clinical context and diverse
names. Overall, it is imperative to address the bias in existing methods
immediately so that downstream stakeholders can build high-quality systems to
serve all demographic parties fairly.</p>
  </details>
</details>
<details>
  <summary>89. <b>标题：Writing your own book: A method for going from closed to open book QA to  improve robustness and performance of smaller LLMs</b></summary>
  <p><b>编号</b>：[267]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11334</p>
  <p><b>作者</b>：Giorgi Kokaia,  Pratyush Sinha,  Yutong Jiang,  Nozha Boujemaa</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, large language, designed to enhance, Tree-Search, Self-contextualizing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce two novel methods, Tree-Search and Self-contextualizing QA,
designed to enhance the performance of large language models (LLMs) in
question-answering tasks. Tree-Search is a sampling technique specifically
created to extract diverse information from an LLM for a given prompt.
Self-contextualizing QA leverages Tree-Search to enable the model to create its
own context using a wide range of information relevant to the prompt, evaluate
it explicitly and return a open book answer to the initial prompt . We
demonstrate that the quality of generated answers improves according to various
metrics, including accuracy, informativeness, coherence, and consistency, as
evaluated by GPT3.5(text-davinci-003). Furthermore, we show that our methods
result in increased robustness and that performance is positively correlated
with tree size, benefiting both answer quality and robustness. Finally, we
discuss other promising applications of Tree-Search, highlighting its potential
to enhance a broad range of tasks beyond question-answering.
\noindent We also discuss several areas for future work, including refining
the Tree-Search and Self-Contextualizing QA methods, improving the coherence of
the generated context, and investigating the impact of bootstrapping on model
robustness</p>
  </details>
</details>
<details>
  <summary>90. <b>标题：Towards the Automatic Generation of Conversational Interfaces to  Facilitate the Exploration of Tabular Data</b></summary>
  <p><b>编号</b>：[270]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11326</p>
  <p><b>作者</b>：Marcos Gomez,  Jordi Cabot,  Robert Clarisó</p>
  <p><b>备注</b>：13 pages, 4 figures</p>
  <p><b>关键词</b>：structured data online, exchange structured data, common format, format to publish, publish and exchange</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Tabular data is the most common format to publish and exchange structured
data online. A clear example is the growing number of open data portals
published by all types of public administrations. However, exploitation of
these data sources is currently limited to technical people able to
programmatically manipulate and digest such data. As an alternative, we propose
the use of chatbots to offer a conversational interface to facilitate the
exploration of tabular data sources. With our approach, any regular citizen can
benefit and leverage them. Moreover, our chatbots are not manually created:
instead, they are automatically generated from the data source itself thanks to
the instantiation of a configurable collection of conversation patterns.</p>
  </details>
</details>
<details>
  <summary>91. <b>标题：Collaborative Generative AI: Integrating GPT-k for Efficient Editing in  Text-to-Image Generation</b></summary>
  <p><b>编号</b>：[274]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11317</p>
  <p><b>作者</b>：Wanrong Zhu,  Xinyi Wang,  Yujie Lu,  Tsu-Jui Fu,  Xin Eric Wang,  Miguel Eckstein,  William Yang Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：garnered significant attention, garnered significant, significant attention, research community, everyday users</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The field of text-to-image (T2I) generation has garnered significant
attention both within the research community and among everyday users. Despite
the advancements of T2I models, a common issue encountered by users is the need
for repetitive editing of input prompts in order to receive a satisfactory
image, which is time-consuming and labor-intensive. Given the demonstrated text
generation power of large-scale language models, such as GPT-k, we investigate
the potential of utilizing such models to improve the prompt editing process
for T2I generation. We conduct a series of experiments to compare the common
edits made by humans and GPT-k, evaluate the performance of GPT-k in prompting
T2I, and examine factors that may influence this process. We found that GPT-k
models focus more on inserting modifiers while humans tend to replace words and
phrases, which includes changes to the subject matter. Experimental results
show that GPT-k are more effective in adjusting modifiers rather than
predicting spontaneous changes in the primary subject matters. Adopting the
edit suggested by GPT-k models may reduce the percentage of remaining edits by
20-30%.</p>
  </details>
</details>
<details>
  <summary>92. <b>标题：Improving Toponym Resolution with Better Candidate Generation,  Transformer-based Reranking, and Two-Stage Resolution</b></summary>
  <p><b>编号</b>：[276]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11315</p>
  <p><b>作者</b>：Zeyu Zhang,  Steven Bethard</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：task of converting, text into structured, structured data, data that encodes, converting location mentions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Geocoding is the task of converting location mentions in text into structured
data that encodes the geospatial semantics. We propose a new architecture for
geocoding, GeoNorm. GeoNorm first uses information retrieval techniques to
generate a list of candidate entries from the geospatial ontology. Then it
reranks the candidate entries using a transformer-based neural network that
incorporates information from the ontology such as the entry's population. This
generate-and-rerank process is applied twice: first to resolve the less
ambiguous countries, states, and counties, and second to resolve the remaining
location mentions, using the identified countries, states, and counties as
context. Our proposed toponym resolution framework achieves state-of-the-art
performance on multiple datasets. Code and models are available at
\url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>93. <b>标题：Towards Collaborative Plan Acquisition through Theory of Mind Modeling  in Situated Dialogue</b></summary>
  <p><b>编号</b>：[297]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11271</p>
  <p><b>作者</b>：Cristian-Paul Bara,  Ziqiao Ma,  Yingzhuo Yu,  Julie Shah,  Joyce Chai</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：incomplete initial plans, incomplete initial, tasks, partner, knowledge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Collaborative tasks often begin with partial task knowledge and incomplete
initial plans from each partner. To complete these tasks, agents need to engage
in situated communication with their partners and coordinate their partial
plans towards a complete plan to achieve a joint task goal. While such
collaboration seems effortless in a human-human team, it is highly challenging
for human-AI collaboration. To address this limitation, this paper takes a step
towards collaborative plan acquisition, where humans and agents strive to learn
and communicate with each other to acquire a complete plan for joint tasks.
Specifically, we formulate a novel problem for agents to predict the missing
task knowledge for themselves and for their partners based on rich perceptual
and dialogue history. We extend a situated dialogue benchmark for symmetric
collaborative tasks in a 3D blocks world and investigate computational
strategies for plan acquisition. Our empirical results suggest that predicting
the partner's missing knowledge is a more viable approach than predicting one's
own. We show that explicit modeling of the partner's dialogue moves and mental
states produces improved and more stable results than without. These results
provide insight for future AI agents that can predict what knowledge their
partner is missing and, therefore, can proactively communicate such information
to help their partner acquire such missing knowledge toward a common
understanding of joint tasks.</p>
  </details>
</details>
<details>
  <summary>94. <b>标题：CHBias: Bias Evaluation and Mitigation of Chinese Conversational  Language Models</b></summary>
  <p><b>编号</b>：[300]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11262</p>
  <p><b>作者</b>：Jiaxu Zhao,  Meng Fang,  Zijing Shi,  Yitong Li,  Ling Chen,  Mykola Pechenizkiy</p>
  <p><b>备注</b>：Accepted by ACL 2023</p>
  <p><b>关键词</b>：offensive or upsetting., safety issues, exhibiting a range, Warning, stereotypical human biases</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>\textit{\textbf{\textcolor{red}{Warning}:} This paper contains content that
may be offensive or upsetting.} Pretrained conversational agents have been
exposed to safety issues, exhibiting a range of stereotypical human biases such
as gender bias. However, there are still limited bias categories in current
research, and most of them only focus on English. In this paper, we introduce a
new Chinese dataset, CHBias, for bias evaluation and mitigation of Chinese
conversational language models. Apart from those previous well-explored bias
categories, CHBias includes under-explored bias categories, such as ageism and
appearance biases, which received less attention. We evaluate two popular
pretrained Chinese conversational models, CDial-GPT and EVA2.0, using CHBias.
Furthermore, to mitigate different biases, we apply several debiasing methods
to the Chinese pretrained models. Experimental results show that these Chinese
pretrained models are potentially risky for generating texts that contain
social biases, and debiasing methods using the proposed dataset can make
response generation less biased while preserving the models' conversational
capabilities.</p>
  </details>
</details>
<details>
  <summary>95. <b>标题：Reasoning Implicit Sentiment with Chain-of-Thought Prompting</b></summary>
  <p><b>编号</b>：[303]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11255</p>
  <p><b>作者</b>：Hao Fei,  Bobo Li,  Qian Liu,  Lidong Bing,  Fei Li,  Tat-Seng Chua</p>
  <p><b>备注</b>：ACL 2023 Short Paper</p>
  <p><b>关键词</b>：sentiment analysis systems, implicit sentiment analysis, key opinion expressions, sentiment analysis, analysis systems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While sentiment analysis systems try to determine the sentiment polarities of
given targets based on the key opinion expressions in input texts, in implicit
sentiment analysis (ISA) the opinion cues come in an implicit and obscure
manner. Thus detecting implicit sentiment requires the common-sense and
multi-hop reasoning ability to infer the latent intent of opinion. Inspired by
the recent chain-of-thought (CoT) idea, in this work we introduce a Three-hop
Reasoning (THOR) CoT framework to mimic the human-like reasoning process for
ISA. We design a three-step prompting principle for THOR to step-by-step induce
the implicit aspect, opinion, and finally the sentiment polarity. Our
THOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 6% F1 on
supervised setup. More strikingly, THOR+GPT3 (175B) boosts the SoTA by over 50%
F1 on zero-shot setting. Our code is at
this https URL.</p>
  </details>
</details>
<details>
  <summary>96. <b>标题：Computational thematics: Comparing algorithms for clustering the genres  of literary fiction</b></summary>
  <p><b>编号</b>：[305]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11251</p>
  <p><b>作者</b>：Oleg Sobchuk,  Artjoms Šeļa</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：capturing thematic similarity, similarity between literary, capturing thematic, thematic similarity, literary texts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>What are the best methods of capturing thematic similarity between literary
texts? Knowing the answer to this question would be useful for automatic
clustering of book genres, or any other thematic grouping. This paper compares
a variety of algorithms for unsupervised learning of thematic similarities
between texts, which we call "computational thematics". These algorithms belong
to three steps of analysis: text preprocessing, extraction of text features,
and measuring distances between the lists of features. Each of these steps
includes a variety of options. We test all the possible combinations of these
options: every combination of algorithms is given a task to cluster a corpus of
books belonging to four pre-tagged genres of fiction. This clustering is then
validated against the "ground truth" genre labels. Such comparison of
algorithms allows us to learn the best and the worst combinations for
computational thematic analysis. To illustrate the sharp difference between the
best and the worst methods, we then cluster 5000 random novels from the
HathiTrust corpus of fiction.</p>
  </details>
</details>
<details>
  <summary>97. <b>标题：A Parameter-Efficient Learning Approach to Arabic Dialect Identification  with Pre-Trained General-Purpose Speech Model</b></summary>
  <p><b>编号</b>：[308]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11244</p>
  <p><b>作者</b>：Srijith Radhakrishnan,  Chao-Han Huck Yang,  Sumeer Ahmad Khan,  Narsis A. Kiani,  David Gomez-Cabrero,  Jesper N. Tegner</p>
  <p><b>备注</b>：Accepted to Interspeech. Code is available at: this https URL under MIT license</p>
  <p><b>关键词</b>：techniques to repurpose, Arabic dialect identification, Arabic dialect, multi-layer encoder-decoder GSM, encoder-decoder GSM formulation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we explore Parameter-Efficient-Learning (PEL) techniques to
repurpose a General-Purpose-Speech (GSM) model for Arabic dialect
identification (ADI). Specifically, we investigate different setups to
incorporate trainable features into a multi-layer encoder-decoder GSM
formulation under frozen pre-trained settings. Our architecture includes
residual adapter and model reprogramming (input-prompting). We design a
token-level label mapping to condition the GSM for Arabic Dialect
Identification (ADI). This is challenging due to the high variation in
vocabulary and pronunciation among the numerous regional dialects. We achieve
new state-of-the-art accuracy on the ADI-17 dataset by vanilla fine-tuning. We
further reduce the training budgets with the PEL method, which performs within
1.86% accuracy to fine-tuning using only 2.5% of (extra) network trainable
parameters. Our study demonstrates how to identify Arabic dialects using a
small dataset and limited computation with open source code and pre-trained
models.</p>
  </details>
</details>
<details>
  <summary>98. <b>标题：Comparing Machines and Children: Using Developmental Psychology  Experiments to Assess the Strengths and Weaknesses of LaMDA Responses</b></summary>
  <p><b>编号</b>：[309]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11243</p>
  <p><b>作者</b>：Eliza Kosoy,  Emily Rose Reagan,  Leslie Lai,  Alison Gopnik,  Danielle Krettek Cobb</p>
  <p><b>备注</b>：9 pages, 7 figures</p>
  <p><b>关键词</b>：spent decades devising, decades devising experiments, tracing the origin, psychologists have spent, spent decades</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Developmental psychologists have spent decades devising experiments to test
the intelligence and knowledge of infants and children, tracing the origin of
crucial concepts and capacities. Moreover, experimental techniques in
developmental psychology have been carefully designed to discriminate the
cognitive capacities that underlie particular behaviors. We propose that using
classical experiments from child development is a particularly effective way to
probe the computational abilities of AI models, in general, and LLMs in
particular. First, the methodological techniques of developmental psychology,
such as the use of novel stimuli to control for past experience or control
conditions to determine whether children are using simple associations, can be
equally helpful for assessing the capacities of LLMs. In parallel, testing LLMs
in this way can tell us whether the information that is encoded in text is
sufficient to enable particular responses, or whether those responses depend on
other kinds of information, such as information from exploration of the
physical world. In this work we adapt classical developmental experiments to
evaluate the capabilities of LaMDA, a large language model from Google. We
propose a novel LLM Response Score (LRS) metric which can be used to evaluate
other language models, such as GPT. We find that LaMDA generates appropriate
responses that are similar to those of children in experiments involving social
understanding, perhaps providing evidence that knowledge of these domains is
discovered through language. On the other hand, LaMDA's responses in early
object and action understanding, theory of mind, and especially causal
reasoning tasks are very different from those of young children, perhaps
showing that these domains require more real-world, self-initiated exploration
and cannot simply be learned from patterns in language input.</p>
  </details>
</details>
<details>
  <summary>99. <b>标题：Comparing Biases and the Impact of Multilingual Training across Multiple  Languages</b></summary>
  <p><b>编号</b>：[310]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11242</p>
  <p><b>作者</b>：Sharon Levy,  Neha Anna John,  Ling Liu,  Yogarshi Vyas,  Jie Ma,  Yoshinari Fujinuma,  Miguel Ballesteros,  Vittorio Castelli,  Dan Roth</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：primarily examined social, examined social biases, natural language processing, fairness in natural, processing have primarily</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Studies in bias and fairness in natural language processing have primarily
examined social biases within a single language and/or across few attributes
(e.g. gender, race). However, biases can manifest differently across various
languages for individual attributes. As a result, it is critical to examine
biases within each language and attribute. Of equal importance is to study how
these biases compare across languages and how the biases are affected when
training a model on multilingual data versus monolingual data. We present a
bias analysis across Italian, Chinese, English, Hebrew, and Spanish on the
downstream sentiment analysis task to observe whether specific demographics are
viewed more positively. We study bias similarities and differences across these
languages and investigate the impact of multilingual vs. monolingual training
data. We adapt existing sentiment bias templates in English to Italian,
Chinese, Hebrew, and Spanish for four attributes: race, religion, nationality,
and gender. Our results reveal similarities in bias expression such as
favoritism of groups that are dominant in each language's culture (e.g.
majority religions and nationalities). Additionally, we find an increased
variation in predictions across protected groups, indicating bias
amplification, after multilingual finetuning in comparison to multilingual
pretraining.</p>
  </details>
</details>
<details>
  <summary>100. <b>标题：Recent Trends in Unsupervised Summarization</b></summary>
  <p><b>编号</b>：[314]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11231</p>
  <p><b>作者</b>：Mohammad Khosravani,  Amine Trabelsi</p>
  <p><b>备注</b>：13 pages, 1 figure</p>
  <p><b>关键词</b>：enables training summarizing, requiring labeled datasets, requiring labeled, Unsupervised summarization, powerful technique</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Unsupervised summarization is a powerful technique that enables training
summarizing models without requiring labeled datasets. This survey covers
different recent techniques and models used for unsupervised summarization. We
cover extractive, abstractive, and hybrid models and strategies used to achieve
unsupervised summarization. While the main focus of this survey is on recent
research, we also cover some of the important previous research. We
additionally introduce a taxonomy, classifying different research based on
their approach to unsupervised training. Finally, we discuss the current
approaches and mention some datasets and evaluation methods.</p>
  </details>
</details>
<details>
  <summary>101. <b>标题：LIMA: Less Is More for Alignment</b></summary>
  <p><b>编号</b>：[318]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11206</p>
  <p><b>作者</b>：Chunting Zhou,  Pengfei Liu,  Puxin Xu,  Srini Iyer,  Jiao Sun,  Yuning Mao,  Xuezhe Ma,  Avia Efrat,  Ping Yu,  Lili Yu,  Susan Zhang,  Gargi Ghosh,  Mike Lewis,  Luke Zettlemoyer,  Omer Levy</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：learn general-purpose representations, Large language models, raw text, general-purpose representations, reinforcement learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models are trained in two stages: (1) unsupervised pretraining
from raw text, to learn general-purpose representations, and (2) large scale
instruction tuning and reinforcement learning, to better align to end tasks and
user preferences. We measure the relative importance of these two stages by
training LIMA, a 65B parameter LLaMa language model fine-tuned with the
standard supervised loss on only 1,000 carefully curated prompts and responses,
without any reinforcement learning or human preference modeling. LIMA
demonstrates remarkably strong performance, learning to follow specific
response formats from only a handful of examples in the training data,
including complex queries that range from planning trip itineraries to
speculating about alternate history. Moreover, the model tends to generalize
well to unseen tasks that did not appear in the training data. In a controlled
human study, responses from LIMA are either equivalent or strictly preferred to
GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard
and 65% versus DaVinci003, which was trained with human feedback. Taken
together, these results strongly suggest that almost all knowledge in large
language models is learned during pretraining, and only limited instruction
tuning data is necessary to teach models to produce high quality output.</p>
  </details>
</details>
<details>
  <summary>102. <b>标题：Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM  Inference with Transferable Prompt</b></summary>
  <p><b>编号</b>：[327]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11186</p>
  <p><b>作者</b>：Zhaozhuo Xu,  Zirui Liu,  Beidi Chen,  Yuxin Tang,  Jue Wang,  Kaixiong Zhou,  Xia Hu,  Anshumali Shrivastava</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Natural Language Processing, Language Processing, Natural Language, exhibit exceptional performance, range of Natural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs), armed with billions of parameters, exhibit
exceptional performance across a wide range of Natural Language Processing
(NLP) tasks. However, they present a significant computational challenge during
inference, especially when deploying on common hardware such as single GPUs. As
such, minimizing the latency of LLM inference by curtailing computational and
memory requirements, though achieved through compression, becomes critically
important. However, this process inevitably instigates a trade-off between
efficiency and accuracy, as compressed LLMs typically experience a reduction in
predictive precision. In this research, we introduce an innovative perspective:
to optimize this trade-off, compressed LLMs require a unique input format that
varies from that of the original models. Our findings indicate that the
generation quality in a compressed LLM can be markedly improved for specific
queries by selecting prompts with precision. Capitalizing on this insight, we
introduce a prompt learning paradigm that cultivates an additive prompt over a
compressed LLM to bolster their accuracy. Our empirical results imply that
through our strategic prompt utilization, compressed LLMs can match, and
occasionally even exceed, the accuracy of the original models. Moreover, we
demonstrated that these learned prompts have a certain degree of
transferability across various datasets, tasks, and compression levels. These
insights shine a light on new possibilities for enhancing the balance between
accuracy and efficiency in LLM inference. Specifically, they underscore the
importance of judicious input editing to a compressed large model, hinting at
potential advancements in scaling LLMs on common hardware.</p>
  </details>
</details>
<details>
  <summary>103. <b>标题：North Sámi Dialect Identification with Self-supervised Speech Models</b></summary>
  <p><b>编号</b>：[332]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11864</p>
  <p><b>作者</b>：Sofoklis Kakouros,  Katri Hiovain-Asikainen</p>
  <p><b>备注</b>：Accepted at Interspeech 2023</p>
  <p><b>关键词</b>：North Sámi, primary dialectal variants, state language, encapsulates four primary, primary dialectal</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The North Sámi (NS) language encapsulates four primary dialectal variants
that are related but that also have differences in their phonology, morphology,
and vocabulary. The unique geopolitical location of NS speakers means that in
many cases they are bilingual in Sámi as well as in the dominant state
language: Norwegian, Swedish, or Finnish. This enables us to study the NS
variants both with respect to the spoken state language and their acoustic
characteristics. In this paper, we investigate an extensive set of acoustic
features, including MFCCs and prosodic features, as well as state-of-the-art
self-supervised representations, namely, XLS-R, WavLM, and HuBERT, for the
automatic detection of the four NS variants. In addition, we examine how the
majority state language is reflected in the dialects. Our results show that NS
dialects are influenced by the state language and that the four dialects are
separable, reaching high classification accuracy, especially with the XLS-R
model.</p>
  </details>
</details>
<details>
  <summary>104. <b>标题：Recycle-and-Distill: Universal Compression Strategy for  Transformer-based Speech SSL Models with Attention Map Reusing and Masking  Distillation</b></summary>
  <p><b>编号</b>：[343]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11685</p>
  <p><b>作者</b>：Kangwook Jang,  Sungnyun Kim,  Se-Young Yun,  Hoirin Kim</p>
  <p><b>备注</b>：Interspeech 2023. Code URL: this https URL</p>
  <p><b>关键词</b>：speech self-supervised learning, Transformer-based speech self-supervised, self-supervised learning, speech SSL models, Transformer-based speech</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transformer-based speech self-supervised learning (SSL) models, such as
HuBERT, show surprising performance in various speech processing tasks.
However, huge number of parameters in speech SSL models necessitate the
compression to a more compact model for wider usage in academia or small
companies. In this study, we suggest to reuse attention maps across the
Transformer layers, so as to remove key and query parameters while retaining
the number of layers. Furthermore, we propose a novel masking distillation
strategy to improve the student model's speech representation quality. We
extend the distillation loss to utilize both masked and unmasked speech frames
to fully leverage the teacher model's high-quality representation. Our
universal compression strategy yields the student model that achieves phoneme
error rate (PER) of 7.72% and word error rate (WER) of 9.96% on the SUPERB
benchmark.</p>
  </details>
</details>
<details>
  <summary>105. <b>标题：Language-universal phonetic encoder for low-resource speech recognition</b></summary>
  <p><b>编号</b>：[346]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11576</p>
  <p><b>作者</b>：Siyuan Feng,  Ming Tu,  Rui Xia,  Chuanzeng Huang,  Yuxuan Wang</p>
  <p><b>备注</b>：Accepted for publication in INTERSPEECH 2023</p>
  <p><b>关键词</b>：improving low-resource ASR, partially be explained, phonetic representation sharing, International Phonetic Alphabet, low-resource ASR</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multilingual training is effective in improving low-resource ASR, which may
partially be explained by phonetic representation sharing between languages. In
end-to-end (E2E) ASR systems, graphemes are often used as basic modeling units,
however graphemes may not be ideal for multilingual phonetic sharing. In this
paper, we leverage International Phonetic Alphabet (IPA) based
language-universal phonetic model to improve low-resource ASR performances, for
the first time within the attention encoder-decoder architecture. We propose an
adaptation method on the phonetic IPA model to further improve the proposed
approach on extreme low-resource languages. Experiments carried out on the
open-source MLS corpus and our internal databases show our approach outperforms
baseline monolingual models and most state-of-the-art works. Our main approach
and adaptation are effective on extremely low-resource languages, even within
domain- and language-mismatched scenarios.</p>
  </details>
</details>
<details>
  <summary>106. <b>标题：Language-Universal Phonetic Representation in Multilingual Speech  Pretraining for Low-Resource Speech Recognition</b></summary>
  <p><b>编号</b>：[348]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11569</p>
  <p><b>作者</b>：Siyuan Feng,  Ming Tu,  Rui Xia,  Chuanzeng Huang,  Yuxuan Wang</p>
  <p><b>备注</b>：Accepted for publication in INTERSPEECH 2023</p>
  <p><b>关键词</b>：improve low-resource ASR, low-resource ASR, ASR by integrating, self-supervised learning, improve low-resource</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We improve low-resource ASR by integrating the ideas of multilingual training
and self-supervised learning. Concretely, we leverage an International Phonetic
Alphabet (IPA) multilingual model to create frame-level pseudo labels for
unlabeled speech, and use these pseudo labels to guide hidden-unit BERT
(HuBERT) based speech pretraining in a phonetically-informed manner. The
experiments on the Multilingual Speech (MLS) Corpus show that the proposed
approach consistently outperforms the standard HuBERT on all the target
languages. Moreover, on 3 of the 4 languages, comparing to the standard HuBERT,
the approach performs better, meanwhile is able to save supervised training
data by 1.5k hours (75%) at most. Our approach outperforms most of the state of
the arts, with much less pretraining data in terms of hours and language
diversity. Compared to XLSR-53 and a retraining based multilingual method, our
approach performs better with full and limited finetuning data scenarios.</p>
  </details>
</details>
<details>
  <summary>107. <b>标题：Blank-regularized CTC for Frame Skipping in Neural Transducer</b></summary>
  <p><b>编号</b>：[349]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11558</p>
  <p><b>作者</b>：Yifan Yang,  Xiaoyu Yang,  Liyong Guo,  Zengwei Yao,  Wei Kang,  Fangjun Kuang,  Long Lin,  Xie Chen,  Daniel Povey</p>
  <p><b>备注</b>：Accepted in INTERSPEECH 2023</p>
  <p><b>关键词</b>：connectionist temporal classification, automatic speech recognition, speech recognition systems, temporal classification, automatic speech</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural Transducer and connectionist temporal classification (CTC) are popular
end-to-end automatic speech recognition systems. Due to their frame-synchronous
design, blank symbols are introduced to address the length mismatch between
acoustic frames and output tokens, which might bring redundant computation.
Previous studies managed to accelerate the training and inference of neural
Transducers by discarding frames based on the blank symbols predicted by a
co-trained CTC. However, there is no guarantee that the co-trained CTC can
maximize the ratio of blank symbols. This paper proposes two novel
regularization methods to explicitly encourage more blanks by constraining the
self-loop of non-blank symbols in the CTC. It is interesting to find that the
frame reduction ratio of the neural Transducer can approach the theoretical
boundary. Experiments on LibriSpeech corpus show that our proposed method
accelerates the inference of neural Transducer by 4 times without sacrificing
performance. Our work is open-sourced and publicly available
this https URL.</p>
  </details>
</details>
<details>
  <summary>108. <b>标题：Syllable Discovery and Cross-Lingual Generalization in a Visually  Grounded, Self-Supervised Speech Mode</b></summary>
  <p><b>编号</b>：[355]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11435</p>
  <p><b>作者</b>：Puyuan Peng,  Shang-Wen Li,  Okko Räsänen,  Abdelrahman Mohamed,  David Harwath</p>
  <p><b>备注</b>：Interspeech 2023. Code & Model: this https URL</p>
  <p><b>关键词</b>：visually-grounded training objective, representations capturing syllabic, capturing syllabic units, syllabic units emerge, visually-grounded training</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we show that representations capturing syllabic units emerge
when training a self-supervised speech model with a visually-grounded training
objective. We demonstrate that a nearly identical model architecture (HuBERT)
trained with a masked language modeling loss does not exhibit this same
ability, suggesting that the visual grounding objective is responsible for the
emergence of this phenomenon. We propose the use of a minimum cut algorithm to
automatically predict syllable boundaries in speech, followed by a 2-stage
clustering method to group identical syllables together. We show that our model
not only outperforms a state-of-the-art syllabic segmentation method on the
language it was trained on (English), but also generalizes in a zero-shot
fashion to Estonian. Finally, we show that the same model is capable of
zero-shot generalization for a word segmentation task on 4 other languages from
the Zerospeech Challenge, in some cases beating the previous state-of-the-art.</p>
  </details>
</details>
<h1>机器学习</h1>
<details>
  <summary>1. <b>标题：Multimodal Web Navigation with Instruction-Finetuned Foundation Models</b></summary>
  <p><b>编号</b>：[11]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11854</p>
  <p><b>作者</b>：Hiroki Furuta,  Ofir Nachum,  Kuang-Huei Lee,  Yutaka Matsuo,  Shixiang Shane Gu,  Izzeddin Gur</p>
  <p><b>备注</b>：Website: this https URL</p>
  <p><b>关键词</b>：online reinforcement learning, domain-specific model designs, autonomous web navigation, reinforcement learning, generalization from rich</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The progress of autonomous web navigation has been hindered by the dependence
on billions of exploratory interactions via online reinforcement learning, and
domain-specific model designs that make it difficult to leverage generalization
from rich out-of-domain data. In this work, we study data-driven offline
training for web agents with vision-language foundation models. We propose an
instruction-following multimodal agent, WebGUM, that observes both webpage
screenshots and HTML pages and outputs web navigation actions, such as click
and type. WebGUM is trained by jointly finetuning an instruction-finetuned
language model and a vision transformer on a large corpus of demonstrations. We
empirically demonstrate this recipe improves the agent's ability of grounded
visual perception, HTML comprehension and multi-step reasoning, outperforming
prior works by a significant margin. On the MiniWoB benchmark, we improve over
the previous best offline methods by more than 31.9%, being close to reaching
online-finetuned SoTA. On the WebShop benchmark, our 3-billion-parameter model
achieves superior performance to the existing SoTA, PaLM-540B. We also collect
347K high-quality demonstrations using our trained models, 38 times larger than
prior work, and make them available to promote future research in this
direction.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Any-to-Any Generation via Composable Diffusion</b></summary>
  <p><b>编号</b>：[13]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11846</p>
  <p><b>作者</b>：Zineng Tang,  Ziyi Yang,  Chenguang Zhu,  Michael Zeng,  Mohit Bansal</p>
  <p><b>备注</b>：Project Page: this https URL</p>
  <p><b>关键词</b>：generative model capable, model capable, capable of generating, modalities, generative model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present Composable Diffusion (CoDi), a novel generative model capable of
generating any combination of output modalities, such as language, image,
video, or audio, from any combination of input modalities. Unlike existing
generative AI systems, CoDi can generate multiple modalities in parallel and
its input is not limited to a subset of modalities like text or image. Despite
the absence of training datasets for many combinations of modalities, we
propose to align modalities in both the input and output space. This allows
CoDi to freely condition on any input combination and generate any group of
modalities, even if they are not present in the training data. CoDi employs a
novel composable generation strategy which involves building a shared
multimodal space by bridging alignment in the diffusion process, enabling the
synchronized generation of intertwined modalities, such as temporally aligned
video and audio. Highly customizable and flexible, CoDi achieves strong
joint-modality generation quality, and outperforms or is on par with the
unimodal state-of-the-art for single-modality synthesis. The project page with
demonstrations and code is at this https URL</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：AI's Regimes of Representation: A Community-centered Study of  Text-to-Image Models in South Asia</b></summary>
  <p><b>编号</b>：[15]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11844</p>
  <p><b>作者</b>：Rida Qadri,  Renee Shelby,  Cynthia L. Bennett,  Emily Denton</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：South Asian context, South Asian cultures, South Asian, Asian context, viewing South Asian</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a community-centered study of cultural limitations of
text-to-image (T2I) models in the South Asian context. We theorize these
failures using scholarship on dominant media regimes of representations and
locate them within participants' reporting of their existing social
marginalizations. We thus show how generative AI can reproduce an outsiders
gaze for viewing South Asian cultures, shaped by global and regional power
inequities. By centering communities as experts and soliciting their
perspectives on T2I limitations, our study adds rich nuance into existing
evaluative frameworks and deepens our understanding of the culturally-specific
ways AI technologies can fail in non-Western and Global South settings. We
distill lessons for responsible development of T2I models, recommending
concrete pathways forward that can allow for recognition of structural
inequalities.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Complexity of Neural Network Training and ETR: Extensions with  Effectively Continuous Functions</b></summary>
  <p><b>编号</b>：[20]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11833</p>
  <p><b>作者</b>：Teemu Hankala,  Miika Hannula,  Juha Kontinen,  Jonni Virtema</p>
  <p><b>备注</b>：Revised version of a manuscript sent for review in April 2023</p>
  <p><b>关键词</b>：sigmoid activation function, activation functions, activation, training problem, sigmoid activation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the complexity of the problem of training neural networks defined
via various activation functions. The training problem is known to be
existsR-complete with respect to linear activation functions and the ReLU
activation function. We consider the complexity of the problem with respect to
the sigmoid activation function and other effectively continuous functions. We
show that these training problems are polynomial-time many-one bireducible to
the existential theory of the reals extended with the corresponding activation
functions. In particular, we establish that the sigmoid activation function
leads to the existential theory of the reals with the exponential function. It
is thus open, and equivalent with the decidability of the existential theory of
the reals with the exponential function, whether training neural networks using
the sigmoid activation function is algorithmically solvable. In contrast, we
obtain that the training problem is undecidable if sinusoidal activation
functions are considered. Finally, we obtain general upper bounds for the
complexity of the training problem in the form of low levels of the
arithmetical hierarchy.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Regularization of Soft Actor-Critic Algorithms with Automatic  Temperature Adjustment</b></summary>
  <p><b>编号</b>：[21]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11831</p>
  <p><b>作者</b>：Ben You</p>
  <p><b>备注</b>：This work aims to clarify the ambiguity and revise certain errors in the original soft actor-cirtic articles</p>
  <p><b>关键词</b>：regularize the Soft, work presents, presents a comprehensive, comprehensive analysis, analysis to regularize</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This work presents a comprehensive analysis to regularize the Soft
Actor-Critic (SAC) algorithm with automatic temperature adjustment. The the
policy evaluation, the policy improvement and the temperature adjustment are
reformulated, addressing certain modification and enhancing the clarity of the
original theory in a more explicit manner.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：On the Fairness Impacts of Private Ensembles Models</b></summary>
  <p><b>编号</b>：[30]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11807</p>
  <p><b>作者</b>：Cuong Tran,  Ferdinando Fioretto</p>
  <p><b>备注</b>：This version is a "full version" of the associated IJCAI-23 article. arXiv admin note: substantial text overlap with arXiv:2109.08630</p>
  <p><b>关键词</b>：machine learning framework, Teacher Ensembles, Private Aggregation, combination of multiple, machine learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The Private Aggregation of Teacher Ensembles (PATE) is a machine learning
framework that enables the creation of private models through the combination
of multiple "teacher" models and a "student" model. The student model learns to
predict an output based on the voting of the teachers, and the resulting model
satisfies differential privacy. PATE has been shown to be effective in creating
private models in semi-supervised settings or when protecting data labels is a
priority. This paper explores whether the use of PATE can result in unfairness,
and demonstrates that it can lead to accuracy disparities among groups of
individuals. The paper also analyzes the algorithmic and data properties that
contribute to these disproportionate impacts, why these aspects are affecting
different groups disproportionately, and offers recommendations for mitigating
these effects</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：The probability flow ODE is provably fast</b></summary>
  <p><b>编号</b>：[32]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11798</p>
  <p><b>作者</b>：Sitan Chen,  Sinho Chewi,  Holden Lee,  Yuanzhi Li,  Jianfeng Lu,  Adil Salim</p>
  <p><b>备注</b>：23 pages, 2 figures</p>
  <p><b>关键词</b>：probability flow ODE, polynomial-time convergence guarantees, flow ODE implementation, score-based generative modeling, polynomial-time convergence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We provide the first polynomial-time convergence guarantees for the
probability flow ODE implementation (together with a corrector step) of
score-based generative modeling. Our analysis is carried out in the wake of
recent results obtaining such guarantees for the SDE-based implementation
(i.e., denoising diffusion probabilistic modeling or DDPM), but requires the
development of novel techniques for studying deterministic dynamics without
contractivity. Through the use of a specially chosen corrector step based on
the underdamped Langevin diffusion, we obtain better dimension dependence than
prior works on DDPM ($O(\sqrt{d})$ vs. $O(d)$, assuming smoothness of the data
distribution), highlighting potential advantages of the ODE framework.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Implicit Bias of Gradient Descent for Logistic Regression at the Edge of  Stability</b></summary>
  <p><b>编号</b>：[38]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11788</p>
  <p><b>作者</b>：Jingfeng Wu,  Vladimir Braverman,  Jason D. Lee</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：machine learning optimization, non-monotonic losses induced, gradient descent, Recent research, learning optimization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent research has observed that in machine learning optimization, gradient
descent (GD) often operates at the edge of stability (EoS) [Cohen, et al.,
2021], where the stepsizes are set to be large, resulting in non-monotonic
losses induced by the GD iterates. This paper studies the convergence and
implicit bias of constant-stepsize GD for logistic regression on linearly
separable data in the EoS regime. Despite the presence of local oscillations,
we prove that the logistic loss can be minimized by GD with any constant
stepsize over a long time scale. Furthermore, we prove that with any constant
stepsize, the GD iterates tend to infinity when projected to a max-margin
direction (the hard-margin SVM direction) and converge to a fixed vector that
minimizes a strongly convex potential when projected to the orthogonal
complement of the max-margin direction. In contrast, we also show that in the
EoS regime, GD iterates may diverge catastrophically under the exponential
loss, highlighting the superiority of the logistic loss. These theoretical
findings are in line with numerical simulations and complement existing
theories on the convergence and implicit bias of GD, which are only applicable
when the stepsizes are sufficiently small.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Cross-Lingual Supervision improves Large Language Models Pre-training</b></summary>
  <p><b>编号</b>：[42]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11778</p>
  <p><b>作者</b>：Andrea Schioppa,  Xavier Garcia,  Orhan Firat</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：recent rapid progress, pre-training Large Language, Machine Translation Systems, self-supervised language modeling, Large Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The recent rapid progress in pre-training Large Language Models has relied on
using self-supervised language modeling objectives like next token prediction
or span corruption. On the other hand, Machine Translation Systems are mostly
trained using cross-lingual supervision that requires aligned data between
source and target languages. We demonstrate that pre-training Large Language
Models on a mixture of a self-supervised Language Modeling objective and the
supervised Machine Translation objective, therefore including cross-lingual
parallel data during pre-training, yields models with better in-context
learning abilities. As pre-training is a very resource-intensive process and a
grid search on the best mixing ratio between the two objectives is
prohibitively expensive, we propose a simple yet effective strategy to learn it
during pre-training.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Tester-Learners for Halfspaces: Universal Algorithms</b></summary>
  <p><b>编号</b>：[46]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11765</p>
  <p><b>作者</b>：Aravind Gollakota,  Adam R. Klivans,  Konstantinos Stavropoulos,  Arsen Vasilyan</p>
  <p><b>备注</b>：26 pages, 2 figures</p>
  <p><b>关键词</b>：Poincaré distributions, distributions, log-concave distributions, Poincaré distributions includes, tester accepts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We give the first tester-learner for halfspaces that succeeds universally
over a wide class of structured distributions. Our universal tester-learner
runs in fully polynomial time and has the following guarantee: the learner
achieves error $O(\mathrm{opt}) + \epsilon$ on any labeled distribution that
the tester accepts, and moreover, the tester accepts whenever the marginal is
any distribution that satisfies a Poincaré inequality. In contrast to prior
work on testable learning, our tester is not tailored to any single target
distribution but rather succeeds for an entire target class of distributions.
The class of Poincaré distributions includes all strongly log-concave
distributions, and, assuming the Kannan--Lóvasz--Simonovits (KLS)
conjecture, includes all log-concave distributions. In the special case where
the label noise is known to be Massart, our tester-learner achieves error
$\mathrm{opt} + \epsilon$ while accepting all log-concave distributions
unconditionally (without assuming KLS). Our tests rely on checking
hypercontractivity of the unknown distribution using a sum-of-squares (SOS)
program, and crucially make use of the fact that Poincaré distributions are
certifiably hypercontractive in the SOS framework.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Marginalized Beam Search Algorithms for Hierarchical HMMs</b></summary>
  <p><b>编号</b>：[50]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11752</p>
  <p><b>作者</b>：Xuechun Xu,  Joakim Jaldén</p>
  <p><b>备注</b>：20 pages, submitted to Elsevier Pattern Recognition journal</p>
  <p><b>关键词</b>：natural language processing, Hidden Markov Models, Hierarchical Hidden Markov, language processing, outer state sequence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Inferring a state sequence from a sequence of measurements is a fundamental
problem in bioinformatics and natural language processing. The Viterbi and the
Beam Search (BS) algorithms are popular inference methods, but they have
limitations when applied to Hierarchical Hidden Markov Models (HHMMs), where
the interest lies in the outer state sequence. The Viterbi algorithm can not
infer outer states without inner states, while the BS algorithm requires
marginalization over prohibitively large state spaces. We propose two new
algorithms to overcome these limitations: the greedy marginalized BS algorithm
and the local focus BS algorithm. We show that they approximate the most likely
outer state sequence with higher performance than the Viterbi algorithm, and we
evaluate the performance of these algorithms on an explicit duration HMM with
simulation and nanopore base calling data.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：MedLens: Improve mortality prediction via medical signs selecting and  regression interpolation</b></summary>
  <p><b>编号</b>：[54]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11742</p>
  <p><b>作者</b>：Xuesong Ye,  Jun Wu,  Chengjie Mou,  Weinan Dai</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：care and treatment, predicting mortality, mortality in advance, timely care, medical signs</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Monitoring the health status of patients and predicting mortality in advance
is vital for providing patients with timely care and treatment. Massive medical
signs in electronic health records (EHR) are fitted into advanced machine
learning models to make predictions. However, the data-quality problem of
original clinical signs is less discussed in the literature. Based on an
in-depth measurement of the missing rate and correlation score across various
medical signs and a large amount of patient hospital admission records, we
discovered the comprehensive missing rate is extremely high, and a large number
of useless signs could hurt the performance of prediction models. Then we
concluded that only improving data-quality could improve the baseline accuracy
of different prediction algorithms. We designed MEDLENS, with an automatic
vital medical signs selection approach via statistics and a flexible
interpolation approach for high missing rate time series. After augmenting the
data-quality of original medical signs, MEDLENS applies ensemble classifiers to
boost the accuracy and reduce the computation overhead at the same time. It
achieves a very high accuracy performance of 0.96% AUC-ROC and 0.81% AUC-PR,
which exceeds the previous benchmark.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Non-stationary Projection-free Online Learning with Dynamic and Adaptive  Regret Guarantees</b></summary>
  <p><b>编号</b>：[63]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11726</p>
  <p><b>作者</b>：Yibo Wang,  Wenhao Yang,  Wei Jiang,  Shiyin Lu,  Bing Wang,  Haihong Tang,  Yuanyu Wan,  Lijun Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：drawn increasing interest, increasing interest due, solving high-dimensional problems, Projection-free online learning, Projection-free online</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Projection-free online learning has drawn increasing interest due to its
efficiency in solving high-dimensional problems with complicated constraints.
However, most existing projection-free online methods focus on minimizing the
static regret, which unfortunately fails to capture the challenge of changing
environments. In this paper, we investigate non-stationary projection-free
online learning, and choose dynamic regret and adaptive regret to measure the
performance. Specifically, we first provide a novel dynamic regret analysis for
an existing projection-free method named $\text{BOGD}_\text{IP}$, and establish
an $\mathcal{O}(T^{3/4}(1+P_T))$ dynamic regret bound, where $P_T$ denotes the
path-length of the comparator sequence. Then, we improve the upper bound to
$\mathcal{O}(T^{3/4}(1+P_T)^{1/4})$ by running multiple $\text{BOGD}_\text{IP}$
algorithms with different step sizes in parallel, and tracking the best one on
the fly. Our results are the first general-case dynamic regret bounds for
projection-free online learning, and can recover the existing
$\mathcal{O}(T^{3/4})$ static regret by setting $P_T = 0$. Furthermore, we
propose a projection-free method to attain an $\tilde{\mathcal{O}}(\tau^{3/4})$
adaptive regret bound for any interval with length $\tau$, which nearly matches
the static regret over that interval. The essential idea is to maintain a set
of $\text{BOGD}_\text{IP}$ algorithms dynamically, and combine them by a meta
algorithm. Moreover, we demonstrate that it is also equipped with an
$\mathcal{O}(T^{3/4}(1+P_T)^{1/4})$ dynamic regret bound. Finally, empirical
studies verify our theoretical findings.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：What Comes Next? Evaluating Uncertainty in Neural Text Generators  Against Human Production Variability</b></summary>
  <p><b>编号</b>：[70]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11707</p>
  <p><b>作者</b>：Mario Giulianelli,  Joris Baan,  Wilker Aziz,  Raquel Fernández,  Barbara Plank</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Natural Language Generation, Natural Language, Language Generation, multiple communicative goals, put into words</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In Natural Language Generation (NLG) tasks, for any input, multiple
communicative goals are plausible, and any goal can be put into words, or
produced, in multiple ways. We characterise the extent to which human
production varies lexically, syntactically, and semantically across four NLG
tasks, connecting human production variability to aleatoric or data
uncertainty. We then inspect the space of output strings shaped by a generation
system's predicted probability distribution and decoding algorithm to probe its
uncertainty. For each test input, we measure the generator's calibration to
human production variability. Following this instance-level approach, we
analyse NLG models and decoding strategies, demonstrating that probing a
generator with multiple samples and, when possible, multiple references,
provides the level of detail necessary to gain understanding of a model's
representation of uncertainty.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：S-JEA: Stacked Joint Embedding Architectures for Self-Supervised Visual  Representation Learning</b></summary>
  <p><b>编号</b>：[71]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11701</p>
  <p><b>作者</b>：Alžběta Manová,  Aiden Durrant,  Georgios Leontidis</p>
  <p><b>备注</b>：9 pages, 4 figures, 3 tables</p>
  <p><b>关键词</b>：demonstrate high empirical, high empirical success, learning image representations, Self-Supervised Learning, learning image</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The recent emergence of Self-Supervised Learning (SSL) as a fundamental
paradigm for learning image representations has, and continues to, demonstrate
high empirical success in a variety of tasks. However, most SSL approaches fail
to learn embeddings that capture hierarchical semantic concepts that are
separable and interpretable. In this work, we aim to learn highly separable
semantic hierarchical representations by stacking Joint Embedding Architectures
(JEA) where higher-level JEAs are input with representations of lower-level
JEA. This results in a representation space that exhibits distinct
sub-categories of semantic concepts (e.g., model and colour of vehicles) in
higher-level JEAs. We empirically show that representations from stacked JEA
perform on a similar level as traditional JEA with comparative parameter counts
and visualise the representation spaces to validate the semantic hierarchies.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：RGCVAE: Relational Graph Conditioned Variational Autoencoder for  Molecule Design</b></summary>
  <p><b>编号</b>：[73]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11699</p>
  <p><b>作者</b>：Davide Rigoni,  Nicolò Navarin,  Alessandro Sperduti</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Graph Variational Autoencoders, Deep Graph Variational, exhibit some pre-specified, pre-specified properties, Graph Variational</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Identifying molecules that exhibit some pre-specified properties is a
difficult problem to solve. In the last few years, deep generative models have
been used for molecule generation. Deep Graph Variational Autoencoders are
among the most powerful machine learning tools with which it is possible to
address this problem. However, existing methods struggle in capturing the true
data distribution and tend to be computationally expensive. In this work, we
propose RGCVAE, an efficient and effective Graph Variational Autoencoder based
on: (i) an encoding network exploiting a new powerful Relational Graph
Isomorphism Network; (ii) a novel probabilistic decoding component. Compared to
several state-of-the-art VAE methods on two widely adopted datasets, RGCVAE
shows state-of-the-art molecule generation performance while being
significantly faster to train.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Surgical-VQLA: Transformer with Gated Vision-Language Embedding for  Visual Question Localized-Answering in Robotic Surgery</b></summary>
  <p><b>编号</b>：[75]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11692</p>
  <p><b>作者</b>：Long Bai,  Mobarakol Islam,  Lalithkumar Seenivasan,  Hongliang Ren</p>
  <p><b>备注</b>：To appear in IEEE ICRA 2023. Code and data availability: this https URL</p>
  <p><b>关键词</b>：junior residents, availability of computer-aided, computer-aided simulators, residents still heavily, heavily rely</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite the availability of computer-aided simulators and recorded videos of
surgical procedures, junior residents still heavily rely on experts to answer
their queries. However, expert surgeons are often overloaded with clinical and
academic workloads and limit their time in answering. For this purpose, we
develop a surgical question-answering system to facilitate robot-assisted
surgical scene and activity understanding from recorded videos. Most of the
existing VQA methods require an object detector and regions based feature
extractor to extract visual features and fuse them with the embedded text of
the question for answer generation. However, (1) surgical object detection
model is scarce due to smaller datasets and lack of bounding box annotation;
(2) current fusion strategy of heterogeneous modalities like text and image is
naive; (3) the localized answering is missing, which is crucial in complex
surgical scenarios. In this paper, we propose Visual Question
Localized-Answering in Robotic Surgery (Surgical-VQLA) to localize the specific
surgical area during the answer prediction. To deal with the fusion of the
heterogeneous modalities, we design gated vision-language embedding (GVLE) to
build input patches for the Language Vision Transformer (LViT) to predict the
answer. To get localization, we add the detection head in parallel with the
prediction head of the LViT. We also integrate GIoU loss to boost localization
performance by preserving the accuracy of the question-answering model. We
annotate two datasets of VQLA by utilizing publicly available surgical videos
from MICCAI challenges EndoVis-17 and 18. Our validation results suggest that
Surgical-VQLA can better understand the surgical scene and localize the
specific area related to the question-answering. GVLE presents an efficient
language-vision embedding technique by showing superior performance over the
existing benchmarks.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Self-Reinforcement Attention Mechanism For Tabular Learning</b></summary>
  <p><b>编号</b>：[76]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11684</p>
  <p><b>作者</b>：Kodjo Mawuena Amekoe,  Mohamed Djallel Dilmi,  Hanene Azzag,  Mustapha Lebbah,  Zaineb Chelly Dagdia,  Gregoire Jaffre</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：find hidden patterns, challenging imbalanced characteristics, fraud detection, credit scoring, machine learning models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Apart from the high accuracy of machine learning models, what interests many
researchers in real-life problems (e.g., fraud detection, credit scoring) is to
find hidden patterns in data; particularly when dealing with their challenging
imbalanced characteristics. Interpretability is also a key requirement that
needs to accompany the used machine learning model. In this concern, often,
intrinsically interpretable models are preferred to complex ones, which are in
most cases black-box models. Also, linear models are used in some high-risk
fields to handle tabular data, even if performance must be sacrificed. In this
paper, we introduce Self-Reinforcement Attention (SRA), a novel attention
mechanism that provides a relevance of features as a weight vector which is
used to learn an intelligible representation. This weight is then used to
reinforce or reduce some components of the raw input through element-wise
vector multiplication. Our results on synthetic and real-world imbalanced data
show that our proposed SRA block is effective in end-to-end combination with
baseline models.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Sensing of inspiration events from speech: comparison of deep learning  and linguistic methods</b></summary>
  <p><b>编号</b>：[77]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11683</p>
  <p><b>作者</b>：Aki Härmä,  Ulf Grossekathöfer,  Okke Ouweltjes,  Venkata Srikanth Nallanthighal</p>
  <p><b>备注</b>：8 pages</p>
  <p><b>关键词</b>：respiratory health parameters, Respiratory chest belt, chest belt sensor, respiratory belt sensor, belt sensor</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Respiratory chest belt sensor can be used to measure the respiratory rate and
other respiratory health parameters. Virtual Respiratory Belt, VRB, algorithms
estimate the belt sensor waveform from speech audio. In this paper we compare
the detection of inspiration events (IE) from respiratory belt sensor data
using a novel neural VRB algorithm and the detections based on time-aligned
linguistic content. The results show the superiority of the VRB method over
word pause detection or grammatical content segmentation. The comparison of the
methods show that both read and spontaneous speech content has a significant
amount of ungrammatical breathing, that is, breathing events that are not
aligned with grammatically appropriate places in language. This study gives new
insights into the development of VRB methods and adds to the general
understanding of speech breathing behavior. Moreover, a new VRB method, VRBOLA,
for the reconstruction of the continuous breathing waveform is demonstrated.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Probabilistic Lexicase Selection</b></summary>
  <p><b>编号</b>：[78]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11681</p>
  <p><b>作者</b>：Li Ding,  Edward Pantridge,  Lee Spector</p>
  <p><b>备注</b>：GECCO 2023</p>
  <p><b>关键词</b>：Lexicase selection, parent selection algorithm, selection, machine learning, Lexicase</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Lexicase selection is a widely used parent selection algorithm in genetic
programming, known for its success in various task domains such as program
synthesis, symbolic regression, and machine learning. Due to its non-parametric
and recursive nature, calculating the probability of each individual being
selected by lexicase selection has been proven to be an NP-hard problem, which
discourages deeper theoretical understanding and practical improvements to the
algorithm. In this work, we introduce probabilistic lexicase selection
(plexicase selection), a novel parent selection algorithm that efficiently
approximates the probability distribution of lexicase selection. Our method not
only demonstrates superior problem-solving capabilities as a semantic-aware
selection method, but also benefits from having a probabilistic representation
of the selection process for enhanced efficiency and flexibility. Experiments
are conducted in two prevalent domains in genetic programming: program
synthesis and symbolic regression, using standard benchmarks including PSB and
SRBench. The empirical results show that plexicase selection achieves
state-of-the-art problem-solving performance that is competitive to the
lexicase selection, and significantly outperforms lexicase selection in
computation efficiency.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：A Generic Performance Model for Deep Learning in a Distributed  Environment</b></summary>
  <p><b>编号</b>：[85]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11665</p>
  <p><b>作者</b>：Tulasi Kavarakuntla,  Liangxiu Han,  Huw Lloyd,  Annabel Latham,  Anthony Kleerekoper,  Samson B. Akintoye</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：deep learning frameworks, deep learning application, learning frameworks, deep learning, essential to improve</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Performance modelling of a deep learning application is essential to improve
and quantify the efficiency of the model framework. However, existing
performance models are mostly case-specific, with limited capability for the
new deep learning frameworks/applications. In this paper, we propose a generic
performance model of an application in a distributed environment with a generic
expression of the application execution time that considers the influence of
both intrinsic factors/operations (e.g. algorithmic parameters/internal
operations) and extrinsic scaling factors (e.g. the number of processors, data
chunks and batch size). We formulate it as a global optimization problem and
solve it using regularization on a cost function and differential evolution
algorithm to find the best-fit values of the constants in the generic
expression to match the experimentally determined computation time. We have
evaluated the proposed model on three deep learning frameworks (i.e.,
TensorFlow, MXnet, and Pytorch). The experimental results show that the
proposed model can provide accurate performance predictions and
interpretability. In addition, the proposed work can be applied to any
distributed deep neural network without instrumenting the code and provides
insight into the factors affecting performance and scalability.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Algorithmic failure as a humanities methodology: machine learning's  mispredictions identify rich cases for qualitative analysis</b></summary>
  <p><b>编号</b>：[87]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11663</p>
  <p><b>作者</b>：Jill Walker Rettberg</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：machine learning, methodology proposed, machine learning algorithm, machine vision technologies, machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This commentary tests a methodology proposed by Munk et al. (2022) for using
failed predictions in machine learning as a method to identify ambiguous and
rich cases for qualitative analysis. Using a dataset describing actions
performed by fictional characters interacting with machine vision technologies
in 500 artworks, movies, novels and videogames, I trained a simple machine
learning algorithm (using the kNN algorithm in R) to predict whether or not an
action was active or passive using only information about the fictional
characters. Predictable actions were generally unemotional and unambiguous
activities where machine vision technologies were treated as simple tools.
Unpredictable actions, that is, actions that the algorithm could not correctly
predict, were more ambivalent and emotionally loaded, with more complex power
relationships between characters and technologies. The results thus support
Munk et al.'s theory that failed predictions can be productively used to
identify rich cases for qualitative analysis. This test goes beyond simply
replicating Munk et al.'s results by demonstrating that the method can be
applied to a broader humanities domain, and that it does not require complex
neural networks but can also work with a simpler machine learning algorithm.
Further research is needed to develop an understanding of what kinds of data
the method is useful for and which kinds of machine learning are most
generative. To support this, the R code required to produce the results is
included so the test can be replicated. The code can also be reused or adapted
to test the method on other datasets.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：V2X-Boosted Federated Learning for Cooperative Intelligent  Transportation Systems with Contextual Client Selection</b></summary>
  <p><b>编号</b>：[92]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11654</p>
  <p><b>作者</b>：Rui Song,  Lingjuan Lyu,  Wei Jiang,  Andreas Festag,  Alois Knoll</p>
  <p><b>备注</b>：Accepted at ICRA 2023 Workshop on Collaborative Perception and Learning</p>
  <p><b>关键词</b>：enabling autonomous driving, revolutionized transportation systems, smart traffic services, enabling autonomous, Machine learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine learning (ML) has revolutionized transportation systems, enabling
autonomous driving and smart traffic services. Federated learning (FL)
overcomes privacy constraints by training ML models in distributed systems,
exchanging model parameters instead of raw data. However, the dynamic states of
connected vehicles affect the network connection quality and influence the FL
performance. To tackle this challenge, we propose a contextual client selection
pipeline that uses Vehicle-to-Everything (V2X) messages to select clients based
on the predicted communication latency. The pipeline includes: (i) fusing V2X
messages, (ii) predicting future traffic topology, (iii) pre-clustering clients
based on local data distribution similarity, and (iv) selecting clients with
minimal latency for future model aggregation. Experiments show that our
pipeline outperforms baselines on various datasets, particularly in non-iid
settings.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Distribution-Free Matrix Prediction Under Arbitrary Missing Pattern</b></summary>
  <p><b>编号</b>：[99]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11640</p>
  <p><b>作者</b>：Meijia Shao,  Yuan Zhang</p>
  <p><b>备注</b>：12 pages, 4 figures</p>
  <p><b>关键词</b>：conformalized entry prediction, paper studies, studies the open, conformalized entry, column-exchangeable matrix</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper studies the open problem of conformalized entry prediction in a
row/column-exchangeable matrix. The matrix setting presents novel and unique
challenges, but there exists little work on this interesting topic. We
meticulously define the problem, differentiate it from closely related
problems, and rigorously delineate the boundary between achievable and
impossible goals. We then propose two practical algorithms. The first method
provides a fast emulation of the full conformal prediction, while the second
method leverages the technique of algorithmic stability for acceleration. Both
methods are computationally efficient and can effectively safeguard coverage
validity in presence of arbitrary missing pattern. Further, we quantify the
impact of missingness on prediction accuracy and establish fundamental limit
results. Empirical evidence from synthetic and real-world data sets
corroborates the superior performance of our proposed methods.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：A Path to Holistic Privacy in Stream Processing Systems</b></summary>
  <p><b>编号</b>：[101]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11638</p>
  <p><b>作者</b>：Mikhail Fomichev</p>
  <p><b>备注</b>：Extended Abstract accepted to MobiSys 2023</p>
  <p><b>关键词</b>：Internet of Things, retain data usefulness, require a timely, timely analysis, analysis to retain</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The massive streams of Internet of Things (IoT) data require a timely
analysis to retain data usefulness. Stream processing systems (SPSs) enable
this task, deriving knowledge from the IoT data in real-time. Such real-time
analytics benefits many applications but can also be used to violate user
privacy, as the IoT data collected from users or their vicinity is inherently
sensitive. In this paper, we present our systematic look into privacy issues
arising from the intersection of SPSs and IoT, identifying key research
challenges towards achieving holistic privacy protection in SPSs and proposing
the solutions.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Goal-Oriented Communications in Federated Learning via Feedback on  Risk-Averse Participation</b></summary>
  <p><b>编号</b>：[103]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11633</p>
  <p><b>作者</b>：Shashi Raj Pandey,  Van Phuc Bui,  Petar Popovski</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：goal-oriented communication problem, Federated Learning, formulate a goal-oriented, Parameter Server, treat the problem</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We treat the problem of client selection in a Federated Learning (FL) setup,
where the learning objective and the local incentives of the participants are
used to formulate a goal-oriented communication problem. Specifically, we
incorporate the risk-averse nature of participants and obtain a
communication-efficient on-device performance, while relying on feedback from
the Parameter Server (\texttt{PS}). A client has to decide its transmission
plan on when not to participate in FL. This is based on its intrinsic
incentive, which is the value of the trained global model upon participation by
this client. Poor updates not only plunge the performance of the global model
with added communication cost but also propagate the loss in performance on
other participating devices. We cast the relevance of local updates as
\emph{semantic information} for developing local transmission strategies, i.e.,
making a decision on when to ``not transmit". The devices use feedback about
the state of the PS and evaluate their contributions in training the learning
model in each aggregation period, which eventually lowers the number of
occupied connections. Simulation results validate the efficacy of our proposed
approach, with up to $1.4\times$ gain in communication links utilization as
compared with the baselines.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD  Detection, Calibration, and Accuracy</b></summary>
  <p><b>编号</b>：[111]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11616</p>
  <p><b>作者</b>：Stanislav Dereka,  Ivan Karpukhin,  Sergey Kolesnikov</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：effectiveness remains limited, remains limited due, Deep ensembles achieved, effectiveness remains, remains limited</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep ensembles achieved state-of-the-art results in classification and
out-of-distribution (OOD) detection; however, their effectiveness remains
limited due to the homogeneity of learned patterns within the ensemble. To
overcome this challenge, our study introduces a novel approach that promotes
diversity among ensemble members by leveraging saliency maps. By incorporating
saliency map diversification, our method outperforms conventional ensemble
techniques in multiple classification and OOD detection tasks, while also
improving calibration. Experiments on well-established OpenOOD benchmarks
highlight the potential of our method in practical applications.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：SFP: Spurious Feature-targeted Pruning for Out-of-Distribution  Generalization</b></summary>
  <p><b>编号</b>：[112]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11615</p>
  <p><b>作者</b>：ingchun Wang,  Jingcai Guo,  Yi Liu,  Song Guo,  Weizhan Zhang,  Xiangyong Cao,  Qinghua Zheng</p>
  <p><b>备注</b>：14 pages, 4 figures. arXiv admin note: substantial text overlap with arXiv:2212.09458</p>
  <p><b>关键词</b>：Sub-optimal OOD generalization, OOD generalization, OOD, spurious features, substructure learning aims</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Model substructure learning aims to find an invariant network substructure
that can have better out-of-distribution (OOD) generalization than the original
full structure. Existing works usually search the invariant substructure using
modular risk minimization (MRM) with fully exposed out-domain data, which may
bring about two drawbacks: 1) Unfairness, due to the dependence of the full
exposure of out-domain data; and 2) Sub-optimal OOD generalization, due to the
equally feature-untargeted pruning on the whole data distribution. Based on the
idea that in-distribution (ID) data with spurious features may have a lower
experience risk, in this paper, we propose a novel Spurious Feature-targeted
model Pruning framework, dubbed SFP, to automatically explore invariant
substructures without referring to the above drawbacks. Specifically, SFP
identifies spurious features within ID instances during training using our
theoretically verified task loss, upon which, SFP attenuates the corresponding
feature projections in model space to achieve the so-called spurious
feature-targeted pruning. This is typically done by removing network branches
with strong dependencies on identified spurious features, thus SFP can push the
model learning toward invariant features and pull that out of spurious features
and devise optimal OOD generalization. Moreover, we also conduct detailed
theoretical analysis to provide the rationality guarantee and a proof framework
for OOD structures via model sparsity, and for the first time, reveal how a
highly biased data distribution affects the model's OOD generalization.
Experiments on various OOD datasets show that SFP can significantly outperform
both structure-based and non-structure-based OOD generalization SOTAs, with
accuracy improvement up to 4.72% and 23.35%, respectively</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：MIDI-Draw: Sketching to Control Melody Generation</b></summary>
  <p><b>编号</b>：[113]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11605</p>
  <p><b>作者</b>：Tashi Namgyal,  Peter Flach,  Raul Santos-Rodriguez</p>
  <p><b>备注</b>：Late-Breaking / Demo Session Extended Abstract, ISMIR 2022 Conference</p>
  <p><b>关键词</b>：note-level input representation, melodies that abstracts, note-level input, input representation, representation via melodic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We describe a proof-of-principle implementation of a system for drawing
melodies that abstracts away from a note-level input representation via melodic
contours. The aim is to allow users to express their musical intentions without
requiring prior knowledge of how notes fit together melodiously. Current
approaches to controllable melody generation often require users to choose
parameters that are static across a whole sequence, via buttons or sliders. In
contrast, our method allows users to quickly specify how parameters should
change over time by drawing a contour.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Latent Imitator: Generating Natural Individual Discriminatory Instances  for Black-Box Fairness Testing</b></summary>
  <p><b>编号</b>：[115]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11602</p>
  <p><b>作者</b>：Yisong Xiao,  Aishan Liu,  Tianlin Li,  Xianglong Liu</p>
  <p><b>备注</b>：Accepted by ISSTA 2023</p>
  <p><b>关键词</b>：achieved remarkable performance, Machine learning, systems have achieved, achieved remarkable, remarkable performance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine learning (ML) systems have achieved remarkable performance across a
wide area of applications. However, they frequently exhibit unfair behaviors in
sensitive application domains, raising severe fairness concerns. To evaluate
and test fairness, engineers often generate individual discriminatory instances
to expose unfair behaviors before model deployment. However, existing baselines
ignore the naturalness of generation and produce instances that deviate from
the real data distribution, which may fail to reveal the actual model fairness
since these unnatural discriminatory instances are unlikely to appear in
practice. To address the problem, this paper proposes a framework named Latent
Imitator (LIMI) to generate more natural individual discriminatory instances
with the help of a generative adversarial network (GAN), where we imitate the
decision boundary of the target model in the semantic latent space of GAN and
further samples latent instances on it. Specifically, we first derive a
surrogate linear boundary to coarsely approximate the decision boundary of the
target model, which reflects the nature of the original data distribution.
Subsequently, to obtain more natural instances, we manipulate random latent
vectors to the surrogate boundary with a one-step movement, and further conduct
vector calculation to probe two potential discriminatory candidates that may be
more closely located in the real decision boundary. Extensive experiments on
various datasets demonstrate that our LIMI outperforms other baselines largely
in effectiveness ($\times$9.42 instances), efficiency ($\times$8.71 speeds),
and naturalness (+19.65%) on average. In addition, we empirically demonstrate
that retraining on test samples generated by our approach can lead to
improvements in both individual fairness (45.67% on $IF_r$ and 32.81% on
$IF_o$) and group fairness (9.86% on $SPD$ and 28.38% on $AOD$}).</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Vision-based DRL Autonomous Driving Agent with Sim2Real Transfer</b></summary>
  <p><b>编号</b>：[123]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11589</p>
  <p><b>作者</b>：Dianzhao Li,  Ostap Okhrin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：fully autonomous driving, achieve fully autonomous, including lane keeping, autonomous driving, well-studied driving</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To achieve fully autonomous driving, vehicles must be capable of continuously
performing various driving tasks, including lane keeping and car following,
both of which are fundamental and well-studied driving ones. However, previous
studies have mainly focused on individual tasks, and car following tasks have
typically relied on complete leader-follower information to attain optimal
performance. To address this limitation, we propose a vision-based deep
reinforcement learning (DRL) agent that can simultaneously perform lane keeping
and car following maneuvers. To evaluate the performance of our DRL agent, we
compare it with a baseline controller and use various performance metrics for
quantitative analysis. Furthermore, we conduct a real-world evaluation to
demonstrate the Sim2Real transfer capability of the trained DRL agent. To the
best of our knowledge, our vision-based car following and lane keeping agent
with Sim2Real transfer capability is the first of its kind.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Bayesian approach to Gaussian process regression with uncertain inputs</b></summary>
  <p><b>编号</b>：[125]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11586</p>
  <p><b>作者</b>：Dongwei Ye,  Mengwu Guo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Conventional Gaussian process, Gaussian process regression, regression exclusively assumes, Gaussian process, process regression exclusively</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Conventional Gaussian process regression exclusively assumes the existence of
noise in the output data of model observations. In many scientific and
engineering applications, however, the input locations of observational data
may also be compromised with uncertainties owing to modeling assumptions,
measurement errors, etc. In this work, we propose a Bayesian method that
integrates the variability of input data into Gaussian process regression.
Considering two types of observables -- noise-corrupted outputs with fixed
inputs and those with prior-distribution-defined uncertain inputs, a posterior
distribution is estimated via a Bayesian framework to infer the uncertain data
locations. Thereafter, such quantified uncertainties of inputs are incorporated
into Gaussian process predictions by means of marginalization. The
effectiveness of this new regression technique is demonstrated through several
numerical examples, in which a consistently good performance of generalization
is observed, while a substantial reduction in the predictive uncertainties is
achieved by the Bayesian inference of uncertain inputs.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Dynamic Regularized Sharpness Aware Minimization in Federated Learning:  Approaching Global Consistency and Smooth Landscape</b></summary>
  <p><b>编号</b>：[126]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11584</p>
  <p><b>作者</b>：Yan Sun,  Li Shen,  Shixiang Chen,  Liang Ding,  Dacheng Tao</p>
  <p><b>备注</b>：ICML2023, Oral Presentation</p>
  <p><b>关键词</b>：federated learning, privacy protection, global Sharpness Aware, server and cooperatively, cooperatively train</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In federated learning (FL), a cluster of local clients are chaired under the
coordination of the global server and cooperatively train one model with
privacy protection. Due to the multiple local updates and the isolated non-iid
dataset, clients are prone to overfit into their own optima, which extremely
deviates from the global objective and significantly undermines the
performance. Most previous works only focus on enhancing the consistency
between the local and global objectives to alleviate this prejudicial client
drifts from the perspective of the optimization view, whose performance would
be prominently deteriorated on the high heterogeneity. In this work, we propose
a novel and general algorithm {\ttfamily FedSMOO} by jointly considering the
optimization and generalization targets to efficiently improve the performance
in FL. Concretely, {\ttfamily FedSMOO} adopts a dynamic regularizer to
guarantee the local optima towards the global objective, which is meanwhile
revised by the global Sharpness Aware Minimization (SAM) optimizer to search
for the consistent flat minima. Our theoretical analysis indicates that
{\ttfamily FedSMOO} achieves fast $\mathcal{O}(1/T)$ convergence rate with low
generalization bound. Extensive numerical studies are conducted on the
real-world dataset to verify its peerless efficiency and excellent generality.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：What You Hear Is What You See: Audio Quality Metrics From Image Quality  Metrics</b></summary>
  <p><b>编号</b>：[128]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11582</p>
  <p><b>作者</b>：Tashi Namgyal,  Alexander Hepburn,  Raul Santos-Rodriguez,  Valero Laparra,  Jesus Malo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：image perceptual metrics, evaluating audio signals, feasibility of utilizing, image perceptual, investigate the feasibility</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this study, we investigate the feasibility of utilizing state-of-the-art
image perceptual metrics for evaluating audio signals by representing them as
spectrograms. The encouraging outcome of the proposed approach is based on the
similarity between the neural mechanisms in the auditory and visual pathways.
Furthermore, we customise one of the metrics which has a psychoacoustically
plausible architecture to account for the peculiarities of sound signals. We
evaluate the effectiveness of our proposed metric and several baseline metrics
using a music dataset, with promising results in terms of the correlation
between the metrics and the perceived quality of audio as rated by human
evaluators.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：TSGM: A Flexible Framework for Generative Modeling of Synthetic Time  Series</b></summary>
  <p><b>编号</b>：[134]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11567</p>
  <p><b>作者</b>：Alexander Nikitin,  Letizia Iannucci,  Samuel Kaski</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Temporally indexed data, Temporally indexed, wide range, range of fields, Time series</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Temporally indexed data are essential in a wide range of fields and of
interest to machine learning researchers. Time series data, however, are often
scarce or highly sensitive, which precludes the sharing of data between
researchers and industrial organizations and the application of existing and
new data-intensive ML methods. A possible solution to this bottleneck is to
generate synthetic data. In this work, we introduce Time Series Generative
Modeling (TSGM), an open-source framework for the generative modeling of
synthetic time series. TSGM includes a broad repertoire of machine learning
methods: generative models, probabilistic, and simulator-based approaches. The
framework enables users to evaluate the quality of the produced data from
different angles: similarity, downstream effectiveness, predictive consistency,
diversity, and privacy. The framework is extensible, which allows researchers
to rapidly implement their own methods and compare them in a shareable
environment. TSGM was tested on open datasets and in production and proved to
be beneficial in both cases. Additionally to the library, the project allows
users to employ command line interfaces for synthetic data generation which
lowers the entry threshold for those without a programming background.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via  Tool Embeddings</b></summary>
  <p><b>编号</b>：[140]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11554</p>
  <p><b>作者</b>：Shibo Hao,  Tianyang Liu,  Zhen Wang,  Zhiting Hu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Augmenting large language, large language models, solving complex problems, language models, tools</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Augmenting large language models (LLMs) with external tools has emerged as a
promising approach to solving complex problems. However, traditional methods,
which finetune LLMs with tool demonstration data, can be both costly and
restricted to a predefined set of tools. Recent in-context learning paradigm
alleviates these issues, but the limited context length only allows for a few
shots of demonstrations, leading to suboptimal understandings of the tools.
Moreover, when there are numerous tools to choose from, in-context learning
could completely fail to work. In this paper, we propose an alternative
approach, $\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our
approach represents each $\underline{tool}$ as a to$\underline{ken}$
($\textit{toolken}$) and learns an embedding for it, enabling tool calls in the
same way as generating a regular word token. Once a toolken is triggered, the
LLM is prompted to complete arguments for the tool to execute. ToolkenGPT
offers the flexibility to plug in an arbitrary number of tools by expanding the
set of toolkens on the fly. In addition, it improves tool use by allowing
extensive demonstration data for learning the toolken embeddings. In diverse
domains, including numerical reasoning, knowledge-based question answering, and
embodied plan generation, our approach effectively augments LLMs with tools and
substantially outperforms various latest baselines. ToolkenGPT demonstrates the
promising ability to use relevant tools from a large tool set in complex
scenarios.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：A Sequence-to-Sequence Approach for Arabic Pronoun Resolution</b></summary>
  <p><b>编号</b>：[154]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11529</p>
  <p><b>作者</b>：Hanan S. Murayshid,  Hafida Benhidour,  Said Kerrache</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：BERT pre-trained Language, natural language processing, pre-trained Language Model, advanced natural language, BERT pre-trained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper proposes a sequence-to-sequence learning approach for Arabic
pronoun resolution, which explores the effectiveness of using advanced natural
language processing (NLP) techniques, specifically Bi-LSTM and the BERT
pre-trained Language Model, in solving the pronoun resolution problem in
Arabic. The proposed approach is evaluated on the AnATAr dataset, and its
performance is compared to several baseline models, including traditional
machine learning models and handcrafted feature-based models. Our results
demonstrate that the proposed model outperforms the baseline models, which
include KNN, logistic regression, and SVM, across all metrics. In addition, we
explore the effectiveness of various modifications to the model, including
concatenating the anaphor text beside the paragraph text as input, adding a
mask to focus on candidate scores, and filtering candidates based on gender and
number agreement with the anaphor. Our results show that these modifications
significantly improve the model's performance, achieving up to 81% on MRR and
71% for F1 score while also demonstrating higher precision, recall, and
accuracy. These findings suggest that the proposed model is an effective
approach to Arabic pronoun resolution and highlights the potential benefits of
leveraging advanced NLP neural models.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：InstructIE: A Chinese Instruction-based Information Extraction Dataset</b></summary>
  <p><b>编号</b>：[155]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11527</p>
  <p><b>作者</b>：Honghao Gui,  Jintian Zhang,  Hongbin Ye,  Ningyu Zhang</p>
  <p><b>备注</b>：Work in progress</p>
  <p><b>关键词</b>：Information Extraction, task dubbed Instruction-based, Extraction, dubbed Instruction-based, extract information</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce a new Information Extraction (IE) task dubbed Instruction-based
IE, which aims to ask the system to follow specific instructions or guidelines
to extract information. To facilitate research in this area, we construct a
dataset called InstructIE, consisting of 270,000 weakly supervised data from
Chinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We
further evaluate the performance of various baseline models on the InstructIE
dataset. The results reveal that although current models exhibit promising
performance, there is still room for improvement. Furthermore, we conduct a
comprehensive case study analysis, underlining the challenges inherent in the
Instruction-based IE task. Code and dataset are available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：Enhancing Short-Term Wind Speed Forecasting using Graph Attention and  Frequency-Enhanced Mechanisms</b></summary>
  <p><b>编号</b>：[156]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11526</p>
  <p><b>作者</b>：Hao Liu,  Tianyu Hu</p>
  <p><b>备注</b>：9 pages, 6 figures</p>
  <p><b>关键词</b>：wind speed forecasting, wind speed, Wind power forecasting, wind, wind power</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The safe and stable operation of power systems is greatly challenged by the
high variability and randomness of wind power in large-scale
wind-power-integrated grids. Wind power forecasting is an effective solution to
tackle this issue, with wind speed forecasting being an essential aspect. In
this paper, a Graph-attentive Frequency-enhanced Spatial-Temporal Wind Speed
Forecasting model based on graph attention and frequency-enhanced mechanisms,
i.e., GFST-WSF, is proposed to improve the accuracy of short-term wind speed
forecasting. The GFST-WSF comprises a Transformer architecture for temporal
feature extraction and a Graph Attention Network (GAT) for spatial feature
extraction. The GAT is specifically designed to capture the complex spatial
dependencies among wind speed stations to effectively aggregate information
from neighboring nodes in the graph, thus enhancing the spatial representation
of the data. To model the time lag in wind speed correlation between adjacent
wind farms caused by geographical factors, a dynamic complex adjacency matrix
is formulated and utilized by the GAT. Benefiting from the effective
spatio-temporal feature extraction and the deep architecture of the
Transformer, the GFST-WSF outperforms other baselines in wind speed forecasting
for the 6-24 hours ahead forecast horizon in case studies.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Enriching Disentanglement: Definitions to Metrics</b></summary>
  <p><b>编号</b>：[163]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11512</p>
  <p><b>作者</b>：Yivan Zhang,  Masashi Sugiyama</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：involves separating multiple, separating multiple factors, challenging task, task that involves, involves separating</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Disentangled representation learning is a challenging task that involves
separating multiple factors of variation in complex data. Although various
metrics for learning and evaluating disentangled representations have been
proposed, it remains unclear what these metrics truly quantify and how to
compare them. In this work, we study the definitions of disentanglement given
by first-order equational predicates and introduce a systematic approach for
transforming an equational definition into a compatible quantitative metric
based on enriched category theory. Specifically, we show how to replace (i)
equality with metric or divergence, (ii) logical connectives with order
operations, (iii) universal quantifier with aggregation, and (iv) existential
quantifier with the best approximation. Using this approach, we derive metrics
for measuring the desired properties of a disentangled representation extractor
and demonstrate their effectiveness on synthetic data. Our proposed approach
provides practical guidance for researchers in selecting appropriate evaluation
metrics and designing effective learning algorithms for disentangled
representation learning.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：From Random Search to Bandit Learning in Metric Measure Spaces</b></summary>
  <p><b>编号</b>：[165]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11509</p>
  <p><b>作者</b>：Chuying Han,  Yasong Feng,  Tianyu Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Random Search, method for Hyperparameter, random search converges, Random, Search</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Random Search is one of the most widely-used method for Hyperparameter
Optimization, and is critical to the success of deep learning models. Despite
its astonishing performance, little non-heuristic theory has been developed to
describe the underlying working mechanism. This paper gives a theoretical
accounting of Random Search. We introduce the concept of \emph{scattering
dimension} that describes the landscape of the underlying function, and
quantifies the performance of random search. We show that, when the environment
is noise-free, the output of random search converges to the optimal value in
probability at rate $ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T}
\right)^{ \frac{1}{d_s} } \right) $, where $ d_s \ge 0 $ is the scattering
dimension of the underlying function. When the observed function values are
corrupted by bounded $iid$ noise, the output of random search converges to the
optimal value in probability at rate $ \widetilde{\mathcal{O}} \left( \left(
\frac{1}{T} \right)^{ \frac{1}{d_s + 1} } \right) $. In addition, based on the
principles of random search, we introduce an algorithm, called BLiN-MOS, for
Lipschitz bandits in doubling metric spaces that are also emdowed with a Borel
measure, and show that BLiN-MOS achieves a regret rate of order $
\widetilde{\mathcal{O}} \left( T^{ \frac{d_z}{d_z + 1} } \right) $, where $d_z$
is the zooming dimension of the problem instance. Our results show that in
metric spaces with a Borel measure, the classic theory of Lipschitz bandits can
be improved. This result suggests an intrinsic axiomatic gap between metric
spaces and metric measure spaces from an algorithmic perspective, since the
upper bound in a metric measure space breaks the known information-theoretical
lower bounds for Lipschitz bandits in a metric space with no measure structure.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Nonconvex Robust High-Order Tensor Completion Using Randomized Low-Rank  Approximation</b></summary>
  <p><b>编号</b>：[173]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11495</p>
  <p><b>作者</b>：Wenjin Qin,  Hailin Wang,  Feng Zhang,  Weijun Ma,  Jianjun Wang,  Tingwen Huang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：made great achievements, singular value decomposition, science and engineering, existing robust low-rank, made great</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Within the tensor singular value decomposition (T-SVD) framework, existing
robust low-rank tensor completion approaches have made great achievements in
various areas of science and engineering. Nevertheless, these methods involve
the T-SVD based low-rank approximation, which suffers from high computational
costs when dealing with large-scale tensor data. Moreover, most of them are
only applicable to third-order tensors. Against these issues, in this article,
two efficient low-rank tensor approximation approaches fusing randomized
techniques are first devised under the order-d (d >= 3) T-SVD framework. On
this basis, we then further investigate the robust high-order tensor completion
(RHTC) problem, in which a double nonconvex model along with its corresponding
fast optimization algorithms with convergence guarantees are developed. To the
best of our knowledge, this is the first study to incorporate the randomized
low-rank approximation into the RHTC problem. Empirical studies on large-scale
synthetic and real tensor data illustrate that the proposed method outperforms
other state-of-the-art approaches in terms of both computational efficiency and
estimated precision.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：LLM Itself Can Read and Generate CXR Images</b></summary>
  <p><b>编号</b>：[174]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11490</p>
  <p><b>作者</b>：Suhyeon Lee,  Won Jun Kim,  Jong Chul Ye</p>
  <p><b>备注</b>：12 pages, 4 figures</p>
  <p><b>关键词</b>：recent remarkable development, large language models, recent remarkable, remarkable development, development of large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Building on the recent remarkable development of large language models
(LLMs), active attempts are being made to extend the utility of LLMs to
multimodal tasks. There have been previous efforts to link language and visual
information, and attempts to add visual capabilities to LLMs are ongoing as
well. However, existing attempts use LLMs only as image decoders and no attempt
has been made to generate images in the same line as the natural language. By
adopting a VQ-GAN framework in which latent representations of images are
treated as a kind of text tokens, we present a novel method to fine-tune a
pre-trained LLM to read and generate images like text without any structural
changes, extra training objectives, or the need for training an ad-hoc network
while still preserving the of the instruction-following capability of the LLM.
We apply this framework to chest X-ray (CXR) image and report generation tasks
as it is a domain in which translation of complex information between visual
and language domains is important. The code will soon be made publicly
available.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Incomplete Multi-view Clustering via Diffusion Completion</b></summary>
  <p><b>编号</b>：[175]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11489</p>
  <p><b>作者</b>：Sifan Fang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Incomplete multi-view clustering, multi-view clustering, Incomplete multi-view, multi-view clustering framework, provide effective data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Incomplete multi-view clustering is a challenging and non-trivial task to
provide effective data analysis for large amounts of unlabeled data in the real
world. All incomplete multi-view clustering methods need to address the problem
of how to reduce the impact of missing views. To address this issue, we propose
diffusion completion to recover the missing views integrated into an incomplete
multi-view clustering framework. Based on the observable views information, the
diffusion model is used to recover the missing views, and then the consistency
information of the multi-view data is learned by contrastive learning to
improve the performance of multi-view clustering. To the best of our knowledge,
this may be the first work to incorporate diffusion models into an incomplete
multi-view clustering framework. Experimental results show that the proposed
method performs well in recovering the missing views while achieving superior
clustering performance compared to state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Learning Diverse Risk Preferences in Population-based Self-play</b></summary>
  <p><b>编号</b>：[183]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11476</p>
  <p><b>作者</b>：Yuhua Jiang,  Qihan Liu,  Xiaoteng Ma,  Chenghao Li,  Yiqin Yang,  Jun Yang,  Bin Liang,  Qianchuan Zhao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：self-play algorithms play, great successes, play an essential, essential role, role in solving</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Among the great successes of Reinforcement Learning (RL), self-play
algorithms play an essential role in solving competitive games. Current
self-play algorithms optimize the agent to maximize expected win-rates against
its current or historical copies, making it often stuck in the local optimum
and its strategy style simple and homogeneous. A possible solution is to
improve the diversity of policies, which helps the agent break the stalemate
and enhances its robustness when facing different opponents. However, enhancing
diversity in the self-play algorithms is not trivial. In this paper, we aim to
introduce diversity from the perspective that agents could have diverse risk
preferences in the face of uncertainty. Specifically, we design a novel
reinforcement learning algorithm called Risk-sensitive Proximal Policy
Optimization (RPPO), which smoothly interpolates between worst-case and
best-case policy learning and allows for policy learning with desired risk
preferences. Seamlessly integrating RPPO with population-based self-play,
agents in the population optimize dynamic risk-sensitive objectives with
experiences from playing against diverse opponents. Empirical results show that
our method achieves comparable or superior performance in competitive games and
that diverse modes of behaviors emerge. Our code is public online at
\url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Curve Your Enthusiasm: Concurvity Regularization in Differentiable  Generalized Additive Models</b></summary>
  <p><b>编号</b>：[184]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11475</p>
  <p><b>作者</b>：Julien Siems,  Konstantin Ditschuneit,  Winfried Ripken,  Alma Lindborg,  Maximilian Schambach,  Johannes S. Otterbach,  Martin Genzel</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Generalized Additive Models, Neural Additive Models, Additive Models, recently experienced, experienced a resurgence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generalized Additive Models (GAMs) have recently experienced a resurgence in
popularity due to their interpretability, which arises from expressing the
target value as a sum of non-linear transformations of the features. Despite
the current enthusiasm for GAMs, their susceptibility to concurvity - i.e.,
(possibly non-linear) dependencies between the features - has hitherto been
largely overlooked. Here, we demonstrate how concurvity can severly impair the
interpretability of GAMs and propose a remedy: a conceptually simple, yet
effective regularizer which penalizes pairwise correlations of the non-linearly
transformed feature variables. This procedure is applicable to any
differentiable additive model, such as Neural Additive Models or NeuralProphet,
and enhances interpretability by eliminating ambiguities due to self-canceling
feature contributions. We validate the effectiveness of our regularizer in
experiments on synthetic as well as real-world datasets for time-series and
tabular data. Our experiments show that concurvity in GAMs can be reduced
without significantly compromising prediction quality, improving
interpretability and reducing variance in the feature importances.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Generative Sliced MMD Flows with Riesz Kernels</b></summary>
  <p><b>编号</b>：[192]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11463</p>
  <p><b>作者</b>：Johannes Hertrich,  Christian Wald,  Fabian Altekrüger,  Paul Hagemann</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：high computational costs, Maximum mean discrepancy, MMD, suffer from high, high computational</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Maximum mean discrepancy (MMD) flows suffer from high computational costs in
large scale computations. In this paper, we show that MMD flows with Riesz
kernels $K(x,y) = - \|x-y\|^r$, $r \in (0,2)$ have exceptional properties which
allow for their efficient computation. First, the MMD of Riesz kernels
coincides with the MMD of their sliced version. As a consequence, the
computation of gradients of MMDs can be performed in the one-dimensional
setting. Here, for $r=1$, a simple sorting algorithm can be applied to reduce
the complexity from $O(MN+N^2)$ to $O((M+N)\log(M+N))$ for two empirical
measures with $M$ and $N$ support points. For the implementations we
approximate the gradient of the sliced MMD by using only a finite number $P$ of
slices. We show that the resulting error has complexity $O(\sqrt{d/P})$, where
$d$ is the data dimension. These results enable us to train generative models
by approximating MMD gradient flows by neural networks even for large scale
applications. We demonstrate the efficiency of our model by image generation on
MNIST, FashionMNIST and CIFAR10.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：A Novel Tensor Factorization-Based Method with Robustness to Inaccurate  Rank Estimation</b></summary>
  <p><b>编号</b>：[196]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11458</p>
  <p><b>作者</b>：Jingjing Zheng,  Wenzhe Wang,  Xiaoqin Zhang,  Xianta Jiang</p>
  <p><b>备注</b>：14 pages, 8 figures</p>
  <p><b>关键词</b>：factorization-based tensor recovery, large computational cost, tensor recovery, tensor factorization-based tensor, rank estimation strategy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This study aims to solve the over-reliance on the rank estimation strategy in
the standard tensor factorization-based tensor recovery and the problem of a
large computational cost in the standard t-SVD-based tensor recovery. To this
end, we proposes a new tensor norm with a dual low-rank constraint, which
utilizes the low-rank prior and rank information at the same time. In the
proposed tensor norm, a series of surrogate functions of the tensor tubal rank
can be used to achieve better performance in harness low-rankness within tensor
data. It is proven theoretically that the resulting tensor completion model can
effectively avoid performance degradation caused by inaccurate rank estimation.
Meanwhile, attributed to the proposed dual low-rank constraint, the t-SVD of a
smaller tensor instead of the original big one is computed by using a sample
trick. Based on this, the total cost at each iteration of the optimization
algorithm is reduced to $\mathcal{O}(n^3\log n +kn^3)$ from $\mathcal{O}(n^4)$
achieved with standard methods, where $k$ is the estimation of the true tensor
rank and far less than $n$. Our method was evaluated on synthetic and
real-world data, and it demonstrated superior performance and efficiency over
several existing state-of-the-art tensor completion methods.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Shattering the Agent-Environment Interface for Fine-Tuning Inclusive  Language Models</b></summary>
  <p><b>编号</b>：[198]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11455</p>
  <p><b>作者</b>：Wanqiao Xu,  Shi Dong,  Dilip Arumugam,  Benjamin Van Roy</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：emulate human feedback, human feedback, autoregressive language models, language model, emulate human</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A centerpiece of the ever-popular reinforcement learning from human feedback
(RLHF) approach to fine-tuning autoregressive language models is the explicit
training of a reward model to emulate human feedback, distinct from the
language model itself. This reward model is then coupled with policy-gradient
methods to dramatically improve the alignment between language model outputs
and desired responses. In this work, we adopt a novel perspective wherein a
pre-trained language model is itself simultaneously a policy, reward function,
and transition function. An immediate consequence of this is that reward
learning and language model fine-tuning can be performed jointly and directly,
without requiring any further downstream policy optimization. While this
perspective does indeed break the traditional agent-environment interface, we
nevertheless maintain that there can be enormous statistical benefits afforded
by bringing to bear traditional algorithmic concepts from reinforcement
learning. Our experiments demonstrate one concrete instance of this through
efficient exploration based on the representation and resolution of epistemic
uncertainty. In order to illustrate these ideas in a transparent manner, we
restrict attention to a simple didactic data generating process and leave for
future work extension to systems of practical scale.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Zero-Shot Text Classification via Self-Supervised Tuning</b></summary>
  <p><b>编号</b>：[205]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11442</p>
  <p><b>作者</b>：Chaoqun Liu,  Wenxuan Zhang,  Guizhen Chen,  Xiaobao Wu,  Anh Tuan Luu,  Chip Hong Chang,  Lidong Bing</p>
  <p><b>备注</b>：Accepted to the Findings of ACL 2023</p>
  <p><b>关键词</b>：zero-shot text classification, large-scale annotated data, text classification tasks, text classification, Existing solutions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing solutions to zero-shot text classification either conduct prompting
with pre-trained language models, which is sensitive to the choices of
templates, or rely on large-scale annotated data of relevant tasks for
meta-tuning. In this work, we propose a new paradigm based on self-supervised
learning to solve zero-shot text classification tasks by tuning the language
models with unlabeled data, called self-supervised tuning. By exploring the
inherent structure of free texts, we propose a new learning objective called
first sentence prediction to bridge the gap between unlabeled data and text
classification tasks. After tuning the model to learn to predict the first
sentence in a paragraph based on the rest, the model is able to conduct
zero-shot inference on unseen tasks such as topic classification and sentiment
analysis. Experimental results show that our model outperforms the
state-of-the-art baselines on 7 out of 10 tasks. Moreover, the analysis reveals
that our model is less sensitive to the prompt design. Our code and pre-trained
models are publicly available at this https URL .</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：PS-FedGAN: An Efficient Federated Learning Framework Based on Partially  Shared Generative Adversarial Networks For Data Privacy</b></summary>
  <p><b>编号</b>：[209]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11437</p>
  <p><b>作者</b>：Achintha Wijesinghe,  Songyang Zhang,  Zhi Ding</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：effective learning paradigm, distributed computation owing, Federated Learning, effective learning, learning paradigm</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Federated Learning (FL) has emerged as an effective learning paradigm for
distributed computation owing to its strong potential in capturing underlying
data statistics while preserving data privacy. However, in cases of practical
data heterogeneity among FL clients, existing FL frameworks still exhibit
deficiency in capturing the overall feature properties of local client data
that exhibit disparate distributions. In response, generative adversarial
networks (GANs) have recently been exploited in FL to address data
heterogeneity since GANs can be integrated for data regeneration without
exposing original raw data. Despite some successes, existing GAN-related FL
frameworks often incur heavy communication cost and also elicit other privacy
concerns, which limit their applications in real scenarios. To this end, this
work proposes a novel FL framework that requires only partial GAN model
sharing. Named as PS-FedGAN, this new framework enhances the GAN releasing and
training mechanism to address heterogeneous data distributions across clients
and to strengthen privacy preservation at reduced communication cost,
especially over wireless networks. Our analysis demonstrates the convergence
and privacy benefits of the proposed PS-FEdGAN framework. Through experimental
results based on several well-known benchmark datasets, our proposed PS-FedGAN
shows great promise to tackle FL under non-IID client data distributions, while
securing data privacy and lowering communication overhead.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks</b></summary>
  <p><b>编号</b>：[211]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11430</p>
  <p><b>作者</b>：Shubhra Kanti Karmaker Santu,  Dongji Feng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：traditional conversational settings, shown great success, performing ill-defined complex, conversational settings, largely under-studied</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While LLMs have shown great success in understanding and generating text in
traditional conversational settings, their potential for performing ill-defined
complex tasks is largely under-studied. Indeed, we are yet to conduct
comprehensive benchmarking studies with multiple LLMs that are exclusively
focused on a complex task. However, conducting such benchmarking studies is
challenging because of the large variations in LLMs' performance when different
prompt types/styles are used and different degrees of detail are provided in
the prompts. To address this issue, the paper proposes a general taxonomy that
can be used to design prompts with specific properties in order to perform a
wide range of complex tasks. This taxonomy will allow future benchmarking
studies to report the specific categories of prompts used as part of the study,
enabling meaningful comparisons across different studies. Also, by establishing
a common standard through this taxonomy, researchers will be able to draw more
accurate conclusions about LLMs' performance on a specific complex task.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Graph Propagation Transformer for Graph Representation Learning</b></summary>
  <p><b>编号</b>：[215]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11424</p>
  <p><b>作者</b>：Zhe Chen,  Hao Tan,  Tao Wang,  Tianrun Shen,  Tong Lu,  Qiuying Peng,  Cheng Cheng,  Yue Qi</p>
  <p><b>备注</b>：Accepted to IJCAI 2023</p>
  <p><b>关键词</b>：paper presents, graph, Graph Propagation Attention, Graph Propagation, propagation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a novel transformer architecture for graph representation
learning. The core insight of our method is to fully consider the information
propagation among nodes and edges in a graph when building the attention module
in the transformer blocks. Specifically, we propose a new attention mechanism
called Graph Propagation Attention (GPA). It explicitly passes the information
among nodes and edges in three ways, i.e. node-to-node, node-to-edge, and
edge-to-node, which is essential for learning graph-structured data. On this
basis, we design an effective transformer architecture named Graph Propagation
Transformer (GPTrans) to further help learn graph data. We verify the
performance of GPTrans in a wide range of graph learning experiments on several
benchmark datasets. These results show that our method outperforms many
state-of-the-art transformer-based graph models with better performance. The
code will be released at this https URL.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Beyond Exponential Graph: Communication-Efficient Topologies for  Decentralized Learning via Finite-time Convergence</b></summary>
  <p><b>编号</b>：[218]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11420</p>
  <p><b>作者</b>：Yuki Takezawa,  Ryoma Sato,  Han Bao,  Kenta Niwa,  Makoto Yamada</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：attracting increasing attention, fast consensus rate, consensus rate, Decentralized learning, privacy preservation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Decentralized learning has recently been attracting increasing attention for
its applications in parallel computation and privacy preservation. Many recent
studies stated that the underlying network topology with a faster consensus
rate (a.k.a. spectral gap) leads to a better convergence rate and accuracy for
decentralized learning. However, a topology with a fast consensus rate, e.g.,
the exponential graph, generally has a large maximum degree, which incurs
significant communication costs. Thus, seeking topologies with both a fast
consensus rate and small maximum degree is important. In this study, we propose
a novel topology combining both a fast consensus rate and small maximum degree
called the Base-$(k + 1)$ Graph. Unlike the existing topologies, the Base-$(k +
1)$ Graph enables all nodes to reach the exact consensus after a finite number
of iterations for any number of nodes and maximum degree k. Thanks to this
favorable property, the Base-$(k + 1)$ Graph endows Decentralized SGD (DSGD)
with both a faster convergence rate and more communication efficiency than the
exponential graph. We conducted experiments with various topologies,
demonstrating that the Base-$(k + 1)$ Graph enables various decentralized
learning methods to achieve higher accuracy with better communication
efficiency than the existing topologies.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：JetSeg: Efficient Real-Time Semantic Segmentation Model for Low-Power  GPU-Embedded Systems</b></summary>
  <p><b>编号</b>：[219]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11419</p>
  <p><b>作者</b>：Miguel Lopez-Montiel,  Daniel Alejandro Lopez,  Oscar Montiel</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Real-time semantic segmentation, requires high-accuracy models, semantic segmentation, challenging task, task that requires</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Real-time semantic segmentation is a challenging task that requires
high-accuracy models with low-inference times. Implementing these models on
embedded systems is limited by hardware capability and memory usage, which
produces bottlenecks. We propose an efficient model for real-time semantic
segmentation called JetSeg, consisting of an encoder called JetNet, and an
improved RegSeg decoder. The JetNet is designed for GPU-Embedded Systems and
includes two main components: a new light-weight efficient block called
JetBlock, that reduces the number of parameters minimizing memory usage and
inference time without sacrificing accuracy; a new strategy that involves the
combination of asymmetric and non-asymmetric convolutions with
depthwise-dilated convolutions called JetConv, a channel shuffle operation,
light-weight activation functions, and a convenient number of group
convolutions for embedded systems, and an innovative loss function named
JetLoss, which integrates the Precision, Recall, and IoUB losses to improve
semantic segmentation and reduce computational complexity. Experiments
demonstrate that JetSeg is much faster on workstation devices and more suitable
for Low-Power GPU-Embedded Systems than existing state-of-the-art models for
real-time semantic segmentation. Our approach outperforms state-of-the-art
real-time encoder-decoder models by reducing 46.70M parameters and 5.14%
GFLOPs, which makes JetSeg up to 2x faster on the NVIDIA Titan RTX GPU and the
Jetson Xavier than other models. The JetSeg code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：Complexity of Feed-Forward Neural Networks from the Perspective of  Functional Equivalence</b></summary>
  <p><b>编号</b>：[221]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11417</p>
  <p><b>作者</b>：Guohao Shen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：feed-forward neural networks, neural networks, neural networks leads, functional equivalence, examining the concept</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we investigate the complexity of feed-forward neural networks
by examining the concept of functional equivalence, which suggests that
different network parameterizations can lead to the same function. We utilize
the permutation invariance property to derive a novel covering number bound for
the class of feedforward neural networks, which reveals that the complexity of
a neural network can be reduced by exploiting this property. Furthermore, based
on the symmetric structure of parameter space, we demonstrate that an
appropriate strategy of random parameter initialization can increase the
probability of convergence for optimization. We found that overparameterized
networks tend to be easier to train in the sense that increasing the width of
neural networks leads to a vanishing volume of the effective parameter space.
Our findings offer new insights into overparameterization and have significant
implications for understanding generalization and optimization in deep
learning.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Federated Foundation Models: Privacy-Preserving and Collaborative  Learning for Large Models</b></summary>
  <p><b>编号</b>：[222]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11414</p>
  <p><b>作者</b>：Sixing Yu,  J. Pablo Muñoz,  Ali Jannesari</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：demonstrated remarkable success, leverage vast amounts, Federated Foundation Models, Foundation Models, Federated Foundation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Foundation Models (FMs), such as BERT, GPT, ViT, and CLIP, have demonstrated
remarkable success in a wide range of applications, driven by their ability to
leverage vast amounts of data for pre-training. However, optimizing FMs often
requires access to sensitive data, raising privacy concerns and limiting their
applicability in certain domains. In this paper, we introduce the concept of
Federated Foundation Models (FFMs), a novel approach that combines the benefits
of FMs and Federated Learning (FL) to enable privacy-preserving and
collaborative learning across multiple institutions. We discuss the potential
benefits and challenges of integrating FL into the lifespan of FMs, covering
pre-training, fine-tuning, and application. We further provide formal
definitions of FFM tasks, including FFM pre-training, FFM fine-tuning, and
federated prompt engineering, allowing for more personalized and context-aware
models while maintaining data privacy. Moreover, we explore the possibility of
continual/lifelong learning in FFMs, as increased computational power at the
edge unlocks the potential for optimizing FMs using newly generated private
data at edges. We present experiments and evaluations comparing the performance
of FFMs to traditional FMs on various downstream tasks, demonstrating the
effectiveness of our approach in preserving privacy, reducing overfitting, and
improving model generalizability. The proposed Federated Foundation Models
offer a flexible and scalable framework for training large language models in a
privacy-preserving manner, paving the way for future advancements in both FM
pre-training and federated learning.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：AlignAtt: Using Attention-based Audio-Translation Alignments as a Guide  for Simultaneous Speech Translation</b></summary>
  <p><b>编号</b>：[225]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11408</p>
  <p><b>作者</b>：Sara Papi,  Marco Turchi,  Matteo Negri</p>
  <p><b>备注</b>：Accepted at Interspeech 2023</p>
  <p><b>关键词</b>：machine translation-related tasks, natural language processing, including its effectiveness, core mechanism, mechanism of today</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Attention is the core mechanism of today's most used architectures for
natural language processing and has been analyzed from many perspectives,
including its effectiveness for machine translation-related tasks. Among these
studies, attention resulted to be a useful source of information to get
insights about word alignment also when the input text is substituted with
audio segments, as in the case of the speech translation (ST) task. In this
paper, we propose AlignAtt, a novel policy for simultaneous ST (SimulST) that
exploits the attention information to generate source-target alignments that
guide the model during inference. Through experiments on the 8 language pairs
of MuST-C v1.0, we show that AlignAtt outperforms previous state-of-the-art
SimulST policies applied to offline-trained models with gains in terms of BLEU
of 2 points and latency reductions ranging from 0.5s to 0.8s across the 8
languages.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Few-Shot Continual Learning for Conditional Generative Adversarial  Networks</b></summary>
  <p><b>编号</b>：[229]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11400</p>
  <p><b>作者</b>：Cat P. Le,  Juncheng Dong,  Ahmed Aloui,  Vahid Tarokh</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：previously learned modes, few-shot continual learning, continual learning, previously learned, continual learning model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In few-shot continual learning for generative models, a target mode must be
learned with limited samples without adversely affecting the previously learned
modes. In this paper, we propose a new continual learning approach for
conditional generative adversarial networks (cGAN) based on a new mode-affinity
measure for generative modeling. Our measure is entirely based on the cGAN's
discriminator and can identify the existing modes that are most similar to the
target. Subsequently, we expand the continual learning model by including the
target mode using a weighted label derived from those of the closest modes. To
prevent catastrophic forgetting, we first generate labeled data samples using
the cGAN's generator, and then train the cGAN model for the target mode while
memory replaying with the generated data. Our experimental results demonstrate
the efficacy of our approach in improving the generation performance over the
baselines and the state-of-the-art approaches for various standard datasets
while utilizing fewer training samples.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：A Survey of Safety and Trustworthiness of Large Language Models through  the Lens of Verification and Validation</b></summary>
  <p><b>编号</b>：[233]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11391</p>
  <p><b>作者</b>：Xiaowei Huang,  Wenjie Ruan,  Wei Huang,  Gaojie Jin,  Yi Dong,  Changshun Wu,  Saddek Bensalem,  Ronghui Mu,  Yi Qi,  Xingyu Zhao,  Kaiwen Cai,  Yanghao Zhang,  Sihao Wu,  Peipei Xu,  Dengyu Wu,  Andre Freitas,  Mustafa A. Mustafa</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Large Language, Language Models, ability to engage, engage end-users</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) have exploded a new heatwave of AI, for their
ability to engage end-users in human-level conversations with detailed and
articulate answers across many knowledge domains. In response to their fast
adoption in many industrial applications, this survey concerns their safety and
trustworthiness. First, we review known vulnerabilities of the LLMs,
categorising them into inherent issues, intended attacks, and unintended bugs.
Then, we consider if and how the Verification and Validation (V&V) techniques,
which have been widely developed for traditional software and deep learning
models such as convolutional neural networks, can be integrated and further
extended throughout the lifecycle of the LLMs to provide rigorous analysis to
the safety and trustworthiness of LLMs and their applications. Specifically, we
consider four complementary techniques: falsification and evaluation,
verification, runtime monitoring, and ethical use. Considering the fast
development of LLMs, this survey does not intend to be complete (although it
includes 300 references), especially when it comes to the applications of LLMs
in various domains, but rather a collection of organised literature reviews and
discussions to support the quick understanding of the safety and
trustworthiness issues from the perspective of V&V.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：ALT: An Automatic System for Long Tail Scenario Modeling</b></summary>
  <p><b>编号</b>：[234]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11390</p>
  <p><b>作者</b>：Ya-Lin Zhang,  Jun Zhou,  Yankun Ren,  Yue Zhang,  Xinxing Yang,  Meng Li,  Qitao Shi,  Longfei Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：model training stage, model inference stage, insufficient human resources, long tail scenario, tail scenario modeling</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we consider the problem of long tail scenario modeling with
budget limitation, i.e., insufficient human resources for model training stage
and limited time and computing resources for model inference stage. This
problem is widely encountered in various applications, yet has received
deficient attention so far. We present an automatic system named ALT to deal
with this problem. Several efforts are taken to improve the algorithms used in
our system, such as employing various automatic machine learning related
techniques, adopting the meta learning philosophy, and proposing an essential
budget-limited neural architecture search method, etc. Moreover, to build the
system, many optimizations are performed from a systematic perspective, and
essential modules are armed, making the system more feasible and efficient. We
perform abundant experiments to validate the effectiveness of our system and
demonstrate the usefulness of the critical modules in our system. Moreover,
online results are provided, which fully verified the efficacy of our system.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：Domain Generalization Deep Graph Transformation</b></summary>
  <p><b>编号</b>：[235]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11389</p>
  <p><b>作者</b>：Shiyu Wang,  Guangji Bai,  Qingyang Zhu,  Zhaohui Qin,  Liang Zhao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Graph transformation, predicts graph transition, common problem, training data, important and common</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Graph transformation that predicts graph transition from one mode to another
is an important and common problem. Despite much progress in developing
advanced graph transformation techniques in recent years, the fundamental
assumption typically required in machine-learning models that the testing and
training data preserve the same distribution does not always hold. As a result,
domain generalization graph transformation that predicts graphs not available
in the training data is under-explored, with multiple key challenges to be
addressed including (1) the extreme space complexity when training on all
input-output mode combinations, (2) difference of graph topologies between the
input and the output modes, and (3) how to generalize the model to (unseen)
target domains that are not in the training data. To fill the gap, we propose a
multi-input, multi-output, hypernetwork-based graph neural network
(MultiHyperGNN) that employs a encoder and a decoder to encode topologies of
both input and output modes and semi-supervised link prediction to enhance the
graph transformation task. Instead of training on all mode combinations,
MultiHyperGNN preserves a constant space complexity with the encoder and the
decoder produced by two novel hypernetworks. Comprehensive experiments show
that MultiHyperGNN has a superior performance than competing models in both
prediction and domain generalization tasks.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Justices for Information Bottleneck Theory</b></summary>
  <p><b>编号</b>：[236]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11387</p>
  <p><b>作者</b>：Faxian Cao,  Yongqiang Cheng,  Adil Mehmood Khan,  Zhijing Yang</p>
  <p><b>备注</b>：9 pages, 1 figures (4 subfigures)</p>
  <p><b>关键词</b>：injecting fresh perspectives, injecting fresh, reaffirm its validity, timely response, response to mounting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This study comes as a timely response to mounting criticism of the
information bottleneck (IB) theory, injecting fresh perspectives to rectify
misconceptions and reaffirm its validity. Firstly, we introduce an auxiliary
function to reinterpret the maximal coding rate reduction method as a special
yet local optimal case of IB theory. Through this auxiliary function, we
clarify the paradox of decreasing mutual information during the application of
ReLU activation in deep learning (DL) networks. Secondly, we challenge the
doubts about IB theory's applicability by demonstrating its capacity to explain
the absence of a compression phase with linear activation functions in hidden
layers, when viewed through the lens of the auxiliary function. Lastly, by
taking a novel theoretical stance, we provide a new way to interpret the inner
organizations of DL networks by using IB theory, aligning them with recent
experimental evidence. Thus, this paper serves as an act of justice for IB
theory, potentially reinvigorating its standing and application in DL and other
fields such as communications and biomedical research.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：Improving Fairness in AI Models on Electronic Health Records: The Case  for Federated Learning Methods</b></summary>
  <p><b>编号</b>：[237]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11386</p>
  <p><b>作者</b>：Raphael Poulain,  Mirza Farhan Bin Tarek,  Rahmatollah Beheshti</p>
  <p><b>备注</b>：Accepted to ACM FAccT 2023</p>
  <p><b>关键词</b>：Developing AI tools, critical importance, specifically in high-stakes, tools that preserve, healthcare</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Developing AI tools that preserve fairness is of critical importance,
specifically in high-stakes applications such as those in healthcare. However,
health AI models' overall prediction performance is often prioritized over the
possible biases such models could have. In this study, we show one possible
approach to mitigate bias concerns by having healthcare institutions
collaborate through a federated learning paradigm (FL; which is a popular
choice in healthcare settings). While FL methods with an emphasis on fairness
have been previously proposed, their underlying model and local implementation
techniques, as well as their possible applications to the healthcare domain
remain widely underinvestigated. Therefore, we propose a comprehensive FL
approach with adversarial debiasing and a fair aggregation method, suitable to
various fairness metrics, in the healthcare domain where electronic health
records are used. Not only our approach explicitly mitigates bias as part of
the optimization process, but an FL-based paradigm would also implicitly help
with addressing data imbalance and increasing the data size, offering a
practical solution for healthcare applications. We empirically demonstrate our
method's superior performance on multiple experiments simulating large-scale
real-world scenarios and compare it to several baselines. Our method has
achieved promising fairness performance with the lowest impact on overall
discrimination performance (accuracy).</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：Online Learning in a Creator Economy</b></summary>
  <p><b>编号</b>：[240]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11381</p>
  <p><b>作者</b>：Banghua Zhu,  Sai Praneeth Karimireddy,  Jiantao Jiao,  Michael I. Jordan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：creator economy, content creator, content, contracts, return-based contracts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The creator economy has revolutionized the way individuals can profit through
online platforms. In this paper, we initiate the study of online learning in
the creator economy by modeling the creator economy as a three-party game
between the users, platform, and content creators, with the platform
interacting with the content creator under a principal-agent model through
contracts to encourage better content. Additionally, the platform interacts
with the users to recommend new content, receive an evaluation, and ultimately
profit from the content, which can be modeled as a recommender system.
Our study aims to explore how the platform can jointly optimize the contract
and recommender system to maximize the utility in an online learning fashion.
We primarily analyze and compare two families of contracts: return-based
contracts and feature-based contracts. Return-based contracts pay the content
creator a fraction of the reward the platform gains. In contrast, feature-based
contracts pay the content creator based on the quality or features of the
content, regardless of the reward the platform receives. We show that under
smoothness assumptions, the joint optimization of return-based contracts and
recommendation policy provides a regret $\Theta(T^{2/3})$. For the
feature-based contract, we introduce a definition of intrinsic dimension $d$ to
characterize the hardness of learning the contract and provide an upper bound
on the regret $\mathcal{O}(T^{(d+1)/(d+2)})$. The upper bound is tight for the
linear family.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：Generalized Precision Matrix for Scalable Estimation of Nonparametric  Markov Networks</b></summary>
  <p><b>编号</b>：[241]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11379</p>
  <p><b>作者</b>：Yujia Zheng,  Ignavier Ng,  Yewen Fan,  Kun Zhang</p>
  <p><b>备注</b>：ICLR 2023</p>
  <p><b>关键词</b>：conditional independence structure, Markov network characterizes, Markov network, Markov network structure, Markov</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A Markov network characterizes the conditional independence structure, or
Markov property, among a set of random variables. Existing work focuses on
specific families of distributions (e.g., exponential families) and/or certain
structures of graphs, and most of them can only handle variables of a single
data type (continuous or discrete). In this work, we characterize the
conditional independence structure in general distributions for all data types
(i.e., continuous, discrete, and mixed-type) with a Generalized Precision
Matrix (GPM). Besides, we also allow general functional relations among
variables, thus giving rise to a Markov network structure learning algorithm in
one of the most general settings. To deal with the computational challenge of
the problem, especially for large graphs, we unify all cases under the same
umbrella of a regularized score matching framework. We validate the theoretical
results and demonstrate the scalability empirically in various settings.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：GraphFC: Customs Fraud Detection with Label Scarcity</b></summary>
  <p><b>编号</b>：[242]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11377</p>
  <p><b>作者</b>：Karandeep Singh,  Yu-Che Tsai,  Cheng-Te Li,  Meeyoug Cha,  Shou-De Lin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：world encounter huge, encounter huge volumes, customs fraud detection, customs fraud, world encounter</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Custom officials across the world encounter huge volumes of transactions.
With increased connectivity and globalization, the customs transactions
continue to grow every year. Associated with customs transactions is the
customs fraud - the intentional manipulation of goods declarations to avoid the
taxes and duties. With limited manpower, the custom offices can only undertake
manual inspection of a limited number of declarations. This necessitates the
need for automating the customs fraud detection by machine learning (ML)
techniques. Due the limited manual inspection for labeling the new-incoming
declarations, the ML approach should have robust performance subject to the
scarcity of labeled data. However, current approaches for customs fraud
detection are not well suited and designed for this real-world setting. In this
work, we propose $\textbf{GraphFC}$ ($\textbf{Graph}$ neural networks for
$\textbf{C}$ustoms $\textbf{F}$raud), a model-agnostic, domain-specific,
semi-supervised graph neural network based customs fraud detection algorithm
that has strong semi-supervised and inductive capabilities. With upto 252%
relative increase in recall over the present state-of-the-art, extensive
experimentation on real customs data from customs administrations of three
different countries demonstrate that GraphFC consistently outperforms various
baselines and the present state-of-art by a large margin.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：Smart Pressure e-Mat for Human Sleeping Posture and Dynamic Activity  Recognition</b></summary>
  <p><b>编号</b>：[245]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11367</p>
  <p><b>作者</b>：Liangqi Yuan,  Yuan Wei,  Jia Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：early childhood education, emphasis on healthcare, early childhood, childhood education, non-invasive measurement</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the emphasis on healthcare, early childhood education, and fitness,
non-invasive measurement and recognition methods have received more attention.
Pressure sensing has been extensively studied due to its advantages of simple
structure, easy access, visualization application, and harmlessness. This paper
introduces a smart pressure e-mat (SPeM) system based on a piezoresistive
material Velostat for human monitoring applications, including sleeping
postures, sports, and yoga recognition. After a subsystem scans e-mat readings
and processes the signal, it generates a pressure image stream. Deep neural
networks (DNNs) are used to fit and train the pressure image stream and
recognize the corresponding human behavior. Four sleeping postures and five
dynamic activities inspired by Nintendo Switch Ring Fit Adventure (RFA) are
used as a preliminary validation of the proposed SPeM system. The SPeM system
achieves high accuracies on both applications, which demonstrates the high
accuracy and generalization ability of the models. Compared with other pressure
sensor-based systems, SPeM possesses more flexible applications and commercial
application prospects, with reliable, robust, and repeatable properties.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：Differentially Private Online Item Pricing</b></summary>
  <p><b>编号</b>：[249]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11362</p>
  <p><b>作者</b>：Joon Suk Huh</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：unlimited supply item-pricing, supply item-pricing auction, unlimited supply, work addresses, addresses the problem</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This work addresses the problem of revenue maximization in a repeated,
unlimited supply item-pricing auction while preserving buyer privacy. We
present a novel algorithm that provides differential privacy with respect to
the buyer's input pair: item selection and bid. Notably, our algorithm is the
first to offer a sublinear $O(\sqrt{T}\log{T})$ regret with a privacy
guarantee. Our method is based on an exponential weights meta-algorithm, and we
mitigate the issue of discontinuities in revenue functions via small random
perturbations. As a result of its structural similarity to the exponential
mechanism, our method inherently secures differential privacy. We also extend
our algorithm to accommodate scenarios where buyers strategically bid over
successive rounds. The inherent differential privacy allows us to adapt our
algorithm with minimal modification to ensure a sublinear regret in this
setting.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：Differentially Private Adapters for Parameter Efficient Acoustic  Modeling</b></summary>
  <p><b>编号</b>：[251]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11360</p>
  <p><b>作者</b>：Chun-Wei Ho,  Chao-Han Huck Yang,  Sabato Marco Siniscalchi</p>
  <p><b>备注</b>：Accepted to Interspeech 2023. Code will be available at: this https URL The authors would like to express their gratitude to Prof. Chin-Hui Lee from Georgia Tech for providing helpful insights and suggestions</p>
  <p><b>关键词</b>：bring differential privacy, differential privacy, frozen pre-trained acoustic, pre-trained acoustic model, devise a parameter-efficient</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we devise a parameter-efficient solution to bring differential
privacy (DP) guarantees into adaptation of a cross-lingual speech classifier.
We investigate a new frozen pre-trained adaptation framework for DP-preserving
speech modeling without full model fine-tuning. First, we introduce a noisy
teacher-student ensemble into a conventional adaptation scheme leveraging a
frozen pre-trained acoustic model and attain superior performance than DP-based
stochastic gradient descent (DPSGD). Next, we insert residual adapters (RA)
between layers of the frozen pre-trained acoustic model. The RAs reduce
training cost and time significantly with a negligible performance drop.
Evaluated on the open-access Multilingual Spoken Words (MLSW) dataset, our
solution reduces the number of trainable parameters by 97.5% using the RAs with
only a 4% performance drop with respect to fine-tuning the cross-lingual speech
classifier while preserving DP guarantees.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：Understanding the World to Solve Social Dilemmas Using Multi-Agent  Reinforcement Learning</b></summary>
  <p><b>编号</b>：[252]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11358</p>
  <p><b>作者</b>：Manuel Rios,  Nicanor Quijano,  Luis Felipe Giraldo</p>
  <p><b>备注</b>：ICLR 2023 - AI4ABM workshop</p>
  <p><b>关键词</b>：conflicting interests impede, individuals can benefit, benefit from mutual, mutual cooperation, cooperation but conflicting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Social dilemmas are situations where groups of individuals can benefit from
mutual cooperation but conflicting interests impede them from doing so. This
type of situations resembles many of humanity's most critical challenges, and
discovering mechanisms that facilitate the emergence of cooperative behaviors
is still an open problem. In this paper, we study the behavior of
self-interested rational agents that learn world models in a multi-agent
reinforcement learning (RL) setting and that coexist in environments where
social dilemmas can arise. Our simulation results show that groups of agents
endowed with world models outperform all the other tested ones when dealing
with scenarios where social dilemmas can arise. We exploit the world model
architecture to qualitatively assess the learnt dynamics and confirm that each
agent's world model is capable to encode information of the behavior of the
changing environment and the other agent's actions. This is the first work that
shows that world models facilitate the emergence of complex coordinated
behaviors that enable interacting agents to ``understand'' both environmental
and social dynamics.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：Data Redaction from Conditional Generative Models</b></summary>
  <p><b>编号</b>：[255]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11351</p>
  <p><b>作者</b>：Zhifeng Kong,  Kamalika Chaudhuri</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：produce undesirable samples, Deep generative models, harmful content, Deep generative, produce undesirable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep generative models are known to produce undesirable samples such as
harmful content. Traditional mitigation methods include re-training from
scratch, filtering, or editing; however, these are either computationally
expensive or can be circumvented by third parties. In this paper, we take a
different approach and study how to post-edit an already-trained conditional
generative model so that it redacts certain conditionals that will, with high
probability, lead to undesirable content. This is done by distilling the
conditioning network in the models, giving a solution that is effective,
efficient, controllable, and universal for a class of deep generative models.
We conduct experiments on redacting prompts in text-to-image models and
redacting voices in text-to-speech models. Our method is computationally light,
leads to better redaction quality and robustness than baseline methods while
still retaining high generation quality.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：Unsupervised Domain-agnostic Fake News Detection using Multi-modal Weak  Signals</b></summary>
  <p><b>编号</b>：[256]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11349</p>
  <p><b>作者</b>：Amila Silva,  Ling Luo,  Shanika Karunasekera,  Christopher Leckie</p>
  <p><b>备注</b>：15 pages</p>
  <p><b>关键词</b>：fake news detection, fake, emergence of social, social media, main platforms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The emergence of social media as one of the main platforms for people to
access news has enabled the wide dissemination of fake news. This has motivated
numerous studies on automating fake news detection. Although there have been
limited attempts at unsupervised fake news detection, their performance suffers
due to not exploiting the knowledge from various modalities related to news
records and due to the presence of various latent biases in the existing news
datasets. To address these limitations, this work proposes an effective
framework for unsupervised fake news detection, which first embeds the
knowledge available in four modalities in news records and then proposes a
novel noise-robust self-supervised learning technique to identify the veracity
of news records from the multi-modal embeddings. Also, we propose a novel
technique to construct news datasets minimizing the latent biases in existing
news datasets. Following the proposed approach for dataset construction, we
produce a Large-scale Unlabelled News Dataset consisting 419,351 news articles
related to COVID-19, acronymed as LUND-COVID. We trained the proposed
unsupervised framework using LUND-COVID to exploit the potential of large
datasets, and evaluate it using a set of existing labelled datasets. Our
results show that the proposed unsupervised framework largely outperforms
existing unsupervised baselines for different tasks such as multi-modal fake
news detection, fake news early detection and few-shot fake news detection,
while yielding notable improvements for unseen domains during training.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：In the Name of Fairness: Assessing the Bias in Clinical Record  De-identification</b></summary>
  <p><b>编号</b>：[257]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11348</p>
  <p><b>作者</b>：Yuxin Xiao,  Shulammite Lim,  Tom Joseph Pollard,  Marzyeh Ghassemi</p>
  <p><b>备注</b>：Accepted by FAccT 2023</p>
  <p><b>关键词</b>：electronic health records, protected health information, clinical data requires, Data sharing, health records</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Data sharing is crucial for open science and reproducible research, but the
legal sharing of clinical data requires the removal of protected health
information from electronic health records. This process, known as
de-identification, is often achieved through the use of machine learning
algorithms by many commercial and open-source systems. While these systems have
shown compelling results on average, the variation in their performance across
different demographic groups has not been thoroughly examined. In this work, we
investigate the bias of de-identification systems on names in clinical notes
via a large-scale empirical analysis. To achieve this, we create 16 name sets
that vary along four demographic dimensions: gender, race, name popularity, and
the decade of popularity. We insert these names into 100 manually curated
clinical templates and evaluate the performance of nine public and private
de-identification methods. Our findings reveal that there are statistically
significant performance gaps along a majority of the demographic dimensions in
most methods. We further illustrate that de-identification quality is affected
by polysemy in names, gender context, and clinical note characteristics. To
mitigate the identified gaps, we propose a simple and method-agnostic solution
by fine-tuning de-identification methods with clinical context and diverse
names. Overall, it is imperative to address the bias in existing methods
immediately so that downstream stakeholders can build high-quality systems to
serve all demographic parties fairly.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：Quantifying the robustness of deep multispectral segmentation models  against natural perturbations and data poisoning</b></summary>
  <p><b>编号</b>：[258]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11347</p>
  <p><b>作者</b>：Elise Bishoff,  Charles Godfrey,  Myles McKay,  Eleanor Byler</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：traditional RGB channels, including additional spectral, additional spectral bands, natural perturbations, spectral bands</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In overhead image segmentation tasks, including additional spectral bands
beyond the traditional RGB channels can improve model performance. However, it
is still unclear how incorporating this additional data impacts model
robustness to adversarial attacks and natural perturbations. For adversarial
robustness, the additional information could improve the model's ability to
distinguish malicious inputs, or simply provide new attack avenues and
vulnerabilities. For natural perturbations, the additional information could
better inform model decisions and weaken perturbation effects or have no
significant influence at all. In this work, we seek to characterize the
performance and robustness of a multispectral (RGB and near infrared) image
segmentation model subjected to adversarial attacks and natural perturbations.
While existing adversarial and natural robustness research has focused
primarily on digital perturbations, we prioritize on creating realistic
perturbations designed with physical world conditions in mind. For adversarial
robustness, we focus on data poisoning attacks whereas for natural robustness,
we focus on extending ImageNet-C common corruptions for fog and snow that
coherently and self-consistently perturbs the input data. Overall, we find both
RGB and multispectral models are vulnerable to data poisoning attacks
regardless of input or fusion architectures and that while physically
realizable natural perturbations still degrade model performance, the impact
differs based on fusion architecture and input data.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：Bayesian Reparameterization of Reward-Conditioned Reinforcement Learning  with Energy-based Models</b></summary>
  <p><b>编号</b>：[263]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11340</p>
  <p><b>作者</b>：Wenhao Ding,  Tong Che,  Ding Zhao,  Marco Pavone</p>
  <p><b>备注</b>：Accepted to ICML 2023</p>
  <p><b>关键词</b>：reward-conditioned reinforcement learning, gained popularity due, Bayesian Reparameterized RCRL, RCRL, vanilla RCRL</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, reward-conditioned reinforcement learning (RCRL) has gained
popularity due to its simplicity, flexibility, and off-policy nature. However,
we will show that current RCRL approaches are fundamentally limited and fail to
address two critical challenges of RCRL -- improving generalization on high
reward-to-go (RTG) inputs, and avoiding out-of-distribution (OOD) RTG queries
during testing time. To address these challenges when training vanilla RCRL
architectures, we propose Bayesian Reparameterized RCRL (BR-RCRL), a novel set
of inductive biases for RCRL inspired by Bayes' theorem. BR-RCRL removes a core
obstacle preventing vanilla RCRL from generalizing on high RTG inputs -- a
tendency that the model treats different RTG inputs as independent values,
which we term ``RTG Independence". BR-RCRL also allows us to design an
accompanying adaptive inference method, which maximizes total returns while
avoiding OOD queries that yield unpredictable behaviors in vanilla RCRL
methods. We show that BR-RCRL achieves state-of-the-art performance on the
Gym-Mujoco and Atari offline RL benchmarks, improving upon vanilla RCRL by up
to 11%.</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：MALM: Mask Augmentation based Local Matching for Food-Recipe Retrieval</b></summary>
  <p><b>编号</b>：[269]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11327</p>
  <p><b>作者</b>：Bhanu Prakash Voutharoja,  Peng Wang,  Lei Wang,  Vivienne Guan</p>
  <p><b>备注</b>：Under review. Link to the dataset repo - this https URL</p>
  <p><b>关键词</b>：significant practical, representations, local matching, food item, matching</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image-to-recipe retrieval is a challenging vision-to-language task of
significant practical value. The main challenge of the task lies in the
ultra-high redundancy in the long recipe and the large variation reflected in
both food item combination and food item appearance. A de-facto idea to address
this task is to learn a shared feature embedding space in which a food image is
aligned better to its paired recipe than other recipes. However, such
supervised global matching is prone to supervision collapse, i.e., only partial
information that is necessary for distinguishing training pairs can be
identified, while other information that is potentially useful in
generalization could be lost. To mitigate such a problem, we propose a
mask-augmentation-based local matching network (MALM), where an image-text
matching module and a masked self-distillation module benefit each other
mutually to learn generalizable cross-modality representations. On one hand, we
perform local matching between the tokenized representations of image and text
to locate fine-grained cross-modality correspondence explicitly. We involve
representations of masked image patches in this process to alleviate
overfitting resulting from local matching especially when some food items are
underrepresented. On the other hand, predicting the hidden representations of
the masked patches through self-distillation helps to learn general-purpose
image representations that are expected to generalize better. And the
multi-task nature of the model enables the representations of masked patches to
be text-aware and thus facilitates the lost information reconstruction.
Experimental results on Recipe1M dataset show our method can clearly outperform
state-of-the-art (SOTA) methods. Our code will be available at
this https URL</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：SpikeCP: Delay-Adaptive Reliable Spiking Neural Networks via Conformal  Prediction</b></summary>
  <p><b>编号</b>：[271]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11322</p>
  <p><b>作者</b>：Jiechen Chen,  Sangwoo Park,  Osvaldo Simeone</p>
  <p><b>备注</b>：Under review</p>
  <p><b>关键词</b>：Spiking neural networks, internal event-driven neural, event-driven neural dynamics, process time-series data, energy consumption depends</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Spiking neural networks (SNNs) process time-series data via internal
event-driven neural dynamics whose energy consumption depends on the number of
spikes exchanged between neurons over the course of the input presentation. In
typical implementations of an SNN classifier, decisions are produced after the
entire input sequence has been processed, resulting in latency and energy
consumption levels that are fairly uniform across inputs. Recently introduced
delay-adaptive SNNs tailor the inference latency -- and, with it, the energy
consumption -- to the difficulty of each example, by producing an early
decision when the SNN model is sufficiently ``confident''. In this paper, we
start by observing that, as an SNN processes input samples, its classification
decisions tend to be first under-confident and then over-confident with respect
to the decision's ground-truth, unknown, test accuracy. This makes it difficult
to determine a stopping time that ensures a desired level of accuracy. To
address this problem, we introduce a novel delay-adaptive SNN-based inference
methodology that, wrapping around any pre-trained SNN classifier, provides
guaranteed reliability for the decisions produced at input-dependent stopping
times. The approach entails minimal added complexity as compared to the
underlying SNN, requiring only thresholding and counting operations at run
time, and it leverages tools from conformal prediction (CP).</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：BELLA: Black box model Explanations by Local Linear Approximations</b></summary>
  <p><b>编号</b>：[278]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11311</p>
  <p><b>作者</b>：Nedeljko Radulovic,  Albert Bifet,  Fabian Suchanek</p>
  <p><b>备注</b>：21 pages,3 figures, submitted to Journal of Artificial Intelligence</p>
  <p><b>关键词</b>：recent years, understanding the decision-making, assess their performance, decision-making process, legal requirement</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, understanding the decision-making process of black-box
models has become not only a legal requirement but also an additional way to
assess their performance. However, the state of the art post-hoc interpretation
approaches rely on synthetic data generation. This introduces uncertainty and
can hurt the reliability of the interpretations. Furthermore, they tend to
produce explanations that apply to only very few data points. This makes the
explanations brittle and limited in scope. Finally, they provide scores that
have no direct verifiable meaning. In this paper, we present BELLA, a
deterministic model-agnostic post-hoc approach for explaining the individual
predictions of regression black-box models. BELLA provides explanations in the
form of a linear model trained in the feature space. Thus, its coefficients can
be used directly to compute the predicted value from the feature values.
Furthermore, BELLA maximizes the size of the neighborhood to which the linear
model applies, so that the explanations are accurate, simple, general, and
robust. BELLA can produce both factual and counterfactual explanations. Our
user study confirms the importance of the desiderata we optimize, and our
experiments show that BELLA outperforms the state-of-the-art approaches on
these desiderata.</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：AMII: Adaptive Multimodal Inter-personal and Intra-personal Model for  Adapted Behavior Synthesis</b></summary>
  <p><b>编号</b>：[279]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11310</p>
  <p><b>作者</b>：Jieyeon Woo,  Mireille Fares,  Catherine Pelachaud,  Catherine Achard</p>
  <p><b>备注</b>：8 pages, 1 figure</p>
  <p><b>关键词</b>：Socially Interactive Agents, virtual embodied agents, Interactive Agents, Socially Interactive, display similar behavior</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Socially Interactive Agents (SIAs) are physical or virtual embodied agents
that display similar behavior as human multimodal behavior. Modeling SIAs'
non-verbal behavior, such as speech and facial gestures, has always been a
challenging task, given that a SIA can take the role of a speaker or a
listener. A SIA must emit appropriate behavior adapted to its own speech, its
previous behaviors (intra-personal), and the User's behaviors (inter-personal)
for both roles. We propose AMII, a novel approach to synthesize adaptive facial
gestures for SIAs while interacting with Users and acting interchangeably as a
speaker or as a listener. AMII is characterized by modality memory encoding
schema - where modality corresponds to either speech or facial gestures - and
makes use of attention mechanisms to capture the intra-personal and
inter-personal relationships. We validate our approach by conducting objective
evaluations and comparing it with the state-of-the-art approaches.</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：pTSE: A Multi-model Ensemble Method for Probabilistic Time Series  Forecasting</b></summary>
  <p><b>编号</b>：[282]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11304</p>
  <p><b>作者</b>：Yunyi Zhou,  Zhixuan Chu,  Yijia Ruan,  Ge Jin,  Yuchen Huang,  Sheng Li</p>
  <p><b>备注</b>：The 32nd International Joint Conference on Artificial Intelligence (IJCAI 2023)</p>
  <p><b>关键词</b>：remarkably good performance, shown remarkably good, time series, good performance, shown remarkably</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Various probabilistic time series forecasting models have sprung up and shown
remarkably good performance. However, the choice of model highly relies on the
characteristics of the input time series and the fixed distribution that the
model is based on. Due to the fact that the probability distributions cannot be
averaged over different models straightforwardly, the current time series model
ensemble methods cannot be directly applied to improve the robustness and
accuracy of forecasting. To address this issue, we propose pTSE, a multi-model
distribution ensemble method for probabilistic forecasting based on Hidden
Markov Model (HMM). pTSE only takes off-the-shelf outputs from member models
without requiring further information about each model. Besides, we provide a
complete theoretical analysis of pTSE to prove that the empirical distribution
of time series subject to an HMM will converge to the stationary distribution
almost surely. Experiments on benchmarks show the superiority of pTSE overall
member models and competitive ensemble methods.</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：Bayesian Risk-Averse Q-Learning with Streaming Observations</b></summary>
  <p><b>编号</b>：[284]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11300</p>
  <p><b>作者</b>：Yuhao Wang,  Enlu Zhou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：reinforcement learning problem, robust reinforcement learning, simulated training environment, Bayesian risk MDP, learning agent learns</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider a robust reinforcement learning problem, where a learning agent
learns from a simulated training environment. To account for the model
mis-specification between this training environment and the real environment
due to lack of data, we adopt a formulation of Bayesian risk MDP (BRMDP) with
infinite horizon, which uses Bayesian posterior to estimate the transition
model and impose a risk functional to account for the model uncertainty.
Observations from the real environment that is out of the agent's control
arrive periodically and are utilized by the agent to update the Bayesian
posterior to reduce model uncertainty. We theoretically demonstrate that BRMDP
balances the trade-off between robustness and conservativeness, and we further
develop a multi-stage Bayesian risk-averse Q-learning algorithm to solve BRMDP
with streaming observations from real environment. The proposed algorithm
learns a risk-averse yet optimal policy that depends on the availability of
real-world observations. We provide a theoretical guarantee of strong
convergence for the proposed algorithm.</p>
  </details>
</details>
<details>
  <summary>83. <b>标题：Massively Scalable Inverse Reinforcement Learning in Google Maps</b></summary>
  <p><b>编号</b>：[290]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11290</p>
  <p><b>作者</b>：Matt Barnes,  Matthew Abueg,  Oliver F. Lange,  Matt Deeds,  Jason Trader,  Denali Molitor,  Markus Wulfmeier,  Shawn O'Banion</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：humans' latent preferences, globally-scalable solutions remain, Optimizing for humans', humans' latent, latent preferences</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Optimizing for humans' latent preferences is a grand challenge in route
recommendation, where globally-scalable solutions remain an open problem.
Although past work created increasingly general solutions for the application
of inverse reinforcement learning (IRL), these have not been successfully
scaled to world-sized MDPs, large datasets, and highly parameterized models;
respectively hundreds of millions of states, trajectories, and parameters. In
this work, we surpass previous limitations through a series of advancements
focused on graph compression, parallelization, and problem initialization based
on dominant eigenvectors. We introduce Receding Horizon Inverse Planning
(RHIP), which generalizes existing work and enables control of key performance
trade-offs via its planning horizon. Our policy achieves a 16-24% improvement
in global route quality, and, to our knowledge, represents the largest instance
of IRL in a real-world setting to date. Our results show critical benefits to
more sustainable modes of transportation (e.g. two-wheelers), where factors
beyond journey time (e.g. route safety) play a substantial role. We conclude
with ablations of key components, negative results on state-of-the-art
eigenvalue solvers, and identify future opportunities to improve scalability
via IRL-specific batching strategies.</p>
  </details>
</details>
<details>
  <summary>84. <b>标题：Riemannian Multiclass Logistics Regression for SPD Neural Networks</b></summary>
  <p><b>编号</b>：[291]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11288</p>
  <p><b>作者</b>：Ziheng Chen,  Yue Song,  Gaowen Liu,  Ramana Rao Kompella,  Xiaojun Wu,  Nicu Sebe</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：symmetric positive definite, gaining increasing attention, SPD networks, SPD, existing SPD networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep neural networks for learning symmetric positive definite (SPD) matrices
are gaining increasing attention in machine learning. Despite the significant
progress, most existing SPD networks use traditional Euclidean classifiers on
approximated spaces rather than intrinsic classifiers that accurately capture
the geometry of SPD manifolds. Inspired by the success of hyperbolic neural
networks (HNNs), we propose Riemannian multiclass logistics regression (RMLR)
for SPD networks. We introduce a general unified framework for a family of
Riemannian metrics on SPD manifolds and showcase the specific
$\orth{n}$-invariant Log-Euclidean Metrics for SPD networks. Moreover, we
encompass the most popular classifier in existing SPD networks as a special
case of our framework. Extensive experiments on popular SPD learning benchmarks
demonstrate the superiority of our classifiers.</p>
  </details>
</details>
<details>
  <summary>85. <b>标题：On the Statistical Efficiency of Mean Field Reinforcement Learning with  General Function Approximation</b></summary>
  <p><b>编号</b>：[293]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11283</p>
  <p><b>作者</b>：Jiawei Huang,  Batuhan Yardim,  Niao He</p>
  <p><b>备注</b>：47 Pages</p>
  <p><b>关键词</b>：Reinforcement Learning, general function approximation, Mean-Field Control, Mean-Field Game, function approximation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we study the statistical efficiency of Reinforcement Learning
in Mean-Field Control (MFC) and Mean-Field Game (MFG) with general function
approximation. We introduce a new concept called Mean-Field Model-Based Eluder
Dimension (MBED), which subsumes a rich family of Mean-Field RL problems.
Additionally, we propose algorithms based on Optimistic Maximal Likelihood
Estimation, which can return an $\epsilon$-optimal policy for MFC or an
$\epsilon$-Nash Equilibrium policy for MFG, with sample complexity polynomial
w.r.t. relevant parameters and independent of the number of states, actions and
the number of agents. Notably, our results only require a mild assumption of
Lipschitz continuity on transition dynamics and avoid strong structural
assumptions in previous work. Finally, in the tabular setting, given the access
to a generative model, we establish an exponential lower bound for MFC setting,
while providing a novel sample-efficient model elimination algorithm to
approximate equilibrium in MFG setting. Our results reveal a fundamental
separation between RL for single-agent, MFC, and MFG from the sample efficiency
perspective.</p>
  </details>
</details>
<details>
  <summary>86. <b>标题：SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models</b></summary>
  <p><b>编号</b>：[294]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11281</p>
  <p><b>作者</b>：Ziyi Wu,  Jingyu Hu,  Wuyue Lu,  Igor Gilitschenski,  Animesh Garg</p>
  <p><b>备注</b>：Project page: this https URL . An earlier version of this work appeared at the ICLR 2023 Workshop on Neurosymbolic Generative Models: this https URL</p>
  <p><b>关键词</b>：enable systematic generalization, providing structured representations, providing structured, systematic generalization, Object-centric learning aims</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Object-centric learning aims to represent visual data with a set of object
entities (a.k.a. slots), providing structured representations that enable
systematic generalization. Leveraging advanced architectures like Transformers,
recent approaches have made significant progress in unsupervised object
discovery. In addition, slot-based representations hold great potential for
generative modeling, such as controllable image generation and object
manipulation in image editing. However, current slot-based methods often
produce blurry images and distorted objects, exhibiting poor generative
modeling capabilities. In this paper, we focus on improving slot-to-image
decoding, a crucial aspect for high-quality visual generation. We introduce
SlotDiffusion -- an object-centric Latent Diffusion Model (LDM) designed for
both image and video data. Thanks to the powerful modeling capacity of LDMs,
SlotDiffusion surpasses previous slot models in unsupervised object
segmentation and visual generation across six datasets. Furthermore, our
learned object features can be utilized by existing object-centric dynamics
models, improving video prediction quality and downstream temporal reasoning
tasks. Finally, we demonstrate the scalability of SlotDiffusion to
unconstrained real-world datasets such as PASCAL VOC and COCO, when integrated
with self-supervised pre-trained image encoders.</p>
  </details>
</details>
<details>
  <summary>87. <b>标题：Towards Collaborative Plan Acquisition through Theory of Mind Modeling  in Situated Dialogue</b></summary>
  <p><b>编号</b>：[297]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11271</p>
  <p><b>作者</b>：Cristian-Paul Bara,  Ziqiao Ma,  Yingzhuo Yu,  Julie Shah,  Joyce Chai</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：incomplete initial plans, incomplete initial, tasks, partner, knowledge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Collaborative tasks often begin with partial task knowledge and incomplete
initial plans from each partner. To complete these tasks, agents need to engage
in situated communication with their partners and coordinate their partial
plans towards a complete plan to achieve a joint task goal. While such
collaboration seems effortless in a human-human team, it is highly challenging
for human-AI collaboration. To address this limitation, this paper takes a step
towards collaborative plan acquisition, where humans and agents strive to learn
and communicate with each other to acquire a complete plan for joint tasks.
Specifically, we formulate a novel problem for agents to predict the missing
task knowledge for themselves and for their partners based on rich perceptual
and dialogue history. We extend a situated dialogue benchmark for symmetric
collaborative tasks in a 3D blocks world and investigate computational
strategies for plan acquisition. Our empirical results suggest that predicting
the partner's missing knowledge is a more viable approach than predicting one's
own. We show that explicit modeling of the partner's dialogue moves and mental
states produces improved and more stable results than without. These results
provide insight for future AI agents that can predict what knowledge their
partner is missing and, therefore, can proactively communicate such information
to help their partner acquire such missing knowledge toward a common
understanding of joint tasks.</p>
  </details>
</details>
<details>
  <summary>88. <b>标题：Constrained Environment Optimization for Prioritized Multi-Agent  Navigation</b></summary>
  <p><b>编号</b>：[302]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11260</p>
  <p><b>作者</b>：Zhan Gao,  Amanda Prorok</p>
  <p><b>备注</b>：arXiv admin note: substantial text overlap with arXiv:2209.11279</p>
  <p><b>关键词</b>：multi-agent navigation algorithms, environment, Traditional approaches, environment optimization, design of multi-agent</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Traditional approaches to the design of multi-agent navigation algorithms
consider the environment as a fixed constraint, despite the influence of
spatial constraints on agents' performance. Yet hand-designing conducive
environment layouts is inefficient and potentially expensive. The goal of this
paper is to consider the environment as a decision variable in a system-level
optimization problem, where both agent performance and environment cost are
incorporated. Towards this end, we propose novel problems of unprioritized and
prioritized environment optimization, where the former considers agents
unbiasedly and the latter accounts for agent priorities. We show, through
formal proofs, under which conditions the environment can change while
guaranteeing completeness (i.e., all agents reach goals), and analyze the role
of agent priorities in the environment optimization. We proceed to impose
real-world constraints on the environment optimization and formulate it
mathematically as a constrained stochastic optimization problem. Since the
relation between agents, environment and performance is challenging to model,
we leverage reinforcement learning to develop a model-free solution and a
primal-dual mechanism to handle constraints. Distinct information processing
architectures are integrated for various implementation scenarios, including
online/offline optimization and discrete/continuous environment. Numerical
results corroborate the theory and demonstrate the validity and adaptability of
our approach.</p>
  </details>
</details>
<details>
  <summary>89. <b>标题：Brain-inspired learning in artificial neural networks: a review</b></summary>
  <p><b>编号</b>：[304]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11252</p>
  <p><b>作者</b>：Samuel Schmidgall,  Jascha Achterberg,  Thomas Miconi,  Louis Kirsch,  Rojin Ziaei,  S. Pardis Hajiseyedrazi,  Jason Eshraghian</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：achieving remarkable success, Artificial neural networks, game playing, achieving remarkable, diverse domains</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Artificial neural networks (ANNs) have emerged as an essential tool in
machine learning, achieving remarkable success across diverse domains,
including image and speech generation, game playing, and robotics. However,
there exist fundamental differences between ANNs' operating mechanisms and
those of the biological brain, particularly concerning learning processes. This
paper presents a comprehensive review of current brain-inspired learning
representations in artificial neural networks. We investigate the integration
of more biologically plausible mechanisms, such as synaptic plasticity, to
enhance these networks' capabilities. Moreover, we delve into the potential
advantages and challenges accompanying this approach. Ultimately, we pinpoint
promising avenues for future research in this rapidly advancing field, which
could bring us closer to understanding the essence of intelligence.</p>
  </details>
</details>
<details>
  <summary>90. <b>标题：A Parameter-Efficient Learning Approach to Arabic Dialect Identification  with Pre-Trained General-Purpose Speech Model</b></summary>
  <p><b>编号</b>：[308]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11244</p>
  <p><b>作者</b>：Srijith Radhakrishnan,  Chao-Han Huck Yang,  Sumeer Ahmad Khan,  Narsis A. Kiani,  David Gomez-Cabrero,  Jesper N. Tegner</p>
  <p><b>备注</b>：Accepted to Interspeech. Code is available at: this https URL under MIT license</p>
  <p><b>关键词</b>：techniques to repurpose, Arabic dialect identification, Arabic dialect, multi-layer encoder-decoder GSM, encoder-decoder GSM formulation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we explore Parameter-Efficient-Learning (PEL) techniques to
repurpose a General-Purpose-Speech (GSM) model for Arabic dialect
identification (ADI). Specifically, we investigate different setups to
incorporate trainable features into a multi-layer encoder-decoder GSM
formulation under frozen pre-trained settings. Our architecture includes
residual adapter and model reprogramming (input-prompting). We design a
token-level label mapping to condition the GSM for Arabic Dialect
Identification (ADI). This is challenging due to the high variation in
vocabulary and pronunciation among the numerous regional dialects. We achieve
new state-of-the-art accuracy on the ADI-17 dataset by vanilla fine-tuning. We
further reduce the training budgets with the PEL method, which performs within
1.86% accuracy to fine-tuning using only 2.5% of (extra) network trainable
parameters. Our study demonstrates how to identify Arabic dialects using a
small dataset and limited computation with open source code and pre-trained
models.</p>
  </details>
</details>
<details>
  <summary>91. <b>标题：Evidence Networks: simple losses for fast, amortized, neural Bayesian  model comparison</b></summary>
  <p><b>编号</b>：[311]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11241</p>
  <p><b>作者</b>：Niall Jeffrey,  Benjamin D. Wandelt</p>
  <p><b>备注</b>：21 pages, 8 figures</p>
  <p><b>关键词</b>：Bayesian model comparison, enable Bayesian model, nested sampling, intractable or unknown, enable Bayesian</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Evidence Networks can enable Bayesian model comparison when state-of-the-art
methods (e.g. nested sampling) fail and even when likelihoods or priors are
intractable or unknown. Bayesian model comparison, i.e. the computation of
Bayes factors or evidence ratios, can be cast as an optimization problem.
Though the Bayesian interpretation of optimal classification is well-known,
here we change perspective and present classes of loss functions that result in
fast, amortized neural estimators that directly estimate convenient functions
of the Bayes factor. This mitigates numerical inaccuracies associated with
estimating individual model probabilities. We introduce the leaky parity-odd
power (l-POP) transform, leading to the novel ``l-POP-Exponential'' loss
function. We explore neural density estimation for data probability in
different models, showing it to be less accurate and scalable than Evidence
Networks. Multiple real-world and synthetic examples illustrate that Evidence
Networks are explicitly independent of dimensionality of the parameter space
and scale mildly with the complexity of the posterior probability density
function. This simple yet powerful approach has broad implications for model
inference tasks. As an application of Evidence Networks to real-world data we
compute the Bayes factor for two models with gravitational lensing data of the
Dark Energy Survey. We briefly discuss applications of our methods to other,
related problems of model comparison and evaluation in implicit inference
settings.</p>
  </details>
</details>
<details>
  <summary>92. <b>标题：Efficient Vertical Federated Learning with Secure Aggregation</b></summary>
  <p><b>编号</b>：[313]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11236</p>
  <p><b>作者</b>：Xinchi Qiu,  Heng Pan,  Wanru Zhao,  Chenyang Ma,  Pedro Porto Buarque de Gusmão,  Nicholas D. Lane</p>
  <p><b>备注</b>：Federated Learning Systems (FLSys) Workshop @ MLSys 2023</p>
  <p><b>关键词</b>：complete models independently, horizontally partitioned datasets, train complete models, privacy-preserving federated learning, federated learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The majority of work in privacy-preserving federated learning (FL) has been
focusing on horizontally partitioned datasets where clients share the same sets
of features and can train complete models independently. However, in many
interesting problems, such as financial fraud detection and disease detection,
individual data points are scattered across different clients/organizations in
vertical federated learning. Solutions for this type of FL require the exchange
of gradients between participants and rarely consider privacy and security
concerns, posing a potential risk of privacy leakage. In this work, we present
a novel design for training vertical FL securely and efficiently using
state-of-the-art security modules for secure aggregation. We demonstrate
empirically that our method does not impact training performance whilst
obtaining 9.1e2 ~3.8e4 speedup compared to homomorphic encryption (HE).</p>
  </details>
</details>
<details>
  <summary>93. <b>标题：Information-Ordered Bottlenecks for Adaptive Semantic Compression</b></summary>
  <p><b>编号</b>：[317]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11213</p>
  <p><b>作者</b>：Matthew Ho,  Xiaosheng Zhao,  Benjamin Wandelt</p>
  <p><b>备注</b>：14 pages, 6 figures, 1 table, Submitted to NeurIPS 2023</p>
  <p><b>关键词</b>：neural layer designed, latent variables ordered, likelihood maximization, present the information-ordered, neural layer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present the information-ordered bottleneck (IOB), a neural layer designed
to adaptively compress data into latent variables ordered by likelihood
maximization. Without retraining, IOB nodes can be truncated at any bottleneck
width, capturing the most crucial information in the first latent variables.
Unifying several previous approaches, we show that IOBs achieve near-optimal
compression for a given encoding architecture and can assign ordering to latent
signals in a manner that is semantically meaningful. IOBs demonstrate a
remarkable ability to compress embeddings of image and text data, leveraging
the performance of SOTA architectures such as CNNs, transformers, and diffusion
models. Moreover, we introduce a novel theory for estimating global intrinsic
dimensionality with IOBs and show that they recover SOTA dimensionality
estimates for complex synthetic data. Furthermore, we showcase the utility of
these models for exploratory analysis through applications on heterogeneous
datasets, enabling computer-aided discovery of dataset complexity.</p>
  </details>
</details>
<details>
  <summary>94. <b>标题：LIMA: Less Is More for Alignment</b></summary>
  <p><b>编号</b>：[318]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11206</p>
  <p><b>作者</b>：Chunting Zhou,  Pengfei Liu,  Puxin Xu,  Srini Iyer,  Jiao Sun,  Yuning Mao,  Xuezhe Ma,  Avia Efrat,  Ping Yu,  Lili Yu,  Susan Zhang,  Gargi Ghosh,  Mike Lewis,  Luke Zettlemoyer,  Omer Levy</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：learn general-purpose representations, Large language models, raw text, general-purpose representations, reinforcement learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models are trained in two stages: (1) unsupervised pretraining
from raw text, to learn general-purpose representations, and (2) large scale
instruction tuning and reinforcement learning, to better align to end tasks and
user preferences. We measure the relative importance of these two stages by
training LIMA, a 65B parameter LLaMa language model fine-tuned with the
standard supervised loss on only 1,000 carefully curated prompts and responses,
without any reinforcement learning or human preference modeling. LIMA
demonstrates remarkably strong performance, learning to follow specific
response formats from only a handful of examples in the training data,
including complex queries that range from planning trip itineraries to
speculating about alternate history. Moreover, the model tends to generalize
well to unseen tasks that did not appear in the training data. In a controlled
human study, responses from LIMA are either equivalent or strictly preferred to
GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard
and 65% versus DaVinci003, which was trained with human feedback. Taken
together, these results strongly suggest that almost all knowledge in large
language models is learned during pretraining, and only limited instruction
tuning data is necessary to teach models to produce high quality output.</p>
  </details>
</details>
<details>
  <summary>95. <b>标题：PDP: Parameter-free Differentiable Pruning is All You Need</b></summary>
  <p><b>编号</b>：[319]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11203</p>
  <p><b>作者</b>：Minsik Cho,  Saurabh Adya,  Devang Naik</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：PDP, DNN accelerators, pruning, DNN, DNN pruning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>DNN pruning is a popular way to reduce the size of a model, improve the
inference latency, and minimize the power consumption on DNN accelerators.
However, existing approaches might be too complex, expensive or ineffective to
apply to a variety of vision/language tasks, DNN architectures and to honor
structured pruning constraints. In this paper, we propose an efficient yet
effective train-time pruning scheme, Parameter-free Differentiable Pruning
(PDP), which offers state-of-the-art qualities in model size, accuracy, and
training cost. PDP uses a dynamic function of weights during training to
generate soft pruning masks for the weights in a parameter-free manner for a
given pruning target. While differentiable, the simplicity and efficiency of
PDP make it universal enough to deliver state-of-the-art
random/structured/channel pruning results on various vision and natural
language tasks. For example, for MobileNet-v1, PDP can achieve 68.2% top-1
ImageNet1k accuracy at 86.6% sparsity, which is 1.7% higher accuracy than those
from the state-of-the-art algorithms. Also, PDP yields over 83.1% accuracy on
Multi-Genre Natural Language Inference with 90% sparsity for BERT, while the
next best from the existing techniques shows 81.5% accuracy. In addition, PDP
can be applied to structured pruning, such as N:M pruning and channel pruning.
For 1:4 structured pruning of ResNet18, PDP improved the top-1 ImageNet1k
accuracy by over 3.6% over the state-of-the-art. For channel pruning of
ResNet50, PDP reduced the top-1 ImageNet1k accuracy by 0.6% from the
state-of-the-art.</p>
  </details>
</details>
<details>
  <summary>96. <b>标题：Prediction with Incomplete Data under Agnostic Mask Distribution Shift</b></summary>
  <p><b>编号</b>：[321]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11197</p>
  <p><b>作者</b>：Yichen Zhu,  Jian Yuan,  Bo Jiang,  Tao Lin,  Haiming Jin,  Xinbing Wang,  Chenghu Zhou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：missing pattern, incomplete data, incomplete data consisting, mask, missing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Data with missing values is ubiquitous in many applications. Recent years
have witnessed increasing attention on prediction with only incomplete data
consisting of observed features and a mask that indicates the missing pattern.
Existing methods assume that the training and testing distributions are the
same, which may be violated in real-world scenarios. In this paper, we consider
prediction with incomplete data in the presence of distribution shift. We focus
on the case where the underlying joint distribution of complete features and
label is invariant, but the missing pattern, i.e., mask distribution may shift
agnostically between training and testing. To achieve generalization, we
leverage the observation that for each mask, there is an invariant optimal
predictor. To avoid the exponential explosion when learning them separately, we
approximate the optimal predictors jointly using a double parameterization
technique. This has the undesirable side effect of allowing the learned
predictors to rely on the intra-mask correlation and that between features and
mask. We perform decorrelation to minimize this effect. Combining the
techniques above, we propose a novel prediction method called StableMiss.
Extensive experiments on both synthetic and real-world datasets show that
StableMiss is robust and outperforms state-of-the-art methods under agnostic
mask distribution shift.</p>
  </details>
</details>
<details>
  <summary>97. <b>标题：DClEVerNet: Deep Combinatorial Learning for Efficient EV Charging  Scheduling in Large-scale Networked Facilities</b></summary>
  <p><b>编号</b>：[323]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11195</p>
  <p><b>作者</b>：Bushra Alshehhi,  Areg Karapetyan,  Khaled Elbassioni,  Sid Chi-Kin Chau,  Majid Khonji</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：distribution networks significantly, stress distribution networks, electrification of transportation, electric vehicles, leaving their performance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the electrification of transportation, the rising uptake of electric
vehicles (EVs) might stress distribution networks significantly, leaving their
performance degraded and stability jeopardized. To accommodate these new loads
cost-effectively, modern power grids require coordinated or ``smart'' charging
strategies capable of optimizing EV charging scheduling in a scalable and
efficient fashion. With this in view, the present work focuses on reservation
management programs for large-scale, networked EV charging stations. We
formulate a time-coupled binary optimization problem that maximizes EV users'
total welfare gain while accounting for the network's available power capacity
and stations' occupancy limits. To tackle the problem at scale while retaining
high solution quality, a data-driven optimization framework combining
techniques from the fields of Deep Learning and Approximation Algorithms is
introduced. The framework's key ingredient is a novel input-output processing
scheme for neural networks that allows direct extrapolation to problem sizes
substantially larger than those included in the training set. Extensive
numerical simulations based on synthetic and real-world data traces verify the
effectiveness and superiority of the presented approach over two representative
scheduling algorithms. Lastly, we round up the contributions by listing several
immediate extensions to the proposed framework and outlining the prospects for
further exploration.</p>
  </details>
</details>
<details>
  <summary>98. <b>标题：Taxonomy of AISecOps Threat Modeling for Cloud Based Medical Chatbots</b></summary>
  <p><b>编号</b>：[326]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11189</p>
  <p><b>作者</b>：Ruby Annette J,  Aisha Banu,  Sharon Priya S,  Subash Chandran</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：technology including cyber, Artificial Intelligence, medical chatbots, playing a vital, vital role</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Artificial Intelligence (AI) is playing a vital role in all aspects of
technology including cyber security. Application of Conversational AI like the
chatbots are also becoming very popular in the medical field to provide timely
and immediate medical assistance to patients in need. As medical chatbots deal
with a lot of sensitive information, the security of these chatbots is crucial.
To secure the confidentiality, integrity, and availability of cloud-hosted
assets like these, medical chatbots can be monitored using AISecOps (Artificial
Intelligence for Secure IT Operations). AISecOPs is an emerging field that
integrates three different but interrelated domains like the IT operation, AI,
and security as one domain, where the expertise from all these three domains
are used cohesively to secure the cyber assets. It considers cloud operations
and security in a holistic framework to collect the metrics required to assess
the security threats and train the AI models to take immediate actions. This
work is focused on applying the STRIDE threat modeling framework to model the
possible threats involved in each component of the chatbot to enable the
automatic threat detection using the AISecOps techniques. This threat modeling
framework is tailored to the medical chatbots that involves sensitive data
sharing but could also be applied for chatbots used in other sectors like the
financial services, public sector, and government sectors that are concerned
with security and compliance.</p>
  </details>
</details>
<details>
  <summary>99. <b>标题：Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM  Inference with Transferable Prompt</b></summary>
  <p><b>编号</b>：[327]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11186</p>
  <p><b>作者</b>：Zhaozhuo Xu,  Zirui Liu,  Beidi Chen,  Yuxin Tang,  Jue Wang,  Kaixiong Zhou,  Xia Hu,  Anshumali Shrivastava</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Natural Language Processing, Language Processing, Natural Language, exhibit exceptional performance, range of Natural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs), armed with billions of parameters, exhibit
exceptional performance across a wide range of Natural Language Processing
(NLP) tasks. However, they present a significant computational challenge during
inference, especially when deploying on common hardware such as single GPUs. As
such, minimizing the latency of LLM inference by curtailing computational and
memory requirements, though achieved through compression, becomes critically
important. However, this process inevitably instigates a trade-off between
efficiency and accuracy, as compressed LLMs typically experience a reduction in
predictive precision. In this research, we introduce an innovative perspective:
to optimize this trade-off, compressed LLMs require a unique input format that
varies from that of the original models. Our findings indicate that the
generation quality in a compressed LLM can be markedly improved for specific
queries by selecting prompts with precision. Capitalizing on this insight, we
introduce a prompt learning paradigm that cultivates an additive prompt over a
compressed LLM to bolster their accuracy. Our empirical results imply that
through our strategic prompt utilization, compressed LLMs can match, and
occasionally even exceed, the accuracy of the original models. Moreover, we
demonstrated that these learned prompts have a certain degree of
transferability across various datasets, tasks, and compression levels. These
insights shine a light on new possibilities for enhancing the balance between
accuracy and efficiency in LLM inference. Specifically, they underscore the
importance of judicious input editing to a compressed large model, hinting at
potential advancements in scaling LLMs on common hardware.</p>
  </details>
</details>
<details>
  <summary>100. <b>标题：Comparison of Transfer Learning based Additive Manufacturing Models via  A Case Study</b></summary>
  <p><b>编号</b>：[329]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11181</p>
  <p><b>作者</b>：Yifan Tang,  M. Rahmani Dehaghani,  G. Gary Wang</p>
  <p><b>备注</b>：16 pages, 8 figures</p>
  <p><b>关键词</b>：based additive manufacturing, Transfer learning, additive manufacturing, emerging field, field to reuse</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transfer learning (TL) based additive manufacturing (AM) modeling is an
emerging field to reuse the data from historical products and mitigate the data
insufficiency in modeling new products. Although some trials have been
conducted recently, the inherent challenges of applying TL in AM modeling are
seldom discussed, e.g., which source domain to use, how much target data is
needed, and whether to apply data preprocessing techniques. This paper aims to
answer those questions through a case study defined based on an open-source
dataset about metal AM products. In the case study, five TL methods are
integrated with decision tree regression (DTR) and artificial neural network
(ANN) to construct six TL-based models, whose performances are then compared
with the baseline DTR and ANN in a proposed validation framework. The
comparisons are used to quantify the performance of applied TL methods and are
discussed from the perspective of similarity, training data size, and data
preprocessing. Finally, the source AM domain with larger qualitative similarity
and a certain range of target-to-source training data size ratio are
recommended. Besides, the data preprocessing should be performed carefully to
balance the modeling performance and the performance improvement due to TL.</p>
  </details>
</details>
<details>
  <summary>101. <b>标题：Vanishing Activations: A Symptom of Deep Capsule Networks</b></summary>
  <p><b>编号</b>：[330]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11178</p>
  <p><b>作者</b>：Miles Everett,  Mingjun Zhong,  Georgios Leontidis</p>
  <p><b>备注</b>：9 pages, 7 figures</p>
  <p><b>关键词</b>：Neural Networks utilizing, original Capsule Network, Capsule Networks, Networks utilizing vector, visual concepts evolve</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Capsule Networks, an extension to Neural Networks utilizing vector or matrix
representations instead of scalars, were initially developed to create a
dynamic parse tree where visual concepts evolve from parts to complete objects.
Early implementations of Capsule Networks achieved and maintain
state-of-the-art results on various datasets. However, recent studies have
revealed shortcomings in the original Capsule Network architecture, notably its
failure to construct a parse tree and its susceptibility to vanishing gradients
when deployed in deeper networks. This paper extends the investigation to a
range of leading Capsule Network architectures, demonstrating that these issues
are not confined to the original design. We argue that the majority of Capsule
Network research has produced architectures that, while modestly divergent from
the original Capsule Network, still retain a fundamentally similar structure.
We posit that this inherent design similarity might be impeding the scalability
of Capsule Networks. Our study contributes to the broader discussion on
improving the robustness and scalability of Capsule Networks.</p>
  </details>
</details>
<details>
  <summary>102. <b>标题：Photo-zSNthesis: Converting Type Ia Supernova Lightcurves to Redshift  Estimates via Deep Learning</b></summary>
  <p><b>编号</b>：[331]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11869</p>
  <p><b>作者</b>：Helen Qu,  Masao Sako</p>
  <p><b>备注</b>：submitted to ApJ</p>
  <p><b>关键词</b>：Type Ia supernovae, thousands of Type, Upcoming photometric surveys, vastly outpacing, Sloan Digital Sky</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Upcoming photometric surveys will discover tens of thousands of Type Ia
supernovae (SNe Ia), vastly outpacing the capacity of our spectroscopic
resources. In order to maximize the science return of these observations in the
absence of spectroscopic information, we must accurately extract key
parameters, such as SN redshifts, with photometric information alone. We
present Photo-zSNthesis, a convolutional neural network-based method for
predicting full redshift probability distributions from multi-band supernova
lightcurves, tested on both simulated Sloan Digital Sky Survey (SDSS) and Vera
C. Rubin Legacy Survey of Space and Time (LSST) data as well as observed SDSS
SNe. We show major improvements over predictions from existing methods on both
simulations and real observations as well as minimal redshift-dependent bias,
which is a challenge due to selection effects, e.g. Malmquist bias. The PDFs
produced by this method are well-constrained and will maximize the cosmological
constraining power of photometric SNe Ia samples.</p>
  </details>
</details>
<details>
  <summary>103. <b>标题：Q-malizing flow and infinitesimal density ratio estimation</b></summary>
  <p><b>编号</b>：[333]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11857</p>
  <p><b>作者</b>：Chen Xu,  Xiuyuan Cheng,  Yao Xie</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Continuous normalizing flows, normal distribution, Continuous normalizing, flow network transports, data distribution</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Continuous normalizing flows are widely used in generative tasks, where a
flow network transports from a data distribution $P$ to a normal distribution.
A flow model that can transport from $P$ to an arbitrary $Q$, where both $P$
and $Q$ are accessible via finite samples, would be of various application
interests, particularly in the recently developed telescoping density ratio
estimation (DRE) which calls for the construction of intermediate densities to
bridge between $P$ and $Q$. In this work, we propose such a ``Q-malizing flow''
by a neural-ODE model which is trained to transport invertibly from $P$ to $Q$
(and vice versa) from empirical samples and is regularized by minimizing the
transport cost. The trained flow model allows us to perform infinitesimal DRE
along the time-parametrized $\log$-density by training an additional
continuous-time flow network using classification loss, which estimates the
time-partial derivative of the $\log$-density. Integrating the time-score
network along time provides a telescopic DRE between $P$ and $Q$ that is more
stable than a one-step DRE. The effectiveness of the proposed model is
empirically demonstrated on mutual information estimation from high-dimensional
data and energy-based generative models of image data.</p>
  </details>
</details>
<details>
  <summary>104. <b>标题：Improving Multimodal Joint Variational Autoencoders through Normalizing  Flows and Correlation Analysis</b></summary>
  <p><b>编号</b>：[335]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11832</p>
  <p><b>作者</b>：Agathe Senellart,  Clément Chadebec,  Stéphanie Allassonnière</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：multimodal variational autoencoder, multimodal variational, variational autoencoder, autoencoder that enables, enables to generate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a new multimodal variational autoencoder that enables to generate
from the joint distribution and conditionally to any number of complex
modalities. The unimodal posteriors are conditioned on the Deep Canonical
Correlation Analysis embeddings which preserve the shared information across
modalities leading to more coherent cross-modal generations. Furthermore, we
use Normalizing Flows to enrich the unimodal posteriors and achieve more
diverse data generation. Finally, we propose to use a Product of Experts for
inferring one modality from several others which makes the model scalable to
any number of modalities. We demonstrate that our method improves likelihood
estimates, diversity of the generations and in particular coherence metrics in
the conditional generations on several datasets.</p>
  </details>
</details>
<details>
  <summary>105. <b>标题：PANNA 2.0: Efficient neural network interatomic potentials and new  architectures</b></summary>
  <p><b>编号</b>：[336]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11805</p>
  <p><b>作者</b>：Franco Pellegrini,  Ruggero Lot,  Yusuf Shaidu,  Emine Küçükbenli</p>
  <p><b>备注</b>：Submitted to J Chem Phys Special Topic on Software for Atomistic Machine Learning</p>
  <p><b>关键词</b>：Properties from Artificial, Artificial Neural, neural network interatomic, latest release, release of PANNA</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present the latest release of PANNA 2.0 (Properties from Artificial Neural
Network Architectures), a code for the generation of neural network interatomic
potentials based on local atomic descriptors and multilayer perceptrons. Built
on a new back end, this new release of PANNA features improved tools for
customizing and monitoring network training, better GPU support including a
fast descriptor calculator, new plugins for external codes and a new
architecture for the inclusion of long-range electrostatic interactions through
a variational charge equilibration scheme. We present an overview of the main
features of the new code, and several benchmarks comparing the accuracy of
PANNA models to the state of the art, on commonly used benchmarks as well as
richer datasets.</p>
  </details>
</details>
<details>
  <summary>106. <b>标题：Multi-Objective Optimization Using the R2 Utility</b></summary>
  <p><b>编号</b>：[337]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11774</p>
  <p><b>作者</b>：Ben Tu,  Nikolas Kantas,  Robert M. Lee,  Behrang Shafei</p>
  <p><b>备注</b>：The code is available at: this https URL</p>
  <p><b>关键词</b>：optimization, points which describe, identify a collection, collection of points, problem</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The goal of multi-objective optimization is to identify a collection of
points which describe the best possible trade-offs between the multiple
objectives. In order to solve this vector-valued optimization problem,
practitioners often appeal to the use of scalarization functions in order to
transform the multi-objective problem into a collection of single-objective
problems. This set of scalarized problems can then be solved using traditional
single-objective optimization techniques. In this work, we formalise this
convention into a general mathematical framework. We show how this strategy
effectively recasts the original multi-objective optimization problem into a
single-objective optimization problem defined over sets. An appropriate class
of objective functions for this new problem is the R2 utility function, which
is defined as a weighted integral over the scalarized optimization problems. We
show that this utility function is a monotone and submodular set function,
which can be optimised effectively using greedy optimization algorithms. We
analyse the performance of these greedy algorithms both theoretically and
empirically. Our analysis largely focusses on Bayesian optimization, which is a
popular probabilistic framework for black-box optimization.</p>
  </details>
</details>
<details>
  <summary>107. <b>标题：Transfer operators on graphs: Spectral clustering and beyond</b></summary>
  <p><b>编号</b>：[339]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11766</p>
  <p><b>作者</b>：Stefan Klus,  Maia Trower</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：artificial neural networks, analyzing complex interconnected, complex interconnected systems, integrated circuits, power grids</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Graphs and networks play an important role in modeling and analyzing complex
interconnected systems such as transportation networks, integrated circuits,
power grids, citation graphs, and biological and artificial neural networks.
Graph clustering algorithms can be used to detect groups of strongly connected
vertices and to derive coarse-grained models. We define transfer operators such
as the Koopman operator and the Perron-Frobenius operator on graphs, study
their spectral properties, introduce Galerkin projections of these operators,
and illustrate how reduced representations can be estimated from data. In
particular, we show that spectral clustering of undirected graphs can be
interpreted in terms of eigenfunctions of the Koopman operator and propose
novel clustering algorithms for directed graphs based on generalized transfer
operators. We demonstrate the efficacy of the resulting algorithms on several
benchmark problems and provide different interpretations of clusters.</p>
  </details>
</details>
<details>
  <summary>108. <b>标题：Recycle-and-Distill: Universal Compression Strategy for  Transformer-based Speech SSL Models with Attention Map Reusing and Masking  Distillation</b></summary>
  <p><b>编号</b>：[343]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11685</p>
  <p><b>作者</b>：Kangwook Jang,  Sungnyun Kim,  Se-Young Yun,  Hoirin Kim</p>
  <p><b>备注</b>：Interspeech 2023. Code URL: this https URL</p>
  <p><b>关键词</b>：speech self-supervised learning, Transformer-based speech self-supervised, self-supervised learning, speech SSL models, Transformer-based speech</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transformer-based speech self-supervised learning (SSL) models, such as
HuBERT, show surprising performance in various speech processing tasks.
However, huge number of parameters in speech SSL models necessitate the
compression to a more compact model for wider usage in academia or small
companies. In this study, we suggest to reuse attention maps across the
Transformer layers, so as to remove key and query parameters while retaining
the number of layers. Furthermore, we propose a novel masking distillation
strategy to improve the student model's speech representation quality. We
extend the distillation loss to utilize both masked and unmasked speech frames
to fully leverage the teacher model's high-quality representation. Our
universal compression strategy yields the student model that achieves phoneme
error rate (PER) of 7.72% and word error rate (WER) of 9.96% on the SUPERB
benchmark.</p>
  </details>
</details>
<details>
  <summary>109. <b>标题：Moment Matching Denoising Gibbs Sampling</b></summary>
  <p><b>编号</b>：[344]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11650</p>
  <p><b>作者</b>：Mingtian Zhang,  Alex Hawkins-Hooker,  Brooks Paige,  David Barber</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：modeling complex data, complex data distributions, offer a versatile, Denoising Score Matching, modeling complex</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Energy-Based Models (EBMs) offer a versatile framework for modeling complex
data distributions. However, training and sampling from EBMs continue to pose
significant challenges. The widely-used Denoising Score Matching (DSM) method
for scalable EBM training suffers from inconsistency issues, causing the energy
model to learn a `noisy' data distribution. In this work, we propose an
efficient sampling framework: (pseudo)-Gibbs sampling with moment matching,
which enables effective sampling from the underlying clean model when given a
`noisy' model that has been well-trained via DSM. We explore the benefits of
our approach compared to related methods and demonstrate how to scale the
method to high-dimensional datasets.</p>
  </details>
</details>
<details>
  <summary>110. <b>标题：The Deep Promotion Time Cure Model</b></summary>
  <p><b>编号</b>：[347]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11575</p>
  <p><b>作者</b>：Victor Medina-Olivares,  Stefan Lessmann,  Nadja Klein</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：cure fractions based, deep neural network, flexible survivals models, survivals models integrated, neural network framework</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a novel method for predicting time-to-event in the presence of
cure fractions based on flexible survivals models integrated into a deep neural
network framework. Our approach allows for non-linear relationships and
high-dimensional interactions between covariates and survival and is suitable
for large-scale applications. Furthermore, we allow the method to incorporate
an identified predictor formed of an additive decomposition of interpretable
linear and non-linear effects and add an orthogonalization layer to capture
potential higher dimensional interactions. We demonstrate the usefulness and
computational efficiency of our method via simulations and apply it to a large
portfolio of US mortgage loans. Here, we find not only a better predictive
performance of our framework but also a more realistic picture of covariate
effects.</p>
  </details>
</details>
<details>
  <summary>111. <b>标题：Generalizing to new calorimeter geometries with Geometry-Aware  Autoregressive Models (GAAMs) for fast calorimeter simulation</b></summary>
  <p><b>编号</b>：[350]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11531</p>
  <p><b>作者</b>：Junze Liu,  Aishik Ghosh,  Dylan Smith,  Pierre Baldi,  Daniel Whiteson</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：particle physics, computationally very expensive, collision products, analysis in particle, Large Hadron Collider</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generation of simulated detector response to collision products is crucial to
data analysis in particle physics, but computationally very expensive. One
subdetector, the calorimeter, dominates the computational time due to the high
granularity of its cells and complexity of the interaction. Generative models
can provide more rapid sample production, but currently require significant
effort to optimize performance for specific detector geometries, often
requiring many networks to describe the varying cell sizes and arrangements,
which do not generalize to other geometries. We develop a {\it geometry-aware}
autoregressive model, which learns how the calorimeter response varies with
geometry, and is capable of generating simulated responses to unseen geometries
without additional training. The geometry-aware model outperforms a baseline,
unaware model by 50\% in metrics such as the Wasserstein distance between
generated and true distributions of key quantities which summarize the
simulated response. A single geometry-aware model could replace the hundreds of
generative models currently designed for calorimeter simulation by physicists
analyzing data collected at the Large Hadron Collider. For the study of future
detectors, such a foundational model will be a crucial tool, dramatically
reducing the large upfront investment usually needed to develop generative
calorimeter models.</p>
  </details>
</details>
<details>
  <summary>112. <b>标题：JOINEDTrans: Prior Guided Multi-task Transformer for Joint Optic  Disc/Cup Segmentation and Fovea Detection</b></summary>
  <p><b>编号</b>：[352]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11504</p>
  <p><b>作者</b>：Huaqing He,  Li Lin,  Zhiyuan Cai,  Pujin Cheng,  Xiaoying Tang</p>
  <p><b>备注</b>：11 pages, 6 figures</p>
  <p><b>关键词</b>：Deep learning-based image, analyzing retinal landmarks, optic disc, optic cup, learning-based image segmentation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning-based image segmentation and detection models have largely
improved the efficiency of analyzing retinal landmarks such as optic disc (OD),
optic cup (OC), and fovea. However, factors including ophthalmic
disease-related lesions and low image quality issues may severely complicate
automatic OD/OC segmentation and fovea detection. Most existing works treat the
identification of each landmark as a single task, and take into account no
prior information. To address these issues, we propose a prior guided
multi-task transformer framework for joint OD/OC segmentation and fovea
detection, named JOINEDTrans. JOINEDTrans effectively combines various spatial
features of the fundus images, relieving the structural distortions induced by
lesions and other imaging issues. It contains a segmentation branch and a
detection branch. To be noted, we employ an encoder pretrained in a vessel
segmentation task to effectively exploit the positional relationship among
vessel, OD/OC, and fovea, successfully incorporating spatial prior into the
proposed JOINEDTrans framework. There are a coarse stage and a fine stage in
JOINEDTrans. In the coarse stage, OD/OC coarse segmentation and fovea heatmap
localization are obtained through a joint segmentation and detection module. In
the fine stage, we crop regions of interest for subsequent refinement and use
predictions obtained in the coarse stage to provide additional information for
better performance and faster convergence. Experimental results demonstrate
that JOINEDTrans outperforms existing state-of-the-art methods on the publicly
available GAMMA, REFUGE, and PALM fundus image datasets. We make our code
available at this https URL</p>
  </details>
</details>
<details>
  <summary>113. <b>标题：Meta-learning for heterogeneous treatment effect estimation with  closed-form solvers</b></summary>
  <p><b>编号</b>：[357]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11353</p>
  <p><b>作者</b>：Tomoharu Iwata,  Yoichi Chikahara</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：average treatment effect, conditional average treatment, CATE estimation, treatment effect, article proposes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This article proposes a meta-learning method for estimating the conditional
average treatment effect (CATE) from a few observational data. The proposed
method learns how to estimate CATEs from multiple tasks and uses the knowledge
for unseen tasks. In the proposed method, based on the meta-learner framework,
we decompose the CATE estimation problem into sub-problems. For each
sub-problem, we formulate our estimation models using neural networks with
task-shared and task-specific parameters. With our formulation, we can obtain
optimal task-specific parameters in a closed form that are differentiable with
respect to task-shared parameters, making it possible to perform effective
meta-learning. The task-shared parameters are trained such that the expected
CATE estimation performance in few-shot settings is improved by minimizing the
difference between a CATE estimated with a large amount of data and one
estimated with just a few data. Our experimental results demonstrate that our
method outperforms the existing meta-learning approaches and CATE estimation
methods.</p>
  </details>
</details>
<details>
  <summary>114. <b>标题：Multi-Fidelity Machine Learning for Excited State Energies of Molecules</b></summary>
  <p><b>编号</b>：[361]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11292</p>
  <p><b>作者</b>：Vivin Vinod,  Sayan Maity,  Peter Zaspel,  Ulrich Kleinekathöfer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：excited state energies, molecular excited states, accurate excited state, machine learning, excited state</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The accurate but fast calculation of molecular excited states is still a very
challenging topic. For many applications, detailed knowledge of the energy
funnel in larger molecular aggregates is of key importance requiring highly
accurate excited state energies. To this end, machine learning techniques can
be an extremely useful tool though the cost of generating highly accurate
training datasets still remains a severe challenge. To overcome this hurdle,
this work proposes the use of multi-fidelity machine learning where very little
training data from high accuracies is combined with cheaper and less accurate
data to achieve the accuracy of the costlier level. In the present study, the
approach is employed to predict the first excited state energies for three
molecules of increasing size, namely, benzene, naphthalene, and anthracene. The
energies are trained and tested for conformations stemming from classical
molecular dynamics simulations and from real-time density functional
tight-binding calculations. It can be shown that the multi-fidelity machine
learning model can achieve the same accuracy as a machine learning model built
only on high cost training data while having a much lower computational effort
to generate the data. The numerical gain observed in these benchmark test
calculations was over a factor of 30 but certainly can be much higher for high
accuracy data.</p>
  </details>
</details>
<details>
  <summary>115. <b>标题：Federated learning for secure development of AI models for Parkinson's  disease detection using speech from different languages</b></summary>
  <p><b>编号</b>：[362]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11284</p>
  <p><b>作者</b>：Soroosh Tayebi Arasteh,  Cristian David Rios-Urrego,  Elmar Noeth,  Andreas Maier,  Seung Hee Yang,  Jan Rusz,  Juan Rafael Orozco-Arroyave</p>
  <p><b>备注</b>：Accepted for INTERSPEECH 2023</p>
  <p><b>关键词</b>：neurological disorder impacting, Parkinson disease, neurological disorder, disorder impacting, impacting a person</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Parkinson's disease (PD) is a neurological disorder impacting a person's
speech. Among automatic PD assessment methods, deep learning models have gained
particular interest. Recently, the community has explored cross-pathology and
cross-language models which can improve diagnostic accuracy even further.
However, strict patient data privacy regulations largely prevent institutions
from sharing patient speech data with each other. In this paper, we employ
federated learning (FL) for PD detection using speech signals from 3 real-world
language corpora of German, Spanish, and Czech, each from a separate
institution. Our results indicate that the FL model outperforms all the local
models in terms of diagnostic accuracy, while not performing very differently
from the model based on centrally combined training sets, with the advantage of
not requiring any data sharing among collaborators. This will simplify
inter-institutional collaborations, resulting in enhancement of patient
outcomes.</p>
  </details>
</details>
<details>
  <summary>116. <b>标题：Real-Time Variational Method for Learning Neural Trajectory and its  Dynamics</b></summary>
  <p><b>编号</b>：[363]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11278</p>
  <p><b>作者</b>：Matthew Dowling,  Yuan Zhao,  Il Memming Park</p>
  <p><b>备注</b>：Published at ICLR 2023</p>
  <p><b>关键词</b>：instrumental in computational, computational neuroscience, neuroscience for reasoning, neural computation, Latent variable models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Latent variable models have become instrumental in computational neuroscience
for reasoning about neural computation. This has fostered the development of
powerful offline algorithms for extracting latent neural trajectories from
neural recordings. However, despite the potential of real time alternatives to
give immediate feedback to experimentalists, and enhance experimental design,
they have received markedly less attention. In this work, we introduce the
exponential family variational Kalman filter (eVKF), an online recursive
Bayesian method aimed at inferring latent trajectories while simultaneously
learning the dynamical system generating them. eVKF works for arbitrary
likelihoods and utilizes the constant base measure exponential family to model
the latent state stochasticity. We derive a closed-form variational analogue to
the predict step of the Kalman filter which leads to a provably tighter bound
on the ELBO compared to another online variational method. We validate our
method on synthetic and real-world data, and, notably, show that it achieves
competitive performance</p>
  </details>
</details>
<details>
  <summary>117. <b>标题：Assessing Exoplanet Habitability through Data-driven Approaches: A  Comprehensive Literature Review</b></summary>
  <p><b>编号</b>：[366]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11204</p>
  <p><b>作者</b>：Mithil Sai Jakka</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：celestial bodies produce, machine learning, challenging scientists, navigate the vast, bodies produce</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The exploration and study of exoplanets remain at the frontier of
astronomical research, challenging scientists to continuously innovate and
refine methodologies to navigate the vast, complex data these celestial bodies
produce. This literature the review aims to illuminate the emerging trends and
advancements within this sphere, specifically focusing on the interplay between
exoplanet detection, classification, and visualization, and the the
increasingly pivotal role of machine learning and computational models. Our
journey through this realm of exploration commences with a comprehensive
analysis of fifteen meticulously selected, seminal papers in the field. These
papers, each representing a distinct facet of exoplanet research, collectively
offer a multi-dimensional perspective on the current state of the field. They
provide valuable insights into the innovative application of machine learning
techniques to overcome the challenges posed by the analysis and interpretation
of astronomical data. From the application of Support Vector Machines (SVM) to
Deep Learning models, the review encapsulates the broad spectrum of machine
learning approaches employed in exoplanet research. The review also seeks to
unravel the story woven by the data within these papers, detailing the triumphs
and tribulations of the field. It highlights the increasing reliance on diverse
datasets, such as Kepler and TESS, and the push for improved accuracy in
exoplanet detection and classification models. The narrative concludes with key
takeaways and insights, drawing together the threads of research to present a
cohesive picture of the direction in which the field is moving. This literature
review, therefore, serves not just as an academic exploration, but also as a
narrative of scientific discovery and innovation in the quest to understand our
cosmic neighborhood.</p>
  </details>
</details>
<details>
  <summary>118. <b>标题：At-Admission Prediction of Mortality and Pulmonary Embolism in COVID-19  Patients Using Statistical and Machine Learning Methods: An International  Cohort Study</b></summary>
  <p><b>编号</b>：[367]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11199</p>
  <p><b>作者</b>：Munib Mesinovic,  Xin Ci Wong,  Giri Shan Rajahram,  Barbara Wanjiru Citarella,  Kalaiarasu M. Peariasamy,  Frank van Someren Greve,  Piero Olliaro,  Laura Merson,  Lei Clifton,  Christiana Kartsonaki,  ISARIC Characterisation Group</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：million cases, reported globally, million deaths, million, September</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>By September, 2022, more than 600 million cases of SARS-CoV-2 infection have
been reported globally, resulting in over 6.5 million deaths. COVID-19
mortality risk estimators are often, however, developed with small
unrepresentative samples and with methodological limitations. It is highly
important to develop predictive tools for pulmonary embolism (PE) in COVID-19
patients as one of the most severe preventable complications of COVID-19. Using
a dataset of more than 800,000 COVID-19 patients from an international cohort,
we propose a cost-sensitive gradient-boosted machine learning model that
predicts occurrence of PE and death at admission. Logistic regression, Cox
proportional hazards models, and Shapley values were used to identify key
predictors for PE and death. Our prediction model had a test AUROC of 75.9% and
74.2%, and sensitivities of 67.5% and 72.7% for PE and all-cause mortality
respectively on a highly diverse and held-out test set. The PE prediction model
was also evaluated on patients in UK and Spain separately with test results of
74.5% AUROC, 63.5% sensitivity and 78.9% AUROC, 95.7% sensitivity. Age, sex,
region of admission, comorbidities (chronic cardiac and pulmonary disease,
dementia, diabetes, hypertension, cancer, obesity, smoking), and symptoms (any,
confusion, chest pain, fatigue, headache, fever, muscle or joint pain,
shortness of breath) were the most important clinical predictors at admission.
Our machine learning model developed from an international cohort can serve to
better regulate hospital risk prioritisation of at-risk patients.</p>
  </details>
</details>
<details>
  <summary>119. <b>标题：Vaxformer: Antigenicity-controlled Transformer for Vaccine Design  Against SARS-CoV-2</b></summary>
  <p><b>编号</b>：[368]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11194</p>
  <p><b>作者</b>：Aryo Pradipta Gema,  Michał Kobiela,  Achille Fraisse,  Ajitha Rajan,  Diego A. Oyarzún,  Javier Antonio Alfaro</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：pandemic has emphasised, emphasised the importance, importance of developing, developing a universal, protect against current</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The SARS-CoV-2 pandemic has emphasised the importance of developing a
universal vaccine that can protect against current and future variants of the
virus. The present study proposes a novel conditional protein Language Model
architecture, called Vaxformer, which is designed to produce natural-looking
antigenicity-controlled SARS-CoV-2 spike proteins. We evaluate the generated
protein sequences of the Vaxformer model using DDGun protein stability measure,
netMHCpan antigenicity score, and a structure fidelity score with AlphaFold to
gauge its viability for vaccine development. Our results show that Vaxformer
outperforms the existing state-of-the-art Conditional Variational Autoencoder
model to generate antigenicity-controlled SARS-CoV-2 spike proteins. These
findings suggest promising opportunities for conditional Transformer models to
expand our understanding of vaccine design and their role in mitigating global
health challenges. The code used in this study is available at
this https URL .</p>
  </details>
</details>
<details>
  <summary>120. <b>标题：Assessing the predicting power of GPS data for aftershocks forecasting</b></summary>
  <p><b>编号</b>：[369]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11183</p>
  <p><b>作者</b>：Vincenzo Maria Schimmenti,  Giuseppe Petrillo,  Alberto Rosso,  Francois P. Landes</p>
  <p><b>备注</b>：15 pages main + appendix. 3 figures main, 2 appendix</p>
  <p><b>关键词</b>：machine learning approach, Global Positioning System, present a machine, machine learning, aftershock forecasting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a machine learning approach for the aftershock forecasting of
Japanese earthquake catalogue from 2015 to 2019. Our method takes as sole input
the ground surface deformation as measured by Global Positioning System (GPS)
stations at the day of the mainshock, and processes it with a Convolutional
Neural Network (CNN), thus capturing the input's spatial correlations. Despite
the moderate amount of data the performance of this new approach is very
promising. The accuracy of the prediction heavily relies on the density of GPS
stations: the predictive power is lost when the mainshocks occur far from
measurement stations, as in offshore regions.</p>
  </details>
</details>
<h1>人工智能</h1>
<details>
  <summary>1. <b>标题：Scaling laws for language encoding models in fMRI</b></summary>
  <p><b>编号</b>：[5]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11863</p>
  <p><b>作者</b>：Richard Antonello,  Aditya Vaidya,  Alexander G. Huth</p>
  <p><b>备注</b>：Under review</p>
  <p><b>关键词</b>：transformer-based unidirectional language, predicting brain responses, unidirectional language models, Representations from transformer-based, transformer-based unidirectional</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Representations from transformer-based unidirectional language models are
known to be effective at predicting brain responses to natural language.
However, most studies comparing language models to brains have used GPT-2 or
similarly sized language models. Here we tested whether larger open-source
models such as those from the OPT and LLaMA families are better at predicting
brain responses recorded using fMRI. Mirroring scaling results from other
contexts, we found that brain prediction performance scales log-linearly with
model size from 125M to 30B parameter models, with ~15% increased encoding
performance as measured by correlation with a held-out test set across 3
subjects. Similar log-linear behavior was observed when scaling the size of the
fMRI training set. We also characterized scaling for acoustic encoding models
that use HuBERT, WavLM, and Whisper, and we found comparable improvements with
model size. A noise ceiling analysis of these large, high-performance encoding
models showed that performance is nearing the theoretical maximum for brain
areas such as the precuneus and higher auditory cortex. These results suggest
that increasing scale in both models and data will yield incredibly effective
models of language processing in the brain, enabling better scientific
understanding as well as applications such as decoding.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Multimodal Web Navigation with Instruction-Finetuned Foundation Models</b></summary>
  <p><b>编号</b>：[11]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11854</p>
  <p><b>作者</b>：Hiroki Furuta,  Ofir Nachum,  Kuang-Huei Lee,  Yutaka Matsuo,  Shixiang Shane Gu,  Izzeddin Gur</p>
  <p><b>备注</b>：Website: this https URL</p>
  <p><b>关键词</b>：online reinforcement learning, domain-specific model designs, autonomous web navigation, reinforcement learning, generalization from rich</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The progress of autonomous web navigation has been hindered by the dependence
on billions of exploratory interactions via online reinforcement learning, and
domain-specific model designs that make it difficult to leverage generalization
from rich out-of-domain data. In this work, we study data-driven offline
training for web agents with vision-language foundation models. We propose an
instruction-following multimodal agent, WebGUM, that observes both webpage
screenshots and HTML pages and outputs web navigation actions, such as click
and type. WebGUM is trained by jointly finetuning an instruction-finetuned
language model and a vision transformer on a large corpus of demonstrations. We
empirically demonstrate this recipe improves the agent's ability of grounded
visual perception, HTML comprehension and multi-step reasoning, outperforming
prior works by a significant margin. On the MiniWoB benchmark, we improve over
the previous best offline methods by more than 31.9%, being close to reaching
online-finetuned SoTA. On the WebShop benchmark, our 3-billion-parameter model
achieves superior performance to the existing SoTA, PaLM-540B. We also collect
347K high-quality demonstrations using our trained models, 38 times larger than
prior work, and make them available to promote future research in this
direction.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：RxnScribe: A Sequence Generation Model for Reaction Diagram Parsing</b></summary>
  <p><b>编号</b>：[14]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11845</p>
  <p><b>作者</b>：Yujie Qian,  Jiang Guo,  Zhengkai Tu,  Connor W. Coley,  Regina Barzilay</p>
  <p><b>备注</b>：To be published in the Journal of Chemical Information and Modeling</p>
  <p><b>关键词</b>：extracting reaction schemes, chemistry literature, Reaction, extracting reaction, reaction schemes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reaction diagram parsing is the task of extracting reaction schemes from a
diagram in the chemistry literature. The reaction diagrams can be arbitrarily
complex, thus robustly parsing them into structured data is an open challenge.
In this paper, we present RxnScribe, a machine learning model for parsing
reaction diagrams of varying styles. We formulate this structured prediction
task with a sequence generation approach, which condenses the traditional
pipeline into an end-to-end model. We train RxnScribe on a dataset of 1,378
diagrams and evaluate it with cross validation, achieving an 80.0% soft match
F1 score, with significant improvements over previous models. Our code and data
are publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：AI's Regimes of Representation: A Community-centered Study of  Text-to-Image Models in South Asia</b></summary>
  <p><b>编号</b>：[15]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11844</p>
  <p><b>作者</b>：Rida Qadri,  Renee Shelby,  Cynthia L. Bennett,  Emily Denton</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：South Asian context, South Asian cultures, South Asian, Asian context, viewing South Asian</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a community-centered study of cultural limitations of
text-to-image (T2I) models in the South Asian context. We theorize these
failures using scholarship on dominant media regimes of representations and
locate them within participants' reporting of their existing social
marginalizations. We thus show how generative AI can reproduce an outsiders
gaze for viewing South Asian cultures, shaped by global and regional power
inequities. By centering communities as experts and soliciting their
perspectives on T2I limitations, our study adds rich nuance into existing
evaluative frameworks and deepens our understanding of the culturally-specific
ways AI technologies can fail in non-Western and Global South settings. We
distill lessons for responsible development of T2I models, recommending
concrete pathways forward that can allow for recognition of structural
inequalities.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Comparing Software Developers with ChatGPT: An Empirical Investigation</b></summary>
  <p><b>编号</b>：[18]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11837</p>
  <p><b>作者</b>：Nathalia Nascimento,  Paulo Alencar,  Donald Cowan</p>
  <p><b>备注</b>：12 pages</p>
  <p><b>关键词</b>：Software Engineering, theory to reality, advent of automation, transitioned from theory, Artificial Intelligence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The advent of automation in particular Software Engineering (SE) tasks has
transitioned from theory to reality. Numerous scholarly articles have
documented the successful application of Artificial Intelligence to address
issues in areas such as project management, modeling, testing, and development.
A recent innovation is the introduction of ChatGPT, an ML-infused chatbot,
touted as a resource proficient in generating programming codes and formulating
software testing strategies for developers and testers respectively. Although
there is speculation that AI-based computation can increase productivity and
even substitute software engineers in software development, there is currently
a lack of empirical evidence to verify this. Moreover, despite the primary
focus on enhancing the accuracy of AI systems, non-functional requirements
including energy efficiency, vulnerability, fairness (i.e., human bias), and
safety frequently receive insufficient attention. This paper posits that a
comprehensive comparison of software engineers and AI-based solutions,
considering various evaluation criteria, is pivotal in fostering human-machine
collaboration, enhancing the reliability of AI-based methods, and understanding
task suitability for humans or AI. Furthermore, it facilitates the effective
implementation of cooperative work structures and human-in-the-loop processes.
This paper conducts an empirical investigation, contrasting the performance of
software engineers and AI systems, like ChatGPT, across different evaluation
metrics. The empirical study includes a case of assessing ChatGPT-generated
code versus code produced by developers and uploaded in Leetcode.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Complexity of Neural Network Training and ETR: Extensions with  Effectively Continuous Functions</b></summary>
  <p><b>编号</b>：[20]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11833</p>
  <p><b>作者</b>：Teemu Hankala,  Miika Hannula,  Juha Kontinen,  Jonni Virtema</p>
  <p><b>备注</b>：Revised version of a manuscript sent for review in April 2023</p>
  <p><b>关键词</b>：sigmoid activation function, activation functions, activation, training problem, sigmoid activation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the complexity of the problem of training neural networks defined
via various activation functions. The training problem is known to be
existsR-complete with respect to linear activation functions and the ReLU
activation function. We consider the complexity of the problem with respect to
the sigmoid activation function and other effectively continuous functions. We
show that these training problems are polynomial-time many-one bireducible to
the existential theory of the reals extended with the corresponding activation
functions. In particular, we establish that the sigmoid activation function
leads to the existential theory of the reals with the exponential function. It
is thus open, and equivalent with the decidability of the existential theory of
the reals with the exponential function, whether training neural networks using
the sigmoid activation function is algorithmically solvable. In contrast, we
obtain that the training problem is undecidable if sinusoidal activation
functions are considered. Finally, we obtain general upper bounds for the
complexity of the training problem in the form of low levels of the
arithmetical hierarchy.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Regularization of Soft Actor-Critic Algorithms with Automatic  Temperature Adjustment</b></summary>
  <p><b>编号</b>：[21]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11831</p>
  <p><b>作者</b>：Ben You</p>
  <p><b>备注</b>：This work aims to clarify the ambiguity and revise certain errors in the original soft actor-cirtic articles</p>
  <p><b>关键词</b>：regularize the Soft, work presents, presents a comprehensive, comprehensive analysis, analysis to regularize</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This work presents a comprehensive analysis to regularize the Soft
Actor-Critic (SAC) algorithm with automatic temperature adjustment. The the
policy evaluation, the policy improvement and the temperature adjustment are
reformulated, addressing certain modification and enhancing the clarity of the
original theory in a more explicit manner.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Appraising the Potential Uses and Harms of LLMs for Medical Systematic  Reviews</b></summary>
  <p><b>编号</b>：[22]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11828</p>
  <p><b>作者</b>：Hye Sun Yun,  Iain J. Marshall,  Thomas Trikalinos,  Byron C. Wallace</p>
  <p><b>备注</b>：33 pages, 3 figures, 7 tables</p>
  <p><b>关键词</b>：informing clinical decision, clinical decision making, crucial for informing, informing clinical, clinical decision</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Medical systematic reviews are crucial for informing clinical decision making
and healthcare policy. But producing such reviews is onerous and
time-consuming. Thus, high-quality evidence synopses are not available for many
questions and may be outdated even when they are available. Large language
models (LLMs) are now capable of generating long-form texts, suggesting the
tantalizing possibility of automatically generating literature reviews on
demand. However, LLMs sometimes generate inaccurate (and potentially
misleading) texts by hallucinating or omitting important information. In the
healthcare context, this may render LLMs unusable at best and dangerous at
worst. Most discussion surrounding the benefits and risks of LLMs have been
divorced from specific applications. In this work, we seek to qualitatively
characterize the potential utility and risks of LLMs for assisting in
production of medical evidence reviews. We conducted 16 semi-structured
interviews with international experts in systematic reviews, grounding
discussion in the context of generating evidence reviews. Domain experts
indicated that LLMs could aid writing reviews, as a tool for drafting or
creating plain language summaries, generating templates or suggestions,
distilling information, crosschecking, and synthesizing or interpreting text
inputs. But they also identified issues with model outputs and expressed
concerns about potential downstream harms of confidently composed but
inaccurate LLM outputs which might mislead. Other anticipated potential
downstream harms included lessened accountability and proliferation of
automatically generated reviews that might be of low quality. Informed by this
qualitative analysis, we identify criteria for rigorous evaluation of
biomedical LLMs aligned with domain expert views.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：STOAT: Structured Data to Analytical Text With Controls</b></summary>
  <p><b>编号</b>：[23]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11826</p>
  <p><b>作者</b>：Deepanway Ghosal,  Preksha Nema,  Aravindan Raghuveer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：made tremendous progress, Recent language models, text generation, Recent language, made tremendous</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent language models have made tremendous progress in the structured data
to text generation task. However, these models still give sub-optimal
performance where logical inference is required to generate the descriptions.
In this work, we specifically focus on analytical text generation from
structured data such as tables. Building on the taxonomy proposed in (Gupta et
al., 2020) we focus on controllable table to text generation for the following
reasoning categories: numerical reasoning, commonsense reasoning, temporal
reasoning, table knowledge, and entity knowledge. We propose STOAT model, which
is table and reasoning aware, with vector-quantization to infuse the given
reasoning categories in the output. We observe that our model provides 10.19%,
1.13% improvement on the PARENT metric in iToTTo and Infotabs for the
analytical sentence task. We also found that our model generates 15.3% more
faithful and analytical descriptions as compared to the baseline models in
human evaluation. We curate and release two reasoning category annotated
table-to-interesting text generation datasets based on the ToTTo (Parikh et
al., 2020) and InfoTabs datasets (Gupta et al.,2020).</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Summarizing Strategy Card Game AI Competition</b></summary>
  <p><b>编号</b>：[26]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11814</p>
  <p><b>作者</b>：Jakub Kowalski,  Radosław Miernik</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：small Collectible Card, Collectible Card Game, Code and Magic, Legends of Code, based on Legends</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper concludes five years of AI competitions based on Legends of Code
and Magic (LOCM), a small Collectible Card Game (CCG), designed with the goal
of supporting research and algorithm development. The game was used in a number
of events, including Community Contests on the CodinGame platform, and Strategy
Card Game AI Competition at the IEEE Congress on Evolutionary Computation and
IEEE Conference on Games. LOCM has been used in a number of publications
related to areas such as game tree search algorithms, neural networks,
evaluation functions, and CCG deckbuilding. We present the rules of the game,
the history of organized competitions, and a listing of the participant and
their approaches, as well as some general advice on organizing AI competitions
for the research community. Although the COG 2022 edition was announced to be
the last one, the game remains available and can be played using an online
leaderboard arena.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Monte-Carlo Search for an Equilibrium in Dec-POMDPs</b></summary>
  <p><b>编号</b>：[28]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11811</p>
  <p><b>作者</b>：Yang You,  Vincent Thomas,  Francis Colas,  Olivier Buffet</p>
  <p><b>备注</b>：Accepted to UAI 2023, preliminary version</p>
  <p><b>关键词</b>：Markov decision processes, partially observable Markov, observable Markov decision, Decentralized partially observable, designing individual controllers</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Decentralized partially observable Markov decision processes (Dec-POMDPs)
formalize the problem of designing individual controllers for a group of
collaborative agents under stochastic dynamics and partial observability.
Seeking a global optimum is difficult (NEXP complete), but seeking a Nash
equilibrium -- each agent policy being a best response to the other agents --
is more accessible, and allowed addressing infinite-horizon problems with
solutions in the form of finite state controllers. In this paper, we show that
this approach can be adapted to cases where only a generative model (a
simulator) of the Dec-POMDP is available. This requires relying on a
simulation-based POMDP solver to construct an agent's FSC node by node. A
related process is used to heuristically derive initial FSCs. Experiment with
benchmarks shows that MC-JESP is competitive with exisiting Dec-POMDP solvers,
even better than many offline methods using explicit models.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：On the Fairness Impacts of Private Ensembles Models</b></summary>
  <p><b>编号</b>：[30]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11807</p>
  <p><b>作者</b>：Cuong Tran,  Ferdinando Fioretto</p>
  <p><b>备注</b>：This version is a "full version" of the associated IJCAI-23 article. arXiv admin note: substantial text overlap with arXiv:2109.08630</p>
  <p><b>关键词</b>：machine learning framework, Teacher Ensembles, Private Aggregation, combination of multiple, machine learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The Private Aggregation of Teacher Ensembles (PATE) is a machine learning
framework that enables the creation of private models through the combination
of multiple "teacher" models and a "student" model. The student model learns to
predict an output based on the voting of the teachers, and the resulting model
satisfies differential privacy. PATE has been shown to be effective in creating
private models in semi-supervised settings or when protecting data labels is a
priority. This paper explores whether the use of PATE can result in unfairness,
and demonstrates that it can lead to accuracy disparities among groups of
individuals. The paper also analyzes the algorithmic and data properties that
contribute to these disproportionate impacts, why these aspects are affecting
different groups disproportionately, and offers recommendations for mitigating
these effects</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Chain-of-thought prompting for responding to in-depth dialogue questions  with LLM</b></summary>
  <p><b>编号</b>：[34]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11792</p>
  <p><b>作者</b>：Hongru Wang,  Rui Wang,  Fei Mi,  Zezhong Wang,  Ruifeng Xu,  Kam-Fai Wong</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：textit, user status, provide insight, current status, user</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The way and content in which users ask questions can provide insight into
their current status, including their personality, emotions, and psychology.
Instead of directly prompting the large language models (LLMs), we explore how
chain-of-thought prompting helps in this scenario to perform reasoning and
planning according to user status, aiming to provide a more personalized and
engaging experience for the user query. To this end, we first construct a
benchmark of 6 dialogue or question-answering datasets in both English and
Chinese, covering 3 different aspects of user status (\textit{including}
\textit{personality}, \textit{emotion}, and \textit{psychology}). Then we
prompt the LLMs to generate the response regarding the user status as
intermediate reasoning processing. We propose a novel demonstration selection
strategy using the semantic similarity of intermediate reasoning instead of
test queries. To evaluate the effectiveness and robustness of our approach, we
conduct extensive experiments with 7 LLMs under zero-shot and one-shot
settings. The experimental results show that our approach consistently
outperforms standard prompting in terms of both \textit{helpfulness} and
\textit{acceptness} across all datasets, regardless of the LLMs used. The code
and dataset can be found at
\url{this https URL\_CoT.git}.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：DMDD: A Large-Scale Dataset for Dataset Mentions Detection</b></summary>
  <p><b>编号</b>：[41]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11779</p>
  <p><b>作者</b>：Huitong Pan,  Qi Zhang,  Eduard Dragut,  Cornelia Caragea,  Longin Jan Latecki</p>
  <p><b>备注</b>：Pre-MIT Press publication version. Submitted to TACL</p>
  <p><b>关键词</b>：dataset mention detection, automatic information extraction, dataset mention, mention detection, identify research opportunities</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The recognition of dataset names is a critical task for automatic information
extraction in scientific literature, enabling researchers to understand and
identify research opportunities. However, existing corpora for dataset mention
detection are limited in size and naming diversity. In this paper, we introduce
the Dataset Mentions Detection Dataset (DMDD), the largest publicly available
corpus for this task. DMDD consists of the DMDD main corpus, comprising 31,219
scientific articles with over 449,000 dataset mentions weakly annotated in the
format of in-text spans, and an evaluation set, which comprises of 450
scientific articles manually annotated for evaluation purposes. We use DMDD to
establish baseline performance for dataset mention detection and linking. By
analyzing the performance of various models on DMDD, we are able to identify
open problems in dataset mention detection. We invite the community to use our
dataset as a challenge to develop novel dataset mention detection models.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Neural Foundations of Mental Simulation: Future Prediction of Latent  Representations on Dynamic Scenes</b></summary>
  <p><b>编号</b>：[43]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11772</p>
  <p><b>作者</b>：Aran Nayebi,  Rishi Rajalingham,  Mehrdad Jazayeri,  Guangyu Robert Yang</p>
  <p><b>备注</b>：17 pages, 6 figures</p>
  <p><b>关键词</b>：underlying dynamical trajectories, plausible future states, physical world, objects and events, consequences of actions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Humans and animals have a rich and flexible understanding of the physical
world, which enables them to infer the underlying dynamical trajectories of
objects and events, plausible future states, and use that to plan and
anticipate the consequences of actions. However, the neural mechanisms
underlying these computations are unclear. We combine a goal-driven modeling
approach with dense neurophysiological data and high-throughput human
behavioral readouts to directly impinge on this question. Specifically, we
construct and evaluate several classes of sensory-cognitive networks to predict
the future state of rich, ethologically-relevant environments, ranging from
self-supervised end-to-end models with pixel-wise or object-centric objectives,
to models that future predict in the latent space of purely static image-based
or dynamic video-based pretrained foundation models. We find strong
differentiation across these model classes in their ability to predict neural
and behavioral data both within and across diverse environments. In particular,
we find that neural responses are currently best predicted by models trained to
predict the future state of their environment in the latent space of pretrained
foundation models optimized for dynamic scenes in a self-supervised manner.
Notably, models that future predict in the latent space of video foundation
models that are optimized to support a diverse range of sensorimotor tasks,
reasonably match both human behavioral error patterns and neural dynamics
across all environmental scenarios that we were able to test. Overall, these
findings suggest that the neural mechanisms and behaviors of primate mental
simulation are thus far most consistent with being optimized to future predict
on dynamic, reusable visual representations that are useful for embodied AI
more generally.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Enhancing Vision-Language Pre-Training with Jointly Learned Questioner  and Dense Captioner</b></summary>
  <p><b>编号</b>：[44]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11769</p>
  <p><b>作者</b>：Zikang Liu,  Sihan Chen,  Longteng Guo,  Handong Li,  Xingjian He,  Jing Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：visual question answering, demonstrated significant success, including image captioning, VQA and dense, Large pre-trained multimodal</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large pre-trained multimodal models have demonstrated significant success in
a range of downstream tasks, including image captioning, image-text retrieval,
visual question answering (VQA), etc. However, many of these methods rely on
image-text pairs collected from the web as pre-training data and unfortunately
overlook the need for fine-grained feature alignment between vision and
language modalities, which requires detailed understanding of images and
language expressions. While integrating VQA and dense captioning (DC) into
pre-training can address this issue, acquiring image-question-answer as well as
image-location-caption triplets is challenging and time-consuming.
Additionally, publicly available datasets for VQA and dense captioning are
typically limited in scale due to manual data collection and labeling efforts.
In this paper, we propose a novel method called Joint QA and DC GEneration
(JADE), which utilizes a pre-trained multimodal model and easily-crawled
image-text pairs to automatically generate and filter large-scale VQA and dense
captioning datasets. We apply this method to the Conceptual Caption (CC3M)
dataset to generate a new dataset called CC3M-QA-DC. Experiments show that when
used for pre-training in a multi-task manner, CC3M-QA-DC can improve the
performance with various backbones on various downstream tasks. Furthermore,
our generated CC3M-QA-DC can be combined with larger image-text datasets (e.g.,
CC15M) and achieve competitive results compared with models using much more
data. Code and dataset will be released.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Controlling the Extraction of Memorized Data from Large Language Models  via Prompt-Tuning</b></summary>
  <p><b>编号</b>：[48]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11759</p>
  <p><b>作者</b>：Mustafa Safa Ozdayi,  Charith Peris,  Jack FitzGerald,  Christophe Dupuy,  Jimit Majmudar,  Haidar Khan,  Rahil Parikh,  Rahul Gupta</p>
  <p><b>备注</b>：5 pages, 3 Figures, ACL 2023</p>
  <p><b>关键词</b>：Large Language Models, memorize significant portions, Large Language, Language Models, memorize significant</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) are known to memorize significant portions of
their training data. Parts of this memorized content have been shown to be
extractable by simply querying the model, which poses a privacy risk. We
present a novel approach which uses prompt-tuning to control the extraction
rates of memorized content in LLMs. We present two prompt training strategies
to increase and decrease extraction rates, which correspond to an attack and a
defense, respectively. We demonstrate the effectiveness of our techniques by
using models from the GPT-Neo family on a public benchmark. For the 1.3B
parameter GPT-Neo model, our attack yields a 9.3 percentage point increase in
extraction rate compared to our baseline. Our defense can be tuned to achieve
different privacy-utility trade-offs by a user-specified hyperparameter. We
achieve an extraction rate reduction of up to 97.7% relative to our baseline,
with a perplexity increase of 16.9%.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Visualization for Recommendation Explainability: A Survey and New  Perspectives</b></summary>
  <p><b>编号</b>：[49]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11755</p>
  <p><b>作者</b>：Mohamed Amine Chatti,  Mouadh Guesmi,  Arham Muslim</p>
  <p><b>备注</b>：36 pages</p>
  <p><b>关键词</b>：recommender systems, Providing system-generated explanations, recommender, trustworthy recommender systems, Providing system-generated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Providing system-generated explanations for recommendations represents an
important step towards transparent and trustworthy recommender systems.
Explainable recommender systems provide a human-understandable rationale for
their outputs. Over the last two decades, explainable recommendation has
attracted much attention in the recommender systems research community. This
paper aims to provide a comprehensive review of research efforts on visual
explanation in recommender systems. More concretely, we systematically review
the literature on explanations in recommender systems based on four dimensions,
namely explanation goal, explanation scope, explanation style, and explanation
format. Recognizing the importance of visualization, we approach the
recommender system literature from the angle of explanatory visualizations,
that is using visualizations as a display style of explanation. As a result, we
derive a set of guidelines that might be constructive for designing explanatory
visualizations in recommender systems and identify perspectives for future work
in this field. The aim of this review is to help recommendation researchers and
practitioners better understand the potential of visually explainable
recommendation research and to support them in the systematic design of visual
explanations in current and future recommender systems.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：MedLens: Improve mortality prediction via medical signs selecting and  regression interpolation</b></summary>
  <p><b>编号</b>：[54]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11742</p>
  <p><b>作者</b>：Xuesong Ye,  Jun Wu,  Chengjie Mou,  Weinan Dai</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：care and treatment, predicting mortality, mortality in advance, timely care, medical signs</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Monitoring the health status of patients and predicting mortality in advance
is vital for providing patients with timely care and treatment. Massive medical
signs in electronic health records (EHR) are fitted into advanced machine
learning models to make predictions. However, the data-quality problem of
original clinical signs is less discussed in the literature. Based on an
in-depth measurement of the missing rate and correlation score across various
medical signs and a large amount of patient hospital admission records, we
discovered the comprehensive missing rate is extremely high, and a large number
of useless signs could hurt the performance of prediction models. Then we
concluded that only improving data-quality could improve the baseline accuracy
of different prediction algorithms. We designed MEDLENS, with an automatic
vital medical signs selection approach via statistics and a flexible
interpolation approach for high missing rate time series. After augmenting the
data-quality of original medical signs, MEDLENS applies ensemble classifiers to
boost the accuracy and reduce the computation overhead at the same time. It
achieves a very high accuracy performance of 0.96% AUC-ROC and 0.81% AUC-PR,
which exceeds the previous benchmark.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：CRITIC: Large Language Models Can Self-Correct with Tool-Interactive  Critiquing</b></summary>
  <p><b>编号</b>：[57]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11738</p>
  <p><b>作者</b>：Zhibin Gou,  Zhihong Shao,  Yeyun Gong,  Yelong Shen,  Yujiu Yang,  Nan Duan,  Weizhu Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, Recent developments, developments in large, large language, language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent developments in large language models (LLMs) have been impressive.
However, these models sometimes show inconsistencies and problematic behavior,
such as hallucinating facts, generating flawed code, or creating offensive and
toxic content. Unlike these models, humans typically utilize external tools to
cross-check and refine their initial content, like using a search engine for
fact-checking, or a code interpreter for debugging. Inspired by this
observation, we introduce a framework called CRITIC that allows LLMs, which are
essentially "black boxes" to validate and progressively amend their own outputs
in a manner similar to human interaction with tools. More specifically,
starting with an initial output, CRITIC interacts with appropriate tools to
evaluate certain aspects of the text, and then revises the output based on the
feedback obtained during this validation process. Comprehensive evaluations
involving free-form question answering, mathematical program synthesis, and
toxicity reduction demonstrate that CRITIC consistently enhances the
performance of LLMs. Meanwhile, our research highlights the crucial importance
of external feedback in promoting the ongoing self-improvement of LLMs.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Persian Typographical Error Type Detection using Many-to-Many Deep  Neural Networks on Algorithmically-Generated Misspellings</b></summary>
  <p><b>编号</b>：[60]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11731</p>
  <p><b>作者</b>：Mohammad Dehghani,  Heshaam Faili</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：text created daily, Digital technologies, technologies have led, created daily, text created</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Digital technologies have led to an influx of text created daily in a variety
of languages, styles, and formats. A great deal of the popularity of
spell-checking systems can be attributed to this phenomenon since they are
crucial to polishing the digitally conceived text. In this study, we tackle
Typographical Error Type Detection in Persian, which has been relatively
understudied. In this paper, we present a public dataset named FarsTypo,
containing 3.4 million chronologically ordered and part-of-speech tagged words
of diverse topics and linguistic styles. An algorithm for applying
Persian-specific errors is developed and applied to a scalable size of these
words, forming a parallel dataset of correct and incorrect words. Using
FarsTypo, we establish a firm baseline and compare different methodologies
using various architectures. In addition, we present a novel Many-to-Many Deep
Sequential Neural Network to perform token classification using both word and
character embeddings in combination with bidirectional LSTM layers to detect
typographical errors across 51 classes. We compare our approach with
highly-advanced industrial systems that, unlike this study, have been developed
utilizing a variety of resources. The results of our final method were
competitive in that we achieved an accuracy of 97.62%, a precision of 98.83%, a
recall of 98.61%, and outperformed the rest in terms of speed.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：What Comes Next? Evaluating Uncertainty in Neural Text Generators  Against Human Production Variability</b></summary>
  <p><b>编号</b>：[70]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11707</p>
  <p><b>作者</b>：Mario Giulianelli,  Joris Baan,  Wilker Aziz,  Raquel Fernández,  Barbara Plank</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Natural Language Generation, Natural Language, Language Generation, multiple communicative goals, put into words</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In Natural Language Generation (NLG) tasks, for any input, multiple
communicative goals are plausible, and any goal can be put into words, or
produced, in multiple ways. We characterise the extent to which human
production varies lexically, syntactically, and semantically across four NLG
tasks, connecting human production variability to aleatoric or data
uncertainty. We then inspect the space of output strings shaped by a generation
system's predicted probability distribution and decoding algorithm to probe its
uncertainty. For each test input, we measure the generator's calibration to
human production variability. Following this instance-level approach, we
analyse NLG models and decoding strategies, demonstrating that probing a
generator with multiple samples and, when possible, multiple references,
provides the level of detail necessary to gain understanding of a model's
representation of uncertainty.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：RGCVAE: Relational Graph Conditioned Variational Autoencoder for  Molecule Design</b></summary>
  <p><b>编号</b>：[73]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11699</p>
  <p><b>作者</b>：Davide Rigoni,  Nicolò Navarin,  Alessandro Sperduti</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Graph Variational Autoencoders, Deep Graph Variational, exhibit some pre-specified, pre-specified properties, Graph Variational</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Identifying molecules that exhibit some pre-specified properties is a
difficult problem to solve. In the last few years, deep generative models have
been used for molecule generation. Deep Graph Variational Autoencoders are
among the most powerful machine learning tools with which it is possible to
address this problem. However, existing methods struggle in capturing the true
data distribution and tend to be computationally expensive. In this work, we
propose RGCVAE, an efficient and effective Graph Variational Autoencoder based
on: (i) an encoding network exploiting a new powerful Relational Graph
Isomorphism Network; (ii) a novel probabilistic decoding component. Compared to
several state-of-the-art VAE methods on two widely adopted datasets, RGCVAE
shows state-of-the-art molecule generation performance while being
significantly faster to train.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Surgical-VQLA: Transformer with Gated Vision-Language Embedding for  Visual Question Localized-Answering in Robotic Surgery</b></summary>
  <p><b>编号</b>：[75]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11692</p>
  <p><b>作者</b>：Long Bai,  Mobarakol Islam,  Lalithkumar Seenivasan,  Hongliang Ren</p>
  <p><b>备注</b>：To appear in IEEE ICRA 2023. Code and data availability: this https URL</p>
  <p><b>关键词</b>：junior residents, availability of computer-aided, computer-aided simulators, residents still heavily, heavily rely</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite the availability of computer-aided simulators and recorded videos of
surgical procedures, junior residents still heavily rely on experts to answer
their queries. However, expert surgeons are often overloaded with clinical and
academic workloads and limit their time in answering. For this purpose, we
develop a surgical question-answering system to facilitate robot-assisted
surgical scene and activity understanding from recorded videos. Most of the
existing VQA methods require an object detector and regions based feature
extractor to extract visual features and fuse them with the embedded text of
the question for answer generation. However, (1) surgical object detection
model is scarce due to smaller datasets and lack of bounding box annotation;
(2) current fusion strategy of heterogeneous modalities like text and image is
naive; (3) the localized answering is missing, which is crucial in complex
surgical scenarios. In this paper, we propose Visual Question
Localized-Answering in Robotic Surgery (Surgical-VQLA) to localize the specific
surgical area during the answer prediction. To deal with the fusion of the
heterogeneous modalities, we design gated vision-language embedding (GVLE) to
build input patches for the Language Vision Transformer (LViT) to predict the
answer. To get localization, we add the detection head in parallel with the
prediction head of the LViT. We also integrate GIoU loss to boost localization
performance by preserving the accuracy of the question-answering model. We
annotate two datasets of VQLA by utilizing publicly available surgical videos
from MICCAI challenges EndoVis-17 and 18. Our validation results suggest that
Surgical-VQLA can better understand the surgical scene and localize the
specific area related to the question-answering. GVLE presents an efficient
language-vision embedding technique by showing superior performance over the
existing benchmarks.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Algorithmic failure as a humanities methodology: machine learning's  mispredictions identify rich cases for qualitative analysis</b></summary>
  <p><b>编号</b>：[87]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11663</p>
  <p><b>作者</b>：Jill Walker Rettberg</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：machine learning, methodology proposed, machine learning algorithm, machine vision technologies, machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This commentary tests a methodology proposed by Munk et al. (2022) for using
failed predictions in machine learning as a method to identify ambiguous and
rich cases for qualitative analysis. Using a dataset describing actions
performed by fictional characters interacting with machine vision technologies
in 500 artworks, movies, novels and videogames, I trained a simple machine
learning algorithm (using the kNN algorithm in R) to predict whether or not an
action was active or passive using only information about the fictional
characters. Predictable actions were generally unemotional and unambiguous
activities where machine vision technologies were treated as simple tools.
Unpredictable actions, that is, actions that the algorithm could not correctly
predict, were more ambivalent and emotionally loaded, with more complex power
relationships between characters and technologies. The results thus support
Munk et al.'s theory that failed predictions can be productively used to
identify rich cases for qualitative analysis. This test goes beyond simply
replicating Munk et al.'s results by demonstrating that the method can be
applied to a broader humanities domain, and that it does not require complex
neural networks but can also work with a simpler machine learning algorithm.
Further research is needed to develop an understanding of what kinds of data
the method is useful for and which kinds of machine learning are most
generative. To support this, the R code required to produce the results is
included so the test can be replicated. The code can also be reused or adapted
to test the method on other datasets.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Evaluating task understanding through multilingual consistency: A  ChatGPT case study</b></summary>
  <p><b>编号</b>：[88]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11662</p>
  <p><b>作者</b>：Xenia Ohmer,  Elia Bruni,  Dieuwke Hupkes</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：creating future-proof evaluation, future-proof evaluation sets, creating future-proof, staggering pace, capabilities of large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>At the staggering pace with which the capabilities of large language models
(LLMs) are increasing, creating future-proof evaluation sets to assess their
understanding becomes more and more challenging. In this paper, we propose a
novel paradigm for evaluating LLMs which leverages the idea that correct world
understanding should be consistent across different (Fregean) senses of the
same meaning. Accordingly, we measure understanding not in terms of correctness
but by evaluating consistency across multiple senses that are generated by the
model itself. We showcase our approach by instantiating a test where the
different senses are different languages, hence using multilingual
self-consistency as a litmus test for the model's understanding and
simultaneously addressing the important topic of multilingualism. Taking one of
the latest versions of ChatGPT as our object of study, we evaluate multilingual
consistency for two different tasks across three different languages. We show
that its multilingual consistency is still lacking, and that its task and world
understanding are thus not language-independent. As our approach does not
require any static evaluation corpora in languages other than English, it can
easily and cheaply be extended to different languages and tasks and could
become an integral part of future benchmarking efforts.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Applying Ising Machines to Multi-objective QUBOs</b></summary>
  <p><b>编号</b>：[95]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11648</p>
  <p><b>作者</b>：Mayowa Ayodele,  Richard Allmendinger,  Manuel López-Ibáñez,  Arnaud Liefooghe,  Matthieu Parizy</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：involve finding solutions, optimisation problems involve, problems involve finding, involve finding, varying trade-offs</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multi-objective optimisation problems involve finding solutions with varying
trade-offs between multiple and often conflicting objectives. Ising machines
are physical devices that aim to find the absolute or approximate ground states
of an Ising model. To apply Ising machines to multi-objective problems, a
weighted sum objective function is used to convert multi-objective into
single-objective problems. However, deriving scalarisation weights that
archives evenly distributed solutions across the Pareto front is not trivial.
Previous work has shown that adaptive weights based on dichotomic search, and
one based on averages of previously explored weights can explore the Pareto
front quicker than uniformly generated weights. However, these adaptive methods
have only been applied to bi-objective problems in the past. In this work, we
extend the adaptive method based on averages in two ways: (i)~we extend the
adaptive method of deriving scalarisation weights for problems with two or more
objectives, and (ii)~we use an alternative measure of distance to improve
performance.
We compare the proposed method with existing ones and show that it leads to
the best performance on multi-objective Unconstrained Binary Quadratic
Programming (mUBQP) instances with 3 and 4 objectives and that it is
competitive with the best one for instances with 2 objectives.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Tune-Mode ConvBN Blocks For Efficient Transfer Learning</b></summary>
  <p><b>编号</b>：[108]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11624</p>
  <p><b>作者</b>：Kaichao You,  Anchang Bao,  Guo Qin,  Meng Cao,  Ping Huang,  Jiulong Shan,  Mingsheng Long</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Deploy mode, Eval mode, mode, Deploy, Tune mode</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Convolution-BatchNorm (ConvBN) blocks are integral components in various
computer vision tasks and other domains. A ConvBN block can operate in three
modes: Train, Eval, and Deploy. While the Train mode is indispensable for
training models from scratch, the Eval mode is suitable for transfer learning
and model validation, and the Deploy mode is designed for the deployment of
models. This paper focuses on the trade-off between stability and efficiency in
ConvBN blocks: Deploy mode is efficient but suffers from training instability;
Eval mode is widely used in transfer learning but lacks efficiency. To solve
the dilemma, we theoretically reveal the reason behind the diminished training
stability observed in the Deploy mode. Subsequently, we propose a novel Tune
mode to bridge the gap between Eval mode and Deploy mode. The proposed Tune
mode is as stable as Eval mode for transfer learning, and its computational
efficiency closely matches that of the Deploy mode. Through extensive
experiments in both object detection and classification tasks, carried out
across various datasets and model architectures, we demonstrate that the
proposed Tune mode does not hurt the original performance while significantly
reducing GPU memory footprint and training time, thereby contributing an
efficient solution to transfer learning with convolutional networks.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Towards Code Generation from BDD Test Case Specifications: A Vision</b></summary>
  <p><b>编号</b>：[109]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11619</p>
  <p><b>作者</b>：Leon Chemnitz,  David Reichenbach,  Hani Aldebes,  Mariam Naveed,  Krishna Narasimhan,  Mira Mezini</p>
  <p><b>备注</b>：Accepted for publication at the International Conference on AI Engineering (CAIN) 2023</p>
  <p><b>关键词</b>：recently attracted large, attracted large attention, software development process, recently attracted, attracted large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automatic code generation has recently attracted large attention and is
becoming more significant to the software development process. Solutions based
on Machine Learning and Artificial Intelligence are being used to increase
human and software efficiency in potent and innovative ways. In this paper, we
aim to leverage these developments and introduce a novel approach to generating
frontend component code for the popular Angular framework. We propose to do
this using behavior-driven development test specifications as input to a
transformer-based machine learning model. Our approach aims to drastically
reduce the development time needed for web applications while potentially
increasing software quality and introducing new research ideas toward automatic
code generation.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD  Detection, Calibration, and Accuracy</b></summary>
  <p><b>编号</b>：[111]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11616</p>
  <p><b>作者</b>：Stanislav Dereka,  Ivan Karpukhin,  Sergey Kolesnikov</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：effectiveness remains limited, remains limited due, Deep ensembles achieved, effectiveness remains, remains limited</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep ensembles achieved state-of-the-art results in classification and
out-of-distribution (OOD) detection; however, their effectiveness remains
limited due to the homogeneity of learned patterns within the ensemble. To
overcome this challenge, our study introduces a novel approach that promotes
diversity among ensemble members by leveraging saliency maps. By incorporating
saliency map diversification, our method outperforms conventional ensemble
techniques in multiple classification and OOD detection tasks, while also
improving calibration. Experiments on well-established OpenOOD benchmarks
highlight the potential of our method in practical applications.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：MIDI-Draw: Sketching to Control Melody Generation</b></summary>
  <p><b>编号</b>：[113]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11605</p>
  <p><b>作者</b>：Tashi Namgyal,  Peter Flach,  Raul Santos-Rodriguez</p>
  <p><b>备注</b>：Late-Breaking / Demo Session Extended Abstract, ISMIR 2022 Conference</p>
  <p><b>关键词</b>：note-level input representation, melodies that abstracts, note-level input, input representation, representation via melodic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We describe a proof-of-principle implementation of a system for drawing
melodies that abstracts away from a note-level input representation via melodic
contours. The aim is to allow users to express their musical intentions without
requiring prior knowledge of how notes fit together melodiously. Current
approaches to controllable melody generation often require users to choose
parameters that are static across a whole sequence, via buttons or sliders. In
contrast, our method allows users to quickly specify how parameters should
change over time by drawing a contour.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Introspective Tips: Large Language Model for In-Context Decision Making</b></summary>
  <p><b>编号</b>：[117]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11598</p>
  <p><b>作者</b>：Liting Chen,  Lu Wang,  Hang Dong,  Yali Du,  Jie Yan,  Fangkai Yang,  Shuang Li,  Pu Zhao,  Si Qin,  Saravan Rajmohan,  Qingwei Lin,  Dongmei Zhang</p>
  <p><b>备注</b>：22 pages, 4 figures</p>
  <p><b>关键词</b>：large language models, natural language processing, influenced natural language, demonstrating exceptional results, substantially influenced natural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The emergence of large language models (LLMs) has substantially influenced
natural language processing, demonstrating exceptional results across various
tasks. In this study, we employ ``Introspective Tips" to facilitate LLMs in
self-optimizing their decision-making. By introspectively examining
trajectories, LLM refines its policy by generating succinct and valuable tips.
Our method enhances the agent's performance in both few-shot and zero-shot
learning situations by considering three essential scenarios: learning from the
agent's past experiences, integrating expert demonstrations, and generalizing
across diverse games. Importantly, we accomplish these improvements without
fine-tuning the LLM parameters; rather, we adjust the prompt to generalize
insights from the three aforementioned situations. Our framework not only
supports but also emphasizes the advantage of employing LLM in in-contxt
decision-making. Experiments involving over 100 games in TextWorld illustrate
the superior performance of our approach.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Flexible and Inherently Comprehensible Knowledge Representation for  Data-Efficient Learning and Trustworthy Human-Machine Teaming in  Manufacturing Environments</b></summary>
  <p><b>编号</b>：[118]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11597</p>
  <p><b>作者</b>：Vedran Galetić,  Alistair Nottle</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：artificially intelligent agents, industrial manufacturing environments, artificially intelligent, acceptance of human-machine, human-machine teaming</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Trustworthiness of artificially intelligent agents is vital for the
acceptance of human-machine teaming in industrial manufacturing environments.
Predictable behaviours and explainable (and understandable) rationale allow
humans collaborating with (and building) these agents to understand their
motivations and therefore validate decisions that are made. To that aim, we
make use of Gärdenfors's cognitively inspired Conceptual Space framework to
represent the agent's knowledge using concepts as convex regions in a space
spanned by inherently comprehensible quality dimensions. A simple typicality
quantification model is built on top of it to determine fuzzy category
membership and classify instances interpretably. We apply it on a use case from
the manufacturing domain, using objects' physical properties obtained from
cobots' onboard sensors and utilisation properties from crowdsourced
commonsense knowledge available at public knowledge bases. Such flexible
knowledge representation based on property decomposition allows for
data-efficient representation learning of typically highly specialist or
specific manufacturing artefacts. In such a setting, traditional data-driven
(e.g., computer vision-based) classification approaches would struggle due to
training data scarcity. This allows for comprehensibility of an AI agent's
acquired knowledge by the human collaborator thus contributing to
trustworthiness. We situate our approach within an existing explainability
framework specifying explanation desiderata. We provide arguments for our
system's applicability and appropriateness for different roles of human agents
collaborating with the AI system throughout its design, validation, and
operation.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Diving into the Inter-Consistency of Large Language Models: An  Insightful Analysis through Debate</b></summary>
  <p><b>编号</b>：[120]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11595</p>
  <p><b>作者</b>：Kai Xiong,  Xiao Ding,  Yixin Cao,  Ting Liu,  Bing Qin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large language models, natural language processing, demonstrated impressive zero-shot, few-shot commonsense reasoning, Large language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have demonstrated impressive zero-shot or
few-shot commonsense reasoning performance on various natural language
processing (NLP) tasks. However, despite their strong commonsense reasoning
abilities, LLMs still exhibit various kinds of inconsistency problems. While
previous researches mainly focused on the self-consistency within a single LLM,
we propose to explore the inter-consistency issue between two or more LLMs,
which is critical for diverse and precise decision-making processes. Since the
LLMs possess human-like intelligence after instruction tuning and reinforcement
learning with human feedback (RLHF), we design a formal debate framework to
delve into the inter-consistency problem among LLMs with three-stage debate:
fair debate, mismatched debate, and roundtable debate. Through extensive
experiments on 7 commonsense reasoning datasets, LLMs not only become more
inter-consistent by compromising and refuting but also achieve higher
performance and stronger interpretability. Furthermore, we find a much stronger
LLM would be dominant in mismatched debates, while it will be easily misled by
relatively weaker LLMs in a more complex debate scenario such as roundtable
debate.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Trustworthy, responsible, ethical AI in manufacturing and supply chains:  synthesis and emerging research questions</b></summary>
  <p><b>编号</b>：[129]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11581</p>
  <p><b>作者</b>：Alexandra Brintrup,  George Baryannis,  Ashutosh Tiwari,  Svetan Ratchev,  Giovanna Martinez-Arellano,  Jatinder Singh</p>
  <p><b>备注</b>：Pre-print under peer-review</p>
  <p><b>关键词</b>：widely noted, manufacturing, manufacturing sector, consolidate potential risks, understanding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While the increased use of AI in the manufacturing sector has been widely
noted, there is little understanding on the risks that it may raise in a
manufacturing organisation. Although various high level frameworks and
definitions have been proposed to consolidate potential risks, practitioners
struggle with understanding and implementing them.
This lack of understanding exposes manufacturing to a multitude of risks,
including the organisation, its workers, as well as suppliers and clients. In
this paper, we explore and interpret the applicability of responsible, ethical,
and trustworthy AI within the context of manufacturing. We then use a broadened
adaptation of a machine learning lifecycle to discuss, through the use of
illustrative examples, how each step may result in a given AI trustworthiness
concern. We additionally propose a number of research questions to the
manufacturing research community, in order to help guide future research so
that the economic and societal benefits envisaged by AI in manufacturing are
delivered safely and responsibly.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Speech-Text Dialog Pre-training for Spoken Dialog Understanding with  Explicit Cross-Modal Alignment</b></summary>
  <p><b>编号</b>：[131]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11579</p>
  <p><b>作者</b>：Tianshu Yu,  Haoyu Gao,  Ting-En Lin,  Min Yang,  Yuchuan Wu,  Wentao Ma,  Chao Wang,  Fei Huang,  Yongbin Li</p>
  <p><b>备注</b>：Accepted at ACL 2023 main conference</p>
  <p><b>关键词</b>：shown remarkable success, natural language processing, language processing tasks, speech-text, pre-training</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, speech-text pre-training methods have shown remarkable success in
many speech and natural language processing tasks. However, most previous
pre-trained models are usually tailored for one or two specific tasks, but fail
to conquer a wide range of speech-text tasks. In addition, existing speech-text
pre-training methods fail to explore the contextual information within a
dialogue to enrich utterance representations. In this paper, we propose
Speech-text dialog Pre-training for spoken dialog understanding with ExpliCiT
cRoss-Modal Alignment (SPECTRA), which is the first-ever speech-text dialog
pre-training model. Concretely, to consider the temporality of speech modality,
we design a novel temporal position prediction task to capture the speech-text
alignment. This pre-training task aims to predict the start and end time of
each textual word in the corresponding speech waveform. In addition, to learn
the characteristics of spoken dialogs, we generalize a response selection task
from textual dialog pre-training to speech-text dialog pre-training scenarios.
Experimental results on four different downstream speech-text tasks demonstrate
the superiority of SPECTRA in learning speech-text alignment and multi-turn
dialog context.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：StereoVAE: A lightweight stereo matching system through embedded GPUs</b></summary>
  <p><b>编号</b>：[135]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11566</p>
  <p><b>作者</b>：Qiong Chang,  Xiang Li,  Xin Xu,  Xin Liu,  Yun Li,  Miyazaki Jun</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：present a lightweight, stereo matching, matching, lightweight system, matching accuracy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a lightweight system for stereo matching through embedded GPUs. It
breaks the trade-off between accuracy and processing speed in stereo matching,
enabling our embedded system to further improve the matching accuracy while
ensuring real-time processing. The main idea of our method is to construct a
tiny neural network based on variational auto-encoder (VAE) to upsample and
refinement a small size of coarse disparity map, which is first generated by a
traditional matching method. The proposed hybrid structure cannot only bring
the advantage of traditional methods in terms of computational complexity, but
also ensure the matching accuracy under the impact of neural network. Extensive
experiments on the KITTI 2015 benchmark demonstrate that our tiny system
exhibits high robustness in improving the accuracy of the coarse disparity maps
generated by different algorithms, while also running in real-time on embedded
GPUs.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Decouple knowledge from paramters for plug-and-play language modeling</b></summary>
  <p><b>编号</b>：[136]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11564</p>
  <p><b>作者</b>：Xin Cheng,  Yankai Lin,  Xiuying Chen,  Dongyan Zhao,  Rui Yan</p>
  <p><b>备注</b>：ACL2023 Findings</p>
  <p><b>关键词</b>：made impressive results, NLP tasks, Pre-trained language models, knowledge, Pre-trained language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pre-trained language models(PLM) have made impressive results in various NLP
tasks. It has been revealed that one of the key factors to their success is the
parameters of these models implicitly learn all kinds of knowledge during
pre-training. However, encoding knowledge implicitly in the model parameters
has two fundamental drawbacks. First, the knowledge is neither editable nor
scalable once the model is trained, which is especially problematic in that
knowledge is consistently evolving. Second, it lacks interpretability and
prevents humans from understanding which knowledge PLM requires for a certain
problem. In this paper, we introduce PlugLM, a pre-training model with
differentiable plug-in memory(DPM). The key intuition is to decouple the
knowledge storage from model parameters with an editable and scalable key-value
memory and leverage knowledge in an explainable manner by knowledge retrieval
in the DPM. To justify this design choice, we conduct evaluations in three
settings including: (1) domain adaptation. PlugLM obtains 3.95 F1 improvements
across four domains on average without any in-domain pre-training. (2)
knowledge update. PlugLM could absorb new knowledge in a training-free way
after pre-training is done. (3) in-task knowledge learning. PlugLM could be
further improved by incorporating training samples into DPM with knowledge
prompting.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：Brain Captioning: Decoding human brain activity into images and text</b></summary>
  <p><b>编号</b>：[137]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11560</p>
  <p><b>作者</b>：Matteo Ferrante,  Furkan Ozcelik,  Tommaso Boccato,  Rufin VanRullen,  Nicola Toschi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：intricate neural mechanisms, relying on intricate, interpret these stimuli, visual information, processes an immense</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Every day, the human brain processes an immense volume of visual information,
relying on intricate neural mechanisms to perceive and interpret these stimuli.
Recent breakthroughs in functional magnetic resonance imaging (fMRI) have
enabled scientists to extract visual information from human brain activity
patterns. In this study, we present an innovative method for decoding brain
activity into meaningful images and captions, with a specific focus on brain
captioning due to its enhanced flexibility as compared to brain decoding into
images. Our approach takes advantage of cutting-edge image captioning models
and incorporates a unique image reconstruction pipeline that utilizes latent
diffusion models and depth estimation. We utilized the Natural Scenes Dataset,
a comprehensive fMRI dataset from eight subjects who viewed images from the
COCO dataset. We employed the Generative Image-to-text Transformer (GIT) as our
backbone for captioning and propose a new image reconstruction pipeline based
on latent diffusion models. The method involves training regularized linear
regression models between brain activity and extracted features. Additionally,
we incorporated depth maps from the ControlNet model to further guide the
reconstruction process. We evaluate our methods using quantitative metrics for
both generated captions and images. Our brain captioning approach outperforms
existing methods, while our image reconstruction pipeline generates plausible
images with improved spatial relationships. In conclusion, we demonstrate
significant progress in brain decoding, showcasing the enormous potential of
integrating vision and language to better understand human cognition. Our
approach provides a flexible platform for future research, with potential
applications in various fields, including neural art, style transfer, and
portable devices.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Empower Large Language Model to Perform Better on Industrial  Domain-Specific Question Answering</b></summary>
  <p><b>编号</b>：[147]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11541</p>
  <p><b>作者</b>：Zezhong Wang,  Fangkai Yang,  Pu Zhao,  Lu Wang,  Jue Zhang,  Mohit Garg,  Qingwei Lin,  Dongmei Zhang</p>
  <p><b>备注</b>：13 pages, 1 figure</p>
  <p><b>关键词</b>：Large Language Model, achieved remarkable results, Large Language, industrial domain-specific scenarios, real industrial domain-specific</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Model (LLM) has gained popularity and achieved remarkable
results in open-domain tasks, but its performance in real industrial
domain-specific scenarios is average since there is no specific knowledge in
it. This issue has attracted widespread attention, but there are few relevant
benchmarks available. In this paper, we provide a benchmark Question Answering
(QA) dataset named MSQA, which is about Microsoft products and IT technical
problems encountered by customers. This dataset contains industry
cloud-specific QA knowledge, which is not available for general LLM, so it is
well suited for evaluating methods aimed at improving domain-specific
capabilities of LLM. In addition, we propose a new model interaction paradigm
that can empower LLM to achieve better performance on domain-specific tasks
where it is not proficient. Extensive experiments demonstrate that the approach
following our model fusion framework outperforms the commonly used LLM with
retrieval methods.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Trustworthy Federated Learning: A Survey</b></summary>
  <p><b>编号</b>：[149]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11537</p>
  <p><b>作者</b>：Asadullah Tariq,  Mohamed Adel Serhani,  Farag Sallabi,  Tariq Qayyum,  Ezedin S. Barka,  Khaled A. Shuaib</p>
  <p><b>备注</b>：45 Pages, 8 Figures, 9 Tables</p>
  <p><b>关键词</b>：Artificial Intelligence, maintaining data privacy, enabling collaborative model, field of Artificial, collaborative model training</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Federated Learning (FL) has emerged as a significant advancement in the field
of Artificial Intelligence (AI), enabling collaborative model training across
distributed devices while maintaining data privacy. As the importance of FL
increases, addressing trustworthiness issues in its various aspects becomes
crucial. In this survey, we provide an extensive overview of the current state
of Trustworthy FL, exploring existing solutions and well-defined pillars
relevant to Trustworthy . Despite the growth in literature on trustworthy
centralized Machine Learning (ML)/Deep Learning (DL), further efforts are
necessary to identify trustworthiness pillars and evaluation metrics specific
to FL models, as well as to develop solutions for computing trustworthiness
levels. We propose a taxonomy that encompasses three main pillars:
Interpretability, Fairness, and Security & Privacy. Each pillar represents a
dimension of trust, further broken down into different notions. Our survey
covers trustworthiness challenges at every level in FL settings. We present a
comprehensive architecture of Trustworthy FL, addressing the fundamental
principles underlying the concept, and offer an in-depth analysis of trust
assessment mechanisms. In conclusion, we identify key research challenges
related to every aspect of Trustworthy FL and suggest future research
directions. This comprehensive survey serves as a valuable resource for
researchers and practitioners working on the development and implementation of
Trustworthy FL systems, contributing to a more secure and reliable AI
landscape.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：InstructIE: A Chinese Instruction-based Information Extraction Dataset</b></summary>
  <p><b>编号</b>：[155]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11527</p>
  <p><b>作者</b>：Honghao Gui,  Jintian Zhang,  Hongbin Ye,  Ningyu Zhang</p>
  <p><b>备注</b>：Work in progress</p>
  <p><b>关键词</b>：Information Extraction, task dubbed Instruction-based, Extraction, dubbed Instruction-based, extract information</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce a new Information Extraction (IE) task dubbed Instruction-based
IE, which aims to ask the system to follow specific instructions or guidelines
to extract information. To facilitate research in this area, we construct a
dataset called InstructIE, consisting of 270,000 weakly supervised data from
Chinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We
further evaluate the performance of various baseline models on the InstructIE
dataset. The results reveal that although current models exhibit promising
performance, there is still room for improvement. Furthermore, we conduct a
comprehensive case study analysis, underlining the challenges inherent in the
Instruction-based IE task. Code and dataset are available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：DiffuSIA: A Spiral Interaction Architecture for Encoder-Decoder Text  Diffusion</b></summary>
  <p><b>编号</b>：[159]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11517</p>
  <p><b>作者</b>：Chao-Hong Tan,  Jia-Chen Gu,  Zhen-Hua Ling</p>
  <p><b>备注</b>：Work in Progress</p>
  <p><b>关键词</b>：attracted increasing attention, recently attracted increasing, deep generative models, generative models, increasing attention</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diffusion models have emerged as the new state-of-the-art family of deep
generative models, and their promising potentials for text generation have
recently attracted increasing attention. Existing studies mostly adopt a single
encoder architecture with partially noising processes for conditional text
generation, but its degree of flexibility for conditional modeling is limited.
In fact, the encoder-decoder architecture is naturally more flexible for its
detachable encoder and decoder modules, which is extensible to multilingual and
multimodal generation tasks for conditions and target texts. However, the
encoding process of conditional texts lacks the understanding of target texts.
To this end, a spiral interaction architecture for encoder-decoder text
diffusion (DiffuSIA) is proposed. Concretely, the conditional information from
encoder is designed to be captured by the diffusion decoder, while the target
information from decoder is designed to be captured by the conditional encoder.
These two types of information flow run through multilayer interaction spirally
for deep fusion and understanding. DiffuSIA is evaluated on four text
generation tasks, including paraphrase, text simplification, question
generation, and open-domain dialogue generation. Experimental results show that
DiffuSIA achieves competitive performance among previous methods on all four
tasks, demonstrating the effectiveness and generalization ability of the
proposed method.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Terraforming -- Environment Manipulation during Disruptions for  Multi-Agent Pickup and Delivery</b></summary>
  <p><b>编号</b>：[164]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11510</p>
  <p><b>作者</b>：David Vainshtein,  Yaakov Sherma,  Kiril Solovey,  Oren Salzman</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：mobile robots fulfill, navigating narrow aisles, narrow aisles formed, transferring inventory pods, tightly packed pods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In automated warehouses, teams of mobile robots fulfill the packaging process
by transferring inventory pods to designated workstations while navigating
narrow aisles formed by tightly packed pods. This problem is typically modeled
as a Multi-Agent Pickup and Delivery (MAPD) problem, which is then solved by
repeatedly planning collision-free paths for agents on a fixed graph, as in the
Rolling-Horizon Collision Resolution (RHCR) algorithm. However, existing
approaches make the limiting assumption that agents are only allowed to move
pods that correspond to their current task, while considering the other pods as
stationary obstacles (even though all pods are movable). This behavior can
result in unnecessarily long paths which could otherwise be avoided by opening
additional corridors via pod manipulation. To this end, we explore the
implications of allowing agents the flexibility of dynamically relocating pods.
We call this new problem Terraforming MAPD (tMAPD) and develop an RHCR-based
approach to tackle it. As the extra flexibility of terraforming comes at a
significant computational cost, we utilize this capability judiciously by
identifying situations where it could make a significant impact on the solution
quality. In particular, we invoke terraforming in response to disruptions that
often occur in automated warehouses, e.g., when an item is dropped from a pod
or when agents malfunction. Empirically, using our approach for tMAPD, where
disruptions are modeled via a stochastic process, we improve throughput by over
10%, reduce the maximum service time (the difference between the drop-off time
and the pickup time of a pod) by more than 50%, without drastically increasing
the runtime, compared to the MAPD setting.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Plug-and-Play Medical Dialogue System</b></summary>
  <p><b>编号</b>：[166]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11508</p>
  <p><b>作者</b>：Chengfeng Dou,  Zhi Jin,  Wenping Jiao,  Haiyan Zhao,  Zhenwei Tao,  Yongqiang Zhao</p>
  <p><b>备注</b>：9 pages, 3 figures, Possible submission to Emnlp or AAAI</p>
  <p><b>关键词</b>：provide accurate answers, necessitating specific domain, specific domain knowledge, necessitating specific, Large Language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Medical dialogue systems aim to provide accurate answers to patients,
necessitating specific domain knowledge. Recent advancements in Large Language
Models (LLMs) have demonstrated their exceptional capabilities in the medical
Q&A domain, indicating a rich understanding of common sense. However, LLMs are
insufficient for direct diagnosis due to the absence of diagnostic strategies.
The conventional approach to address this challenge involves expensive
fine-tuning of LLMs. Alternatively, a more appealing solution is the
development of a plugin that empowers LLMs to perform medical conversation
tasks. Drawing inspiration from in-context learning, we propose PlugMed, a
Plug-and-Play Medical Dialogue System that facilitates appropriate dialogue
actions by LLMs through two modules: the prompt generation (PG) module and the
response ranking (RR) module. The PG module is designed to capture dialogue
information from both global and local perspectives. It selects suitable
prompts by assessing their similarity to the entire dialogue history and recent
utterances grouped by patient symptoms, respectively. Additionally, the RR
module incorporates fine-tuned SLMs as response filters and selects appropriate
responses generated by LLMs. Moreover, we devise a novel evaluation method
based on intent and medical entities matching to assess the efficacy of
dialogue strategies in medical conversations more effectively. Experimental
evaluations conducted on three unlabeled medical dialogue datasets, including
both automatic and manual assessments, demonstrate that our model surpasses the
strong fine-tuning baselines.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Recouple Event Field via Probabilistic Bias for Event Extraction</b></summary>
  <p><b>编号</b>：[171]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11498</p>
  <p><b>作者</b>：Xingyu Bai,  Taiqiang Wu,  Han Guo,  Zhe Zhao,  Xuefeng Yang,  Jiayi Li,  Weijie Liu,  Qi Ju,  Weigang Guo,  Yujiu Yang</p>
  <p><b>备注</b>：Published in: ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</p>
  <p><b>关键词</b>：pre-trained language models, aiming to identify, classify event triggers, identify and classify, benefited from pre-trained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Event Extraction (EE), aiming to identify and classify event triggers and
arguments from event mentions, has benefited from pre-trained language models
(PLMs). However, existing PLM-based methods ignore the information of
trigger/argument fields, which is crucial for understanding event schemas. To
this end, we propose a Probabilistic reCoupling model enhanced Event extraction
framework (ProCE). Specifically, we first model the syntactic-related event
fields as probabilistic biases, to clarify the event fields from ambiguous
entanglement. Furthermore, considering multiple occurrences of the same
triggers/arguments in EE, we explore probabilistic interaction strategies among
multiple fields of the same triggers/arguments, to recouple the corresponding
clarified distributions and capture more latent information fields. Experiments
on EE datasets demonstrate the effectiveness and generalization of our proposed
approach.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：TreePrompt: Learning to Compose Tree Prompts for Explainable Visual  Grounding</b></summary>
  <p><b>编号</b>：[172]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11497</p>
  <p><b>作者</b>：Chenchi Zhang,  Jun Xiao,  Lei Chen,  Jian Shao,  Long Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：achieved great success, large pretrained vision-language, pretrained vision-language models, downstream tasks, visual grounding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Prompt tuning has achieved great success in transferring the knowledge from
large pretrained vision-language models into downstream tasks, and has
dominated the performance on visual grounding (VG). However, almost all
existing prompt tuning paradigms suffer from poor interpretability. In this
paper, we argue that their poor interpretability is attributed to the holistic
prompt generation and inference process. By "holistic", we mean that they
usually directly learn a set of vectors as the prompt (i.e., prompt
generation), and use the learned global prompt to augment the textual input for
the VG model (i.e., prompt inference). To this end, we propose a new prompt
construction paradigm with explicit explainable ability, named TreePrompt.
Specifically, we first deconstruct a complex sentence into a tree, that is
consistent with human reasoning. Then, following the syntax tree, we compose a
structured prompt in a bottom-up manner. Thanks to this step-by-step prompt
construction process, each intermediate prompt (i.e., tree node) permits us to
understand the reasoning process. Extensive ablations on various backbones and
benchmarks consistently demonstrate the effectiveness and interpretability of
our TreePrompt.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：LLM Itself Can Read and Generate CXR Images</b></summary>
  <p><b>编号</b>：[174]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11490</p>
  <p><b>作者</b>：Suhyeon Lee,  Won Jun Kim,  Jong Chul Ye</p>
  <p><b>备注</b>：12 pages, 4 figures</p>
  <p><b>关键词</b>：recent remarkable development, large language models, recent remarkable, remarkable development, development of large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Building on the recent remarkable development of large language models
(LLMs), active attempts are being made to extend the utility of LLMs to
multimodal tasks. There have been previous efforts to link language and visual
information, and attempts to add visual capabilities to LLMs are ongoing as
well. However, existing attempts use LLMs only as image decoders and no attempt
has been made to generate images in the same line as the natural language. By
adopting a VQ-GAN framework in which latent representations of images are
treated as a kind of text tokens, we present a novel method to fine-tune a
pre-trained LLM to read and generate images like text without any structural
changes, extra training objectives, or the need for training an ad-hoc network
while still preserving the of the instruction-following capability of the LLM.
We apply this framework to chest X-ray (CXR) image and report generation tasks
as it is a domain in which translation of complex information between visual
and language domains is important. The code will soon be made publicly
available.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Incomplete Multi-view Clustering via Diffusion Completion</b></summary>
  <p><b>编号</b>：[175]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11489</p>
  <p><b>作者</b>：Sifan Fang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Incomplete multi-view clustering, multi-view clustering, Incomplete multi-view, multi-view clustering framework, provide effective data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Incomplete multi-view clustering is a challenging and non-trivial task to
provide effective data analysis for large amounts of unlabeled data in the real
world. All incomplete multi-view clustering methods need to address the problem
of how to reduce the impact of missing views. To address this issue, we propose
diffusion completion to recover the missing views integrated into an incomplete
multi-view clustering framework. Based on the observable views information, the
diffusion model is used to recover the missing views, and then the consistency
information of the multi-view data is learned by contrastive learning to
improve the performance of multi-view clustering. To the best of our knowledge,
this may be the first work to incorporate diffusion models into an incomplete
multi-view clustering framework. Experimental results show that the proposed
method performs well in recovering the missing views while achieving superior
clustering performance compared to state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Sensecape: Enabling Multilevel Exploration and Sensemaking with Large  Language Models</b></summary>
  <p><b>编号</b>：[179]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11483</p>
  <p><b>作者</b>：Sangho Suh,  Bryan Min,  Srishti Palani,  Haijun Xia</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, complex information tasks, People are increasingly, language models, increasingly turning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>People are increasingly turning to large language models (LLMs) for complex
information tasks like academic research or planning a move to another city.
However, while they often require working in a nonlinear manner - e.g., to
arrange information spatially to organize and make sense of it, current
interfaces for interacting with LLMs are generally linear to support
conversational interaction. To address this limitation and explore how we can
support LLM-powered exploration and sensemaking, we developed Sensecape, an
interactive system designed to support complex information tasks with an LLM by
enabling users to (1) manage the complexity of information through multilevel
abstraction and (2) seamlessly switch between foraging and sensemaking. Our
within-subject user study reveals that Sensecape empowers users to explore more
topics and structure their knowledge hierarchically. We contribute implications
for LLM-based workflows and interfaces for information tasks.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：CCGen: Explainable Complementary Concept Generation in E-Commerce</b></summary>
  <p><b>编号</b>：[182]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11480</p>
  <p><b>作者</b>：Jie Huang,  Yifan Gao,  Zheng Li,  Jingfeng Yang,  Yangqiu Song,  Chao Zhang,  Zining Zhu,  Haoming Jiang,  Kevin Chen-Chuan Chang,  Bing Yin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Digital Cameras, Camera Lenses, Camera Cases, Complementary Concept Generation, Concept Generation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose and study Complementary Concept Generation (CCGen): given a
concept of interest, e.g., "Digital Cameras", generating a list of
complementary concepts, e.g., 1) Camera Lenses 2) Batteries 3) Camera Cases 4)
Memory Cards 5) Battery Chargers. CCGen is beneficial for various applications
like query suggestion and item recommendation, especially in the e-commerce
domain. To solve CCGen, we propose to train language models to generate ranked
lists of concepts with a two-step training strategy. We also teach the models
to generate explanations by incorporating explanations distilled from large
teacher models. Extensive experiments and analysis demonstrate that our model
can generate high-quality concepts complementary to the input concept while
producing explanations to justify the predictions.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Learning Diverse Risk Preferences in Population-based Self-play</b></summary>
  <p><b>编号</b>：[183]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11476</p>
  <p><b>作者</b>：Yuhua Jiang,  Qihan Liu,  Xiaoteng Ma,  Chenghao Li,  Yiqin Yang,  Jun Yang,  Bin Liang,  Qianchuan Zhao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：self-play algorithms play, great successes, play an essential, essential role, role in solving</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Among the great successes of Reinforcement Learning (RL), self-play
algorithms play an essential role in solving competitive games. Current
self-play algorithms optimize the agent to maximize expected win-rates against
its current or historical copies, making it often stuck in the local optimum
and its strategy style simple and homogeneous. A possible solution is to
improve the diversity of policies, which helps the agent break the stalemate
and enhances its robustness when facing different opponents. However, enhancing
diversity in the self-play algorithms is not trivial. In this paper, we aim to
introduce diversity from the perspective that agents could have diverse risk
preferences in the face of uncertainty. Specifically, we design a novel
reinforcement learning algorithm called Risk-sensitive Proximal Policy
Optimization (RPPO), which smoothly interpolates between worst-case and
best-case policy learning and allows for policy learning with desired risk
preferences. Seamlessly integrating RPPO with population-based self-play,
agents in the population optimize dynamic risk-sensitive objectives with
experiences from playing against diverse opponents. Empirical results show that
our method achieves comparable or superior performance in competitive games and
that diverse modes of behaviors emerge. Our code is public online at
\url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：RAMiT: Reciprocal Attention Mixing Transformer for Lightweight Image  Restoration</b></summary>
  <p><b>编号</b>：[185]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11474</p>
  <p><b>作者</b>：Haram Choi,  Cheolwoong Na,  Jihyeon Oh,  Seungjae Lee,  Jinseop Kim,  Subeen Choe,  Jeongmin Lee,  Taehoon Kim,  Jihoon Yang</p>
  <p><b>备注</b>：Technical report. 9 pages for main contents + 14 pages for appendix + 6 pages for references</p>
  <p><b>关键词</b>：Attention Mixing Transformer, Reciprocal Attention Mixing, image restoration, recent works, works have made</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Although many recent works have made advancements in the image restoration
(IR) field, they often suffer from an excessive number of parameters. Another
issue is that most Transformer-based IR methods focus only on either local or
global features, leading to limited receptive fields or deficient parameter
issues. To address these problems, we propose a lightweight IR network,
Reciprocal Attention Mixing Transformer (RAMiT). It employs our proposed
dimensional reciprocal attention mixing Transformer (D-RAMiT) blocks, which
compute bi-dimensional (spatial and channel) self-attentions in parallel with
different numbers of multi-heads. The bi-dimensional attentions help each other
to complement their counterpart's drawbacks and are then mixed. Additionally,
we introduce a hierarchical reciprocal attention mixing (H-RAMi) layer that
compensates for pixel-level information losses and utilizes semantic
information while maintaining an efficient hierarchical structure. Furthermore,
we revisit and modify MobileNet V1 and V2 to attach efficient convolutions to
our proposed components. The experimental results demonstrate that RAMiT
achieves state-of-the-art performance on multiple lightweight IR tasks,
including super-resolution, color denoising, grayscale denoising, low-light
enhancement, and deraining. Codes will be available soon.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Graphologue: Exploring Large Language Model Responses with Interactive  Diagrams</b></summary>
  <p><b>编号</b>：[186]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11473</p>
  <p><b>作者</b>：Peiling Jiang,  Jude Rayan,  Steven P. Dow,  Haijun Xia</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large language models, unprecedented intelligence exhibited, Large language, language models, diverse applications</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have recently soared in popularity due to their
ease of access and the unprecedented intelligence exhibited on diverse
applications. However, LLMs like ChatGPT present significant limitations in
supporting complex information tasks due to the insufficient affordances of the
text-based medium and linear conversational structure. Through a formative
study with ten participants, we found that LLM interfaces often present
long-winded responses, making it difficult for people to quickly comprehend and
interact flexibly with various pieces of information, particularly during more
complex tasks. We present Graphologue, an interactive system that converts
text-based responses from LLMs into graphical diagrams to facilitate
information-seeking and question-answering tasks. Graphologue employs novel
prompting strategies and interface designs to extract entities and
relationships from LLM responses and constructs node-link diagrams in
real-time. Further, users can interact with the diagrams to flexibly adjust the
graphical presentation and to submit context-specific prompts to obtain more
information. Utilizing diagrams, Graphologue enables graphical, non-linear
dialogues between humans and LLMs, facilitating information exploration,
organization, and comprehension.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Testing System Intelligence</b></summary>
  <p><b>编号</b>：[187]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11472</p>
  <p><b>作者</b>：Joseph Sifakis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：practical problems raised, replacement test, intelligent systems, test, practical problems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We discuss the adequacy of tests for intelligent systems and practical
problems raised by their implementation. We propose the replacement test as the
ability of a system to replace successfully another system performing a task in
a given context. We show how it can characterize salient aspects of human
intelligence that cannot be taken into account by the Turing test. We argue
that building intelligent systems passing the replacement test involves a
series of technical problems that are outside the scope of current AI. We
present a framework for implementing the proposed test and validating the
properties of the intelligent systems. We discuss the inherent limitations of
intelligent system validation and advocate new theoretical foundations for
extending existing rigorous test methods. We suggest that the replacement test,
based on the complementarity of skills between human and machine, can lead to a
multitude of intelligence concepts reflecting the ability to combine data-based
and symbolic knowledge to varying degrees.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：SelfzCoT: a Self-Prompt Zero-shot CoT from Semantic-level to Code-level  for a Better Utilization of LLMs</b></summary>
  <p><b>编号</b>：[194]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11461</p>
  <p><b>作者</b>：IokTong Lei,  ZhiDong Deng</p>
  <p><b>备注</b>：preprint, under review</p>
  <p><b>关键词</b>：arithmetic reasoning tasks, paper show, show a work, zero-shot arithmetic reasoning, reasoning tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper show a work on better use of LLMs with SelfzCoT a self-prompt
zero-shot CoT. Specifically, on the zero-shot arithmetic reasoning tasks, the
accuracy of the proposed SelfzCoT is improved with GSM8K from 40.50% to 82.34%,
with MultiArith from 79.3% to 94.7%, with ADDSUB from 74.70% to 94.10%, with
SingleEq from 78.70% to 91.30%, with AQUA from 31.90% to 82.33%, and with SVAMP
from 63.70% to 79.70%. Totally, using the first two lasting path activations to
LLM and particularly, the code-level self-prompt, the SelfzCoT has a huge
improvement on all six zero-shot arithmetic reasoning tasks. Additionally, our
modified zero-shot CoT (MzCoT) also achieves remarkable performance in the
reasoning tasks. The accuracy of the proposed MzCoT is enhanced with GSM8K from
40.50% to 76.32%, with MultiArith from 79.3% to 96.97%, with ADDSUB from 74.70%
to 92.39%, with SingleEq from 78.70% to 94.60%, with AQUA from 31.90% to
79.90%, and with SVAMP from 63.70% to 81.50%. Notably, SelfzCoT has the best
performance on GSM8K among all the recent zero-shot methods.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Self-Agreement: A Framework for Fine-tuning Language Models to Find  Agreement among Diverse Opinions</b></summary>
  <p><b>编号</b>：[195]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11460</p>
  <p><b>作者</b>：Shiyao Ding,  Takayuki Ito</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：multiagent systems, challenging topic, topic in multiagent, agreement, opinions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Finding an agreement among diverse opinions is a challenging topic in
multiagent systems. Recently, large language models (LLMs) have shown great
potential in addressing this challenge due to their remarkable capabilities in
comprehending human opinions and generating human-like text. However, they
typically rely on extensive human-annotated data. In this paper, we propose
Self-Agreement, a novel framework for fine-tuning LLMs to autonomously find
agreement using data generated by LLM itself. Specifically, our approach
employs the generative pre-trained transformer-3 (GPT-3) to generate multiple
opinions for each question in a question dataset and create several agreement
candidates among these opinions. Then, a bidirectional encoder representations
from transformers (BERT)-based model evaluates the agreement score of each
agreement candidate and selects the one with the highest agreement score. This
process yields a dataset of question-opinion-agreements, which we use to
fine-tune a pre-trained LLM for discovering agreements among diverse opinions.
Remarkably, a pre-trained LLM fine-tuned by our Self-Agreement framework
achieves comparable performance to GPT-3 with only 1/25 of its parameters,
showcasing its ability to identify agreement among various opinions without the
need for human-annotated data.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Shattering the Agent-Environment Interface for Fine-Tuning Inclusive  Language Models</b></summary>
  <p><b>编号</b>：[198]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11455</p>
  <p><b>作者</b>：Wanqiao Xu,  Shi Dong,  Dilip Arumugam,  Benjamin Van Roy</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：emulate human feedback, human feedback, autoregressive language models, language model, emulate human</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A centerpiece of the ever-popular reinforcement learning from human feedback
(RLHF) approach to fine-tuning autoregressive language models is the explicit
training of a reward model to emulate human feedback, distinct from the
language model itself. This reward model is then coupled with policy-gradient
methods to dramatically improve the alignment between language model outputs
and desired responses. In this work, we adopt a novel perspective wherein a
pre-trained language model is itself simultaneously a policy, reward function,
and transition function. An immediate consequence of this is that reward
learning and language model fine-tuning can be performed jointly and directly,
without requiring any further downstream policy optimization. While this
perspective does indeed break the traditional agent-environment interface, we
nevertheless maintain that there can be enormous statistical benefits afforded
by bringing to bear traditional algorithmic concepts from reinforcement
learning. Our experiments demonstrate one concrete instance of this through
efficient exploration based on the representation and resolution of epistemic
uncertainty. In order to illustrate these ideas in a transparent manner, we
restrict attention to a simple didactic data generating process and leave for
future work extension to systems of practical scale.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Arukikata Travelogue Dataset</b></summary>
  <p><b>编号</b>：[203]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11444</p>
  <p><b>作者</b>：Hiroki Ouchi,  Hiroyuki Shindo,  Shoko Wakamiya,  Yuki Matsuda,  Naoya Inoue,  Shohei Higashiyama,  Satoshi Nakamura,  Taro Watanabe</p>
  <p><b>备注</b>：The application website for Arukikata Travelogue Dataset: this https URL</p>
  <p><b>关键词</b>：constructed Arukikata Travelogue, Arukikata Travelogue Dataset, constructed Arukikata, Arukikata Travelogue, released it free</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We have constructed Arukikata Travelogue Dataset and released it free of
charge for academic research. This dataset is a Japanese text dataset with a
total of over 31 million words, comprising 4,672 Japanese domestic travelogues
and 9,607 overseas travelogues. Before providing our dataset, there was a
scarcity of widely available travelogue data for research purposes, and each
researcher had to prepare their own data. This hinders the replication of
existing studies and fair comparative analysis of experimental results. Our
dataset enables any researchers to conduct investigation on the same data and
to ensure transparency and reproducibility in research. In this paper, we
describe the academic significance, characteristics, and prospects of our
dataset.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Zero-Shot Text Classification via Self-Supervised Tuning</b></summary>
  <p><b>编号</b>：[205]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11442</p>
  <p><b>作者</b>：Chaoqun Liu,  Wenxuan Zhang,  Guizhen Chen,  Xiaobao Wu,  Anh Tuan Luu,  Chip Hong Chang,  Lidong Bing</p>
  <p><b>备注</b>：Accepted to the Findings of ACL 2023</p>
  <p><b>关键词</b>：zero-shot text classification, large-scale annotated data, text classification tasks, text classification, Existing solutions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing solutions to zero-shot text classification either conduct prompting
with pre-trained language models, which is sensitive to the choices of
templates, or rely on large-scale annotated data of relevant tasks for
meta-tuning. In this work, we propose a new paradigm based on self-supervised
learning to solve zero-shot text classification tasks by tuning the language
models with unlabeled data, called self-supervised tuning. By exploring the
inherent structure of free texts, we propose a new learning objective called
first sentence prediction to bridge the gap between unlabeled data and text
classification tasks. After tuning the model to learn to predict the first
sentence in a paragraph based on the rest, the model is able to conduct
zero-shot inference on unseen tasks such as topic classification and sentiment
analysis. Experimental results show that our model outperforms the
state-of-the-art baselines on 7 out of 10 tasks. Moreover, the analysis reveals
that our model is less sensitive to the prompt design. Our code and pre-trained
models are publicly available at this https URL .</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：PS-FedGAN: An Efficient Federated Learning Framework Based on Partially  Shared Generative Adversarial Networks For Data Privacy</b></summary>
  <p><b>编号</b>：[209]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11437</p>
  <p><b>作者</b>：Achintha Wijesinghe,  Songyang Zhang,  Zhi Ding</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：effective learning paradigm, distributed computation owing, Federated Learning, effective learning, learning paradigm</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Federated Learning (FL) has emerged as an effective learning paradigm for
distributed computation owing to its strong potential in capturing underlying
data statistics while preserving data privacy. However, in cases of practical
data heterogeneity among FL clients, existing FL frameworks still exhibit
deficiency in capturing the overall feature properties of local client data
that exhibit disparate distributions. In response, generative adversarial
networks (GANs) have recently been exploited in FL to address data
heterogeneity since GANs can be integrated for data regeneration without
exposing original raw data. Despite some successes, existing GAN-related FL
frameworks often incur heavy communication cost and also elicit other privacy
concerns, which limit their applications in real scenarios. To this end, this
work proposes a novel FL framework that requires only partial GAN model
sharing. Named as PS-FedGAN, this new framework enhances the GAN releasing and
training mechanism to address heterogeneous data distributions across clients
and to strengthen privacy preservation at reduced communication cost,
especially over wireless networks. Our analysis demonstrates the convergence
and privacy benefits of the proposed PS-FEdGAN framework. Through experimental
results based on several well-known benchmark datasets, our proposed PS-FedGAN
shows great promise to tackle FL under non-IID client data distributions, while
securing data privacy and lowering communication overhead.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks</b></summary>
  <p><b>编号</b>：[211]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11430</p>
  <p><b>作者</b>：Shubhra Kanti Karmaker Santu,  Dongji Feng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：traditional conversational settings, shown great success, performing ill-defined complex, conversational settings, largely under-studied</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While LLMs have shown great success in understanding and generating text in
traditional conversational settings, their potential for performing ill-defined
complex tasks is largely under-studied. Indeed, we are yet to conduct
comprehensive benchmarking studies with multiple LLMs that are exclusively
focused on a complex task. However, conducting such benchmarking studies is
challenging because of the large variations in LLMs' performance when different
prompt types/styles are used and different degrees of detail are provided in
the prompts. To address this issue, the paper proposes a general taxonomy that
can be used to design prompts with specific properties in order to perform a
wide range of complex tasks. This taxonomy will allow future benchmarking
studies to report the specific categories of prompts used as part of the study,
enabling meaningful comparisons across different studies. Also, by establishing
a common standard through this taxonomy, researchers will be able to draw more
accurate conclusions about LLMs' performance on a specific complex task.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Post Hoc Explanations of Language Models Can Improve Language Models</b></summary>
  <p><b>编号</b>：[214]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11426</p>
  <p><b>作者</b>：Satyapriya,  Krishna,  Jiaqi Ma,  Dylan Slack,  Asma Ghandeharioun,  Sameer Singh,  Himabindu Lakkaraju</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：demonstrated remarkable capabilities, performing complex tasks, Post Hoc Explanations, Large Language Models, Amplifying Model Performance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) have demonstrated remarkable capabilities in
performing complex tasks. Moreover, recent research has shown that
incorporating human-annotated rationales (e.g., Chain-of- Thought prompting)
during in-context learning can significantly enhance the performance of these
models, particularly on tasks that require reasoning capabilities. However,
incorporating such rationales poses challenges in terms of scalability as this
requires a high degree of human involvement. In this work, we present a novel
framework, Amplifying Model Performance by Leveraging In-Context Learning with
Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges
by automating the process of rationale generation. To this end, we leverage
post hoc explanation methods which output attribution scores (explanations)
capturing the influence of each of the input features on model predictions.
More specifically, we construct automated natural language rationales that
embed insights from post hoc explanations to provide corrective signals to
LLMs. Extensive experimentation with real-world datasets demonstrates that our
framework, AMPLIFY, leads to prediction accuracy improvements of about 10-25%
over a wide range of tasks, including those where prior approaches which rely
on human-annotated rationales such as Chain-of-Thought prompting fall short.
Our work makes one of the first attempts at highlighting the potential of post
hoc explanations as valuable tools for enhancing the effectiveness of LLMs.
Furthermore, we conduct additional empirical analyses and ablation studies to
demonstrate the impact of each of the components of AMPLIFY, which, in turn,
lead to critical insights for refining in-context learning.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：Graph Propagation Transformer for Graph Representation Learning</b></summary>
  <p><b>编号</b>：[215]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11424</p>
  <p><b>作者</b>：Zhe Chen,  Hao Tan,  Tao Wang,  Tianrun Shen,  Tong Lu,  Qiuying Peng,  Cheng Cheng,  Yue Qi</p>
  <p><b>备注</b>：Accepted to IJCAI 2023</p>
  <p><b>关键词</b>：paper presents, graph, Graph Propagation Attention, Graph Propagation, propagation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a novel transformer architecture for graph representation
learning. The core insight of our method is to fully consider the information
propagation among nodes and edges in a graph when building the attention module
in the transformer blocks. Specifically, we propose a new attention mechanism
called Graph Propagation Attention (GPA). It explicitly passes the information
among nodes and edges in three ways, i.e. node-to-node, node-to-edge, and
edge-to-node, which is essential for learning graph-structured data. On this
basis, we design an effective transformer architecture named Graph Propagation
Transformer (GPTrans) to further help learn graph data. We verify the
performance of GPTrans in a wide range of graph learning experiments on several
benchmark datasets. These results show that our method outperforms many
state-of-the-art transformer-based graph models with better performance. The
code will be released at this https URL.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：PastNet: Introducing Physical Inductive Biases for Spatio-temporal Video  Prediction</b></summary>
  <p><b>编号</b>：[217]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11421</p>
  <p><b>作者</b>：Hao Wu,  Wei Xion,  Fan Xu,  Xiao Luo,  Chong Chen,  Xian-Sheng Hua,  Haixin Wang</p>
  <p><b>备注</b>：11</p>
  <p><b>关键词</b>：historical data streams, future videos based, involves generating future, video prediction, data streams</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we investigate the challenge of spatio-temporal video
prediction, which involves generating future videos based on historical data
streams. Existing approaches typically utilize external information such as
semantic maps to enhance video prediction, which often neglect the inherent
physical knowledge embedded within videos. Furthermore, their high
computational demands could impede their applications for high-resolution
videos. To address these constraints, we introduce a novel approach called
Physics-assisted Spatio-temporal Network (PastNet) for generating high-quality
video predictions. The core of our PastNet lies in incorporating a spectral
convolution operator in the Fourier domain, which efficiently introduces
inductive biases from the underlying physical laws. Additionally, we employ a
memory bank with the estimated intrinsic dimensionality to discretize local
features during the processing of complex spatio-temporal signals, thereby
reducing computational costs and facilitating efficient high-resolution video
prediction. Extensive experiments on various widely-used datasets demonstrate
the effectiveness and efficiency of the proposed PastNet compared with
state-of-the-art methods, particularly in high-resolution scenarios.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：JetSeg: Efficient Real-Time Semantic Segmentation Model for Low-Power  GPU-Embedded Systems</b></summary>
  <p><b>编号</b>：[219]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11419</p>
  <p><b>作者</b>：Miguel Lopez-Montiel,  Daniel Alejandro Lopez,  Oscar Montiel</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Real-time semantic segmentation, requires high-accuracy models, semantic segmentation, challenging task, task that requires</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Real-time semantic segmentation is a challenging task that requires
high-accuracy models with low-inference times. Implementing these models on
embedded systems is limited by hardware capability and memory usage, which
produces bottlenecks. We propose an efficient model for real-time semantic
segmentation called JetSeg, consisting of an encoder called JetNet, and an
improved RegSeg decoder. The JetNet is designed for GPU-Embedded Systems and
includes two main components: a new light-weight efficient block called
JetBlock, that reduces the number of parameters minimizing memory usage and
inference time without sacrificing accuracy; a new strategy that involves the
combination of asymmetric and non-asymmetric convolutions with
depthwise-dilated convolutions called JetConv, a channel shuffle operation,
light-weight activation functions, and a convenient number of group
convolutions for embedded systems, and an innovative loss function named
JetLoss, which integrates the Precision, Recall, and IoUB losses to improve
semantic segmentation and reduce computational complexity. Experiments
demonstrate that JetSeg is much faster on workstation devices and more suitable
for Low-Power GPU-Embedded Systems than existing state-of-the-art models for
real-time semantic segmentation. Our approach outperforms state-of-the-art
real-time encoder-decoder models by reducing 46.70M parameters and 5.14%
GFLOPs, which makes JetSeg up to 2x faster on the NVIDIA Titan RTX GPU and the
Jetson Xavier than other models. The JetSeg code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：Federated Foundation Models: Privacy-Preserving and Collaborative  Learning for Large Models</b></summary>
  <p><b>编号</b>：[222]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11414</p>
  <p><b>作者</b>：Sixing Yu,  J. Pablo Muñoz,  Ali Jannesari</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：demonstrated remarkable success, leverage vast amounts, Federated Foundation Models, Foundation Models, Federated Foundation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Foundation Models (FMs), such as BERT, GPT, ViT, and CLIP, have demonstrated
remarkable success in a wide range of applications, driven by their ability to
leverage vast amounts of data for pre-training. However, optimizing FMs often
requires access to sensitive data, raising privacy concerns and limiting their
applicability in certain domains. In this paper, we introduce the concept of
Federated Foundation Models (FFMs), a novel approach that combines the benefits
of FMs and Federated Learning (FL) to enable privacy-preserving and
collaborative learning across multiple institutions. We discuss the potential
benefits and challenges of integrating FL into the lifespan of FMs, covering
pre-training, fine-tuning, and application. We further provide formal
definitions of FFM tasks, including FFM pre-training, FFM fine-tuning, and
federated prompt engineering, allowing for more personalized and context-aware
models while maintaining data privacy. Moreover, we explore the possibility of
continual/lifelong learning in FFMs, as increased computational power at the
edge unlocks the potential for optimizing FMs using newly generated private
data at edges. We present experiments and evaluations comparing the performance
of FFMs to traditional FMs on various downstream tasks, demonstrating the
effectiveness of our approach in preserving privacy, reducing overfitting, and
improving model generalizability. The proposed Federated Foundation Models
offer a flexible and scalable framework for training large language models in a
privacy-preserving manner, paving the way for future advancements in both FM
pre-training and federated learning.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：LATTE: Label-efficient Incident Phenotyping from Longitudinal Electronic  Health Records</b></summary>
  <p><b>编号</b>：[226]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11407</p>
  <p><b>作者</b>：Jun Wen,  Jue Hou,  Clara-Lea Bonzel,  Yihan Zhao,  Victor M. Castro,  Vivian S. Gainer,  Dana Weisenfeld,  Tianrun Cai,  Yuk-Lam Ho,  Vidul A. Panickan,  Lauren Costa,  Chuan Hong,  J. Michael Gaziano,  Katherine P. Liao,  Junwei Lu,  Kelly Cho,  Tianxi Cai</p>
  <p><b>备注</b>：ERHs data</p>
  <p><b>关键词</b>：Electronic health record, support real-world evidence, LATTE, generate reliable RWE, Electronic health</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Electronic health record (EHR) data are increasingly used to support
real-world evidence (RWE) studies. Yet its ability to generate reliable RWE is
limited by the lack of readily available precise information on the timing of
clinical events such as the onset time of heart failure. We propose a
LAbel-efficienT incidenT phEnotyping (LATTE) algorithm to accurately annotate
the timing of clinical events from longitudinal EHR data. By leveraging the
pre-trained semantic embedding vectors from large-scale EHR data as prior
knowledge, LATTE selects predictive EHR features in a concept re-weighting
module by mining their relationship to the target event and compresses their
information into longitudinal visit embeddings through a visit attention
learning network. LATTE employs a recurrent neural network to capture the
sequential dependency between the target event and visit embeddings
before/after it. To improve label efficiency, LATTE constructs highly
informative longitudinal silver-standard labels from large-scale unlabeled
patients to perform unsupervised pre-training and semi-supervised joint
training. Finally, LATTE enhances cross-site portability via contrastive
representation learning. LATTE is evaluated on three analyses: the onset of
type-2 diabetes, heart failure, and the onset and relapses of multiple
sclerosis. We use various evaluation metrics present in the literature
including the $ABC_{gain}$, the proportion of reduction in the area between the
observed event indicator and the predicted cumulative incidences in reference
to the prediction per incident prevalence. LATTE consistently achieves
substantial improvement over benchmark methods such as SAMGEP and RETAIN in all
settings.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：A Survey of Safety and Trustworthiness of Large Language Models through  the Lens of Verification and Validation</b></summary>
  <p><b>编号</b>：[233]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11391</p>
  <p><b>作者</b>：Xiaowei Huang,  Wenjie Ruan,  Wei Huang,  Gaojie Jin,  Yi Dong,  Changshun Wu,  Saddek Bensalem,  Ronghui Mu,  Yi Qi,  Xingyu Zhao,  Kaiwen Cai,  Yanghao Zhang,  Sihao Wu,  Peipei Xu,  Dengyu Wu,  Andre Freitas,  Mustafa A. Mustafa</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Large Language, Language Models, ability to engage, engage end-users</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) have exploded a new heatwave of AI, for their
ability to engage end-users in human-level conversations with detailed and
articulate answers across many knowledge domains. In response to their fast
adoption in many industrial applications, this survey concerns their safety and
trustworthiness. First, we review known vulnerabilities of the LLMs,
categorising them into inherent issues, intended attacks, and unintended bugs.
Then, we consider if and how the Verification and Validation (V&V) techniques,
which have been widely developed for traditional software and deep learning
models such as convolutional neural networks, can be integrated and further
extended throughout the lifecycle of the LLMs to provide rigorous analysis to
the safety and trustworthiness of LLMs and their applications. Specifically, we
consider four complementary techniques: falsification and evaluation,
verification, runtime monitoring, and ethical use. Considering the fast
development of LLMs, this survey does not intend to be complete (although it
includes 300 references), especially when it comes to the applications of LLMs
in various domains, but rather a collection of organised literature reviews and
discussions to support the quick understanding of the safety and
trustworthiness issues from the perspective of V&V.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：ALT: An Automatic System for Long Tail Scenario Modeling</b></summary>
  <p><b>编号</b>：[234]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11390</p>
  <p><b>作者</b>：Ya-Lin Zhang,  Jun Zhou,  Yankun Ren,  Yue Zhang,  Xinxing Yang,  Meng Li,  Qitao Shi,  Longfei Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：model training stage, model inference stage, insufficient human resources, long tail scenario, tail scenario modeling</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we consider the problem of long tail scenario modeling with
budget limitation, i.e., insufficient human resources for model training stage
and limited time and computing resources for model inference stage. This
problem is widely encountered in various applications, yet has received
deficient attention so far. We present an automatic system named ALT to deal
with this problem. Several efforts are taken to improve the algorithms used in
our system, such as employing various automatic machine learning related
techniques, adopting the meta learning philosophy, and proposing an essential
budget-limited neural architecture search method, etc. Moreover, to build the
system, many optimizations are performed from a systematic perspective, and
essential modules are armed, making the system more feasible and efficient. We
perform abundant experiments to validate the effectiveness of our system and
demonstrate the usefulness of the critical modules in our system. Moreover,
online results are provided, which fully verified the efficacy of our system.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：Domain Generalization Deep Graph Transformation</b></summary>
  <p><b>编号</b>：[235]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11389</p>
  <p><b>作者</b>：Shiyu Wang,  Guangji Bai,  Qingyang Zhu,  Zhaohui Qin,  Liang Zhao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Graph transformation, predicts graph transition, common problem, training data, important and common</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Graph transformation that predicts graph transition from one mode to another
is an important and common problem. Despite much progress in developing
advanced graph transformation techniques in recent years, the fundamental
assumption typically required in machine-learning models that the testing and
training data preserve the same distribution does not always hold. As a result,
domain generalization graph transformation that predicts graphs not available
in the training data is under-explored, with multiple key challenges to be
addressed including (1) the extreme space complexity when training on all
input-output mode combinations, (2) difference of graph topologies between the
input and the output modes, and (3) how to generalize the model to (unseen)
target domains that are not in the training data. To fill the gap, we propose a
multi-input, multi-output, hypernetwork-based graph neural network
(MultiHyperGNN) that employs a encoder and a decoder to encode topologies of
both input and output modes and semi-supervised link prediction to enhance the
graph transformation task. Instead of training on all mode combinations,
MultiHyperGNN preserves a constant space complexity with the encoder and the
decoder produced by two novel hypernetworks. Comprehensive experiments show
that MultiHyperGNN has a superior performance than competing models in both
prediction and domain generalization tasks.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：Justices for Information Bottleneck Theory</b></summary>
  <p><b>编号</b>：[236]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11387</p>
  <p><b>作者</b>：Faxian Cao,  Yongqiang Cheng,  Adil Mehmood Khan,  Zhijing Yang</p>
  <p><b>备注</b>：9 pages, 1 figures (4 subfigures)</p>
  <p><b>关键词</b>：injecting fresh perspectives, injecting fresh, reaffirm its validity, timely response, response to mounting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This study comes as a timely response to mounting criticism of the
information bottleneck (IB) theory, injecting fresh perspectives to rectify
misconceptions and reaffirm its validity. Firstly, we introduce an auxiliary
function to reinterpret the maximal coding rate reduction method as a special
yet local optimal case of IB theory. Through this auxiliary function, we
clarify the paradox of decreasing mutual information during the application of
ReLU activation in deep learning (DL) networks. Secondly, we challenge the
doubts about IB theory's applicability by demonstrating its capacity to explain
the absence of a compression phase with linear activation functions in hidden
layers, when viewed through the lens of the auxiliary function. Lastly, by
taking a novel theoretical stance, we provide a new way to interpret the inner
organizations of DL networks by using IB theory, aligning them with recent
experimental evidence. Thus, this paper serves as an act of justice for IB
theory, potentially reinvigorating its standing and application in DL and other
fields such as communications and biomedical research.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：Do Models Really Learn to Follow Instructions? An Empirical Study of  Instruction Tuning</b></summary>
  <p><b>编号</b>：[239]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11383</p>
  <p><b>作者</b>：Po-Nien Kung,  Nanyun Peng</p>
  <p><b>备注</b>：Proceedings of the 61th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</p>
  <p><b>关键词</b>：generalizability to unseen, achieved great performance, achieved great, performance, Recent works</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent works on instruction tuning (IT) have achieved great performance with
zero-shot generalizability to unseen tasks. With additional context (e.g., task
definition, examples) provided to models for fine-tuning, they achieved much
higher performance than untuned models. However, despite impressive performance
gains, the underlying mechanism for IT to work remains understudied. In this
work, we analyze how models utilize instructions during IT by comparing model
training with altered vs. original instructions. Specifically, we create
simplified task definitions by removing all semantic components and only
leaving the output space information, and delusive examples that contain
incorrect input-output mapping. Our experiments show that models trained on
simplified task definition or delusive examples can achieve comparable
performance to the ones trained on the original instructions and examples.
Furthermore, we introduce a random baseline to perform zero-shot classification
tasks, and find it achieves similar performance (40% accuracy) as IT does (48%
accuracy). In summary, our analysis provides evidence that the impressive
performance gain of current IT models can come from picking up superficial
patterns, such as learning the output format and guessing. Our study highlights
the urgent need for more reliable IT methods and evaluation.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：Visualizing Linguistic Diversity of Text Datasets Synthesized by Large  Language Models</b></summary>
  <p><b>编号</b>：[248]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11364</p>
  <p><b>作者</b>：Emily Reif,  Minsuk Kahng,  Savvas Petridis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large language models, Large language, language models, generate smaller, prompting for benchmarking</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) can be used to generate smaller, more refined
datasets via few-shot prompting for benchmarking, fine-tuning or other use
cases. However, understanding and evaluating these datasets is difficult, and
the failure modes of LLM-generated data are still not well understood.
Specifically, the data can be repetitive in surprising ways, not only
semantically but also syntactically and lexically. We present LinguisticLens, a
novel inter-active visualization tool for making sense of and analyzing
syntactic diversity of LLM-generated datasets. LinguisticLens clusters text
along syntactic, lexical, and semantic axes. It supports hierarchical
visualization of a text dataset, allowing users to quickly scan for an overview
and inspect individual examples. The live demo is available at
this http URL.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：Quantifying the robustness of deep multispectral segmentation models  against natural perturbations and data poisoning</b></summary>
  <p><b>编号</b>：[258]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11347</p>
  <p><b>作者</b>：Elise Bishoff,  Charles Godfrey,  Myles McKay,  Eleanor Byler</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：traditional RGB channels, including additional spectral, additional spectral bands, natural perturbations, spectral bands</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In overhead image segmentation tasks, including additional spectral bands
beyond the traditional RGB channels can improve model performance. However, it
is still unclear how incorporating this additional data impacts model
robustness to adversarial attacks and natural perturbations. For adversarial
robustness, the additional information could improve the model's ability to
distinguish malicious inputs, or simply provide new attack avenues and
vulnerabilities. For natural perturbations, the additional information could
better inform model decisions and weaken perturbation effects or have no
significant influence at all. In this work, we seek to characterize the
performance and robustness of a multispectral (RGB and near infrared) image
segmentation model subjected to adversarial attacks and natural perturbations.
While existing adversarial and natural robustness research has focused
primarily on digital perturbations, we prioritize on creating realistic
perturbations designed with physical world conditions in mind. For adversarial
robustness, we focus on data poisoning attacks whereas for natural robustness,
we focus on extending ImageNet-C common corruptions for fog and snow that
coherently and self-consistently perturbs the input data. Overall, we find both
RGB and multispectral models are vulnerable to data poisoning attacks
regardless of input or fusion architectures and that while physically
realizable natural perturbations still degrade model performance, the impact
differs based on fusion architecture and input data.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：Writing your own book: A method for going from closed to open book QA to  improve robustness and performance of smaller LLMs</b></summary>
  <p><b>编号</b>：[267]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11334</p>
  <p><b>作者</b>：Giorgi Kokaia,  Pratyush Sinha,  Yutong Jiang,  Nozha Boujemaa</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, large language, designed to enhance, Tree-Search, Self-contextualizing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce two novel methods, Tree-Search and Self-contextualizing QA,
designed to enhance the performance of large language models (LLMs) in
question-answering tasks. Tree-Search is a sampling technique specifically
created to extract diverse information from an LLM for a given prompt.
Self-contextualizing QA leverages Tree-Search to enable the model to create its
own context using a wide range of information relevant to the prompt, evaluate
it explicitly and return a open book answer to the initial prompt . We
demonstrate that the quality of generated answers improves according to various
metrics, including accuracy, informativeness, coherence, and consistency, as
evaluated by GPT3.5(text-davinci-003). Furthermore, we show that our methods
result in increased robustness and that performance is positively correlated
with tree size, benefiting both answer quality and robustness. Finally, we
discuss other promising applications of Tree-Search, highlighting its potential
to enhance a broad range of tasks beyond question-answering.
\noindent We also discuss several areas for future work, including refining
the Tree-Search and Self-Contextualizing QA methods, improving the coherence of
the generated context, and investigating the impact of bootstrapping on model
robustness</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：SpikeCP: Delay-Adaptive Reliable Spiking Neural Networks via Conformal  Prediction</b></summary>
  <p><b>编号</b>：[271]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11322</p>
  <p><b>作者</b>：Jiechen Chen,  Sangwoo Park,  Osvaldo Simeone</p>
  <p><b>备注</b>：Under review</p>
  <p><b>关键词</b>：Spiking neural networks, internal event-driven neural, event-driven neural dynamics, process time-series data, energy consumption depends</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Spiking neural networks (SNNs) process time-series data via internal
event-driven neural dynamics whose energy consumption depends on the number of
spikes exchanged between neurons over the course of the input presentation. In
typical implementations of an SNN classifier, decisions are produced after the
entire input sequence has been processed, resulting in latency and energy
consumption levels that are fairly uniform across inputs. Recently introduced
delay-adaptive SNNs tailor the inference latency -- and, with it, the energy
consumption -- to the difficulty of each example, by producing an early
decision when the SNN model is sufficiently ``confident''. In this paper, we
start by observing that, as an SNN processes input samples, its classification
decisions tend to be first under-confident and then over-confident with respect
to the decision's ground-truth, unknown, test accuracy. This makes it difficult
to determine a stopping time that ensures a desired level of accuracy. To
address this problem, we introduce a novel delay-adaptive SNN-based inference
methodology that, wrapping around any pre-trained SNN classifier, provides
guaranteed reliability for the decisions produced at input-dependent stopping
times. The approach entails minimal added complexity as compared to the
underlying SNN, requiring only thresholding and counting operations at run
time, and it leverages tools from conformal prediction (CP).</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：Parameter-Efficient Learning for Text-to-Speech Accent Adaptation</b></summary>
  <p><b>编号</b>：[273]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11320</p>
  <p><b>作者</b>：Li-Jen Yang,  Chao-Han Huck Yang,  Jen-Tzung Chien</p>
  <p><b>备注</b>：Accepted to Interspeech 2023</p>
  <p><b>关键词</b>：paper presents, develop a low-resource, low-resource accent adaptation, frozen pre-trained TTS, TTS</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a parameter-efficient learning (PEL) to develop a
low-resource accent adaptation for text-to-speech (TTS). A resource-efficient
adaptation from a frozen pre-trained TTS model is developed by using only 1.2\%
to 0.8\% of original trainable parameters to achieve competitive performance in
voice synthesis. Motivated by a theoretical foundation of optimal transport
(OT), this study carries out PEL for TTS where an auxiliary unsupervised loss
based on OT is introduced to maximize a difference between the pre-trained
source domain and the (unseen) target domain, in addition to its supervised
training loss. Further, we leverage upon this unsupervised loss refinement to
boost system performance via either sliced Wasserstein distance or maximum mean
discrepancy. The merit of this work is demonstrated by fulfilling PEL solutions
based on residual adapter learning, and model reprogramming when evaluating the
Mandarin accent adaptation. Experiment results show that the proposed methods
can achieve competitive naturalness with parameter-efficient decoder
fine-tuning, and the auxiliary unsupervised loss improves model performance
empirically.</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：BELLA: Black box model Explanations by Local Linear Approximations</b></summary>
  <p><b>编号</b>：[278]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11311</p>
  <p><b>作者</b>：Nedeljko Radulovic,  Albert Bifet,  Fabian Suchanek</p>
  <p><b>备注</b>：21 pages,3 figures, submitted to Journal of Artificial Intelligence</p>
  <p><b>关键词</b>：recent years, understanding the decision-making, assess their performance, decision-making process, legal requirement</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, understanding the decision-making process of black-box
models has become not only a legal requirement but also an additional way to
assess their performance. However, the state of the art post-hoc interpretation
approaches rely on synthetic data generation. This introduces uncertainty and
can hurt the reliability of the interpretations. Furthermore, they tend to
produce explanations that apply to only very few data points. This makes the
explanations brittle and limited in scope. Finally, they provide scores that
have no direct verifiable meaning. In this paper, we present BELLA, a
deterministic model-agnostic post-hoc approach for explaining the individual
predictions of regression black-box models. BELLA provides explanations in the
form of a linear model trained in the feature space. Thus, its coefficients can
be used directly to compute the predicted value from the feature values.
Furthermore, BELLA maximizes the size of the neighborhood to which the linear
model applies, so that the explanations are accurate, simple, general, and
robust. BELLA can produce both factual and counterfactual explanations. Our
user study confirms the importance of the desiderata we optimize, and our
experiments show that BELLA outperforms the state-of-the-art approaches on
these desiderata.</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：Counterfactuals for Design: A Model-Agnostic Method For Design  Recommendations</b></summary>
  <p><b>编号</b>：[280]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11308</p>
  <p><b>作者</b>：Lyle Regenwetter,  Yazan Abu Obaideh,  Faez Ahmed</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：MCD, Design, introduce Multi-Objective Counterfactuals, counterfactual search, counterfactual optimization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce Multi-Objective Counterfactuals for Design (MCD), a novel method
for counterfactual optimization in design problems. Counterfactuals are
hypothetical situations that can lead to a different decision or choice. In
this paper, the authors frame the counterfactual search problem as a design
recommendation tool that can help identify modifications to a design, leading
to better functional performance. MCD improves upon existing counterfactual
search methods by supporting multi-objective queries, which are crucial in
design problems, and by decoupling the counterfactual search and sampling
processes, thus enhancing efficiency and facilitating objective tradeoff
visualization. The paper demonstrates MCD's core functionality using a
two-dimensional test case, followed by three case studies of bicycle design
that showcase MCD's effectiveness in real-world design problems. In the first
case study, MCD excels at recommending modifications to query designs that can
significantly enhance functional performance, such as weight savings and
improvements to the structural safety factor. The second case study
demonstrates that MCD can work with a pre-trained language model to suggest
design changes based on a subjective text prompt effectively. Lastly, the
authors task MCD with increasing a query design's similarity to a target image
and text prompt while simultaneously reducing weight and improving structural
performance, demonstrating MCD's performance on a complex multimodal query.
Overall, MCD has the potential to provide valuable recommendations for
practitioners and design automation researchers looking for answers to their
``What if'' questions by exploring hypothetical design modifications and their
impact on multiple design objectives. The code, test problems, and datasets
used in the paper are available to the public at
this http URL.</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：NeuSTIP: A Novel Neuro-Symbolic Model for Link and Time Prediction in  Temporal Knowledge Graphs</b></summary>
  <p><b>编号</b>：[283]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11301</p>
  <p><b>作者</b>：Ishaan Singh,  Navdeep Kaur,  Garima Gaur,  Mausam</p>
  <p><b>备注</b>：13 pages, 2 Figures</p>
  <p><b>关键词</b>：Knowledge Graph Completion, Graph Completion, Knowledge Graph, Temporal Knowledge Graph, static facts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While Knowledge Graph Completion (KGC) on static facts is a matured field,
Temporal Knowledge Graph Completion (TKGC), that incorporates validity time
into static facts is still in its nascent stage. The KGC methods fall into
multiple categories including embedding-based, rule-based, GNN-based,
pretrained Language Model based approaches. However, such dimensions have not
been explored in TKG. To that end, we propose a novel temporal neuro-symbolic
model, NeuSTIP, that performs link prediction and time interval prediction in a
TKG. NeuSTIP learns temporal rules in the presence of the Allen predicates that
ensure the temporal consistency between neighboring predicates in a given rule.
We further design a unique scoring function that evaluates the confidence of
the candidate answers while performing link prediction and time interval
prediction by utilizing the learned rules. Our empirical evaluation on two time
interval based TKGC datasets suggests that our model outperforms
state-of-the-art models for both link prediction and the time interval
prediction task.</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：Solving probability puzzles with logic toolkit</b></summary>
  <p><b>编号</b>：[287]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11294</p>
  <p><b>作者</b>：Adrian Groza</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：models, FOL, favorable models, interpretation models, FOL theory</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The proposed approach is to formalise the probabilistic puzzle in equational
FOL. Two formalisations are needed: one theory for all models of the given
puzzle, and a second theory for the favorable models. Then Mace4 - that
computes all the interpretation models of a FOL theory - is called twice.
First, it is asked to compute all the possible models M p .Second, the
additional constraint is added, and Mace4 computes only favourabile models M f.
Finally, the definition of probability is applied: the number of favorable
models is divided by the number of possible models. The proposed approach
equips students from the logic tribe to find the correct solution for puzzles
from the probabilitistic tribe, by using their favourite instruments: modelling
and formalisation. I have exemplified here five probabilistic puzzles and how
they can be solved by translating the min FOL and then find the corresponding
interpretation models. Mace4 was the tool of choice here. Ongoing work is
investigating the limits of this method on various collections of probabilistic
puzzles</p>
  </details>
</details>
<details>
  <summary>83. <b>标题：On the Statistical Efficiency of Mean Field Reinforcement Learning with  General Function Approximation</b></summary>
  <p><b>编号</b>：[293]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11283</p>
  <p><b>作者</b>：Jiawei Huang,  Batuhan Yardim,  Niao He</p>
  <p><b>备注</b>：47 Pages</p>
  <p><b>关键词</b>：Reinforcement Learning, general function approximation, Mean-Field Control, Mean-Field Game, function approximation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we study the statistical efficiency of Reinforcement Learning
in Mean-Field Control (MFC) and Mean-Field Game (MFG) with general function
approximation. We introduce a new concept called Mean-Field Model-Based Eluder
Dimension (MBED), which subsumes a rich family of Mean-Field RL problems.
Additionally, we propose algorithms based on Optimistic Maximal Likelihood
Estimation, which can return an $\epsilon$-optimal policy for MFC or an
$\epsilon$-Nash Equilibrium policy for MFG, with sample complexity polynomial
w.r.t. relevant parameters and independent of the number of states, actions and
the number of agents. Notably, our results only require a mild assumption of
Lipschitz continuity on transition dynamics and avoid strong structural
assumptions in previous work. Finally, in the tabular setting, given the access
to a generative model, we establish an exponential lower bound for MFC setting,
while providing a novel sample-efficient model elimination algorithm to
approximate equilibrium in MFG setting. Our results reveal a fundamental
separation between RL for single-agent, MFC, and MFG from the sample efficiency
perspective.</p>
  </details>
</details>
<details>
  <summary>84. <b>标题：Towards Collaborative Plan Acquisition through Theory of Mind Modeling  in Situated Dialogue</b></summary>
  <p><b>编号</b>：[297]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11271</p>
  <p><b>作者</b>：Cristian-Paul Bara,  Ziqiao Ma,  Yingzhuo Yu,  Julie Shah,  Joyce Chai</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：incomplete initial plans, incomplete initial, tasks, partner, knowledge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Collaborative tasks often begin with partial task knowledge and incomplete
initial plans from each partner. To complete these tasks, agents need to engage
in situated communication with their partners and coordinate their partial
plans towards a complete plan to achieve a joint task goal. While such
collaboration seems effortless in a human-human team, it is highly challenging
for human-AI collaboration. To address this limitation, this paper takes a step
towards collaborative plan acquisition, where humans and agents strive to learn
and communicate with each other to acquire a complete plan for joint tasks.
Specifically, we formulate a novel problem for agents to predict the missing
task knowledge for themselves and for their partners based on rich perceptual
and dialogue history. We extend a situated dialogue benchmark for symmetric
collaborative tasks in a 3D blocks world and investigate computational
strategies for plan acquisition. Our empirical results suggest that predicting
the partner's missing knowledge is a more viable approach than predicting one's
own. We show that explicit modeling of the partner's dialogue moves and mental
states produces improved and more stable results than without. These results
provide insight for future AI agents that can predict what knowledge their
partner is missing and, therefore, can proactively communicate such information
to help their partner acquire such missing knowledge toward a common
understanding of joint tasks.</p>
  </details>
</details>
<details>
  <summary>85. <b>标题：Brain-inspired learning in artificial neural networks: a review</b></summary>
  <p><b>编号</b>：[304]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11252</p>
  <p><b>作者</b>：Samuel Schmidgall,  Jascha Achterberg,  Thomas Miconi,  Louis Kirsch,  Rojin Ziaei,  S. Pardis Hajiseyedrazi,  Jason Eshraghian</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：achieving remarkable success, Artificial neural networks, game playing, achieving remarkable, diverse domains</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Artificial neural networks (ANNs) have emerged as an essential tool in
machine learning, achieving remarkable success across diverse domains,
including image and speech generation, game playing, and robotics. However,
there exist fundamental differences between ANNs' operating mechanisms and
those of the biological brain, particularly concerning learning processes. This
paper presents a comprehensive review of current brain-inspired learning
representations in artificial neural networks. We investigate the integration
of more biologically plausible mechanisms, such as synaptic plasticity, to
enhance these networks' capabilities. Moreover, we delve into the potential
advantages and challenges accompanying this approach. Ultimately, we pinpoint
promising avenues for future research in this rapidly advancing field, which
could bring us closer to understanding the essence of intelligence.</p>
  </details>
</details>
<details>
  <summary>86. <b>标题：A Parameter-Efficient Learning Approach to Arabic Dialect Identification  with Pre-Trained General-Purpose Speech Model</b></summary>
  <p><b>编号</b>：[308]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11244</p>
  <p><b>作者</b>：Srijith Radhakrishnan,  Chao-Han Huck Yang,  Sumeer Ahmad Khan,  Narsis A. Kiani,  David Gomez-Cabrero,  Jesper N. Tegner</p>
  <p><b>备注</b>：Accepted to Interspeech. Code is available at: this https URL under MIT license</p>
  <p><b>关键词</b>：techniques to repurpose, Arabic dialect identification, Arabic dialect, multi-layer encoder-decoder GSM, encoder-decoder GSM formulation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we explore Parameter-Efficient-Learning (PEL) techniques to
repurpose a General-Purpose-Speech (GSM) model for Arabic dialect
identification (ADI). Specifically, we investigate different setups to
incorporate trainable features into a multi-layer encoder-decoder GSM
formulation under frozen pre-trained settings. Our architecture includes
residual adapter and model reprogramming (input-prompting). We design a
token-level label mapping to condition the GSM for Arabic Dialect
Identification (ADI). This is challenging due to the high variation in
vocabulary and pronunciation among the numerous regional dialects. We achieve
new state-of-the-art accuracy on the ADI-17 dataset by vanilla fine-tuning. We
further reduce the training budgets with the PEL method, which performs within
1.86% accuracy to fine-tuning using only 2.5% of (extra) network trainable
parameters. Our study demonstrates how to identify Arabic dialects using a
small dataset and limited computation with open source code and pre-trained
models.</p>
  </details>
</details>
<details>
  <summary>87. <b>标题：Comparing Machines and Children: Using Developmental Psychology  Experiments to Assess the Strengths and Weaknesses of LaMDA Responses</b></summary>
  <p><b>编号</b>：[309]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11243</p>
  <p><b>作者</b>：Eliza Kosoy,  Emily Rose Reagan,  Leslie Lai,  Alison Gopnik,  Danielle Krettek Cobb</p>
  <p><b>备注</b>：9 pages, 7 figures</p>
  <p><b>关键词</b>：spent decades devising, decades devising experiments, tracing the origin, psychologists have spent, spent decades</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Developmental psychologists have spent decades devising experiments to test
the intelligence and knowledge of infants and children, tracing the origin of
crucial concepts and capacities. Moreover, experimental techniques in
developmental psychology have been carefully designed to discriminate the
cognitive capacities that underlie particular behaviors. We propose that using
classical experiments from child development is a particularly effective way to
probe the computational abilities of AI models, in general, and LLMs in
particular. First, the methodological techniques of developmental psychology,
such as the use of novel stimuli to control for past experience or control
conditions to determine whether children are using simple associations, can be
equally helpful for assessing the capacities of LLMs. In parallel, testing LLMs
in this way can tell us whether the information that is encoded in text is
sufficient to enable particular responses, or whether those responses depend on
other kinds of information, such as information from exploration of the
physical world. In this work we adapt classical developmental experiments to
evaluate the capabilities of LaMDA, a large language model from Google. We
propose a novel LLM Response Score (LRS) metric which can be used to evaluate
other language models, such as GPT. We find that LaMDA generates appropriate
responses that are similar to those of children in experiments involving social
understanding, perhaps providing evidence that knowledge of these domains is
discovered through language. On the other hand, LaMDA's responses in early
object and action understanding, theory of mind, and especially causal
reasoning tasks are very different from those of young children, perhaps
showing that these domains require more real-world, self-initiated exploration
and cannot simply be learned from patterns in language input.</p>
  </details>
</details>
<details>
  <summary>88. <b>标题：Milestones in Autonomous Driving and Intelligent Vehicles Part  \uppercase\expandafter{\romannumeral1}: Control, Computing System Design,  Communication, HD Map, Testing, and Human Behaviors</b></summary>
  <p><b>编号</b>：[312]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11239</p>
  <p><b>作者</b>：Long Chen,  Yuchen Li,  Chao Huang,  Yang Xing,  Daxin Tian,  Li Li,  Zhongxu Hu,  Siyu Teng,  Chen Lv,  Jinjun Wang,  Dongpu Cao,  Nanning Zheng,  Fei-Yue Wang</p>
  <p><b>备注</b>：18 pages, 4 figures, 3 tables</p>
  <p><b>关键词</b>：rapid pace due, Interest in autonomous, autonomous driving, intelligent vehicles, economic benefits</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Interest in autonomous driving (AD) and intelligent vehicles (IVs) is growing
at a rapid pace due to the convenience, safety, and economic benefits. Although
a number of surveys have reviewed research achievements in this field, they are
still limited in specific tasks and lack systematic summaries and research
directions in the future. Our work is divided into 3 independent articles and
the first part is a Survey of Surveys (SoS) for total technologies of AD and
IVs that involves the history, summarizes the milestones, and provides the
perspectives, ethics, and future research directions. This is the second part
(Part \uppercase\expandafter{\romannumeral1} for this technical survey) to
review the development of control, computing system design, communication, High
Definition map (HD map), testing, and human behaviors in IVs. In addition, the
third part (Part \uppercase\expandafter{\romannumeral2} for this technical
survey) is to review the perception and planning sections. The objective of
this paper is to involve all the sections of AD, summarize the latest technical
milestones, and guide abecedarians to quickly understand the development of AD
and IVs. Combining the SoS and Part \uppercase\expandafter{\romannumeral2}, we
anticipate that this work will bring novel and diverse insights to researchers
and abecedarians, and serve as a bridge between past and future.</p>
  </details>
</details>
<details>
  <summary>89. <b>标题：Efficient Vertical Federated Learning with Secure Aggregation</b></summary>
  <p><b>编号</b>：[313]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11236</p>
  <p><b>作者</b>：Xinchi Qiu,  Heng Pan,  Wanru Zhao,  Chenyang Ma,  Pedro Porto Buarque de Gusmão,  Nicholas D. Lane</p>
  <p><b>备注</b>：Federated Learning Systems (FLSys) Workshop @ MLSys 2023</p>
  <p><b>关键词</b>：complete models independently, horizontally partitioned datasets, train complete models, privacy-preserving federated learning, federated learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The majority of work in privacy-preserving federated learning (FL) has been
focusing on horizontally partitioned datasets where clients share the same sets
of features and can train complete models independently. However, in many
interesting problems, such as financial fraud detection and disease detection,
individual data points are scattered across different clients/organizations in
vertical federated learning. Solutions for this type of FL require the exchange
of gradients between participants and rarely consider privacy and security
concerns, posing a potential risk of privacy leakage. In this work, we present
a novel design for training vertical FL securely and efficiently using
state-of-the-art security modules for secure aggregation. We demonstrate
empirically that our method does not impact training performance whilst
obtaining 9.1e2 ~3.8e4 speedup compared to homomorphic encryption (HE).</p>
  </details>
</details>
<details>
  <summary>90. <b>标题：LIMA: Less Is More for Alignment</b></summary>
  <p><b>编号</b>：[318]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11206</p>
  <p><b>作者</b>：Chunting Zhou,  Pengfei Liu,  Puxin Xu,  Srini Iyer,  Jiao Sun,  Yuning Mao,  Xuezhe Ma,  Avia Efrat,  Ping Yu,  Lili Yu,  Susan Zhang,  Gargi Ghosh,  Mike Lewis,  Luke Zettlemoyer,  Omer Levy</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：learn general-purpose representations, Large language models, raw text, general-purpose representations, reinforcement learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models are trained in two stages: (1) unsupervised pretraining
from raw text, to learn general-purpose representations, and (2) large scale
instruction tuning and reinforcement learning, to better align to end tasks and
user preferences. We measure the relative importance of these two stages by
training LIMA, a 65B parameter LLaMa language model fine-tuned with the
standard supervised loss on only 1,000 carefully curated prompts and responses,
without any reinforcement learning or human preference modeling. LIMA
demonstrates remarkably strong performance, learning to follow specific
response formats from only a handful of examples in the training data,
including complex queries that range from planning trip itineraries to
speculating about alternate history. Moreover, the model tends to generalize
well to unseen tasks that did not appear in the training data. In a controlled
human study, responses from LIMA are either equivalent or strictly preferred to
GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard
and 65% versus DaVinci003, which was trained with human feedback. Taken
together, these results strongly suggest that almost all knowledge in large
language models is learned during pretraining, and only limited instruction
tuning data is necessary to teach models to produce high quality output.</p>
  </details>
</details>
<details>
  <summary>91. <b>标题：PDP: Parameter-free Differentiable Pruning is All You Need</b></summary>
  <p><b>编号</b>：[319]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11203</p>
  <p><b>作者</b>：Minsik Cho,  Saurabh Adya,  Devang Naik</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：PDP, DNN accelerators, pruning, DNN, DNN pruning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>DNN pruning is a popular way to reduce the size of a model, improve the
inference latency, and minimize the power consumption on DNN accelerators.
However, existing approaches might be too complex, expensive or ineffective to
apply to a variety of vision/language tasks, DNN architectures and to honor
structured pruning constraints. In this paper, we propose an efficient yet
effective train-time pruning scheme, Parameter-free Differentiable Pruning
(PDP), which offers state-of-the-art qualities in model size, accuracy, and
training cost. PDP uses a dynamic function of weights during training to
generate soft pruning masks for the weights in a parameter-free manner for a
given pruning target. While differentiable, the simplicity and efficiency of
PDP make it universal enough to deliver state-of-the-art
random/structured/channel pruning results on various vision and natural
language tasks. For example, for MobileNet-v1, PDP can achieve 68.2% top-1
ImageNet1k accuracy at 86.6% sparsity, which is 1.7% higher accuracy than those
from the state-of-the-art algorithms. Also, PDP yields over 83.1% accuracy on
Multi-Genre Natural Language Inference with 90% sparsity for BERT, while the
next best from the existing techniques shows 81.5% accuracy. In addition, PDP
can be applied to structured pruning, such as N:M pruning and channel pruning.
For 1:4 structured pruning of ResNet18, PDP improved the top-1 ImageNet1k
accuracy by over 3.6% over the state-of-the-art. For channel pruning of
ResNet50, PDP reduced the top-1 ImageNet1k accuracy by 0.6% from the
state-of-the-art.</p>
  </details>
</details>
<details>
  <summary>92. <b>标题：Prediction with Incomplete Data under Agnostic Mask Distribution Shift</b></summary>
  <p><b>编号</b>：[321]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11197</p>
  <p><b>作者</b>：Yichen Zhu,  Jian Yuan,  Bo Jiang,  Tao Lin,  Haiming Jin,  Xinbing Wang,  Chenghu Zhou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：missing pattern, incomplete data, incomplete data consisting, mask, missing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Data with missing values is ubiquitous in many applications. Recent years
have witnessed increasing attention on prediction with only incomplete data
consisting of observed features and a mask that indicates the missing pattern.
Existing methods assume that the training and testing distributions are the
same, which may be violated in real-world scenarios. In this paper, we consider
prediction with incomplete data in the presence of distribution shift. We focus
on the case where the underlying joint distribution of complete features and
label is invariant, but the missing pattern, i.e., mask distribution may shift
agnostically between training and testing. To achieve generalization, we
leverage the observation that for each mask, there is an invariant optimal
predictor. To avoid the exponential explosion when learning them separately, we
approximate the optimal predictors jointly using a double parameterization
technique. This has the undesirable side effect of allowing the learned
predictors to rely on the intra-mask correlation and that between features and
mask. We perform decorrelation to minimize this effect. Combining the
techniques above, we propose a novel prediction method called StableMiss.
Extensive experiments on both synthetic and real-world datasets show that
StableMiss is robust and outperforms state-of-the-art methods under agnostic
mask distribution shift.</p>
  </details>
</details>
<details>
  <summary>93. <b>标题：Taxonomy of AISecOps Threat Modeling for Cloud Based Medical Chatbots</b></summary>
  <p><b>编号</b>：[326]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11189</p>
  <p><b>作者</b>：Ruby Annette J,  Aisha Banu,  Sharon Priya S,  Subash Chandran</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：technology including cyber, Artificial Intelligence, medical chatbots, playing a vital, vital role</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Artificial Intelligence (AI) is playing a vital role in all aspects of
technology including cyber security. Application of Conversational AI like the
chatbots are also becoming very popular in the medical field to provide timely
and immediate medical assistance to patients in need. As medical chatbots deal
with a lot of sensitive information, the security of these chatbots is crucial.
To secure the confidentiality, integrity, and availability of cloud-hosted
assets like these, medical chatbots can be monitored using AISecOps (Artificial
Intelligence for Secure IT Operations). AISecOPs is an emerging field that
integrates three different but interrelated domains like the IT operation, AI,
and security as one domain, where the expertise from all these three domains
are used cohesively to secure the cyber assets. It considers cloud operations
and security in a holistic framework to collect the metrics required to assess
the security threats and train the AI models to take immediate actions. This
work is focused on applying the STRIDE threat modeling framework to model the
possible threats involved in each component of the chatbot to enable the
automatic threat detection using the AISecOps techniques. This threat modeling
framework is tailored to the medical chatbots that involves sensitive data
sharing but could also be applied for chatbots used in other sectors like the
financial services, public sector, and government sectors that are concerned
with security and compliance.</p>
  </details>
</details>
<details>
  <summary>94. <b>标题：Artificial intelligence moral agent as Adam Smith's impartial spectator</b></summary>
  <p><b>编号</b>：[351]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11519</p>
  <p><b>作者</b>：Nikodem Tomczak</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Adam Smith developed, Adam Smith, Smith developed, developed a version, decisions are made</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Adam Smith developed a version of moral philosophy where better decisions are
made by interrogating an impartial spectator within us. We discuss the
possibility of using an external non-human-based substitute tool that would
augment our internal mental processes and play the role of the impartial
spectator. Such tool would have more knowledge about the world, be more
impartial, and would provide a more encompassing perspective on moral
assessment.</p>
  </details>
</details>
<details>
  <summary>95. <b>标题：Syllable Discovery and Cross-Lingual Generalization in a Visually  Grounded, Self-Supervised Speech Mode</b></summary>
  <p><b>编号</b>：[355]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11435</p>
  <p><b>作者</b>：Puyuan Peng,  Shang-Wen Li,  Okko Räsänen,  Abdelrahman Mohamed,  David Harwath</p>
  <p><b>备注</b>：Interspeech 2023. Code & Model: this https URL</p>
  <p><b>关键词</b>：visually-grounded training objective, representations capturing syllabic, capturing syllabic units, syllabic units emerge, visually-grounded training</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we show that representations capturing syllabic units emerge
when training a self-supervised speech model with a visually-grounded training
objective. We demonstrate that a nearly identical model architecture (HuBERT)
trained with a masked language modeling loss does not exhibit this same
ability, suggesting that the visual grounding objective is responsible for the
emergence of this phenomenon. We propose the use of a minimum cut algorithm to
automatically predict syllable boundaries in speech, followed by a 2-stage
clustering method to group identical syllables together. We show that our model
not only outperforms a state-of-the-art syllabic segmentation method on the
language it was trained on (English), but also generalizes in a zero-shot
fashion to Estonian. Finally, we show that the same model is capable of
zero-shot generalization for a word segmentation task on 4 other languages from
the Zerospeech Challenge, in some cases beating the previous state-of-the-art.</p>
  </details>
</details>
<details>
  <summary>96. <b>标题：Meta-learning for heterogeneous treatment effect estimation with  closed-form solvers</b></summary>
  <p><b>编号</b>：[357]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11353</p>
  <p><b>作者</b>：Tomoharu Iwata,  Yoichi Chikahara</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：average treatment effect, conditional average treatment, CATE estimation, treatment effect, article proposes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This article proposes a meta-learning method for estimating the conditional
average treatment effect (CATE) from a few observational data. The proposed
method learns how to estimate CATEs from multiple tasks and uses the knowledge
for unseen tasks. In the proposed method, based on the meta-learner framework,
we decompose the CATE estimation problem into sub-problems. For each
sub-problem, we formulate our estimation models using neural networks with
task-shared and task-specific parameters. With our formulation, we can obtain
optimal task-specific parameters in a closed form that are differentiable with
respect to task-shared parameters, making it possible to perform effective
meta-learning. The task-shared parameters are trained such that the expected
CATE estimation performance in few-shot settings is improved by minimizing the
difference between a CATE estimated with a large amount of data and one
estimated with just a few data. Our experimental results demonstrate that our
method outperforms the existing meta-learning approaches and CATE estimation
methods.</p>
  </details>
</details>
<details>
  <summary>97. <b>标题：Federated learning for secure development of AI models for Parkinson's  disease detection using speech from different languages</b></summary>
  <p><b>编号</b>：[362]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11284</p>
  <p><b>作者</b>：Soroosh Tayebi Arasteh,  Cristian David Rios-Urrego,  Elmar Noeth,  Andreas Maier,  Seung Hee Yang,  Jan Rusz,  Juan Rafael Orozco-Arroyave</p>
  <p><b>备注</b>：Accepted for INTERSPEECH 2023</p>
  <p><b>关键词</b>：neurological disorder impacting, Parkinson disease, neurological disorder, disorder impacting, impacting a person</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Parkinson's disease (PD) is a neurological disorder impacting a person's
speech. Among automatic PD assessment methods, deep learning models have gained
particular interest. Recently, the community has explored cross-pathology and
cross-language models which can improve diagnostic accuracy even further.
However, strict patient data privacy regulations largely prevent institutions
from sharing patient speech data with each other. In this paper, we employ
federated learning (FL) for PD detection using speech signals from 3 real-world
language corpora of German, Spanish, and Czech, each from a separate
institution. Our results indicate that the FL model outperforms all the local
models in terms of diagnostic accuracy, while not performing very differently
from the model based on centrally combined training sets, with the advantage of
not requiring any data sharing among collaborators. This will simplify
inter-institutional collaborations, resulting in enhancement of patient
outcomes.</p>
  </details>
</details>
<details>
  <summary>98. <b>标题：Explaining V1 Properties with a Biologically Constrained Deep Learning  Architecture</b></summary>
  <p><b>编号</b>：[364]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11275</p>
  <p><b>作者</b>：Galen Pogoncheff,  Jacob Granley,  Michael Beyeler</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ventral visual stream, Convolutional neural networks, recently emerged, emerged as promising, visual stream</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Convolutional neural networks (CNNs) have recently emerged as promising
models of the ventral visual stream, despite their lack of biological
specificity. While current state-of-the-art models of the primary visual cortex
(V1) have surfaced from training with adversarial examples and extensively
augmented data, these models are still unable to explain key neural properties
observed in V1 that arise from biological circuitry. To address this gap, we
systematically incorporated neuroscience-derived architectural components into
CNNs to identify a set of mechanisms and architectures that comprehensively
explain neural activity in V1. We show drastic improvements in model-V1
alignment driven by the integration of architectural components that simulate
center-surround antagonism, local receptive fields, tuned normalization, and
cortical magnification. Upon enhancing task-driven CNNs with a collection of
these specialized components, we uncover models with latent representations
that yield state-of-the-art explanation of V1 neural activity and tuning
properties. Our results highlight an important advancement in the field of
NeuroAI, as we systematically establish a set of architectural components that
contribute to unprecedented explanation of V1. The neuroscience insights that
could be gleaned from increasingly accurate in-silico models of the brain have
the potential to greatly advance the fields of both neuroscience and artificial
intelligence.</p>
  </details>
</details>
<details>
  <summary>99. <b>标题：Robust Quantum Controllers: Quantum Information -- Thermodynamic Hidden  Force Control in Intelligent Robotics based on Quantum Soft Computing</b></summary>
  <p><b>编号</b>：[365]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2305.11254</p>
  <p><b>作者</b>：Sergey V. Ulyanov,  Viktor S. Ulyanov,  Takakhide Hagiwara</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：soft computing technologies, intelligent robust control, robust control systems, control systems based, soft computing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A generalized strategy for the design of intelligent robust control systems
based on quantum / soft computing technologies is described. The reliability of
hybrid intelligent controllers increase by providing the ability to
self-organize of imperfect knowledge bases. The main attention is paid to
increasing the level of robustness of intelligent control systems in
unpredictable control situations with the demonstration by illustrative
examples. A SW & HW platform and support tools for a supercomputer accelerator
for modeling quantum algorithms on a classical computer are described.</p>
  </details>
</details>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">徐耀彬</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://louishsu.xyz/2023/05/22/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">http://louishsu.xyz/2023/05/22/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://louishsu.xyz" target="_blank">LOUIS' BLOG</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2023/05/07/%E3%80%90%E6%A2%B3%E7%90%86%E3%80%91%E9%99%86%E5%A5%87%E6%9C%80%E6%96%B0%E6%BC%94%E8%AE%B2%E5%AE%9E%E5%BD%95%EF%BC%9A%E6%88%91%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%96%E7%95%8C%E8%A7%82%20.html"><img class="next-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">【梳理】陆奇最新演讲实录：我的大模型世界观</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">徐耀彬</div><div class="author-info__description">专注于自然语言处理前沿技术与应用价值！</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">19</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/isLouisHsu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/isLouisHsu" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:is.louishsu@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">记录和分享一些学习和开源内容，若有问题可通过邮箱is.louishsu@foxmail.com联系，欢迎交流！！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">统计</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">计算机视觉</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">自然语言处理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">机器学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">人工智能</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/05/22/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2023-05-22)"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv每日速递(2023-05-22)"/></a><div class="content"><a class="title" href="/2023/05/22/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2023-05-22)">Arxiv每日速递(2023-05-22)</a><time datetime="2023-05-22T00:42:30.587Z" title="发表于 2023-05-22 08:42:30">2023-05-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/07/%E3%80%90%E6%A2%B3%E7%90%86%E3%80%91%E9%99%86%E5%A5%87%E6%9C%80%E6%96%B0%E6%BC%94%E8%AE%B2%E5%AE%9E%E5%BD%95%EF%BC%9A%E6%88%91%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%96%E7%95%8C%E8%A7%82%20.html" title="【梳理】陆奇最新演讲实录：我的大模型世界观"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【梳理】陆奇最新演讲实录：我的大模型世界观"/></a><div class="content"><a class="title" href="/2023/05/07/%E3%80%90%E6%A2%B3%E7%90%86%E3%80%91%E9%99%86%E5%A5%87%E6%9C%80%E6%96%B0%E6%BC%94%E8%AE%B2%E5%AE%9E%E5%BD%95%EF%BC%9A%E6%88%91%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%96%E7%95%8C%E8%A7%82%20.html" title="【梳理】陆奇最新演讲实录：我的大模型世界观">【梳理】陆奇最新演讲实录：我的大模型世界观</a><time datetime="2023-05-07T11:07:45.000Z" title="发表于 2023-05-07 19:07:45">2023-05-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/05/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8(Variational%20AutoEncoder).html" title="变分自编码器(Variational AutoEncoder)"><img src="https://lilianweng.github.io/posts/2018-08-12-vae/autoencoder-architecture.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="变分自编码器(Variational AutoEncoder)"/></a><div class="content"><a class="title" href="/2023/05/05/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8(Variational%20AutoEncoder).html" title="变分自编码器(Variational AutoEncoder)">变分自编码器(Variational AutoEncoder)</a><time datetime="2023-05-05T11:28:37.000Z" title="发表于 2023-05-05 19:28:37">2023-05-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/04/08/transformers.generation.GenerationMixin.html" title="transformers.generation.GenerationMixin"><img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="transformers.generation.GenerationMixin"/></a><div class="content"><a class="title" href="/2023/04/08/transformers.generation.GenerationMixin.html" title="transformers.generation.GenerationMixin">transformers.generation.GenerationMixin</a><time datetime="2023-04-08T13:42:45.000Z" title="发表于 2023-04-08 21:42:45">2023-04-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/27/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91ChatGPT%20%E6%A0%87%E6%B3%A8%E6%8C%87%E5%8D%97%EF%BC%9A%E4%BB%BB%E5%8A%A1%E3%80%81%E6%95%B0%E6%8D%AE%E4%B8%8E%E8%A7%84%E8%8C%83.html" title="【转载】ChatGPT 标注指南：任务、数据与规范"><img src="https://openaicom.imgix.net/8d14e8f0-e267-4b8b-a9f2-a79120808f5a/chatgpt.jpg?auto=compress%2Cformat&amp;fit=min&amp;fm=jpg&amp;q=80&amp;rect=0%2C0%2C2048%2C2048&amp;w=3200" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【转载】ChatGPT 标注指南：任务、数据与规范"/></a><div class="content"><a class="title" href="/2023/03/27/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91ChatGPT%20%E6%A0%87%E6%B3%A8%E6%8C%87%E5%8D%97%EF%BC%9A%E4%BB%BB%E5%8A%A1%E3%80%81%E6%95%B0%E6%8D%AE%E4%B8%8E%E8%A7%84%E8%8C%83.html" title="【转载】ChatGPT 标注指南：任务、数据与规范">【转载】ChatGPT 标注指南：任务、数据与规范</a><time datetime="2023-03-27T14:35:45.000Z" title="发表于 2023-03-27 22:35:45">2023-03-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2023 By 徐耀彬</div><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender"title="主题采用butterfly"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr"title="本站使用JsDelivr为静态资源提供CDN加速"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub"title="本站项目由Gtihub托管"alt="img"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"alt="img"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></br></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    let initData = {
      el: '#twikoo-wrap',
      envId: 'blog-',
      region: ''
    }

    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    twikoo.init(initData)
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'blog-',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/机器学习/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐱 机器学习 (3)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/自然语言处理/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 自然语言处理 (5)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/竞赛相关/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 竞赛相关 (3)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/阅读笔记/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 阅读笔记 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="http://louishsu.xyz/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style>
  <script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/hexo-github-calendar@1.21/hexo_githubcalendar.js"></script>
  <script data-pjax>
        function GithubCalendarConfig(){
            var git_githubapiurl ="https://python-github-calendar-api.vercel.app/api?isLouisHsu";
            var git_color =['#ebedf0', '#fdcdec', '#fc9bd9', '#fa6ac5', '#f838b2', '#f5089f', '#c4067e', '#92055e', '#540336', '#48022f', '#30021f'];
            var git_user ="isLouisHsu";
            var parent_div_git = document.getElementById('recent-posts');
            var git_div_html = '<div class="recent-post-item" style="width:100%;height:auto;padding:10px;"><div id="github_loading" style="width:10%;height:100%;margin:0 auto;display: block"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"  viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animateTransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animateTransform></path></svg></div><div id="github_container"></div></div>';
            if(parent_div_git && location.pathname =='/'){
                console.log('已挂载github calendar')
                // parent_div_git.innerHTML=git_div_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",git_div_html) // 有报错，但不影响使用(支持pjax跳转)
            };
            GithubCalendar(git_githubapiurl,git_color,git_user)
        }
        if(document.getElementById('recent-posts')){
            GithubCalendarConfig()
        }
    </script>
    <style>#github_container{min-height:280px}@media screen and (max-width:650px) {#github_container{background-image:;min-height:0px}}</style>
    <style></style><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>