<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Arxiv每日速递(2023-08-14) | LOUIS' BLOG</title><meta name="author" content="徐耀彬"><meta name="copyright" content="徐耀彬"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。 统计 今日共更新268篇论文，其中：  84篇计算机视觉（cs.CV） 22篇自然语言处理（cs.CL） 61篇机器学习（cs.LG） 57篇人工智能（cs.AI）  计算机视觉    1. 标题：Iterative Reweighted Least Squares Net">
<meta property="og:type" content="article">
<meta property="og:title" content="Arxiv每日速递(2023-08-14)">
<meta property="og:url" content="http://louishsu.xyz/2023/08/14/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">
<meta property="og:site_name" content="LOUIS&#39; BLOG">
<meta property="og:description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。 统计 今日共更新268篇论文，其中：  84篇计算机视觉（cs.CV） 22篇自然语言处理（cs.CL） 61篇机器学习（cs.LG） 57篇人工智能（cs.AI）  计算机视觉    1. 标题：Iterative Reweighted Least Squares Net">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png">
<meta property="article:published_time" content="2023-08-14T00:34:35.949Z">
<meta property="article:modified_time" content="2023-08-14T00:36:11.512Z">
<meta property="article:author" content="徐耀彬">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://louishsu.xyz/2023/08/14/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-08-14 08:36:11'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/background.css"><script src="https://cdn.jsdelivr.net/npm/echarts@4.7.0/dist/echarts.min.js"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">18</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-brands fa-app-store"></i><span> 利器</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://code.visualstudio.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> VSCode：微软旗下的跨平台代码编辑软件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mobaxterm.mobatek.net/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> MobaXterm：超好用的全能远程终端</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.zotero.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zotero：便于收集、组织、引用、共享的文献管理工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/CopyTranslator/CopyTranslator"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> CopyTranslator：“复制即翻译”的外文辅助阅读翻译解决方案</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ditto-cp.sourceforge.io/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Ditto：强大的Windows剪贴板增强工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/indiff/qttabbar"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> QtTabBar：在Windows资源管理器中使用多标签功能扩展工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/sentialx/multrin"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Multrin：“窗口合并”辅助小工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/Wox-launcher/Wox"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Wox &amp; Everything：基于名称快速定位文件和文件夹的搜索工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/NickeManarin/ScreenToGif"><i class="fa-fw fa-regular fa-star"></i><span> ScreenToGif：快速录制屏幕指定区域并保存为动图文件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mathpix.com/"><i class="fa-fw fa-regular fa-star"></i><span> Mathpix Snipping：识别数学公式并转换成LaTeX</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">LOUIS' BLOG</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-brands fa-app-store"></i><span> 利器</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://code.visualstudio.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> VSCode：微软旗下的跨平台代码编辑软件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mobaxterm.mobatek.net/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> MobaXterm：超好用的全能远程终端</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.zotero.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zotero：便于收集、组织、引用、共享的文献管理工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/CopyTranslator/CopyTranslator"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> CopyTranslator：“复制即翻译”的外文辅助阅读翻译解决方案</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ditto-cp.sourceforge.io/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Ditto：强大的Windows剪贴板增强工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/indiff/qttabbar"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> QtTabBar：在Windows资源管理器中使用多标签功能扩展工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/sentialx/multrin"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Multrin：“窗口合并”辅助小工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/Wox-launcher/Wox"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Wox &amp; Everything：基于名称快速定位文件和文件夹的搜索工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/NickeManarin/ScreenToGif"><i class="fa-fw fa-regular fa-star"></i><span> ScreenToGif：快速录制屏幕指定区域并保存为动图文件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mathpix.com/"><i class="fa-fw fa-regular fa-star"></i><span> Mathpix Snipping：识别数学公式并转换成LaTeX</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Arxiv每日速递(2023-08-14)<a class="post-edit-link" href="https://github.com/isLouisHsu/blog/tree/master/source_posts/Arxiv每日速递.md" title="编辑" target="_blank"><i class="fas fa-pencil-square"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-08-14T00:34:35.949Z" title="发表于 2023-08-14 08:34:35">2023-08-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-08-14T00:36:11.512Z" title="更新于 2023-08-14 08:36:11">2023-08-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">阅读笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">13.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>81分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2023/08/14/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。</p>
<h1>统计</h1>
<p>今日共更新268篇论文，其中：</p>
<ul>
<li><a href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89">84篇计算机视觉（cs.CV）</a></li>
<li><a href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86">22篇自然语言处理（cs.CL）</a></li>
<li><a href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">61篇机器学习（cs.LG）</a></li>
<li><a href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD">57篇人工智能（cs.AI）</a></li>
</ul>
<h1>计算机视觉</h1>
<details>
  <summary>1. <b>标题：Iterative Reweighted Least Squares Networks With Convergence Guarantees  for Solving Inverse Imaging Problems</b></summary>
  <p><b>编号</b>：[1]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05745</p>
  <p><b>作者</b>：Iaroslav Koshelev,  Stamatios Lefkimmiatis</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2304.10536</p>
  <p><b>关键词</b>：learned transform domain, transform domain, work we present, promotes sparse, low-rank solutions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work we present a novel optimization strategy for image
reconstruction tasks under analysis-based image regularization, which promotes
sparse and/or low-rank solutions in some learned transform domain. We
parameterize such regularizers using potential functions that correspond to
weighted extensions of the $\ell_p^p$-vector and $\mathcal{S}_p^p$
Schatten-matrix quasi-norms with $0 < p \le 1$. Our proposed minimization
strategy extends the Iteratively Reweighted Least Squares (IRLS) method,
typically used for synthesis-based $\ell_p$ and $\mathcal{S}_p$ norm and
analysis-based $\ell_1$ and nuclear norm regularization. We prove that under
mild conditions our minimization algorithm converges linearly to a stationary
point, and we provide an upper bound for its convergence rate. Further, to
select the parameters of the regularizers that deliver the best results for the
problem at hand, we propose to learn them from training data by formulating the
supervised learning process as a stochastic bilevel optimization problem. We
show that thanks to the convergence guarantees of our proposed minimization
strategy, such optimization can be successfully performed with a
memory-efficient implicit back-propagation scheme. We implement our learned
IRLS variants as recurrent networks and assess their performance on the
challenging image reconstruction tasks of non-blind deblurring,
super-resolution and demosaicking. The comparisons against other existing
learned reconstruction approaches demonstrate that our overall method is very
competitive and in many cases outperforms existing unrolled networks, whose
number of parameters is orders of magnitude higher than in our case.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：PlankAssembly: Robust 3D Reconstruction from Three Orthographic Views  with Learnt Shape Programs</b></summary>
  <p><b>编号</b>：[2]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05744</p>
  <p><b>作者</b>：Wentao Hu,  Jia Zheng,  Zixin Zhang,  Xiaojun Yuan,  Jian Yin,  Zihan Zhou</p>
  <p><b>备注</b>：To Appear in ICCV 2023. The first three authors contributed equally to this work. The project page is at this https URL</p>
  <p><b>关键词</b>：automatically convert, orthographic views, line drawings, input drawings created, input</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we develop a new method to automatically convert 2D line
drawings from three orthographic views into 3D CAD models. Existing methods for
this problem reconstruct 3D models by back-projecting the 2D observations into
3D space while maintaining explicit correspondence between the input and
output. Such methods are sensitive to errors and noises in the input, thus
often fail in practice where the input drawings created by human designers are
imperfect. To overcome this difficulty, we leverage the attention mechanism in
a Transformer-based sequence generation model to learn flexible mappings
between the input and output. Further, we design shape programs which are
suitable for generating the objects of interest to boost the reconstruction
accuracy and facilitate CAD modeling applications. Experiments on a new
benchmark dataset show that our method significantly outperforms existing ones
when the inputs are noisy or incomplete.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Neural Progressive Meshes</b></summary>
  <p><b>编号</b>：[3]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05741</p>
  <p><b>作者</b>：Yun-Chun Chen,  Vladimir G. Kim,  Noam Aigerman,  Alec Jacobson</p>
  <p><b>备注</b>：SIGGRAPH 2023</p>
  <p><b>关键词</b>：hand-held devices necessitates, devices necessitates efficient, necessitates efficient tools, recent proliferation, consumed on hand-held</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The recent proliferation of 3D content that can be consumed on hand-held
devices necessitates efficient tools for transmitting large geometric data,
e.g., 3D meshes, over the Internet. Detailed high-resolution assets can pose a
challenge to storage as well as transmission bandwidth, and level-of-detail
techniques are often used to transmit an asset using an appropriate bandwidth
budget. It is especially desirable for these methods to transmit data
progressively, improving the quality of the geometry with more data. Our key
insight is that the geometric details of 3D meshes often exhibit similar local
patterns even across different shapes, and thus can be effectively represented
with a shared learned generative space. We learn this space using a
subdivision-based encoder-decoder architecture trained in advance on a large
collection of surfaces. We further observe that additional residual features
can be transmitted progressively between intermediate levels of subdivision
that enable the client to control the tradeoff between bandwidth cost and
quality of reconstruction, providing a neural progressive mesh representation.
We evaluate our method on a diverse set of complex 3D shapes and demonstrate
that it outperforms baselines in terms of compression ratio and reconstruction
quality.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Zero Grads Ever Given: Learning Local Surrogate Losses for  Non-Differentiable Graphics</b></summary>
  <p><b>编号</b>：[4]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05739</p>
  <p><b>作者</b>：Michael Fischer,  Tobias Ritschel</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Gradient-based optimization, surrogate, objective, graphics, problems with undefined</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Gradient-based optimization is now ubiquitous across graphics, but
unfortunately can not be applied to problems with undefined or zero gradients.
To circumvent this issue, the loss function can be manually replaced by a
"surrogate" that has similar minima but is differentiable. Our proposed
framework, ZeroGrads, automates this process by learning a neural approximation
of the objective function, the surrogate, which in turn can be used to
differentiate through arbitrary black-box graphics pipelines. We train the
surrogate on an actively smoothed version of the objective and encourage
locality, focusing the surrogate's capacity on what matters at the current
training episode. The fitting is performed online, alongside the parameter
optimization, and self-supervised, without pre-computed data or pre-trained
models. As sampling the objective is expensive (it requires a full rendering or
simulator run), we devise an efficient sampling scheme that allows for
tractable run-times and competitive performance at little overhead. We
demonstrate optimizing diverse non-convex, non-differentiable black-box
problems in graphics, such as visibility in rendering, discrete parameter
spaces in procedural modelling or optimal control in physics-driven animation.
In contrast to more traditional algorithms, our approach scales well to higher
dimensions, which we demonstrate on problems with up to 35k interlinked
variables.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Follow Anything: Open-set detection, tracking, and following in  real-time</b></summary>
  <p><b>编号</b>：[5]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05737</p>
  <p><b>作者</b>：Alaa Maalouf,  Ninad Jadhav,  Krishna Murthy Jatavallabhula,  Makram Chahine,  Daniel M.Vogt,  Robert J. Wood,  Antonio Torralba,  Daniela Rus</p>
  <p><b>备注</b>：Project webpage: this https URL Explainer video: this https URL</p>
  <p><b>关键词</b>：ranging from industrial, logistics and warehousing, healthcare and security, industrial automation, automation to logistics</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Tracking and following objects of interest is critical to several robotics
use cases, ranging from industrial automation to logistics and warehousing, to
healthcare and security. In this paper, we present a robotic system to detect,
track, and follow any object in real-time. Our approach, dubbed ``follow
anything'' (FAn), is an open-vocabulary and multimodal model -- it is not
restricted to concepts seen at training time and can be applied to novel
classes at inference time using text, images, or click queries. Leveraging rich
visual descriptors from large-scale pre-trained models (foundation models), FAn
can detect and segment objects by matching multimodal queries (text, images,
clicks) against an input image sequence. These detected and segmented objects
are tracked across image frames, all while accounting for occlusion and object
re-emergence. We demonstrate FAn on a real-world robotic system (a micro aerial
vehicle) and report its ability to seamlessly follow the objects of interest in
a real-time control loop. FAn can be deployed on a laptop with a lightweight
(6-8 GB) graphics card, achieving a throughput of 6-20 frames per second. To
enable rapid adoption, deployment, and extensibility, we open-source all our
code on our project webpage at this https URL .
We also encourage the reader the watch our 5-minutes explainer video in this
this https URL .</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：MapTRv2: An End-to-End Framework for Online Vectorized HD Map  Construction</b></summary>
  <p><b>编号</b>：[6]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05736</p>
  <p><b>作者</b>：Bencheng Liao,  Shaoyu Chen,  Yunchi Zhang,  Bo Jiang,  Qian Zhang,  Wenyu Liu,  Chang Huang,  Xinggang Wang</p>
  <p><b>备注</b>：Code available at this https URL . arXiv admin note: substantial text overlap with arXiv:2208.14437</p>
  <p><b>关键词</b>：precise static environmental, static environmental information, autonomous driving system, map, precise static</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>High-definition (HD) map provides abundant and precise static environmental
information of the driving scene, serving as a fundamental and indispensable
component for planning in autonomous driving system. In this paper, we present
\textbf{Map} \textbf{TR}ansformer, an end-to-end framework for online
vectorized HD map construction. We propose a unified permutation-equivalent
modeling approach, \ie, modeling map element as a point set with a group of
equivalent permutations, which accurately describes the shape of map element
and stabilizes the learning process. We design a hierarchical query embedding
scheme to flexibly encode structured map information and perform hierarchical
bipartite matching for map element learning. To speed up convergence, we
further introduce auxiliary one-to-many matching and dense supervision. The
proposed method well copes with various map elements with arbitrary shapes. It
runs at real-time inference speed and achieves state-of-the-art performance on
both nuScenes and Argoverse2 datasets. Abundant qualitative results show stable
and robust map construction quality in complex and various driving scenes. Code
and more demos are available at \url{this https URL} for
facilitating further studies and applications.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：FrozenRecon: Pose-free 3D Scene Reconstruction with Frozen Depth Models</b></summary>
  <p><b>编号</b>：[8]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05733</p>
  <p><b>作者</b>：Guangkai Xu,  Wei Yin,  Hao Chen,  Chunhua Shen,  Kai Cheng,  Feng Zhao</p>
  <p><b>备注</b>：Accepted to ICCV 2023. Project webpage is at: this https URL</p>
  <p><b>关键词</b>：long-standing vision task, vision task, long-standing vision, depth, reconstruction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>3D scene reconstruction is a long-standing vision task. Existing approaches
can be categorized into geometry-based and learning-based methods. The former
leverages multi-view geometry but can face catastrophic failures due to the
reliance on accurate pixel correspondence across views. The latter was
proffered to mitigate these issues by learning 2D or 3D representation
directly. However, without a large-scale video or 3D training data, it can
hardly generalize to diverse real-world scenarios due to the presence of tens
of millions or even billions of optimization parameters in the deep network.
Recently, robust monocular depth estimation models trained with large-scale
datasets have been proven to possess weak 3D geometry prior, but they are
insufficient for reconstruction due to the unknown camera parameters, the
affine-invariant property, and inter-frame inconsistency. Here, we propose a
novel test-time optimization approach that can transfer the robustness of
affine-invariant depth models such as LeReS to challenging diverse scenes while
ensuring inter-frame consistency, with only dozens of parameters to optimize
per video frame. Specifically, our approach involves freezing the pre-trained
affine-invariant depth model's depth predictions, rectifying them by optimizing
the unknown scale-shift values with a geometric consistency alignment module,
and employing the resulting scale-consistent depth maps to robustly obtain
camera poses and achieve dense scene reconstruction, even in low-texture
regions. Experiments show that our method achieves state-of-the-art
cross-dataset reconstruction on five zero-shot testing datasets.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Rethinking Integration of Prediction and Planning in Deep Learning-Based  Automated Driving Systems: A Review</b></summary>
  <p><b>编号</b>：[10]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05731</p>
  <p><b>作者</b>：Steffen Hagedorn,  Marcel Hallgarten,  Martin Stoll,  Alexandru Condurache</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Automated driving, revolutionize personal, freight mobility, potential to revolutionize, automated driving comprises</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automated driving has the potential to revolutionize personal, public, and
freight mobility. Besides the enormous challenge of perception, i.e. accurately
perceiving the environment using available sensor data, automated driving
comprises planning a safe, comfortable, and efficient motion trajectory. To
promote safety and progress, many works rely on modules that predict the future
motion of surrounding traffic. Modular automated driving systems commonly
handle prediction and planning as sequential separate tasks. While this
accounts for the influence of surrounding traffic on the ego-vehicle, it fails
to anticipate the reactions of traffic participants to the ego-vehicle's
behavior. Recent works suggest that integrating prediction and planning in an
interdependent joint step is necessary to achieve safe, efficient, and
comfortable driving. While various models implement such integrated systems, a
comprehensive overview and theoretical understanding of different principles
are lacking. We systematically review state-of-the-art deep learning-based
prediction, planning, and integrated prediction and planning models. Different
facets of the integration ranging from model architecture and model design to
behavioral aspects are considered and related to each other. Moreover, we
discuss the implications, strengths, and limitations of different integration
methods. By pointing out research gaps, describing relevant future challenges,
and highlighting trends in the research field, we identify promising directions
for future research.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Deformable Mixer Transformer with Gating for Multi-Task Learning of  Dense Prediction</b></summary>
  <p><b>编号</b>：[13]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05721</p>
  <p><b>作者</b>：Yangyang Xu,  Yibo Yang,  Bernard Ghanemm,  Lefei Zhang</p>
  <p><b>备注</b>：submitted to IJCV; an extension to our previous AAAI 2023 paper arXiv:2301.03461</p>
  <p><b>关键词</b>：MTL, MTL solely rely, Transformer, dense prediction, deformable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>CNNs and Transformers have their own advantages and both have been widely
used for dense prediction in multi-task learning (MTL). Most of the current
studies on MTL solely rely on CNN or Transformer. In this work, we present a
novel MTL model by combining both merits of deformable CNN and query-based
Transformer with shared gating for multi-task learning of dense prediction.
This combination may offer a simple and efficient solution owing to its
powerful and flexible task-specific learning and advantages of lower cost, less
complexity and smaller parameters than the traditional MTL methods. We
introduce deformable mixer Transformer with gating (DeMTG), a simple and
effective encoder-decoder architecture up-to-date that incorporates the
convolution and attention mechanism in a unified network for MTL. It is
exquisitely designed to use advantages of each block, and provide deformable
and comprehensive features for all tasks from local and global perspective.
First, the deformable mixer encoder contains two types of operators: the
channel-aware mixing operator leveraged to allow communication among different
channels, and the spatial-aware deformable operator with deformable convolution
applied to efficiently sample more informative spatial locations. Second, the
task-aware gating transformer decoder is used to perform the task-specific
predictions, in which task interaction block integrated with self-attention is
applied to capture task interaction features, and the task query block
integrated with gating attention is leveraged to select corresponding
task-specific features. Further, the experiment results demonstrate that the
proposed DeMTG uses fewer GFLOPs and significantly outperforms current
Transformer-based and CNN-based competitive models on a variety of metrics on
three dense prediction datasets. Our code and models are available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Shadow Datasets, New challenging datasets for Causal Representation  Learning</b></summary>
  <p><b>编号</b>：[17]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05707</p>
  <p><b>作者</b>：Jiageng Zhu,  Hanchen Xie,  Jianhua Wu,  Jiazhi Li,  Mahyar Khayatkhoei,  Mohamed E. Hussein,  Wael AbdAlmageed</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：representation learning, Discovering causal relations, CelebA, relations among semantic, emergent topic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Discovering causal relations among semantic factors is an emergent topic in
representation learning. Most causal representation learning (CRL) methods are
fully supervised, which is impractical due to costly labeling. To resolve this
restriction, weakly supervised CRL methods were introduced. To evaluate CRL
performance, four existing datasets, Pendulum, Flow, CelebA(BEARD) and
CelebA(SMILE), are utilized. However, existing CRL datasets are limited to
simple graphs with few generative factors. Thus we propose two new datasets
with a larger number of diverse generative factors and more sophisticated
causal graphs. In addition, current real datasets, CelebA(BEARD) and
CelebA(SMILE), the originally proposed causal graphs are not aligned with the
dataset distributions. Thus, we propose modifications to them.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Masked Diffusion as Self-supervised Representation Learner</b></summary>
  <p><b>编号</b>：[23]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05695</p>
  <p><b>作者</b>：Zixuan Pan,  Jianxu Chen,  Yiyu Shi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Denoising diffusion probabilistic, strong pixel-level representation, recently demonstrated, pixel-level representation learners, strong pixel-level</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Denoising diffusion probabilistic models have recently demonstrated
state-of-the-art generative performance and been used as strong pixel-level
representation learners. This paper decomposes the interrelation between the
generative capability and representation learning ability inherent in diffusion
models. We present masked diffusion model (MDM), a scalable self-supervised
representation learner that substitutes the conventional additive Gaussian
noise of traditional diffusion with a masking mechanism. Our proposed approach
convincingly surpasses prior benchmarks, demonstrating remarkable advancements
in both medical and natural image semantic segmentation tasks, particularly
within the context of few-shot scenario.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Hard No-Box Adversarial Attack on Skeleton-Based Human Action  Recognition with Skeleton-Motion-Informed Gradient</b></summary>
  <p><b>编号</b>：[27]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05681</p>
  <p><b>作者</b>：Zhengzhi Lu,  He Wang,  Ziyi Chang,  Guoan Yang,  Hubert P. H. Shum</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：skeleton-based human activity, human activity recognition, attack methods, skeleton-based human, human activity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, methods for skeleton-based human activity recognition have been
shown to be vulnerable to adversarial attacks. However, these attack methods
require either the full knowledge of the victim (i.e. white-box attacks),
access to training data (i.e. transfer-based attacks) or frequent model queries
(i.e. black-box attacks). All their requirements are highly restrictive,
raising the question of how detrimental the vulnerability is. In this paper, we
show that the vulnerability indeed exists. To this end, we consider a new
attack task: the attacker has no access to the victim model or the training
data or labels, where we coin the term hard no-box attack. Specifically, we
first learn a motion manifold where we define an adversarial loss to compute a
new gradient for the attack, named skeleton-motion-informed (SMI) gradient. Our
gradient contains information of the motion dynamics, which is different from
existing gradient-based attack methods that compute the loss gradient assuming
each dimension in the data is independent. The SMI gradient can augment many
gradient-based attack methods, leading to a new family of no-box attack
methods. Extensive evaluation and comparison show that our method imposes a
real threat to existing classifiers. They also show that the SMI gradient
improves the transferability and imperceptibility of adversarial samples in
both no-box and transfer-based black-box settings.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：2D3D-MATR: 2D-3D Matching Transformer for Detection-free Registration  between Images and Point Clouds</b></summary>
  <p><b>编号</b>：[30]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05667</p>
  <p><b>作者</b>：Minhao Li,  Zheng Qin,  Zhirui Gao,  Renjiao Yi,  Chengyang Zhu,  Kai Xu</p>
  <p><b>备注</b>：Accepted by ICCV 2023</p>
  <p><b>关键词</b>：inconsistent feature description, incompatible keypoint detection, cross-modality cases due, commonly adopted, feature description</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The commonly adopted detect-then-match approach to registration finds
difficulties in the cross-modality cases due to the incompatible keypoint
detection and inconsistent feature description. We propose, 2D3D-MATR, a
detection-free method for accurate and robust registration between images and
point clouds. Our method adopts a coarse-to-fine pipeline where it first
computes coarse correspondences between downsampled patches of the input image
and the point cloud and then extends them to form dense correspondences between
pixels and points within the patch region. The coarse-level patch matching is
based on transformer which jointly learns global contextual constraints with
self-attention and cross-modality correlations with cross-attention. To resolve
the scale ambiguity in patch matching, we construct a multi-scale pyramid for
each image patch and learn to find for each point patch the best matching image
patch at a proper resolution level. Extensive experiments on two public
benchmarks demonstrate that 2D3D-MATR outperforms the previous state-of-the-art
P2-Net by around $20$ percentage points on inlier ratio and over $10$ points on
registration recall. Our code and models are available at
\url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：AD-CLIP: Adapting Domains in Prompt Space Using CLIP</b></summary>
  <p><b>编号</b>：[32]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05659</p>
  <p><b>作者</b>：Mainak Singha,  Harsh Pal,  Ankit Jha,  Biplab Banerjee</p>
  <p><b>备注</b>：10 pages, 8 figures, 4 tables. Accepted at OOD-CV, ICCV Workshop, 2023</p>
  <p><b>关键词</b>：shown impressive performance, shown impressive, impressive performance, struggle to generalize, deep learning models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Although deep learning models have shown impressive performance on supervised
learning tasks, they often struggle to generalize well when the training
(source) and test (target) domains differ. Unsupervised domain adaptation (DA)
has emerged as a popular solution to this problem. However, current DA
techniques rely on visual backbones, which may lack semantic richness. Despite
the potential of large-scale vision-language foundation models like CLIP, their
effectiveness for DA has yet to be fully explored. To address this gap, we
introduce AD-CLIP, a domain-agnostic prompt learning strategy for CLIP that
aims to solve the DA problem in the prompt space. We leverage the frozen vision
backbone of CLIP to extract both image style (domain) and content information,
which we apply to learn prompt tokens. Our prompts are designed to be
domain-invariant and class-generalizable, by conditioning prompt learning on
image style and content features simultaneously. We use standard supervised
contrastive learning in the source domain, while proposing an entropy
minimization strategy to align domains in the embedding space given the target
domain data. We also consider a scenario where only target domain samples are
available during testing, without any source domain data, and propose a
cross-domain style mapping network to hallucinate domain-agnostic tokens. Our
extensive experiments on three benchmark DA datasets demonstrate the
effectiveness of AD-CLIP compared to existing literature.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Counterfactual Cross-modality Reasoning for Weakly Supervised Video  Moment Localization</b></summary>
  <p><b>编号</b>：[38]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05648</p>
  <p><b>作者</b>：Zezhong Lv,  Bing Su,  Ji-Rong Wen</p>
  <p><b>备注</b>：Accepted by ACM MM 2023</p>
  <p><b>关键词</b>：moment localization aims, Video moment localization, target segment, natural language query, moment localization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Video moment localization aims to retrieve the target segment of an untrimmed
video according to the natural language query. Weakly supervised methods gains
attention recently, as the precise temporal location of the target segment is
not always available. However, one of the greatest challenges encountered by
the weakly supervised method is implied in the mismatch between the video and
language induced by the coarse temporal annotations. To refine the
vision-language alignment, recent works contrast the cross-modality
similarities driven by reconstructing masked queries between positive and
negative video proposals. However, the reconstruction may be influenced by the
latent spurious correlation between the unmasked and the masked parts, which
distorts the restoring process and further degrades the efficacy of contrastive
learning since the masked words are not completely reconstructed from the
cross-modality knowledge. In this paper, we discover and mitigate this spurious
correlation through a novel proposed counterfactual cross-modality reasoning
method. Specifically, we first formulate query reconstruction as an aggregated
causal effect of cross-modality and query knowledge. Then by introducing
counterfactual cross-modality knowledge into this aggregation, the spurious
impact of the unmasked part contributing to the reconstruction is explicitly
modeled. Finally, by suppressing the unimodal effect of masked query, we can
rectify the reconstructions of video proposals to perform reasonable
contrastive learning. Extensive experimental evaluations demonstrate the
effectiveness of our proposed method. The code is available at
\href{this https URL}{this https URL}.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：IIHT: Medical Report Generation with Image-to-Indicator Hierarchical  Transformer</b></summary>
  <p><b>编号</b>：[45]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05633</p>
  <p><b>作者</b>：Keqiang Fan,  Xiaohao Cai,  Mahesan Niranjan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：medical report generation, Automated medical report, medical report, report generation, increasingly important</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automated medical report generation has become increasingly important in
medical analysis. It can produce computer-aided diagnosis descriptions and thus
significantly alleviate the doctors' work. Inspired by the huge success of
neural machine translation and image captioning, various deep learning methods
have been proposed for medical report generation. However, due to the inherent
properties of medical data, including data imbalance and the length and
correlation between report sequences, the generated reports by existing methods
may exhibit linguistic fluency but lack adequate clinical accuracy. In this
work, we propose an image-to-indicator hierarchical transformer (IIHT)
framework for medical report generation. It consists of three modules, i.e., a
classifier module, an indicator expansion module and a generator module. The
classifier module first extracts image features from the input medical images
and produces disease-related indicators with their corresponding states. The
disease-related indicators are subsequently utilised as input for the indicator
expansion module, incorporating the "data-text-data" strategy. The
transformer-based generator then leverages these extracted features along with
image features as auxiliary information to generate final reports. Furthermore,
the proposed IIHT method is feasible for radiologists to modify disease
indicators in real-world scenarios and integrate the operations into the
indicator expansion module for fluent and accurate medical report generation.
Extensive experiments and comparisons with state-of-the-art methods under
various evaluation metrics demonstrate the great performance of the proposed
method.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Self-Supervised Monocular Depth Estimation by Direction-aware Cumulative  Convolution Network</b></summary>
  <p><b>编号</b>：[53]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05605</p>
  <p><b>作者</b>：Wencheng Han,  Junbo Yin,  Jianbing Shen</p>
  <p><b>备注</b>：ICCV2023</p>
  <p><b>关键词</b>：Monocular depth estimation, ill-posed task, Cumulative Convolution Network, depth, Monocular depth</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Monocular depth estimation is known as an ill-posed task in which objects in
a 2D image usually do not contain sufficient information to predict their
depth. Thus, it acts differently from other tasks (e.g., classification and
segmentation) in many ways. In this paper, we find that self-supervised
monocular depth estimation shows a direction sensitivity and environmental
dependency in the feature representation. But the current backbones borrowed
from other tasks pay less attention to handling different types of
environmental information, limiting the overall depth accuracy. To bridge this
gap, we propose a new Direction-aware Cumulative Convolution Network (DaCCN),
which improves the depth feature representation in two aspects. First, we
propose a direction-aware module, which can learn to adjust the feature
extraction in each direction, facilitating the encoding of different types of
information. Secondly, we design a new cumulative convolution to improve the
efficiency for aggregating important environmental information. Experiments
show that our method achieves significant improvements on three widely used
benchmarks, KITTI, Cityscapes, and Make3D, setting a new state-of-the-art
performance on the popular benchmarks with all three types of self-supervision.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Object Goal Navigation with Recursive Implicit Maps</b></summary>
  <p><b>编号</b>：[54]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05602</p>
  <p><b>作者</b>：Shizhe Chen,  Thomas Chabal,  Ivan Laptev,  Cordelia Schmid</p>
  <p><b>备注</b>：Accepted to IROS 2023</p>
  <p><b>关键词</b>：aims to navigate, navigate an agent, agent to locations, category in unseen, Object goal navigation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Object goal navigation aims to navigate an agent to locations of a given
object category in unseen environments. Classical methods explicitly build maps
of environments and require extensive engineering while lacking semantic
information for object-oriented exploration. On the other hand, end-to-end
learning methods alleviate manual map design and predict actions using implicit
representations. Such methods, however, lack an explicit notion of geometry and
may have limited ability to encode navigation history. In this work, we propose
an implicit spatial map for object goal navigation. Our implicit map is
recursively updated with new observations at each step using a transformer. To
encourage spatial reasoning, we introduce auxiliary tasks and train our model
to reconstruct explicit maps as well as to predict visual features, semantic
labels and actions. Our method significantly outperforms the state of the art
on the challenging MP3D dataset and generalizes well to the HM3D dataset. We
successfully deploy our model on a real robot and achieve encouraging object
goal navigation results in real scenes using only a few real-world
demonstrations. Code, trained models and videos are available at
\url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：NUPES : Non-Uniform Post-Training Quantization via Power Exponent Search</b></summary>
  <p><b>编号</b>：[56]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05600</p>
  <p><b>作者</b>：Edouard Yvinec,  Arnaud Dapogny,  Kevin Bailly</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：expensive computational requirements, larger hardware devices, hardware devices due, Deep neural network, neural network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep neural network (DNN) deployment has been confined to larger hardware
devices due to their expensive computational requirements. This challenge has
recently reached another scale with the emergence of large language models
(LLMs). In order to reduce both their memory footprint and latency, a promising
technique is quantization. It consists in converting floating point
representations to low bit-width fixed point representations, usually by
assuming a uniform mapping onto a regular grid. This process, referred to in
the literature as uniform quantization, may however be ill-suited as most DNN
weights and activations follow a bell-shaped distribution. This is even worse
on LLMs whose weight distributions are known to exhibit large, high impact,
outlier values. In this work, we propose an improvement over the most commonly
adopted way to tackle this limitation in deep learning models quantization,
namely, non-uniform quantization. NUPES leverages automorphisms to preserve the
scalar multiplications. Such transformations are derived from power functions.
However, the optimization of the exponent parameter and weight values remains a
challenging and novel problem which could not be solved with previous post
training optimization techniques which only learn to round up or down weight
values in order to preserve the predictive function. We circumvent this
limitation with a new paradigm: learning new quantized weights over the entire
quantized space. Similarly, we enable the optimization of the power exponent,
i.e. the optimization of the quantization operator itself during training by
alleviating all the numerical instabilities. The resulting predictive function
is compatible with integer-only low-bit inference. We show the ability of the
method to achieve state-of-the-art compression rates in both, data-free and
data-driven configurations.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Test-Time Selection for Robust Skin Lesion Analysis</b></summary>
  <p><b>编号</b>：[58]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05595</p>
  <p><b>作者</b>：Alceu Bissoto,  Catarina Barata,  Eduardo Valle,  Sandra Avila</p>
  <p><b>备注</b>：Accepted at ISIC Workshop @ MICCAI 2023</p>
  <p><b>关键词</b>：Skin lesion analysis, influence model predictions, clinical information, predictions despite carrying, carrying no clinical</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Skin lesion analysis models are biased by artifacts placed during image
acquisition, which influence model predictions despite carrying no clinical
information. Solutions that address this problem by regularizing models to
prevent learning those spurious features achieve only partial success, and
existing test-time debiasing techniques are inappropriate for skin lesion
analysis due to either making unrealistic assumptions on the distribution of
test data or requiring laborious annotation from medical practitioners. We
propose TTS (Test-Time Selection), a human-in-the-loop method that leverages
positive (e.g., lesion area) and negative (e.g., artifacts) keypoints in test
samples. TTS effectively steers models away from exploiting spurious
artifact-related correlations without retraining, and with less annotation
requirements. Our solution is robust to a varying availability of annotations,
and different levels of bias. We showcase on the ISIC2019 dataset (for which we
release a subset of annotated images) how our model could be deployed in the
real-world for mitigating bias.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Category Feature Transformer for Semantic Segmentation</b></summary>
  <p><b>编号</b>：[65]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05581</p>
  <p><b>作者</b>：Quan Tang,  Chuanjian Liu,  Fagui Liu,  Yifan Liu,  Jun Jiang,  Bowen Zhang,  Kai Han,  Yunhe Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：revealed to play, play a significant, significant role, CFT, proposed CFT</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Aggregation of multi-stage features has been revealed to play a significant
role in semantic segmentation. Unlike previous methods employing point-wise
summation or concatenation for feature aggregation, this study proposes the
Category Feature Transformer (CFT) that explores the flow of category embedding
and transformation among multi-stage features through the prevalent multi-head
attention mechanism. CFT learns unified feature embeddings for individual
semantic categories from high-level features during each aggregation process
and dynamically broadcasts them to high-resolution features. Integrating the
proposed CFT into a typical feature pyramid structure exhibits superior
performance over a broad range of backbone networks. We conduct extensive
experiments on popular semantic segmentation benchmarks. Specifically, the
proposed CFT obtains a compelling 55.1% mIoU with greatly reduced model
parameters and computations on the challenging ADE20K dataset.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Exploring Linguistic Similarity and Zero-Shot Learning for Multilingual  Translation of Dravidian Languages</b></summary>
  <p><b>编号</b>：[69]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05574</p>
  <p><b>作者</b>：Danish Ebadulla,  Rahul Raman,  S. Natarajan,  Hridhay Kiran Shetty,  Ashish Harish Shenoy</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：high compute requirements, compute requirements, high compute, increased training time, increased training</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Current research in zero-shot translation is plagued by several issues such
as high compute requirements, increased training time and off target
translations. Proposed remedies often come at the cost of additional data or
compute requirements. Pivot based neural machine translation is preferred over
a single-encoder model for most settings despite the increased training and
evaluation time. In this work, we overcome the shortcomings of zero-shot
translation by taking advantage of transliteration and linguistic similarity.
We build a single encoder-decoder neural machine translation system for
Dravidian-Dravidian multilingual translation and perform zero-shot translation.
We compare the data vs zero-shot accuracy tradeoff and evaluate the performance
of our vanilla method against the current state of the art pivot based method.
We also test the theory that morphologically rich languages require large
vocabularies by restricting the vocabulary using an optimal transport based
technique. Our model manages to achieves scores within 3 BLEU of large-scale
pivot-based models when it is trained on 50\% of the language directions.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Cross-Domain Product Representation Learning for Rich-Content E-Commerce</b></summary>
  <p><b>编号</b>：[77]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05550</p>
  <p><b>作者</b>：Xuehan Bai,  Yan Li,  Yanhua Cheng,  Wenjie Yang,  Quan Chen,  Han Li</p>
  <p><b>备注</b>：ICCV23</p>
  <p><b>关键词</b>：product, short videos, cross-domain product, product representation, live streams</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The proliferation of short video and live-streaming platforms has
revolutionized how consumers engage in online shopping. Instead of browsing
product pages, consumers are now turning to rich-content e-commerce, where they
can purchase products through dynamic and interactive media like short videos
and live streams. This emerging form of online shopping has introduced
technical challenges, as products may be presented differently across various
media domains. Therefore, a unified product representation is essential for
achieving cross-domain product recognition to ensure an optimal user search
experience and effective product recommendations. Despite the urgent industrial
need for a unified cross-domain product representation, previous studies have
predominantly focused only on product pages without taking into account short
videos and live streams. To fill the gap in the rich-content e-commerce area,
in this paper, we introduce a large-scale cRoss-dOmain Product Ecognition
dataset, called ROPE. ROPE covers a wide range of product categories and
contains over 180,000 products, corresponding to millions of short videos and
live streams. It is the first dataset to cover product pages, short videos, and
live streams simultaneously, providing the basis for establishing a unified
product representation across different media domains. Furthermore, we propose
a Cross-dOmain Product rEpresentation framework, namely COPE, which unifies
product representations in different domains through multimodal learning
including text and vision. Extensive experiments on downstream tasks
demonstrate the effectiveness of COPE in learning a joint feature space for all
product domains.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Deep Richardson-Lucy Deconvolution for Low-Light Image Deblurring</b></summary>
  <p><b>编号</b>：[81]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05543</p>
  <p><b>作者</b>：Liang Chen,  Jiawei Zhang,  Zhenhua Li,  Yunxuan Wei,  Faming Fang,  Jimmy Ren,  Jinshan Pan</p>
  <p><b>备注</b>：Accepted by IJCV</p>
  <p><b>关键词</b>：saturated pixels, low-light condition, latent map, saturated, pixels</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Images taken under the low-light condition often contain blur and saturated
pixels at the same time. Deblurring images with saturated pixels is quite
challenging. Because of the limited dynamic range, the saturated pixels are
usually clipped in the imaging process and thus cannot be modeled by the linear
blur model. Previous methods use manually designed smooth functions to
approximate the clipping procedure. Their deblurring processes often require
empirically defined parameters, which may not be the optimal choices for
different images. In this paper, we develop a data-driven approach to model the
saturated pixels by a learned latent map. Based on the new model, the non-blind
deblurring task can be formulated into a maximum a posterior (MAP) problem,
which can be effectively solved by iteratively computing the latent map and the
latent image. Specifically, the latent map is computed by learning from a map
estimation network (MEN), and the latent image estimation process is
implemented by a Richardson-Lucy (RL)-based updating scheme. To estimate
high-quality deblurred images without amplified artifacts, we develop a prior
estimation network (PEN) to obtain prior information, which is further
integrated into the RL scheme. Experimental results demonstrate that the
proposed method performs favorably against state-of-the-art algorithms both
quantitatively and qualitatively on synthetic and real-world images.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Robust Asymmetric Loss for Multi-Label Long-Tailed Learning</b></summary>
  <p><b>编号</b>：[82]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05542</p>
  <p><b>作者</b>：Wongi Park,  Inhyuk Park,  Sungeun Kim,  Jongbin Ryu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：samples typically show, typically show long-tailed, show long-tailed distributions, real medical data, training samples typically</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In real medical data, training samples typically show long-tailed
distributions with multiple labels. Class distribution of the medical data has
a long-tailed shape, in which the incidence of different diseases is quite
varied, and at the same time, it is not unusual for images taken from
symptomatic patients to be multi-label diseases. Therefore, in this paper, we
concurrently address these two issues by putting forth a robust asymmetric loss
on the polynomial function. Since our loss tackles both long-tailed and
multi-label classification problems simultaneously, it leads to a complex
design of the loss function with a large number of hyper-parameters. Although a
model can be highly fine-tuned due to a large number of hyper-parameters, it is
difficult to optimize all hyper-parameters at the same time, and there might be
a risk of overfitting a model. Therefore, we regularize the loss function using
the Hill loss approach, which is beneficial to be less sensitive against the
numerous hyper-parameters so that it reduces the risk of overfitting the model.
For this reason, the proposed loss is a generic method that can be applied to
most medical image classification tasks and does not make the training process
more time-consuming. We demonstrate that the proposed robust asymmetric loss
performs favorably against the long-tailed with multi-label medical image
classification in addition to the various long-tailed single-label datasets.
Notably, our method achieves Top-5 results on the CXR-LT dataset of the ICCV
CVAMD 2023 competition. We opensource our implementation of the robust
asymmetric loss in the public repository: this https URL.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Is there progress in activity progress prediction?</b></summary>
  <p><b>编号</b>：[86]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05533</p>
  <p><b>作者</b>：Frans de Boer,  Jan C. van Gemert,  Jouke Dijkstra,  Silvia L. Pintea</p>
  <p><b>备注</b>：Accepted at ICCVw-2023 (AI for Creative Video Editing and Understanding, ICCV workshop 2023)</p>
  <p><b>关键词</b>：progress prediction, progress prediction aims, progress, prediction, Activity progress prediction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Activity progress prediction aims to estimate what percentage of an activity
has been completed. Currently this is done with machine learning approaches,
trained and evaluated on complicated and realistic video datasets. The videos
in these datasets vary drastically in length and appearance. And some of the
activities have unanticipated developments, making activity progression
difficult to estimate. In this work, we examine the results obtained by
existing progress prediction methods on these datasets. We find that current
progress prediction methods seem not to extract useful visual information for
the progress prediction task. Therefore, these methods fail to exceed simple
frame-counting baselines. We design a precisely controlled dataset for activity
progress prediction and on this synthetic dataset we show that the considered
methods can make use of the visual information, when this directly relates to
the progress prediction. We conclude that the progress prediction task is
ill-posed on the currently used real-world datasets. Moreover, to fairly
measure activity progression we advise to consider a, simple but effective,
frame-counting baseline.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Critical Points ++: An Agile Point Cloud Importance Measure for Robust  Classification, Adversarial Defense and Explainable AI</b></summary>
  <p><b>编号</b>：[87]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05525</p>
  <p><b>作者</b>：Meir Yossef Levi,  Guy Gilboa</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：real-world safety demanding, safety demanding applications, ability to cope, cope accurately, crucial in real-world</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The ability to cope accurately and fast with Out-Of-Distribution (OOD)
samples is crucial in real-world safety demanding applications. In this work we
first study the interplay between critical points of 3D point clouds and OOD
samples. Our findings are that common corruptions and outliers are often
interpreted as critical points. We generalize the notion of critical points
into importance measures. We show that training a classification network based
only on less important points dramatically improves robustness, at a cost of
minor performance loss on the clean set. We observe that normalized entropy is
highly informative for corruption analysis. An adaptive threshold based on
normalized entropy is suggested for selecting the set of uncritical points. Our
proposed importance measure is extremely fast to compute. We show it can be
used for a variety of applications, such as Explainable AI (XAI), Outlier
Removal, Uncertainty Estimation, Robust Classification and Adversarial Defense.
We reach SOTA results on the two latter tasks.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Look at the Neighbor: Distortion-aware Unsupervised Domain Adaptation  for Panoramic Semantic Segmentation</b></summary>
  <p><b>编号</b>：[103]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05493</p>
  <p><b>作者</b>：Xu Zheng,  Tianbo Pan,  Yunhao Luo,  Lin Wang</p>
  <p><b>备注</b>：Accepted to ICCV 2023</p>
  <p><b>关键词</b>：labeled pinhole image, pinhole image domain, unlabeled panoramic image, image domain, panoramic image domain</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Endeavors have been recently made to transfer knowledge from the labeled
pinhole image domain to the unlabeled panoramic image domain via Unsupervised
Domain Adaptation (UDA). The aim is to tackle the domain gaps caused by the
style disparities and distortion problem from the non-uniformly distributed
pixels of equirectangular projection (ERP). Previous works typically focus on
transferring knowledge based on geometric priors with specially designed
multi-branch network architectures. As a result, considerable computational
costs are induced, and meanwhile, their generalization abilities are profoundly
hindered by the variation of distortion among pixels. In this paper, we find
that the pixels' neighborhood regions of the ERP indeed introduce less
distortion. Intuitively, we propose a novel UDA framework that can effectively
address the distortion problems for panoramic semantic segmentation. In
comparison, our method is simpler, easier to implement, and more
computationally efficient. Specifically, we propose distortion-aware attention
(DA) capturing the neighboring pixel distribution without using any geometric
constraints. Moreover, we propose a class-wise feature aggregation (CFA) module
to iteratively update the feature representations with a memory bank. As such,
the feature similarity between two domains can be consistently optimized.
Extensive experiments show that our method achieves new state-of-the-art
performance while remarkably reducing 80% parameters.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：YOLO-MS: Rethinking Multi-Scale Representation Learning for Real-time  Object Detection</b></summary>
  <p><b>编号</b>：[108]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05480</p>
  <p><b>作者</b>：Yuming Chen,  Xinbin Yuan,  Ruiqi Wu,  Jiabao Wang,  Qibin Hou,  Ming-Ming Cheng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：performant object detector, object detection community, real-time object detectors, aim at providing, efficient and performant</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We aim at providing the object detection community with an efficient and
performant object detector, termed YOLO-MS. The core design is based on a
series of investigations on how convolutions with different kernel sizes affect
the detection performance of objects at different scales. The outcome is a new
strategy that can strongly enhance multi-scale feature representations of
real-time object detectors. To verify the effectiveness of our strategy, we
build a network architecture, termed YOLO-MS. We train our YOLO-MS on the MS
COCO dataset from scratch without relying on any other large-scale datasets,
like ImageNet, or pre-trained weights. Without bells and whistles, our YOLO-MS
outperforms the recent state-of-the-art real-time object detectors, including
YOLO-v7 and RTMDet, when using a comparable number of parameters and FLOPs.
Taking the XS version of YOLO-MS as an example, with only 4.5M learnable
parameters and 8.7G FLOPs, it can achieve an AP score of 43%+ on MS COCO, which
is about 2%+ higher than RTMDet with the same model size. Moreover, our work
can also be used as a plug-and-play module for other YOLO models. Typically,
our method significantly improves the AP of YOLOv8 from 37%+ to 40%+ with even
fewer parameters and FLOPs. Code is available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Reviewing 3D Object Detectors in the Context of High-Resolution 3+1D  Radar</b></summary>
  <p><b>编号</b>：[109]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05478</p>
  <p><b>作者</b>：Patrick Palmer,  Martin Krueger,  Richard Altendorfer,  Ganesh Adam,  Torsten Bertram</p>
  <p><b>备注</b>：Published at CVPR 2023 Workshop on 3D Vision and Robotics (this https URL)</p>
  <p><b>关键词</b>：beginning market introduction, point clouds, initialized deep learning-based, deep learning-based, point</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent developments and the beginning market introduction of high-resolution
imaging 4D (3+1D) radar sensors have initialized deep learning-based radar
perception research. We investigate deep learning-based models operating on
radar point clouds for 3D object detection. 3D object detection on lidar point
cloud data is a mature area of 3D vision. Many different architectures have
been proposed, each with strengths and weaknesses. Due to similarities between
3D lidar point clouds and 3+1D radar point clouds, those existing 3D object
detectors are a natural basis to start deep learning-based 3D object detection
on radar data. Thus, the first step is to analyze the detection performance of
the existing models on the new data modality and evaluate them in depth. In
order to apply existing 3D point cloud object detectors developed for lidar
point clouds to the radar domain, they need to be adapted first. While some
detectors, such as PointPillars, have already been adapted to be applicable to
radar data, we have adapted others, e.g., Voxel R-CNN, SECOND, PointRCNN, and
PV-RCNN. To this end, we conduct a cross-model validation (evaluating a set of
models on one particular data set) as well as a cross-data set validation
(evaluating all models in the model set on several data sets). The
high-resolution radar data used are the View-of-Delft and Astyx data sets.
Finally, we evaluate several adaptations of the models and their training
procedures. We also discuss major factors influencing the detection performance
on radar data and propose possible solutions indicating potential future
research avenues.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：KS-APR: Keyframe Selection for Robust Absolute Pose Regression</b></summary>
  <p><b>编号</b>：[114]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05459</p>
  <p><b>作者</b>：Changkun Liu,  Yukun Zhao,  Tristan Braud</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Mobile Augmented Reality, Markerless Mobile Augmented, Augmented Reality, anchor digital content, Mobile Augmented</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Markerless Mobile Augmented Reality (AR) aims to anchor digital content in
the physical world without using specific 2D or 3D objects. Absolute Pose
Regressors (APR) are end-to-end machine learning solutions that infer the
device's pose from a single monocular image. Thanks to their low computation
cost, they can be directly executed on the constrained hardware of mobile AR
devices. However, APR methods tend to yield significant inaccuracies for input
images that are too distant from the training set. This paper introduces
KS-APR, a pipeline that assesses the reliability of an estimated pose with
minimal overhead by combining the inference results of the APR and the prior
images in the training set. Mobile AR systems tend to rely upon visual-inertial
odometry to track the relative pose of the device during the experience. As
such, KS-APR favours reliability over frequency, discarding unreliable poses.
This pipeline can integrate most existing APR methods to improve accuracy by
filtering unreliable images with their pose estimates. We implement the
pipeline on three types of APR models on indoor and outdoor datasets. The
median error on position and orientation is reduced for all models, and the
proportion of large errors is minimized across datasets. Our method enables
state-of-the-art APRs such as DFNetdm to outperform single-image and sequential
APR methods. These results demonstrate the scalability and effectiveness of
KS-APR for visual localization tasks that do not require one-shot decisions.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：A Generalized Physical-knowledge-guided Dynamic Model for Underwater  Image Enhancement</b></summary>
  <p><b>编号</b>：[116]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05447</p>
  <p><b>作者</b>：Pan Mu,  Hanning Xu,  Zheyuan Liu,  Zheng Wang,  Sixian Chan,  Cong Bai</p>
  <p><b>备注</b>：Accepted by ACMMM 2023</p>
  <p><b>关键词</b>：low contrast resulting, distortion and low, low contrast, contrast resulting, scattering and absorption</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Underwater images often suffer from color distortion and low contrast
resulting in various image types, due to the scattering and absorption of light
by water. While it is difficult to obtain high-quality paired training samples
with a generalized model. To tackle these challenges, we design a Generalized
Underwater image enhancement method via a Physical-knowledge-guided Dynamic
Model (short for GUPDM), consisting of three parts: Atmosphere-based Dynamic
Structure (ADS), Transmission-guided Dynamic Structure (TDS), and Prior-based
Multi-scale Structure (PMS). In particular, to cover complex underwater scenes,
this study changes the global atmosphere light and the transmission to simulate
various underwater image types (e.g., the underwater image color ranging from
yellow to blue) through the formation model. We then design ADS and TDS that
use dynamic convolutions to adaptively extract prior information from
underwater images and generate parameters for PMS. These two modules enable the
network to select appropriate parameters for various water types adaptively.
Besides, the multi-scale feature extraction module in PMS uses convolution
blocks with different kernel sizes and obtains weights for each feature map via
channel attention block and fuses them to boost the receptive field of the
network. The source code will be available at
\href{this https URL}{this https URL}.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Benchmarking Algorithmic Bias in Face Recognition: An Experimental  Approach Using Synthetic Faces and Human Evaluation</b></summary>
  <p><b>编号</b>：[120]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05441</p>
  <p><b>作者</b>：Hao Liang,  Pietro Perona,  Guha Balakrishnan</p>
  <p><b>备注</b>：accepted to iccv2023; 18 figures</p>
  <p><b>关键词</b>：measuring bias, face recognition systems, synthetic, face, attributes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose an experimental method for measuring bias in face recognition
systems. Existing methods to measure bias depend on benchmark datasets that are
collected in the wild and annotated for protected (e.g., race, gender) and
non-protected (e.g., pose, lighting) attributes. Such observational datasets
only permit correlational conclusions, e.g., "Algorithm A's accuracy is
different on female and male faces in dataset X.". By contrast, experimental
methods manipulate attributes individually and thus permit causal conclusions,
e.g., "Algorithm A's accuracy is affected by gender and skin color."
Our method is based on generating synthetic faces using a neural face
generator, where each attribute of interest is modified independently while
leaving all other attributes constant. Human observers crucially provide the
ground truth on perceptual identity similarity between synthetic image pairs.
We validate our method quantitatively by evaluating race and gender biases of
three research-grade face recognition models. Our synthetic pipeline reveals
that for these algorithms, accuracy is lower for Black and East Asian
population subgroups. Our method can also quantify how perceptual changes in
attributes affect face identity distances reported by these models. Our large
synthetic dataset, consisting of 48,000 synthetic face image pairs (10,200
unique synthetic faces) and 555,000 human annotations (individual attributes
and pairwise identity comparisons) is available to researchers in this
important area.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Deep Fusion Transformer Network with Weighted Vector-Wise Keypoints  Voting for Robust 6D Object Pose Estimation</b></summary>
  <p><b>编号</b>：[121]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05438</p>
  <p><b>作者</b>：Jun Zhou,  Kai Chen,  Linlin Xu,  Qi Dou,  Jing Qin</p>
  <p><b>备注</b>：Accepted by ICCV2023</p>
  <p><b>关键词</b>：single RGBD image, object pose estimation, single RGBD, RGBD image, Deep Fusion Transformer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>One critical challenge in 6D object pose estimation from a single RGBD image
is efficient integration of two different modalities, i.e., color and depth. In
this work, we tackle this problem by a novel Deep Fusion Transformer~(DFTr)
block that can aggregate cross-modality features for improving pose estimation.
Unlike existing fusion methods, the proposed DFTr can better model
cross-modality semantic correlation by leveraging their semantic similarity,
such that globally enhanced features from different modalities can be better
integrated for improved information extraction. Moreover, to further improve
robustness and efficiency, we introduce a novel weighted vector-wise voting
algorithm that employs a non-iterative global optimization strategy for precise
3D keypoint localization while achieving near real-time inference. Extensive
experiments show the effectiveness and strong generalization capability of our
proposed 3D keypoint voting algorithm. Results on four widely used benchmarks
also demonstrate that our method outperforms the state-of-the-art methods by
large margins.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Ensemble Modeling for Multimodal Visual Action Recognition</b></summary>
  <p><b>编号</b>：[122]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05430</p>
  <p><b>作者</b>：Jyoti Kini,  Sarah Fleischer,  Ishan Dave,  Mubarak Shah</p>
  <p><b>备注</b>：Technical Report accepted at the Multimodal Action Recognition Challenge on the MECCANO Dataset - ICIAP 2023</p>
  <p><b>关键词</b>：multimodal action recognition, ensemble modeling approach, focal loss, ensemble modeling, focal loss tailored</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we propose an ensemble modeling approach for multimodal action
recognition. We independently train individual modality models using a variant
of focal loss tailored to handle the long-tailed distribution of the MECCANO
[21] dataset. Based on the underlying principle of focal loss, which captures
the relationship between tail (scarce) classes and their prediction
difficulties, we propose an exponentially decaying variant of focal loss for
our current task. It initially emphasizes learning from the hard misclassified
examples and gradually adapts to the entire range of examples in the dataset.
This annealing process encourages the model to strike a balance between
focusing on the sparse set of hard samples, while still leveraging the
information provided by the easier ones. Additionally, we opt for the late
fusion strategy to combine the resultant probability distributions from RGB and
Depth modalities for final action prediction. Experimental evaluations on the
MECCANO dataset demonstrate the effectiveness of our approach.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Speech-Driven 3D Face Animation with Composite and Regional Facial  Movements</b></summary>
  <p><b>编号</b>：[123]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05428</p>
  <p><b>作者</b>：Haozhe Wu,  Songtao Zhou,  Jia Jia,  Junliang Xing,  Qi Wen,  Xiang Wen</p>
  <p><b>备注</b>：Accepted by MM 2023, 9 pages, 7 figures</p>
  <p><b>关键词</b>：poses significant challenges, significant challenges due, facial movements, animation poses significant, face animation poses</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Speech-driven 3D face animation poses significant challenges due to the
intricacy and variability inherent in human facial movements. This paper
emphasizes the importance of considering both the composite and regional
natures of facial movements in speech-driven 3D face animation. The composite
nature pertains to how speech-independent factors globally modulate
speech-driven facial movements along the temporal dimension. Meanwhile, the
regional nature alludes to the notion that facial movements are not globally
correlated but are actuated by local musculature along the spatial dimension.
It is thus indispensable to incorporate both natures for engendering vivid
animation. To address the composite nature, we introduce an adaptive modulation
module that employs arbitrary facial movements to dynamically adjust
speech-driven facial movements across frames on a global scale. To accommodate
the regional nature, our approach ensures that each constituent of the facial
features for every frame focuses on the local spatial movements of 3D faces.
Moreover, we present a non-autoregressive backbone for translating audio to 3D
facial movements, which maintains high-frequency nuances of facial movements
and facilitates efficient inference. Comprehensive experiments and user studies
demonstrate that our method surpasses contemporary state-of-the-art approaches
both qualitatively and quantitatively.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Adaptive Low Rank Adaptation of Segment Anything to Salient Object  Detection</b></summary>
  <p><b>编号</b>：[124]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05426</p>
  <p><b>作者</b>：Ruikai Cui,  Siyuan He,  Shi Qiu</p>
  <p><b>备注</b>：13 pages, 0 figures</p>
  <p><b>关键词</b>：Meta LLaMA, Foundation models, Salient Object Model, Meta, Segment Salient Object</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Foundation models, such as OpenAI's GPT-3 and GPT-4, Meta's LLaMA, and
Google's PaLM2, have revolutionized the field of artificial intelligence. A
notable paradigm shift has been the advent of the Segment Anything Model (SAM),
which has exhibited a remarkable capability to segment real-world objects,
trained on 1 billion masks and 11 million images. Although SAM excels in
general object segmentation, it lacks the intrinsic ability to detect salient
objects, resulting in suboptimal performance in this domain. To address this
challenge, we present the Segment Salient Object Model (SSOM), an innovative
approach that adaptively fine-tunes SAM for salient object detection by
harnessing the low-rank structure inherent in deep learning. Comprehensive
qualitative and quantitative evaluations across five challenging RGB benchmark
datasets demonstrate the superior performance of our approach, surpassing
state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Progressive Spatio-temporal Perception for Audio-Visual Question  Answering</b></summary>
  <p><b>编号</b>：[126]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05421</p>
  <p><b>作者</b>：Guangyao Li,  Wenxuan Hou,  Di Hu</p>
  <p><b>备注</b>：Accepted by ACM MM 2023</p>
  <p><b>关键词</b>：Audio-Visual Question Answering, task aims, aims to answer, Spatio-Temporal Perception Network, Answering</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Audio-Visual Question Answering (AVQA) task aims to answer questions about
different visual objects, sounds, and their associations in videos. Such
naturally multi-modal videos are composed of rich and complex dynamic
audio-visual components, where most of which could be unrelated to the given
questions, or even play as interference in answering the content of interest.
Oppositely, only focusing on the question-aware audio-visual content could get
rid of influence, meanwhile enabling the model to answer more efficiently. In
this paper, we propose a Progressive Spatio-Temporal Perception Network
(PSTP-Net), which contains three modules that progressively identify key
spatio-temporal regions w.r.t. questions. Specifically, a temporal segment
selection module is first introduced to select the most relevant audio-visual
segments related to the given question. Then, a spatial region selection module
is utilized to choose the most relevant regions associated with the question
from the selected temporal segments. To further refine the selection of
features, an audio-guided visual attention module is employed to perceive the
association between auido and selected spatial regions. Finally, the
spatio-temporal features from these modules are integrated for answering the
question. Extensive experimental results on the public MUSIC-AVQA and AVQA
datasets provide compelling evidence of the effectiveness and efficiency of
PSTP-Net. Code is available at:
\href{this https URL}{this https URL}</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：SC3K: Self-supervised and Coherent 3D Keypoints Estimation from Rotated,  Noisy, and Decimated Point Cloud Data</b></summary>
  <p><b>编号</b>：[129]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05410</p>
  <p><b>作者</b>：Mohammad Zohaib,  Alessio Del Bue</p>
  <p><b>备注</b>：This paper has been accepted in International Conference on Computer Vision (ICCV) 2023. For code and data, please refer to the following GitHub page: this https URL</p>
  <p><b>关键词</b>：arbitrary object categories, point cloud data, down-sampled and arbitrarily, arbitrarily rotated, paper proposes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper proposes a new method to infer keypoints from arbitrary object
categories in practical scenarios where point cloud data (PCD) are noisy,
down-sampled and arbitrarily rotated. Our proposed model adheres to the
following principles: i) keypoints inference is fully unsupervised (no
annotation given), ii) keypoints position error should be low and resilient to
PCD perturbations (robustness), iii) keypoints should not change their indexes
for the intra-class objects (semantic coherence), iv) keypoints should be close
to or proximal to PCD surface (compactness). We achieve these desiderata by
proposing a new self-supervised training strategy for keypoints estimation that
does not assume any a priori knowledge of the object class, and a model
architecture with coupled auxiliary losses that promotes the desired keypoints
properties. We compare the keypoints estimated by the proposed approach with
those of the state-of-the-art unsupervised approaches. The experiments show
that our approach outperforms by estimating keypoints with improved coverage
(+9.41%) while being semantically consistent (+4.66%) that best characterizes
the object's 3D shape for downstream tasks. Code and data are available at:
this https URL</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：A Comparative Assessment of Multi-view fusion learning for Crop  Classification</b></summary>
  <p><b>编号</b>：[130]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05407</p>
  <p><b>作者</b>：Francisco Mena,  Diego Arenas,  Marlon Nuske,  Andreas Dengel</p>
  <p><b>备注</b>：Accepted at IEEE International Geoscience and Remote Sensing Symposium 2023</p>
  <p><b>关键词</b>：rapidly increasing amount, multi-view learning modeling, remote sensing, learning modeling, rapidly increasing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With a rapidly increasing amount and diversity of remote sensing (RS) data
sources, there is a strong need for multi-view learning modeling. This is a
complex task when considering the differences in resolution, magnitude, and
noise of RS data. The typical approach for merging multiple RS sources has been
input-level fusion, but other - more advanced - fusion strategies may
outperform this traditional approach. This work assesses different fusion
strategies for crop classification in the CropHarvest dataset. The fusion
methods proposed in this work outperform models based on individual views and
previous fusion methods. We do not find one single fusion method that
consistently outperforms all other approaches. Instead, we present a comparison
of multi-view fusion methods for three different datasets and show that,
depending on the test region, different methods obtain the best performance.
Despite this, we suggest a preliminary criterion for the selection of fusion
methods.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Enhancing Low-light Light Field Images with A Deep Compensation  Unfolding Network</b></summary>
  <p><b>编号</b>：[131]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05404</p>
  <p><b>作者</b>：Xianqiang Lyu,  Junhui Hou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：restoring light field, compensation unfolding network, light field, low-light conditions, restoring light</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a novel and interpretable end-to-end learning framework,
called the deep compensation unfolding network (DCUNet), for restoring light
field (LF) images captured under low-light conditions. DCUNet is designed with
a multi-stage architecture that mimics the optimization process of solving an
inverse imaging problem in a data-driven fashion. The framework uses the
intermediate enhanced result to estimate the illumination map, which is then
employed in the unfolding process to produce a new enhanced result.
Additionally, DCUNet includes a content-associated deep compensation module at
each optimization stage to suppress noise and illumination map estimation
errors. To properly mine and leverage the unique characteristics of LF images,
this paper proposes a pseudo-explicit feature interaction module that
comprehensively exploits redundant information in LF images. The experimental
results on both simulated and real datasets demonstrate the superiority of our
DCUNet over state-of-the-art methods, both qualitatively and quantitatively.
Moreover, DCUNet preserves the essential geometric structure of enhanced LF
images much better. The code will be publicly available at
this https URL.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Learning Gabor Texture Features for Fine-Grained Recognition</b></summary>
  <p><b>编号</b>：[133]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05396</p>
  <p><b>作者</b>：Lanyun Zhu,  Tianrun Chen,  Jianxiong Yin,  Simon See,  Jun Liu</p>
  <p><b>备注</b>：Accepted to ICCV2023</p>
  <p><b>关键词</b>：Gabor filters, Gabor, fine-grained recognition, features, detailed local information</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Extracting and using class-discriminative features is critical for
fine-grained recognition. Existing works have demonstrated the possibility of
applying deep CNNs to exploit features that distinguish similar classes.
However, CNNs suffer from problems including frequency bias and loss of
detailed local information, which restricts the performance of recognizing
fine-grained categories. To address the challenge, we propose a novel texture
branch as complimentary to the CNN branch for feature extraction. We
innovatively utilize Gabor filters as a powerful extractor to exploit texture
features, motivated by the capability of Gabor filters in effectively capturing
multi-frequency features and detailed local information. We implement several
designs to enhance the effectiveness of Gabor filters, including imposing
constraints on parameter values and developing a learning method to determine
the optimal parameters. Moreover, we introduce a statistical feature extractor
to utilize informative statistical information from the signals captured by
Gabor filters, and a gate selection mechanism to enable efficient computation
by only considering qualified regions as input for texture extraction. Through
the integration of features from the Gabor-filter-based texture branch and
CNN-based semantic branch, we achieve comprehensive information extraction. We
demonstrate the efficacy of our method on multiple datasets, including
CUB-200-2011, NA-bird, Stanford Dogs, and GTOS-mobile. State-of-the-art
performance is achieved using our approach.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：Robust Localization with Visual-Inertial Odometry Constraints for  Markerless Mobile AR</b></summary>
  <p><b>编号</b>：[134]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05394</p>
  <p><b>作者</b>：Changkun Liu,  Yukun Zhao,  Tristan Braud</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Visual Inertial Odometry, Inertial Odometry, Visual Inertial, modern Augmented, essential component</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Visual Inertial Odometry (VIO) is an essential component of modern Augmented
Reality (AR) applications. However, VIO only tracks the relative pose of the
device, leading to drift over time. Absolute pose estimation methods infer the
device's absolute pose, but their accuracy depends on the input quality. This
paper introduces VIO-APR, a new framework for markerless mobile AR that
combines an absolute pose regressor (APR) with a local VIO tracking system.
VIO-APR uses VIO to assess the reliability of the APR and the APR to identify
and compensate for VIO drift. This feedback loop results in more accurate
positioning and more stable AR experiences. To evaluate VIO-APR, we created a
dataset that combines camera images with ARKit's VIO system output for six
indoor and outdoor scenes of various scales. Over this dataset, VIO-APR
improves the median accuracy of popular APR by up to 36\% in position and 29\%
in orientation, increases the percentage of frames in the high ($0.25 m,
2^{\circ}$) accuracy level by up to 112\% and reduces the percentage of frames
predicted below the low ($5 m, 10^\circ$) accuracy greatly. We implement
VIO-APR into a mobile AR application using Unity to demonstrate its
capabilities. VIO-APR results in noticeably more accurate localization and a
more stable overall experience.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Product Review Image Ranking for Fashion E-commerce</b></summary>
  <p><b>编号</b>：[136]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05390</p>
  <p><b>作者</b>：Sangeet Jaiswal,  Dhruv Patel,  Sreekanth Vempati,  Konduru Saiswaroop</p>
  <p><b>备注</b>：Accepted in Proceedings of ACM SIGIR Workshop on eCommerce (SIGIR eCom'22)</p>
  <p><b>关键词</b>：making purchase decisions, examine the products, purchase decisions, physically examine, customers' text</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In a fashion e-commerce platform where customers can't physically examine the
products on their own, being able to see other customers' text and image
reviews of the product is critical while making purchase decisions. Given the
high reliance on these reviews, over the years we have observed customers
proactively sharing their reviews. With an increase in the coverage of User
Generated Content (UGC), there has been a corresponding increase in the number
of customer images. It is thus imperative to display the most relevant images
on top as it may influence users' online shopping choices and behavior. In this
paper, we propose a simple yet effective training procedure for ranking
customer images. We created a dataset consisting of Myntra (A Major Indian
Fashion e-commerce company) studio posts and highly engaged (upvotes/downvotes)
UGC images as our starting point and used selected distortion techniques on the
images of the above dataset to bring their quality at par with those of bad UGC
images. We train our network to rank bad-quality images lower than high-quality
ones. Our proposed method outperforms the baseline models on two metrics,
namely correlation coefficient, and accuracy, by substantial margins.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：HGDNet: A Height-Hierarchy Guided Dual-Decoder Network for Single View  Building Extraction and Height Estimation</b></summary>
  <p><b>编号</b>：[137]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05387</p>
  <p><b>作者</b>：Chaoran Lu,  Ningning Cao,  Pan Zhang,  Ting Liu,  Baochai Peng,  Guozhang Liu,  Mengke Yuan,  Sen Zhang,  Simin Huang,  Tao Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：correlative single-view satellite, single-view satellite image, acquire generalist model, satellite image building, Unifying the correlative</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Unifying the correlative single-view satellite image building extraction and
height estimation tasks indicates a promising way to share representations and
acquire generalist model for large-scale urban 3D reconstruction. However, the
common spatial misalignment between building footprints and
stereo-reconstructed nDSM height labels incurs degraded performance on both
tasks. To address this issue, we propose a Height-hierarchy Guided Dual-decoder
Network (HGDNet) to estimate building height. Under the guidance of synthesized
discrete height-hierarchy nDSM, auxiliary height-hierarchical building
extraction branch enhance the height estimation branch with implicit
constraints, yielding an accuracy improvement of more than 6% on the DFC 2023
track2 dataset. Additional two-stage cascade architecture is adopted to achieve
more accurate building extraction. Experiments on the DFC 2023 Track 2 dataset
shows the superiority of the proposed method in building height estimation
({\delta}1:0.8012), instance extraction (AP50:0.7730), and the final average
score 0.7871 ranks in the first place in test phase.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Interaction-aware Joint Attention Estimation Using People Attributes</b></summary>
  <p><b>编号</b>：[141]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05382</p>
  <p><b>作者</b>：Chihiro Nakatani,  Hiroaki Kawashima,  Norimichi Ukita</p>
  <p><b>备注</b>：Accepted to ICCV2023</p>
  <p><b>关键词</b>：paper proposes joint, single image, joint attention, attention, proposes joint attention</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper proposes joint attention estimation in a single image. Different
from related work in which only the gaze-related attributes of people are
independently employed, (I) their locations and actions are also employed as
contextual cues for weighting their attributes, and (ii) interactions among all
of these attributes are explicitly modeled in our method. For the interaction
modeling, we propose a novel Transformer-based attention network to encode
joint attention as low-dimensional features. We introduce a specialized MLP
head with positional embedding to the Transformer so that it predicts pixelwise
confidence of joint attention for generating the confidence heatmap. This
pixelwise prediction improves the heatmap accuracy by avoiding the ill-posed
problem in which the high-dimensional heatmap is predicted from the
low-dimensional features. The estimated joint attention is further improved by
being integrated with general image-based attention estimation. Our method
outperforms SOTA methods quantitatively in comparative experiments. Code:
https://anonymous.4open.science/r/anonymized_codes-ECA4.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Flexible Isosurface Extraction for Gradient-Based Mesh Optimization</b></summary>
  <p><b>编号</b>：[145]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05371</p>
  <p><b>作者</b>：Tianchang Shen,  Jacob Munkberg,  Jon Hasselgren,  Kangxue Yin,  Zian Wang,  Wenzheng Chen,  Zan Gojcic,  Sanja Fidler,  Nicholas Sharp,  Jun Gao</p>
  <p><b>备注</b>：SIGGRAPH 2023. Project page: this https URL</p>
  <p><b>关键词</b>：increasingly common paradigm, applications including photogrammetry, generative modeling, including photogrammetry, inverse physics</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This work considers gradient-based mesh optimization, where we iteratively
optimize for a 3D surface mesh by representing it as the isosurface of a scalar
field, an increasingly common paradigm in applications including
photogrammetry, generative modeling, and inverse physics. Existing
implementations adapt classic isosurface extraction algorithms like Marching
Cubes or Dual Contouring; these techniques were designed to extract meshes from
fixed, known fields, and in the optimization setting they lack the degrees of
freedom to represent high-quality feature-preserving meshes, or suffer from
numerical instabilities. We introduce FlexiCubes, an isosurface representation
specifically designed for optimizing an unknown mesh with respect to geometric,
visual, or even physical objectives. Our main insight is to introduce
additional carefully-chosen parameters into the representation, which allow
local flexible adjustments to the extracted mesh geometry and connectivity.
These parameters are updated along with the underlying scalar field via
automatic differentiation when optimizing for a downstream task. We base our
extraction scheme on Dual Marching Cubes for improved topological properties,
and present extensions to optionally generate tetrahedral and
hierarchically-adaptive meshes. Extensive experiments validate FlexiCubes on
both synthetic benchmarks and real-world applications, showing that it offers
significant improvements in mesh quality and geometric fidelity.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Pseudo-label Alignment for Semi-supervised Instance Segmentation</b></summary>
  <p><b>编号</b>：[151]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05359</p>
  <p><b>作者</b>：Jie Hu,  Chen Chen,  Liujuan Cao,  Shengchuan Zhang,  Annan Shu,  Guannan Jiang,  Rongrong Ji</p>
  <p><b>备注</b>：ICCV 2023</p>
  <p><b>关键词</b>：Pseudo-labeling is significant, subsequent training, instance segmentation, semi-supervised instance segmentation, classes from unannotated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pseudo-labeling is significant for semi-supervised instance segmentation,
which generates instance masks and classes from unannotated images for
subsequent training. However, in existing pipelines, pseudo-labels that contain
valuable information may be directly filtered out due to mismatches in class
and mask quality. To address this issue, we propose a novel framework, called
pseudo-label aligning instance segmentation (PAIS), in this paper. In PAIS, we
devise a dynamic aligning loss (DALoss) that adjusts the weights of
semi-supervised loss terms with varying class and mask score pairs. Through
extensive experiments conducted on the COCO and Cityscapes datasets, we
demonstrate that PAIS is a promising framework for semi-supervised instance
segmentation, particularly in cases where labeled data is severely limited.
Notably, with just 1\% labeled data, PAIS achieves 21.2 mAP (based on
Mask-RCNN) and 19.9 mAP (based on K-Net) on the COCO dataset, outperforming the
current state-of-the-art model, \ie, NoisyBoundary with 7.7 mAP, by a margin of
over 12 points. Code is available at: \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Fine-grained building roof instance segmentation based on domain adapted  pretraining and composite dual-backbone</b></summary>
  <p><b>编号</b>：[152]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05358</p>
  <p><b>作者</b>：Guozhang Liu,  Baochai Peng,  Ting Liu,  Pan Zhang,  Mengke Yuan,  Chaoran Lu,  Ningning Cao,  Sen Zhang,  Simin Huang,  Tao Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：roof types pose, global cities situated, significant inter-class imbalance, types pose challenges, accurate building roof</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The diversity of building architecture styles of global cities situated on
various landforms, the degraded optical imagery affected by clouds and shadows,
and the significant inter-class imbalance of roof types pose challenges for
designing a robust and accurate building roof instance segmentor. To address
these issues, we propose an effective framework to fulfill semantic
interpretation of individual buildings with high-resolution optical satellite
imagery. Specifically, the leveraged domain adapted pretraining strategy and
composite dual-backbone greatly facilitates the discriminative feature
learning. Moreover, new data augmentation pipeline, stochastic weight averaging
(SWA) training and instance segmentation based model ensemble in testing are
utilized to acquire additional performance boost. Experiment results show that
our approach ranks in the first place of the 2023 IEEE GRSS Data Fusion Contest
(DFC) Track 1 test phase ($mAP_{50}$:50.6\%). Note-worthily, we have also
explored the potential of multimodal data fusion with both optical satellite
imagery and SAR data.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：TCSloT: Text Guided 3D Context and Slope Aware Triple Network for Dental  Implant Position Prediction</b></summary>
  <p><b>编号</b>：[153]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05355</p>
  <p><b>作者</b>：Xinquan Yang,  Jinheng Xie,  Xuechen Li,  Xuguang Li,  Linlin Shen,  Yongqiang Deng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ensure accurate implantation, implant prosthesis treatment, implant position, prosthesis treatment, accurate implantation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In implant prosthesis treatment, the surgical guide of implant is used to
ensure accurate implantation. However, such design heavily relies on the manual
location of the implant position. When deep neural network has been proposed to
assist the dentist in locating the implant position, most of them take a single
slice as input, which do not fully explore 3D contextual information and
ignoring the influence of implant slope. In this paper, we design a Text Guided
3D Context and Slope Aware Triple Network (TCSloT) which enables the perception
of contextual information from multiple adjacent slices and awareness of
variation of implant slopes. A Texture Variation Perception (TVP) module is
correspondingly elaborated to process the multiple slices and capture the
texture variation among slices and a Slope-Aware Loss (SAL) is proposed to
dynamically assign varying weights for the regression head. Additionally, we
design a conditional text guidance (CTG) module to integrate the text condition
(i.e., left, middle and right) from the CLIP for assisting the implant position
prediction. Extensive experiments on a dental implant dataset through five-fold
cross-validation demonstrated that the proposed TCSloT achieves superior
performance than existing methods.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Towards General and Fast Video Derain via Knowledge Distillation</b></summary>
  <p><b>编号</b>：[156]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05346</p>
  <p><b>作者</b>：Defang Cai,  Pan Mu,  Sixian Chan,  Zhanpeng Shao,  Cong Bai</p>
  <p><b>备注</b>：6 pages; Accepted at IEEE ICME</p>
  <p><b>关键词</b>：natural weather condition, common natural weather, rain streak types, weather condition, visual system</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As a common natural weather condition, rain can obscure video frames and thus
affect the performance of the visual system, so video derain receives a lot of
attention. In natural environments, rain has a wide variety of streak types,
which increases the difficulty of the rain removal task. In this paper, we
propose a Rain Review-based General video derain Network via knowledge
distillation (named RRGNet) that handles different rain streak types with one
pre-training weight. Specifically, we design a frame grouping-based
encoder-decoder network that makes full use of the temporal information of the
video. Further, we use the old task model to guide the current model in
learning new rain streak types while avoiding forgetting. To consolidate the
network's ability to derain, we design a rain review module to play back data
from old tasks for the current model. The experimental results show that our
developed general method achieves the best results in terms of running speed
and derain effect.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Prostate Age Gap (PAG): An MRI surrogate marker of aging for prostate  cancer detection</b></summary>
  <p><b>编号</b>：[158]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05344</p>
  <p><b>作者</b>：Alvaro Fernandez-Quilez,  Tobias Nordström,  Fredrik Jäderling,  Svein Reidar Kjosavik,  Martin Eklund</p>
  <p><b>备注</b>：Under review</p>
  <p><b>关键词</b>：MRI-based risk calculators, prostate MRI, age, MRI, calculators are commonly</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Background: Prostate cancer (PC) MRI-based risk calculators are commonly
based on biological (e.g. PSA), MRI markers (e.g. volume), and patient age.
Whilst patient age measures the amount of years an individual has existed,
biological age (BA) might better reflect the physiology of an individual.
However, surrogates from prostate MRI and linkage with clinically significant
PC (csPC) remain to be explored. Purpose: To obtain and evaluate Prostate Age
Gap (PAG) as an MRI marker tool for csPC risk. Study type: Retrospective.
Population: A total of 7243 prostate MRI slices from 468 participants who had
undergone prostate biopsies. A deep learning model was trained on 3223 MRI
slices cropped around the gland from 81 low-grade PC (ncsPC, Gleason score <=6) 131 256 and negative cases tested on the remaining participants. assessment: chronological age was defined as of participant at time visit used to train deep learning model predict patient. following, we obtained pag, predicted minus patient's age. multivariate logistic regression models were estimate association through odds ratio (or) predictive value pag compared against psa levels pi-rads>=3. Statistical tests:
T-test, Mann-Whitney U test, Permutation test and ROC curve analysis. Results:
The multivariate adjusted model showed a significant difference in the odds of
clinically significant PC (csPC, Gleason score >=7) (OR =3.78, 95% confidence
interval (CI):2.32-6.16, P <.001). pag showed a better predictive ability when compared to pi-rads>=3 and adjusted by other risk factors, including PSA
levels: AUC =0.981 vs AUC =0.704, p<.001. conclusion: pag was significantly associated with the risk of clinically significant pc and outperformed other well-established factors.< p>
  </.001.></.001).></=6)></p></details>
</details>
<details>
  <summary>53. <b>标题：Adv-Inpainting: Generating Natural and Transferable Adversarial Patch  via Attention-guided Feature Fusion</b></summary>
  <p><b>编号</b>：[169]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05320</p>
  <p><b>作者</b>：Yanjie Li,  Mingxing Duan,  Bin Xiao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：attack facial recognition, adversarial patches, facial recognition, adversarial, utilize additive noise</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rudimentary adversarial attacks utilize additive noise to attack facial
recognition (FR) models. However, because manipulating the total face is
impractical in the physical setting, most real-world FR attacks are based on
adversarial patches, which limit perturbations to a small area. Previous
adversarial patch attacks often resulted in unnatural patterns and clear
boundaries that were easily noticeable. In this paper, we argue that generating
adversarial patches with plausible content can result in stronger
transferability than using additive noise or directly sampling from the latent
space. To generate natural-looking and highly transferable adversarial patches,
we propose an innovative two-stage coarse-to-fine attack framework called
Adv-Inpainting. In the first stage, we propose an attention-guided StyleGAN
(Att-StyleGAN) that adaptively combines texture and identity features based on
the attention map to generate high-transferable and natural adversarial
patches. In the second stage, we design a refinement network with a new
boundary variance loss to further improve the coherence between the patch and
its surrounding area. Experiment results demonstrate that Adv-Inpainting is
stealthy and can produce adversarial patches with stronger transferability and
improved visual quality than previous adversarial patch attacks.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：RLSAC: Reinforcement Learning enhanced Sample Consensus for End-to-End  Robust Estimation</b></summary>
  <p><b>编号</b>：[170]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05318</p>
  <p><b>作者</b>：Chang Nie,  Guangming Wang,  Zhe Liu,  Luca Cavalli,  Marc Pollefeys,  Hesheng Wang</p>
  <p><b>备注</b>：Accepted by ICCV2023. Codes are released at this https URL</p>
  <p><b>关键词</b>：involves estimating model, estimating model parameters, RLSAC, noisy environments, involves estimating</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Robust estimation is a crucial and still challenging task, which involves
estimating model parameters in noisy environments. Although conventional
sampling consensus-based algorithms sample several times to achieve robustness,
these algorithms cannot use data features and historical information
effectively. In this paper, we propose RLSAC, a novel Reinforcement Learning
enhanced SAmple Consensus framework for end-to-end robust estimation. RLSAC
employs a graph neural network to utilize both data and memory features to
guide exploring directions for sampling the next minimum set. The feedback of
downstream tasks serves as the reward for unsupervised training. Therefore,
RLSAC can avoid differentiating to learn the features and the feedback of
downstream tasks for end-to-end robust estimation. In addition, RLSAC
integrates a state transition module that encodes both data and memory
features. Our experimental results demonstrate that RLSAC can learn from
features to gradually explore a better hypothesis. Through analysis, it is
apparent that RLSAC can be easily transferred to other sampling consensus-based
robust estimation tasks. To the best of our knowledge, RLSAC is also the first
method that uses reinforcement learning to sample consensus for end-to-end
robust estimation. We release our codes at this https URL.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Deep Semantic Graph Matching for Large-scale Outdoor Point Clouds  Registration</b></summary>
  <p><b>编号</b>：[173]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05314</p>
  <p><b>作者</b>：Shaocong Liu,  Tao Wang,  Yan Zhang,  Ruqin Zhou,  Li Li,  Chenguang Dai,  Yongsheng Zhang,  Hanyun Wang</p>
  <p><b>备注</b>：10 pages, 7 figures</p>
  <p><b>关键词</b>：point cloud registration, point cloud, current point cloud, cloud registration, cloud registration methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The current point cloud registration methods are mainly based on geometric
information and usually ignore the semantic information in the point clouds. In
this paper, we treat the point cloud registration problem as semantic instance
matching and registration task, and propose a deep semantic graph matching
method for large-scale outdoor point cloud registration. Firstly, the semantic
category labels of 3D point clouds are obtained by utilizing large-scale point
cloud semantic segmentation network. The adjacent points with the same category
labels are then clustered together by using Euclidean clustering algorithm to
obtain the semantic instances. Secondly, the semantic adjacency graph is
constructed based on the spatial adjacency relation of semantic instances.
Three kinds of high-dimensional features including geometric shape features,
semantic categorical features and spatial distribution features are learned
through graph convolutional network, and enhanced based on attention mechanism.
Thirdly, the semantic instance matching problem is modeled as an optimal
transport problem, and solved through an optimal matching layer. Finally,
according to the matched semantic instances, the geometric transformation
matrix between two point clouds is first obtained by SVD algorithm and then
refined by ICP algorithm. The experiments are cconducted on the KITTI Odometry
dataset, and the average relative translation error and average relative
rotation error of the proposed method are 6.6cm and 0.229° respectively.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：DAOT: Domain-Agnostically Aligned Optimal Transport for Domain-Adaptive  Crowd Counting</b></summary>
  <p><b>编号</b>：[174]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05311</p>
  <p><b>作者</b>：Huilin Zhu,  Jingling Yuan,  Xian Zhong,  Zhengwei Yang,  Zheng Wang,  Shengfeng He</p>
  <p><b>备注</b>：11 pages, 12 figures, 5 tables</p>
  <p><b>关键词</b>：Domain adaptation, existing domain adaptation, domain-agnostic factors, crowd counting, domain adaptation methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Domain adaptation is commonly employed in crowd counting to bridge the domain
gaps between different datasets. However, existing domain adaptation methods
tend to focus on inter-dataset differences while overlooking the
intra-differences within the same dataset, leading to additional learning
ambiguities. These domain-agnostic factors, e.g., density, surveillance
perspective, and scale, can cause significant in-domain variations, and the
misalignment of these factors across domains can lead to a drop in performance
in cross-domain crowd counting. To address this issue, we propose a
Domain-agnostically Aligned Optimal Transport (DAOT) strategy that aligns
domain-agnostic factors between domains. The DAOT consists of three steps.
First, individual-level differences in domain-agnostic factors are measured
using structural similarity (SSIM). Second, the optimal transfer (OT) strategy
is employed to smooth out these differences and find the optimal
domain-to-domain misalignment, with outlier individuals removed via a virtual
"dustbin" column. Third, knowledge is transferred based on the aligned
domain-agnostic factors, and the model is retrained for domain adaptation to
bridge the gap across domains. We conduct extensive experiments on five
standard crowd-counting benchmarks and demonstrate that the proposed method has
strong generalizability across diverse datasets. Our code will be available at:
this https URL.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Multi-Visual-Inertial System: Analysis,Calibration and Estimation</b></summary>
  <p><b>编号</b>：[177]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05303</p>
  <p><b>作者</b>：Yulin Yang,  Patrick Geneva,  Guoquan Huang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：rolling shutter cameras, study state estimation, rolling shutter, develop sensor fusion, study state</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we study state estimation of multi-visual-inertial systems
(MVIS) and develop sensor fusion algorithms to optimally fuse an arbitrary
number of asynchronous inertial measurement units (IMUs) or gyroscopes and
global and(or) rolling shutter cameras. We are especially interested in the
full calibration of the associated visual-inertial sensors, including the IMU
or camera intrinsics and the IMU-IMU(or camera) spatiotemporal extrinsics as
well as the image readout time of rolling-shutter cameras (if used). To this
end, we develop a new analytic combined IMU integration with intrinsics-termed
ACI3-to preintegrate IMU measurements, which is leveraged to fuse auxiliary
IMUs and(or) gyroscopes alongside a base IMU. We model the multi-inertial
measurements to include all the necessary inertial intrinsic and IMU-IMU
spatiotemporal extrinsic parameters, while leveraging IMU-IMU rigid-body
constraints to eliminate the necessity of auxiliary inertial poses and thus
reducing computational complexity. By performing observability analysis of
MVIS, we prove that the standard four unobservable directions remain - no
matter how many inertial sensors are used, and also identify, for the first
time, degenerate motions for IMU-IMU spatiotemporal extrinsics and auxiliary
inertial intrinsics. In addition to the extensive simulations that validate our
analysis and algorithms, we have built our own MVIS sensor rig and collected
over 25 real-world datasets to experimentally verify the proposed calibration
against the state-of-the-art calibration method such as Kalibr. We show that
the proposed MVIS calibration is able to achieve competing accuracy with
improved convergence and repeatability, which is open sourced to better benefit
the community.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Double-chain Constraints for 3D Human Pose Estimation in Images and  Videos</b></summary>
  <p><b>编号</b>：[178]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05298</p>
  <p><b>作者</b>：Hongbo Kang,  Yong Wang,  Mengyuan Liu,  Doudou Wu,  Peng Liu,  Wenming Yang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：poses lacking depth, lacking depth information, Local Constraint Module, Constraint Module, Feature Interaction Module</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reconstructing 3D poses from 2D poses lacking depth information is
particularly challenging due to the complexity and diversity of human motion.
The key is to effectively model the spatial constraints between joints to
leverage their inherent dependencies. Thus, we propose a novel model, called
Double-chain Graph Convolutional Transformer (DC-GCT), to constrain the pose
through a double-chain design consisting of local-to-global and global-to-local
chains to obtain a complex representation more suitable for the current human
pose. Specifically, we combine the advantages of GCN and Transformer and design
a Local Constraint Module (LCM) based on GCN and a Global Constraint Module
(GCM) based on self-attention mechanism as well as a Feature Interaction Module
(FIM). The proposed method fully captures the multi-level dependencies between
human body joints to optimize the modeling capability of the model. Moreover,
we propose a method to use temporal information into the single-frame model by
guiding the video sequence embedding through the joint embedding of the target
frame, with negligible increase in computational cost. Experimental results
demonstrate that DC-GCT achieves state-of-the-art performance on two
challenging datasets (Human3.6M and MPI-INF-3DHP). Notably, our model achieves
state-of-the-art performance on all action categories in the Human3.6M dataset
using detected 2D poses from CPN, and our code is available at:
this https URL.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Informative Scene Graph Generation via Debiasing</b></summary>
  <p><b>编号</b>：[183]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05286</p>
  <p><b>作者</b>：Lianli Gao,  Xinyu Lyu,  Yuyu Guo,  Yuxuan Hu,  Yuan-Fang Li,  Lu Xu,  Heng Tao Shen,  Jingkuan Song</p>
  <p><b>备注</b>：arXiv admin note: substantial text overlap with arXiv:2108.13129</p>
  <p><b>关键词</b>：Scene graph generation, visual relationship triplets, graph generation aims, detect visual relationship, generation aims</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Scene graph generation aims to detect visual relationship triplets, (subject,
predicate, object). Due to biases in data, current models tend to predict
common predicates, e.g. "on" and "at", instead of informative ones, e.g.
"standing on" and "looking at". This tendency results in the loss of precise
information and overall performance. If a model only uses "stone on road"
rather than "stone blocking road" to describe an image, it may be a grave
misunderstanding. We argue that this phenomenon is caused by two imbalances:
semantic space level imbalance and training sample level imbalance. For this
problem, we propose DB-SGG, an effective framework based on debiasing but not
the conventional distribution fitting. It integrates two components: Semantic
Debiasing (SD) and Balanced Predicate Learning (BPL), for these imbalances. SD
utilizes a confusion matrix and a bipartite graph to construct predicate
relationships. BPL adopts a random undersampling strategy and an ambiguity
removing strategy to focus on informative predicates. Benefiting from the
model-agnostic process, our method can be easily applied to SGG models and
outperforms Transformer by 136.3%, 119.5%, and 122.6% on mR@20 at three SGG
sub-tasks on the SGG-VG dataset. Our method is further verified on another
complex SGG dataset (SGG-GQA) and two downstream tasks (sentence-to-graph
retrieval and image captioning).</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Local-Global Information Interaction Debiasing for Dynamic Scene Graph  Generation</b></summary>
  <p><b>编号</b>：[190]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05274</p>
  <p><b>作者</b>：Xinyu Lyu,  Jingwei Liu,  Yuyu Guo,  Lianli Gao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：involves modeling, previous DynSGG models, local spatial-temporal information, dynamic scene graph, scene graph generation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The task of dynamic scene graph generation (DynSGG) aims to generate scene
graphs for given videos, which involves modeling the spatial-temporal
information in the video. However, due to the long-tailed distribution of
samples in the dataset, previous DynSGG models fail to predict the tail
predicates. We argue that this phenomenon is due to previous methods that only
pay attention to the local spatial-temporal information and neglect the
consistency of multiple frames. To solve this problem, we propose a novel
DynSGG model based on multi-task learning, DynSGG-MTL, which introduces the
local interaction information and global human-action interaction information.
The interaction between objects and frame features makes the model more fully
understand the visual context of the single image. Long-temporal human actions
supervise the model to generate multiple scene graphs that conform to the
global constraints and avoid the model being unable to learn the tail
predicates. Extensive experiments on Action Genome dataset demonstrate the
efficacy of our proposed framework, which not only improves the dynamic scene
graph generation but also alleviates the long-tail problem.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：TrainFors: A Large Benchmark Training Dataset for Image Manipulation  Detection and Localization</b></summary>
  <p><b>编号</b>：[194]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05264</p>
  <p><b>作者</b>：Soumyaroop Nandi,  Prem Natarajan,  Wael Abd-Almageed</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：image manipulation detection, detection and localization, training dataset, manipulation detection, training</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The evaluation datasets and metrics for image manipulation detection and
localization (IMDL) research have been standardized. But the training dataset
for such a task is still nonstandard. Previous researchers have used
unconventional and deviating datasets to train neural networks for detecting
image forgeries and localizing pixel maps of manipulated regions. For a fair
comparison, the training set, test set, and evaluation metrics should be
persistent. Hence, comparing the existing methods may not seem fair as the
results depend heavily on the training datasets as well as the model
architecture. Moreover, none of the previous works release the synthetic
training dataset used for the IMDL task. We propose a standardized benchmark
training dataset for image splicing, copy-move forgery, removal forgery, and
image enhancement forgery. Furthermore, we identify the problems with the
existing IMDL datasets and propose the required modifications. We also train
the state-of-the-art IMDL methods on our proposed TrainFors1 dataset for a fair
evaluation and report the actual performance of these methods under similar
conditions.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：Advancing Early Detection of Virus Yellows: Developing a Hybrid  Convolutional Neural Network for Automatic Aphid Counting in Sugar Beet  Fields</b></summary>
  <p><b>编号</b>：[196]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05257</p>
  <p><b>作者</b>：Xumin Gao,  Wenxin Xue,  Callum Lennox,  Mark Stevens,  Junfeng Gao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：transmit virus yellows, Aphids, virus yellows, efficient vectors, vectors to transmit</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Aphids are efficient vectors to transmit virus yellows in sugar beet fields.
Timely monitoring and control of their populations are thus critical to prevent
the large-scale outbreak of virus yellows. However, the manual counting of
aphids, which is the most common practice, is labor-intensive and
time-consuming. Additionally, two of the biggest challenges in aphid counting
are that aphids are small objects and their density distributions are varied in
different areas of the field. To address these challenges, we proposed a hybrid
automatic aphid counting network architecture which integrates the detection
network and the density map estimation network. When the distribution density
of aphids is low, it utilizes an improved Yolov5 to count aphids. Conversely,
when the distribution density of aphids is high, its witches to CSRNet to count
aphids. To the best of our knowledge, this is the first framework integrating
the detection network and the density map estimation network for counting
tasks. Through comparison experiments of counting aphids, it verified that our
proposed approach outperforms all other methods in counting aphids. It achieved
the lowest MAE and RMSE values for both the standard and high-density aphid
datasets: 2.93 and 4.01 (standard), and 34.19 and 38.66 (high-density),
respectively. Moreover, the AP of the improved Yolov5 is 5% higher than that of
the original Yolov5. Especially for extremely small aphids and densely
distributed aphids, the detection performance of the improved Yolov5 is
significantly better than the original Yolov5. This work provides an effective
early warning for the virus yellows risk caused by aphids in sugar beet fields,
offering protection for sugar beet growth and ensuring sugar beet yield. The
datasets and project code are released at:
this https URL.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Vector quantization loss analysis in VQGANs: a single-GPU ablation study  for image-to-image synthesis</b></summary>
  <p><b>编号</b>：[201]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05242</p>
  <p><b>作者</b>：Luv Verma,  Varun Mohan</p>
  <p><b>备注</b>：16 pages, 18 figures</p>
  <p><b>关键词</b>：Vector Quantized Generative, Quantized Generative, Vector Quantized, performs an ablation, ablation analysis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This study performs an ablation analysis of Vector Quantized Generative
Adversarial Networks (VQGANs), concentrating on image-to-image synthesis
utilizing a single NVIDIA A100 GPU. The current work explores the nuanced
effects of varying critical parameters including the number of epochs, image
count, and attributes of codebook vectors and latent dimensions, specifically
within the constraint of limited resources. Notably, our focus is pinpointed on
the vector quantization loss, keeping other hyperparameters and loss components
(GAN loss) fixed. This was done to delve into a deeper understanding of the
discrete latent space, and to explore how varying its size affects the
reconstruction. Though, our results do not surpass the existing benchmarks,
however, our findings shed significant light on VQGAN's behaviour for a smaller
dataset, particularly concerning artifacts, codebook size optimization, and
comparative analysis with Principal Component Analysis (PCA). The study also
uncovers the promising direction by introducing 2D positional encodings,
revealing a marked reduction in artifacts and insights into balancing clarity
and overfitting.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：Spatial Gated Multi-Layer Perceptron for Land Use and Land Cover Mapping</b></summary>
  <p><b>编号</b>：[203]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05235</p>
  <p><b>作者</b>：Ali Jamali,  Swalpa Kumar Roy,  Danfeng Hong,  Peter M Atkinson,  Pedram Ghamisi</p>
  <p><b>备注</b>：Submitted in IEEE</p>
  <p><b>关键词</b>：Convolutional Neural Networks, Neural Networks, Convolutional Neural, extraction of features, utilized extensively</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Convolutional Neural Networks (CNNs) are models that are utilized extensively
for the hierarchical extraction of features. Vision transformers (ViTs),
through the use of a self-attention mechanism, have recently achieved superior
modeling of global contextual information compared to CNNs. However, to realize
their image classification strength, ViTs require substantial training
datasets. Where the available training data are limited, current advanced
multi-layer perceptrons (MLPs) can provide viable alternatives to both deep
CNNs and ViTs. In this paper, we developed the SGU-MLP, a learning algorithm
that effectively uses both MLPs and spatial gating units (SGUs) for precise
land use land cover (LULC) mapping. Results illustrated the superiority of the
developed SGU-MLP classification algorithm over several CNN and CNN-ViT-based
models, including HybridSN, ResNet, iFormer, EfficientFormer and CoAtNet. The
proposed SGU-MLP algorithm was tested through three experiments in Houston,
USA, Berlin, Germany and Augsburg, Germany. The SGU-MLP classification model
was found to consistently outperform the benchmark CNN and CNN-ViT-based
algorithms. For example, for the Houston experiment, SGU-MLP significantly
outperformed HybridSN, CoAtNet, Efficientformer, iFormer and ResNet by
approximately 15%, 19%, 20%, 21%, and 25%, respectively, in terms of average
accuracy. The code will be made publicly available at
this https URL</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：Leveraging the Edge and Cloud for V2X-Based Real-Time Object Detection  in Autonomous Driving</b></summary>
  <p><b>编号</b>：[204]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05234</p>
  <p><b>作者</b>：Faisal Hawlader,  François Robinet,  Raphaël Frank</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：module influences core, core driving decisions, influences core driving, perception module influences, key element</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Environmental perception is a key element of autonomous driving because the
information received from the perception module influences core driving
decisions. An outstanding challenge in real-time perception for autonomous
driving lies in finding the best trade-off between detection quality and
latency. Major constraints on both computation and power have to be taken into
account for real-time perception in autonomous vehicles. Larger object
detection models tend to produce the best results, but are also slower at
runtime. Since the most accurate detectors cannot run in real-time locally, we
investigate the possibility of offloading computation to edge and cloud
platforms, which are less resource-constrained. We create a synthetic dataset
to train object detection models and evaluate different offloading strategies.
Using real hardware and network simulations, we compare different trade-offs
between prediction quality and end-to-end delay. Since sending raw frames over
the network implies additional transmission delays, we also explore the use of
JPEG and H.265 compression at varying qualities and measure their impact on
prediction metrics. We show that models with adequate compression can be run in
real-time on the cloud while outperforming local detection performance.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：SegMatch: A semi-supervised learning method for surgical instrument  segmentation</b></summary>
  <p><b>编号</b>：[205]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05232</p>
  <p><b>作者</b>：Meng Wei,  Charlie Budd,  Luis C. Garcia-Peraza-Herrera,  Reuben Dorent,  Miaojing Shi,  Tom Vercauteren</p>
  <p><b>备注</b>：preprint under review, 12 pages, 7 figures</p>
  <p><b>关键词</b>：computer assisted interventions, improve computer assisted, provide advanced surgical, advanced surgical assistance, Surgical instrument segmentation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Surgical instrument segmentation is recognised as a key enabler to provide
advanced surgical assistance and improve computer assisted interventions. In
this work, we propose SegMatch, a semi supervised learning method to reduce the
need for expensive annotation for laparoscopic and robotic surgical images.
SegMatch builds on FixMatch, a widespread semi supervised classification
pipeline combining consistency regularization and pseudo labelling, and adapts
it for the purpose of segmentation. In our proposed SegMatch, the unlabelled
images are weakly augmented and fed into the segmentation model to generate a
pseudo-label to enforce the unsupervised loss against the output of the model
for the adversarial augmented image on the pixels with a high confidence score.
Our adaptation for segmentation tasks includes carefully considering the
equivariance and invariance properties of the augmentation functions we rely
on. To increase the relevance of our augmentations, we depart from using only
handcrafted augmentations and introduce a trainable adversarial augmentation
strategy. Our algorithm was evaluated on the MICCAI Instrument Segmentation
Challenge datasets Robust-MIS 2019 and EndoVis 2017. Our results demonstrate
that adding unlabelled data for training purposes allows us to surpass the
performance of fully supervised approaches which are limited by the
availability of training data in these challenges. SegMatch also outperforms a
range of state-of-the-art semi-supervised learning semantic segmentation models
in different labelled to unlabelled data ratios.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：Hierarchical Representations for Spatio-Temporal Visual Attention  Modeling and Understanding</b></summary>
  <p><b>编号</b>：[216]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05189</p>
  <p><b>作者</b>：Miguel-Ángel Fernández-Torres</p>
  <p><b>备注</b>：PhD thesis</p>
  <p><b>关键词</b>：visual attention, visual attention modeling, spatio-temporal visual attention, attention, attention modeling</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This PhD. Thesis concerns the study and development of hierarchical
representations for spatio-temporal visual attention modeling and understanding
in video sequences. More specifically, we propose two computational models for
visual attention. First, we present a generative probabilistic model for
context-aware visual attention modeling and understanding. Secondly, we develop
a deep network architecture for visual attention modeling, which first
estimates top-down spatio-temporal visual attention, and ultimately serves for
modeling attention in the temporal domain.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：JutePestDetect: An Intelligent Approach for Jute Pest Identification  Using Fine-Tuned Transfer Learning</b></summary>
  <p><b>编号</b>：[219]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05179</p>
  <p><b>作者</b>：Md. Simul Hasan Talukder,  Mohammad Raziuddin Chowdhury,  Md Sakib Ullah Sourav,  Abdullah Al Rakin,  Shabbir Ahmed Shuvo,  Rejwan Bin Sulaiman,  Musarrat Saberin Nipun,  Muntarin Islam,  Mst Rumpa Islam,  Md Aminul Islam,  Zubaer Haque</p>
  <p><b>备注</b>：29 Pages, 7 Tables, 7 Figures, 5 Appendix</p>
  <p><b>关键词</b>：Gross Domestic Product, Asian countries, primary sources, sources of income, Domestic Product</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In certain Asian countries, Jute is one of the primary sources of income and
Gross Domestic Product (GDP) for the agricultural sector. Like many other
crops, Jute is prone to pest infestations, and its identification is typically
made visually in countries like Bangladesh, India, Myanmar, and China. In
addition, this method is time-consuming, challenging, and somewhat imprecise,
which poses a substantial financial risk. To address this issue, the study
proposes a high-performing and resilient transfer learning (TL) based
JutePestDetect model to identify jute pests at the early stage. Firstly, we
prepared jute pest dataset containing 17 classes and around 380 photos per pest
class, which were evaluated after manual and automatic pre-processing and
cleaning, such as background removal and resizing. Subsequently, five prominent
pre-trained models -DenseNet201, InceptionV3, MobileNetV2, VGG19, and ResNet50
were selected from a previous study to design the JutePestDetect model. Each
model was revised by replacing the classification layer with a global average
pooling layer and incorporating a dropout layer for regularization. To evaluate
the models performance, various metrics such as precision, recall, F1 score,
ROC curve, and confusion matrix were employed. These analyses provided
additional insights for determining the efficacy of the models. Among them, the
customized regularized DenseNet201-based proposed JutePestDetect model
outperformed the others, achieving an impressive accuracy of 99%. As a result,
our proposed method and strategy offer an enhanced approach to pest
identification in the case of Jute, which can significantly benefit farmers
worldwide.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：A Unified Interactive Model Evaluation for Classification, Object  Detection, and Instance Segmentation in Computer Vision</b></summary>
  <p><b>编号</b>：[222]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05168</p>
  <p><b>作者</b>：Changjian Chen,  Yukai Guo,  Fengyuan Tian,  Shilong Liu,  Weikai Yang,  Zhaowei Wang,  Jing Wu,  Hang Su,  Hanspeter Pfister,  Shixia Liu</p>
  <p><b>备注</b>：Accepted to IEEE VIS 2023</p>
  <p><b>关键词</b>：leaving a gap, Existing model evaluation, object detection, model evaluation, model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing model evaluation tools mainly focus on evaluating classification
models, leaving a gap in evaluating more complex models, such as object
detection. In this paper, we develop an open-source visual analysis tool,
Uni-Evaluator, to support a unified model evaluation for classification, object
detection, and instance segmentation in computer vision. The key idea behind
our method is to formulate both discrete and continuous predictions in
different tasks as unified probability distributions. Based on these
distributions, we develop 1) a matrix-based visualization to provide an
overview of model performance; 2) a table visualization to identify the
problematic data subsets where the model performs poorly; 3) a grid
visualization to display the samples of interest. These visualizations work
together to facilitate the model evaluation from a global overview to
individual samples. Two case studies demonstrate the effectiveness of
Uni-Evaluator in evaluating model performance and making informed improvements.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：Robust Object Modeling for Visual Tracking</b></summary>
  <p><b>编号</b>：[224]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05140</p>
  <p><b>作者</b>：Yidong Cai,  Jie Liu,  Jie Tang,  Gangshan Wu</p>
  <p><b>备注</b>：Accepted by ICCV2023. 19 pages. Code is available at this https URL</p>
  <p><b>关键词</b>：template, hybrid template features, core part, part of recent, hybrid template</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Object modeling has become a core part of recent tracking frameworks. Current
popular tackers use Transformer attention to extract the template feature
separately or interactively with the search region. However, separate template
learning lacks communication between the template and search regions, which
brings difficulty in extracting discriminative target-oriented features. On the
other hand, interactive template learning produces hybrid template features,
which may introduce potential distractors to the template via the cluttered
search regions. To enjoy the merits of both methods, we propose a robust object
modeling framework for visual tracking (ROMTrack), which simultaneously models
the inherent template and the hybrid template features. As a result, harmful
distractors can be suppressed by combining the inherent features of target
objects with search regions' guidance. Target-related features can also be
extracted using the hybrid template, thus resulting in a more robust object
modeling framework. To further enhance robustness, we present novel variation
tokens to depict the ever-changing appearance of target objects. Variation
tokens are adaptable to object deformation and appearance variations, which can
boost overall performance with negligible computation. Experiments show that
our ROMTrack sets a new state-of-the-art on multiple benchmarks.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：Discrepancy-based Active Learning for Weakly Supervised Bleeding  Segmentation in Wireless Capsule Endoscopy Images</b></summary>
  <p><b>编号</b>：[225]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05137</p>
  <p><b>作者</b>：Fan Bai,  Xiaohan Xing,  Yutian Shen,  Han Ma,  Max Q.-H. Meng</p>
  <p><b>备注</b>：accepted by MICCAI 2022</p>
  <p><b>关键词</b>：class activation maps, achieve bleeding segmentation, Wireless Capsule Endoscopy, Weakly supervised methods, low annotation efforts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Weakly supervised methods, such as class activation maps (CAM) based, have
been applied to achieve bleeding segmentation with low annotation efforts in
Wireless Capsule Endoscopy (WCE) images. However, the CAM labels tend to be
extremely noisy, and there is an irreparable gap between CAM labels and ground
truths for medical images. This paper proposes a new Discrepancy-basEd Active
Learning (DEAL) approach to bridge the gap between CAMs and ground truths with
a few annotations. Specifically, to liberate labor, we design a novel
discrepancy decoder model and a CAMPUS (CAM, Pseudo-label and groUnd-truth
Selection) criterion to replace the noisy CAMs with accurate model predictions
and a few human labels. The discrepancy decoder model is trained with a unique
scheme to generate standard, coarse and fine predictions. And the CAMPUS
criterion is proposed to predict the gaps between CAMs and ground truths based
on model divergence and CAM divergence. We evaluate our method on the WCE
dataset and results show that our method outperforms the state-of-the-art
active learning methods and reaches comparable performance to those trained
with full annotated datasets with only 10% of the training data labeled.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：High-Level Features Parallelization for Inference Cost Reduction Through  Selective Attention</b></summary>
  <p><b>编号</b>：[227]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05128</p>
  <p><b>作者</b>：André Peter Kelm,  Lucas Schmidt,  Tim Rolff,  Christian Wilms,  Ehsan Yaghoubi,  Simone Frintrop</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：select class-specific features, parallelize high-level features, selectively skip, skip or select, class-specific features</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we parallelize high-level features in deep networks to
selectively skip or select class-specific features to reduce inference costs.
This challenges most deep learning methods due to their limited ability to
efficiently and effectively focus on selected class-specific features without
retraining. We propose a serial-parallel hybrid architecture with serial
generic low-level features and parallel high-level features. This accounts for
the fact that many high-level features are class-specific rather than generic,
and has connections to recent neuroscientific findings that observe spatially
and contextually separated neural activations in the human brain. Our approach
provides the unique functionality of cutouts: selecting parts of the network to
focus on only relevant subsets of classes without requiring retraining. High
performance is maintained, but the cost of inference can be significantly
reduced. In some of our examples, up to $75\,\%$ of parameters are skipped and
$35\,\%$ fewer GMACs (Giga multiply-accumulate) operations are used as the
approach adapts to a change in task complexity. This is important for mobile,
industrial, and robotic applications where reducing the number of parameters,
the computational complexity, and thus the power consumption can be paramount.
Another unique functionality is that it allows processing to be directly
influenced by enhancing or inhibiting high-level class-specific features,
similar to the mechanism of selective attention in the human brain. This can be
relevant for cross-modal applications, the use of semantic prior knowledge,
and/or context-aware processing.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：Data-Free Model Extraction Attacks in the Context of Object Detection</b></summary>
  <p><b>编号</b>：[228]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05127</p>
  <p><b>作者</b>：Harshit Shah,  Aravindhan G,  Pavan Kulkarni,  Yuvaraj Govidarajulu,  Manojkumar Parmar</p>
  <p><b>备注</b>：Submitted to The 14th International Conference on Computer Vision Systems (ICVS 2023), to be published in Springer, Lecture Notes in Computer Science</p>
  <p><b>关键词</b>：machine learning models, target model, Generative Adversarial Nets, number of machine, machine learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A significant number of machine learning models are vulnerable to model
extraction attacks, which focus on stealing the models by using specially
curated queries against the target model. This task is well accomplished by
using part of the training data or a surrogate dataset to train a new model
that mimics a target model in a white-box environment. In pragmatic situations,
however, the target models are trained on private datasets that are
inaccessible to the adversary. The data-free model extraction technique
replaces this problem when it comes to using queries artificially curated by a
generator similar to that used in Generative Adversarial Nets. We propose for
the first time, to the best of our knowledge, an adversary black box attack
extending to a regression problem for predicting bounding box coordinates in
object detection. As part of our study, we found that defining a loss function
and using a novel generator setup is one of the key aspects in extracting the
target model. We find that the proposed model extraction method achieves
significant results by using reasonable queries. The discovery of this object
detection vulnerability will support future prospects for securing such models.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：Explicifying Neural Implicit Fields for Efficient Dynamic Human Avatar  Modeling via a Neural Explicit Surface</b></summary>
  <p><b>编号</b>：[231]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05112</p>
  <p><b>作者</b>：Ruiqi Zhang,  Jie Chen,  Qiang Wang</p>
  <p><b>备注</b>：8 pages, accepted by ACMMM 2023</p>
  <p><b>关键词</b>：implicit neural fields, Neural Explicit Surface, neural fields, efficiently modeling dynamic, implicit neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper proposes a technique for efficiently modeling dynamic humans by
explicifying the implicit neural fields via a Neural Explicit Surface (NES).
Implicit neural fields have advantages over traditional explicit
representations in modeling dynamic 3D content from sparse observations and
effectively representing complex geometries and appearances. Implicit neural
fields defined in 3D space, however, are expensive to render due to the need
for dense sampling during volumetric rendering. Moreover, their memory
efficiency can be further optimized when modeling sparse 3D space. To overcome
these issues, the paper proposes utilizing Neural Explicit Surface (NES) to
explicitly represent implicit neural fields, facilitating memory and
computational efficiency. To achieve this, the paper creates a fully
differentiable conversion between the implicit neural fields and the explicit
rendering interface of NES, leveraging the strengths of both implicit and
explicit approaches. This conversion enables effective training of the hybrid
representation using implicit methods and efficient rendering by integrating
the explicit rendering interface with a newly proposed rasterization-based
neural renderer that only incurs a texture color query once for the initial ray
interaction with the explicit surface, resulting in improved inference
efficiency. NES describes dynamic human geometries with pose-dependent neural
implicit surface deformation fields and their dynamic neural textures both in
2D space, which is a more memory-efficient alternative to traditional 3D
methods, reducing redundancy and computational load. The comprehensive
experiments show that NES performs similarly to previous 3D approaches, with
greatly improved rendering speed and reduced memory cost.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：Balancing Accuracy and Training Time in Federated Learning for Violence  Detection in Surveillance Videos: A Study of Neural Network Architectures</b></summary>
  <p><b>编号</b>：[233]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05106</p>
  <p><b>作者</b>：Pajon Quentin,  Serre Swan,  Wissocq Hugo,  Rabaud Léo,  Haidar Siba,  Yaacoub Antoun</p>
  <p><b>备注</b>：8 pages, 2 figures, FL-IJCAI'23</p>
  <p><b>关键词</b>：federated learning context, machine learning techniques, federated learning, paper presents, presents an investigation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents an investigation into machine learning techniques for
violence detection in videos and their adaptation to a federated learning
context. The study includes experiments with spatio-temporal features extracted
from benchmark video datasets, comparison of different methods, and proposal of
a modified version of the "Flow-Gated" architecture called "Diff-Gated."
Additionally, various machine learning techniques, including super-convergence
and transfer learning, are explored, and a method for adapting centralized
datasets to a federated learning context is developed. The research achieves
better accuracy results compared to state-of-the-art models by training the
best violence detection model in a federated learning context.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：Attention-based 3D CNN with Multi-layer Features for Alzheimer's Disease  Diagnosis using Brain Images</b></summary>
  <p><b>编号</b>：[238]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05655</p>
  <p><b>作者</b>：Yanteng Zhang,  Qizhi Teng,  Xiaohai He,  Tong Niu,  Lipei Zhang,  Yan Liu,  Chao Ren</p>
  <p><b>备注</b>：4 pages, 4 figures</p>
  <p><b>关键词</b>：PET imaging play, Structural MRI, MRI and PET, imaging play, play an important</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Structural MRI and PET imaging play an important role in the diagnosis of
Alzheimer's disease (AD), showing the morphological changes and glucose
metabolism changes in the brain respectively. The manifestations in the brain
image of some cognitive impairment patients are relatively inconspicuous, for
example, it still has difficulties in achieving accurate diagnosis through sMRI
in clinical practice. With the emergence of deep learning, convolutional neural
network (CNN) has become a valuable method in AD-aided diagnosis, but some CNN
methods cannot effectively learn the features of brain image, making the
diagnosis of AD still presents some challenges. In this work, we propose an
end-to-end 3D CNN framework for AD diagnosis based on ResNet, which integrates
multi-layer features obtained under the effect of the attention mechanism to
better capture subtle differences in brain images. The attention maps showed
our model can focus on key brain regions related to the disease diagnosis. Our
method was verified in ablation experiments with two modality images on 792
subjects from the ADNI database, where AD diagnostic accuracies of 89.71% and
91.18% were achieved based on sMRI and PET respectively, and also outperformed
some state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：Surface Masked AutoEncoder: Self-Supervision for Cortical Imaging Data</b></summary>
  <p><b>编号</b>：[244]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05474</p>
  <p><b>作者</b>：Simon Dahan,  Mariana da Silva,  Daniel Rueckert,  Emma C Robinson</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：vision transformer architectures, widely explored, addressing the lack, lack of inductive, inductive biases</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Self-supervision has been widely explored as a means of addressing the lack
of inductive biases in vision transformer architectures, which limits
generalisation when networks are trained on small datasets. This is crucial in
the context of cortical imaging, where phenotypes are complex and
heterogeneous, but the available datasets are limited in size. This paper
builds upon recent advancements in translating vision transformers to surface
meshes and investigates the potential of Masked AutoEncoder (MAE)
self-supervision for cortical surface learning. By reconstructing surface data
from a masked version of the input, the proposed method effectively models
cortical structure to learn strong representations that translate to improved
performance in downstream tasks. We evaluate our approach on cortical phenotype
regression using the developing Human Connectome Project (dHCP) and demonstrate
that pre-training leads to a 26\% improvement in performance, with an 80\%
faster convergence, compared to models trained from scratch. Furthermore, we
establish that pre-training vision transformer models on large datasets, such
as the UK Biobank (UKB), enables the acquisition of robust representations for
finetuning in low-data scenarios. Our code and pre-trained models are publicly
available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：Transforming Breast Cancer Diagnosis: Towards Real-Time Ultrasound to  Mammogram Conversion for Cost-Effective Diagnosis</b></summary>
  <p><b>编号</b>：[246]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05449</p>
  <p><b>作者</b>：Sahar Almahfouz Nasser,  Ashutosh Sharma,  Anmol Saraf,  Amruta Mahendra Parulekar,  Purvi Haria,  Amit Sethi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：imaging techniques, suited for intraoperative, intraoperative settings, images, ultrasound images</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Ultrasound (US) imaging is better suited for intraoperative settings because
it is real-time and more portable than other imaging techniques, such as
mammography. However, US images are characterized by lower spatial resolution
noise-like artifacts. This research aims to address these limitations by
providing surgeons with mammogram-like image quality in real-time from noisy US
images. Unlike previous approaches for improving US image quality that aim to
reduce artifacts by treating them as (speckle noise), we recognize their value
as informative wave interference pattern (WIP). To achieve this, we utilize the
Stride software to numerically solve the forward model, generating ultrasound
images from mammograms images by solving wave-equations. Additionally, we
leverage the power of domain adaptation to enhance the realism of the simulated
ultrasound images. Then, we utilize generative adversarial networks (GANs) to
tackle the inverse problem of generating mammogram-quality images from
ultrasound images. The resultant images have considerably more discernible
details than the original US images.</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction  from Low-Dose Sinograms</b></summary>
  <p><b>编号</b>：[250]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05365</p>
  <p><b>作者</b>：Jiaqi Cui,  Pinxian Zeng,  Xinyi Zeng,  Peng Wang,  Xi Wu,  Jiliu Zhou,  Yan Wang,  Dinggang Shen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：positron emission tomography, minimizing radiation exposure, obtain high-quality positron, high-quality positron emission, reconstructing standard-dose PET</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To obtain high-quality positron emission tomography (PET) images while
minimizing radiation exposure, various methods have been proposed for
reconstructing standard-dose PET (SPET) images from low-dose PET (LPET)
sinograms directly. However, current methods often neglect boundaries during
sinogram-to-image reconstruction, resulting in high-frequency distortion in the
frequency domain and diminished or fuzzy edges in the reconstructed images.
Furthermore, the convolutional architectures, which are commonly used, lack the
ability to model long-range non-local interactions, potentially leading to
inaccurate representations of global structures. To alleviate these problems,
we propose a transformer-based model that unites triple domains of sinogram,
image, and frequency for direct PET reconstruction, namely TriDo-Former.
Specifically, the TriDo-Former consists of two cascaded networks, i.e., a
sinogram enhancement transformer (SE-Former) for denoising the input LPET
sinograms and a spatial-spectral reconstruction transformer (SSR-Former) for
reconstructing SPET images from the denoised sinograms. Different from the
vanilla transformer that splits an image into 2D patches, based specifically on
the PET imaging mechanism, our SE-Former divides the sinogram into 1D
projection view angles to maintain its inner-structure while denoising,
preventing the noise in the sinogram from prorogating into the image domain.
Moreover, to mitigate high-frequency distortion and improve reconstruction
details, we integrate global frequency parsers (GFPs) into SSR-Former. The GFP
serves as a learnable frequency filter that globally adjusts the frequency
components in the frequency domain, enforcing the network to restore
high-frequency details resembling real SPET images. Validations on a clinical
dataset demonstrate that our TriDo-Former outperforms the state-of-the-art
methods qualitatively and quantitatively.</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：Shadow Datasets, New challenging datasets for Causal Representation  Learning</b></summary>
  <p><b>编号</b>：[253]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05707</p>
  <p><b>作者</b>：Jiageng Zhu,  Hanchen Xie,  Jianhua Wu,  Jiazhi Li,  Mahyar Khayatkhoei,  Mohamed E. Hussein,  Wael AbdAlmageed</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：representation learning, Discovering causal relations, CelebA, relations among semantic, emergent topic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Discovering causal relations among semantic factors is an emergent topic in
representation learning. Most causal representation learning (CRL) methods are
fully supervised, which is impractical due to costly labeling. To resolve this
restriction, weakly supervised CRL methods were introduced. To evaluate CRL
performance, four existing datasets, Pendulum, Flow, CelebA(BEARD) and
CelebA(SMILE), are utilized. However, existing CRL datasets are limited to
simple graphs with few generative factors. Thus we propose two new datasets
with a larger number of diverse generative factors and more sophisticated
causal graphs. In addition, current real datasets, CelebA(BEARD) and
CelebA(SMILE), the originally proposed causal graphs are not aligned with the
dataset distributions. Thus, we propose modifications to them.</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：Masked Diffusion as Self-supervised Representation Learner</b></summary>
  <p><b>编号</b>：[259]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05695</p>
  <p><b>作者</b>：Zixuan Pan,  Jianxu Chen,  Yiyu Shi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Denoising diffusion probabilistic, strong pixel-level representation, recently demonstrated, pixel-level representation learners, strong pixel-level</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Denoising diffusion probabilistic models have recently demonstrated
state-of-the-art generative performance and been used as strong pixel-level
representation learners. This paper decomposes the interrelation between the
generative capability and representation learning ability inherent in diffusion
models. We present masked diffusion model (MDM), a scalable self-supervised
representation learner that substitutes the conventional additive Gaussian
noise of traditional diffusion with a masking mechanism. Our proposed approach
convincingly surpasses prior benchmarks, demonstrating remarkable advancements
in both medical and natural image semantic segmentation tasks, particularly
within the context of few-shot scenario.</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：Hard No-Box Adversarial Attack on Skeleton-Based Human Action  Recognition with Skeleton-Motion-Informed Gradient</b></summary>
  <p><b>编号</b>：[263]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05681</p>
  <p><b>作者</b>：Zhengzhi Lu,  He Wang,  Ziyi Chang,  Guoan Yang,  Hubert P. H. Shum</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：skeleton-based human activity, human activity recognition, attack methods, skeleton-based human, human activity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, methods for skeleton-based human activity recognition have been
shown to be vulnerable to adversarial attacks. However, these attack methods
require either the full knowledge of the victim (i.e. white-box attacks),
access to training data (i.e. transfer-based attacks) or frequent model queries
(i.e. black-box attacks). All their requirements are highly restrictive,
raising the question of how detrimental the vulnerability is. In this paper, we
show that the vulnerability indeed exists. To this end, we consider a new
attack task: the attacker has no access to the victim model or the training
data or labels, where we coin the term hard no-box attack. Specifically, we
first learn a motion manifold where we define an adversarial loss to compute a
new gradient for the attack, named skeleton-motion-informed (SMI) gradient. Our
gradient contains information of the motion dynamics, which is different from
existing gradient-based attack methods that compute the loss gradient assuming
each dimension in the data is independent. The SMI gradient can augment many
gradient-based attack methods, leading to a new family of no-box attack
methods. Extensive evaluation and comparison show that our method imposes a
real threat to existing classifiers. They also show that the SMI gradient
improves the transferability and imperceptibility of adversarial samples in
both no-box and transfer-based black-box settings.</p>
  </details>
</details>
<details>
  <summary>83. <b>标题：2D3D-MATR: 2D-3D Matching Transformer for Detection-free Registration  between Images and Point Clouds</b></summary>
  <p><b>编号</b>：[266]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05667</p>
  <p><b>作者</b>：Minhao Li,  Zheng Qin,  Zhirui Gao,  Renjiao Yi,  Chengyang Zhu,  Kai Xu</p>
  <p><b>备注</b>：Accepted by ICCV 2023</p>
  <p><b>关键词</b>：inconsistent feature description, incompatible keypoint detection, cross-modality cases due, commonly adopted, feature description</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The commonly adopted detect-then-match approach to registration finds
difficulties in the cross-modality cases due to the incompatible keypoint
detection and inconsistent feature description. We propose, 2D3D-MATR, a
detection-free method for accurate and robust registration between images and
point clouds. Our method adopts a coarse-to-fine pipeline where it first
computes coarse correspondences between downsampled patches of the input image
and the point cloud and then extends them to form dense correspondences between
pixels and points within the patch region. The coarse-level patch matching is
based on transformer which jointly learns global contextual constraints with
self-attention and cross-modality correlations with cross-attention. To resolve
the scale ambiguity in patch matching, we construct a multi-scale pyramid for
each image patch and learn to find for each point patch the best matching image
patch at a proper resolution level. Extensive experiments on two public
benchmarks demonstrate that 2D3D-MATR outperforms the previous state-of-the-art
P2-Net by around $20$ percentage points on inlier ratio and over $10$ points on
registration recall. Our code and models are available at
\url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>84. <b>标题：AD-CLIP: Adapting Domains in Prompt Space Using CLIP</b></summary>
  <p><b>编号</b>：[268]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05659</p>
  <p><b>作者</b>：Mainak Singha,  Harsh Pal,  Ankit Jha,  Biplab Banerjee</p>
  <p><b>备注</b>：10 pages, 8 figures, 4 tables. Accepted at OOD-CV, ICCV Workshop, 2023</p>
  <p><b>关键词</b>：shown impressive performance, shown impressive, impressive performance, struggle to generalize, deep learning models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Although deep learning models have shown impressive performance on supervised
learning tasks, they often struggle to generalize well when the training
(source) and test (target) domains differ. Unsupervised domain adaptation (DA)
has emerged as a popular solution to this problem. However, current DA
techniques rely on visual backbones, which may lack semantic richness. Despite
the potential of large-scale vision-language foundation models like CLIP, their
effectiveness for DA has yet to be fully explored. To address this gap, we
introduce AD-CLIP, a domain-agnostic prompt learning strategy for CLIP that
aims to solve the DA problem in the prompt space. We leverage the frozen vision
backbone of CLIP to extract both image style (domain) and content information,
which we apply to learn prompt tokens. Our prompts are designed to be
domain-invariant and class-generalizable, by conditioning prompt learning on
image style and content features simultaneously. We use standard supervised
contrastive learning in the source domain, while proposing an entropy
minimization strategy to align domains in the embedding space given the target
domain data. We also consider a scenario where only target domain samples are
available during testing, without any source domain data, and propose a
cross-domain style mapping network to hallucinate domain-agnostic tokens. Our
extensive experiments on three benchmark DA datasets demonstrate the
effectiveness of AD-CLIP compared to existing literature.</p>
  </details>
</details>
<h1>自然语言处理</h1>
<details>
  <summary>1. <b>标题：EXPRESSO: A Benchmark and Analysis of Discrete Expressive Speech  Resynthesis</b></summary>
  <p><b>编号</b>：[11]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05725</p>
  <p><b>作者</b>：Tu Anh Nguyen,  Wei-Ning Hsu,  Antony D'Avirro,  Bowen Shi,  Itai Gat,  Maryam Fazel-Zarani,  Tal Remez,  Jade Copet,  Gabriel Synnaeve,  Michael Hassid,  Felix Kreuk,  Yossi Adi,  Emmanuel Dupoux</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：capture expressive aspects, high-quality speech based, non-verbal vocalization, Recent work, hard to transcribe</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent work has shown that it is possible to resynthesize high-quality speech
based, not on text, but on low bitrate discrete units that have been learned in
a self-supervised fashion and can therefore capture expressive aspects of
speech that are hard to transcribe (prosody, voice styles, non-verbal
vocalization). The adoption of these methods is still limited by the fact that
most speech synthesis datasets are read, severely limiting spontaneity and
expressivity. Here, we introduce Expresso, a high-quality expressive speech
dataset for textless speech synthesis that includes both read speech and
improvised dialogues rendered in 26 spontaneous expressive styles. We
illustrate the challenges and potentials of this dataset with an expressive
resynthesis benchmark where the task is to encode the input in low-bitrate
units and resynthesize it in a target voice while preserving content and style.
We evaluate resynthesis quality with automatic metrics for different
self-supervised discrete encoders, and explore tradeoffs between quality,
bitrate and invariance to speaker and style. All the dataset, evaluation
metrics and baseline models are open source</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：A Preliminary Study of the Intrinsic Relationship between Complexity and  Alignment</b></summary>
  <p><b>编号</b>：[22]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05696</p>
  <p><b>作者</b>：Yingxiu Zhao,  Bowen Yu,  Binyuan Hui,  Haiyang Yu,  Fei Huang,  Yongbin Li,  Nevin L. Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, yielded remarkable success, Training large language, instruction data, open-domain instruction data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Training large language models (LLMs) with open-domain instruction data has
yielded remarkable success in aligning to end tasks and user preferences.
Extensive research has highlighted that enhancing the quality and diversity of
instruction data consistently improves performance. However, the impact of data
complexity, as a crucial metric, remains relatively unexplored in three
aspects: (1) scaling law, where the sustainability of performance improvements
with increasing complexity is uncertain, (2) additional tokens, whether the
improvement brought by complexity comes from introducing more training tokens,
and (3) curriculum tuning, where the potential advantages of incorporating
instructions ranging from easy to difficult are not yet fully understood. In
this paper, we propose \textit{tree-instruct} to systematically enhance the
complexity of instruction data in a controllable manner. This approach adds a
specified number of nodes into the instruction semantic tree, yielding new
instruction data based on the modified tree. By adjusting the number of added
nodes, we can control the difficulty level in the modified instruction data.
Our preliminary experiments reveal the following insights: (1) Increasing
complexity consistently leads to sustained performance improvements. For
instance, using 1,000 instruction data and 10 nodes resulted in a substantial
24\% increase in win rate. (2) Under the same token budget, a few complex
instructions outperform diverse yet simple instructions. (3) Curriculum
instruction tuning might not yield the anticipated results; focusing on
increasing complexity appears to be the key.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Finding Already Debunked Narratives via Multistage Retrieval: Enabling  Cross-Lingual, Cross-Dataset and Zero-Shot Learning</b></summary>
  <p><b>编号</b>：[28]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05680</p>
  <p><b>作者</b>：Iknoor Singh,  Carolina Scarton,  Xingyi Song,  Kalina Bontcheva</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：debunked narratives aims, aims to detect, detect stories, debunked narratives, Transformer models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The task of retrieving already debunked narratives aims to detect stories
that have already been fact-checked. The successful detection of claims that
have already been debunked not only reduces the manual efforts of professional
fact-checkers but can also contribute to slowing the spread of misinformation.
Mainly due to the lack of readily available data, this is an understudied
problem, particularly when considering the cross-lingual task, i.e. the
retrieval of fact-checking articles in a language different from the language
of the online post being checked. This paper fills this gap by (i) creating a
novel dataset to enable research on cross-lingual retrieval of already debunked
narratives, using tweets as queries to a database of fact-checking articles;
(ii) presenting an extensive experiment to benchmark fine-tuned and
off-the-shelf multilingual pre-trained Transformer models for this task; and
(iii) proposing a novel multistage framework that divides this cross-lingual
debunk retrieval task into refinement and re-ranking stages. Results show that
the task of cross-lingual retrieval of already debunked narratives is
challenging and off-the-shelf Transformer models fail to outperform a strong
lexical-based baseline (BM25). Nevertheless, our multistage retrieval framework
is robust, outperforming BM25 in most scenarios and enabling cross-domain and
zero-shot learning, without significantly harming the model's performance.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：AST-MHSA : Code Summarization using Multi-Head Self-Attention</b></summary>
  <p><b>编号</b>：[39]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05646</p>
  <p><b>作者</b>：Yeshwanth Nagaraj,  Ujjwal Gupta</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Code summarization aims, Abstract Syntax Tree, source code, AST, summarization aims</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Code summarization aims to generate concise natural language descriptions for
source code. The prevailing approaches adopt transformer-based encoder-decoder
architectures, where the Abstract Syntax Tree (AST) of the source code is
utilized for encoding structural information. However, ASTs are much longer
than the corresponding source code, and existing methods ignore this size
constraint by directly feeding the entire linearized AST into the encoders.
This simplistic approach makes it challenging to extract truly valuable
dependency relations from the overlong input sequence and leads to significant
computational overhead due to self-attention applied to all nodes in the AST.
To address this issue effectively and efficiently, we present a model,
AST-MHSA that uses multi-head attention to extract the important semantic
information from the AST. The model consists of two main components: an encoder
and a decoder. The encoder takes as input the abstract syntax tree (AST) of the
code and generates a sequence of hidden states. The decoder then takes these
hidden states as input and generates a natural language summary of the code.
The multi-head attention mechanism allows the model to learn different
representations of the input code, which can be combined to generate a more
comprehensive summary. The model is trained on a dataset of code and summaries,
and the parameters of the model are optimized to minimize the loss between the
generated summaries and the ground-truth summaries.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：IIHT: Medical Report Generation with Image-to-Indicator Hierarchical  Transformer</b></summary>
  <p><b>编号</b>：[45]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05633</p>
  <p><b>作者</b>：Keqiang Fan,  Xiaohao Cai,  Mahesan Niranjan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：medical report generation, Automated medical report, medical report, report generation, increasingly important</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automated medical report generation has become increasingly important in
medical analysis. It can produce computer-aided diagnosis descriptions and thus
significantly alleviate the doctors' work. Inspired by the huge success of
neural machine translation and image captioning, various deep learning methods
have been proposed for medical report generation. However, due to the inherent
properties of medical data, including data imbalance and the length and
correlation between report sequences, the generated reports by existing methods
may exhibit linguistic fluency but lack adequate clinical accuracy. In this
work, we propose an image-to-indicator hierarchical transformer (IIHT)
framework for medical report generation. It consists of three modules, i.e., a
classifier module, an indicator expansion module and a generator module. The
classifier module first extracts image features from the input medical images
and produces disease-related indicators with their corresponding states. The
disease-related indicators are subsequently utilised as input for the indicator
expansion module, incorporating the "data-text-data" strategy. The
transformer-based generator then leverages these extracted features along with
image features as auxiliary information to generate final reports. Furthermore,
the proposed IIHT method is feasible for radiologists to modify disease
indicators in real-world scenarios and integrate the operations into the
indicator expansion module for fluent and accurate medical report generation.
Extensive experiments and comparisons with state-of-the-art methods under
various evaluation metrics demonstrate the great performance of the proposed
method.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：LASIGE and UNICAGE solution to the NASA LitCoin NLP Competition</b></summary>
  <p><b>编号</b>：[52]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05609</p>
  <p><b>作者</b>：Pedro Ruas,  Diana F. Sousa,  André Neves,  Carlos Cruz,  Francisco M. Couto</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Natural Language Processing, Biomedical Natural Language, Natural Language, Language Processing, frequently due</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Biomedical Natural Language Processing (NLP) tends to become cumbersome for
most researchers, frequently due to the amount and heterogeneity of text to be
processed. To address this challenge, the industry is continuously developing
highly efficient tools and creating more flexible engineering solutions. This
work presents the integration between industry data engineering solutions for
efficient data processing and academic systems developed for Named Entity
Recognition (LasigeUnicage\_NER) and Relation Extraction (BiOnt). Our design
reflects an integration of those components with external knowledge in the form
of additional training data from other datasets and biomedical ontologies. We
used this pipeline in the 2022 LitCoin NLP Challenge, where our team
LasigeUnicage was awarded the 7th Prize out of approximately 200 participating
teams, reflecting a successful collaboration between the academia (LASIGE) and
the industry (Unicage). The software supporting this work is available at
\url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：You Only Prompt Once: On the Capabilities of Prompt Learning on Large  Language Models to Tackle Toxic Content</b></summary>
  <p><b>编号</b>：[57]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05596</p>
  <p><b>作者</b>：Xinlei He,  Savvas Zannettou,  Yun Shen,  Yang Zhang</p>
  <p><b>备注</b>：To Appear in the 45th IEEE Symposium on Security and Privacy, May 20-23, 2024</p>
  <p><b>关键词</b>：user experience online, toxic content, Toxic Span Detection, toxic content online, adverse effects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The spread of toxic content online is an important problem that has adverse
effects on user experience online and in our society at large. Motivated by the
importance and impact of the problem, research focuses on developing solutions
to detect toxic content, usually leveraging machine learning (ML) models
trained on human-annotated datasets. While these efforts are important, these
models usually do not generalize well and they can not cope with new trends
(e.g., the emergence of new toxic terms). Currently, we are witnessing a shift
in the approach to tackling societal issues online, particularly leveraging
large language models (LLMs) like GPT-3 or T5 that are trained on vast corpora
and have strong generalizability. In this work, we investigate how we can use
LLMs and prompt learning to tackle the problem of toxic content, particularly
focusing on three tasks; 1) Toxicity Classification, 2) Toxic Span Detection,
and 3) Detoxification. We perform an extensive evaluation over five model
architectures and eight datasets demonstrating that LLMs with prompt learning
can achieve similar or even better performance compared to models trained on
these specific tasks. We find that prompt learning achieves around 10\%
improvement in the toxicity classification task compared to the baselines,
while for the toxic span detection task we find better performance to the best
baseline (0.643 vs. 0.640 in terms of $F_1$-score). Finally, for the
detoxification task, we find that prompt learning can successfully reduce the
average toxicity score (from 0.775 to 0.213) while preserving semantic meaning.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Do Language Models Refer?</b></summary>
  <p><b>编号</b>：[67]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05576</p>
  <p><b>作者</b>：Matthew Mandelkern,  Tal Linzen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：language models, language, LMs, models, coherent sentences</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>What do language models (LMs) do with language? Everyone agrees that they
produce sequences of (mostly) coherent sentences. But are they saying anything
with those strings or simply babbling in a convincing simulacrum of language
use? This is a vague question, and there are many ways of making it precise.
Here we will address one aspect of the question, namely, whether LMs' words
refer: that is, whether the outputs of LMs achieve "word-to-world" connections.
There is prima facie reason to think they do not since LMs do not interact with
the world in the way that ordinary language users do. Drawing on insights from
the externalist tradition in philosophy of language, we argue that appearances
are misleading and that there is good reason to think that LMs can refer.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Exploring Linguistic Similarity and Zero-Shot Learning for Multilingual  Translation of Dravidian Languages</b></summary>
  <p><b>编号</b>：[69]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05574</p>
  <p><b>作者</b>：Danish Ebadulla,  Rahul Raman,  S. Natarajan,  Hridhay Kiran Shetty,  Ashish Harish Shenoy</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：high compute requirements, compute requirements, high compute, increased training time, increased training</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Current research in zero-shot translation is plagued by several issues such
as high compute requirements, increased training time and off target
translations. Proposed remedies often come at the cost of additional data or
compute requirements. Pivot based neural machine translation is preferred over
a single-encoder model for most settings despite the increased training and
evaluation time. In this work, we overcome the shortcomings of zero-shot
translation by taking advantage of transliteration and linguistic similarity.
We build a single encoder-decoder neural machine translation system for
Dravidian-Dravidian multilingual translation and perform zero-shot translation.
We compare the data vs zero-shot accuracy tradeoff and evaluate the performance
of our vanilla method against the current state of the art pivot based method.
We also test the theory that morphologically rich languages require large
vocabularies by restricting the vocabulary using an optimal transport based
technique. Our model manages to achieves scores within 3 BLEU of large-scale
pivot-based models when it is trained on 50\% of the language directions.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Bringing order into the realm of Transformer-based language models for  artificial intelligence and law</b></summary>
  <p><b>编号</b>：[97]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05502</p>
  <p><b>作者</b>：Candida M. Greco,  Andrea Tagarelli</p>
  <p><b>备注</b>：Accepted for publication with Artificial Intelligence and Law, Springer Nature</p>
  <p><b>关键词</b>：require natural language, natural language processing, Transformer-based language models, Transformer-based language, natural language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transformer-based language models (TLMs) have widely been recognized to be a
cutting-edge technology for the successful development of deep-learning-based
solutions to problems and applications that require natural language processing
and understanding. Like for other textual domains, TLMs have indeed pushed the
state-of-the-art of AI approaches for many tasks of interest in the legal
domain. Despite the first Transformer model being proposed about six years ago,
there has been a rapid progress of this technology at an unprecedented rate,
whereby BERT and related models represent a major reference, also in the legal
domain. This article provides the first systematic overview of TLM-based
methods for AI-driven problems and tasks in the legal sphere. A major goal is
to highlight research advances in this field so as to understand, on the one
hand, how the Transformers have contributed to the success of AI in supporting
legal processes, and on the other hand, what are the current limitations and
opportunities for further research development.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：LLM As DBA</b></summary>
  <p><b>编号</b>：[107]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05481</p>
  <p><b>作者</b>：Xuanhe Zhou,  Guoliang Li,  Zhiyuan Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ensure data availability, play a crucial, role in managing, maintaining and optimizing, data availability</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Database administrators (DBAs) play a crucial role in managing, maintaining
and optimizing a database system to ensure data availability, performance, and
reliability. However, it is hard and tedious for DBAs to manage a large number
of database instances (e.g., millions of instances on the cloud databases).
Recently large language models (LLMs) have shown great potential to understand
valuable documents and accordingly generate reasonable answers. Thus, we
propose D-Bot, a LLM-based database administrator that can continuously acquire
database maintenance experience from textual sources, and provide reasonable,
well-founded, in-time diagnosis and optimization advice for target databases.
This paper presents a revolutionary LLM-centric framework for database
maintenance, including (i) database maintenance knowledge detection from
documents and tools, (ii) tree of thought reasoning for root cause analysis,
and (iii) collaborative diagnosis among multiple LLMs. Our preliminary
experimental results that D-Bot can efficiently and effectively diagnose the
root causes and our code is available at
this http URL.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Exploring Machine Learning and Transformer-based Approaches for  Deceptive Text Classification: A Comparative Analysis</b></summary>
  <p><b>编号</b>：[110]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05476</p>
  <p><b>作者</b>：Anusuya Krishnan</p>
  <p><b>备注</b>：12 pages, 8 figures</p>
  <p><b>关键词</b>：natural language processing, Deceptive text classification, Deceptive text, text classification, critical task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deceptive text classification is a critical task in natural language
processing that aims to identify deceptive o fraudulent content. This study
presents a comparative analysis of machine learning and transformer-based
approaches for deceptive text classification. We investigate the effectiveness
of traditional machine learning algorithms and state-of-the-art transformer
models, such as BERT, XLNET, DistilBERT, and RoBERTa, in detecting deceptive
text. A labeled dataset consisting of deceptive and non-deceptive texts is used
for training and evaluation purposes. Through extensive experimentation, we
compare the performance metrics, including accuracy, precision, recall, and F1
score, of the different approaches. The results of this study shed light on the
strengths and limitations of machine learning and transformer-based methods for
deceptive text classification, enabling researchers and practitioners to make
informed decisions when dealing with deceptive content.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：WeaverBird: Empowering Financial Decision-Making with Large Language  Model, Knowledge Base, and Search Engine</b></summary>
  <p><b>编号</b>：[150]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05361</p>
  <p><b>作者</b>：Siqiao Xue,  Fan Zhou,  Yi Xu,  Hongyu Zhao,  Shuo Xie,  Caigao Jiang,  James Zhang,  Jun Zhou,  Peng Xu,  Dacheng Xiu,  Hongyuan Mei</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：intelligent dialogue system, dialogue system designed, system designed specifically, present WeaverBird, finance domain</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present WeaverBird, an intelligent dialogue system designed specifically
for the finance domain. Our system harnesses a large language model of GPT
architecture that has been tuned using extensive corpora of finance-related
text. As a result, our system possesses the capability to understand complex
financial queries, such as "How should I manage my investments during
inflation?", and provide informed responses. Furthermore, our system
incorporates a local knowledge base and a search engine to retrieve relevant
information. The final responses are conditioned on the search results and
include proper citations to the sources, thus enjoying an enhanced credibility.
Through a range of finance-related questions, we have demonstrated the superior
performance of our system compared to other models. To experience our system
firsthand, users can interact with our live demo at
this https URL, as well as watch our 2-min video illustration at
this https URL.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Metacognitive Prompting Improves Understanding in Large Language Models</b></summary>
  <p><b>编号</b>：[159]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05342</p>
  <p><b>作者</b>：Yuqing Wang,  Yun Zhao</p>
  <p><b>备注</b>：9 pages, in submission</p>
  <p><b>关键词</b>：effective prompt design, Large Language Models, Large Language, largely influenced, prompt design</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In Large Language Models (LLMs), there have been consistent advancements in
task-specific performance, largely influenced by effective prompt design. While
recent research on prompting has enhanced the reasoning capabilities of LLMs, a
gap remains in further improving their understanding abilities. In this study,
we introduce metacognitive prompting (MP), a strategy inspired by human
introspective reasoning processes. Using MP, LLMs undergo a systematic series
of structured, self-aware evaluations, drawing on both their vast inherent
knowledge and new insights. Our experiments involve five prevalent LLMs:
Llama2, Vicuna, PaLM, GPT-3.5, and GPT-4, all of which span various general
natural language understanding (NLU) tasks from the GLUE and SuperGLUE
benchmarks. Results indicate that, although GPT-4 consistently excels in most
tasks, PaLM, when equipped with MP, approaches its performance level.
Furthermore, across models and datasets, MP consistently outperforms existing
prompting methods, including standard and chain-of-thought prompting. This
study underscores the potential to amplify the understanding abilities of LLMs
and highlights the benefits of mirroring human introspective reasoning in NLU
tasks.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Classification of Human- and AI-Generated Texts: Investigating Features  for ChatGPT</b></summary>
  <p><b>编号</b>：[160]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05341</p>
  <p><b>作者</b>：Lorenz Mindner,  Tim Schlippe,  Kristina Schaaff</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：generative AIs, wide public, AIs like ChatGPT, text, Recently</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, generative AIs like ChatGPT have become available to the wide
public. These tools can for instance be used by students to generate essays or
whole theses. But how does a teacher know whether a text is written by a
student or an AI? In our work, we explore traditional and new features to (1)
detect text generated by AI from scratch and (2) text rephrased by AI. Since we
found that classification is more difficult when the AI has been instructed to
create the text in a way that a human would not recognize that it was generated
by an AI, we also investigate this more advanced case. For our experiments, we
produced a new text corpus covering 10 school topics. Our best systems to
classify basic and advanced human-generated/AI-generated texts have F1-scores
of over 96%. Our best systems for classifying basic and advanced
human-generated/AI-rephrased texts have F1-scores of more than 78%. The systems
use a combination of perplexity, semantic, list lookup, error-based,
readability, AI feedback, and text vector features. Our results show that the
new features substantially help to improve the performance of many classifiers.
Our best basic text rephrasing detection system even outperforms GPTZero by
183.8% relative in F1-score.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Developing an Informal-Formal Persian Corpus</b></summary>
  <p><b>编号</b>：[162]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05336</p>
  <p><b>作者</b>：Vahide Tajalli,  Fateme Kalantari,  Mehrnoush Shamsfard</p>
  <p><b>备注</b>：16 pages, 1 Figure and 3 tables</p>
  <p><b>关键词</b>：written language frequently, social media, casual conversations, emails and text, text messages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Informal language is a style of spoken or written language frequently used in
casual conversations, social media, weblogs, emails and text messages. In
informal writing, the language faces some lexical and/or syntactic changes
varying among different languages. Persian is one of the languages with many
differences between its formal and informal styles of writing, thus developing
informal language processing tools for this language seems necessary. Such a
converter needs a large aligned parallel corpus of colloquial-formal sentences
which can be useful for linguists to extract a regulated grammar and
orthography for colloquial Persian as is done for the formal language. In this
paper we explain our methodology in building a parallel corpus of 50,000
sentence pairs with alignments in the word/phrase level. The sentences were
attempted to cover almost all kinds of lexical and syntactic changes between
informal and formal Persian, therefore both methods of exploring and collecting
from the different resources of informal scripts and following the phonological
and morphological patterns of changes were applied to find as much instances as
possible. The resulting corpus has about 530,000 alignments and a dictionary
containing 49,397 word and phrase pairs.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Few-Shot Data-to-Text Generation via Unified Representation and  Multi-Source Learning</b></summary>
  <p><b>编号</b>：[171]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05317</p>
  <p><b>作者</b>：Alexander Hanbo Li,  Mingyue Shang,  Evangelia Spiliopoulou,  Jie Ma,  Patrick Ng,  Zhiguo Wang,  Bonan Min,  William Wang,  Kathleen McKeown,  Vittorio Castelli,  Dan Roth,  Bing Xiang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：addresses the limitations, limitations of existing, primarily focus, focus on specific, specific types</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a novel approach for structured data-to-text generation that
addresses the limitations of existing methods that primarily focus on specific
types of structured data. Our proposed method aims to improve performance in
multi-task training, zero-shot and few-shot scenarios by providing a unified
representation that can handle various forms of structured data such as tables,
knowledge graph triples, and meaning representations. We demonstrate that our
proposed approach can effectively adapt to new structured forms, and can
improve performance in comparison to current methods. For example, our method
resulted in a 66% improvement in zero-shot BLEU scores when transferring models
trained on table inputs to a knowledge graph dataset. Our proposed method is an
important step towards a more general data-to-text generation framework.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Investigating disaster response through social media data and the  Susceptible-Infected-Recovered (SIR) model: A case study of 2020 Western U.S.  wildfire season</b></summary>
  <p><b>编号</b>：[185]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05281</p>
  <p><b>作者</b>：Zihui Ma,  Lingyao Li,  Libby Hemphill,  Gregory B. Baecher</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Effective disaster response, critical for affected, social media, Effective disaster, affected communities</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Effective disaster response is critical for affected communities. Responders
and decision-makers would benefit from reliable, timely measures of the issues
impacting their communities during a disaster, and social media offers a
potentially rich data source. Social media can reflect public concerns and
demands during a disaster, offering valuable insights for decision-makers to
understand evolving situations and optimize resource allocation. We used
Bidirectional Encoder Representations from Transformers (BERT) topic modeling
to cluster topics from Twitter data. Then, we conducted a temporal-spatial
analysis to examine the distribution of these topics across different regions
during the 2020 western U.S. wildfire season. Our results show that Twitter
users mainly focused on three topics:"health impact," "damage," and
"evacuation." We used the Susceptible-Infected-Recovered (SIR) theory to
explore the magnitude and velocity of topic diffusion on Twitter. The results
displayed a clear relationship between topic trends and wildfire propagation
patterns. The estimated parameters obtained from the SIR model in selected
cities revealed that residents exhibited a high level of several concerns
during the wildfire. Our study details how the SIR model and topic modeling
using social media data can provide decision-makers with a quantitative
approach to measure disaster response and support their decision-making
processes.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：A Novel Self-training Approach for Low-resource Speech Recognition</b></summary>
  <p><b>编号</b>：[192]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05269</p>
  <p><b>作者</b>：Satwinder Singh,  Feng Hou,  Ruili Wang</p>
  <p><b>备注</b>：Accepted to Interspeech 2023</p>
  <p><b>关键词</b>：automatic speech recognition, Common Voice Punjabi, low-resource settings, Punjabi, Voice Punjabi dataset</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we propose a self-training approach for automatic speech
recognition (ASR) for low-resource settings. While self-training approaches
have been extensively developed and evaluated for high-resource languages such
as English, their applications to low-resource languages like Punjabi have been
limited, despite the language being spoken by millions globally. The scarcity
of annotated data has hindered the development of accurate ASR systems,
especially for low-resource languages (e.g., Punjabi and Māori languages). To
address this issue, we propose an effective self-training approach that
generates highly accurate pseudo-labels for unlabeled low-resource speech. Our
experimental analysis demonstrates that our approach significantly improves
word error rate, achieving a relative improvement of 14.94% compared to a
baseline model across four real speech datasets. Further, our proposed approach
reports the best results on the Common Voice Punjabi dataset.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Decoding Layer Saliency in Language Transformers</b></summary>
  <p><b>编号</b>：[209]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05219</p>
  <p><b>作者</b>：Elizabeth M. Hou,  Gregory Castanon</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large-scale language models, language models applied, identifying textual saliency, introduce a strategy, strategy for identifying</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we introduce a strategy for identifying textual saliency in
large-scale language models applied to classification tasks. In visual networks
where saliency is more well-studied, saliency is naturally localized through
the convolutional layers of the network; however, the same is not true in
modern transformer-stack networks used to process natural language. We adapt
gradient-based saliency methods for these networks, propose a method for
evaluating the degree of semantic coherence of each layer, and demonstrate
consistent improvement over numerous other methods for textual saliency on
multiple benchmark classification datasets. Our approach requires no additional
training or access to labelled data, and is comparatively very computationally
efficient.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：A Preliminary Study of the Intrinsic Relationship between Complexity and  Alignment</b></summary>
  <p><b>编号</b>：[258]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05696</p>
  <p><b>作者</b>：Yingxiu Zhao,  Bowen Yu,  Binyuan Hui,  Haiyang Yu,  Fei Huang,  Yongbin Li,  Nevin L. Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, yielded remarkable success, Training large language, instruction data, open-domain instruction data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Training large language models (LLMs) with open-domain instruction data has
yielded remarkable success in aligning to end tasks and user preferences.
Extensive research has highlighted that enhancing the quality and diversity of
instruction data consistently improves performance. However, the impact of data
complexity, as a crucial metric, remains relatively unexplored in three
aspects: (1) scaling law, where the sustainability of performance improvements
with increasing complexity is uncertain, (2) additional tokens, whether the
improvement brought by complexity comes from introducing more training tokens,
and (3) curriculum tuning, where the potential advantages of incorporating
instructions ranging from easy to difficult are not yet fully understood. In
this paper, we propose \textit{tree-instruct} to systematically enhance the
complexity of instruction data in a controllable manner. This approach adds a
specified number of nodes into the instruction semantic tree, yielding new
instruction data based on the modified tree. By adjusting the number of added
nodes, we can control the difficulty level in the modified instruction data.
Our preliminary experiments reveal the following insights: (1) Increasing
complexity consistently leads to sustained performance improvements. For
instance, using 1,000 instruction data and 10 nodes resulted in a substantial
24\% increase in win rate. (2) Under the same token budget, a few complex
instructions outperform diverse yet simple instructions. (3) Curriculum
instruction tuning might not yield the anticipated results; focusing on
increasing complexity appears to be the key.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Finding Already Debunked Narratives via Multistage Retrieval: Enabling  Cross-Lingual, Cross-Dataset and Zero-Shot Learning</b></summary>
  <p><b>编号</b>：[264]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05680</p>
  <p><b>作者</b>：Iknoor Singh,  Carolina Scarton,  Xingyi Song,  Kalina Bontcheva</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：debunked narratives aims, aims to detect, detect stories, debunked narratives, Transformer models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The task of retrieving already debunked narratives aims to detect stories
that have already been fact-checked. The successful detection of claims that
have already been debunked not only reduces the manual efforts of professional
fact-checkers but can also contribute to slowing the spread of misinformation.
Mainly due to the lack of readily available data, this is an understudied
problem, particularly when considering the cross-lingual task, i.e. the
retrieval of fact-checking articles in a language different from the language
of the online post being checked. This paper fills this gap by (i) creating a
novel dataset to enable research on cross-lingual retrieval of already debunked
narratives, using tweets as queries to a database of fact-checking articles;
(ii) presenting an extensive experiment to benchmark fine-tuned and
off-the-shelf multilingual pre-trained Transformer models for this task; and
(iii) proposing a novel multistage framework that divides this cross-lingual
debunk retrieval task into refinement and re-ranking stages. Results show that
the task of cross-lingual retrieval of already debunked narratives is
challenging and off-the-shelf Transformer models fail to outperform a strong
lexical-based baseline (BM25). Nevertheless, our multistage retrieval framework
is robust, outperforming BM25 in most scenarios and enabling cross-domain and
zero-shot learning, without significantly harming the model's performance.</p>
  </details>
</details>
<h1>机器学习</h1>
<details>
  <summary>1. <b>标题：Neural Progressive Meshes</b></summary>
  <p><b>编号</b>：[3]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05741</p>
  <p><b>作者</b>：Yun-Chun Chen,  Vladimir G. Kim,  Noam Aigerman,  Alec Jacobson</p>
  <p><b>备注</b>：SIGGRAPH 2023</p>
  <p><b>关键词</b>：hand-held devices necessitates, devices necessitates efficient, necessitates efficient tools, recent proliferation, consumed on hand-held</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The recent proliferation of 3D content that can be consumed on hand-held
devices necessitates efficient tools for transmitting large geometric data,
e.g., 3D meshes, over the Internet. Detailed high-resolution assets can pose a
challenge to storage as well as transmission bandwidth, and level-of-detail
techniques are often used to transmit an asset using an appropriate bandwidth
budget. It is especially desirable for these methods to transmit data
progressively, improving the quality of the geometry with more data. Our key
insight is that the geometric details of 3D meshes often exhibit similar local
patterns even across different shapes, and thus can be effectively represented
with a shared learned generative space. We learn this space using a
subdivision-based encoder-decoder architecture trained in advance on a large
collection of surfaces. We further observe that additional residual features
can be transmitted progressively between intermediate levels of subdivision
that enable the client to control the tradeoff between bandwidth cost and
quality of reconstruction, providing a neural progressive mesh representation.
We evaluate our method on a diverse set of complex 3D shapes and demonstrate
that it outperforms baselines in terms of compression ratio and reconstruction
quality.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Zero Grads Ever Given: Learning Local Surrogate Losses for  Non-Differentiable Graphics</b></summary>
  <p><b>编号</b>：[4]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05739</p>
  <p><b>作者</b>：Michael Fischer,  Tobias Ritschel</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Gradient-based optimization, surrogate, objective, graphics, problems with undefined</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Gradient-based optimization is now ubiquitous across graphics, but
unfortunately can not be applied to problems with undefined or zero gradients.
To circumvent this issue, the loss function can be manually replaced by a
"surrogate" that has similar minima but is differentiable. Our proposed
framework, ZeroGrads, automates this process by learning a neural approximation
of the objective function, the surrogate, which in turn can be used to
differentiate through arbitrary black-box graphics pipelines. We train the
surrogate on an actively smoothed version of the objective and encourage
locality, focusing the surrogate's capacity on what matters at the current
training episode. The fitting is performed online, alongside the parameter
optimization, and self-supervised, without pre-computed data or pre-trained
models. As sampling the objective is expensive (it requires a full rendering or
simulator run), we devise an efficient sampling scheme that allows for
tractable run-times and competitive performance at little overhead. We
demonstrate optimizing diverse non-convex, non-differentiable black-box
problems in graphics, such as visibility in rendering, discrete parameter
spaces in procedural modelling or optimal control in physics-driven animation.
In contrast to more traditional algorithms, our approach scales well to higher
dimensions, which we demonstrate on problems with up to 35k interlinked
variables.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Follow Anything: Open-set detection, tracking, and following in  real-time</b></summary>
  <p><b>编号</b>：[5]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05737</p>
  <p><b>作者</b>：Alaa Maalouf,  Ninad Jadhav,  Krishna Murthy Jatavallabhula,  Makram Chahine,  Daniel M.Vogt,  Robert J. Wood,  Antonio Torralba,  Daniela Rus</p>
  <p><b>备注</b>：Project webpage: this https URL Explainer video: this https URL</p>
  <p><b>关键词</b>：ranging from industrial, logistics and warehousing, healthcare and security, industrial automation, automation to logistics</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Tracking and following objects of interest is critical to several robotics
use cases, ranging from industrial automation to logistics and warehousing, to
healthcare and security. In this paper, we present a robotic system to detect,
track, and follow any object in real-time. Our approach, dubbed ``follow
anything'' (FAn), is an open-vocabulary and multimodal model -- it is not
restricted to concepts seen at training time and can be applied to novel
classes at inference time using text, images, or click queries. Leveraging rich
visual descriptors from large-scale pre-trained models (foundation models), FAn
can detect and segment objects by matching multimodal queries (text, images,
clicks) against an input image sequence. These detected and segmented objects
are tracked across image frames, all while accounting for occlusion and object
re-emergence. We demonstrate FAn on a real-world robotic system (a micro aerial
vehicle) and report its ability to seamlessly follow the objects of interest in
a real-time control loop. FAn can be deployed on a laptop with a lightweight
(6-8 GB) graphics card, achieving a throughput of 6-20 frames per second. To
enable rapid adoption, deployment, and extensibility, we open-source all our
code on our project webpage at this https URL .
We also encourage the reader the watch our 5-minutes explainer video in this
this https URL .</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers</b></summary>
  <p><b>编号</b>：[9]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05732</p>
  <p><b>作者</b>：Phillip Lippe,  Bastiaan S. Veeling,  Paris Perdikaris,  Richard E. Turner,  Johannes Brandstetter</p>
  <p><b>备注</b>：Project website: this https URL</p>
  <p><b>关键词</b>：Time-dependent partial differential, partial differential equations, Time-dependent partial, differential equations, science and engineering</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Time-dependent partial differential equations (PDEs) are ubiquitous in
science and engineering. Recently, mostly due to the high computational cost of
traditional solution techniques, deep neural network based surrogates have
gained increased interest. The practical utility of such neural PDE solvers
relies on their ability to provide accurate, stable predictions over long time
horizons, which is a notoriously hard problem. In this work, we present a
large-scale analysis of common temporal rollout strategies, identifying the
neglect of non-dominant spatial frequency information, often associated with
high frequencies in PDE solutions, as the primary pitfall limiting stable,
accurate rollout performance. Based on these insights, we draw inspiration from
recent advances in diffusion models to introduce PDE-Refiner; a novel model
class that enables more accurate modeling of all frequency components via a
multistep refinement process. We validate PDE-Refiner on challenging benchmarks
of complex fluid dynamics, demonstrating stable and accurate rollouts that
consistently outperform state-of-the-art models, including neural, numerical,
and hybrid neural-numerical architectures. We further demonstrate that
PDE-Refiner greatly enhances data efficiency, since the denoising objective
implicitly induces a novel form of spectral data augmentation. Finally,
PDE-Refiner's connection to diffusion models enables an accurate and efficient
assessment of the model's predictive uncertainty, allowing us to estimate when
the surrogate becomes inaccurate.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Rethinking Integration of Prediction and Planning in Deep Learning-Based  Automated Driving Systems: A Review</b></summary>
  <p><b>编号</b>：[10]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05731</p>
  <p><b>作者</b>：Steffen Hagedorn,  Marcel Hallgarten,  Martin Stoll,  Alexandru Condurache</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Automated driving, revolutionize personal, freight mobility, potential to revolutionize, automated driving comprises</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automated driving has the potential to revolutionize personal, public, and
freight mobility. Besides the enormous challenge of perception, i.e. accurately
perceiving the environment using available sensor data, automated driving
comprises planning a safe, comfortable, and efficient motion trajectory. To
promote safety and progress, many works rely on modules that predict the future
motion of surrounding traffic. Modular automated driving systems commonly
handle prediction and planning as sequential separate tasks. While this
accounts for the influence of surrounding traffic on the ego-vehicle, it fails
to anticipate the reactions of traffic participants to the ego-vehicle's
behavior. Recent works suggest that integrating prediction and planning in an
interdependent joint step is necessary to achieve safe, efficient, and
comfortable driving. While various models implement such integrated systems, a
comprehensive overview and theoretical understanding of different principles
are lacking. We systematically review state-of-the-art deep learning-based
prediction, planning, and integrated prediction and planning models. Different
facets of the integration ranging from model architecture and model design to
behavioral aspects are considered and related to each other. Moreover, we
discuss the implications, strengths, and limitations of different integration
methods. By pointing out research gaps, describing relevant future challenges,
and highlighting trends in the research field, we identify promising directions
for future research.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：EXPRESSO: A Benchmark and Analysis of Discrete Expressive Speech  Resynthesis</b></summary>
  <p><b>编号</b>：[11]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05725</p>
  <p><b>作者</b>：Tu Anh Nguyen,  Wei-Ning Hsu,  Antony D'Avirro,  Bowen Shi,  Itai Gat,  Maryam Fazel-Zarani,  Tal Remez,  Jade Copet,  Gabriel Synnaeve,  Michael Hassid,  Felix Kreuk,  Yossi Adi,  Emmanuel Dupoux</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：capture expressive aspects, high-quality speech based, non-verbal vocalization, Recent work, hard to transcribe</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent work has shown that it is possible to resynthesize high-quality speech
based, not on text, but on low bitrate discrete units that have been learned in
a self-supervised fashion and can therefore capture expressive aspects of
speech that are hard to transcribe (prosody, voice styles, non-verbal
vocalization). The adoption of these methods is still limited by the fact that
most speech synthesis datasets are read, severely limiting spontaneity and
expressivity. Here, we introduce Expresso, a high-quality expressive speech
dataset for textless speech synthesis that includes both read speech and
improvised dialogues rendered in 26 spontaneous expressive styles. We
illustrate the challenges and potentials of this dataset with an expressive
resynthesis benchmark where the task is to encode the input in low-bitrate
units and resynthesize it in a target voice while preserving content and style.
We evaluate resynthesis quality with automatic metrics for different
self-supervised discrete encoders, and explore tradeoffs between quality,
bitrate and invariance to speaker and style. All the dataset, evaluation
metrics and baseline models are open source</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Optimizing Performance of Feedforward and Convolutional Neural Networks  through Dynamic Activation Functions</b></summary>
  <p><b>编号</b>：[12]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05724</p>
  <p><b>作者</b>：Chinmay Rane,  Kanishka Tyagi,  Michael Manry</p>
  <p><b>备注</b>：Under submission in Neurocomputing</p>
  <p><b>关键词</b>：learning training training, training training algorithms, fields including speech, Deep learning training, training training</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning training training algorithms are a huge success in recent years
in many fields including speech, text,image video etc. Deeper and deeper layers
are proposed with huge success with resnet structures having around 152 layers.
Shallow convolution neural networks(CNN's) are still an active research, where
some phenomena are still unexplained. Activation functions used in the network
are of utmost importance, as they provide non linearity to the networks. Relu's
are the most commonly used activation function.We show a complex piece-wise
linear(PWL) activation in the hidden layer. We show that these PWL activations
work much better than relu activations in our networks for convolution neural
networks and multilayer perceptrons. Result comparison in PyTorch for shallow
and deep CNNs are given to further strengthen our case.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：A Comparison of Classical and Deep Reinforcement Learning Methods for  HVAC Control</b></summary>
  <p><b>编号</b>：[15]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05711</p>
  <p><b>作者</b>：Marshall Wang,  John Willes,  Thomas Jiralerspong,  Matin Moezzi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：optimizing HVAC control, Reinforcement learning, promising approach, approach for optimizing, HVAC control</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reinforcement learning (RL) is a promising approach for optimizing HVAC
control. RL offers a framework for improving system performance, reducing
energy consumption, and enhancing cost efficiency. We benchmark two popular
classical and deep RL methods (Q-Learning and Deep-Q-Networks) across multiple
HVAC environments and explore the practical consideration of model
hyper-parameter selection and reward tuning. The findings provide insight for
configuring RL agents in HVAC systems, promoting energy-efficient and
cost-effective operation.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Shadow Datasets, New challenging datasets for Causal Representation  Learning</b></summary>
  <p><b>编号</b>：[17]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05707</p>
  <p><b>作者</b>：Jiageng Zhu,  Hanchen Xie,  Jianhua Wu,  Jiazhi Li,  Mahyar Khayatkhoei,  Mohamed E. Hussein,  Wael AbdAlmageed</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：representation learning, Discovering causal relations, CelebA, relations among semantic, emergent topic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Discovering causal relations among semantic factors is an emergent topic in
representation learning. Most causal representation learning (CRL) methods are
fully supervised, which is impractical due to costly labeling. To resolve this
restriction, weakly supervised CRL methods were introduced. To evaluate CRL
performance, four existing datasets, Pendulum, Flow, CelebA(BEARD) and
CelebA(SMILE), are utilized. However, existing CRL datasets are limited to
simple graphs with few generative factors. Thus we propose two new datasets
with a larger number of diverse generative factors and more sophisticated
causal graphs. In addition, current real datasets, CelebA(BEARD) and
CelebA(SMILE), the originally proposed causal graphs are not aligned with the
dataset distributions. Thus, we propose modifications to them.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Hard No-Box Adversarial Attack on Skeleton-Based Human Action  Recognition with Skeleton-Motion-Informed Gradient</b></summary>
  <p><b>编号</b>：[27]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05681</p>
  <p><b>作者</b>：Zhengzhi Lu,  He Wang,  Ziyi Chang,  Guoan Yang,  Hubert P. H. Shum</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：skeleton-based human activity, human activity recognition, attack methods, skeleton-based human, human activity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, methods for skeleton-based human activity recognition have been
shown to be vulnerable to adversarial attacks. However, these attack methods
require either the full knowledge of the victim (i.e. white-box attacks),
access to training data (i.e. transfer-based attacks) or frequent model queries
(i.e. black-box attacks). All their requirements are highly restrictive,
raising the question of how detrimental the vulnerability is. In this paper, we
show that the vulnerability indeed exists. To this end, we consider a new
attack task: the attacker has no access to the victim model or the training
data or labels, where we coin the term hard no-box attack. Specifically, we
first learn a motion manifold where we define an adversarial loss to compute a
new gradient for the attack, named skeleton-motion-informed (SMI) gradient. Our
gradient contains information of the motion dynamics, which is different from
existing gradient-based attack methods that compute the loss gradient assuming
each dimension in the data is independent. The SMI gradient can augment many
gradient-based attack methods, leading to a new family of no-box attack
methods. Extensive evaluation and comparison show that our method imposes a
real threat to existing classifiers. They also show that the SMI gradient
improves the transferability and imperceptibility of adversarial samples in
both no-box and transfer-based black-box settings.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Finding Already Debunked Narratives via Multistage Retrieval: Enabling  Cross-Lingual, Cross-Dataset and Zero-Shot Learning</b></summary>
  <p><b>编号</b>：[28]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05680</p>
  <p><b>作者</b>：Iknoor Singh,  Carolina Scarton,  Xingyi Song,  Kalina Bontcheva</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：debunked narratives aims, aims to detect, detect stories, debunked narratives, Transformer models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The task of retrieving already debunked narratives aims to detect stories
that have already been fact-checked. The successful detection of claims that
have already been debunked not only reduces the manual efforts of professional
fact-checkers but can also contribute to slowing the spread of misinformation.
Mainly due to the lack of readily available data, this is an understudied
problem, particularly when considering the cross-lingual task, i.e. the
retrieval of fact-checking articles in a language different from the language
of the online post being checked. This paper fills this gap by (i) creating a
novel dataset to enable research on cross-lingual retrieval of already debunked
narratives, using tweets as queries to a database of fact-checking articles;
(ii) presenting an extensive experiment to benchmark fine-tuned and
off-the-shelf multilingual pre-trained Transformer models for this task; and
(iii) proposing a novel multistage framework that divides this cross-lingual
debunk retrieval task into refinement and re-ranking stages. Results show that
the task of cross-lingual retrieval of already debunked narratives is
challenging and off-the-shelf Transformer models fail to outperform a strong
lexical-based baseline (BM25). Nevertheless, our multistage retrieval framework
is robust, outperforming BM25 in most scenarios and enabling cross-domain and
zero-shot learning, without significantly harming the model's performance.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：AST-MHSA : Code Summarization using Multi-Head Self-Attention</b></summary>
  <p><b>编号</b>：[39]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05646</p>
  <p><b>作者</b>：Yeshwanth Nagaraj,  Ujjwal Gupta</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Code summarization aims, Abstract Syntax Tree, source code, AST, summarization aims</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Code summarization aims to generate concise natural language descriptions for
source code. The prevailing approaches adopt transformer-based encoder-decoder
architectures, where the Abstract Syntax Tree (AST) of the source code is
utilized for encoding structural information. However, ASTs are much longer
than the corresponding source code, and existing methods ignore this size
constraint by directly feeding the entire linearized AST into the encoders.
This simplistic approach makes it challenging to extract truly valuable
dependency relations from the overlong input sequence and leads to significant
computational overhead due to self-attention applied to all nodes in the AST.
To address this issue effectively and efficiently, we present a model,
AST-MHSA that uses multi-head attention to extract the important semantic
information from the AST. The model consists of two main components: an encoder
and a decoder. The encoder takes as input the abstract syntax tree (AST) of the
code and generates a sequence of hidden states. The decoder then takes these
hidden states as input and generates a natural language summary of the code.
The multi-head attention mechanism allows the model to learn different
representations of the input code, which can be combined to generate a more
comprehensive summary. The model is trained on a dataset of code and summaries,
and the parameters of the model are optimized to minimize the loss between the
generated summaries and the ground-truth summaries.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：IIHT: Medical Report Generation with Image-to-Indicator Hierarchical  Transformer</b></summary>
  <p><b>编号</b>：[45]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05633</p>
  <p><b>作者</b>：Keqiang Fan,  Xiaohao Cai,  Mahesan Niranjan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：medical report generation, Automated medical report, medical report, report generation, increasingly important</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automated medical report generation has become increasingly important in
medical analysis. It can produce computer-aided diagnosis descriptions and thus
significantly alleviate the doctors' work. Inspired by the huge success of
neural machine translation and image captioning, various deep learning methods
have been proposed for medical report generation. However, due to the inherent
properties of medical data, including data imbalance and the length and
correlation between report sequences, the generated reports by existing methods
may exhibit linguistic fluency but lack adequate clinical accuracy. In this
work, we propose an image-to-indicator hierarchical transformer (IIHT)
framework for medical report generation. It consists of three modules, i.e., a
classifier module, an indicator expansion module and a generator module. The
classifier module first extracts image features from the input medical images
and produces disease-related indicators with their corresponding states. The
disease-related indicators are subsequently utilised as input for the indicator
expansion module, incorporating the "data-text-data" strategy. The
transformer-based generator then leverages these extracted features along with
image features as auxiliary information to generate final reports. Furthermore,
the proposed IIHT method is feasible for radiologists to modify disease
indicators in real-world scenarios and integrate the operations into the
indicator expansion module for fluent and accurate medical report generation.
Extensive experiments and comparisons with state-of-the-art methods under
various evaluation metrics demonstrate the great performance of the proposed
method.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：ReLU and Addition-based Gated RNN</b></summary>
  <p><b>编号</b>：[46]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05629</p>
  <p><b>作者</b>：Rickard Brännvall,  Henrik Forsgren,  Fredrik Sandin,  Marcus Liwicki</p>
  <p><b>备注</b>：12 pages, 4 tables</p>
  <p><b>关键词</b>：Recurrent Neural Networks, ReLU activation, sigmoid function, gate with addition, addition and ReLU</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We replace the multiplication and sigmoid function of the conventional
recurrent gate with addition and ReLU activation. This mechanism is designed to
maintain long-term memory for sequence processing but at a reduced
computational cost, thereby opening up for more efficient execution or larger
models on restricted hardware. Recurrent Neural Networks (RNNs) with gating
mechanisms such as LSTM and GRU have been widely successful in learning from
sequential data due to their ability to capture long-term dependencies.
Conventionally, the update based on current inputs and the previous state
history is each multiplied with dynamic weights and combined to compute the
next state. However, multiplication can be computationally expensive,
especially for certain hardware architectures or alternative arithmetic systems
such as homomorphic encryption. It is demonstrated that the novel gating
mechanism can capture long-term dependencies for a standard synthetic sequence
learning task while significantly reducing computational costs such that
execution time is reduced by half on CPU and by one-third under encryption.
Experimental results on handwritten text recognition tasks furthermore show
that the proposed architecture can be trained to achieve comparable accuracy to
conventional GRU and LSTM baselines. The gating mechanism introduced in this
paper may enable privacy-preserving AI applications operating under homomorphic
encryption by avoiding the multiplication of encrypted variables. It can also
support quantization in (unencrypted) plaintext applications, with the
potential for substantial performance gains since the addition-based
formulation can avoid the expansion to double precision often required for
multiplication.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Normalized Gradients for All</b></summary>
  <p><b>编号</b>：[48]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05621</p>
  <p><b>作者</b>：Francesco Orabona</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Hölder smoothness, short note, local Hölder smoothness, normalized gradients, Hölder</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this short note, I show how to adapt to Hölder smoothness using
normalized gradients in a black-box way. Moreover, the bound will depend on a
novel notion of local Hölder smoothness. The main idea directly comes from
Levy [2017].</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Multi-graph Spatio-temporal Graph Convolutional Network for Traffic Flow  Prediction</b></summary>
  <p><b>编号</b>：[55]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05601</p>
  <p><b>作者</b>：Weilong Ding,  Tianpu Zhang,  Jianwu Wang,  Zhuofeng Zhao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Inter-city highway transportation, urban life, traffic flow, daily traffic flow, network-wide toll stations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Inter-city highway transportation is significant for urban life. As one of
the key functions in intelligent transportation system (ITS), traffic
evaluation always plays significant role nowadays, and daily traffic flow
prediction still faces challenges at network-wide toll stations. On the one
hand, the data imbalance in practice among various locations deteriorates the
performance of prediction. On the other hand, complex correlative
spatio-temporal factors cannot be comprehensively employed in long-term
duration. In this paper, a prediction method is proposed for daily traffic flow
in highway domain through spatio-temporal deep learning. In our method, data
normalization strategy is used to deal with data imbalance, due to long-tail
distribution of traffic flow at network-wide toll stations. And then, based on
graph convolutional network, we construct networks in distinct semantics to
capture spatio-temporal features. Beside that, meteorology and calendar
features are used by our model in the full connection stage to extra external
characteristics of traffic flow. By extensive experiments and case studies in
one Chinese provincial highway, our method shows clear improvement in
predictive accuracy than baselines and practical benefits in business.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：NUPES : Non-Uniform Post-Training Quantization via Power Exponent Search</b></summary>
  <p><b>编号</b>：[56]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05600</p>
  <p><b>作者</b>：Edouard Yvinec,  Arnaud Dapogny,  Kevin Bailly</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：expensive computational requirements, larger hardware devices, hardware devices due, Deep neural network, neural network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep neural network (DNN) deployment has been confined to larger hardware
devices due to their expensive computational requirements. This challenge has
recently reached another scale with the emergence of large language models
(LLMs). In order to reduce both their memory footprint and latency, a promising
technique is quantization. It consists in converting floating point
representations to low bit-width fixed point representations, usually by
assuming a uniform mapping onto a regular grid. This process, referred to in
the literature as uniform quantization, may however be ill-suited as most DNN
weights and activations follow a bell-shaped distribution. This is even worse
on LLMs whose weight distributions are known to exhibit large, high impact,
outlier values. In this work, we propose an improvement over the most commonly
adopted way to tackle this limitation in deep learning models quantization,
namely, non-uniform quantization. NUPES leverages automorphisms to preserve the
scalar multiplications. Such transformations are derived from power functions.
However, the optimization of the exponent parameter and weight values remains a
challenging and novel problem which could not be solved with previous post
training optimization techniques which only learn to round up or down weight
values in order to preserve the predictive function. We circumvent this
limitation with a new paradigm: learning new quantized weights over the entire
quantized space. Similarly, we enable the optimization of the power exponent,
i.e. the optimization of the quantization operator itself during training by
alleviating all the numerical instabilities. The resulting predictive function
is compatible with integer-only low-bit inference. We show the ability of the
method to achieve state-of-the-art compression rates in both, data-free and
data-driven configurations.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Symmetry Defense Against XGBoost Adversarial Perturbation Attacks</b></summary>
  <p><b>编号</b>：[68]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05575</p>
  <p><b>作者</b>：Blerta Lindqvist</p>
  <p><b>备注</b>：16 pages</p>
  <p><b>关键词</b>：gradient-boosting decision trees, defend tree-based ensemble, tree-based ensemble classifiers, adversarial samples, decision trees</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We examine whether symmetry can be used to defend tree-based ensemble
classifiers such as gradient-boosting decision trees (GBDTs) against
adversarial perturbation attacks. The idea is based on a recent symmetry
defense for convolutional neural network classifiers (CNNs) that utilizes CNNs'
lack of invariance with respect to symmetries. CNNs lack invariance because
they can classify a symmetric sample, such as a horizontally flipped image,
differently from the original sample. CNNs' lack of invariance also means that
CNNs can classify symmetric adversarial samples differently from the incorrect
classification of adversarial samples. Using CNNs' lack of invariance, the
recent CNN symmetry defense has shown that the classification of symmetric
adversarial samples reverts to the correct sample classification. In order to
apply the same symmetry defense to GBDTs, we examine GBDT invariance and are
the first to show that GBDTs also lack invariance with respect to symmetries.
We apply and evaluate the GBDT symmetry defense for nine datasets against six
perturbation attacks with a threat model that ranges from zero-knowledge to
perfect-knowledge adversaries. Using the feature inversion symmetry against
zero-knowledge adversaries, we achieve up to 100% accuracy on adversarial
samples even when default and robust classifiers have 0% accuracy. Using the
feature inversion and horizontal flip symmetries against perfect-knowledge
adversaries, we achieve up to over 95% accuracy on adversarial samples for the
GBDT classifier of the F-MNIST dataset even when default and robust classifiers
have 0% accuracy.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：AutoGluon-TimeSeries: AutoML for Probabilistic Time Series Forecasting</b></summary>
  <p><b>编号</b>：[71]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05566</p>
  <p><b>作者</b>：Oleksandr Shchur,  Caner Turkmen,  Nick Erickson,  Huibin Shen,  Alexander Shirkov,  Tony Hu,  Yuyang Wang</p>
  <p><b>备注</b>：Published at AutoML Conference 2023</p>
  <p><b>关键词</b>：open-source AutoML library, probabilistic time series, open-source AutoML, AutoML library, library for probabilistic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce AutoGluon-TimeSeries - an open-source AutoML library for
probabilistic time series forecasting. Focused on ease of use and robustness,
AutoGluon-TimeSeries enables users to generate accurate point and quantile
forecasts with just 3 lines of Python code. Built on the design philosophy of
AutoGluon, AutoGluon-TimeSeries leverages ensembles of diverse forecasting
models to deliver high accuracy within a short training time.
AutoGluon-TimeSeries combines both conventional statistical models,
machine-learning based forecasting approaches, and ensembling techniques. In
our evaluation on 29 benchmark datasets, AutoGluon-TimeSeries demonstrates
strong empirical performance, outperforming a range of forecasting methods in
terms of both point and quantile forecast accuracy, and often even improving
upon the best-in-hindsight combination of prior methods.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Critical Points ++: An Agile Point Cloud Importance Measure for Robust  Classification, Adversarial Defense and Explainable AI</b></summary>
  <p><b>编号</b>：[87]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05525</p>
  <p><b>作者</b>：Meir Yossef Levi,  Guy Gilboa</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：real-world safety demanding, safety demanding applications, ability to cope, cope accurately, crucial in real-world</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The ability to cope accurately and fast with Out-Of-Distribution (OOD)
samples is crucial in real-world safety demanding applications. In this work we
first study the interplay between critical points of 3D point clouds and OOD
samples. Our findings are that common corruptions and outliers are often
interpreted as critical points. We generalize the notion of critical points
into importance measures. We show that training a classification network based
only on less important points dramatically improves robustness, at a cost of
minor performance loss on the clean set. We observe that normalized entropy is
highly informative for corruption analysis. An adaptive threshold based on
normalized entropy is suggested for selecting the set of uncritical points. Our
proposed importance measure is extremely fast to compute. We show it can be
used for a variety of applications, such as Explainable AI (XAI), Outlier
Removal, Uncertainty Estimation, Robust Classification and Adversarial Defense.
We reach SOTA results on the two latter tasks.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Models Matter: The Impact of Single-Step Retrosynthesis on Synthesis  Planning</b></summary>
  <p><b>编号</b>：[88]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05522</p>
  <p><b>作者</b>：Paula Torren-Peraire,  Alan Kai Hassen,  Samuel Genheden,  Jonas Verhoeven,  Djork-Arne Clevert,  Mike Preuss,  Igor Tetko</p>
  <p><b>备注</b>：The following authors contributed equally: Paula Torren-Peraire, Alan Kai Hassen</p>
  <p><b>关键词</b>：chemical compound recursively, multi-step synthesis planning, synthesis planning, synthesis, multi-step synthesis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Retrosynthesis consists of breaking down a chemical compound recursively
step-by-step into molecular precursors until a set of commercially available
molecules is found with the goal to provide a synthesis route. Its two primary
research directions, single-step retrosynthesis prediction, which models the
chemical reaction logic, and multi-step synthesis planning, which tries to find
the correct sequence of reactions, are inherently intertwined. Still, this
connection is not reflected in contemporary research. In this work, we combine
these two major research directions by applying multiple single-step
retrosynthesis models within multi-step synthesis planning and analyzing their
impact using public and proprietary reaction data. We find a disconnection
between high single-step performance and potential route-finding success,
suggesting that single-step models must be evaluated within synthesis planning
in the future. Furthermore, we show that the commonly used single-step
retrosynthesis benchmark dataset USPTO-50k is insufficient as this evaluation
task does not represent model performance and scalability on larger and more
diverse datasets. For multi-step synthesis planning, we show that the choice of
the single-step model can improve the overall success rate of synthesis
planning by up to +28% compared to the commonly used baseline model. Finally,
we show that each single-step model finds unique synthesis routes, and differs
in aspects such as route-finding success, the number of found synthesis routes,
and chemical validity, making the combination of single-step retrosynthesis
prediction and multi-step synthesis planning a crucial aspect when developing
future methods.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：On the Optimal Expressive Power of ReLU DNNs and Its Application in  Approximation with Kolmogorov Superposition Theorem</b></summary>
  <p><b>编号</b>：[91]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05509</p>
  <p><b>作者</b>：Juncai He</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：deep neural networks, ReLU deep neural, optimal expressive power, neural networks, paper is devoted</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper is devoted to studying the optimal expressive power of ReLU deep
neural networks (DNNs) and its application in approximation via the Kolmogorov
Superposition Theorem. We first constructively prove that any continuous
piecewise linear functions on $[0,1]$, comprising $O(N^2L)$ segments, can be
represented by ReLU DNNs with $L$ hidden layers and $N$ neurons per layer.
Subsequently, we demonstrate that this construction is optimal regarding the
parameter count of the DNNs, achieved through investigating the shattering
capacity of ReLU DNNs. Moreover, by invoking the Kolmogorov Superposition
Theorem, we achieve an enhanced approximation rate for ReLU DNNs of arbitrary
width and depth when dealing with continuous functions in high-dimensional
spaces.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Quality Diversity under Sparse Reward and Sparse Interaction:  Application to Grasping in Robotics</b></summary>
  <p><b>编号</b>：[106]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05483</p>
  <p><b>作者</b>：J. Huber,  F. Hélénon,  M. Coninx,  F. Ben Amar,  S. Doncieux</p>
  <p><b>备注</b>：37 pages, 17 figures. Draft version</p>
  <p><b>关键词</b>：aim to generate, diverse and high-performing, set of diverse, generate a set, high-performing solutions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Quality-Diversity (QD) methods are algorithms that aim to generate a set of
diverse and high-performing solutions to a given problem. Originally developed
for evolutionary robotics, most QD studies are conducted on a limited set of
domains - mainly applied to locomotion, where the fitness and the behavior
signal are dense. Grasping is a crucial task for manipulation in robotics.
Despite the efforts of many research communities, this task is yet to be
solved. Grasping cumulates unprecedented challenges in QD literature: it
suffers from reward sparsity, behavioral sparsity, and behavior space
misalignment. The present work studies how QD can address grasping. Experiments
have been conducted on 15 different methods on 10 grasping domains,
corresponding to 2 different robot-gripper setups and 5 standard objects. An
evaluation framework that distinguishes the evaluation of an algorithm from its
internal components has also been proposed for a fair comparison. The obtained
results show that MAP-Elites variants that select successful solutions in
priority outperform all the compared methods on the studied metrics by a large
margin. We also found experimental evidence that sparse interaction can lead to
deceptive novelty. To our knowledge, the ability to efficiently produce
examples of grasping trajectories demonstrated in this work has no precedent in
the literature.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：LLM As DBA</b></summary>
  <p><b>编号</b>：[107]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05481</p>
  <p><b>作者</b>：Xuanhe Zhou,  Guoliang Li,  Zhiyuan Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ensure data availability, play a crucial, role in managing, maintaining and optimizing, data availability</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Database administrators (DBAs) play a crucial role in managing, maintaining
and optimizing a database system to ensure data availability, performance, and
reliability. However, it is hard and tedious for DBAs to manage a large number
of database instances (e.g., millions of instances on the cloud databases).
Recently large language models (LLMs) have shown great potential to understand
valuable documents and accordingly generate reasonable answers. Thus, we
propose D-Bot, a LLM-based database administrator that can continuously acquire
database maintenance experience from textual sources, and provide reasonable,
well-founded, in-time diagnosis and optimization advice for target databases.
This paper presents a revolutionary LLM-centric framework for database
maintenance, including (i) database maintenance knowledge detection from
documents and tools, (ii) tree of thought reasoning for root cause analysis,
and (iii) collaborative diagnosis among multiple LLMs. Our preliminary
experimental results that D-Bot can efficiently and effectively diagnose the
root causes and our code is available at
this http URL.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Exploring Machine Learning and Transformer-based Approaches for  Deceptive Text Classification: A Comparative Analysis</b></summary>
  <p><b>编号</b>：[110]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05476</p>
  <p><b>作者</b>：Anusuya Krishnan</p>
  <p><b>备注</b>：12 pages, 8 figures</p>
  <p><b>关键词</b>：natural language processing, Deceptive text classification, Deceptive text, text classification, critical task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deceptive text classification is a critical task in natural language
processing that aims to identify deceptive o fraudulent content. This study
presents a comparative analysis of machine learning and transformer-based
approaches for deceptive text classification. We investigate the effectiveness
of traditional machine learning algorithms and state-of-the-art transformer
models, such as BERT, XLNET, DistilBERT, and RoBERTa, in detecting deceptive
text. A labeled dataset consisting of deceptive and non-deceptive texts is used
for training and evaluation purposes. Through extensive experimentation, we
compare the performance metrics, including accuracy, precision, recall, and F1
score, of the different approaches. The results of this study shed light on the
strengths and limitations of machine learning and transformer-based methods for
deceptive text classification, enabling researchers and practitioners to make
informed decisions when dealing with deceptive content.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Provably Efficient Algorithm for Nonstationary Low-Rank MDPs</b></summary>
  <p><b>编号</b>：[112]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05471</p>
  <p><b>作者</b>：Yuan Cheng,  Jing Yang,  Yingbin Liang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Markov Decision Processes, nonstationary Markov Decision, Decision Processes, gains considerable interest, Markov Decision</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reinforcement learning (RL) under changing environment models many real-world
applications via nonstationary Markov Decision Processes (MDPs), and hence
gains considerable interest. However, theoretical studies on nonstationary MDPs
in the literature have mainly focused on tabular and linear (mixture) MDPs,
which do not capture the nature of unknown representation in deep RL. In this
paper, we make the first effort to investigate nonstationary RL under episodic
low-rank MDPs, where both transition kernels and rewards may vary over time,
and the low-rank model contains unknown representation in addition to the
linear state embedding function. We first propose a parameter-dependent policy
optimization algorithm called PORTAL, and further improve PORTAL to its
parameter-free version of Ada-PORTAL, which is able to tune its
hyper-parameters adaptively without any prior knowledge of nonstationarity. For
both algorithms, we provide upper bounds on the average dynamic suboptimality
gap, which show that as long as the nonstationarity is not significantly large,
PORTAL and Ada-PORTAL are sample-efficient and can achieve arbitrarily small
average dynamic suboptimality gap with polynomial sample complexity.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：$\mathcal{G}^2Pxy$: Generative Open-Set Node Classification on Graphs  with Proxy Unknowns</b></summary>
  <p><b>编号</b>：[113]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05463</p>
  <p><b>作者</b>：Qin Zhang,  Zelin Shi,  Xiaolin Zhang,  Xiaojun Chen,  Philippe Fournier-Viger,  Shirui Pan</p>
  <p><b>备注</b>：8 pages, 1 figure</p>
  <p><b>关键词</b>：task of predicting, classification, open-set, mathcal, unknown</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Node classification is the task of predicting the labels of unlabeled nodes
in a graph. State-of-the-art methods based on graph neural networks achieve
excellent performance when all labels are available during training. But in
real-life, models are often applied on data with new classes, which can lead to
massive misclassification and thus significantly degrade performance. Hence,
developing open-set classification methods is crucial to determine if a given
sample belongs to a known class. Existing methods for open-set node
classification generally use transductive learning with part or all of the
features of real unseen class nodes to help with open-set classification. In
this paper, we propose a novel generative open-set node classification method,
i.e. $\mathcal{G}^2Pxy$, which follows a stricter inductive learning setting
where no information about unknown classes is available during training and
validation. Two kinds of proxy unknown nodes, inter-class unknown proxies and
external unknown proxies are generated via mixup to efficiently anticipate the
distribution of novel classes. Using the generated proxies, a closed-set
classifier can be transformed into an open-set one, by augmenting it with an
extra proxy classifier. Under the constraints of both cross entropy loss and
complement entropy loss, $\mathcal{G}^2Pxy$ achieves superior effectiveness for
unknown class detection and known class classification, which is validated by
experiments on benchmark graph datasets. Moreover, $\mathcal{G}^2Pxy$ does not
have specific requirement on the GNN architecture and shows good
generalizations.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Explainable AI applications in the Medical Domain: a systematic review</b></summary>
  <p><b>编号</b>：[128]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05411</p>
  <p><b>作者</b>：Nicoletta Prentzas,  Antonis Kakas,  Constantinos S. Pattichis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：made significant progress, Artificial Intelligence, Intelligence in Medicine, patient care, made significant</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Artificial Intelligence in Medicine has made significant progress with
emerging applications in medical imaging, patient care, and other areas. While
these applications have proven successful in retrospective studies, very few of
them were applied in practice.The field of Medical AI faces various challenges,
in terms of building user trust, complying with regulations, using data
ethically.Explainable AI (XAI) aims to enable humans understand AI and trust
its results. This paper presents a literature review on the recent developments
of XAI solutions for medical decision support, based on a representative sample
of 198 articles published in recent years. The systematic synthesis of the
relevant articles resulted in several findings. (1) model-agnostic XAI
techniques were mostly employed in these solutions, (2) deep learning models
are utilized more than other types of machine learning models, (3)
explainability was applied to promote trust, but very few works reported the
physicians participation in the loop, (4) visual and interactive user interface
is more useful in understanding the explanation and the recommendation of the
system. More research is needed in collaboration between medical and AI
experts, that could guide the development of suitable frameworks for the
design, implementation, and evaluation of XAI solutions in medicine.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：A Comparative Assessment of Multi-view fusion learning for Crop  Classification</b></summary>
  <p><b>编号</b>：[130]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05407</p>
  <p><b>作者</b>：Francisco Mena,  Diego Arenas,  Marlon Nuske,  Andreas Dengel</p>
  <p><b>备注</b>：Accepted at IEEE International Geoscience and Remote Sensing Symposium 2023</p>
  <p><b>关键词</b>：rapidly increasing amount, multi-view learning modeling, remote sensing, learning modeling, rapidly increasing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With a rapidly increasing amount and diversity of remote sensing (RS) data
sources, there is a strong need for multi-view learning modeling. This is a
complex task when considering the differences in resolution, magnitude, and
noise of RS data. The typical approach for merging multiple RS sources has been
input-level fusion, but other - more advanced - fusion strategies may
outperform this traditional approach. This work assesses different fusion
strategies for crop classification in the CropHarvest dataset. The fusion
methods proposed in this work outperform models based on individual views and
previous fusion methods. We do not find one single fusion method that
consistently outperforms all other approaches. Instead, we present a comparison
of multi-view fusion methods for three different datasets and show that,
depending on the test region, different methods obtain the best performance.
Despite this, we suggest a preliminary criterion for the selection of fusion
methods.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Product Review Image Ranking for Fashion E-commerce</b></summary>
  <p><b>编号</b>：[136]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05390</p>
  <p><b>作者</b>：Sangeet Jaiswal,  Dhruv Patel,  Sreekanth Vempati,  Konduru Saiswaroop</p>
  <p><b>备注</b>：Accepted in Proceedings of ACM SIGIR Workshop on eCommerce (SIGIR eCom'22)</p>
  <p><b>关键词</b>：making purchase decisions, examine the products, purchase decisions, physically examine, customers' text</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In a fashion e-commerce platform where customers can't physically examine the
products on their own, being able to see other customers' text and image
reviews of the product is critical while making purchase decisions. Given the
high reliance on these reviews, over the years we have observed customers
proactively sharing their reviews. With an increase in the coverage of User
Generated Content (UGC), there has been a corresponding increase in the number
of customer images. It is thus imperative to display the most relevant images
on top as it may influence users' online shopping choices and behavior. In this
paper, we propose a simple yet effective training procedure for ranking
customer images. We created a dataset consisting of Myntra (A Major Indian
Fashion e-commerce company) studio posts and highly engaged (upvotes/downvotes)
UGC images as our starting point and used selected distortion techniques on the
images of the above dataset to bring their quality at par with those of bad UGC
images. We train our network to rank bad-quality images lower than high-quality
ones. Our proposed method outperforms the baseline models on two metrics,
namely correlation coefficient, and accuracy, by substantial margins.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language  Models' Alignment</b></summary>
  <p><b>编号</b>：[144]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05374</p>
  <p><b>作者</b>：Yang Liu,  Yuanshun Yao,  Jean-Francois Ton,  Xiaoying Zhang,  Ruocheng Guo Hao Cheng,  Yegor Klochkov,  Muhammad Faaiz Taufiq,  Hang Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：deploying large language, large language models, human intentions, behave in accordance, accordance with human</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Ensuring alignment, which refers to making models behave in accordance with
human intentions [1,2], has become a critical task before deploying large
language models (LLMs) in real-world applications. For instance, OpenAI devoted
six months to iteratively aligning GPT-4 before its release [3]. However, a
major challenge faced by practitioners is the lack of clear guidance on
evaluating whether LLM outputs align with social norms, values, and
regulations. This obstacle hinders systematic iteration and deployment of LLMs.
To address this issue, this paper presents a comprehensive survey of key
dimensions that are crucial to consider when assessing LLM trustworthiness. The
survey covers seven major categories of LLM trustworthiness: reliability,
safety, fairness, resistance to misuse, explainability and reasoning, adherence
to social norms, and robustness. Each major category is further divided into
several sub-categories, resulting in a total of 29 sub-categories.
Additionally, a subset of 8 sub-categories is selected for further
investigation, where corresponding measurement studies are designed and
conducted on several widely-used LLMs. The measurement results indicate that,
in general, more aligned models tend to perform better in terms of overall
trustworthiness. However, the effectiveness of alignment varies across the
different trustworthiness categories considered. This highlights the importance
of conducting more fine-grained analyses, testing, and making continuous
improvements on LLM alignment. By shedding light on these key dimensions of LLM
trustworthiness, this paper aims to provide valuable insights and guidance to
practitioners in the field. Understanding and addressing these concerns will be
crucial in achieving reliable and ethically sound deployment of LLMs in various
applications.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Flexible Isosurface Extraction for Gradient-Based Mesh Optimization</b></summary>
  <p><b>编号</b>：[145]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05371</p>
  <p><b>作者</b>：Tianchang Shen,  Jacob Munkberg,  Jon Hasselgren,  Kangxue Yin,  Zian Wang,  Wenzheng Chen,  Zan Gojcic,  Sanja Fidler,  Nicholas Sharp,  Jun Gao</p>
  <p><b>备注</b>：SIGGRAPH 2023. Project page: this https URL</p>
  <p><b>关键词</b>：increasingly common paradigm, applications including photogrammetry, generative modeling, including photogrammetry, inverse physics</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This work considers gradient-based mesh optimization, where we iteratively
optimize for a 3D surface mesh by representing it as the isosurface of a scalar
field, an increasingly common paradigm in applications including
photogrammetry, generative modeling, and inverse physics. Existing
implementations adapt classic isosurface extraction algorithms like Marching
Cubes or Dual Contouring; these techniques were designed to extract meshes from
fixed, known fields, and in the optimization setting they lack the degrees of
freedom to represent high-quality feature-preserving meshes, or suffer from
numerical instabilities. We introduce FlexiCubes, an isosurface representation
specifically designed for optimizing an unknown mesh with respect to geometric,
visual, or even physical objectives. Our main insight is to introduce
additional carefully-chosen parameters into the representation, which allow
local flexible adjustments to the extracted mesh geometry and connectivity.
These parameters are updated along with the underlying scalar field via
automatic differentiation when optimizing for a downstream task. We base our
extraction scheme on Dual Marching Cubes for improved topological properties,
and present extensions to optionally generate tetrahedral and
hierarchically-adaptive meshes. Extensive experiments validate FlexiCubes on
both synthetic benchmarks and real-world applications, showing that it offers
significant improvements in mesh quality and geometric fidelity.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Machine Learning aided Computer Architecture Design for CNN Inferencing  Systems</b></summary>
  <p><b>编号</b>：[148]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05364</p>
  <p><b>作者</b>：Christopher A. Metz</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Machine Learning, calculations of Machine, Convolutional Neural Networks, Efficient and timely, autonomous driving</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Efficient and timely calculations of Machine Learning (ML) algorithms are
essential for emerging technologies like autonomous driving, the Internet of
Things (IoT), and edge computing. One of the primary ML algorithms used in such
systems is Convolutional Neural Networks (CNNs), which demand high
computational resources. This requirement has led to the use of ML accelerators
like GPGPUs to meet design constraints. However, selecting the most suitable
accelerator involves Design Space Exploration (DSE), a process that is usually
time-consuming and requires significant manual effort. Our work presents
approaches to expedite the DSE process by identifying the most appropriate
GPGPU for CNN inferencing systems. We have developed a quick and precise
technique for forecasting the power and performance of CNNs during inference,
with a MAPE of 5.03% and 5.94%, respectively. Our approach empowers computer
architects to estimate power and performance in the early stages of
development, reducing the necessity for numerous prototypes. This saves time
and money while also improving the time-to-market period.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：FINER: Enhancing State-of-the-art Classifiers with Feature Attribution  to Facilitate Security Analysis</b></summary>
  <p><b>编号</b>：[149]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05362</p>
  <p><b>作者</b>：Yiling He,  Jian Lou,  Zhan Qin,  Kui Ren</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：risk detection applications, learning classifiers achieve, risk detection, detection applications, classifiers achieve</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning classifiers achieve state-of-the-art performance in various
risk detection applications. They explore rich semantic representations and are
supposed to automatically discover risk behaviors. However, due to the lack of
transparency, the behavioral semantics cannot be conveyed to downstream
security experts to reduce their heavy workload in security analysis. Although
feature attribution (FA) methods can be used to explain deep learning, the
underlying classifier is still blind to what behavior is suspicious, and the
generated explanation cannot adapt to downstream tasks, incurring poor
explanation fidelity and intelligibility. In this paper, we propose FINER, the
first framework for risk detection classifiers to generate high-fidelity and
high-intelligibility explanations. The high-level idea is to gather explanation
efforts from model developer, FA designer, and security experts. To improve
fidelity, we fine-tune the classifier with an explanation-guided multi-task
learning strategy. To improve intelligibility, we engage task knowledge to
adjust and ensemble FA methods. Extensive evaluations show that FINER improves
explanation quality for risk detection. Moreover, we demonstrate that FINER
outperforms a state-of-the-art tool in facilitating malware analysis.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Preemptive Detection of Fake Accounts on Social Networks via Multi-Class  Preferential Attachment Classifiers</b></summary>
  <p><b>编号</b>：[154]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05353</p>
  <p><b>作者</b>：Adam Breuer,  Nazanin Khosravani,  Michael Tingley,  Bradford Cottel</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Attachment k-class Classifier, k-class Classifier, Preferential Attachment k-class, called Preferential Attachment, Preferential Attachment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we describe a new algorithm called Preferential Attachment
k-class Classifier (PreAttacK) for detecting fake accounts in a social network.
Recently, several algorithms have obtained high accuracy on this problem.
However, they have done so by relying on information about fake accounts'
friendships or the content they share with others--the very things we seek to
prevent. PreAttacK represents a significant departure from these approaches. We
provide some of the first detailed distributional analyses of how new fake (and
real) accounts first attempt to request friends after joining a major network
(Facebook). We show that even before a new account has made friends or shared
content, these initial friend request behaviors evoke a natural multi-class
extension of the canonical Preferential Attachment model of social network
growth. We use this model to derive a new algorithm, PreAttacK. We prove that
in relevant problem instances, PreAttacK near-optimally approximates the
posterior probability that a new account is fake under this multi-class
Preferential Attachment model of new accounts' (not-yet-answered) friend
requests. These are the first provable guarantees for fake account detection
that apply to new users, and that do not require strong homophily assumptions.
This principled approach also makes PreAttacK the only algorithm with provable
guarantees that obtains state-of-the-art performance on new users on the global
Facebook network, where it converges to AUC=0.9 after new users send + receive
a total of just 20 not-yet-answered friend requests. For comparison,
state-of-the-art benchmarks do not obtain this AUC even after observing
additional data on new users' first 100 friend requests. Thus, unlike
mainstream algorithms, PreAttacK converges before the median new fake account
has made a single friendship (accepted friend request) with a human.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：RTLLM: An Open-Source Benchmark for Design RTL Generation with Large  Language Model</b></summary>
  <p><b>编号</b>：[157]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05345</p>
  <p><b>作者</b>：Yao Lu,  Shang Liu,  Qijun Zhang,  Zhiyao Xie</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：agile hardware design, large language models, design RTL based, design RTL, generating design RTL</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Inspired by the recent success of large language models (LLMs) like ChatGPT,
researchers start to explore the adoption of LLMs for agile hardware design,
such as generating design RTL based on natural-language instructions. However,
in existing works, their target designs are all relatively simple and in a
small scale, and proposed by the authors themselves, making a fair comparison
among different LLM solutions challenging. In addition, many prior works only
focus on the design correctness, without evaluating the design qualities of
generated design RTL. In this work, we propose an open-source benchmark named
RTLLM, for generating design RTL with natural language instructions. To
systematically evaluate the auto-generated design RTL, we summarized three
progressive goals, named syntax goal, functionality goal, and design quality
goal. This benchmark can automatically provide a quantitative evaluation of any
given LLM-based solution. Furthermore, we propose an easy-to-use yet
surprisingly effective prompt engineering technique named self-planning, which
proves to significantly boost the performance of GPT-3.5 in our proposed
benchmark.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Homophily-enhanced Structure Learning for Graph Clustering</b></summary>
  <p><b>编号</b>：[175]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05309</p>
  <p><b>作者</b>：Ming Gu,  Gaoming Yang,  Sheng Zhou,  Ning Ma,  Jiawei Chen,  Qiaoyu Tan,  Meihan Liu,  Jiajun Bu</p>
  <p><b>备注</b>：11 pages with 7 figures. Accepted by CIKM'23</p>
  <p><b>关键词</b>：shown impressive results, graph neural networks, utilizing graph neural, graph structure, Graph</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Graph clustering is a fundamental task in graph analysis, and recent advances
in utilizing graph neural networks (GNNs) have shown impressive results.
Despite the success of existing GNN-based graph clustering methods, they often
overlook the quality of graph structure, which is inherent in real-world graphs
due to their sparse and multifarious nature, leading to subpar performance.
Graph structure learning allows refining the input graph by adding missing
links and removing spurious connections. However, previous endeavors in graph
structure learning have predominantly centered around supervised settings, and
cannot be directly applied to our specific clustering tasks due to the absence
of ground-truth labels. To bridge the gap, we propose a novel method called
\textbf{ho}mophily-enhanced structure \textbf{le}arning for graph clustering
(HoLe). Our motivation stems from the observation that subtly enhancing the
degree of homophily within the graph structure can significantly improve GNNs
and clustering outcomes. To realize this objective, we develop two
clustering-oriented structure learning modules, i.e., hierarchical correlation
estimation and cluster-aware sparsification. The former module enables a more
accurate estimation of pairwise node relationships by leveraging guidance from
latent and clustering spaces, while the latter one generates a sparsified
structure based on the similarity matrix and clustering assignments.
Additionally, we devise a joint optimization approach alternating between
training the homophily-enhanced structure learning and GNN-based clustering,
thereby enforcing their reciprocal effects. Extensive experiments on seven
benchmark datasets of various types and scales, across a range of clustering
metrics, demonstrate the superiority of HoLe against state-of-the-art
baselines.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Byzantine-Robust Decentralized Stochastic Optimization with Stochastic  Gradient Noise-Independent Learning Error</b></summary>
  <p><b>编号</b>：[180]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05292</p>
  <p><b>作者</b>：Jie Peng,  Weiyu Li,  Qing Ling</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：exchange local models, agent periodically communicates, paper studies Byzantine-robust, local models, exchange local</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper studies Byzantine-robust stochastic optimization over a
decentralized network, where every agent periodically communicates with its
neighbors to exchange local models, and then updates its own local model by
stochastic gradient descent (SGD). The performance of such a method is affected
by an unknown number of Byzantine agents, which conduct adversarially during
the optimization process. To the best of our knowledge, there is no existing
work that simultaneously achieves a linear convergence speed and a small
learning error. We observe that the learning error is largely dependent on the
intrinsic stochastic gradient noise. Motivated by this observation, we
introduce two variance reduction methods, stochastic average gradient algorithm
(SAGA) and loopless stochastic variance-reduced gradient (LSVRG), to
Byzantine-robust decentralized stochastic optimization for eliminating the
negative effect of the stochastic gradient noise. The two resulting methods,
BRAVO-SAGA and BRAVO-LSVRG, enjoy both linear convergence speeds and stochastic
gradient noise-independent learning errors. Such learning errors are optimal
for a class of methods based on total variation (TV)-norm regularization and
stochastic subgradient update. We conduct extensive numerical experiments to
demonstrate their effectiveness under various Byzantine attacks.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：Investigating disaster response through social media data and the  Susceptible-Infected-Recovered (SIR) model: A case study of 2020 Western U.S.  wildfire season</b></summary>
  <p><b>编号</b>：[185]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05281</p>
  <p><b>作者</b>：Zihui Ma,  Lingyao Li,  Libby Hemphill,  Gregory B. Baecher</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Effective disaster response, critical for affected, social media, Effective disaster, affected communities</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Effective disaster response is critical for affected communities. Responders
and decision-makers would benefit from reliable, timely measures of the issues
impacting their communities during a disaster, and social media offers a
potentially rich data source. Social media can reflect public concerns and
demands during a disaster, offering valuable insights for decision-makers to
understand evolving situations and optimize resource allocation. We used
Bidirectional Encoder Representations from Transformers (BERT) topic modeling
to cluster topics from Twitter data. Then, we conducted a temporal-spatial
analysis to examine the distribution of these topics across different regions
during the 2020 western U.S. wildfire season. Our results show that Twitter
users mainly focused on three topics:"health impact," "damage," and
"evacuation." We used the Susceptible-Infected-Recovered (SIR) theory to
explore the magnitude and velocity of topic diffusion on Twitter. The results
displayed a clear relationship between topic trends and wildfire propagation
patterns. The estimated parameters obtained from the SIR model in selected
cities revealed that residents exhibited a high level of several concerns
during the wildfire. Our study details how the SIR model and topic modeling
using social media data can provide decision-makers with a quantitative
approach to measure disaster response and support their decision-making
processes.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Cross-heterogeneity Graph Few-shot Learning</b></summary>
  <p><b>编号</b>：[189]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05275</p>
  <p><b>作者</b>：Pengfei Ding,  Yan Wang,  Guanfeng Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：label sparsity issue, graph few-shot learning, recent years, proposed to address, address the label</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, heterogeneous graph few-shot learning has been proposed to
address the label sparsity issue in heterogeneous graphs (HGs), which contain
various types of nodes and edges. The existing methods have achieved good
performance by transferring generalized knowledge extracted from rich-labeled
classes in source HG(s) to few-labeled classes in a target HG. However, these
methods only consider the single-heterogeneity scenario where the source and
target HGs share a fixed set of node/edge types, ignoring the more general
scenario of cross-heterogeneity, where each HG can have a different and
non-fixed set of node/edge types. To this end, we focus on the unexplored
cross-heterogeneity scenario and propose a novel model for Cross-heterogeneity
Graph Few-shot Learning, namely CGFL. In CGFL, we first extract meta-patterns
to capture heterogeneous information and propose a multi-view heterogeneous
graph neural network (MHGN) to learn meta-patterns across HGs. Then, we propose
a score module to measure the informativeness of labeled samples and determine
the transferability of each source HG. Finally, by integrating MHGN and the
score module into a meta-learning mechanism, CGFL can effectively transfer
generalized knowledge to predict new classes with few-labeled data. Extensive
experiments on four real-world datasets have demonstrated the superior
performance of CGFL over the state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Data-driven Intra-Autonomous Systems Graph Generator</b></summary>
  <p><b>编号</b>：[198]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05254</p>
  <p><b>作者</b>：Caio Vinicius Dadauto,  Nelson Luis Saldanha da Fonseca,  Ricardo da Silva Torres</p>
  <p><b>备注</b>：12 pages, 15 figures</p>
  <p><b>关键词</b>：represent intra-Autonomous System, intra-Autonomous System, deep-learning based generator, paper introduces, deep-learning based</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper introduces a novel deep-learning based generator of synthetic
graphs that represent intra-Autonomous System (AS) in the Internet, named
Deep-generative graphs for the Internet (DGGI). It also presents a novel
massive dataset of real intra-AS graphs extracted from the project Internet
Topology Data Kit (ITDK), called Internet Graphs (IGraphs). To create IGraphs,
the Filtered Recurrent Multi-level (FRM) algorithm for community extraction was
developed. It is shown that DGGI creates synthetic graphs which accurately
reproduce the properties of centrality, clustering, assortativity, and node
degree. The DGGI generator overperforms existing Internet topology generators.
On average, DGGI improves the Maximum Mean Discrepancy (MMD) metric 84.4%,
95.1%, 97.9%, and 94.7% for assortativity, betweenness, clustering, and node
degree, respectively.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：AI-Enabled Software and System Architecture Frameworks: Focusing on  smart Cyber-Physical Systems (CPS)</b></summary>
  <p><b>编号</b>：[202]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05239</p>
  <p><b>作者</b>：Armin Moin,  Atta Badii,  Stephan Günnemann,  Moharram Challenger</p>
  <p><b>备注</b>：Technical Report</p>
  <p><b>关键词</b>：architecture frameworks, defined architecture viewpoints, architecture viewpoints, architecture, existing architecture frameworks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Several architecture frameworks for software, systems, and enterprises have
been proposed in the literature. They identified various stakeholders and
defined architecture viewpoints and views to frame and address stakeholder
concerns. However, the stakeholders with data science and Machine Learning (ML)
related concerns, such as data scientists and data engineers, are yet to be
included in existing architecture frameworks. Therefore, they failed to address
the architecture viewpoints and views responsive to the concerns of the data
science community. In this paper, we address this gap by establishing the
architecture frameworks adapted to meet the requirements of modern applications
and organizations where ML artifacts are both prevalent and crucial. In
particular, we focus on ML-enabled Cyber-Physical Systems (CPSs) and propose
two sets of merit criteria for their efficient development and performance
assessment, namely the criteria for evaluating and benchmarking ML-enabled
CPSs, and the criteria for evaluation and benchmarking of the tools intended to
support users through the modeling and development pipeline. In this study, we
deploy multiple empirical and qualitative research methods based on literature
review and survey instruments including expert interviews and an online
questionnaire. We collect, analyze, and integrate the opinions of 77 experts
from more than 25 organizations in over 10 countries to devise and validate the
proposed framework.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：Spatial Gated Multi-Layer Perceptron for Land Use and Land Cover Mapping</b></summary>
  <p><b>编号</b>：[203]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05235</p>
  <p><b>作者</b>：Ali Jamali,  Swalpa Kumar Roy,  Danfeng Hong,  Peter M Atkinson,  Pedram Ghamisi</p>
  <p><b>备注</b>：Submitted in IEEE</p>
  <p><b>关键词</b>：Convolutional Neural Networks, Neural Networks, Convolutional Neural, extraction of features, utilized extensively</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Convolutional Neural Networks (CNNs) are models that are utilized extensively
for the hierarchical extraction of features. Vision transformers (ViTs),
through the use of a self-attention mechanism, have recently achieved superior
modeling of global contextual information compared to CNNs. However, to realize
their image classification strength, ViTs require substantial training
datasets. Where the available training data are limited, current advanced
multi-layer perceptrons (MLPs) can provide viable alternatives to both deep
CNNs and ViTs. In this paper, we developed the SGU-MLP, a learning algorithm
that effectively uses both MLPs and spatial gating units (SGUs) for precise
land use land cover (LULC) mapping. Results illustrated the superiority of the
developed SGU-MLP classification algorithm over several CNN and CNN-ViT-based
models, including HybridSN, ResNet, iFormer, EfficientFormer and CoAtNet. The
proposed SGU-MLP algorithm was tested through three experiments in Houston,
USA, Berlin, Germany and Augsburg, Germany. The SGU-MLP classification model
was found to consistently outperform the benchmark CNN and CNN-ViT-based
algorithms. For example, for the Houston experiment, SGU-MLP significantly
outperformed HybridSN, CoAtNet, Efficientformer, iFormer and ResNet by
approximately 15%, 19%, 20%, 21%, and 25%, respectively, in terms of average
accuracy. The code will be made publicly available at
this https URL</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Leveraging the Edge and Cloud for V2X-Based Real-Time Object Detection  in Autonomous Driving</b></summary>
  <p><b>编号</b>：[204]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05234</p>
  <p><b>作者</b>：Faisal Hawlader,  François Robinet,  Raphaël Frank</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：module influences core, core driving decisions, influences core driving, perception module influences, key element</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Environmental perception is a key element of autonomous driving because the
information received from the perception module influences core driving
decisions. An outstanding challenge in real-time perception for autonomous
driving lies in finding the best trade-off between detection quality and
latency. Major constraints on both computation and power have to be taken into
account for real-time perception in autonomous vehicles. Larger object
detection models tend to produce the best results, but are also slower at
runtime. Since the most accurate detectors cannot run in real-time locally, we
investigate the possibility of offloading computation to edge and cloud
platforms, which are less resource-constrained. We create a synthetic dataset
to train object detection models and evaluate different offloading strategies.
Using real hardware and network simulations, we compare different trade-offs
between prediction quality and end-to-end delay. Since sending raw frames over
the network implies additional transmission delays, we also explore the use of
JPEG and H.265 compression at varying qualities and measure their impact on
prediction metrics. We show that models with adequate compression can be run in
real-time on the cloud while outperforming local detection performance.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：SegMatch: A semi-supervised learning method for surgical instrument  segmentation</b></summary>
  <p><b>编号</b>：[205]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05232</p>
  <p><b>作者</b>：Meng Wei,  Charlie Budd,  Luis C. Garcia-Peraza-Herrera,  Reuben Dorent,  Miaojing Shi,  Tom Vercauteren</p>
  <p><b>备注</b>：preprint under review, 12 pages, 7 figures</p>
  <p><b>关键词</b>：computer assisted interventions, improve computer assisted, provide advanced surgical, advanced surgical assistance, Surgical instrument segmentation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Surgical instrument segmentation is recognised as a key enabler to provide
advanced surgical assistance and improve computer assisted interventions. In
this work, we propose SegMatch, a semi supervised learning method to reduce the
need for expensive annotation for laparoscopic and robotic surgical images.
SegMatch builds on FixMatch, a widespread semi supervised classification
pipeline combining consistency regularization and pseudo labelling, and adapts
it for the purpose of segmentation. In our proposed SegMatch, the unlabelled
images are weakly augmented and fed into the segmentation model to generate a
pseudo-label to enforce the unsupervised loss against the output of the model
for the adversarial augmented image on the pixels with a high confidence score.
Our adaptation for segmentation tasks includes carefully considering the
equivariance and invariance properties of the augmentation functions we rely
on. To increase the relevance of our augmentations, we depart from using only
handcrafted augmentations and introduce a trainable adversarial augmentation
strategy. Our algorithm was evaluated on the MICCAI Instrument Segmentation
Challenge datasets Robust-MIS 2019 and EndoVis 2017. Our results demonstrate
that adding unlabelled data for training purposes allows us to surpass the
performance of fully supervised approaches which are limited by the
availability of training data in these challenges. SegMatch also outperforms a
range of state-of-the-art semi-supervised learning semantic segmentation models
in different labelled to unlabelled data ratios.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Decoding Layer Saliency in Language Transformers</b></summary>
  <p><b>编号</b>：[209]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05219</p>
  <p><b>作者</b>：Elizabeth M. Hou,  Gregory Castanon</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large-scale language models, language models applied, identifying textual saliency, introduce a strategy, strategy for identifying</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we introduce a strategy for identifying textual saliency in
large-scale language models applied to classification tasks. In visual networks
where saliency is more well-studied, saliency is naturally localized through
the convolutional layers of the network; however, the same is not true in
modern transformer-stack networks used to process natural language. We adapt
gradient-based saliency methods for these networks, propose a method for
evaluating the degree of semantic coherence of each layer, and demonstrate
consistent improvement over numerous other methods for textual saliency on
multiple benchmark classification datasets. Our approach requires no additional
training or access to labelled data, and is comparatively very computationally
efficient.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Conformer-based Target-Speaker Automatic Speech Recognition for  Single-Channel Audio</b></summary>
  <p><b>编号</b>：[210]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05218</p>
  <p><b>作者</b>：Yang Zhang,  Krishna C. Puvvada,  Vitaly Lavrukhin,  Boris Ginsburg</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：time-frequency domain architecture, automatic speech recognition, propose CONF-TSASR, time-frequency domain, single-channel target-speaker automatic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose CONF-TSASR, a non-autoregressive end-to-end time-frequency domain
architecture for single-channel target-speaker automatic speech recognition
(TS-ASR). The model consists of a TitaNet based speaker embedding module, a
Conformer based masking as well as ASR modules. These modules are jointly
optimized to transcribe a target-speaker, while ignoring speech from other
speakers. For training we use Connectionist Temporal Classification (CTC) loss
and introduce a scale-invariant spectrogram reconstruction loss to encourage
the model better separate the target-speaker's spectrogram from mixture. We
obtain state-of-the-art target-speaker word error rate (TS-WER) on
WSJ0-2mix-extr (4.2%). Further, we report for the first time TS-WER on
WSJ0-3mix-extr (12.4%), LibriSpeech2Mix (4.2%) and LibriSpeech3Mix (7.6%)
datasets, establishing new benchmarks for TS-ASR. The proposed model will be
open-sourced through NVIDIA NeMo toolkit.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Evaluating Pedestrian Trajectory Prediction Methods for the Application  in Autonomous Driving</b></summary>
  <p><b>编号</b>：[215]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05194</p>
  <p><b>作者</b>：Nico Uhlemann,  Felix Fent,  Markus Lienkamp</p>
  <p><b>备注</b>：Submitted to the IEEE Transactions on Intelligent Transportation Systems (T-ITS); 9 pages, 5 figures, 4 tables</p>
  <p><b>关键词</b>：constant velocity model, Final Displacement Error, autonomous vehicles, field of pedestrian, evaluated alongside</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, the state of the art in the field of pedestrian trajectory
prediction is evaluated alongside the constant velocity model (CVM) with
respect to its applicability in autonomous vehicles. The evaluation is
conducted on the widely-used ETH/UCY dataset where the Average Displacement
Error (ADE) and the Final Displacement Error (FDE) are reported. To align with
requirements in real-world applications, modifications are made to the input
features of the initially proposed models. An ablation study is conducted to
examine the influence of the observed motion history on the prediction
performance, thereby establishing a better understanding of its impact.
Additionally, the inference time of each model is measured to evaluate the
scalability of each model when confronted with varying amounts of agents. The
results demonstrate that simple models remain competitive when generating
single trajectories, and certain features commonly thought of as useful have
little impact on the overall performance across different architectures. Based
on these findings, recommendations are proposed to guide the future development
of trajectory prediction algorithms.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Hierarchical Representations for Spatio-Temporal Visual Attention  Modeling and Understanding</b></summary>
  <p><b>编号</b>：[216]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05189</p>
  <p><b>作者</b>：Miguel-Ángel Fernández-Torres</p>
  <p><b>备注</b>：PhD thesis</p>
  <p><b>关键词</b>：visual attention, visual attention modeling, spatio-temporal visual attention, attention, attention modeling</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This PhD. Thesis concerns the study and development of hierarchical
representations for spatio-temporal visual attention modeling and understanding
in video sequences. More specifically, we propose two computational models for
visual attention. First, we present a generative probabilistic model for
context-aware visual attention modeling and understanding. Secondly, we develop
a deep network architecture for visual attention modeling, which first
estimates top-down spatio-temporal visual attention, and ultimately serves for
modeling attention in the temporal domain.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Sound propagation in realistic interactive 3D scenes with parameterized  sources using deep neural operators</b></summary>
  <p><b>编号</b>：[223]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05141</p>
  <p><b>作者</b>：Nikolas Borrel-Jensen,  Somdatta Goswami,  Allan P. Engsig-Karup,  George Em Karniadakis,  Cheol-Ho Jeong</p>
  <p><b>备注</b>：25 pages, 10 figures, 4 tables</p>
  <p><b>关键词</b>：augmented reality, game audio, spatial computing, address the challenge, sound propagation simulations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We address the challenge of sound propagation simulations in $3$D virtual
rooms with moving sources, which have applications in virtual/augmented
reality, game audio, and spatial computing. Solutions to the wave equation can
describe wave phenomena such as diffraction and interference. However,
simulating them using conventional numerical discretization methods with
hundreds of source and receiver positions is intractable, making stimulating a
sound field with moving sources impractical. To overcome this limitation, we
propose using deep operator networks to approximate linear wave-equation
operators. This enables the rapid prediction of sound propagation in realistic
3D acoustic scenes with moving sources, achieving millisecond-scale
computations. By learning a compact surrogate model, we avoid the offline
calculation and storage of impulse responses for all relevant source/listener
pairs. Our experiments, including various complex scene geometries, show good
agreement with reference solutions, with root mean squared errors ranging from
0.02 Pa to 0.10 Pa. Notably, our method signifies a paradigm shift as no prior
machine learning approach has achieved precise predictions of complete wave
fields within realistic domains. We anticipate that our findings will drive
further exploration of deep neural operator methods, advancing research in
immersive user experiences within virtual environments.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Data-Free Model Extraction Attacks in the Context of Object Detection</b></summary>
  <p><b>编号</b>：[228]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05127</p>
  <p><b>作者</b>：Harshit Shah,  Aravindhan G,  Pavan Kulkarni,  Yuvaraj Govidarajulu,  Manojkumar Parmar</p>
  <p><b>备注</b>：Submitted to The 14th International Conference on Computer Vision Systems (ICVS 2023), to be published in Springer, Lecture Notes in Computer Science</p>
  <p><b>关键词</b>：machine learning models, target model, Generative Adversarial Nets, number of machine, machine learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A significant number of machine learning models are vulnerable to model
extraction attacks, which focus on stealing the models by using specially
curated queries against the target model. This task is well accomplished by
using part of the training data or a surrogate dataset to train a new model
that mimics a target model in a white-box environment. In pragmatic situations,
however, the target models are trained on private datasets that are
inaccessible to the adversary. The data-free model extraction technique
replaces this problem when it comes to using queries artificially curated by a
generator similar to that used in Generative Adversarial Nets. We propose for
the first time, to the best of our knowledge, an adversary black box attack
extending to a regression problem for predicting bounding box coordinates in
object detection. As part of our study, we found that defining a loss function
and using a novel generator setup is one of the key aspects in extracting the
target model. We find that the proposed model extraction method achieves
significant results by using reasonable queries. The discovery of this object
detection vulnerability will support future prospects for securing such models.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Dynamic Model Agnostic Reliability Evaluation of Machine-Learning  Methods Integrated in Instrumentation & Control Systems</b></summary>
  <p><b>编号</b>：[230]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05120</p>
  <p><b>作者</b>：Edward Chen,  Han Bao,  Nam Dinh</p>
  <p><b>备注</b>：This paper was originally presented at the 13th Nuclear Plant Instrumentation, Control & Human-Machine Interface Technologies conference and was awarded best student paper</p>
  <p><b>关键词</b>：network-based machine learning, machine learning, field of data-driven, spurred research, applicability to instrumentation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, the field of data-driven neural network-based machine
learning (ML) algorithms has grown significantly and spurred research in its
applicability to instrumentation and control systems. While they are promising
in operational contexts, the trustworthiness of such algorithms is not
adequately assessed. Failures of ML-integrated systems are poorly understood;
the lack of comprehensive risk modeling can degrade the trustworthiness of
these systems. In recent reports by the National Institute for Standards and
Technology, trustworthiness in ML is a critical barrier to adoption and will
play a vital role in intelligent systems' safe and accountable operation. Thus,
in this work, we demonstrate a real-time model-agnostic method to evaluate the
relative reliability of ML predictions by incorporating out-of-distribution
detection on the training dataset. It is well documented that ML algorithms
excel at interpolation (or near-interpolation) tasks but significantly degrade
at extrapolation. This occurs when new samples are "far" from training samples.
The method, referred to as the Laplacian distributed decay for reliability
(LADDR), determines the difference between the operational and training
datasets, which is used to calculate a prediction's relative reliability. LADDR
is demonstrated on a feedforward neural network-based model used to predict
safety significant factors during different loss-of-flow transients. LADDR is
intended as a "data supervisor" and determines the appropriateness of
well-trained ML models in the context of operational conditions. Ultimately,
LADDR illustrates how training data can be used as evidence to support the
trustworthiness of ML predictions when utilized for conventional interpolation
tasks.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Can Attention Be Used to Explain EHR-Based Mortality Prediction Tasks: A  Case Study on Hemorrhagic Stroke</b></summary>
  <p><b>编号</b>：[232]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05110</p>
  <p><b>作者</b>：Qizhang Feng,  Jiayi Yuan,  Forhan Bin Emdad,  Karim Hanna,  Xia Hu,  Zhe He</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Chronic Health Evaluation, Simplified Acute Physiology, Acute Physiology, Acute Physiology Score, Physiology Score III</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Stroke is a significant cause of mortality and morbidity, necessitating early
predictive strategies to minimize risks. Traditional methods for evaluating
patients, such as Acute Physiology and Chronic Health Evaluation (APACHE II,
IV) and Simplified Acute Physiology Score III (SAPS III), have limited accuracy
and interpretability. This paper proposes a novel approach: an interpretable,
attention-based transformer model for early stroke mortality prediction. This
model seeks to address the limitations of previous predictive models, providing
both interpretability (providing clear, understandable explanations of the
model) and fidelity (giving a truthful explanation of the model's dynamics from
input to output). Furthermore, the study explores and compares fidelity and
interpretability scores using Shapley values and attention-based scores to
improve model explainability. The research objectives include designing an
interpretable attention-based transformer model, evaluating its performance
compared to existing models, and providing feature importance derived from the
model.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Balancing Accuracy and Training Time in Federated Learning for Violence  Detection in Surveillance Videos: A Study of Neural Network Architectures</b></summary>
  <p><b>编号</b>：[233]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05106</p>
  <p><b>作者</b>：Pajon Quentin,  Serre Swan,  Wissocq Hugo,  Rabaud Léo,  Haidar Siba,  Yaacoub Antoun</p>
  <p><b>备注</b>：8 pages, 2 figures, FL-IJCAI'23</p>
  <p><b>关键词</b>：federated learning context, machine learning techniques, federated learning, paper presents, presents an investigation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents an investigation into machine learning techniques for
violence detection in videos and their adaptation to a federated learning
context. The study includes experiments with spatio-temporal features extracted
from benchmark video datasets, comparison of different methods, and proposal of
a modified version of the "Flow-Gated" architecture called "Diff-Gated."
Additionally, various machine learning techniques, including super-convergence
and transfer learning, are explored, and a method for adapting centralized
datasets to a federated learning context is developed. The research achieves
better accuracy results compared to state-of-the-art models by training the
best violence detection model in a federated learning context.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Updating Clinical Risk Stratification Models Using Rank-Based  Compatibility: Approaches for Evaluating and Optimizing Clinician-Model Team  Performance</b></summary>
  <p><b>编号</b>：[239]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05619</p>
  <p><b>作者</b>：Erkin Ötleş,  Brian T. Denton,  Jenna Wiens</p>
  <p><b>备注</b>：Conference paper accepted at the 2023 Machine Learning for Healthcare Conference Includes supplemental: 32 pages, 17 figures</p>
  <p><b>关键词</b>：machine learning models, clinical machine learning, updating clinical machine, machine learning, maintain or improve</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As data shift or new data become available, updating clinical machine
learning models may be necessary to maintain or improve performance over time.
However, updating a model can introduce compatibility issues when the behavior
of the updated model does not align with user expectations, resulting in poor
user-model team performance. Existing compatibility measures depend on model
decision thresholds, limiting their applicability in settings where models are
used to generate rankings based on estimated risk. To address this limitation,
we propose a novel rank-based compatibility measure, $C^R$, and a new loss
function that aims to optimize discriminative performance while encouraging
good compatibility. Applied to a case study in mortality risk stratification
leveraging data from MIMIC, our approach yields more compatible models while
maintaining discriminative performance compared to existing model selection
techniques, with an increase in $C^R$ of $0.019$ ($95\%$ confidence interval:
$0.005$, $0.035$). This work provides new tools to analyze and update risk
stratification models used in clinical care.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：Efficient Variational Inference for Large Skew-t Copulas with  Application to Intraday Equity Returns</b></summary>
  <p><b>编号</b>：[240]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05564</p>
  <p><b>作者</b>：Lin Deng,  Michael Stanley Smith,  Worapree Maneesoonthorn</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large skew-t factor, skew-t factor copula, modeling of financial, financial data, factor copula models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large skew-t factor copula models are attractive for the modeling of
financial data because they allow for asymmetric and extreme tail dependence.
We show that the copula implicit in the skew-t distribution of Azzalini and
Capitanio (2003) allows for a higher level of pairwise asymmetric dependence
than two popular alternative skew-t copulas. Estimation of this copula in high
dimensions is challenging, and we propose a fast and accurate Bayesian
variational inference (VI) approach to do so. The method uses a conditionally
Gaussian generative representation of the skew-t distribution to define an
augmented posterior that can be approximated accurately. A fast stochastic
gradient ascent algorithm is used to solve the variational optimization. The
new methodology is used to estimate copula models for intraday returns from
2017 to 2021 on 93 U.S. equities. The copula captures substantial heterogeneity
in asymmetric dependence over equity pairs, in addition to the variability in
pairwise correlations. We show that intraday predictive densities from the
skew-t copula are more accurate than from some other copula models, while
portfolio selection strategies based on the estimated pairwise tail
dependencies improve performance relative to the benchmark index.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：A Forecaster's Review of Judea Pearl's Causality: Models, Reasoning and  Inference, Second Edition, 2009</b></summary>
  <p><b>编号</b>：[245]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05451</p>
  <p><b>作者</b>：Feng Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Judea Pearl original, Pearl original causality, original causality book, main topics updated, Judea Pearl</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the big popularity and success of Judea Pearl's original causality book,
this review covers the main topics updated in the second edition in 2009 and
illustrates an easy-to-follow causal inference strategy in a forecast scenario.
It further discusses some potential benefits and challenges for causal
inference with time series forecasting when modeling the counterfactuals,
estimating the uncertainty and incorporating prior knowledge to estimate causal
effects in different forecasting scenarios.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：A Comparison of Classical and Deep Reinforcement Learning Methods for  HVAC Control</b></summary>
  <p><b>编号</b>：[251]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05711</p>
  <p><b>作者</b>：Marshall Wang,  John Willes,  Thomas Jiralerspong,  Matin Moezzi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：optimizing HVAC control, Reinforcement learning, promising approach, approach for optimizing, HVAC control</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reinforcement learning (RL) is a promising approach for optimizing HVAC
control. RL offers a framework for improving system performance, reducing
energy consumption, and enhancing cost efficiency. We benchmark two popular
classical and deep RL methods (Q-Learning and Deep-Q-Networks) across multiple
HVAC environments and explore the practical consideration of model
hyper-parameter selection and reward tuning. The findings provide insight for
configuring RL agents in HVAC systems, promoting energy-efficient and
cost-effective operation.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Shadow Datasets, New challenging datasets for Causal Representation  Learning</b></summary>
  <p><b>编号</b>：[253]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05707</p>
  <p><b>作者</b>：Jiageng Zhu,  Hanchen Xie,  Jianhua Wu,  Jiazhi Li,  Mahyar Khayatkhoei,  Mohamed E. Hussein,  Wael AbdAlmageed</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：representation learning, Discovering causal relations, CelebA, relations among semantic, emergent topic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Discovering causal relations among semantic factors is an emergent topic in
representation learning. Most causal representation learning (CRL) methods are
fully supervised, which is impractical due to costly labeling. To resolve this
restriction, weakly supervised CRL methods were introduced. To evaluate CRL
performance, four existing datasets, Pendulum, Flow, CelebA(BEARD) and
CelebA(SMILE), are utilized. However, existing CRL datasets are limited to
simple graphs with few generative factors. Thus we propose two new datasets
with a larger number of diverse generative factors and more sophisticated
causal graphs. In addition, current real datasets, CelebA(BEARD) and
CelebA(SMILE), the originally proposed causal graphs are not aligned with the
dataset distributions. Thus, we propose modifications to them.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Hard No-Box Adversarial Attack on Skeleton-Based Human Action  Recognition with Skeleton-Motion-Informed Gradient</b></summary>
  <p><b>编号</b>：[263]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05681</p>
  <p><b>作者</b>：Zhengzhi Lu,  He Wang,  Ziyi Chang,  Guoan Yang,  Hubert P. H. Shum</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：skeleton-based human activity, human activity recognition, attack methods, skeleton-based human, human activity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, methods for skeleton-based human activity recognition have been
shown to be vulnerable to adversarial attacks. However, these attack methods
require either the full knowledge of the victim (i.e. white-box attacks),
access to training data (i.e. transfer-based attacks) or frequent model queries
(i.e. black-box attacks). All their requirements are highly restrictive,
raising the question of how detrimental the vulnerability is. In this paper, we
show that the vulnerability indeed exists. To this end, we consider a new
attack task: the attacker has no access to the victim model or the training
data or labels, where we coin the term hard no-box attack. Specifically, we
first learn a motion manifold where we define an adversarial loss to compute a
new gradient for the attack, named skeleton-motion-informed (SMI) gradient. Our
gradient contains information of the motion dynamics, which is different from
existing gradient-based attack methods that compute the loss gradient assuming
each dimension in the data is independent. The SMI gradient can augment many
gradient-based attack methods, leading to a new family of no-box attack
methods. Extensive evaluation and comparison show that our method imposes a
real threat to existing classifiers. They also show that the SMI gradient
improves the transferability and imperceptibility of adversarial samples in
both no-box and transfer-based black-box settings.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：Finding Already Debunked Narratives via Multistage Retrieval: Enabling  Cross-Lingual, Cross-Dataset and Zero-Shot Learning</b></summary>
  <p><b>编号</b>：[264]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05680</p>
  <p><b>作者</b>：Iknoor Singh,  Carolina Scarton,  Xingyi Song,  Kalina Bontcheva</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：debunked narratives aims, aims to detect, detect stories, debunked narratives, Transformer models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The task of retrieving already debunked narratives aims to detect stories
that have already been fact-checked. The successful detection of claims that
have already been debunked not only reduces the manual efforts of professional
fact-checkers but can also contribute to slowing the spread of misinformation.
Mainly due to the lack of readily available data, this is an understudied
problem, particularly when considering the cross-lingual task, i.e. the
retrieval of fact-checking articles in a language different from the language
of the online post being checked. This paper fills this gap by (i) creating a
novel dataset to enable research on cross-lingual retrieval of already debunked
narratives, using tweets as queries to a database of fact-checking articles;
(ii) presenting an extensive experiment to benchmark fine-tuned and
off-the-shelf multilingual pre-trained Transformer models for this task; and
(iii) proposing a novel multistage framework that divides this cross-lingual
debunk retrieval task into refinement and re-ranking stages. Results show that
the task of cross-lingual retrieval of already debunked narratives is
challenging and off-the-shelf Transformer models fail to outperform a strong
lexical-based baseline (BM25). Nevertheless, our multistage retrieval framework
is robust, outperforming BM25 in most scenarios and enabling cross-domain and
zero-shot learning, without significantly harming the model's performance.</p>
  </details>
</details>
<h1>人工智能</h1>
<details>
  <summary>1. <b>标题：Neural Progressive Meshes</b></summary>
  <p><b>编号</b>：[3]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05741</p>
  <p><b>作者</b>：Yun-Chun Chen,  Vladimir G. Kim,  Noam Aigerman,  Alec Jacobson</p>
  <p><b>备注</b>：SIGGRAPH 2023</p>
  <p><b>关键词</b>：hand-held devices necessitates, devices necessitates efficient, necessitates efficient tools, recent proliferation, consumed on hand-held</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The recent proliferation of 3D content that can be consumed on hand-held
devices necessitates efficient tools for transmitting large geometric data,
e.g., 3D meshes, over the Internet. Detailed high-resolution assets can pose a
challenge to storage as well as transmission bandwidth, and level-of-detail
techniques are often used to transmit an asset using an appropriate bandwidth
budget. It is especially desirable for these methods to transmit data
progressively, improving the quality of the geometry with more data. Our key
insight is that the geometric details of 3D meshes often exhibit similar local
patterns even across different shapes, and thus can be effectively represented
with a shared learned generative space. We learn this space using a
subdivision-based encoder-decoder architecture trained in advance on a large
collection of surfaces. We further observe that additional residual features
can be transmitted progressively between intermediate levels of subdivision
that enable the client to control the tradeoff between bandwidth cost and
quality of reconstruction, providing a neural progressive mesh representation.
We evaluate our method on a diverse set of complex 3D shapes and demonstrate
that it outperforms baselines in terms of compression ratio and reconstruction
quality.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：AudioLDM 2: Learning Holistic Audio Generation with Self-supervised  Pretraining</b></summary>
  <p><b>编号</b>：[7]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05734</p>
  <p><b>作者</b>：Haohe Liu,  Qiao Tian,  Yi Yuan,  Xubo Liu,  Xinhao Mei,  Qiuqiang Kong,  Yuping Wang,  Wenwu Wang,  Yuxuan Wang,  Mark D. Plumbley</p>
  <p><b>备注</b>：AudioLDM 2 project page is this https URL</p>
  <p><b>关键词</b>：type requires careful, requires careful consideration, generation shares commonalities, sound effect generation, audio generation shares</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Although audio generation shares commonalities across different types of
audio, such as speech, music, and sound effects, designing models for each type
requires careful consideration of specific objectives and biases that can
significantly differ from those of other types. To bring us closer to a unified
perspective of audio generation, this paper proposes a framework that utilizes
the same learning method for speech, music, and sound effect generation. Our
framework introduces a general representation of audio, called language of
audio (LOA). Any audio can be translated into LOA based on AudioMAE, a
self-supervised pre-trained representation learning model. In the generation
process, we translate any modalities into LOA by using a GPT-2 model, and we
perform self-supervised audio generation learning with a latent diffusion model
conditioned on LOA. The proposed framework naturally brings advantages such as
in-context learning abilities and reusable self-supervised pretrained AudioMAE
and latent diffusion models. Experiments on the major benchmarks of
text-to-audio, text-to-music, and text-to-speech demonstrate new
state-of-the-art or competitive performance to previous approaches. Our demo
and code are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers</b></summary>
  <p><b>编号</b>：[9]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05732</p>
  <p><b>作者</b>：Phillip Lippe,  Bastiaan S. Veeling,  Paris Perdikaris,  Richard E. Turner,  Johannes Brandstetter</p>
  <p><b>备注</b>：Project website: this https URL</p>
  <p><b>关键词</b>：Time-dependent partial differential, partial differential equations, Time-dependent partial, differential equations, science and engineering</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Time-dependent partial differential equations (PDEs) are ubiquitous in
science and engineering. Recently, mostly due to the high computational cost of
traditional solution techniques, deep neural network based surrogates have
gained increased interest. The practical utility of such neural PDE solvers
relies on their ability to provide accurate, stable predictions over long time
horizons, which is a notoriously hard problem. In this work, we present a
large-scale analysis of common temporal rollout strategies, identifying the
neglect of non-dominant spatial frequency information, often associated with
high frequencies in PDE solutions, as the primary pitfall limiting stable,
accurate rollout performance. Based on these insights, we draw inspiration from
recent advances in diffusion models to introduce PDE-Refiner; a novel model
class that enables more accurate modeling of all frequency components via a
multistep refinement process. We validate PDE-Refiner on challenging benchmarks
of complex fluid dynamics, demonstrating stable and accurate rollouts that
consistently outperform state-of-the-art models, including neural, numerical,
and hybrid neural-numerical architectures. We further demonstrate that
PDE-Refiner greatly enhances data efficiency, since the denoising objective
implicitly induces a novel form of spectral data augmentation. Finally,
PDE-Refiner's connection to diffusion models enables an accurate and efficient
assessment of the model's predictive uncertainty, allowing us to estimate when
the surrogate becomes inaccurate.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Rethinking Integration of Prediction and Planning in Deep Learning-Based  Automated Driving Systems: A Review</b></summary>
  <p><b>编号</b>：[10]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05731</p>
  <p><b>作者</b>：Steffen Hagedorn,  Marcel Hallgarten,  Martin Stoll,  Alexandru Condurache</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Automated driving, revolutionize personal, freight mobility, potential to revolutionize, automated driving comprises</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automated driving has the potential to revolutionize personal, public, and
freight mobility. Besides the enormous challenge of perception, i.e. accurately
perceiving the environment using available sensor data, automated driving
comprises planning a safe, comfortable, and efficient motion trajectory. To
promote safety and progress, many works rely on modules that predict the future
motion of surrounding traffic. Modular automated driving systems commonly
handle prediction and planning as sequential separate tasks. While this
accounts for the influence of surrounding traffic on the ego-vehicle, it fails
to anticipate the reactions of traffic participants to the ego-vehicle's
behavior. Recent works suggest that integrating prediction and planning in an
interdependent joint step is necessary to achieve safe, efficient, and
comfortable driving. While various models implement such integrated systems, a
comprehensive overview and theoretical understanding of different principles
are lacking. We systematically review state-of-the-art deep learning-based
prediction, planning, and integrated prediction and planning models. Different
facets of the integration ranging from model architecture and model design to
behavioral aspects are considered and related to each other. Moreover, we
discuss the implications, strengths, and limitations of different integration
methods. By pointing out research gaps, describing relevant future challenges,
and highlighting trends in the research field, we identify promising directions
for future research.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math  and science problems</b></summary>
  <p><b>编号</b>：[14]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05713</p>
  <p><b>作者</b>：Ernest Davis,  Scott Aaronson</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language model, Code Interpreter plug-ins, language model, report describes, large language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This report describes a test of the large language model GPT-4 with the
Wolfram Alpha and the Code Interpreter plug-ins on 105 original problems in
science and math, at the high school and college levels, carried out in
June-August 2023. Our tests suggest that the plug-ins significantly enhance
GPT's ability to solve these problems. Having said that, there are still often
"interface" failures; that is, GPT often has trouble formulating problems in a
way that elicits useful answers from the plug-ins. Fixing these interface
failures seems like a central challenge in making GPT a reliable tool for
college-level calculation problems.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Exploring the Potential of World Models for Anomaly Detection in  Autonomous Driving</b></summary>
  <p><b>编号</b>：[18]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05701</p>
  <p><b>作者</b>：Daniel Bogdoll,  Lukas Bosch,  Tim Joseph,  Helen Gremmelmaier,  Yitian Yang,  J. Marius Zöllner</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：recent years, remarkable advancements, autonomous driving, world models, autonomous vehicles demonstrate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years there have been remarkable advancements in autonomous
driving. While autonomous vehicles demonstrate high performance in closed-set
conditions, they encounter difficulties when confronted with unexpected
situations. At the same time, world models emerged in the field of model-based
reinforcement learning as a way to enable agents to predict the future
depending on potential actions. This led to outstanding results in sparse
reward and complex control tasks. This work provides an overview of how world
models can be leveraged to perform anomaly detection in the domain of
autonomous driving. We provide a characterization of world models and relate
individual components to previous works in anomaly detection to facilitate
further research in the field.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：SSLRec: A Self-Supervised Learning Library for Recommendation</b></summary>
  <p><b>编号</b>：[21]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05697</p>
  <p><b>作者</b>：Xubin Ren,  Lianghao Xia,  Yuhao Yang,  Wei Wei,  Tianle Wang,  Xuheng Cai,  Chao Huang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：gained significant interest, gained significant, significant interest, interest in recent, recent years</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Self-supervised learning (SSL) has gained significant interest in recent
years as a solution to address the challenges posed by sparse and noisy data in
recommender systems. Despite the growing number of SSL algorithms designed to
provide state-of-the-art performance in various recommendation scenarios (e.g.,
graph collaborative filtering, sequential recommendation, social
recommendation, KG-enhanced recommendation), there is still a lack of unified
frameworks that integrate recommendation algorithms across different domains.
Such a framework could serve as the cornerstone for self-supervised
recommendation algorithms, unifying the validation of existing methods and
driving the design of new ones. To address this gap, we introduce SSLRec, a
novel benchmark platform that provides a standardized, flexible, and
comprehensive framework for evaluating various SSL-enhanced recommenders. The
SSLRec library features a modular architecture that allows users to easily
evaluate state-of-the-art models and a complete set of data augmentation and
self-supervised toolkits to help create SSL recommendation models with specific
needs. Furthermore, SSLRec simplifies the process of training and evaluating
different recommendation models with consistent and fair settings. Our SSLRec
platform covers a comprehensive set of state-of-the-art SSL-enhanced
recommendation models across different scenarios, enabling researchers to
evaluate these cutting-edge models and drive further innovation in the field.
Our implemented SSLRec framework is available at the source code repository
this https URL.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Hard No-Box Adversarial Attack on Skeleton-Based Human Action  Recognition with Skeleton-Motion-Informed Gradient</b></summary>
  <p><b>编号</b>：[27]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05681</p>
  <p><b>作者</b>：Zhengzhi Lu,  He Wang,  Ziyi Chang,  Guoan Yang,  Hubert P. H. Shum</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：skeleton-based human activity, human activity recognition, attack methods, skeleton-based human, human activity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, methods for skeleton-based human activity recognition have been
shown to be vulnerable to adversarial attacks. However, these attack methods
require either the full knowledge of the victim (i.e. white-box attacks),
access to training data (i.e. transfer-based attacks) or frequent model queries
(i.e. black-box attacks). All their requirements are highly restrictive,
raising the question of how detrimental the vulnerability is. In this paper, we
show that the vulnerability indeed exists. To this end, we consider a new
attack task: the attacker has no access to the victim model or the training
data or labels, where we coin the term hard no-box attack. Specifically, we
first learn a motion manifold where we define an adversarial loss to compute a
new gradient for the attack, named skeleton-motion-informed (SMI) gradient. Our
gradient contains information of the motion dynamics, which is different from
existing gradient-based attack methods that compute the loss gradient assuming
each dimension in the data is independent. The SMI gradient can augment many
gradient-based attack methods, leading to a new family of no-box attack
methods. Extensive evaluation and comparison show that our method imposes a
real threat to existing classifiers. They also show that the SMI gradient
improves the transferability and imperceptibility of adversarial samples in
both no-box and transfer-based black-box settings.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Exploring Deep Learning Approaches to Predict Person and Vehicle Trips:  An Analysis of NHTS Data</b></summary>
  <p><b>编号</b>：[31]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05665</p>
  <p><b>作者</b>：Kojo Adu-Gyamfi,  Sharma Anuj</p>
  <p><b>备注</b>：15 pages, 11 figures</p>
  <p><b>关键词</b>：planning relies heavily, Modern transportation planning, deep learning, transportation planning relies, transportation planning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Modern transportation planning relies heavily on accurate predictions of
person and vehicle trips. However, traditional planning models often fail to
account for the intricacies and dynamics of travel behavior, leading to
less-than-optimal accuracy in these predictions. This study explores the
potential of deep learning techniques to transform the way we approach trip
predictions, and ultimately, transportation planning. Utilizing a comprehensive
dataset from the National Household Travel Survey (NHTS), we developed and
trained a deep learning model for predicting person and vehicle trips. The
proposed model leverages the vast amount of information in the NHTS data,
capturing complex, non-linear relationships that were previously overlooked by
traditional models. As a result, our deep learning model achieved an impressive
accuracy of 98% for person trip prediction and 96% for vehicle trip estimation.
This represents a significant improvement over the performances of traditional
transportation planning models, thereby demonstrating the power of deep
learning in this domain. The implications of this study extend beyond just more
accurate predictions. By enhancing the accuracy and reliability of trip
prediction models, planners can formulate more effective, data-driven
transportation policies, infrastructure, and services. As such, our research
underscores the need for the transportation planning field to embrace advanced
techniques like deep learning. The detailed methodology, along with a thorough
discussion of the results and their implications, are presented in the
subsequent sections of this paper.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Automatic Extraction of Relevant Road Infrastructure using Connected  vehicle data and Deep Learning Model</b></summary>
  <p><b>编号</b>：[33]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05658</p>
  <p><b>作者</b>：Adu-Gyamfi Kojo,  Kandiboina Raghupathi,  Ravichandra-Mouli Varsha,  Knickerbocker Skylar,  Hans Zachary N,  Hawkins,  Neal R,  Sharma Anuj</p>
  <p><b>备注</b>：18 pages, 13 figures</p>
  <p><b>关键词</b>：today rapidly evolving, enhancing road safety, evolving urban landscapes, rapidly evolving urban, drivers and commuters</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In today's rapidly evolving urban landscapes, efficient and accurate mapping
of road infrastructure is critical for optimizing transportation systems,
enhancing road safety, and improving the overall mobility experience for
drivers and commuters. Yet, a formidable bottleneck obstructs progress - the
laborious and time-intensive manual identification of intersections. Simply
considering the shear number of intersections that need to be identified, and
the labor hours required per intersection, the need for an automated solution
becomes undeniable. To address this challenge, we propose a novel approach that
leverages connected vehicle data and cutting-edge deep learning techniques. By
employing geohashing to segment vehicle trajectories and then generating image
representations of road segments, we utilize the YOLOv5 (You Only Look Once
version 5) algorithm for accurate classification of both straight road segments
and intersections. Experimental results demonstrate an impressive overall
classification accuracy of 95%, with straight roads achieving a remarkable 97%
F1 score and intersections reaching a 90% F1 score. This approach not only
saves time and resources but also enables more frequent updates and a
comprehensive understanding of the road network. Our research showcases the
potential impact on traffic management, urban planning, and autonomous vehicle
navigation systems. The fusion of connected vehicle data and deep learning
models holds promise for a transformative shift in road infrastructure mapping,
propelling us towards a smarter, safer, and more connected transportation
ecosystem.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：A Neural Network Based Choice Model for Assortment Optimization</b></summary>
  <p><b>编号</b>：[50]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05617</p>
  <p><b>作者</b>：Hanzhao Wang,  Zhongze Cai,  Xiaocheng Li,  Kalyan Talluri</p>
  <p><b>备注</b>：arXiv admin note: substantial text overlap with arXiv:2208.09325</p>
  <p><b>关键词</b>：marketing and revenue, revenue management, function of prices, customer behaviour model, neural network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Discrete-choice models are used in economics, marketing and revenue
management to predict customer purchase probabilities, say as a function of
prices and other features of the offered assortment. While they have been shown
to be expressive, capturing customer heterogeneity and behaviour, they are also
hard to estimate, often based on many unobservables like utilities; and
moreover, they still fail to capture many salient features of customer
behaviour. A natural question then, given their success in other contexts, is
if neural networks can eliminate the necessity of carefully building a
context-dependent customer behaviour model and hand-coding and tuning the
estimation. It is unclear however how one would incorporate assortment effects
into such a neural network, and also how one would optimize the assortment with
such a black-box generative model of choice probabilities. In this paper we
investigate first whether a single neural network architecture can predict
purchase probabilities for datasets from various contexts and generated under
various models and assumptions. Next, we develop an assortment optimization
formulation that is solvable by off-the-shelf integer programming solvers. We
compare against a variety of benchmark discrete-choice models on simulated as
well as real-world datasets, developing training tricks along the way to make
the neural network prediction and subsequent optimization robust and comparable
in performance to the alternates.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：A Smart Robotic System for Industrial Plant Supervision</b></summary>
  <p><b>编号</b>：[51]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05612</p>
  <p><b>作者</b>：D. Adriana Gómez-Rosal,  Max Bergau,  Georg K.J. Fischer,  Andreas Wachaja,  Johannes Gräter,  Matthias Odenweller,  Uwe Piechottka,  Fabian Hoeflinger,  Nikhil Gosala,  Niklas Wetzel,  Daniel Büscher,  Abhinav Valada,  Wolfram Burgard</p>
  <p><b>备注</b>：Accepted for publication in IEEE Sensors 2023</p>
  <p><b>关键词</b>：high safety standards, today chemical production, field operators perform, operators perform frequent, perform frequent checks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In today's chemical production plants, human field operators perform frequent
checks on the plant's integrity to guarantee high safety standards, and thus
are possibly the first to encounter dangerous operating conditions. To
alleviate their tasks of failure detection and monitoring by audio, visual, and
olfactory perceptions, we present a robotic system that consists of an
autonomously navigating robot integrated with various sensors and data
processing. We aim to resemble the human sensing and interpretation
capabilities of sight, smell, and hearing, for providing automated inspection.
We evaluate our system extensively at a wastewater facility in full working
conditions. Our results demonstrate that the system is able to robustly
navigate a plant and to provide useful information about critical operating
conditions.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Multi-graph Spatio-temporal Graph Convolutional Network for Traffic Flow  Prediction</b></summary>
  <p><b>编号</b>：[55]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05601</p>
  <p><b>作者</b>：Weilong Ding,  Tianpu Zhang,  Jianwu Wang,  Zhuofeng Zhao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Inter-city highway transportation, urban life, traffic flow, daily traffic flow, network-wide toll stations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Inter-city highway transportation is significant for urban life. As one of
the key functions in intelligent transportation system (ITS), traffic
evaluation always plays significant role nowadays, and daily traffic flow
prediction still faces challenges at network-wide toll stations. On the one
hand, the data imbalance in practice among various locations deteriorates the
performance of prediction. On the other hand, complex correlative
spatio-temporal factors cannot be comprehensively employed in long-term
duration. In this paper, a prediction method is proposed for daily traffic flow
in highway domain through spatio-temporal deep learning. In our method, data
normalization strategy is used to deal with data imbalance, due to long-tail
distribution of traffic flow at network-wide toll stations. And then, based on
graph convolutional network, we construct networks in distinct semantics to
capture spatio-temporal features. Beside that, meteorology and calendar
features are used by our model in the full connection stage to extra external
characteristics of traffic flow. By extensive experiments and case studies in
one Chinese provincial highway, our method shows clear improvement in
predictive accuracy than baselines and practical benefits in business.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Proximal Policy Optimization Actual Combat: Manipulating Output  Tokenizer Length</b></summary>
  <p><b>编号</b>：[62]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05585</p>
  <p><b>作者</b>：Miao Fan,  Chen Hu,  Shuchang Zhou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Reinforcement Learning, large language models, selecting output styles, controlling output toxicity, Human Feedback</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The Reinforcement Learning from Human Feedback (RLHF) plays a pivotal role in
shaping the impact of large language models (LLMs), contributing significantly
to controlling output toxicity and selecting output styles, particularly as
LLMs often harbor misleading content, highlighting the urgency to align them
with human values for secure AI systems. The RLHF, characterized by complexity,
instability, and sensitivity to hyperparameters, makes the evaluation of the
reward model for complex tasks challenging, thereby further complicating the
use of Proximal Policy Optimization (PPO). In this paper, we introduce a simple
task designed to employ Gloden as a reward model that validates the
effectiveness of PPO and inspires it, primarily explaining the task of
utilizing PPO to manipulate the tokenizer length of the output generated by the
model. Experiments confirm that PPO is not only effective in manipulating the
output tokenizer length to a certain extent in this type of task but also
exhibits facilitated training once the influence of the reward model effect is
excluded, making it an exciting development.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Generative Diffusion Models for Radio Wireless Channel Modelling and  Sampling</b></summary>
  <p><b>编号</b>：[63]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05583</p>
  <p><b>作者</b>：Ushnish Sengupta,  Chinkuo Jao,  Alberto Bernacchia,  Sattar Vakili,  Da-shan Shiu</p>
  <p><b>备注</b>：2023 IEEE Global Communications Conference</p>
  <p><b>关键词</b>：wireless communication systems, modern wireless communication, designing modern wireless, communication systems, essential to designing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Channel modelling is essential to designing modern wireless communication
systems. The increasing complexity of channel modelling and the cost of
collecting high-quality wireless channel data have become major challenges. In
this paper, we propose a diffusion model based channel sampling approach for
rapidly synthesizing channel realizations from limited data. We use a diffusion
model with a U Net based architecture operating in the frequency space domain.
To evaluate how well the proposed model reproduces the true distribution of
channels in the training dataset, two evaluation metrics are used: $i)$ the
approximate $2$-Wasserstein distance between real and generated distributions
of the normalized power spectrum in the antenna and frequency domains and $ii)$
precision and recall metric for distributions. We show that, compared to
existing GAN based approaches which suffer from mode collapse and unstable
training, our diffusion based approach trains stably and generates diverse and
high-fidelity samples from the true channel distribution. We also show that we
can pretrain the model on a simulated urban macro-cellular channel dataset and
fine-tune it on a smaller, out-of-distribution urban micro-cellular dataset,
therefore showing that it is feasible to model real world channels using
limited data with this approach.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：C5: Towards Better Conversation Comprehension and Contextual Continuity  for ChatGPT</b></summary>
  <p><b>编号</b>：[70]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05567</p>
  <p><b>作者</b>：Pan Liang,  Danwei Ye,  Zihao Zhu,  Yunchao Wang,  Wang Xia,  Ronghua Liang,  Guodao Sun</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：natural language understanding, demonstrated outstanding performance, Large language models, Large language, natural language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs), such as ChatGPT, have demonstrated outstanding
performance in various fields, particularly in natural language understanding
and generation tasks. In complex application scenarios, users tend to engage in
multi-turn conversations with ChatGPT to keep contextual information and obtain
comprehensive responses. However, human forgetting and model contextual
forgetting remain prominent issues in multi-turn conversation scenarios, which
challenge the users' conversation comprehension and contextual continuity for
ChatGPT. To address these challenges, we propose an interactive conversation
visualization system called C5, which includes Global View, Topic View, and
Context-associated Q\&A View. The Global View uses the GitLog diagram metaphor
to represent the conversation structure, presenting the trend of conversation
evolution and supporting the exploration of locally salient features. The Topic
View is designed to display all the question and answer nodes and their
relationships within a topic using the structure of a knowledge graph, thereby
display the relevance and evolution of conversations. The Context-associated
Q\&A View consists of three linked views, which allow users to explore
individual conversations deeply while providing specific contextual information
when posing questions. The usefulness and effectiveness of C5 were evaluated
through a case study and a user study.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Recent Advancements In The Field Of Deepfake Detection</b></summary>
  <p><b>编号</b>：[73]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05563</p>
  <p><b>作者</b>：Natalie Krueger,  Dr. Mounika Vanamala,  Dr. Rushit Dave</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：digitally altered, altered or partially, partially replaced, person whose image, image</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A deepfake is a photo or video of a person whose image has been digitally
altered or partially replaced with an image of someone else. Deepfakes have the
potential to cause a variety of problems and are often used maliciously. A
common usage is altering videos of prominent political figures and celebrities.
These deepfakes can portray them making offensive, problematic, and/or untrue
statements. Current deepfakes can be very realistic, and when used in this way,
can spread panic and even influence elections and political opinions. There are
many deepfake detection strategies currently in use but finding the most
comprehensive and universal method is critical. So, in this survey we will
address the problems of malicious deepfake creation and the lack of universal
deepfake detection methods. Our objective is to survey and analyze a variety of
current methods and advances in the field of deepfake detection.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Enhancing AUV Autonomy With Model Predictive Path Integral Control</b></summary>
  <p><b>编号</b>：[79]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05547</p>
  <p><b>作者</b>：Pierre Nicolay,  Yvan Petillot,  Mykhaylo Marfeychuk,  Sen Wang,  Ignacio Carlucho</p>
  <p><b>备注</b>：10 pages, 11 figures</p>
  <p><b>关键词</b>：Autonomous underwater vehicles, underwater inspection tasks, surveying marine environments, Autonomous underwater, underwater vehicles</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Autonomous underwater vehicles (AUVs) play a crucial role in surveying marine
environments, carrying out underwater inspection tasks, and ocean exploration.
However, in order to ensure that the AUV is able to carry out its mission
successfully, a control system capable of adapting to changing environmental
conditions is required. Furthermore, to ensure the robotic platform's safe
operation, the onboard controller should be able to operate under certain
constraints. In this work, we investigate the feasibility of Model Predictive
Path Integral Control (MPPI) for the control of an AUV. We utilise a non-linear
model of the AUV to propagate the samples of the MPPI, which allow us to
compute the control action in real time. We provide a detailed evaluation of
the effect of the main hyperparameters on the performance of the MPPI
controller. Furthermore, we compared the performance of the proposed method
with a classical PID and Cascade PID approach, demonstrating the superiority of
our proposed controller. Finally, we present results where environmental
constraints are added and show how MPPI can handle them by simply incorporating
those constraints in the cost function.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Models Matter: The Impact of Single-Step Retrosynthesis on Synthesis  Planning</b></summary>
  <p><b>编号</b>：[88]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05522</p>
  <p><b>作者</b>：Paula Torren-Peraire,  Alan Kai Hassen,  Samuel Genheden,  Jonas Verhoeven,  Djork-Arne Clevert,  Mike Preuss,  Igor Tetko</p>
  <p><b>备注</b>：The following authors contributed equally: Paula Torren-Peraire, Alan Kai Hassen</p>
  <p><b>关键词</b>：chemical compound recursively, multi-step synthesis planning, synthesis planning, synthesis, multi-step synthesis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Retrosynthesis consists of breaking down a chemical compound recursively
step-by-step into molecular precursors until a set of commercially available
molecules is found with the goal to provide a synthesis route. Its two primary
research directions, single-step retrosynthesis prediction, which models the
chemical reaction logic, and multi-step synthesis planning, which tries to find
the correct sequence of reactions, are inherently intertwined. Still, this
connection is not reflected in contemporary research. In this work, we combine
these two major research directions by applying multiple single-step
retrosynthesis models within multi-step synthesis planning and analyzing their
impact using public and proprietary reaction data. We find a disconnection
between high single-step performance and potential route-finding success,
suggesting that single-step models must be evaluated within synthesis planning
in the future. Furthermore, we show that the commonly used single-step
retrosynthesis benchmark dataset USPTO-50k is insufficient as this evaluation
task does not represent model performance and scalability on larger and more
diverse datasets. For multi-step synthesis planning, we show that the choice of
the single-step model can improve the overall success rate of synthesis
planning by up to +28% compared to the commonly used baseline model. Finally,
we show that each single-step model finds unique synthesis routes, and differs
in aspects such as route-finding success, the number of found synthesis routes,
and chemical validity, making the combination of single-step retrosynthesis
prediction and multi-step synthesis planning a crucial aspect when developing
future methods.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Mono-hydra: Real-time 3D scene graph construction from monocular camera  input with IMU</b></summary>
  <p><b>编号</b>：[90]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05515</p>
  <p><b>作者</b>：U.V.B.L. Udugama,  G. Vosselman,  F. Nex</p>
  <p><b>备注</b>：7 pages, 5 figures, GSW 2023 conference paper</p>
  <p><b>关键词</b>：ranging from low-level, ability of robots, robots to autonomously, autonomously navigate, low-level geometry</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The ability of robots to autonomously navigate through 3D environments
depends on their comprehension of spatial concepts, ranging from low-level
geometry to high-level semantics, such as objects, places, and buildings. To
enable such comprehension, 3D scene graphs have emerged as a robust tool for
representing the environment as a layered graph of concepts and their
relationships. However, building these representations using monocular vision
systems in real-time remains a difficult task that has not been explored in
depth. This paper puts forth a real-time spatial perception system Mono-Hydra,
combining a monocular camera and an IMU sensor setup, focusing on indoor
scenarios. However, the proposed approach is adaptable to outdoor applications,
offering flexibility in its potential uses. The system employs a suite of deep
learning algorithms to derive depth and semantics. It uses a robocentric
visual-inertial odometry (VIO) algorithm based on square-root information,
thereby ensuring consistent visual odometry with an IMU and a monocular camera.
This system achieves sub-20 cm error in real-time processing at 15 fps,
enabling real-time 3D scene graph construction using a laptop GPU (NVIDIA
3080). This enhances decision-making efficiency and effectiveness in simple
camera setups, augmenting robotic system agility. We make Mono-Hydra publicly
available at: this https URL</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Multi-domain Recommendation with Embedding Disentangling and Domain  Alignment</b></summary>
  <p><b>编号</b>：[92]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05508</p>
  <p><b>作者</b>：Wentao Ning,  Xiao Yan,  Weiwen Liu,  Reynold Cheng,  Rui Zhang,  Bo Tang</p>
  <p><b>备注</b>：Accepted by CIKM'23</p>
  <p><b>关键词</b>：host multiple services, Multi-domain recommendation, types of products, provide recommendations, aims to provide</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multi-domain recommendation (MDR) aims to provide recommendations for
different domains (e.g., types of products) with overlapping users/items and is
common for platforms such as Amazon, Facebook, and LinkedIn that host multiple
services. Existing MDR models face two challenges: First, it is difficult to
disentangle knowledge that generalizes across domains (e.g., a user likes cheap
items) and knowledge specific to a single domain (e.g., a user likes blue
clothing but not blue cars). Second, they have limited ability to transfer
knowledge across domains with small overlaps. We propose a new MDR method named
EDDA with two key components, i.e., embedding disentangling recommender and
domain alignment, to tackle the two challenges respectively. In particular, the
embedding disentangling recommender separates both the model and embedding for
the inter-domain part and the intra-domain part, while most existing MDR
methods only focus on model-level disentangling. The domain alignment leverages
random walks from graph processing to identify similar user/item pairs from
different domains and encourages similar user/item pairs to have similar
embeddings, enhancing knowledge transfer. We compare EDDA with 12
state-of-the-art baselines on 3 real datasets. The results show that EDDA
consistently outperforms the baselines on all datasets and domains. All
datasets and codes are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：EFX Allocations Exist for Binary Valuations</b></summary>
  <p><b>编号</b>：[96]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05503</p>
  <p><b>作者</b>：Xiaolin Bu,  Jiaxin Song,  Ziqi Yu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：fairness criterion envy-freeness, fair division problem, fair division literature, fair division, satisfying the fairness</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the fair division problem and the existence of allocations
satisfying the fairness criterion envy-freeness up to any item (EFX). The
existence of EFX allocations is a major open problem in the fair division
literature. We consider binary valuations where the marginal gain of the value
by receiving an extra item is either $0$ or $1$. Babaioff et al. [2021] proved
that EFX allocations always exist for binary and submodular valuations. In this
paper, by using completely different techniques, we extend this existence
result to general binary valuations that are not necessarily submodular, and we
present a polynomial time algorithm for computing an EFX allocation.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Bringing order into the realm of Transformer-based language models for  artificial intelligence and law</b></summary>
  <p><b>编号</b>：[97]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05502</p>
  <p><b>作者</b>：Candida M. Greco,  Andrea Tagarelli</p>
  <p><b>备注</b>：Accepted for publication with Artificial Intelligence and Law, Springer Nature</p>
  <p><b>关键词</b>：require natural language, natural language processing, Transformer-based language models, Transformer-based language, natural language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transformer-based language models (TLMs) have widely been recognized to be a
cutting-edge technology for the successful development of deep-learning-based
solutions to problems and applications that require natural language processing
and understanding. Like for other textual domains, TLMs have indeed pushed the
state-of-the-art of AI approaches for many tasks of interest in the legal
domain. Despite the first Transformer model being proposed about six years ago,
there has been a rapid progress of this technology at an unprecedented rate,
whereby BERT and related models represent a major reference, also in the legal
domain. This article provides the first systematic overview of TLM-based
methods for AI-driven problems and tasks in the legal sphere. A major goal is
to highlight research advances in this field so as to understand, on the one
hand, how the Transformers have contributed to the success of AI in supporting
legal processes, and on the other hand, what are the current limitations and
opportunities for further research development.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：More Than Meets the Eye: Analyzing Anesthesiologists' Visual Attention  in the Operating Room Using Deep Learning Models</b></summary>
  <p><b>编号</b>：[98]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05501</p>
  <p><b>作者</b>：Sapir Gershov,  Fadi Mahameed,  Aeyal Raz,  Shlomi Laufer</p>
  <p><b>备注</b>：Submitted to MICCAI Aml4HC 2023</p>
  <p><b>关键词</b>：Patient vital signs, acquire specific cues, anesthesiologist visual attention, patient outcome, Patient vital</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Patient's vital signs, which are displayed on monitors, make the
anesthesiologist's visual attention (VA) a key component in the safe management
of patients under general anesthesia; moreover, the distribution of said VA and
the ability to acquire specific cues throughout the anesthetic, may have a
direct impact on patient's outcome. Currently, most studies employ wearable
eye-tracking technologies to analyze anesthesiologists' visual patterns. Albeit
being able to produce meticulous data, wearable devices are not a sustainable
solution for large-scale or long-term use for data collection in the operating
room (OR). Thus, by utilizing a novel eye-tracking method in the form of deep
learning models that process monitor-mounted webcams, we collected continuous
behavioral data and gained insight into the anesthesiologist's VA distribution
with minimal disturbance to their natural workflow. In this study, we collected
OR video recordings using the proposed framework and compared different visual
behavioral patterns. We distinguished between baseline VA distribution during
uneventful periods to patterns associated with active phases or during
critical, unanticipated incidents. In the future, such a platform may serve as
a crucial component of context-aware assistive technologies in the OR.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Exploring XAI for the Arts: Explaining Latent Space in Generative Music</b></summary>
  <p><b>编号</b>：[102]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05496</p>
  <p><b>作者</b>：Nick Bryan-Kinns,  Berker Banar,  Corey Ford,  Courtney N. Reed,  Yixiao Zhang,  Simon Colton,  Jack Armitage</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：latent space, potential to support, support more interactive, interactive and fluid, fluid co-creative</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Explainable AI has the potential to support more interactive and fluid
co-creative AI systems which can creatively collaborate with people. To do
this, creative AI models need to be amenable to debugging by offering
eXplainable AI (XAI) features which are inspectable, understandable, and
modifiable. However, currently there is very little XAI for the arts. In this
work, we demonstrate how a latent variable model for music generation can be
made more explainable; specifically we extend MeasureVAE which generates
measures of music. We increase the explainability of the model by: i) using
latent space regularisation to force some specific dimensions of the latent
space to map to meaningful musical attributes, ii) providing a user interface
feedback loop to allow people to adjust dimensions of the latent space and
observe the results of these changes in real-time, iii) providing a
visualisation of the musical attributes in the latent space to help people
understand and predict the effect of changes to latent space dimensions. We
suggest that in doing so we bridge the gap between the latent space and the
generated musical outcomes in a meaningful way which makes the model and its
outputs more explainable and more debuggable.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：LLM As DBA</b></summary>
  <p><b>编号</b>：[107]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05481</p>
  <p><b>作者</b>：Xuanhe Zhou,  Guoliang Li,  Zhiyuan Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ensure data availability, play a crucial, role in managing, maintaining and optimizing, data availability</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Database administrators (DBAs) play a crucial role in managing, maintaining
and optimizing a database system to ensure data availability, performance, and
reliability. However, it is hard and tedious for DBAs to manage a large number
of database instances (e.g., millions of instances on the cloud databases).
Recently large language models (LLMs) have shown great potential to understand
valuable documents and accordingly generate reasonable answers. Thus, we
propose D-Bot, a LLM-based database administrator that can continuously acquire
database maintenance experience from textual sources, and provide reasonable,
well-founded, in-time diagnosis and optimization advice for target databases.
This paper presents a revolutionary LLM-centric framework for database
maintenance, including (i) database maintenance knowledge detection from
documents and tools, (ii) tree of thought reasoning for root cause analysis,
and (iii) collaborative diagnosis among multiple LLMs. Our preliminary
experimental results that D-Bot can efficiently and effectively diagnose the
root causes and our code is available at
this http URL.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Reviewing 3D Object Detectors in the Context of High-Resolution 3+1D  Radar</b></summary>
  <p><b>编号</b>：[109]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05478</p>
  <p><b>作者</b>：Patrick Palmer,  Martin Krueger,  Richard Altendorfer,  Ganesh Adam,  Torsten Bertram</p>
  <p><b>备注</b>：Published at CVPR 2023 Workshop on 3D Vision and Robotics (this https URL)</p>
  <p><b>关键词</b>：beginning market introduction, point clouds, initialized deep learning-based, deep learning-based, point</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent developments and the beginning market introduction of high-resolution
imaging 4D (3+1D) radar sensors have initialized deep learning-based radar
perception research. We investigate deep learning-based models operating on
radar point clouds for 3D object detection. 3D object detection on lidar point
cloud data is a mature area of 3D vision. Many different architectures have
been proposed, each with strengths and weaknesses. Due to similarities between
3D lidar point clouds and 3+1D radar point clouds, those existing 3D object
detectors are a natural basis to start deep learning-based 3D object detection
on radar data. Thus, the first step is to analyze the detection performance of
the existing models on the new data modality and evaluate them in depth. In
order to apply existing 3D point cloud object detectors developed for lidar
point clouds to the radar domain, they need to be adapted first. While some
detectors, such as PointPillars, have already been adapted to be applicable to
radar data, we have adapted others, e.g., Voxel R-CNN, SECOND, PointRCNN, and
PV-RCNN. To this end, we conduct a cross-model validation (evaluating a set of
models on one particular data set) as well as a cross-data set validation
(evaluating all models in the model set on several data sets). The
high-resolution radar data used are the View-of-Delft and Astyx data sets.
Finally, we evaluate several adaptations of the models and their training
procedures. We also discuss major factors influencing the detection performance
on radar data and propose possible solutions indicating potential future
research avenues.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Explainable AI applications in the Medical Domain: a systematic review</b></summary>
  <p><b>编号</b>：[128]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05411</p>
  <p><b>作者</b>：Nicoletta Prentzas,  Antonis Kakas,  Constantinos S. Pattichis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：made significant progress, Artificial Intelligence, Intelligence in Medicine, patient care, made significant</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Artificial Intelligence in Medicine has made significant progress with
emerging applications in medical imaging, patient care, and other areas. While
these applications have proven successful in retrospective studies, very few of
them were applied in practice.The field of Medical AI faces various challenges,
in terms of building user trust, complying with regulations, using data
ethically.Explainable AI (XAI) aims to enable humans understand AI and trust
its results. This paper presents a literature review on the recent developments
of XAI solutions for medical decision support, based on a representative sample
of 198 articles published in recent years. The systematic synthesis of the
relevant articles resulted in several findings. (1) model-agnostic XAI
techniques were mostly employed in these solutions, (2) deep learning models
are utilized more than other types of machine learning models, (3)
explainability was applied to promote trust, but very few works reported the
physicians participation in the loop, (4) visual and interactive user interface
is more useful in understanding the explanation and the recommendation of the
system. More research is needed in collaboration between medical and AI
experts, that could guide the development of suitable frameworks for the
design, implementation, and evaluation of XAI solutions in medicine.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：A Comparative Assessment of Multi-view fusion learning for Crop  Classification</b></summary>
  <p><b>编号</b>：[130]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05407</p>
  <p><b>作者</b>：Francisco Mena,  Diego Arenas,  Marlon Nuske,  Andreas Dengel</p>
  <p><b>备注</b>：Accepted at IEEE International Geoscience and Remote Sensing Symposium 2023</p>
  <p><b>关键词</b>：rapidly increasing amount, multi-view learning modeling, remote sensing, learning modeling, rapidly increasing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With a rapidly increasing amount and diversity of remote sensing (RS) data
sources, there is a strong need for multi-view learning modeling. This is a
complex task when considering the differences in resolution, magnitude, and
noise of RS data. The typical approach for merging multiple RS sources has been
input-level fusion, but other - more advanced - fusion strategies may
outperform this traditional approach. This work assesses different fusion
strategies for crop classification in the CropHarvest dataset. The fusion
methods proposed in this work outperform models based on individual views and
previous fusion methods. We do not find one single fusion method that
consistently outperforms all other approaches. Instead, we present a comparison
of multi-view fusion methods for three different datasets and show that,
depending on the test region, different methods obtain the best performance.
Despite this, we suggest a preliminary criterion for the selection of fusion
methods.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Enhancing Trust in LLM-Based AI Automation Agents: New Considerations  and Future Challenges</b></summary>
  <p><b>编号</b>：[135]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05391</p>
  <p><b>作者</b>：Sivan Schwartz,  Avi Yaeli,  Segev Shlomov</p>
  <p><b>备注</b>：Accepted to the First International Workshop on the Future of No-Code Digital Apprentices</p>
  <p><b>关键词</b>：Large Language Models, resulting in significant, extensively studied, Language Models, Large Language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Trust in AI agents has been extensively studied in the literature, resulting
in significant advancements in our understanding of this field. However, the
rapid advancements in Large Language Models (LLMs) and the emergence of
LLM-based AI agent frameworks pose new challenges and opportunities for further
research. In the field of process automation, a new generation of AI-based
agents has emerged, enabling the execution of complex tasks. At the same time,
the process of building automation has become more accessible to business users
via user-friendly no-code tools and training mechanisms. This paper explores
these new challenges and opportunities, analyzes the main aspects of trust in
AI agents discussed in existing literature, and identifies specific
considerations and challenges relevant to this new generation of automation
agents. We also evaluate how nascent products in this category address these
considerations. Finally, we highlight several challenges that the research
community should address in this evolving landscape.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Adaptive Taxonomy Learning and Historical Patterns Modelling for Patent  Classification</b></summary>
  <p><b>编号</b>：[139]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05385</p>
  <p><b>作者</b>：Tao Zou,  Le Yu,  Leilei Sun,  Bowen Du,  Deqing Wang,  Fuzhen Zhuang</p>
  <p><b>备注</b>：13 pages</p>
  <p><b>关键词</b>：assign multiple International, multiple International Patent, multiple International, International Patent, IPC codes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Patent classification aims to assign multiple International Patent
Classification (IPC) codes to a given patent. Recent methods for automatically
classifying patents mainly focus on analyzing the text descriptions of patents.
However, apart from the texts, each patent is also associated with some
assignees, and the knowledge of their applied patents is often valuable for
classification. Furthermore, the hierarchical taxonomy formulated by the IPC
system provides important contextual information and enables models to leverage
the correlations between IPC codes for more accurate classification. However,
existing methods fail to incorporate the above aspects. In this paper, we
propose an integrated framework that comprehensively considers the information
on patents for patent classification. To be specific, we first present an IPC
codes correlations learning module to derive their semantic representations via
adaptively passing and aggregating messages within the same level and across
different levels along the hierarchical taxonomy. Moreover, we design a
historical application patterns learning component to incorporate the
corresponding assignee's previous patents by a dual channel aggregation
mechanism. Finally, we combine the contextual information of patent texts that
contains the semantics of IPC codes, and assignees' sequential preferences to
make predictions. Experiments on real-world datasets demonstrate the
superiority of our approach over the existing methods. Besides, we present the
model's ability to capture the temporal patterns of assignees and the semantic
dependencies among IPC codes.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Beyond Semantics: Learning a Behavior Augmented Relevance Model with  Self-supervised Learning</b></summary>
  <p><b>编号</b>：[143]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05379</p>
  <p><b>作者</b>：Zeyuan Chen,  Wei Chen,  Jia Xu,  Zhongyi Liu,  Wei Zhang</p>
  <p><b>备注</b>：CIKM2023</p>
  <p><b>关键词</b>：ensure user experience, Relevance modeling aims, locate desirable items, modeling aims, aims to locate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Relevance modeling aims to locate desirable items for corresponding queries,
which is crucial for search engines to ensure user experience. Although most
conventional approaches address this problem by assessing the semantic
similarity between the query and item, pure semantic matching is not
everything. In reality, auxiliary query-item interactions extracted from user
historical behavior data of the search log could provide hints to reveal users'
search intents further. Drawing inspiration from this, we devise a novel
Behavior Augmented Relevance Learning model for Alipay Search (BARL-ASe) that
leverages neighbor queries of target item and neighbor items of target query to
complement target query-item semantic matching. Specifically, our model builds
multi-level co-attention for distilling coarse-grained and fine-grained
semantic representations from both neighbor and target views. The model
subsequently employs neighbor-target self-supervised learning to improve the
accuracy and robustness of BARL-ASe by strengthening representation and logit
learning. Furthermore, we discuss how to deal with the long-tail query-item
matching of the mini apps search scenario of Alipay practically. Experiments on
real-world industry data and online A/B testing demonstrate our proposal
achieves promising performance with low latency.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language  Models' Alignment</b></summary>
  <p><b>编号</b>：[144]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05374</p>
  <p><b>作者</b>：Yang Liu,  Yuanshun Yao,  Jean-Francois Ton,  Xiaoying Zhang,  Ruocheng Guo Hao Cheng,  Yegor Klochkov,  Muhammad Faaiz Taufiq,  Hang Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：deploying large language, large language models, human intentions, behave in accordance, accordance with human</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Ensuring alignment, which refers to making models behave in accordance with
human intentions [1,2], has become a critical task before deploying large
language models (LLMs) in real-world applications. For instance, OpenAI devoted
six months to iteratively aligning GPT-4 before its release [3]. However, a
major challenge faced by practitioners is the lack of clear guidance on
evaluating whether LLM outputs align with social norms, values, and
regulations. This obstacle hinders systematic iteration and deployment of LLMs.
To address this issue, this paper presents a comprehensive survey of key
dimensions that are crucial to consider when assessing LLM trustworthiness. The
survey covers seven major categories of LLM trustworthiness: reliability,
safety, fairness, resistance to misuse, explainability and reasoning, adherence
to social norms, and robustness. Each major category is further divided into
several sub-categories, resulting in a total of 29 sub-categories.
Additionally, a subset of 8 sub-categories is selected for further
investigation, where corresponding measurement studies are designed and
conducted on several widely-used LLMs. The measurement results indicate that,
in general, more aligned models tend to perform better in terms of overall
trustworthiness. However, the effectiveness of alignment varies across the
different trustworthiness categories considered. This highlights the importance
of conducting more fine-grained analyses, testing, and making continuous
improvements on LLM alignment. By shedding light on these key dimensions of LLM
trustworthiness, this paper aims to provide valuable insights and guidance to
practitioners in the field. Understanding and addressing these concerns will be
crucial in achieving reliable and ethically sound deployment of LLMs in various
applications.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Machine Learning aided Computer Architecture Design for CNN Inferencing  Systems</b></summary>
  <p><b>编号</b>：[148]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05364</p>
  <p><b>作者</b>：Christopher A. Metz</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Machine Learning, calculations of Machine, Convolutional Neural Networks, Efficient and timely, autonomous driving</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Efficient and timely calculations of Machine Learning (ML) algorithms are
essential for emerging technologies like autonomous driving, the Internet of
Things (IoT), and edge computing. One of the primary ML algorithms used in such
systems is Convolutional Neural Networks (CNNs), which demand high
computational resources. This requirement has led to the use of ML accelerators
like GPGPUs to meet design constraints. However, selecting the most suitable
accelerator involves Design Space Exploration (DSE), a process that is usually
time-consuming and requires significant manual effort. Our work presents
approaches to expedite the DSE process by identifying the most appropriate
GPGPU for CNN inferencing systems. We have developed a quick and precise
technique for forecasting the power and performance of CNNs during inference,
with a MAPE of 5.03% and 5.94%, respectively. Our approach empowers computer
architects to estimate power and performance in the early stages of
development, reducing the necessity for numerous prototypes. This saves time
and money while also improving the time-to-market period.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Metacognitive Prompting Improves Understanding in Large Language Models</b></summary>
  <p><b>编号</b>：[159]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05342</p>
  <p><b>作者</b>：Yuqing Wang,  Yun Zhao</p>
  <p><b>备注</b>：9 pages, in submission</p>
  <p><b>关键词</b>：effective prompt design, Large Language Models, Large Language, largely influenced, prompt design</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In Large Language Models (LLMs), there have been consistent advancements in
task-specific performance, largely influenced by effective prompt design. While
recent research on prompting has enhanced the reasoning capabilities of LLMs, a
gap remains in further improving their understanding abilities. In this study,
we introduce metacognitive prompting (MP), a strategy inspired by human
introspective reasoning processes. Using MP, LLMs undergo a systematic series
of structured, self-aware evaluations, drawing on both their vast inherent
knowledge and new insights. Our experiments involve five prevalent LLMs:
Llama2, Vicuna, PaLM, GPT-3.5, and GPT-4, all of which span various general
natural language understanding (NLU) tasks from the GLUE and SuperGLUE
benchmarks. Results indicate that, although GPT-4 consistently excels in most
tasks, PaLM, when equipped with MP, approaches its performance level.
Furthermore, across models and datasets, MP consistently outperforms existing
prompting methods, including standard and chain-of-thought prompting. This
study underscores the potential to amplify the understanding abilities of LLMs
and highlights the benefits of mirroring human introspective reasoning in NLU
tasks.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Classification of Human- and AI-Generated Texts: Investigating Features  for ChatGPT</b></summary>
  <p><b>编号</b>：[160]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05341</p>
  <p><b>作者</b>：Lorenz Mindner,  Tim Schlippe,  Kristina Schaaff</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：generative AIs, wide public, AIs like ChatGPT, text, Recently</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, generative AIs like ChatGPT have become available to the wide
public. These tools can for instance be used by students to generate essays or
whole theses. But how does a teacher know whether a text is written by a
student or an AI? In our work, we explore traditional and new features to (1)
detect text generated by AI from scratch and (2) text rephrased by AI. Since we
found that classification is more difficult when the AI has been instructed to
create the text in a way that a human would not recognize that it was generated
by an AI, we also investigate this more advanced case. For our experiments, we
produced a new text corpus covering 10 school topics. Our best systems to
classify basic and advanced human-generated/AI-generated texts have F1-scores
of over 96%. Our best systems for classifying basic and advanced
human-generated/AI-rephrased texts have F1-scores of more than 78%. The systems
use a combination of perplexity, semantic, list lookup, error-based,
readability, AI feedback, and text vector features. Our results show that the
new features substantially help to improve the performance of many classifiers.
Our best basic text rephrasing detection system even outperforms GPTZero by
183.8% relative in F1-score.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Adv-Inpainting: Generating Natural and Transferable Adversarial Patch  via Attention-guided Feature Fusion</b></summary>
  <p><b>编号</b>：[169]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05320</p>
  <p><b>作者</b>：Yanjie Li,  Mingxing Duan,  Bin Xiao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：attack facial recognition, adversarial patches, facial recognition, adversarial, utilize additive noise</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rudimentary adversarial attacks utilize additive noise to attack facial
recognition (FR) models. However, because manipulating the total face is
impractical in the physical setting, most real-world FR attacks are based on
adversarial patches, which limit perturbations to a small area. Previous
adversarial patch attacks often resulted in unnatural patterns and clear
boundaries that were easily noticeable. In this paper, we argue that generating
adversarial patches with plausible content can result in stronger
transferability than using additive noise or directly sampling from the latent
space. To generate natural-looking and highly transferable adversarial patches,
we propose an innovative two-stage coarse-to-fine attack framework called
Adv-Inpainting. In the first stage, we propose an attention-guided StyleGAN
(Att-StyleGAN) that adaptively combines texture and identity features based on
the attention map to generate high-transferable and natural adversarial
patches. In the second stage, we design a refinement network with a new
boundary variance loss to further improve the coherence between the patch and
its surrounding area. Experiment results demonstrate that Adv-Inpainting is
stealthy and can produce adversarial patches with stronger transferability and
improved visual quality than previous adversarial patch attacks.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Homophily-enhanced Structure Learning for Graph Clustering</b></summary>
  <p><b>编号</b>：[175]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05309</p>
  <p><b>作者</b>：Ming Gu,  Gaoming Yang,  Sheng Zhou,  Ning Ma,  Jiawei Chen,  Qiaoyu Tan,  Meihan Liu,  Jiajun Bu</p>
  <p><b>备注</b>：11 pages with 7 figures. Accepted by CIKM'23</p>
  <p><b>关键词</b>：shown impressive results, graph neural networks, utilizing graph neural, graph structure, Graph</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Graph clustering is a fundamental task in graph analysis, and recent advances
in utilizing graph neural networks (GNNs) have shown impressive results.
Despite the success of existing GNN-based graph clustering methods, they often
overlook the quality of graph structure, which is inherent in real-world graphs
due to their sparse and multifarious nature, leading to subpar performance.
Graph structure learning allows refining the input graph by adding missing
links and removing spurious connections. However, previous endeavors in graph
structure learning have predominantly centered around supervised settings, and
cannot be directly applied to our specific clustering tasks due to the absence
of ground-truth labels. To bridge the gap, we propose a novel method called
\textbf{ho}mophily-enhanced structure \textbf{le}arning for graph clustering
(HoLe). Our motivation stems from the observation that subtly enhancing the
degree of homophily within the graph structure can significantly improve GNNs
and clustering outcomes. To realize this objective, we develop two
clustering-oriented structure learning modules, i.e., hierarchical correlation
estimation and cluster-aware sparsification. The former module enables a more
accurate estimation of pairwise node relationships by leveraging guidance from
latent and clustering spaces, while the latter one generates a sparsified
structure based on the similarity matrix and clustering assignments.
Additionally, we devise a joint optimization approach alternating between
training the homophily-enhanced structure learning and GNN-based clustering,
thereby enforcing their reciprocal effects. Extensive experiments on seven
benchmark datasets of various types and scales, across a range of clustering
metrics, demonstrate the superiority of HoLe against state-of-the-art
baselines.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：Double-chain Constraints for 3D Human Pose Estimation in Images and  Videos</b></summary>
  <p><b>编号</b>：[178]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05298</p>
  <p><b>作者</b>：Hongbo Kang,  Yong Wang,  Mengyuan Liu,  Doudou Wu,  Peng Liu,  Wenming Yang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：poses lacking depth, lacking depth information, Local Constraint Module, Constraint Module, Feature Interaction Module</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reconstructing 3D poses from 2D poses lacking depth information is
particularly challenging due to the complexity and diversity of human motion.
The key is to effectively model the spatial constraints between joints to
leverage their inherent dependencies. Thus, we propose a novel model, called
Double-chain Graph Convolutional Transformer (DC-GCT), to constrain the pose
through a double-chain design consisting of local-to-global and global-to-local
chains to obtain a complex representation more suitable for the current human
pose. Specifically, we combine the advantages of GCN and Transformer and design
a Local Constraint Module (LCM) based on GCN and a Global Constraint Module
(GCM) based on self-attention mechanism as well as a Feature Interaction Module
(FIM). The proposed method fully captures the multi-level dependencies between
human body joints to optimize the modeling capability of the model. Moreover,
we propose a method to use temporal information into the single-frame model by
guiding the video sequence embedding through the joint embedding of the target
frame, with negligible increase in computational cost. Experimental results
demonstrate that DC-GCT achieves state-of-the-art performance on two
challenging datasets (Human3.6M and MPI-INF-3DHP). Notably, our model achieves
state-of-the-art performance on all action categories in the Human3.6M dataset
using detected 2D poses from CPN, and our code is available at:
this https URL.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Multimodal Pretrained Models for Sequential Decision-Making: Synthesis,  Verification, Grounding, and Perception</b></summary>
  <p><b>编号</b>：[179]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05295</p>
  <p><b>作者</b>：Yunhao Yang,  Cyrus Neary,  Ufuk Topcu</p>
  <p><b>备注</b>：27 pages, 19 figures, submitted to AIJ</p>
  <p><b>关键词</b>：Recently developed pretrained, rich world knowledge, world knowledge expressed, developed pretrained models, encode rich world</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently developed pretrained models can encode rich world knowledge
expressed in multiple modalities, such as text and images. However, the outputs
of these models cannot be integrated into algorithms to solve sequential
decision-making tasks. We develop an algorithm that utilizes the knowledge from
pretrained models to construct and verify controllers for sequential
decision-making tasks, and to ground these controllers to task environments
through visual observations. In particular, the algorithm queries a pretrained
model with a user-provided, text-based task description and uses the model's
output to construct an automaton-based controller that encodes the model's
task-relevant knowledge. It then verifies whether the knowledge encoded in the
controller is consistent with other independently available knowledge, which
may include abstract information on the environment or user-provided
specifications. If this verification step discovers any inconsistency, the
algorithm automatically refines the controller to resolve the inconsistency.
Next, the algorithm leverages the vision and language capabilities of
pretrained models to ground the controller to the task environment. It collects
image-based observations from the task environment and uses the pretrained
model to link these observations to the text-based control logic encoded in the
controller (e.g., actions and conditions that trigger the actions). We propose
a mechanism to ensure the controller satisfies the user-provided specification
even when perceptual uncertainties are present. We demonstrate the algorithm's
ability to construct, verify, and ground automaton-based controllers through a
suite of real-world tasks, including daily life and robot manipulation tasks.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Cross-heterogeneity Graph Few-shot Learning</b></summary>
  <p><b>编号</b>：[189]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05275</p>
  <p><b>作者</b>：Pengfei Ding,  Yan Wang,  Guanfeng Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：label sparsity issue, graph few-shot learning, recent years, proposed to address, address the label</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, heterogeneous graph few-shot learning has been proposed to
address the label sparsity issue in heterogeneous graphs (HGs), which contain
various types of nodes and edges. The existing methods have achieved good
performance by transferring generalized knowledge extracted from rich-labeled
classes in source HG(s) to few-labeled classes in a target HG. However, these
methods only consider the single-heterogeneity scenario where the source and
target HGs share a fixed set of node/edge types, ignoring the more general
scenario of cross-heterogeneity, where each HG can have a different and
non-fixed set of node/edge types. To this end, we focus on the unexplored
cross-heterogeneity scenario and propose a novel model for Cross-heterogeneity
Graph Few-shot Learning, namely CGFL. In CGFL, we first extract meta-patterns
to capture heterogeneous information and propose a multi-view heterogeneous
graph neural network (MHGN) to learn meta-patterns across HGs. Then, we propose
a score module to measure the informativeness of labeled samples and determine
the transferability of each source HG. Finally, by integrating MHGN and the
score module into a meta-learning mechanism, CGFL can effectively transfer
generalized knowledge to predict new classes with few-labeled data. Extensive
experiments on four real-world datasets have demonstrated the superior
performance of CGFL over the state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：AI4GCC -- Track 3: Consumption and the Challenges of Multi-Agent RL</b></summary>
  <p><b>编号</b>：[195]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05260</p>
  <p><b>作者</b>：Marco Jiralerspong,  Gauthier Gidel</p>
  <p><b>备注</b>：Presented at AI For Global Climate Cooperation Competition, 2023 (arXiv:cs/2307.06951)</p>
  <p><b>关键词</b>：economic policy analysis, bold step forward, traditional economic policy, integrating machine learning, proposed negotiation protocols</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The AI4GCC competition presents a bold step forward in the direction of
integrating machine learning with traditional economic policy analysis. Below,
we highlight two potential areas for improvement that could enhance the
competition's ability to identify and evaluate proposed negotiation protocols.
Firstly, we suggest the inclusion of an additional index that accounts for
consumption/utility as part of the evaluation criteria. Secondly, we recommend
further investigation into the learning dynamics of agents in the simulator and
the game theoretic properties of outcomes from proposed negotiation protocols.
We hope that these suggestions can be of use for future iterations of the
competition/simulation.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：Vector quantization loss analysis in VQGANs: a single-GPU ablation study  for image-to-image synthesis</b></summary>
  <p><b>编号</b>：[201]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05242</p>
  <p><b>作者</b>：Luv Verma,  Varun Mohan</p>
  <p><b>备注</b>：16 pages, 18 figures</p>
  <p><b>关键词</b>：Vector Quantized Generative, Quantized Generative, Vector Quantized, performs an ablation, ablation analysis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This study performs an ablation analysis of Vector Quantized Generative
Adversarial Networks (VQGANs), concentrating on image-to-image synthesis
utilizing a single NVIDIA A100 GPU. The current work explores the nuanced
effects of varying critical parameters including the number of epochs, image
count, and attributes of codebook vectors and latent dimensions, specifically
within the constraint of limited resources. Notably, our focus is pinpointed on
the vector quantization loss, keeping other hyperparameters and loss components
(GAN loss) fixed. This was done to delve into a deeper understanding of the
discrete latent space, and to explore how varying its size affects the
reconstruction. Though, our results do not surpass the existing benchmarks,
however, our findings shed significant light on VQGAN's behaviour for a smaller
dataset, particularly concerning artifacts, codebook size optimization, and
comparative analysis with Principal Component Analysis (PCA). The study also
uncovers the promising direction by introducing 2D positional encodings,
revealing a marked reduction in artifacts and insights into balancing clarity
and overfitting.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Leveraging the Edge and Cloud for V2X-Based Real-Time Object Detection  in Autonomous Driving</b></summary>
  <p><b>编号</b>：[204]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05234</p>
  <p><b>作者</b>：Faisal Hawlader,  François Robinet,  Raphaël Frank</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：module influences core, core driving decisions, influences core driving, perception module influences, key element</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Environmental perception is a key element of autonomous driving because the
information received from the perception module influences core driving
decisions. An outstanding challenge in real-time perception for autonomous
driving lies in finding the best trade-off between detection quality and
latency. Major constraints on both computation and power have to be taken into
account for real-time perception in autonomous vehicles. Larger object
detection models tend to produce the best results, but are also slower at
runtime. Since the most accurate detectors cannot run in real-time locally, we
investigate the possibility of offloading computation to edge and cloud
platforms, which are less resource-constrained. We create a synthetic dataset
to train object detection models and evaluate different offloading strategies.
Using real hardware and network simulations, we compare different trade-offs
between prediction quality and end-to-end delay. Since sending raw frames over
the network implies additional transmission delays, we also explore the use of
JPEG and H.265 compression at varying qualities and measure their impact on
prediction metrics. We show that models with adequate compression can be run in
real-time on the cloud while outperforming local detection performance.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Alexa, play with robot: Introducing the First Alexa Prize SimBot  Challenge on Embodied AI</b></summary>
  <p><b>编号</b>：[208]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05221</p>
  <p><b>作者</b>：Hangjie Shi,  Leslie Ball,  Govind Thattai,  Desheng Zhang,  Lucy Hu,  Qiaozi Gao,  Suhaila Shakiah,  Xiaofeng Gao,  Aishwarya Padmakumar,  Bofei Yang,  Cadence Chung,  Dinakar Guthy,  Gaurav Sukhatme,  Karthika Arumugam,  Matthew Wen,  Osman Ipek,  Patrick Lange,  Rohan Khanna,  Shreyas Pansare,  Vasu Sharma,  Chao Zhang,  Cris Flagg,  Daniel Pressel,  Lavina Vaz,  Luke Dai,  Prasoon Goyal,  Sattvik Sahai,  Shaohua Liu,  Yao Lu,  Anna Gottardi,  Shui Hu,  Yang Liu,  Dilek Hakkani-Tur,  Kate Bland,  Heather Rocker,  James Jeun,  Yadunandana Rao,  Michael Johnston,  Akshaya Iyengar,  Arindam Mandal,  Prem Natarajan,  Reza Ghanadan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Alexa Prize program, SocialBot Grand Challenge, Prize program, SocialBot Grand, empowered numerous university</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The Alexa Prize program has empowered numerous university students to
explore, experiment, and showcase their talents in building conversational
agents through challenges like the SocialBot Grand Challenge and the TaskBot
Challenge. As conversational agents increasingly appear in multimodal and
embodied contexts, it is important to explore the affordances of conversational
interaction augmented with computer vision and physical embodiment. This paper
describes the SimBot Challenge, a new challenge in which university teams
compete to build robot assistants that complete tasks in a simulated physical
environment. This paper provides an overview of the SimBot Challenge, which
included both online and offline challenge phases. We describe the
infrastructure and support provided to the teams including Alexa Arena, the
simulated environment, and the ML toolkit provided to teams to accelerate their
building of vision and language models. We summarize the approaches the
participating teams took to overcome research challenges and extract key
lessons learned. Finally, we provide analysis of the performance of the
competing SimBots during the competition.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题："Generate" the Future of Work through AI: Empirical Evidence from Online  Labor Markets</b></summary>
  <p><b>编号</b>：[212]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05201</p>
  <p><b>作者</b>：Jin Liu (1),  Xingchen Xu (2),  Yongjun Li (1),  Yong Tan (2) ((1) University of Science and Technology of China, (2) University of Washington)</p>
  <p><b>备注</b>：32 pages, 2 figures, 13 tables</p>
  <p><b>关键词</b>：labor market escalates, interest in discerning, discerning its impact, online labor marketplace, labor market</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the advent of general-purpose Generative AI, the interest in discerning
its impact on the labor market escalates. In an attempt to bridge the extant
empirical void, we interpret the launch of ChatGPT as an exogenous shock, and
implement a Difference-in-Differences (DID) approach to quantify its influence
on text-related jobs and freelancers within an online labor marketplace. Our
results reveal a significant decrease in transaction volume for gigs and
freelancers directly exposed to ChatGPT. Additionally, this decline is
particularly marked in units of relatively higher past transaction volume or
lower quality standards. Yet, the negative effect is not universally
experienced among service providers. Subsequent analyses illustrate that
freelancers proficiently adapting to novel advancements and offering services
that augment AI technologies can yield substantial benefits amidst this
transformative period. Consequently, even though the advent of ChatGPT could
conceivably substitute existing occupations, it also unfolds immense
opportunities and carries the potential to reconfigure the future of work. This
research contributes to the limited empirical repository exploring the profound
influence of LLM-based generative AI on the labor market, furnishing invaluable
insights for workers, job intermediaries, and regulatory bodies navigating this
evolving landscape.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Hierarchical Representations for Spatio-Temporal Visual Attention  Modeling and Understanding</b></summary>
  <p><b>编号</b>：[216]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05189</p>
  <p><b>作者</b>：Miguel-Ángel Fernández-Torres</p>
  <p><b>备注</b>：PhD thesis</p>
  <p><b>关键词</b>：visual attention, visual attention modeling, spatio-temporal visual attention, attention, attention modeling</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This PhD. Thesis concerns the study and development of hierarchical
representations for spatio-temporal visual attention modeling and understanding
in video sequences. More specifically, we propose two computational models for
visual attention. First, we present a generative probabilistic model for
context-aware visual attention modeling and understanding. Secondly, we develop
a deep network architecture for visual attention modeling, which first
estimates top-down spatio-temporal visual attention, and ultimately serves for
modeling attention in the temporal domain.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：PromptPaint: Steering Text-to-Image Generation Through Paint Medium-like  Interactions</b></summary>
  <p><b>编号</b>：[218]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05184</p>
  <p><b>作者</b>：John Joon Young Chung,  Eytan Adar</p>
  <p><b>备注</b>：Accepted to UIST2023</p>
  <p><b>关键词</b>：simple and powerful, generate images, generation remains, prompts, PromptPaint</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While diffusion-based text-to-image (T2I) models provide a simple and
powerful way to generate images, guiding this generation remains a challenge.
For concepts that are difficult to describe through language, users may
struggle to create prompts. Moreover, many of these models are built as
end-to-end systems, lacking support for iterative shaping of the image. In
response, we introduce PromptPaint, which combines T2I generation with
interactions that model how we use colored paints. PromptPaint allows users to
go beyond language to mix prompts that express challenging concepts. Just as we
iteratively tune colors through layered placements of paint on a physical
canvas, PromptPaint similarly allows users to apply different prompts to
different canvas areas and times of the generative process. Through a set of
studies, we characterize different approaches for mixing prompts, design
trade-offs, and socio-technical challenges for generative models. With
PromptPaint we provide insight into future steerable generative tools.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：FPGA Resource-aware Structured Pruning for Real-Time Neural Networks</b></summary>
  <p><b>编号</b>：[221]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05170</p>
  <p><b>作者</b>：Benjamin Ramhorst (Imperial College London),  George A. Constantinides (Imperial College London),  Vladimir Loncar (Massachusetts Institute of Technology)</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：speech recognition, scientific analysis, application areas, deep learning inference, Neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural networks achieve state-of-the-art performance in image classification,
speech recognition, scientific analysis and many more application areas. With
the ever-increasing need for faster computation and lower power consumption,
driven by real-time systems and Internet-of-Things (IoT) devices, FPGAs have
emerged as suitable devices for deep learning inference. Due to the high
computational complexity and memory footprint of neural networks, various
compression techniques, such as pruning, quantization and knowledge
distillation, have been proposed in literature. Pruning sparsifies a neural
network, reducing the number of multiplications and memory. However, pruning
often fails to capture properties of the underlying hardware, causing
unstructured sparsity and load-balance inefficiency, thus bottlenecking
resource improvements. We propose a hardware-centric formulation of pruning, by
formulating it as a knapsack problem with resource-aware tensor structures. The
primary emphasis is on real-time inference, with latencies in the order of
1$\mu$s, accelerated with hls4ml, an open-source framework for deep learning
inference on FPGAs. Evaluated on a range of tasks, including real-time particle
classification at CERN's Large Hadron Collider and fast image classification,
the proposed method achieves a reduction ranging between 55% and 92% in the
utilization of digital signal processing blocks (DSP) and up to 81% in block
memory (BRAM) utilization.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Data-Free Model Extraction Attacks in the Context of Object Detection</b></summary>
  <p><b>编号</b>：[228]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05127</p>
  <p><b>作者</b>：Harshit Shah,  Aravindhan G,  Pavan Kulkarni,  Yuvaraj Govidarajulu,  Manojkumar Parmar</p>
  <p><b>备注</b>：Submitted to The 14th International Conference on Computer Vision Systems (ICVS 2023), to be published in Springer, Lecture Notes in Computer Science</p>
  <p><b>关键词</b>：machine learning models, target model, Generative Adversarial Nets, number of machine, machine learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A significant number of machine learning models are vulnerable to model
extraction attacks, which focus on stealing the models by using specially
curated queries against the target model. This task is well accomplished by
using part of the training data or a surrogate dataset to train a new model
that mimics a target model in a white-box environment. In pragmatic situations,
however, the target models are trained on private datasets that are
inaccessible to the adversary. The data-free model extraction technique
replaces this problem when it comes to using queries artificially curated by a
generator similar to that used in Generative Adversarial Nets. We propose for
the first time, to the best of our knowledge, an adversary black box attack
extending to a regression problem for predicting bounding box coordinates in
object detection. As part of our study, we found that defining a loss function
and using a novel generator setup is one of the key aspects in extracting the
target model. We find that the proposed model extraction method achieves
significant results by using reasonable queries. The discovery of this object
detection vulnerability will support future prospects for securing such models.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Balancing Accuracy and Training Time in Federated Learning for Violence  Detection in Surveillance Videos: A Study of Neural Network Architectures</b></summary>
  <p><b>编号</b>：[233]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05106</p>
  <p><b>作者</b>：Pajon Quentin,  Serre Swan,  Wissocq Hugo,  Rabaud Léo,  Haidar Siba,  Yaacoub Antoun</p>
  <p><b>备注</b>：8 pages, 2 figures, FL-IJCAI'23</p>
  <p><b>关键词</b>：federated learning context, machine learning techniques, federated learning, paper presents, presents an investigation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents an investigation into machine learning techniques for
violence detection in videos and their adaptation to a federated learning
context. The study includes experiments with spatio-temporal features extracted
from benchmark video datasets, comparison of different methods, and proposal of
a modified version of the "Flow-Gated" architecture called "Diff-Gated."
Additionally, various machine learning techniques, including super-convergence
and transfer learning, are explored, and a method for adapting centralized
datasets to a federated learning context is developed. The research achieves
better accuracy results compared to state-of-the-art models by training the
best violence detection model in a federated learning context.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Updating Clinical Risk Stratification Models Using Rank-Based  Compatibility: Approaches for Evaluating and Optimizing Clinician-Model Team  Performance</b></summary>
  <p><b>编号</b>：[239]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05619</p>
  <p><b>作者</b>：Erkin Ötleş,  Brian T. Denton,  Jenna Wiens</p>
  <p><b>备注</b>：Conference paper accepted at the 2023 Machine Learning for Healthcare Conference Includes supplemental: 32 pages, 17 figures</p>
  <p><b>关键词</b>：machine learning models, clinical machine learning, updating clinical machine, machine learning, maintain or improve</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As data shift or new data become available, updating clinical machine
learning models may be necessary to maintain or improve performance over time.
However, updating a model can introduce compatibility issues when the behavior
of the updated model does not align with user expectations, resulting in poor
user-model team performance. Existing compatibility measures depend on model
decision thresholds, limiting their applicability in settings where models are
used to generate rankings based on estimated risk. To address this limitation,
we propose a novel rank-based compatibility measure, $C^R$, and a new loss
function that aims to optimize discriminative performance while encouraging
good compatibility. Applied to a case study in mortality risk stratification
leveraging data from MIMIC, our approach yields more compatible models while
maintaining discriminative performance compared to existing model selection
techniques, with an increase in $C^R$ of $0.019$ ($95\%$ confidence interval:
$0.005$, $0.035$). This work provides new tools to analyze and update risk
stratification models used in clinical care.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Learning (With) Distributed Optimization</b></summary>
  <p><b>编号</b>：[241]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05548</p>
  <p><b>作者</b>：Aadharsh Aadhithya A,  Abinesh S,  Akshaya J,  Jayanth M,  Vishnu Radhakrishnan,  Sowmya V,  Soman K.P</p>
  <p><b>备注</b>：23 pages</p>
  <p><b>关键词</b>：Direction Inexact Newton, Augmented Lagrangian Alternating, Alternating Direction Inexact, Lagrangian Alternating Direction, Inexact Newton</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper provides an overview of the historical progression of distributed
optimization techniques, tracing their development from early duality-based
methods pioneered by Dantzig, Wolfe, and Benders in the 1960s to the emergence
of the Augmented Lagrangian Alternating Direction Inexact Newton (ALADIN)
algorithm. The initial focus on Lagrangian relaxation for convex problems and
decomposition strategies led to the refinement of methods like the Alternating
Direction Method of Multipliers (ADMM). The resurgence of interest in
distributed optimization in the late 2000s, particularly in machine learning
and imaging, demonstrated ADMM's practical efficacy and its unifying potential.
This overview also highlights the emergence of the proximal center method and
its applications in diverse domains. Furthermore, the paper underscores the
distinctive features of ALADIN, which offers convergence guarantees for
non-convex scenarios without introducing auxiliary variables, differentiating
it from traditional augmentation techniques. In essence, this work encapsulates
the historical trajectory of distributed optimization and underscores the
promising prospects of ALADIN in addressing non-convex optimization challenges.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Exploring the Potential of World Models for Anomaly Detection in  Autonomous Driving</b></summary>
  <p><b>编号</b>：[254]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05701</p>
  <p><b>作者</b>：Daniel Bogdoll,  Lukas Bosch,  Tim Joseph,  Helen Gremmelmaier,  Yitian Yang,  J. Marius Zöllner</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：recent years, remarkable advancements, autonomous driving, world models, autonomous vehicles demonstrate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years there have been remarkable advancements in autonomous
driving. While autonomous vehicles demonstrate high performance in closed-set
conditions, they encounter difficulties when confronted with unexpected
situations. At the same time, world models emerged in the field of model-based
reinforcement learning as a way to enable agents to predict the future
depending on potential actions. This led to outstanding results in sparse
reward and complex control tasks. This work provides an overview of how world
models can be leveraged to perform anomaly detection in the domain of
autonomous driving. We provide a characterization of world models and relate
individual components to previous works in anomaly detection to facilitate
further research in the field.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：SSLRec: A Self-Supervised Learning Library for Recommendation</b></summary>
  <p><b>编号</b>：[257]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05697</p>
  <p><b>作者</b>：Xubin Ren,  Lianghao Xia,  Yuhao Yang,  Wei Wei,  Tianle Wang,  Xuheng Cai,  Chao Huang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：gained significant interest, gained significant, significant interest, interest in recent, recent years</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Self-supervised learning (SSL) has gained significant interest in recent
years as a solution to address the challenges posed by sparse and noisy data in
recommender systems. Despite the growing number of SSL algorithms designed to
provide state-of-the-art performance in various recommendation scenarios (e.g.,
graph collaborative filtering, sequential recommendation, social
recommendation, KG-enhanced recommendation), there is still a lack of unified
frameworks that integrate recommendation algorithms across different domains.
Such a framework could serve as the cornerstone for self-supervised
recommendation algorithms, unifying the validation of existing methods and
driving the design of new ones. To address this gap, we introduce SSLRec, a
novel benchmark platform that provides a standardized, flexible, and
comprehensive framework for evaluating various SSL-enhanced recommenders. The
SSLRec library features a modular architecture that allows users to easily
evaluate state-of-the-art models and a complete set of data augmentation and
self-supervised toolkits to help create SSL recommendation models with specific
needs. Furthermore, SSLRec simplifies the process of training and evaluating
different recommendation models with consistent and fair settings. Our SSLRec
platform covers a comprehensive set of state-of-the-art SSL-enhanced
recommendation models across different scenarios, enabling researchers to
evaluate these cutting-edge models and drive further innovation in the field.
Our implemented SSLRec framework is available at the source code repository
this https URL.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：Hard No-Box Adversarial Attack on Skeleton-Based Human Action  Recognition with Skeleton-Motion-Informed Gradient</b></summary>
  <p><b>编号</b>：[263]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05681</p>
  <p><b>作者</b>：Zhengzhi Lu,  He Wang,  Ziyi Chang,  Guoan Yang,  Hubert P. H. Shum</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：skeleton-based human activity, human activity recognition, attack methods, skeleton-based human, human activity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, methods for skeleton-based human activity recognition have been
shown to be vulnerable to adversarial attacks. However, these attack methods
require either the full knowledge of the victim (i.e. white-box attacks),
access to training data (i.e. transfer-based attacks) or frequent model queries
(i.e. black-box attacks). All their requirements are highly restrictive,
raising the question of how detrimental the vulnerability is. In this paper, we
show that the vulnerability indeed exists. To this end, we consider a new
attack task: the attacker has no access to the victim model or the training
data or labels, where we coin the term hard no-box attack. Specifically, we
first learn a motion manifold where we define an adversarial loss to compute a
new gradient for the attack, named skeleton-motion-informed (SMI) gradient. Our
gradient contains information of the motion dynamics, which is different from
existing gradient-based attack methods that compute the loss gradient assuming
each dimension in the data is independent. The SMI gradient can augment many
gradient-based attack methods, leading to a new family of no-box attack
methods. Extensive evaluation and comparison show that our method imposes a
real threat to existing classifiers. They also show that the SMI gradient
improves the transferability and imperceptibility of adversarial samples in
both no-box and transfer-based black-box settings.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Exploring Deep Learning Approaches to Predict Person and Vehicle Trips:  An Analysis of NHTS Data</b></summary>
  <p><b>编号</b>：[267]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2308.05665</p>
  <p><b>作者</b>：Kojo Adu-Gyamfi,  Sharma Anuj</p>
  <p><b>备注</b>：15 pages, 11 figures</p>
  <p><b>关键词</b>：planning relies heavily, Modern transportation planning, deep learning, transportation planning relies, transportation planning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Modern transportation planning relies heavily on accurate predictions of
person and vehicle trips. However, traditional planning models often fail to
account for the intricacies and dynamics of travel behavior, leading to
less-than-optimal accuracy in these predictions. This study explores the
potential of deep learning techniques to transform the way we approach trip
predictions, and ultimately, transportation planning. Utilizing a comprehensive
dataset from the National Household Travel Survey (NHTS), we developed and
trained a deep learning model for predicting person and vehicle trips. The
proposed model leverages the vast amount of information in the NHTS data,
capturing complex, non-linear relationships that were previously overlooked by
traditional models. As a result, our deep learning model achieved an impressive
accuracy of 98% for person trip prediction and 96% for vehicle trip estimation.
This represents a significant improvement over the performances of traditional
transportation planning models, thereby demonstrating the power of deep
learning in this domain. The implications of this study extend beyond just more
accurate predictions. By enhancing the accuracy and reliability of trip
prediction models, planners can formulate more effective, data-driven
transportation policies, infrastructure, and services. As such, our research
underscores the need for the transportation planning field to embrace advanced
techniques like deep learning. The detailed methodology, along with a thorough
discussion of the results and their implications, are presented in the
subsequent sections of this paper.</p>
  </details>
</details>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">徐耀彬</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://louishsu.xyz/2023/08/14/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">http://louishsu.xyz/2023/08/14/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://louishsu.xyz" target="_blank">LOUIS' BLOG</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2023/05/07/%E3%80%90%E6%A2%B3%E7%90%86%E3%80%91%E9%99%86%E5%A5%87%E6%9C%80%E6%96%B0%E6%BC%94%E8%AE%B2%E5%AE%9E%E5%BD%95%EF%BC%9A%E6%88%91%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%96%E7%95%8C%E8%A7%82%20.html"><img class="next-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">【梳理】陆奇最新演讲实录：我的大模型世界观</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">徐耀彬</div><div class="author-info__description">专注于自然语言处理前沿技术与应用价值！</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">18</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/isLouisHsu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/isLouisHsu" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:is.louishsu@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">记录和分享一些学习和开源内容，若有问题可通过邮箱is.louishsu@foxmail.com联系，欢迎交流！！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">统计</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">计算机视觉</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">自然语言处理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">机器学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">人工智能</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/08/14/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2023-08-14)"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv每日速递(2023-08-14)"/></a><div class="content"><a class="title" href="/2023/08/14/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2023-08-14)">Arxiv每日速递(2023-08-14)</a><time datetime="2023-08-14T00:34:35.949Z" title="发表于 2023-08-14 08:34:35">2023-08-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/07/%E3%80%90%E6%A2%B3%E7%90%86%E3%80%91%E9%99%86%E5%A5%87%E6%9C%80%E6%96%B0%E6%BC%94%E8%AE%B2%E5%AE%9E%E5%BD%95%EF%BC%9A%E6%88%91%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%96%E7%95%8C%E8%A7%82%20.html" title="【梳理】陆奇最新演讲实录：我的大模型世界观"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【梳理】陆奇最新演讲实录：我的大模型世界观"/></a><div class="content"><a class="title" href="/2023/05/07/%E3%80%90%E6%A2%B3%E7%90%86%E3%80%91%E9%99%86%E5%A5%87%E6%9C%80%E6%96%B0%E6%BC%94%E8%AE%B2%E5%AE%9E%E5%BD%95%EF%BC%9A%E6%88%91%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%96%E7%95%8C%E8%A7%82%20.html" title="【梳理】陆奇最新演讲实录：我的大模型世界观">【梳理】陆奇最新演讲实录：我的大模型世界观</a><time datetime="2023-05-07T11:07:45.000Z" title="发表于 2023-05-07 19:07:45">2023-05-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/05/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8(Variational%20AutoEncoder).html" title="变分自编码器(Variational AutoEncoder)"><img src="https://lilianweng.github.io/posts/2018-08-12-vae/autoencoder-architecture.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="变分自编码器(Variational AutoEncoder)"/></a><div class="content"><a class="title" href="/2023/05/05/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8(Variational%20AutoEncoder).html" title="变分自编码器(Variational AutoEncoder)">变分自编码器(Variational AutoEncoder)</a><time datetime="2023-05-05T11:28:37.000Z" title="发表于 2023-05-05 19:28:37">2023-05-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/04/08/transformers.generation.GenerationMixin.html" title="transformers.generation.GenerationMixin"><img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="transformers.generation.GenerationMixin"/></a><div class="content"><a class="title" href="/2023/04/08/transformers.generation.GenerationMixin.html" title="transformers.generation.GenerationMixin">transformers.generation.GenerationMixin</a><time datetime="2023-04-08T13:42:45.000Z" title="发表于 2023-04-08 21:42:45">2023-04-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/27/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91ChatGPT%20%E6%A0%87%E6%B3%A8%E6%8C%87%E5%8D%97%EF%BC%9A%E4%BB%BB%E5%8A%A1%E3%80%81%E6%95%B0%E6%8D%AE%E4%B8%8E%E8%A7%84%E8%8C%83.html" title="【转载】ChatGPT 标注指南：任务、数据与规范"><img src="https://openaicom.imgix.net/8d14e8f0-e267-4b8b-a9f2-a79120808f5a/chatgpt.jpg?auto=compress%2Cformat&amp;fit=min&amp;fm=jpg&amp;q=80&amp;rect=0%2C0%2C2048%2C2048&amp;w=3200" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【转载】ChatGPT 标注指南：任务、数据与规范"/></a><div class="content"><a class="title" href="/2023/03/27/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91ChatGPT%20%E6%A0%87%E6%B3%A8%E6%8C%87%E5%8D%97%EF%BC%9A%E4%BB%BB%E5%8A%A1%E3%80%81%E6%95%B0%E6%8D%AE%E4%B8%8E%E8%A7%84%E8%8C%83.html" title="【转载】ChatGPT 标注指南：任务、数据与规范">【转载】ChatGPT 标注指南：任务、数据与规范</a><time datetime="2023-03-27T14:35:45.000Z" title="发表于 2023-03-27 22:35:45">2023-03-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2023 By 徐耀彬</div><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender"title="主题采用butterfly"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr"title="本站使用JsDelivr为静态资源提供CDN加速"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub"title="本站项目由Gtihub托管"alt="img"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"alt="img"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></br></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    let initData = {
      el: '#twikoo-wrap',
      envId: 'blog-',
      region: ''
    }

    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    twikoo.init(initData)
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'blog-',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/机器学习/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐱 机器学习 (3)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/自然语言处理/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 自然语言处理 (5)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/竞赛相关/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 竞赛相关 (3)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/阅读笔记/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 阅读笔记 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="http://louishsu.xyz/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style>
  <script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/hexo-github-calendar@1.21/hexo_githubcalendar.js"></script>
  <script data-pjax>
        function GithubCalendarConfig(){
            var git_githubapiurl ="https://python-github-calendar-api.vercel.app/api?isLouisHsu";
            var git_color =['#ebedf0', '#fdcdec', '#fc9bd9', '#fa6ac5', '#f838b2', '#f5089f', '#c4067e', '#92055e', '#540336', '#48022f', '#30021f'];
            var git_user ="isLouisHsu";
            var parent_div_git = document.getElementById('recent-posts');
            var git_div_html = '<div class="recent-post-item" style="width:100%;height:auto;padding:10px;"><div id="github_loading" style="width:10%;height:100%;margin:0 auto;display: block"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"  viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animateTransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animateTransform></path></svg></div><div id="github_container"></div></div>';
            if(parent_div_git && location.pathname =='/'){
                console.log('已挂载github calendar')
                // parent_div_git.innerHTML=git_div_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",git_div_html) // 有报错，但不影响使用(支持pjax跳转)
            };
            GithubCalendar(git_githubapiurl,git_color,git_user)
        }
        if(document.getElementById('recent-posts')){
            GithubCalendarConfig()
        }
    </script>
    <style>#github_container{min-height:280px}@media screen and (max-width:650px) {#github_container{background-image:;min-height:0px}}</style>
    <style></style><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>