<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Arxiv每日速递(2023-10-16) | LOUIS' BLOG</title><meta name="author" content="徐耀彬"><meta name="copyright" content="徐耀彬"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。 统计 今日共更新442篇论文，其中：  125篇计算机视觉（cs.CV） 76篇自然语言处理（cs.CL） 132篇机器学习（cs.LG） 110篇人工智能（cs.AI）  计算机视觉    1. 标题：Octopus: Embodied Vision-Language P">
<meta property="og:type" content="article">
<meta property="og:title" content="Arxiv每日速递(2023-10-16)">
<meta property="og:url" content="http://louishsu.xyz/2023/10/16/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">
<meta property="og:site_name" content="LOUIS&#39; BLOG">
<meta property="og:description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。 统计 今日共更新442篇论文，其中：  125篇计算机视觉（cs.CV） 76篇自然语言处理（cs.CL） 132篇机器学习（cs.LG） 110篇人工智能（cs.AI）  计算机视觉    1. 标题：Octopus: Embodied Vision-Language P">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png">
<meta property="article:published_time" content="2023-10-16T15:18:17.921Z">
<meta property="article:modified_time" content="2023-10-16T15:20:34.766Z">
<meta property="article:author" content="徐耀彬">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://louishsu.xyz/2023/10/16/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2023-10-16 23:20:34'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/background.css"><script src="https://cdn.jsdelivr.net/npm/echarts@4.7.0/dist/echarts.min.js"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-brands fa-app-store"></i><span> 利器</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://code.visualstudio.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> VSCode：微软旗下的跨平台代码编辑软件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mobaxterm.mobatek.net/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> MobaXterm：超好用的全能远程终端</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.zotero.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zotero：便于收集、组织、引用、共享的文献管理工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/CopyTranslator/CopyTranslator"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> CopyTranslator：“复制即翻译”的外文辅助阅读翻译解决方案</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ditto-cp.sourceforge.io/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Ditto：强大的Windows剪贴板增强工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/indiff/qttabbar"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> QtTabBar：在Windows资源管理器中使用多标签功能扩展工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/sentialx/multrin"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Multrin：“窗口合并”辅助小工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/Wox-launcher/Wox"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Wox &amp; Everything：基于名称快速定位文件和文件夹的搜索工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/NickeManarin/ScreenToGif"><i class="fa-fw fa-regular fa-star"></i><span> ScreenToGif：快速录制屏幕指定区域并保存为动图文件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mathpix.com/"><i class="fa-fw fa-regular fa-star"></i><span> Mathpix Snipping：识别数学公式并转换成LaTeX</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">LOUIS' BLOG</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-brands fa-app-store"></i><span> 利器</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://code.visualstudio.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> VSCode：微软旗下的跨平台代码编辑软件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mobaxterm.mobatek.net/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> MobaXterm：超好用的全能远程终端</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.zotero.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zotero：便于收集、组织、引用、共享的文献管理工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/CopyTranslator/CopyTranslator"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> CopyTranslator：“复制即翻译”的外文辅助阅读翻译解决方案</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ditto-cp.sourceforge.io/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Ditto：强大的Windows剪贴板增强工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/indiff/qttabbar"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> QtTabBar：在Windows资源管理器中使用多标签功能扩展工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/sentialx/multrin"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Multrin：“窗口合并”辅助小工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/Wox-launcher/Wox"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Wox &amp; Everything：基于名称快速定位文件和文件夹的搜索工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/NickeManarin/ScreenToGif"><i class="fa-fw fa-regular fa-star"></i><span> ScreenToGif：快速录制屏幕指定区域并保存为动图文件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mathpix.com/"><i class="fa-fw fa-regular fa-star"></i><span> Mathpix Snipping：识别数学公式并转换成LaTeX</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Arxiv每日速递(2023-10-16)<a class="post-edit-link" href="https://github.com/isLouisHsu/blog/tree/master/source_posts/Arxiv每日速递.md" title="编辑" target="_blank"><i class="fas fa-pencil-square"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-10-16T15:18:17.921Z" title="发表于 2023-10-16 23:18:17">2023-10-16</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-10-16T15:20:34.766Z" title="更新于 2023-10-16 23:20:34">2023-10-16</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">阅读笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>45分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2023/10/16/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以计算机视觉、自然语言处理、机器学习、人工智能等大方向进行划分。</p>
<h1>统计</h1>
<p>今日共更新442篇论文，其中：</p>
<ul>
<li><a href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89">125篇计算机视觉（cs.CV）</a></li>
<li><a href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86">76篇自然语言处理（cs.CL）</a></li>
<li><a href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">132篇机器学习（cs.LG）</a></li>
<li><a href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD">110篇人工智能（cs.AI）</a></li>
</ul>
<h1>计算机视觉</h1>
<details>
  <summary>1. <b>标题：Octopus: Embodied Vision-Language Programmer from Environmental Feedback</b></summary>
  <p><b>编号</b>：[1]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08588</p>
  <p><b>作者</b>：Jingkang Yang,  Yuhao Dong,  Shuai Liu,  Bo Li,  Ziyue Wang,  Chencheng Jiang,  Haoran Tan,  Jiamu Kang,  Yuanhan Zhang,  Kaiyang Zhou,  Ziwei Liu</p>
  <p><b>备注</b>：Project Page: this https URL, Codebase: this https URL</p>
  <p><b>关键词</b>：achieved substantial progress, Large vision-language models, Large vision-language, perception and reasoning, achieved substantial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large vision-language models (VLMs) have achieved substantial progress in multimodal perception and reasoning. Furthermore, when seamlessly integrated into an embodied agent, it signifies a crucial stride towards the creation of autonomous and context-aware systems capable of formulating plans and executing commands with precision. In this paper, we introduce Octopus, a novel VLM designed to proficiently decipher an agent's vision and textual task objectives and to formulate intricate action sequences and generate executable code. Our design allows the agent to adeptly handle a wide spectrum of tasks, ranging from mundane daily chores in simulators to sophisticated interactions in complex video games. Octopus is trained by leveraging GPT-4 to control an explorative agent to generate training data, i.e., action blueprints and the corresponding executable code, within our experimental environment called OctoVerse. We also collect the feedback that allows the enhanced training scheme of Reinforcement Learning with Environmental Feedback (RLEF). Through a series of experiments, we illuminate Octopus's functionality and present compelling results, and the proposed RLEF turns out to refine the agent's decision-making. By open-sourcing our model architecture, simulator, and dataset, we aspire to ignite further innovation and foster collaborative applications within the broader embodied AI community.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Is Generalized Dynamic Novel View Synthesis from Monocular Videos  Possible Today?</b></summary>
  <p><b>编号</b>：[2]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08587</p>
  <p><b>作者</b>：Xiaoming Zhao,  Alex Colburn,  Fangchang Ma,  Miguel Angel Bautista,  Joshua M. Susskind,  Alexander G. Schwing</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：Rendering scenes observed, challenging problem, Rendering scenes, scene-specific, optimization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Rendering scenes observed in a monocular video from novel viewpoints is a challenging problem. For static scenes the community has studied both scene-specific optimization techniques, which optimize on every test scene, and generalized techniques, which only run a deep net forward pass on a test scene. In contrast, for dynamic scenes, scene-specific optimization techniques exist, but, to our best knowledge, there is currently no generalized method for dynamic novel view synthesis from a given monocular video. To answer whether generalized dynamic novel view synthesis from monocular videos is possible today, we establish an analysis framework based on existing techniques and work toward the generalized approach. We find a pseudo-generalized process without scene-specific appearance optimization is possible, but geometrically and temporally consistent depth estimates are needed. Despite no scene-specific appearance optimization, the pseudo-generalized approach improves upon some scene-specific methods.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：PonderV2: Pave the Way for 3D Foundation Model with A Universal  Pre-training Paradigm</b></summary>
  <p><b>编号</b>：[3]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08586</p>
  <p><b>作者</b>：Haoyi Zhu,  Honghui Yang,  Xiaoyang Wu,  Di Huang,  Sha Zhang,  Xianglong He,  Tong He,  Hengshuang Zhao,  Chunhua Shen,  Yu Qiao,  Wanli Ouyang</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2301.00157</p>
  <p><b>关键词</b>：poses considerably greater, computer vision foundational, model poses considerably, numerous NLP, vision foundational models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In contrast to numerous NLP and 2D computer vision foundational models, the learning of a robust and highly generalized 3D foundational model poses considerably greater challenges. This is primarily due to the inherent data variability and the diversity of downstream tasks. In this paper, we introduce a comprehensive 3D pre-training framework designed to facilitate the acquisition of efficient 3D representations, thereby establishing a pathway to 3D foundational models. Motivated by the fact that informative 3D features should be able to encode rich geometry and appearance cues that can be utilized to render realistic images, we propose a novel universal paradigm to learn point cloud representations by differentiable neural rendering, serving as a bridge between 3D and 2D worlds. We train a point cloud encoder within a devised volumetric neural renderer by comparing the rendered images with the real images. Notably, our approach demonstrates the seamless integration of the learned 3D encoder into diverse downstream tasks. These tasks encompass not only high-level challenges such as 3D detection and segmentation but also low-level objectives like 3D reconstruction and image synthesis, spanning both indoor and outdoor scenarios. Besides, we also illustrate the capability of pre-training a 2D backbone using the proposed universal methodology, surpassing conventional pre-training methods by a large margin. For the first time, PonderV2 achieves state-of-the-art performance on 11 indoor and outdoor benchmarks. The consistent improvements in various settings imply the effectiveness of the proposed method. Code and models will be made available at this https URL.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Im4D: High-Fidelity and Real-Time Novel View Synthesis for Dynamic  Scenes</b></summary>
  <p><b>编号</b>：[4]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08585</p>
  <p><b>作者</b>：Haotong Lin,  Sida Peng,  Zhen Xu,  Tao Xie,  Xingyi He,  Hujun Bao,  Xiaowei Zhou</p>
  <p><b>备注</b>：SIGGRAPH Asia 2023; Project page: this https URL</p>
  <p><b>关键词</b>：paper aims, aims to tackle, tackle the challenge, dynamic view synthesis, multi-view image-based appearance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper aims to tackle the challenge of dynamic view synthesis from multi-view videos. The key observation is that while previous grid-based methods offer consistent rendering, they fall short in capturing appearance details of a complex dynamic scene, a domain where multi-view image-based rendering methods demonstrate the opposite properties. To combine the best of two worlds, we introduce Im4D, a hybrid scene representation that consists of a grid-based geometry representation and a multi-view image-based appearance representation. Specifically, the dynamic geometry is encoded as a 4D density function composed of spatiotemporal feature planes and a small MLP network, which globally models the scene structure and facilitates the rendering consistency. We represent the scene appearance by the original multi-view videos and a network that learns to predict the color of a 3D point from image features, instead of memorizing detailed appearance totally with networks, thereby naturally making the learning of networks easier. Our method is evaluated on five dynamic view synthesis datasets including DyNeRF, ZJU-MoCap, NHR, DNA-Rendering and ENeRF-Outdoor datasets. The results show that Im4D exhibits state-of-the-art performance in rendering quality and can be trained efficiently, while realizing real-time rendering with a speed of 79.8 FPS for 512x512 images, on a single RTX 3090 GPU.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Is ImageNet worth 1 video? Learning strong image encoders from 1 long  unlabelled video</b></summary>
  <p><b>编号</b>：[5]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08584</p>
  <p><b>作者</b>：Shashanka Venkataramanan,  Mamshad Nayeem Rizve,  João Carreira,  Yuki M. Asano,  Yannis Avrithis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：annotation is unnecessary, unlocked the potential, potential of scaling, Walking Tours, Walking Tours video</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Self-supervised learning has unlocked the potential of scaling up pretraining to billions of images, since annotation is unnecessary. But are we making the best use of data? How more economical can we be? In this work, we attempt to answer this question by making two contributions. First, we investigate first-person videos and introduce a "Walking Tours" dataset. These videos are high-resolution, hours-long, captured in a single uninterrupted take, depicting a large number of objects and actions with natural scene transitions. They are unlabeled and uncurated, thus realistic for self-supervision and comparable with human learning.
Second, we introduce a novel self-supervised image pretraining method tailored for learning from continuous videos. Existing methods typically adapt image-based pretraining approaches to incorporate more frames. Instead, we advocate a "tracking to learn to recognize" approach. Our method called DoRA, leads to attention maps that Discover and tRAck objects over time in an end-to-end manner, using transformer cross-attention. We derive multiple views from the tracks and use them in a classical self-supervised distillation loss. Using our novel approach, a single Walking Tours video remarkably becomes a strong competitor to ImageNet for several image and video downstream tasks.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Universal Visual Decomposer: Long-Horizon Manipulation Made Easy</b></summary>
  <p><b>编号</b>：[8]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08581</p>
  <p><b>作者</b>：Zichen Zhang,  Yunshuang Li,  Osbert Bastani,  Abhishek Gupta,  Dinesh Jayaraman,  Yecheng Jason Ma,  Luca Weihs</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：encompass multiple stages, multiple stages, encompass multiple, UVD, robotic tasks stretch</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Real-world robotic tasks stretch over extended horizons and encompass multiple stages. Learning long-horizon manipulation tasks, however, is a long-standing challenge, and demands decomposing the overarching task into several manageable subtasks to facilitate policy learning and generalization to unseen tasks. Prior task decomposition methods require task-specific knowledge, are computationally intensive, and cannot readily be applied to new tasks. To address these shortcomings, we propose Universal Visual Decomposer (UVD), an off-the-shelf task decomposition method for visual long horizon manipulation using pre-trained visual representations designed for robotic control. At a high level, UVD discovers subgoals by detecting phase shifts in the embedding space of the pre-trained representation. Operating purely on visual demonstrations without auxiliary information, UVD can effectively extract visual subgoals embedded in the videos, while incurring zero additional training cost on top of standard visuomotor policy training. Goal-conditioned policies learned with UVD-discovered subgoals exhibit significantly improved compositional generalization at test time to unseen tasks. Furthermore, UVD-discovered subgoals can be used to construct goal-based reward shaping that jump-starts temporally extended exploration for reinforcement learning. We extensively evaluate UVD on both simulation and real-world tasks, and in all cases, UVD substantially outperforms baselines across imitation and reinforcement learning settings on in-domain and out-of-domain task sequences alike, validating the clear advantage of automated visual task decomposition within the simple, compact UVD framework.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：OmniControl: Control Any Joint at Any Time for Human Motion Generation</b></summary>
  <p><b>编号</b>：[9]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08580</p>
  <p><b>作者</b>：Yiming Xie,  Varun Jampani,  Lei Zhong,  Deqing Sun,  Huaizu Jiang</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：generation model based, flexible spatial control, text-conditioned human motion, human motion generation, approach named OmniControl</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a novel approach named OmniControl for incorporating flexible spatial control signals into a text-conditioned human motion generation model based on the diffusion process. Unlike previous methods that can only control the pelvis trajectory, OmniControl can incorporate flexible spatial control signals over different joints at different times with only one model. Specifically, we propose analytic spatial guidance that ensures the generated motion can tightly conform to the input control signals. At the same time, realism guidance is introduced to refine all the joints to generate more coherent motion. Both the spatial and realism guidance are essential and they are highly complementary for balancing control accuracy and motion realism. By combining them, OmniControl generates motions that are realistic, coherent, and consistent with the spatial constraints. Experiments on HumanML3D and KIT-ML datasets show that OmniControl not only achieves significant improvement over state-of-the-art methods on pelvis control but also shows promising results when incorporating the constraints over other joints.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：HyperHuman: Hyper-Realistic Human Generation with Latent Structural  Diffusion</b></summary>
  <p><b>编号</b>：[10]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08579</p>
  <p><b>作者</b>：Xian Liu,  Jian Ren,  Aliaksandr Siarohin,  Ivan Skorokhodov,  Yanyu Li,  Dahua Lin,  Xihui Liu,  Ziwei Liu,  Sergey Tulyakov</p>
  <p><b>备注</b>：Project Page: this https URL</p>
  <p><b>关键词</b>：unsolved task, human images, significant advances, remains a desirable, desirable yet unsolved</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite significant advances in large-scale text-to-image models, achieving hyper-realistic human image generation remains a desirable yet unsolved task. Existing models like Stable Diffusion and DALL-E 2 tend to generate human images with incoherent parts or unnatural poses. To tackle these challenges, our key insight is that human image is inherently structural over multiple granularities, from the coarse-level body skeleton to fine-grained spatial geometry. Therefore, capturing such correlations between the explicit appearance and latent structure in one model is essential to generate coherent and natural human images. To this end, we propose a unified framework, HyperHuman, that generates in-the-wild human images of high realism and diverse layouts. Specifically, 1) we first build a large-scale human-centric dataset, named HumanVerse, which consists of 340M images with comprehensive annotations like human pose, depth, and surface normal. 2) Next, we propose a Latent Structural Diffusion Model that simultaneously denoises the depth and surface normal along with the synthesized RGB image. Our model enforces the joint learning of image appearance, spatial relationship, and geometry in a unified network, where each branch in the model complements to each other with both structural awareness and textural richness. 3) Finally, to further boost the visual quality, we propose a Structure-Guided Refiner to compose the predicted conditions for more detailed generation of higher resolution. Extensive experiments demonstrate that our framework yields the state-of-the-art performance, generating hyper-realistic human images under diverse scenarios. Project Page: this https URL</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Visual Data-Type Understanding does not emerge from Scaling  Vision-Language Models</b></summary>
  <p><b>编号</b>：[11]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08577</p>
  <p><b>作者</b>：Vishaal Udandarao,  Max F. Burg,  Samuel Albanie,  Matthias Bethge</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：including impressive instances, yielding remarkable success, textit, Recent advances, including impressive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advances in the development of vision-language models (VLMs) are yielding remarkable success in recognizing visual semantic content, including impressive instances of compositional image understanding. Here, we introduce the novel task of \textit{Visual Data-Type Identification}, a basic perceptual skill with implications for data curation (e.g., noisy data-removal from large datasets, domain-specific retrieval) and autonomous vision (e.g., distinguishing changing weather conditions from camera lens staining). We develop two datasets consisting of animal images altered across a diverse set of 27 visual \textit{data-types}, spanning four broad categories. An extensive zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a nuanced performance landscape. While VLMs are reasonably good at identifying certain stylistic \textit{data-types}, such as cartoons and sketches, they struggle with simpler \textit{data-types} arising from basic manipulations like image rotations or additive noise. Our findings reveal that (i) model scaling alone yields marginal gains for contrastively-trained models like CLIP, and (ii) there is a pronounced drop in performance for the largest auto-regressively trained VLMs like OpenFlamingo. This finding points to a blind spot in current frontier VLMs: they excel in recognizing semantic content but fail to acquire an understanding of visual \textit{data-types} through scaling. By analyzing the pre-training distributions of these models and incorporating \textit{data-type} information into the captions during fine-tuning, we achieve a significant enhancement in performance. By exploring this previously uncharted task, we aim to set the stage for further advancing VLMs to equip them with visual data-type understanding. Code and datasets are released \href{this https URL}{here}.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Learning to Act from Actionless Videos through Dense Correspondences</b></summary>
  <p><b>编号</b>：[12]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08576</p>
  <p><b>作者</b>：Po-Chen Ko,  Jiayuan Mao,  Yilun Du,  Shao-Hua Sun,  Joshua B. Tenenbaum</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：reliably executing diverse, construct a video-based, capable of reliably, executing diverse tasks, video-based robot policy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we present an approach to construct a video-based robot policy capable of reliably executing diverse tasks across different robots and environments from few video demonstrations without using any action annotations. Our method leverages images as a task-agnostic representation, encoding both the state and action information, and text as a general representation for specifying robot goals. By synthesizing videos that ``hallucinate'' robot executing actions and in combination with dense correspondences between frames, our approach can infer the closed-formed action to execute to an environment without the need of any explicit action labels. This unique capability allows us to train the policy solely based on RGB videos and deploy learned policies to various robotic tasks. We demonstrate the efficacy of our approach in learning policies on table-top manipulation and navigation tasks. Additionally, we contribute an open-source framework for efficient video modeling, enabling the training of high-fidelity policy models with four GPUs within a single day.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic  Image Design and Generation</b></summary>
  <p><b>编号</b>：[27]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08541</p>
  <p><b>作者</b>：Zhengyuan Yang,  Jianfeng Wang,  Linjie Li,  Kevin Lin,  Chung-Ching Lin,  Zicheng Liu,  Lijuan Wang</p>
  <p><b>备注</b>：Project page at this https URL</p>
  <p><b>关键词</b>：iterative, iterative self-refinement, self-refinement, ideas, multimodal</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce ``Idea to Image,'' a system that enables multimodal iterative self-refinement with GPT-4V(ision) for automatic image design and generation. Humans can quickly identify the characteristics of different text-to-image (T2I) models via iterative explorations. This enables them to efficiently convert their high-level generation ideas into effective T2I prompts that can produce good images. We investigate if systems based on large multimodal models (LMMs) can develop analogous multimodal self-refinement abilities that enable exploring unknown models or environments via self-refining tries. Idea2Img cyclically generates revised T2I prompts to synthesize draft images, and provides directional feedback for prompt revision, both conditioned on its memory of the probed T2I model's characteristics. The iterative self-refinement brings Idea2Img various advantages over vanilla T2I models. Notably, Idea2Img can process input ideas with interleaved image-text sequences, follow ideas with design instructions, and generate images of better semantic and visual qualities. The user preference study validates the efficacy of multimodal iterative self-refinement on automatic image design and generation.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Image2PCI -- A Multitask Learning Framework for Estimating Pavement  Condition Indices Directly from Images</b></summary>
  <p><b>编号</b>：[29]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08538</p>
  <p><b>作者</b>：Neema Jakisa Owor,  Hang Du,  Abdulateef Daud,  Armstrong Aboah,  Yaw Adu-Gyamfi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Pavement Condition Index, Condition Index, evaluating pavement performance, pavement performance based, PCI</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The Pavement Condition Index (PCI) is a widely used metric for evaluating pavement performance based on the type, extent and severity of distresses detected on a pavement surface. In recent times, significant progress has been made in utilizing deep-learning approaches to automate PCI estimation process. However, the current approaches rely on at least two separate models to estimate PCI values -- one model dedicated to determining the type and extent and another for estimating their severity. This approach presents several challenges, including complexities, high computational resource demands, and maintenance burdens that necessitate careful consideration and resolution. To overcome these challenges, the current study develops a unified multi-tasking model that predicts the PCI directly from a top-down pavement image. The proposed architecture is a multi-task model composed of one encoder for feature extraction and four decoders to handle specific tasks: two detection heads, one segmentation head and one PCI estimation head. By multitasking, we are able to extract features from the detection and segmentation heads for automatically estimating the PCI directly from the images. The model performs very well on our benchmarked and open pavement distress dataset that is annotated for multitask learning (the first of its kind). To our best knowledge, this is the first work that can estimate PCI directly from an image at real time speeds while maintaining excellent accuracy on all related tasks for crack detection and segmentation.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：XAI Benchmark for Visual Explanation</b></summary>
  <p><b>编号</b>：[30]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08537</p>
  <p><b>作者</b>：Yifei Zhang,  Siyi Gu,  James Song,  Bo Pan,  Liang Zhao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：computer vision tasks, deep learning algorithms, black box, Explainable Artificial Intelligence, visual explanation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rise of deep learning algorithms has led to significant advancements in computer vision tasks, but their "black box" nature has raised concerns regarding interpretability. Explainable AI (XAI) has emerged as a critical area of research aiming to open this "black box", and shed light on the decision-making process of AI models. Visual explanations, as a subset of Explainable Artificial Intelligence (XAI), provide intuitive insights into the decision-making processes of AI models handling visual data by highlighting influential areas in an input image. Despite extensive research conducted on visual explanations, most evaluations are model-centered since the availability of corresponding real-world datasets with ground truth explanations is scarce in the context of image data. To bridge this gap, we introduce an XAI Benchmark comprising a dataset collection from diverse topics that provide both class labels and corresponding explanation annotations for images. We have processed data from diverse domains to align with our unified visual explanation framework. We introduce a comprehensive Visual Explanation pipeline, which integrates data loading, preprocessing, experimental setup, and model evaluation processes. This structure enables researchers to conduct fair comparisons of various visual explanation techniques. In addition, we provide a comprehensive review of over 10 evaluation methods for visual explanation to assist researchers in effectively utilizing our dataset collection. To further assess the performance of existing visual explanation methods, we conduct experiments on selected datasets using various model-centered and ground truth-centered evaluation metrics. We envision this benchmark could facilitate the advancement of visual explanation models. The XAI dataset collection and easy-to-use code for evaluation are publicly accessible at this https URL.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Animating Street View</b></summary>
  <p><b>编号</b>：[32]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08534</p>
  <p><b>作者</b>：Mengyi Shan,  Brian Curless,  Ira Kemelmacher-Shlizerman,  Steve Seitz</p>
  <p><b>备注</b>：SIGGRAPH Asia 2023 Conference Track</p>
  <p><b>关键词</b>：automatically brings street, brings street view, street view imagery, naturally behaving, animated pedestrians</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a system that automatically brings street view imagery to life by populating it with naturally behaving, animated pedestrians and vehicles. Our approach is to remove existing people and vehicles from the input image, insert moving objects with proper scale, angle, motion, and appearance, plan paths and traffic behavior, as well as render the scene with plausible occlusion and shadowing effects. The system achieves these by reconstructing the still image street scene, simulating crowd behavior, and rendering with consistent lighting, visibility, occlusions, and shadows. We demonstrate results on a diverse range of street scenes including regular still images and panoramas.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：UniPose: Detecting Any Keypoints</b></summary>
  <p><b>编号</b>：[34]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08530</p>
  <p><b>作者</b>：Jie Yang,  Ailing Zeng,  Ruimao Zhang,  Lei Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：keypoint detection, human and animal, work proposes, prompt-based keypoint detection, Keypoint</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This work proposes a unified framework called UniPose to detect keypoints of any articulated (e.g., human and animal), rigid, and soft objects via visual or textual prompts for fine-grained vision understanding and manipulation. Keypoint is a structure-aware, pixel-level, and compact representation of any object, especially articulated objects. Existing fine-grained promptable tasks mainly focus on object instance detection and segmentation but often fail to identify fine-grained granularity and structured information of image and instance, such as eyes, leg, paw, etc. Meanwhile, prompt-based keypoint detection is still under-explored. To bridge the gap, we make the first attempt to develop an end-to-end prompt-based keypoint detection framework called UniPose to detect keypoints of any objects. As keypoint detection tasks are unified in this framework, we can leverage 13 keypoint detection datasets with 338 keypoints across 1,237 categories over 400K instances to train a generic keypoint detection model. UniPose can effectively align text-to-keypoint and image-to-keypoint due to the mutual enhancement of textual and visual prompts based on the cross-modality contrastive learning optimization objectives. Our experimental results show that UniPose has strong fine-grained localization and generalization abilities across image styles, categories, and poses. Based on UniPose as a generalist keypoint detector, we hope it could serve fine-grained visual perception, understanding, and generation.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：GaussianDreamer: Fast Generation from Text to 3D Gaussian Splatting with  Point Cloud Priors</b></summary>
  <p><b>编号</b>：[35]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08529</p>
  <p><b>作者</b>：Taoran Yi,  Jiemin Fang,  Guanjun Wu,  Lingxi Xie,  Xiaopeng Zhang,  Wenyu Liu,  Qi Tian,  Xinggang Wang</p>
  <p><b>备注</b>：Work in progress. Project page: this https URL</p>
  <p><b>关键词</b>：shown impressive results, diffusion models, assets from text, impressive results, shown impressive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent times, the generation of 3D assets from text prompts has shown impressive results. Both 2D and 3D diffusion models can generate decent 3D objects based on prompts. 3D diffusion models have good 3D consistency, but their quality and generalization are limited as trainable 3D data is expensive and hard to obtain. 2D diffusion models enjoy strong abilities of generalization and fine generation, but the 3D consistency is hard to guarantee. This paper attempts to bridge the power from the two types of diffusion models via the recent explicit and efficient 3D Gaussian splatting representation. A fast 3D generation framework, named as \name, is proposed, where the 3D diffusion model provides point cloud priors for initialization and the 2D diffusion model enriches the geometry and appearance. Operations of noisy point growing and color perturbation are introduced to enhance the initialized Gaussians. Our \name can generate a high-quality 3D instance within 25 minutes on one GPU, much faster than previous methods, while the generated instances can be directly rendered in real time. Demos and code are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：4D Gaussian Splatting for Real-Time Dynamic Scene Rendering</b></summary>
  <p><b>编号</b>：[36]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08528</p>
  <p><b>作者</b>：Guanjun Wu,  Taoran Yi,  Jiemin Fang,  Lingxi Xie,  Xiaopeng Zhang,  Wei Wei,  Wenyu Liu,  Qi Tian,  Xinggang Wang</p>
  <p><b>备注</b>：Work in progress. Project page: this https URL</p>
  <p><b>关键词</b>：challenging task, important but challenging, Gaussian Splatting, rendering dynamic scenes, dynamic scene rendering</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Representing and rendering dynamic scenes has been an important but challenging task. Especially, to accurately model complex motions, high efficiency is usually hard to maintain. We introduce the 4D Gaussian Splatting (4D-GS) to achieve real-time dynamic scene rendering while also enjoying high training and storage efficiency. An efficient deformation field is constructed to model both Gaussian motions and shape deformations. Different adjacent Gaussians are connected via a HexPlane to produce more accurate position and shape deformations. Our 4D-GS method achieves real-time rendering under high resolutions, 70 FPS at a 800$\times$800 resolution on an RTX 3090 GPU, while maintaining comparable or higher quality than previous state-of-the-art methods. More demos and code are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Unsupervised Learning of Object-Centric Embeddings for Cell Instance  Segmentation in Microscopy Images</b></summary>
  <p><b>编号</b>：[42]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08501</p>
  <p><b>作者</b>：Steffen Wolf,  Manan Lalit,  Henry Westmacott,  Katie McDole,  Jan Funke</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：biomedical applications, microscopy images, image patches, method, embed image patches</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Segmentation of objects in microscopy images is required for many biomedical applications. We introduce object-centric embeddings (OCEs), which embed image patches such that the spatial offsets between patches cropped from the same object are preserved. Those learnt embeddings can be used to delineate individual objects and thus obtain instance segmentations. Here, we show theoretically that, under assumptions commonly found in microscopy images, OCEs can be learnt through a self-supervised task that predicts the spatial offset between image patches. Together, this forms an unsupervised cell instance segmentation method which we evaluate on nine diverse large-scale microscopy datasets. Segmentations obtained with our method lead to substantially improved results, compared to state-of-the-art baselines on six out of nine datasets, and perform on par on the remaining three datasets. If ground-truth annotations are available, our method serves as an excellent starting point for supervised training, reducing the required amount of ground-truth needed by one order of magnitude, thus substantially increasing the practical applicability of our method. Source code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Can We Edit Multimodal Large Language Models?</b></summary>
  <p><b>编号</b>：[50]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08475</p>
  <p><b>作者</b>：Siyuan Cheng,  Bozhong Tian,  Qingbin Liu,  Xi Chen,  Yongheng Wang,  Huajun Chen,  Ningyu Zhang</p>
  <p><b>备注</b>：EMNLP 2023</p>
  <p><b>关键词</b>：Large Language Models, Multimodal Large Language, Large Language, editing Multimodal Large, Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in this https URL.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：WinSyn: A High Resolution Testbed for Synthetic Data</b></summary>
  <p><b>编号</b>：[52]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08471</p>
  <p><b>作者</b>：Tom Kelly,  John Femiani,  Peter Wonka</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：high-resolution photographs, synthetic data, synthetic data generation, synthetic, data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present WinSyn, a dataset consisting of high-resolution photographs and renderings of 3D models as a testbed for synthetic-to-real research. The dataset consists of 75,739 high-resolution photographs of building windows, including traditional and modern designs, captured globally. These include 89,318 cropped subimages of windows, of which 9,002 are semantically labeled. Further, we present our domain-matched photorealistic procedural model which enables experimentation over a variety of parameter distributions and engineering approaches. Our procedural model provides a second corresponding dataset of 21,290 synthetic images. This jointly developed dataset is designed to facilitate research in the field of synthetic-to-real learning and synthetic data generation. WinSyn allows experimentation into the factors that make it challenging for synthetic data to compete with real-world data. We perform ablations using our synthetic model to identify the salient rendering, materials, and geometric factors pertinent to accuracy within the labeling task. We chose windows as a benchmark because they exhibit a large variability of geometry and materials in their design, making them ideal to study synthetic data generation in a constrained setting. We argue that the dataset is a crucial step to enable future research in synthetic data generation for deep learning.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：MotionDirector: Motion Customization of Text-to-Video Diffusion Models</b></summary>
  <p><b>编号</b>：[54]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08465</p>
  <p><b>作者</b>：Rui Zhao,  Yuchao Gu,  Jay Zhangjie Wu,  David Junhao Zhang,  Jiawei Liu,  Weijia Wu,  Jussi Keppo,  Mike Zheng Shou</p>
  <p><b>备注</b>：Project Page: this https URL</p>
  <p><b>关键词</b>：exhibited remarkable capabilities, Large-scale pre-trained diffusion, pre-trained diffusion models, motion, Large-scale pre-trained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large-scale pre-trained diffusion models have exhibited remarkable capabilities in diverse video generations. Given a set of video clips of the same motion concept, the task of Motion Customization is to adapt existing text-to-video diffusion models to generate videos with this motion. For example, generating a video with a car moving in a prescribed manner under specific camera movements to make a movie, or a video illustrating how a bear would lift weights to inspire creators. Adaptation methods have been developed for customizing appearance like subject or style, yet unexplored for motion. It is straightforward to extend mainstream adaption methods for motion customization, including full model tuning, parameter-efficient tuning of additional layers, and Low-Rank Adaptions (LoRAs). However, the motion concept learned by these methods is often coupled with the limited appearances in the training videos, making it difficult to generalize the customized motion to other appearances. To overcome this challenge, we propose MotionDirector, with a dual-path LoRAs architecture to decouple the learning of appearance and motion. Further, we design a novel appearance-debiased temporal loss to mitigate the influence of appearance on the temporal training objective. Experimental results show the proposed method can generate videos of diverse appearances for the customized motions. Our method also supports various downstream applications, such as the mixing of different videos with their appearance and motion respectively, and animating a single image with customized motions. Our code and model weights will be released.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Proving the Potential of Skeleton Based Action Recognition to Automate  the Analysis of Manual Processes</b></summary>
  <p><b>编号</b>：[60]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08451</p>
  <p><b>作者</b>：Marlin Berger,  Frederik Cloppenburg,  Jens Eufinger,  Thomas Gries</p>
  <p><b>备注</b>：16 pages, 6 figures. Find peer-reviewed version in Proceedings of IntelliSys 2023</p>
  <p><b>关键词</b>：textiles and electronics, manual processes, manufacturing sectors, fundamental part, manual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In manufacturing sectors such as textiles and electronics, manual processes are a fundamental part of production. The analysis and monitoring of the processes is necessary for efficient production design. Traditional methods for analyzing manual processes are complex, expensive, and inflexible. Compared to established approaches such as Methods-Time-Measurement (MTM), machine learning (ML) methods promise: Higher flexibility, self-sufficient & permanent use, lower costs. In this work, based on a video stream, the current motion class in a manual assembly process is detected. With information on the current motion, Key-Performance-Indicators (KPIs) can be derived easily. A skeleton-based action recognition approach is taken, as this field recently shows major success in machine vision tasks. For skeleton-based action recognition in manual assembly, no sufficient pre-work could be found. Therefore, a ML pipeline is developed, to enable extensive research on different (pre-) processing methods and neural nets. Suitable well generalizing approaches are found, proving the potential of ML to enhance analyzation of manual processes. Models detect the current motion, performed by an operator in manual assembly, but the results can be transferred to all kinds of manual processes.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Debias the Training of Diffusion Models</b></summary>
  <p><b>编号</b>：[66]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08442</p>
  <p><b>作者</b>：Hu Yu,  Li Shen,  Jie Huang,  Man Zhou,  Hongsheng Li,  Feng Zhao</p>
  <p><b>备注</b>：University of Science and Technology of China, Alibaba Group, The Chinese University of Hong Kong</p>
  <p><b>关键词</b>：demonstrated compelling generation, variational lower bound, score matching loss, simple denoising score, denoising score matching</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diffusion models have demonstrated compelling generation quality by optimizing the variational lower bound through a simple denoising score matching loss. In this paper, we provide theoretical evidence that the prevailing practice of using a constant loss weight strategy in diffusion models leads to biased estimation during the training phase. Simply optimizing the denoising network to predict Gaussian noise with constant weighting may hinder precise estimations of original images. To address the issue, we propose an elegant and effective weighting strategy grounded in the theoretically unbiased principle. Moreover, we conduct a comprehensive and systematic exploration to dissect the inherent bias problem deriving from constant weighting loss from the perspectives of its existence, impact and reasons. These analyses are expected to advance our understanding and demystify the inner workings of diffusion models. Through empirical evaluation, we demonstrate that our proposed debiased estimation method significantly enhances sample quality without the reliance on complex techniques, and exhibits improved efficiency compared to the baseline method both in training and sampling processes.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Assessing of Soil Erosion Risk Through Geoinformation Sciences and  Remote Sensing -- A Review</b></summary>
  <p><b>编号</b>：[71]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08430</p>
  <p><b>作者</b>：Lachezar Filchev,  Vasil Kolev</p>
  <p><b>备注</b>：Chapter 21 (pages 54)</p>
  <p><b>关键词</b>：Soil Loss Equation, widespread erosion phenomena, Universal Soil Loss, soil erosion risk, Land Degradation Neutrality</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>During past decades a marked manifestation of widespread erosion phenomena was studied worldwide. Global conservation community has launched campaigns at local, regional and continental level in developing countries for preservation of soil resources in order not only to stop or mitigate human impact on nature but also to improve life in rural areas introducing new approaches for soil cultivation. After the adoption of Sustainable Development Goals of UNs and launching several world initiatives such as the Land Degradation Neutrality (LDN) the world came to realize the very importance of the soil resources on which the biosphere relies for its existence. The main goal of the chapter is to review different types and structures erosion models as well as their applications. Several methods using spatial analysis capabilities of geographic information systems (GIS) are in operation for soil erosion risk assessment, such as Universal Soil Loss Equation (USLE), Revised Universal Soil Loss Equation (RUSLE) in operation worldwide and in the USA and MESALES model. These and more models are being discussed in the present work alongside more experimental models and methods for assessing soil erosion risk such as Artificial Intelligence (AI), Machine and Deep Learning, etc. At the end of this work, a prospectus for the future development of soil erosion risk assessment is drawn.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Revisiting Data Augmentation for Rotational Invariance in Convolutional  Neural Networks</b></summary>
  <p><b>编号</b>：[72]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08429</p>
  <p><b>作者</b>：Facundo Manuel Quiroga,  Franco Ronchetti,  Laura Lanzarini,  Aurelio Fernandez-Bariviera</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Convolutional Neural Networks, computer vision tasks, Convolutional Neural, Neural Networks, offer state</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Convolutional Neural Networks (CNN) offer state of the art performance in various computer vision tasks. Many of those tasks require different subtypes of affine invariances (scale, rotational, translational) to image transformations. Convolutional layers are translation equivariant by design, but in their basic form lack invariances. In this work we investigate how best to include rotational invariance in a CNN for image classification. Our experiments show that networks trained with data augmentation alone can classify rotated images nearly as well as in the normal unrotated case; this increase in representational power comes only at the cost of training time. We also compare data augmentation versus two modified CNN models for achieving rotational invariance or equivariance, Spatial Transformer Networks and Group Equivariant CNNs, finding no significant accuracy increase with these specialized methods. In the case of data augmented networks, we also analyze which layers help the network to encode the rotational invariance, which is important for understanding its limitations and how to best retrain a network with data augmentation to achieve invariance to rotation.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题："SegLoc": Study on Novel Visual Self-supervised Learning Scheme (Segment  Localization) Tailored for Dense Prediction Tasks of Security Inspection  X-ray Images</b></summary>
  <p><b>编号</b>：[74]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08421</p>
  <p><b>作者</b>：Shervin Halat,  Mohammad Rahmati,  Ehsan Nazerfard</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：self-supervised learning scheme, SSL models, remarkable advancements, advancements of artificial, artificial intelligence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Lately, remarkable advancements of artificial intelligence have been attributed to the integration of self-supervised learning scheme. Despite impressive achievements within NLP, yet SSL in computer vision has not been able to stay on track comparatively. Recently, integration of contrastive learning on top of existing SSL models has established considerable progress in computer vision through which visual SSL models have outperformed their supervised counterparts. Nevertheless, most of these improvements were limited to classification tasks, and also, few works have been dedicated to evaluation of SSL models in real-world scenarios of computer vision, while the majority of works are centered around datasets containing class-wise portrait images, most notably, ImageNet. Consequently, in this work, we have considered dense prediction task of semantic segmentation in security inspection x-ray images to evaluate our proposed model Segmentation Localization. Based upon the model Instance Localization, our model SegLoc has managed to address one of the most challenging downsides of contrastive learning, i.e., false negative pairs of query embeddings. In order to do so, in contrast to baseline model InsLoc, our pretraining dataset is synthesized by cropping, transforming, then pasting already labeled segments from an available labeled dataset, foregrounds, onto instances of an unlabeled dataset, backgrounds. In our case, PIDray and SIXray datasets are considered as labeled and unlabeled datasets, respectively. Moreover, we fully harness labels by avoiding false negative pairs through implementing the idea, one queue per class, in MoCo-v2 whereby negative pairs corresponding to each query are extracted from its corresponding queue within the memory bank. Our approach has outperformed random initialization by 3% to 6%, while having underperformed supervised initialization.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Visual Attention-Prompted Prediction and Learning</b></summary>
  <p><b>编号</b>：[75]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08420</p>
  <p><b>作者</b>：Yifei Zhang,  Siyi Gu,  Bo Pan,  Guangji Bai,  Xiaofeng Yang,  Liang Zhao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：incorporating human understanding, visual attention prompt, model predictive power, visual attention, attention</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Explanation(attention)-guided learning is a method that enhances a model's predictive power by incorporating human understanding during the training phase. While attention-guided learning has shown promising results, it often involves time-consuming and computationally expensive model retraining. To address this issue, we introduce the attention-prompted prediction technique, which enables direct prediction guided by the attention prompt without the need for model retraining. However, this approach presents several challenges, including: 1) How to incorporate the visual attention prompt into the model's decision-making process and leverage it for future predictions even in the absence of a prompt? and 2) How to handle the incomplete information from the visual attention prompt? To tackle these challenges, we propose a novel framework called Visual Attention-Prompted Prediction and Learning, which seamlessly integrates visual attention prompts into the model's decision-making process and adapts to images both with and without attention prompts for prediction. To address the incomplete information of the visual attention prompt, we introduce a perturbation-based attention map modification method. Additionally, we propose an optimization-based mask aggregation method with a new weight learning function for adaptive perturbed annotation aggregation in the attention map modification process. Our overall framework is designed to learn in an attention-prompt guided multi-task manner to enhance future predictions even for samples without attention prompts and trained in an alternating manner for better convergence. Extensive experiments conducted on two datasets demonstrate the effectiveness of our proposed framework in enhancing predictions for samples, both with and without provided prompts.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Towards Design and Development of an ArUco Markers-Based Quantitative  Surface Tactile Sensor</b></summary>
  <p><b>编号</b>：[83]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08398</p>
  <p><b>作者</b>：Ozdemir Can Kara,  Charles Everson,  Farshid Alambeigi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Vision-based Tactile Sensor, Surface Tactile Sensor, Quantitative Surface Tactile, qualitative image outputs, Vision-based Tactile</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, with the goal of quantifying the qualitative image outputs of a Vision-based Tactile Sensor (VTS), we present the design, fabrication, and characterization of a novel Quantitative Surface Tactile Sensor (called QS-TS). QS-TS directly estimates the sensor's gel layer deformation in real-time enabling safe and autonomous tactile manipulation and servoing of delicate objects using robotic manipulators. The core of the proposed sensor is the utilization of miniature 1.5 mm x 1.5 mm synthetic square markers with inner binary patterns and a broad black border, called ArUco Markers. Each ArUco marker can provide real-time camera pose estimation that, in our design, is used as a quantitative measure for obtaining deformation of the QS-TS gel layer. Moreover, thanks to the use of ArUco markers, we propose a unique fabrication procedure that mitigates various challenges associated with the fabrication of the existing marker-based VTSs and offers an intuitive and less-arduous method for the construction of the VTS. Remarkably, the proposed fabrication facilitates the integration and adherence of markers with the gel layer to robustly and reliably obtain a quantitative measure of deformation in real-time regardless of the orientation of ArUco Markers. The performance and efficacy of the proposed QS-TS in estimating the deformation of the sensor's gel layer were experimentally evaluated and verified. Results demonstrate the phenomenal performance of the QS-TS in estimating the deformation of the gel layer with a relative error of <5%.< p>
  </5%.<></p></details>
</details>
<details>
  <summary>29. <b>标题：Hyp-UML: Hyperbolic Image Retrieval with Uncertainty-aware Metric  Learning</b></summary>
  <p><b>编号</b>：[88]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08390</p>
  <p><b>作者</b>：Shiyang Yan,  Zongxuan Liu,  Lin Xu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Metric learning, uncertainty-aware metric learning, learning, role in training, Metric</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Metric learning plays a critical role in training image retrieval and classification. It is also a key algorithm in representation learning, e.g., for feature learning and its alignment in metric space. Hyperbolic embedding has been recently developed, compared to the conventional Euclidean embedding in most of the previously developed models, and can be more effective in representing the hierarchical data structure. Second, uncertainty estimation/measurement is a long-lasting challenge in artificial intelligence. Successful uncertainty estimation can improve a machine learning model's performance, robustness, and security. In Hyperbolic space, uncertainty measurement is at least with equivalent, if not more, critical importance. In this paper, we develop a Hyperbolic image embedding with uncertainty-aware metric learning for image retrieval. We call our method Hyp-UML: Hyperbolic Uncertainty-aware Metric Learning. Our contribution are threefold: we propose an image embedding algorithm based on Hyperbolic space, with their corresponding uncertainty value; we propose two types of uncertainty-aware metric learning, for the popular Contrastive learning and conventional margin-based metric learning, respectively. We perform extensive experimental validations to prove that the proposed algorithm can achieve state-of-the-art results among related methods. The comprehensive ablation study validates the effectiveness of each component of the proposed algorithm.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：MeanAP-Guided Reinforced Active Learning for Object Detection</b></summary>
  <p><b>编号</b>：[89]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08387</p>
  <p><b>作者</b>：Zhixuan Liang,  Xingyu Zeng,  Rui Zhao,  Ping Luo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：minimal labeled data, Active learning presents, Active learning, Reinforced Active Learning, active object detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Active learning presents a promising avenue for training high-performance models with minimal labeled data, achieved by judiciously selecting the most informative instances to label and incorporating them into the task learner. Despite notable advancements in active learning for image recognition, metrics devised or learned to gauge the information gain of data, crucial for query strategy design, do not consistently align with task model performance metrics, such as Mean Average Precision (MeanAP) in object detection tasks. This paper introduces MeanAP-Guided Reinforced Active Learning for Object Detection (MAGRAL), a novel approach that directly utilizes the MeanAP metric of the task model to devise a sampling strategy employing a reinforcement learning-based sampling agent. Built upon LSTM architecture, the agent efficiently explores and selects subsequent training instances, and optimizes the process through policy gradient with MeanAP serving as reward. Recognizing the time-intensive nature of MeanAP computation at each step, we propose fast look-up tables to expedite agent training. We assess MAGRAL's efficacy across popular benchmarks, PASCAL VOC and MS COCO, utilizing different backbone architectures. Empirical findings substantiate MAGRAL's superiority over recent state-of-the-art methods, showcasing substantial performance gains. MAGRAL establishes a robust baseline for reinforced active object detection, signifying its potential in advancing the field.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：AutoVP: An Automated Visual Prompting Framework and Benchmark</b></summary>
  <p><b>编号</b>：[92]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08381</p>
  <p><b>作者</b>：Hsi-Ai Tsao,  Lei Hsiung,  Pin-Yu Chen,  Sijia Liu,  Tsung-Yi Ho</p>
  <p><b>备注</b>：Preprint. The code is available at this https URL</p>
  <p><b>关键词</b>：emerging parameter-efficient fine-tuning, parameter-efficient fine-tuning approach, adapting pre-trained vision, downstream image-classification tasks, Visual prompting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Visual prompting (VP) is an emerging parameter-efficient fine-tuning approach to adapting pre-trained vision models to solve various downstream image-classification tasks. However, there has hitherto been little systematic study of the design space of VP and no clear benchmark for evaluating its performance. To bridge this gap, we propose AutoVP, an end-to-end expandable framework for automating VP design choices, along with 12 downstream image-classification tasks that can serve as a holistic VP-performance benchmark. Our design space covers 1) the joint optimization of the prompts; 2) the selection of pre-trained models, including image classifiers and text-image encoders; and 3) model output mapping strategies, including nonparametric and trainable label mapping. Our extensive experimental results show that AutoVP outperforms the best-known current VP methods by a substantial margin, having up to 6.7% improvement in accuracy; and attains a maximum performance increase of 27.5% compared to linear-probing (LP) baseline. AutoVP thus makes a two-fold contribution: serving both as an efficient tool for hyperparameter tuning on VP design choices, and as a comprehensive benchmark that can reasonably be expected to accelerate VP's development. The source code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Worst-Case Morphs using Wasserstein ALI and Improved MIPGAN</b></summary>
  <p><b>编号</b>：[96]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08371</p>
  <p><b>作者</b>：Una M. Kelly,  Meike Nauta,  Lu Liu,  Luuk J. Spreeuwers,  Raymond N. J. Veldhuis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Generative Adversarial Networks, biometric Face Recognition, Face Recognition, Morphs, Adversarial Networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A morph is a combination of two separate facial images and contains identity information of two different people. When used in an identity document, both people can be authenticated by a biometric Face Recognition (FR) system. Morphs can be generated using either a landmark-based approach or approaches based on deep learning such as Generative Adversarial Networks (GAN). In a recent paper, we introduced a \emph{worst-case} upper bound on how challenging morphing attacks can be for an FR system. The closer morphs are to this upper bound, the bigger the challenge they pose to FR. We introduced an approach with which it was possible to generate morphs that approximate this upper bound for a known FR system (white box), but not for unknown (black box) FR systems.
In this paper, we introduce a morph generation method that can approximate worst-case morphs even when the FR system is not known. A key contribution is that we include the goal of generating difficult morphs \emph{during} training. Our method is based on Adversarially Learned Inference (ALI) and uses concepts from Wasserstein GANs trained with Gradient Penalty, which were introduced to stabilise the training of GANs. We include these concepts to achieve similar improvement in training stability and call the resulting method Wasserstein ALI (WALI). We finetune WALI using loss functions designed specifically to improve the ability to manipulate identity information in facial images and show how it can generate morphs that are more challenging for FR systems than landmark- or GAN-based morphs. We also show how our findings can be used to improve MIPGAN, an existing StyleGAN-based morph generator.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：UniPAD: A Universal Pre-training Paradigm for Autonomous Driving</b></summary>
  <p><b>编号</b>：[97]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08370</p>
  <p><b>作者</b>：Honghui Yang,  Sha Zhang,  Di Huang,  Xiaoyang Wu,  Haoyi Zhu,  Tong He,  Shixiang Tang,  Hengshuang Zhao,  Qibo Qiu,  Binbin Lin,  Xiaofei He,  Wanli Ouyang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：effective feature learning, autonomous driving, widely acknowledged, context of autonomous, significance of effective</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the context of autonomous driving, the significance of effective feature learning is widely acknowledged. While conventional 3D self-supervised pre-training methods have shown widespread success, most methods follow the ideas originally designed for 2D images. In this paper, we present UniPAD, a novel self-supervised learning paradigm applying 3D volumetric differentiable rendering. UniPAD implicitly encodes 3D space, facilitating the reconstruction of continuous 3D shape structures and the intricate appearance characteristics of their 2D projections. The flexibility of our method enables seamless integration into both 2D and 3D frameworks, enabling a more holistic comprehension of the scenes. We manifest the feasibility and effectiveness of UniPAD by conducting extensive experiments on various downstream 3D tasks. Our method significantly improves lidar-, camera-, and lidar-camera-based baseline by 9.1, 7.7, and 6.9 NDS, respectively. Notably, our pre-training pipeline achieves 73.2 NDS for 3D object detection and 79.4 mIoU for 3D semantic segmentation on the nuScenes validation set, achieving state-of-the-art results in comparison with previous methods. The code will be available at this https URL.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Mapping Memes to Words for Multimodal Hateful Meme Classification</b></summary>
  <p><b>编号</b>：[99]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08368</p>
  <p><b>作者</b>：Giovanni Burbi,  Alberto Baldrati,  Lorenzo Agnolucci,  Marco Bertini,  Alberto Del Bimbo</p>
  <p><b>备注</b>：ICCV2023 CLVL Workshop</p>
  <p><b>关键词</b>：convey humor, unique form, form of communication, communication that combines, combines visual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multimodal image-text memes are prevalent on the internet, serving as a unique form of communication that combines visual and textual elements to convey humor, ideas, or emotions. However, some memes take a malicious turn, promoting hateful content and perpetuating discrimination. Detecting hateful memes within this multimodal context is a challenging task that requires understanding the intertwined meaning of text and images. In this work, we address this issue by proposing a novel approach named ISSUES for multimodal hateful meme classification. ISSUES leverages a pre-trained CLIP vision-language model and the textual inversion technique to effectively capture the multimodal semantic content of the memes. The experiments show that our method achieves state-of-the-art results on the Hateful Memes Challenge and HarMeme datasets. The code and the pre-trained models are publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：MCU: A Task-centric Framework for Open-ended Agent Evaluation in  Minecraft</b></summary>
  <p><b>编号</b>：[100]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08367</p>
  <p><b>作者</b>：Haowei Lin,  Zihao Wang,  Jianzhu Ma,  Yitao Liang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：framework named MCU, task-centric framework named, MCU framework, open-ended game environment, MCU</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To pursue the goal of creating an open-ended agent in Minecraft, an open-ended game environment with unlimited possibilities, this paper introduces a task-centric framework named MCU for Minecraft agent evaluation. The MCU framework leverages the concept of atom tasks as fundamental building blocks, enabling the generation of diverse or even arbitrary tasks. Within the MCU framework, each task is measured with six distinct difficulty scores (time consumption, operational effort, planning complexity, intricacy, creativity, novelty). These scores offer a multi-dimensional assessment of a task from different angles, and thus can reveal an agent's capability on specific facets. The difficulty scores also serve as the feature of each task, which creates a meaningful task space and unveils the relationship between tasks. For efficient evaluation of Minecraft agents employing the MCU framework, we maintain a unified benchmark, namely SkillForge, which comprises representative tasks with diverse categories and difficulty distribution. We also provide convenient filters for users to select tasks to assess specific capabilities of agents. We show that MCU has the high expressivity to cover all tasks used in recent literature on Minecraft agent, and underscores the need for advancements in areas such as creativity, precise control, and out-of-distribution generalization under the goal of open-ended Minecraft agent development.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：A Generic Software Framework for Distributed Topological Analysis  Pipelines</b></summary>
  <p><b>编号</b>：[113]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08339</p>
  <p><b>作者</b>：Eve Le Guillou,  Michael Will,  Pierre Guillou,  Jonas Lukasczyk,  Pierre Fortin,  Christoph Garth,  Julien Tierny</p>
  <p><b>备注</b>：18 pages, 12 figures</p>
  <p><b>关键词</b>：system paper presents, topological analysis pipelines, topological analysis, analysis pipelines, TTK</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This system paper presents a software framework for the support of topological analysis pipelines in a distributed-memory model. While several recent papers introduced topology-based approaches for distributed-memory environments, these were reporting experiments obtained with tailored, mono-algorithm implementations. In contrast, we describe in this paper a general-purpose, generic framework for topological analysis pipelines, i.e. a sequence of topological algorithms interacting together, possibly on distinct numbers of processes. Specifically, we instantiated our framework with the MPI model, within the Topology ToolKit (TTK). While developing this framework, we faced several algorithmic and software engineering challenges, which we document in this paper. We provide a taxonomy for the distributed-memory topological algorithms supported by TTK, depending on their communication needs and provide examples of hybrid MPI+thread parallelizations. Detailed performance analyses show that parallel efficiencies range from $20\%$ to $80\%$ (depending on the algorithms), and that the MPI-specific preconditioning introduced by our framework induces a negligible computation time overhead. We illustrate the new distributed-memory capabilities of TTK with an example of advanced analysis pipeline, combining multiple algorithms, run on the largest publicly available dataset we have found (120 billion vertices) on a standard cluster with 64 nodes (for a total of 1,536 cores). Finally, we provide a roadmap for the completion of TTK's MPI extension, along with generic recommendations for each algorithm communication category.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Real-Time Neural BRDF with Spherically Distributed Primitives</b></summary>
  <p><b>编号</b>：[116]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08332</p>
  <p><b>作者</b>：Yishun Dou,  Zhong Zheng,  Qiaoqiao Jin,  Bingbing Ni,  Yugang Chen,  Junxiang Ke</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：offering highly versatile, BRDF offering highly, achieving real-time rendering, neural computation consumption, versatile material representation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a novel compact and efficient neural BRDF offering highly versatile material representation, yet with very-light memory and neural computation consumption towards achieving real-time rendering. The results in Figure 1, rendered at full HD resolution on a current desktop machine, show that our system achieves real-time rendering with a wide variety of appearances, which is approached by the following two designs. On the one hand, noting that bidirectional reflectance is distributed in a very sparse high-dimensional subspace, we propose to project the BRDF into two low-dimensional components, i.e., two hemisphere feature-grids for incoming and outgoing directions, respectively. On the other hand, learnable neural reflectance primitives are distributed on our highly-tailored spherical surface grid, which offer informative features for each component and alleviate the conventional heavy feature learning network to a much smaller one, leading to very fast evaluation. These primitives are centrally stored in a codebook and can be shared across multiple grids and even across materials, based on the low-cost indices stored in material-specific spherical surface grids. Our neural BRDF, which is agnostic to the material, provides a unified framework that can represent a variety of materials in consistent manner. Comprehensive experimental results on measured BRDF compression, Monte Carlo simulated BRDF acceleration, and extension to spatially varying effect demonstrate the superior quality and generalizability achieved by the proposed scheme.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：NSM4D: Neural Scene Model Based Online 4D Point Cloud Sequence  Understanding</b></summary>
  <p><b>编号</b>：[119]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08326</p>
  <p><b>作者</b>：Yuhao Dong,  Zhuoyang Zhang,  Yunze Liu,  Li Yi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：point cloud sequences, point cloud, autonomous driving, redundant point cloud, neural scene model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Understanding 4D point cloud sequences online is of significant practical value in various scenarios such as VR/AR, robotics, and autonomous driving. The key goal is to continuously analyze the geometry and dynamics of a 3D scene as unstructured and redundant point cloud sequences arrive. And the main challenge is to effectively model the long-term history while keeping computational costs manageable. To tackle these challenges, we introduce a generic online 4D perception paradigm called NSM4D. NSM4D serves as a plug-and-play strategy that can be adapted to existing 4D backbones, significantly enhancing their online perception capabilities for both indoor and outdoor scenarios. To efficiently capture the redundant 4D history, we propose a neural scene model that factorizes geometry and motion information by constructing geometry tokens separately storing geometry and motion features. Exploiting the history becomes as straightforward as querying the neural scene model. As the sequence progresses, the neural scene model dynamically deforms to align with new observations, effectively providing the historical context and updating itself with the new observations. By employing token representation, NSM4D also exhibits robustness to low-level sensor noise and maintains a compact size through a geometric sampling scheme. We integrate NSM4D with state-of-the-art 4D perception backbones, demonstrating significant improvements on various online perception benchmarks in indoor and outdoor settings. Notably, we achieve a 9.6% accuracy improvement for HOI4D online action segmentation and a 3.4% mIoU improvement for SemanticKITTI online semantic segmentation. Furthermore, we show that NSM4D inherently offers excellent scalability to longer sequences beyond the training set, which is crucial for real-world applications.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：Defending Our Privacy With Backdoors</b></summary>
  <p><b>编号</b>：[121]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08320</p>
  <p><b>作者</b>：Dominik Hintersdorf,  Lukas Struppek,  Daniel Neider,  Kristian Kersting</p>
  <p><b>备注</b>：14 pages, 4 figures</p>
  <p><b>关键词</b>：raised significant privacy, significant privacy concerns, proliferation of large, raised significant, web-scraped data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The proliferation of large AI models trained on uncurated, often sensitive web-scraped data has raised significant privacy concerns. One of the concerns is that adversaries can extract information about the training data using privacy attacks. Unfortunately, the task of removing specific information from the models without sacrificing performance is not straightforward and has proven to be challenging. We propose a rather easy yet effective defense based on backdoor attacks to remove private information such as names of individuals from models, and focus in this work on text encoders. Specifically, through strategic insertion of backdoors, we align the embeddings of sensitive phrases with those of neutral terms-"a person" instead of the person's name. Our empirical results demonstrate the effectiveness of our backdoor-based defense on CLIP by assessing its performance using a specialized privacy attack for zero-shot classifiers. Our approach provides not only a new "dual-use" perspective on backdoor attacks, but also presents a promising avenue to enhance the privacy of individuals within models trained on uncurated web-scraped data.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Extended target tracking utilizing machine-learning software -- with  applications to animal classification</b></summary>
  <p><b>编号</b>：[123]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08316</p>
  <p><b>作者</b>：Magnus Malmström,  Anton Kullberg,  Isaac Skog,  Daniel Axehill,  Fredrik Gustafsson</p>
  <p><b>备注</b>：5 pages, 3 figures</p>
  <p><b>关键词</b>：object-detection algorithm outputs, object-detection algorithm, tracking objects, problem of detecting, object-detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper considers the problem of detecting and tracking objects in a sequence of images. The problem is formulated in a filtering framework, using the output of object-detection algorithms as measurements. An extension to the filtering formulation is proposed that incorporates class information from the previous frame to robustify the classification, even if the object-detection algorithm outputs an incorrect prediction. Further, the properties of the object-detection algorithm are exploited to quantify the uncertainty of the bounding box detection in each frame. The complete filtering method is evaluated on camera trap images of the four large Swedish carnivores, bear, lynx, wolf, and wolverine. The experiments show that the class tracking formulation leads to a more robust classification.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：GePSAn: Generative Procedure Step Anticipation in Cooking Videos</b></summary>
  <p><b>编号</b>：[125]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08312</p>
  <p><b>作者</b>：Mohamed Ashraf Abdelsalam,  Samrudhdhi B. Rangrej,  Isma Hadji,  Nikita Dvornik,  Konstantinos G. Derpanis,  Afsaneh Fazly</p>
  <p><b>备注</b>：published at ICCV 2023</p>
  <p><b>关键词</b>：step, video, future step, future, model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the problem of future step anticipation in procedural videos. Given a video of an ongoing procedural activity, we predict a plausible next procedure step described in rich natural language. While most previous work focus on the problem of data scarcity in procedural video datasets, another core challenge of future anticipation is how to account for multiple plausible future realizations in natural settings. This problem has been largely overlooked in previous work. To address this challenge, we frame future step prediction as modelling the distribution of all possible candidates for the next step. Specifically, we design a generative model that takes a series of video clips as input, and generates multiple plausible and diverse candidates (in natural language) for the next step. Following previous work, we side-step the video annotation scarcity by pretraining our model on a large text-based corpus of procedural activities, and then transfer the model to the video domain. Our experiments, both in textual and video domains, show that our model captures diversity in the next step prediction and generates multiple plausible future predictions. Moreover, our model establishes new state-of-the-art results on YouCookII, where it outperforms existing baselines on the next step anticipation. Finally, we also show that our model can successfully transfer from text to the video domain zero-shot, ie, without fine-tuning or adaptation, and produces good-quality future step predictions from video.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：CHIP: Contrastive Hierarchical Image Pretraining</b></summary>
  <p><b>编号</b>：[128]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08304</p>
  <p><b>作者</b>：Arpit Mittal,  Harshil Jhaveri,  Swapnil Mallick,  Abhishek Ajmera</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：limited number, Few-shot object classification, few-shot classification model, few-shot classification, model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Few-shot object classification is the task of classifying objects in an image with limited number of examples as supervision. We propose a one-shot/few-shot classification model that can classify an object of any unseen class into a relatively general category in an hierarchically based classification. Our model uses a three-level hierarchical contrastive loss based ResNet152 classifier for classifying an object based on its features extracted from Image embedding, not used during the training phase. For our experimentation, we have used a subset of the ImageNet (ILSVRC-12) dataset that contains only the animal classes for training our model and created our own dataset of unseen classes for evaluating our trained model. Our model provides satisfactory results in classifying the unknown objects into a generic category which has been later discussed in greater detail.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：Multimodal Variational Auto-encoder based Audio-Visual Segmentation</b></summary>
  <p><b>编号</b>：[129]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08303</p>
  <p><b>作者</b>：Yuxin Mao,  Jing Zhang,  Mochu Xiang,  Yiran Zhong,  Yuchao Dai</p>
  <p><b>备注</b>：Accepted by ICCV2023,Project page(https://npucvr.github.io/MMVAE-AVS),Code(this https URL)</p>
  <p><b>关键词</b>：Explicit Conditional Multimodal, Conditional Multimodal Variational, Multimodal Variational Auto-Encoder, Explicit Conditional, Conditional Multimodal</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose an Explicit Conditional Multimodal Variational Auto-Encoder (ECMVAE) for audio-visual segmentation (AVS), aiming to segment sound sources in the video sequence. Existing AVS methods focus on implicit feature fusion strategies, where models are trained to fit the discrete samples in the dataset. With a limited and less diverse dataset, the resulting performance is usually unsatisfactory. In contrast, we address this problem from an effective representation learning perspective, aiming to model the contribution of each modality explicitly. Specifically, we find that audio contains critical category information of the sound producers, and visual data provides candidate sound producer(s). Their shared information corresponds to the target sound producer(s) shown in the visual data. In this case, cross-modal shared representation learning is especially important for AVS. To achieve this, our ECMVAE factorizes the representations of each modality with a modality-shared representation and a modality-specific representation. An orthogonality constraint is applied between the shared and specific representations to maintain the exclusive attribute of the factorized latent code. Further, a mutual information maximization regularizer is introduced to achieve extensive exploration of each modality. Quantitative and qualitative evaluations on the AVSBench demonstrate the effectiveness of our approach, leading to a new state-of-the-art for AVS, with a 3.84 mIOU performance leap on the challenging MS3 subset for multiple sound source segmentation.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Direction-Oriented Visual-semantic Embedding Model for Remote Sensing  Image-text Retrieval</b></summary>
  <p><b>编号</b>：[138]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08276</p>
  <p><b>作者</b>：Qing Ma,  Jiancheng Pan,  Cong Bai</p>
  <p><b>备注</b>：13 pages, 11 figures</p>
  <p><b>关键词</b>：Image-text retrieval, recent years, retrieval has developed, developed rapidly, rapidly in recent</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image-text retrieval has developed rapidly in recent years. However, it is still a challenge in remote sensing due to visual-semantic imbalance, which leads to incorrect matching of non-semantic visual and textual features. To solve this problem, we propose a novel Direction-Oriented Visual-semantic Embedding Model (DOVE) to mine the relationship between vision and language. Concretely, a Regional-Oriented Attention Module (ROAM) adaptively adjusts the distance between the final visual and textual embeddings in the latent semantic space, oriented by regional visual features. Meanwhile, a lightweight Digging Text Genome Assistant (DTGA) is designed to expand the range of tractable textual representation and enhance global word-level semantic connections using less attention operations. Ultimately, we exploit a global visual-semantic constraint to reduce single visual dependency and serve as an external constraint for the final visual and textual representations. The effectiveness and superiority of our method are verified by extensive experiments including parameter evaluation, quantitative comparison, ablation studies and visual analysis, on two benchmark datasets, RSICD and RSITMD.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：GraphAlign: Enhancing Accurate Feature Alignment by Graph matching for  Multi-Modal 3D Object Detection</b></summary>
  <p><b>编号</b>：[146]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08261</p>
  <p><b>作者</b>：Ziying Song,  Haiyue Wei,  Lin Bai,  Lei Yang,  Caiyan Jia</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：point cloud features, feature alignment, point cloud, cloud features, autonomous driving</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>LiDAR and cameras are complementary sensors for 3D object detection in autonomous driving. However, it is challenging to explore the unnatural interaction between point clouds and images, and the critical factor is how to conduct feature alignment of heterogeneous modalities. Currently, many methods achieve feature alignment by projection calibration only, without considering the problem of coordinate conversion accuracy errors between sensors, leading to sub-optimal performance. In this paper, we present GraphAlign, a more accurate feature alignment strategy for 3D object detection by graph matching. Specifically, we fuse image features from a semantic segmentation encoder in the image branch and point cloud features from a 3D Sparse CNN in the LiDAR branch. To save computation, we construct the nearest neighbor relationship by calculating Euclidean distance within the subspaces that are divided into the point cloud features. Through the projection calibration between the image and point cloud, we project the nearest neighbors of point cloud features onto the image features. Then by matching the nearest neighbors with a single point cloud to multiple images, we search for a more appropriate feature alignment. In addition, we provide a self-attention module to enhance the weights of significant relations to fine-tune the feature alignment between heterogeneous modalities. Extensive experiments on nuScenes benchmark demonstrate the effectiveness and efficiency of our GraphAlign.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Invisible Threats: Backdoor Attack in OCR Systems</b></summary>
  <p><b>编号</b>：[147]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08259</p>
  <p><b>作者</b>：Mauro Conti,  Nicola Farronato,  Stefanos Koffas,  Luca Pajola,  Stjepan Picek</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Optical Character Recognition, Character Recognition, scanned documents, widely used tool, tool to extract</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Optical Character Recognition (OCR) is a widely used tool to extract text from scanned documents. Today, the state-of-the-art is achieved by exploiting deep neural networks. However, the cost of this performance is paid at the price of system vulnerability. For instance, in backdoor attacks, attackers compromise the training phase by inserting a backdoor in the victim's model that will be activated at testing time by specific patterns while leaving the overall model performance intact. This work proposes a backdoor attack for OCR resulting in the injection of non-readable characters from malicious input images. This simple but effective attack exposes the state-of-the-art OCR weakness, making the extracted text correct to human eyes but simultaneously unusable for the NLP application that uses OCR as a preprocessing step. Experimental results show that the attacked models successfully output non-readable characters for around 90% of the poisoned instances without harming their performance for the remaining instances.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Distilling from Vision-Language Models for Improved OOD Generalization  in Vision Tasks</b></summary>
  <p><b>编号</b>：[149]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08255</p>
  <p><b>作者</b>：Sravanti Addepalli,  Ashish Ramayee Asokan,  Lakshay Sharma,  R. Venkatesh Babu</p>
  <p><b>备注</b>：Code is available at this https URL</p>
  <p><b>关键词</b>：CLIP are trained, valuable Intellectual Property, student model, image-text pairs, resulting in remarkable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision-Language Models (VLMs) such as CLIP are trained on large amounts of image-text pairs, resulting in remarkable generalization across several data distributions. The prohibitively expensive training and data collection/curation costs of these models make them valuable Intellectual Property (IP) for organizations. This motivates a vendor-client paradigm, where a vendor trains a large-scale VLM and grants only input-output access to clients on a pay-per-query basis in a black-box setting. The client aims to minimize inference cost by distilling the VLM to a student model using the limited available task-specific data, and further deploying this student model in the downstream application. While naive distillation largely improves the In-Domain (ID) accuracy of the student, it fails to transfer the superior out-of-distribution (OOD) generalization of the VLM teacher using the limited available labeled images. To mitigate this, we propose Vision-Language to Vision-Align, Distill, Predict (VL2V-ADiP), which first aligns the vision and language modalities of the teacher model with the vision modality of a pre-trained student model, and further distills the aligned VLM embeddings to the student. This maximally retains the pre-trained features of the student, while also incorporating the rich representations of the VLM image encoder and the superior generalization of the text embeddings. The proposed approach achieves state-of-the-art results on the standard Domain Generalization benchmarks in a black-box teacher setting, and also when weights of the VLM are accessible.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Fast Discrete Optimisation for Geometrically Consistent 3D Shape  Matching</b></summary>
  <p><b>编号</b>：[162]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08230</p>
  <p><b>作者</b>：Paul Roetzer,  Ahmed Abbas,  Dongliang Cao,  Florian Bernard,  Paul Swoboda</p>
  <p><b>备注</b>：Paul Roetzer and Ahmed Abbas contributed equally</p>
  <p><b>关键词</b>：learning-based shape matching, shape matching, work we propose, shape matching solutions, learning-based shape</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work we propose to combine the advantages of learning-based and combinatorial formalisms for 3D shape matching. While learning-based shape matching solutions lead to state-of-the-art matching performance, they do not ensure geometric consistency, so that obtained matchings are locally unsmooth. On the contrary, axiomatic methods allow to take geometric consistency into account by explicitly constraining the space of valid matchings. However, existing axiomatic formalisms are impractical since they do not scale to practically relevant problem sizes, or they require user input for the initialisation of non-convex optimisation problems. In this work we aim to close this gap by proposing a novel combinatorial solver that combines a unique set of favourable properties: our approach is (i) initialisation free, (ii) massively parallelisable powered by a quasi-Newton method, (iii) provides optimality gaps, and (iv) delivers decreased runtime and globally optimal results for many instances.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Structural analysis of Hindi online handwritten characters for character  recognition</b></summary>
  <p><b>编号</b>：[165]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08222</p>
  <p><b>作者</b>：Anand Sharma (MIET, Meerut),  A. G. Ramakrishnan (IISc, Bengaluru)</p>
  <p><b>备注</b>：34 pages, 36 jpg figures</p>
  <p><b>关键词</b>：Hindi online handwritten, satisfying common geometric, Hindi online, Hindi, common geometric properties</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Direction properties of online strokes are used to analyze them in terms of homogeneous regions or sub-strokes with points satisfying common geometric properties. Such sub-strokes are called sub-units. These properties are used to extract sub-units from Hindi ideal online characters. These properties along with some heuristics are used to extract sub-units from Hindi online handwritten characters.\\ A method is developed to extract point stroke, clockwise curve stroke, counter-clockwise curve stroke and loop stroke segments as sub-units from Hindi online handwritten characters. These extracted sub-units are close in structure to the sub-units of the corresponding Hindi online ideal characters.\\ Importance of local representation of online handwritten characters in terms of sub-units is assessed by training a classifier with sub-unit level local and character level global features extracted from characters for character recognition. The classifier has the recognition accuracy of 93.5\% on the testing set. This accuracy is the highest when compared with that of the classifiers trained only with global features extracted from characters in the same training set and evaluated on the same testing set.\\ Sub-unit extraction algorithm and the sub-unit based character classifier are tested on Hindi online handwritten character dataset. This dataset consists of samples from 96 different characters. There are 12832 and 2821 samples in the training and testing sets, respectively.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：TriRE: A Multi-Mechanism Learning Paradigm for Continual Knowledge  Retention and Promotion</b></summary>
  <p><b>编号</b>：[168]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08217</p>
  <p><b>作者</b>：Preetha Vijayan,  Prashant Bhat,  Elahe Arani,  Bahram Zonooz</p>
  <p><b>备注</b>：Accepted at 37th Conference on Neural Information Processing Systems (NeurIPS 2023)</p>
  <p><b>关键词</b>：deep neural networks, neural networks due, previously learned tasks, Continual learning, persistent challenge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Continual learning (CL) has remained a persistent challenge for deep neural networks due to catastrophic forgetting (CF) of previously learned tasks. Several techniques such as weight regularization, experience rehearsal, and parameter isolation have been proposed to alleviate CF. Despite their relative success, these research directions have predominantly remained orthogonal and suffer from several shortcomings, while missing out on the advantages of competing strategies. On the contrary, the brain continually learns, accommodates, and transfers knowledge across tasks by simultaneously leveraging several neurophysiological processes, including neurogenesis, active forgetting, neuromodulation, metaplasticity, experience rehearsal, and context-dependent gating, rarely resulting in CF. Inspired by how the brain exploits multiple mechanisms concurrently, we propose TriRE, a novel CL paradigm that encompasses retaining the most prominent neurons for each task, revising and solidifying the extracted knowledge of current and past tasks, and actively promoting less active neurons for subsequent tasks through rewinding and relearning. Across CL settings, TriRE significantly reduces task interference and surpasses different CL approaches considered in isolation.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Long-Tailed Classification Based on Coarse-Grained Leading Forest and  Multi-Center Loss</b></summary>
  <p><b>编号</b>：[172]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08206</p>
  <p><b>作者</b>：Jinye Yang,  Ji Xu</p>
  <p><b>备注</b>：This is another research work to apply leading tree structure along with deep learning architecture</p>
  <p><b>关键词</b>：long-tailed classification methods, long-tailed classification, real world, existing long-tailed classification, unavoidable and challenging</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Long-tailed(LT) classification is an unavoidable and challenging problem in the real world. Most of the existing long-tailed classification methods focus only on solving the inter-class imbalance in which there are more samples in the head class than in the tail class, while ignoring the intra-lass imbalance in which the number of samples of the head attribute within the same class is much larger than the number of samples of the tail attribute. The deviation in the model is caused by both of these factors, and due to the fact that attributes are implicit in most datasets and the combination of attributes is very complex, the intra-class imbalance is more difficult to handle. For this purpose, we proposed a long-tailed classification framework, known as \textbf{\textsc{Cognisance}}, which is founded on Coarse-Grained Leading Forest (CLF) and Multi-Center Loss (MCL), aiming to build a multi-granularity joint solution model by means of invariant feature learning. In this method, we designed an unsupervised learning method, i.e., CLF, to better characterize the distribution of attributes within a class. Depending on the distribution of attributes, we can flexibly construct sampling strategies suitable for different environments. In addition, we introduce a new metric learning loss (MCL), which aims to gradually eliminate confusing attributes during the feature learning process. More importantly, this approach does not depend on a specific model structure and can be integrated with existing LT methods as an independent component. We have conducted extensive experiments and our approach has state-of-the-art performance in both existing benchmarks ImageNet-GLT and MSCOCO-GLT, and can improve the performance of existing LT methods. Our codes are available on GitHub: \url{this https URL}</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Lifelong Audio-video Masked Autoencoder with Forget-robust Localized  Alignments</b></summary>
  <p><b>编号</b>：[174]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08204</p>
  <p><b>作者</b>：Jaewoo Lee,  Jaehong Yoon,  Wonjae Kim,  Yunji Kim,  Sung Ju Hwang</p>
  <p><b>备注</b>：Preprint, project page: this https URL</p>
  <p><b>关键词</b>：distribution continually shifts, lifelong audio-video masked, audio-video masked autoencoder, distribution continually, continually shifts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a lifelong audio-video masked autoencoder that continually learns the multimodal representations from a video stream containing audio-video pairs, while its distribution continually shifts over time. Specifically, we propose two novel ideas to tackle the problem: (1) Localized Alignment: We introduce a small trainable multimodal encoder that predicts the audio and video tokens that are well-aligned with each other. This allows the model to learn only the highly correlated audiovisual patches with accurate multimodal relationships. (2) Forget-robust multimodal patch selection: We compare the relative importance of each audio-video patch between the current and past data pair to mitigate unintended drift of the previously learned audio-video representations. Our proposed method, FLAVA (Forget-robust Localized Audio-Video Alignment), therefore, captures the complex relationships between the audio and video modalities during training on a sequence of pre-training tasks while alleviating the forgetting of learned audiovisual correlations. Our experiments validate that FLAVA outperforms the state-of-the-art continual learning methods on several benchmark datasets under continual audio-video representation learning scenarios.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：XIMAGENET-12: An Explainable AI Benchmark Dataset for Model Robustness  Evaluation</b></summary>
  <p><b>编号</b>：[181]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08182</p>
  <p><b>作者</b>：Qiang Li,  Dan Zhang,  Shengzhao Lei,  Xun Zhao,  Shuyan Li,  Porawit Kamnoedboon,  WeiWei Li</p>
  <p><b>备注</b>：UnderSubmission</p>
  <p><b>关键词</b>：academically validated robust, numerous unrelated benchmark, problematic practical adoption, unrelated benchmark datasets, standardized robustness metrics</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The lack of standardized robustness metrics and the widespread reliance on numerous unrelated benchmark datasets for testing have created a gap between academically validated robust models and their often problematic practical adoption. To address this, we introduce XIMAGENET-12, an explainable benchmark dataset with over 200K images and 15,600 manual semantic annotations. Covering 12 categories from ImageNet to represent objects commonly encountered in practical life and simulating six diverse scenarios, including overexposure, blurring, color changing, etc., we further propose a novel robustness criterion that extends beyond model generation ability assessment. This benchmark dataset, along with related code, is available at this https URL. Researchers and practitioners can leverage this resource to evaluate the robustness of their visual models under challenging conditions and ultimately benefit from the demands of practical computer vision systems.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Improving Fast Minimum-Norm Attacks with Hyperparameter Optimization</b></summary>
  <p><b>编号</b>：[182]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08177</p>
  <p><b>作者</b>：Giuseppe Floris,  Raffaele Mura,  Luca Scionis,  Giorgio Piras,  Maura Pintor,  Ambra Demontis,  Battista Biggio</p>
  <p><b>备注</b>：Accepted at ESANN23</p>
  <p><b>关键词</b>：machine learning models, fast minimum-norm attacks, Evaluating the adversarial, adversarial robustness, robustness of machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Evaluating the adversarial robustness of machine learning models using gradient-based attacks is challenging. In this work, we show that hyperparameter optimization can improve fast minimum-norm attacks by automating the selection of the loss function, the optimizer and the step-size scheduler, along with the corresponding hyperparameters. Our extensive evaluation involving several robust models demonstrates the improved efficacy of fast minimum-norm attacks when hyper-up with hyperparameter optimization. We release our open-source code at this https URL.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：A Deep Learning Framework for Spatiotemporal Ultrasound Localization  Microscopy</b></summary>
  <p><b>编号</b>：[195]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08143</p>
  <p><b>作者</b>：Léo Milecki,  Jonathan Porée,  Hatim Belgharbi,  Chloé Bourquin,  Rafat Damseh,  Patrick Delafontaine-Martel,  Frédéric Lesage,  Maxime Gasse,  Jean Provost</p>
  <p><b>备注</b>：Copyright 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works</p>
  <p><b>关键词</b>：Ultrasound Localization Microscopy, Localization Microscopy, Ultrasound Localization, Localization, microvascular network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Ultrasound Localization Microscopy can resolve the microvascular bed down to a few micrometers. To achieve such performance microbubble contrast agents must perfuse the entire microvascular network. Microbubbles are then located individually and tracked over time to sample individual vessels, typically over hundreds of thousands of images. To overcome the fundamental limit of diffraction and achieve a dense reconstruction of the network, low microbubble concentrations must be used, which lead to acquisitions lasting several minutes. Conventional processing pipelines are currently unable to deal with interference from multiple nearby microbubbles, further reducing achievable concentrations. This work overcomes this problem by proposing a Deep Learning approach to recover dense vascular networks from ultrasound acquisitions with high microbubble concentrations. A realistic mouse brain microvascular network, segmented from 2-photon microscopy, was used to train a three-dimensional convolutional neural network based on a V-net architecture. Ultrasound data sets from multiple microbubbles flowing through the microvascular network were simulated and used as ground truth to train the 3D CNN to track microbubbles. The 3D-CNN approach was validated in silico using a subset of the data and in vivo on a rat brain acquisition. In silico, the CNN reconstructed vascular networks with higher precision (81%) than a conventional ULM framework (70%). In vivo, the CNN could resolve micro vessels as small as 10 $\mu$m with an increase in resolution when compared against a conventional approach.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：Fine-Grained Annotation for Face Anti-Spoofing</b></summary>
  <p><b>编号</b>：[196]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08142</p>
  <p><b>作者</b>：Xu Chen,  Yunde Jia,  Yuwei Wu</p>
  <p><b>备注</b>：10 pages, 5 figures</p>
  <p><b>关键词</b>：safeguarding facial recognition, facial recognition systems, presentation attacks, plays a critical, critical role</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Face anti-spoofing plays a critical role in safeguarding facial recognition systems against presentation attacks. While existing deep learning methods show promising results, they still suffer from the lack of fine-grained annotations, which lead models to learn task-irrelevant or unfaithful features. In this paper, we propose a fine-grained annotation method for face anti-spoofing. Specifically, we first leverage the Segment Anything Model (SAM) to obtain pixel-wise segmentation masks by utilizing face landmarks as point prompts. The face landmarks provide segmentation semantics, which segments the face into regions. We then adopt these regions as masks and assemble them into three separate annotation maps: spoof, living, and background maps. Finally, we combine three separate maps into a three-channel map as annotations for model training. Furthermore, we introduce the Multi-Channel Region Exchange Augmentation (MCREA) to diversify training data and reduce overfitting. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches in both intra-dataset and cross-dataset evaluations.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：DualAug: Exploiting Additional Heavy Augmentation with OOD Data  Rejection</b></summary>
  <p><b>编号</b>：[198]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08139</p>
  <p><b>作者</b>：Zehao Wang,  Yiwen Guo,  Qizhang Li,  Guanglei Yang,  Wangmeng Zuo</p>
  <p><b>备注</b>：14 pages, 6 figures</p>
  <p><b>关键词</b>：reducing model overfitting, Data, Data augmentation, augmentation, improving generalization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Data augmentation is a dominant method for reducing model overfitting and improving generalization. Most existing data augmentation methods tend to find a compromise in augmenting the data, \textit{i.e.}, increasing the amplitude of augmentation carefully to avoid degrading some data too much and doing harm to the model performance. We delve into the relationship between data augmentation and model performance, revealing that the performance drop with heavy augmentation comes from the presence of out-of-distribution (OOD) data. Nonetheless, as the same data transformation has different effects for different training samples, even for heavy augmentation, there remains part of in-distribution data which is beneficial to model training. Based on the observation, we propose a novel data augmentation method, named \textbf{DualAug}, to keep the augmentation in distribution as much as possible at a reasonable time and computational cost. We design a data mixing strategy to fuse augmented data from both the basic- and the heavy-augmentation branches. Extensive experiments on supervised image classification benchmarks show that DualAug improve various automated data augmentation method. Moreover, the experiments on semi-supervised learning and contrastive self-supervised learning demonstrate that our DualAug can also improve related method. Code is available at \href{this https URL}{this https URL}.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Tailored Visions: Enhancing Text-to-Image Generation with Personalized  Prompt Rewriting</b></summary>
  <p><b>编号</b>：[204]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08129</p>
  <p><b>作者</b>：Zijie Chen,  Lichao Zhang,  Fangsheng Weng,  Lili Pan,  Zhenzhong Lan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：perspective of viewing, enabling the repurposing, search engine performance, techniques previously, viewing large pretrained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a novel perspective of viewing large pretrained models as search engines, thereby enabling the repurposing of techniques previously used to enhance search engine performance. As an illustration, we employ a personalized query rewriting technique in the realm of text-to-image generation. Despite significant progress in the field, it is still challenging to create personalized visual representations that align closely with the desires and preferences of individual users. This process requires users to articulate their ideas in words that are both comprehensible to the models and accurately capture their vision, posing difficulties for many users. In this paper, we tackle this challenge by leveraging historical user interactions with the system to enhance user prompts. We propose a novel approach that involves rewriting user prompts based a new large-scale text-to-image dataset with over 300k prompts from 3115 users. Our rewriting model enhances the expressiveness and alignment of user prompts with their intended visual outputs. Experimental results demonstrate the superiority of our methods over baseline approaches, as evidenced in our new offline evaluation method and online tests. Our approach opens up exciting possibilities of applying more search engine techniques to build truly personalized large pretrained models.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：DUSA: Decoupled Unsupervised Sim2Real Adaptation for  Vehicle-to-Everything Collaborative Perception</b></summary>
  <p><b>编号</b>：[209]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08117</p>
  <p><b>作者</b>：Xianghao Kong,  Wentao Jiang,  Jinrang Jia,  Yifeng Shi,  Runsheng Xu,  Si Liu</p>
  <p><b>备注</b>：ACM MM 2023</p>
  <p><b>关键词</b>：real-world data, data, Simulated data, adaptation, real-world</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vehicle-to-Everything (V2X) collaborative perception is crucial for autonomous driving. However, achieving high-precision V2X perception requires a significant amount of annotated real-world data, which can always be expensive and hard to acquire. Simulated data have raised much attention since they can be massively produced at an extremely low cost. Nevertheless, the significant domain gap between simulated and real-world data, including differences in sensor type, reflectance patterns, and road surroundings, often leads to poor performance of models trained on simulated data when evaluated on real-world data. In addition, there remains a domain gap between real-world collaborative agents, e.g. different types of sensors may be installed on autonomous vehicles and roadside infrastructures with different extrinsics, further increasing the difficulty of sim2real generalization. To take full advantage of simulated data, we present a new unsupervised sim2real domain adaptation method for V2X collaborative detection named Decoupled Unsupervised Sim2Real Adaptation (DUSA). Our new method decouples the V2X collaborative sim2real domain adaptation problem into two sub-problems: sim2real adaptation and inter-agent adaptation. For sim2real adaptation, we design a Location-adaptive Sim2Real Adapter (LSA) module to adaptively aggregate features from critical locations of the feature map and align the features between simulated data and real-world data via a sim/real discriminator on the aggregated global feature. For inter-agent adaptation, we further devise a Confidence-aware Inter-agent Adapter (CIA) module to align the fine-grained features from heterogeneous agents under the guidance of agent-wise confidence maps. Experiments demonstrate the effectiveness of the proposed DUSA approach on unsupervised sim2real adaptation from the simulated V2XSet dataset to the real-world DAIR-V2X-C dataset.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Multimodal Active Measurement for Human Mesh Recovery in Close Proximity</b></summary>
  <p><b>编号</b>：[210]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08116</p>
  <p><b>作者</b>：Takahiro Maeda,  Keisuke Takeshita,  Kazuhito Tanaka</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：physical human-robot interactions, sophisticated physical human-robot, target person, target person body, human pose estimation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>For safe and sophisticated physical human-robot interactions (pHRI), a robot needs to estimate the accurate body pose or mesh of the target person. However, in these pHRI scenarios, the robot cannot fully observe the target person's body with equipped cameras because the target person is usually close to the robot. This leads to severe truncation and occlusions, and results in poor accuracy of human pose estimation. For better accuracy of human pose estimation or mesh recovery on this limited information from cameras, we propose an active measurement and sensor fusion framework of the equipped cameras and other sensors such as touch sensors and 2D LiDAR. These touch and LiDAR sensing are obtained attendantly through pHRI without additional costs. These sensor measurements are sparse but reliable and informative cues for human mesh recovery. In our active measurement process, camera viewpoints and sensor placements are optimized based on the uncertainty of the estimated pose, which is closely related to the truncated or occluded areas. In our sensor fusion process, we fuse the sensor measurements to the camera-based estimated pose by minimizing the distance between the estimated mesh and measured positions. Our method is agnostic to robot configurations. Experiments were conducted using the Toyota Human Support Robot, which has a camera, 2D LiDAR, and a touch sensor on the robot arm. Our proposed method demonstrated the superiority in the human pose estimation accuracy on the quantitative comparison. Furthermore, our proposed method reliably estimated the pose of the target person in practical settings such as target people occluded by a blanket and standing aid with the robot arm.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：Generalized Logit Adjustment: Calibrating Fine-tuned Models by Removing  Label Bias in Foundation Models</b></summary>
  <p><b>编号</b>：[212]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08106</p>
  <p><b>作者</b>：Beier Zhu,  Kaihua Tang,  Qianru Sun,  Hanwang Zhang</p>
  <p><b>备注</b>：Accepted by NeurIPS2023</p>
  <p><b>关键词</b>：Foundation models, CLIP allow zero-shot, Foundation, zero-shot transfer, models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Foundation models like CLIP allow zero-shot transfer on various tasks without additional training data. Yet, the zero-shot performance is less competitive than a fully supervised one. Thus, to enhance the performance, fine-tuning and ensembling are also commonly adopted to better fit the downstream tasks. However, we argue that such prior work has overlooked the inherent biases in foundation models. Due to the highly imbalanced Web-scale training set, these foundation models are inevitably skewed toward frequent semantics, and thus the subsequent fine-tuning or ensembling is still biased. In this study, we systematically examine the biases in foundation models and demonstrate the efficacy of our proposed Generalized Logit Adjustment (GLA) method. Note that bias estimation in foundation models is challenging, as most pre-train data cannot be explicitly accessed like in traditional long-tailed classification tasks. To this end, GLA has an optimization-based bias estimation approach for debiasing foundation models. As our work resolves a fundamental flaw in the pre-training, the proposed GLA demonstrates significant improvements across a diverse range of tasks: it achieves 1.5 pp accuracy gains on ImageNet, an large average improvement (1.4-4.6 pp) on 11 few-shot datasets, 2.4 pp gains on long-tailed classification. Codes are in \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：SingleInsert: Inserting New Concepts from a Single Image into  Text-to-Image Models for Flexible Editing</b></summary>
  <p><b>编号</b>：[220]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08094</p>
  <p><b>作者</b>：Zijie Wu,  Chaohui Yu,  Zhen Zhu,  Fan Wang,  Xiang Bai</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：models enables high-quality, enables high-quality image, flexible textual control, high-quality image generation, Recent progress</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent progress in text-to-image (T2I) models enables high-quality image generation with flexible textual control. To utilize the abundant visual priors in the off-the-shelf T2I models, a series of methods try to invert an image to proper embedding that aligns with the semantic space of the T2I model. However, these image-to-text (I2T) inversion methods typically need multiple source images containing the same concept or struggle with the imbalance between editing flexibility and visual fidelity. In this work, we point out that the critical problem lies in the foreground-background entanglement when learning an intended concept, and propose a simple and effective baseline for single-image I2T inversion, named SingleInsert. SingleInsert adopts a two-stage scheme. In the first stage, we regulate the learned embedding to concentrate on the foreground area without being associated with the irrelevant background. In the second stage, we finetune the T2I model for better visual resemblance and devise a semantic loss to prevent the language drift problem. With the proposed techniques, SingleInsert excels in single concept generation with high visual fidelity while allowing flexible editing. Additionally, SingleInsert can perform single-image novel view synthesis and multiple concepts composition without requiring joint training. To facilitate evaluation, we design an editing prompt list and introduce a metric named Editing Success Rate (ESR) for quantitative assessment of editing flexibility. Our project page is: this https URL</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Consistent123: Improve Consistency for One Image to 3D Object Synthesis</b></summary>
  <p><b>编号</b>：[221]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08092</p>
  <p><b>作者</b>：Haohan Weng,  Tianyu Yang,  Jianan Wang,  Yu Li,  Tong Zhang,  C. L. Philip Chen,  Lei Zhang</p>
  <p><b>备注</b>：For more qualitative results, please see this https URL</p>
  <p><b>关键词</b>：excellent zero-shot capability, diffusion models enable, image diffusion models, Large image diffusion, zero-shot capability</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large image diffusion models enable novel view synthesis with high quality and excellent zero-shot capability. However, such models based on image-to-image translation have no guarantee of view consistency, limiting the performance for downstream tasks like 3D reconstruction and image-to-3D generation. To empower consistency, we propose Consistent123 to synthesize novel views simultaneously by incorporating additional cross-view attention layers and the shared self-attention mechanism. The proposed attention mechanism improves the interaction across all synthesized views, as well as the alignment between the condition view and novel views. In the sampling stage, such architecture supports simultaneously generating an arbitrary number of views while training at a fixed length. We also introduce a progressive classifier-free guidance strategy to achieve the trade-off between texture and geometry for synthesized object views. Qualitative and quantitative experiments show that Consistent123 outperforms baselines in view consistency by a large margin. Furthermore, we demonstrate a significant improvement of Consistent123 on varying downstream tasks, showing its great potential in the 3D generation field. The project page is available at this http URL.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：Volumetric Medical Image Segmentation via Scribble Annotations and Shape  Priors</b></summary>
  <p><b>编号</b>：[226]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08084</p>
  <p><b>作者</b>：Qiuhui Chen,  Haiying Lyu,  Xinyue Hu,  Yong Lu,  Yi Hong</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2205.06779</p>
  <p><b>关键词</b>：medical image analysis, gained great attention, weakly-supervised image segmentation, image segmentation, voxel level</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, weakly-supervised image segmentation using weak annotations like scribbles has gained great attention in computer vision and medical image analysis, since such annotations are much easier to obtain compared to time-consuming and labor-intensive labeling at the pixel/voxel level. However, due to a lack of structure supervision on regions of interest (ROIs), existing scribble-based methods suffer from poor boundary localization. Furthermore, most current methods are designed for 2D image segmentation, which do not fully leverage the volumetric information if directly applied to each image slice. In this paper, we propose a scribble-based volumetric image segmentation, Scribble2D5, which tackles 3D anisotropic image segmentation and aims to its improve boundary prediction. To achieve this, we augment a 2.5D attention UNet with a proposed label propagation module to extend semantic information from scribbles and use a combination of static and active boundary prediction to learn ROI's boundary and regularize its shape. Also, we propose an optional add-on component, which incorporates the shape prior information from unpaired segmentation masks to further improve model accuracy. Extensive experiments on three public datasets and one private dataset demonstrate our Scribble2D5 achieves state-of-the-art performance on volumetric image segmentation using scribbles and shape prior if available.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：Jointly Optimized Global-Local Visual Localization of UAVs</b></summary>
  <p><b>编号</b>：[228]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08082</p>
  <p><b>作者</b>：Haoling Li,  Jiuniu Wang,  Zhiwei Wei,  Wenjia Xu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：navigation satellite systems, global navigation satellite, global navigation, disrupted and unreliable, localization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Navigation and localization of UAVs present a challenge when global navigation satellite systems (GNSS) are disrupted and unreliable. Traditional techniques, such as simultaneous localization and mapping (SLAM) and visual odometry (VO), exhibit certain limitations in furnishing absolute coordinates and mitigating error accumulation. Existing visual localization methods achieve autonomous visual localization without error accumulation by matching with ortho satellite images. However, doing so cannot guarantee real-time performance due to the complex matching process. To address these challenges, we propose a novel Global-Local Visual Localization (GLVL) network. Our GLVL network is a two-stage visual localization approach, combining a large-scale retrieval module that finds similar regions with the UAV flight scene, and a fine-grained matching module that localizes the precise UAV coordinate, enabling real-time and precise localization. The training process is jointly optimized in an end-to-end manner to further enhance the model capability. Experiments on six UAV flight scenes encompassing both texture-rich and texture-sparse regions demonstrate the ability of our model to achieve the real-time precise localization requirements of UAVs. Particularly, our method achieves a localization error of only 2.39 meters in 0.48 seconds in a village scene with sparse texture features.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：Samples on Thin Ice: Re-Evaluating Adversarial Pruning of Neural  Networks</b></summary>
  <p><b>编号</b>：[231]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08073</p>
  <p><b>作者</b>：Giorgio Piras,  Maura Pintor,  Ambra Demontis,  Battista Biggio</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：trading desirable properties, Neural network pruning, trading desirable, higher sparsity, adversarial pruning methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural network pruning has shown to be an effective technique for reducing the network size, trading desirable properties like generalization and robustness to adversarial attacks for higher sparsity. Recent work has claimed that adversarial pruning methods can produce sparse networks while also preserving robustness to adversarial examples. In this work, we first re-evaluate three state-of-the-art adversarial pruning methods, showing that their robustness was indeed overestimated. We then compare pruned and dense versions of the same models, discovering that samples on thin ice, i.e., closer to the unpruned model's decision boundary, are typically misclassified after pruning. We conclude by discussing how this intuition may lead to designing more effective adversarial pruning methods in future work.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：Learning Transferable Conceptual Prototypes for Interpretable  Unsupervised Domain Adaptation</b></summary>
  <p><b>编号</b>：[233]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08071</p>
  <p><b>作者</b>：Junyu Gao,  Xinhong Ma,  Changsheng Xu</p>
  <p><b>备注</b>：Submitted to IEEE TIP</p>
  <p><b>关键词</b>：deep neural networks, controllable model decisions, current UDA models, unsupervised domain adaptation, neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite the great progress of unsupervised domain adaptation (UDA) with the deep neural networks, current UDA models are opaque and cannot provide promising explanations, limiting their applications in the scenarios that require safe and controllable model decisions. At present, a surge of work focuses on designing deep interpretable methods with adequate data annotations and only a few methods consider the distributional shift problem. Most existing interpretable UDA methods are post-hoc ones, which cannot facilitate the model learning process for performance enhancement. In this paper, we propose an inherently interpretable method, named Transferable Conceptual Prototype Learning (TCPL), which could simultaneously interpret and improve the processes of knowledge transfer and decision-making in UDA. To achieve this goal, we design a hierarchically prototypical module that transfers categorical basic concepts from the source domain to the target domain and learns domain-shared prototypes for explaining the underlying reasoning process. With the learned transferable prototypes, a self-predictive consistent pseudo-label strategy that fuses confidence, predictions, and prototype information, is designed for selecting suitable target samples for pseudo annotations and gradually narrowing down the domain gap. Comprehensive experiments show that the proposed method can not only provide effective and intuitive explanations but also outperform previous state-of-the-arts.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：Age Estimation Based on Graph Convolutional Networks and Multi-head  Attention Mechanisms</b></summary>
  <p><b>编号</b>：[238]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08064</p>
  <p><b>作者</b>：Miaomiao Yang,  Changwei Yao,  Shijin Yan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：part of facial, Convolutional Neural Network, Graph Convolutional Network, Neural Network, Age estimation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Age estimation technology is a part of facial recognition and has been applied to identity authentication. This technology achieves the development and application of a juvenile anti-addiction system by authenticating users in the game. Convolutional Neural Network (CNN) and Transformer algorithms are widely used in this application scenario. However, these two models cannot flexibly extract and model features of faces with irregular shapes, and they are ineffective in capturing key information. Furthermore, the above methods will contain a lot of background information while extracting features, which will interfere with the model. In consequence, it is easy to extract redundant information from images. In this paper, a new modeling idea is proposed to solve this problem, which can flexibly model irregular objects. The Graph Convolutional Network (GCN) is used to extract features from irregular face images effectively, and multi-head attention mechanisms are added to avoid redundant features and capture key region information in the image. This model can effectively improve the accuracy of age estimation and reduce the MAE error value to about 3.64, which is better than the effect of today's age estimation model, to improve the accuracy of face recognition and identity authentication.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：EC-Depth: Exploring the consistency of self-supervised monocular depth  estimation under challenging scenes</b></summary>
  <p><b>编号</b>：[244]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08044</p>
  <p><b>作者</b>：Ruijie Zhu,  Ziyang Song,  Chuxin Wang,  Jianfeng He,  Tianzhu Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：holds significant importance, estimation holds significant, monocular depth estimation, Self-supervised monocular depth, driving and robotics</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Self-supervised monocular depth estimation holds significant importance in the fields of autonomous driving and robotics. However, existing methods are typically designed to train and test on clear and pristine datasets, overlooking the impact of various adverse conditions prevalent in real-world scenarios. As a result, it is commonly observed that most self-supervised monocular depth estimation methods struggle to perform adequately under challenging conditions. To address this issue, we present EC-Depth, a novel self-supervised two-stage training framework to achieve a robust depth estimation, starting from the foundation of depth prediction consistency under different perturbations. Leveraging the proposed perturbation-invariant depth consistency constraint module and the consistency-based pseudo-label selection module, our model attains accurate and consistent depth predictions in both standard and challenging scenarios. Extensive experiments substantiate the effectiveness of the proposed method. Moreover, our method surpasses existing state-of-the-art methods on KITTI, KITTI-C and DrivingStereo benchmarks, demonstrating its potential for enhancing the reliability of self-supervised monocular depth estimation models in real-world applications.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：X-HRNet: Towards Lightweight Human Pose Estimation with Spatially  Unidimensional Self-Attention</b></summary>
  <p><b>编号</b>：[246]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08042</p>
  <p><b>作者</b>：Yixuan Zhou,  Xuanhan Wang,  Xing Xu,  Lei Zhao,  Jingkuan Song</p>
  <p><b>备注</b>：Accepted by ICME 2022</p>
  <p><b>关键词</b>：achieve high performance, human pose estimation, pose estimation, high performance, high computational complexity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>High-resolution representation is necessary for human pose estimation to achieve high performance, and the ensuing problem is high computational complexity. In particular, predominant pose estimation methods estimate human joints by 2D single-peak heatmaps. Each 2D heatmap can be horizontally and vertically projected to and reconstructed by a pair of 1D heat vectors. Inspired by this observation, we introduce a lightweight and powerful alternative, Spatially Unidimensional Self-Attention (SUSA), to the pointwise (1x1) convolution that is the main computational bottleneck in the depthwise separable 3c3 convolution. Our SUSA reduces the computational complexity of the pointwise (1x1) convolution by 96% without sacrificing accuracy. Furthermore, we use the SUSA as the main module to build our lightweight pose estimation backbone X-HRNet, where `X' represents the estimated cross-shape attention vectors. Extensive experiments on the COCO benchmark demonstrate the superiority of our X-HRNet, and comprehensive ablation studies show the effectiveness of the SUSA modules. The code is publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：Continual Learning via Manifold Expansion Replay</b></summary>
  <p><b>编号</b>：[250]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08038</p>
  <p><b>作者</b>：Zihao Xu,  Xuan Tang,  Yufei Shi,  Jianfeng Zhang,  Jian Yang,  Mingsong Chen,  Xian Wei</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：learner learns multiple, learns multiple tasks, continual learning, learner learns, learns multiple</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In continual learning, the learner learns multiple tasks in sequence, with data being acquired only once for each task. Catastrophic forgetting is a major challenge to continual learning. To reduce forgetting, some existing rehearsal-based methods use episodic memory to replay samples of previous tasks. However, in the process of knowledge integration when learning a new task, this strategy also suffers from catastrophic forgetting due to an imbalance between old and new knowledge. To address this problem, we propose a novel replay strategy called Manifold Expansion Replay (MaER). We argue that expanding the implicit manifold of the knowledge representation in the episodic memory helps to improve the robustness and expressiveness of the model. To this end, we propose a greedy strategy to keep increasing the diameter of the implicit manifold represented by the knowledge in the buffer during memory management. In addition, we introduce Wasserstein distance instead of cross entropy as distillation loss to preserve previous knowledge. With extensive experimental validation on MNIST, CIFAR10, CIFAR100, and TinyImageNet, we show that the proposed method significantly improves the accuracy in continual learning setup, outperforming the state of the arts.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：BaSAL: Size Balanced Warm Start Active Learning for LiDAR Semantic  Segmentation</b></summary>
  <p><b>编号</b>：[253]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08035</p>
  <p><b>作者</b>：Jiarong Wei,  Yancong Lin,  Holger Caesar</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Active learning, Active learning strives, existing active learning, active learning methods, LiDAR semantic segmentation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Active learning strives to reduce the need for costly data annotation, by repeatedly querying an annotator to label the most informative samples from a pool of unlabeled data and retraining a model from these samples. We identify two problems with existing active learning methods for LiDAR semantic segmentation. First, they ignore the severe class imbalance inherent in LiDAR semantic segmentation datasets. Second, to bootstrap the active learning loop, they train their initial model from randomly selected data samples, which leads to low performance and is referred to as the cold start problem. To address these problems we propose BaSAL, a size-balanced warm start active learning model, based on the observation that each object class has a characteristic size. By sampling object clusters according to their size, we can thus create a size-balanced dataset that is also more class-balanced. Furthermore, in contrast to existing information measures like entropy or CoreSet, size-based sampling does not require an already trained model and thus can be used to address the cold start problem. Results show that we are able to improve the performance of the initial model by a large margin. Combining size-balanced sampling and warm start with established information measures, our approach achieves a comparable performance to training on the entire SemanticKITTI dataset, despite using only 5% of the annotations, which outperforms existing active learning methods. We also match the existing state-of-the-art in active learning on nuScenes. Our code will be made available upon paper acceptance.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：Exploring Large Language Models for Multi-Modal Out-of-Distribution  Detection</b></summary>
  <p><b>编号</b>：[257]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08027</p>
  <p><b>作者</b>：Yi Dai,  Hao Lang,  Kaisheng Zeng,  Fei Huang,  Yongbin Li</p>
  <p><b>备注</b>：EMNLP2023 Findings Long Paper</p>
  <p><b>关键词</b>：trustworthy machine learning, OOD detection, machine learning, OOD, essential for reliable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Out-of-distribution (OOD) detection is essential for reliable and trustworthy machine learning. Recent multi-modal OOD detection leverages textual information from in-distribution (ID) class names for visual OOD detection, yet it currently neglects the rich contextual information of ID classes. Large language models (LLMs) encode a wealth of world knowledge and can be prompted to generate descriptive features for each class. Indiscriminately using such knowledge causes catastrophic damage to OOD detection due to LLMs' hallucinations, as is observed by our analysis. In this paper, we propose to apply world knowledge to enhance OOD detection performance through selective generation from LLMs. Specifically, we introduce a consistency-based uncertainty calibration method to estimate the confidence score of each generation. We further extract visual objects from each image to fully capitalize on the aforementioned world knowledge. Extensive experiments demonstrate that our method consistently outperforms the state-of-the-art.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：Beyond Sharing Weights in Decoupling Feature Learning Network for UAV  RGB-Infrared Vehicle Re-Identification</b></summary>
  <p><b>编号</b>：[258]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08026</p>
  <p><b>作者</b>：Xingyue Liu,  Jiahao Qi,  Chen Chen,  Kangcheng Bin,  Ping Zhong</p>
  <p><b>备注</b>：13 pages, 10 figures, 64 citations, submitted to TMM</p>
  <p><b>关键词</b>：full-time target search, performing full-time target, unmanned aerial vehicle, cross-modality vehicle re-identification, cross-modality vehicle Re-ID</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Owing to the capacity of performing full-time target search, cross-modality vehicle re-identification (Re-ID) based on unmanned aerial vehicle (UAV) is gaining more attention in both video surveillance and public security. However, this promising and innovative research has not been studied sufficiently due to the data inadequacy issue. Meanwhile, the cross-modality discrepancy and orientation discrepancy challenges further aggravate the difficulty of this task. To this end, we pioneer a cross-modality vehicle Re-ID benchmark named UAV Cross-Modality Vehicle Re-ID (UCM-VeID), containing 753 identities with 16015 RGB and 13913 infrared images. Moreover, to meet cross-modality discrepancy and orientation discrepancy challenges, we present a hybrid weights decoupling network (HWDNet) to learn the shared discriminative orientation-invariant features. For the first challenge, we proposed a hybrid weights siamese network with a well-designed weight restrainer and its corresponding objective function to learn both modality-specific and modality shared information. In terms of the second challenge, three effective decoupling structures with two pretext tasks are investigated to learn orientation-invariant feature. Comprehensive experiments are carried out to validate the effectiveness of the proposed method. The dataset and codes will be released at this https URL.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：Dual-Stream Knowledge-Preserving Hashing for Unsupervised Video  Retrieval</b></summary>
  <p><b>编号</b>：[265]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08009</p>
  <p><b>作者</b>：Pandeng Li,  Hongtao Xie,  Jiannan Ge,  Lei Zhang,  Shaobo Min,  Yongdong Zhang</p>
  <p><b>备注</b>：17 pages, 8 figures, ECCV 2022</p>
  <p><b>关键词</b>：Unsupervised video hashing, reconstruct input videos, hashing usually optimizes, learning to reconstruct, reconstruct input</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Unsupervised video hashing usually optimizes binary codes by learning to reconstruct input videos. Such reconstruction constraint spends much effort on frame-level temporal context changes without focusing on video-level global semantics that are more useful for retrieval. Hence, we address this problem by decomposing video information into reconstruction-dependent and semantic-dependent information, which disentangles the semantic extraction from reconstruction constraint. Specifically, we first design a simple dual-stream structure, including a temporal layer and a hash layer. Then, with the help of semantic similarity knowledge obtained from self-supervision, the hash layer learns to capture information for semantic retrieval, while the temporal layer learns to capture the information for reconstruction. In this way, the model naturally preserves the disentangled semantics into binary codes. Validated by comprehensive experiments, our method consistently outperforms the state-of-the-arts on three video benchmarks.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：Point-NeuS: Point-Guided Neural Implicit Surface Reconstruction by  Volume Rendering</b></summary>
  <p><b>编号</b>：[271]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07997</p>
  <p><b>作者</b>：Chen Zhang,  Wanjuan Su,  Wenbing Tao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：learning neural implicit, volume rendering, multi-view reconstruction, learning neural, neural implicit surface</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, learning neural implicit surface by volume rendering has been a promising way for multi-view reconstruction. However, limited accuracy and excessive time complexity remain bottlenecks that current methods urgently need to overcome. To address these challenges, we propose a new method called Point-NeuS, utilizing point-guided mechanisms to achieve accurate and efficient reconstruction. Point modeling is organically embedded into the volume rendering to enhance and regularize the representation of implicit surface. Specifically, to achieve precise point guidance and noise robustness, aleatoric uncertainty of the point cloud is modeled to capture the distribution of noise and estimate the reliability of points. Additionally, a Neural Projection module connecting points and images is introduced to add geometric constraints to the Signed Distance Function (SDF). To better compensate for geometric bias between volume rendering and point modeling, high-fidelity points are filtered into an Implicit Displacement Network to improve the representation of SDF. Benefiting from our effective point guidance, lightweight networks are employed to achieve an impressive 11x speedup compared to NeuS. Extensive experiments show that our method yields high-quality surfaces, especially for fine-grained details and smooth regions. Moreover, it exhibits strong robustness to both noisy and sparse data.</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：Reset It and Forget It: Relearning Last-Layer Weights Improves Continual  and Transfer Learning</b></summary>
  <p><b>编号</b>：[272]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07996</p>
  <p><b>作者</b>：Lapo Frati,  Neil Traft,  Jeff Clune,  Nick Cheney</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：simple pre-training mechanism, continual learning, pre-training mechanism, continual learning settings, simple pre-training</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This work identifies a simple pre-training mechanism that leads to representations exhibiting better continual and transfer learning. This mechanism -- the repeated resetting of weights in the last layer, which we nickname "zapping" -- was originally designed for a meta-continual-learning procedure, yet we show it is surprisingly applicable in many settings beyond both meta-learning and continual learning. In our experiments, we wish to transfer a pre-trained image classifier to a new set of classes, in a few shots. We show that our zapping procedure results in improved transfer accuracy and/or more rapid adaptation in both standard fine-tuning and continual learning settings, while being simple to implement and computationally efficient. In many cases, we achieve performance on par with state of the art meta-learning without needing the expensive higher-order gradients, by using a combination of zapping and sequential learning. An intuitive explanation for the effectiveness of this zapping procedure is that representations trained with repeated zapping learn features that are capable of rapidly adapting to newly initialized classifiers. Such an approach may be considered a computationally cheaper type of, or alternative to, meta-learning rapidly adaptable features with higher-order gradients. This adds to recent work on the usefulness of resetting neural network parameters during training, and invites further investigation of this mechanism.</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：HeightFormer: A Multilevel Interaction and Image-adaptive  Classification-regression Network for Monocular Height Estimation with Aerial  Images</b></summary>
  <p><b>编号</b>：[273]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07995</p>
  <p><b>作者</b>：Zhan Chen,  Yidan Zhang,  Xiyu Qi,  Yongqiang Mao,  Xin Zhou,  Lulu Niu,  Hui Wu,  Lei Wang,  Yunping Ge</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：urban modelling, proving critical, autonomous driving, pivotal topic, topic within measurement</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Height estimation has long been a pivotal topic within measurement and remote sensing disciplines, proving critical for endeavours such as 3D urban modelling, MR and autonomous driving. Traditional methods utilise stereo matching or multisensor fusion, both well-established techniques that typically necessitate multiple images from varying perspectives and adjunct sensors like SAR, leading to substantial deployment costs. Single image height estimation has emerged as an attractive alternative, boasting a larger data source variety and simpler deployment. However, current methods suffer from limitations such as fixed receptive fields, a lack of global information interaction, leading to noticeable instance-level height deviations. The inherent complexity of height prediction can result in a blurry estimation of object edge depth when using mainstream regression methods based on fixed height division. This paper presents a comprehensive solution for monocular height estimation in remote sensing, termed HeightFormer, combining multilevel interactions and image-adaptive classification-regression. It features the Multilevel Interaction Backbone (MIB) and Image-adaptive Classification-regression Height Generator (ICG). MIB supplements the fixed sample grid in CNN of the conventional backbone network with tokens of different interaction ranges. It is complemented by a pixel-, patch-, and feature map-level hierarchical interaction mechanism, designed to relay spatial geometry information across different scales and introducing a global receptive field to enhance the quality of instance-level height estimation. The ICG dynamically generates height partition for each image and reframes the traditional regression task, using a refinement from coarse to fine classification-regression that significantly mitigates the innate ill-posedness issue and drastically improves edge sharpness.</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：Self-supervised visual learning for analyzing firearms trafficking  activities on the Web</b></summary>
  <p><b>编号</b>：[283]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07975</p>
  <p><b>作者</b>：Sotirios Konstantakos,  Despina Ioanna Chalkiadaki,  Ioannis Mademlis,  Adamantia Anna Rebolledo Chrysochoou,  Georgios Th. Papadopoulos</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：public space security, law enforcement investigations, World Wide Web, RGB images, Convolutional Neural Networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automated visual firearms classification from RGB images is an important real-world task with applications in public space security, intelligence gathering and law enforcement investigations. When applied to images massively crawled from the World Wide Web (including social media and dark Web sites), it can serve as an important component of systems that attempt to identify criminal firearms trafficking networks, by analyzing Big Data from open-source intelligence. Deep Neural Networks (DNN) are the state-of-the-art methodology for achieving this, with Convolutional Neural Networks (CNN) being typically employed. The common transfer learning approach consists of pretraining on a large-scale, generic annotated dataset for whole-image classification, such as ImageNet-1k, and then finetuning the DNN on a smaller, annotated, task-specific, downstream dataset for visual firearms classification. Neither Visual Transformer (ViT) neural architectures nor Self-Supervised Learning (SSL) approaches have been so far evaluated on this critical task. SSL essentially consists of replacing the traditional supervised pretraining objective with an unsupervised pretext task that does not require ground-truth labels..</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：CleftGAN: Adapting A Style-Based Generative Adversarial Network To  Create Images Depicting Cleft Lip Deformity</b></summary>
  <p><b>编号</b>：[287]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07969</p>
  <p><b>作者</b>：Abdullah Hayajneh,  Erchin Serpedin,  Mohammad Shaqfeh,  Graeme Glass,  Mitchell A. Stotland</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ethics board-approved patient, board-approved patient images, machine learning system, ethics board-approved, images</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A major obstacle when attempting to train a machine learning system to evaluate facial clefts is the scarcity of large datasets of high-quality, ethics board-approved patient images. In response, we have built a deep learning-based cleft lip generator designed to produce an almost unlimited number of artificial images exhibiting high-fidelity facsimiles of cleft lip with wide variation. We undertook a transfer learning protocol testing different versions of StyleGAN-ADA (a generative adversarial network image generator incorporating adaptive data augmentation (ADA)) as the base model. Training images depicting a variety of cleft deformities were pre-processed to adjust for rotation, scaling, color adjustment and background blurring. The ADA modification of the primary algorithm permitted construction of our new generative model while requiring input of a relatively small number of training images. Adversarial training was carried out using 514 unique frontal photographs of cleft-affected faces to adapt a pre-trained model based on 70,000 normal faces. The Frechet Inception Distance (FID) was used to measure the similarity of the newly generated facial images to the cleft training dataset, while Perceptual Path Length (PPL) and the novel Divergence Index of Severity Histograms (DISH) measures were also used to assess the performance of the image generator that we dub CleftGAN. We found that StyleGAN3 with translation invariance (StyleGAN3-t) performed optimally as a base model. Generated images achieved a low FID reflecting a close similarity to our training input dataset of genuine cleft images. Low PPL and DISH measures reflected a smooth and semantically valid interpolation of images through the transfer learning process and a similar distribution of severity in the training and generated images, respectively.</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：What Matters to You? Towards Visual Representation Alignment for Robot  Learning</b></summary>
  <p><b>编号</b>：[299]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07932</p>
  <p><b>作者</b>：Ran Tian,  Chenfeng Xu,  Masayoshi Tomizuka,  Jitendra Malik,  Andrea Bajcsy</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：optimize rewards aligned, service of people, operating in service, visual, RGB images</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>When operating in service of people, robots need to optimize rewards aligned with end-user preferences. Since robots will rely on raw perceptual inputs like RGB images, their rewards will inevitably use visual representations. Recently there has been excitement in using representations from pre-trained visual models, but key to making these work in robotics is fine-tuning, which is typically done via proxy tasks like dynamics prediction or enforcing temporal cycle-consistency. However, all these proxy tasks bypass the human's input on what matters to them, exacerbating spurious correlations and ultimately leading to robot behaviors that are misaligned with user preferences. In this work, we propose that robots should leverage human feedback to align their visual representations with the end-user and disentangle what matters for the task. We propose Representation-Aligned Preference-based Learning (RAPL), a method for solving the visual representation alignment problem and visual reward learning problem through the lens of preference-based learning and optimal transport. Across experiments in X-MAGICAL and in robotic manipulation, we find that RAPL's reward consistently generates preferred robot behaviors with high sample efficiency, and shows strong zero-shot generalization when the visual representation is learned from a different embodiment than the robot's.</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：D2 Pruning: Message Passing for Balancing Diversity and Difficulty in  Data Pruning</b></summary>
  <p><b>编号</b>：[300]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07931</p>
  <p><b>作者</b>：Adyasha Maharana,  Prateek Yadav,  Mohit Bansal</p>
  <p><b>备注</b>：17 pages (Our code is available at this https URL)</p>
  <p><b>关键词</b>：Analytical theories suggest, lower test errors, Coreset, Analytical theories, pruning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Analytical theories suggest that higher-quality data can lead to lower test errors in models trained on a fixed data budget. Moreover, a model can be trained on a lower compute budget without compromising performance if a dataset can be stripped of its redundancies. Coreset selection (or data pruning) seeks to select a subset of the training data so as to maximize the performance of models trained on this subset, also referred to as coreset. There are two dominant approaches: (1) geometry-based data selection for maximizing data diversity in the coreset, and (2) functions that assign difficulty scores to samples based on training dynamics. Optimizing for data diversity leads to a coreset that is biased towards easier samples, whereas, selection by difficulty ranking omits easy samples that are necessary for the training of deep learning models. This demonstrates that data diversity and importance scores are two complementary factors that need to be jointly considered during coreset selection. We represent a dataset as an undirected graph and propose a novel pruning algorithm, D2 Pruning, that uses forward and reverse message passing over this dataset graph for coreset selection. D2 Pruning updates the difficulty scores of each example by incorporating the difficulty of its neighboring examples in the dataset graph. Then, these updated difficulty scores direct a graph-based sampling method to select a coreset that encapsulates both diverse and difficult regions of the dataset space. We evaluate supervised and self-supervised versions of our method on various vision and language datasets. Results show that D2 Pruning improves coreset selection over previous state-of-the-art methods for up to 70% pruning rates. Additionally, we find that using D2 Pruning for filtering large multimodal datasets leads to increased diversity in the dataset and improved generalization of pretrained models.</p>
  </details>
</details>
<details>
  <summary>83. <b>标题：Ultrasound Image Segmentation of Thyroid Nodule via Latent Semantic  Feature Co-Registration</b></summary>
  <p><b>编号</b>：[305]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.09221</p>
  <p><b>作者</b>：Xuewei Li,  Yaqiao Zhu,  Jie Gao,  Xi Wei,  Ruixuan Zhang,  Yuan Tian,  Mei Yu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ultrasound imaging plays, thyroid ultrasound imaging, plays a crucial, crucial role, detection and treatment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Segmentation of nodules in thyroid ultrasound imaging plays a crucial role in the detection and treatment of thyroid cancer. However, owing to the diversity of scanner vendors and imaging protocols in different hospitals, the automatic segmentation model, which has already demonstrated expert-level accuracy in the field of medical image segmentation, finds its accuracy reduced as the result of its weak generalization performance when being applied in clinically realistic environments. To address this issue, the present paper proposes ASTN, a framework for thyroid nodule segmentation achieved through a new type co-registration network. By extracting latent semantic information from the atlas and target images and utilizing in-depth features to accomplish the co-registration of nodules in thyroid ultrasound images, this framework can ensure the integrity of anatomical structure and reduce the impact on segmentation as the result of overall differences in image caused by different devices. In addition, this paper also provides an atlas selection algorithm to mitigate the difficulty of co-registration. As shown by the evaluation results collected from the datasets of different devices, thanks to the method we proposed, the model generalization has been greatly improved while maintaining a high level of segmentation accuracy.</p>
  </details>
</details>
<details>
  <summary>84. <b>标题：Physics-guided Noise Neural Proxy for Low-light Raw Image Denoising</b></summary>
  <p><b>编号</b>：[312]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.09126</p>
  <p><b>作者</b>：Hansen Feng,  Lizhi Wang,  Yiqi Huang,  Yuzhi Wang,  Hua Huang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Low-light raw image, raw image denoising, noise neural proxy, image denoising plays, noise neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Low-light raw image denoising plays a crucial role in mobile photography, and learning-based methods have become the mainstream approach. Training the learning-based methods with synthetic data emerges as an efficient and practical alternative to paired real data. However, the quality of synthetic data is inherently limited by the low accuracy of the noise model, which decreases the performance of low-light raw image denoising. In this paper, we develop a novel framework for accurate noise modeling that learns a physics-guided noise neural proxy (PNNP) from dark frames. PNNP integrates three efficient techniques: physics-guided noise decoupling (PND), physics-guided proxy model (PPM), and differentiable distribution-oriented loss (DDL). The PND decouples the dark frame into different components and handles different levels of noise in a flexible manner, which reduces the complexity of the noise neural proxy. The PPM incorporates physical priors to effectively constrain the generated noise, which promotes the accuracy of the noise neural proxy. The DDL provides explicit and reliable supervision for noise modeling, which promotes the precision of the noise neural proxy. Extensive experiments on public low-light raw image denoising datasets and real low-light imaging scenarios demonstrate the superior performance of our PNNP framework.</p>
  </details>
</details>
<details>
  <summary>85. <b>标题：Faster 3D cardiac CT segmentation with Vision Transformers</b></summary>
  <p><b>编号</b>：[314]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.09099</p>
  <p><b>作者</b>：Lee Jollans,  Mariana Bustamante,  Lilian Henriksson,  Anders Persson,  Tino Ebbers</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：surgical intervention planning, personalized blood flow, blood flow simulations, intervention planning, essential for personalized</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Accurate segmentation of the heart is essential for personalized blood flow simulations and surgical intervention planning. A recent advancement in image recognition is the Vision Transformer (ViT), which expands the field of view to encompass a greater portion of the global image context. We adapted ViT for three-dimensional volume inputs. Cardiac computed tomography (CT) volumes from 39 patients, featuring up to 20 timepoints representing the complete cardiac cycle, were utilized. Our network incorporates a modified ResNet50 block as well as a ViT block and employs cascade upsampling with skip connections. Despite its increased model complexity, our hybrid Transformer-Residual U-Net framework, termed TRUNet, converges in significantly less time than residual U-Net while providing comparable or superior segmentations of the left ventricle, left atrium, left atrial appendage, ascending aorta, and pulmonary veins. TRUNet offers more precise vessel boundary segmentation and better captures the heart's overall anatomical structure compared to residual U-Net, as confirmed by the absence of extraneous clusters of missegmented voxels. In terms of both performance and training speed, TRUNet exceeded U-Net, a commonly used segmentation architecture, making it a promising tool for 3D semantic segmentation tasks in medical imaging. The code for TRUNet is available at this http URL.</p>
  </details>
</details>
<details>
  <summary>86. <b>标题：Self supervised convolutional kernel based handcrafted feature  harmonization: Enhanced left ventricle hypertension disease phenotyping on  echocardiography</b></summary>
  <p><b>编号</b>：[318]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08897</p>
  <p><b>作者</b>：Jina Lee,  Youngtaek Hong,  Dawun Jeong,  Yeonggul Jang,  Sihyeon Jeong,  Taekgeun Jung,  Yeonyee E. Yoon,  Inki Moon,  Seung-Ah Lee,  Hyuk-Jae Chang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：extracts quantitative handcrafted, extracts quantitative, Left Ventricular Hypertrophy, Hypertensive Heart Disease, medical imaging technique</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Radiomics, a medical imaging technique, extracts quantitative handcrafted features from images to predict diseases. Harmonization in those features ensures consistent feature extraction across various imaging devices and protocols. Methods for harmonization include standardized imaging protocols, statistical adjustments, and evaluating feature robustness. Myocardial diseases such as Left Ventricular Hypertrophy (LVH) and Hypertensive Heart Disease (HHD) are diagnosed via echocardiography, but variable imaging settings pose challenges. Harmonization techniques are crucial for applying handcrafted features in disease diagnosis in such scenario. Self-supervised learning (SSL) enhances data understanding within limited datasets and adapts to diverse data settings. ConvNeXt-V2 integrates convolutional layers into SSL, displaying superior performance in various tasks. This study focuses on convolutional filters within SSL, using them as preprocessing to convert images into feature maps for handcrafted feature harmonization. Our proposed method excelled in harmonization evaluation and exhibited superior LVH classification performance compared to existing methods.</p>
  </details>
</details>
<details>
  <summary>87. <b>标题：Two-Stage Deep Learning Framework for Quality Assessment of Left Atrial  Late Gadolinium Enhanced MRI Images</b></summary>
  <p><b>编号</b>：[324]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08805</p>
  <p><b>作者</b>：K M Arefeen Sultan,  Benjamin Orkild,  Alan Morris,  Eugene Kholmovski,  Erik Bieging,  Eugene Kwan,  Ravi Ranjan,  Ed DiBella,  Shireen Elhabian</p>
  <p><b>备注</b>：Accepted to STACOM 2023. 11 pages, 3 figures</p>
  <p><b>关键词</b>：late gadolinium enhancement, atrial fibrillation relies, MRI images, late gadolinium, gadolinium enhancement</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Accurate assessment of left atrial fibrosis in patients with atrial fibrillation relies on high-quality 3D late gadolinium enhancement (LGE) MRI images. However, obtaining such images is challenging due to patient motion, changing breathing patterns, or sub-optimal choice of pulse sequence parameters. Automated assessment of LGE-MRI image diagnostic quality is clinically significant as it would enhance diagnostic accuracy, improve efficiency, ensure standardization, and contributes to better patient outcomes by providing reliable and high-quality LGE-MRI scans for fibrosis quantification and treatment planning. To address this, we propose a two-stage deep-learning approach for automated LGE-MRI image diagnostic quality assessment. The method includes a left atrium detector to focus on relevant regions and a deep network to evaluate diagnostic quality. We explore two training strategies, multi-task learning, and pretraining using contrastive learning, to overcome limited annotated data in medical imaging. Contrastive Learning result shows about $4\%$, and $9\%$ improvement in F1-Score and Specificity compared to Multi-Task learning when there's limited data.</p>
  </details>
</details>
<details>
  <summary>88. <b>标题：Intelligent Scoliosis Screening and Diagnosis: A Survey</b></summary>
  <p><b>编号</b>：[328]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08756</p>
  <p><b>作者</b>：Zhang Zhenlin,  Pu Lixin,  Li Ang,  Zhang Jun,  Li Xianjie,  Fan Jipeng</p>
  <p><b>备注</b>：in Chinese language</p>
  <p><b>关键词</b>：three-dimensional spinal deformity, spinal deformity, thoracic deformity, abnormal morphologies, pelvic tilt</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Scoliosis is a three-dimensional spinal deformity, which may lead to abnormal morphologies, such as thoracic deformity, and pelvic tilt. Severe patients may suffer from nerve damage and urinary abnormalities. At present, the number of scoliosis patients in primary and secondary schools has exceeded five million in China, the incidence rate is about 3% to 5% which is growing every year. The research on scoliosis, therefore, has important clinical value. This paper systematically introduces computer-assisted scoliosis screening and diagnosis as well as analyzes the advantages and limitations of different algorithm models in the current issue field. Moreover, the paper also discusses the current development bottlenecks in this field and looks forward to future development trends.</p>
  </details>
</details>
<details>
  <summary>89. <b>标题：BrainVoxGen: Deep learning framework for synthesis of Ultrasound to MRI</b></summary>
  <p><b>编号</b>：[336]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08608</p>
  <p><b>作者</b>：Shubham Singh,  Dr. Mrunal Bewoor,  Ammar Ranapurwala,  Satyam Rai,  Sheetal Patil</p>
  <p><b>备注</b>：6 pages</p>
  <p><b>关键词</b>：learning framework aimed, deep learning framework, GAN model, aimed at synthesizing, three-dimensional ultrasound images</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The study presents a deep learning framework aimed at synthesizing 3D MRI volumes from three-dimensional ultrasound images of the brain utilizing the Pix2Pix GAN model. The process involves inputting a 3D volume of ultrasounds into a UNET generator and patch discriminator, generating a corresponding 3D volume of MRI. Model performance was evaluated using losses on the discriminator and generator applied to a dataset of 3D ultrasound and MRI images. The results indicate that the synthesized MRI images exhibit some similarity to the expected outcomes. Despite challenges related to dataset size, computational resources, and technical complexities, the method successfully generated MRI volume with a satisfactory similarity score meant to serve as a baseline for further research. It underscores the potential of deep learning-based volume synthesis techniques for ultrasound to MRI conversion, showcasing their viability for medical applications. Further refinement and exploration are warranted for enhanced clinical relevance.</p>
  </details>
</details>
<details>
  <summary>90. <b>标题：Domain Generalization for Medical Image Analysis: A Survey</b></summary>
  <p><b>编号</b>：[339]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08598</p>
  <p><b>作者</b>：Jee Seok Yoon,  Kwanseok Oh,  Yooseung Shin,  Maciej A. Mazurowski,  Heung-Il Suk</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Medical Image Analysis, made significant contributions, Medical Image, Image Analysis, medicine and healthcare</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Medical Image Analysis (MedIA) has become an essential tool in medicine and healthcare, aiding in disease diagnosis, prognosis, and treatment planning, and recent successes in deep learning (DL) have made significant contributions to its advances. However, DL models for MedIA remain challenging to deploy in real-world situations, failing for generalization under the distributional gap between training and testing samples, known as a distribution shift problem. Researchers have dedicated their efforts to developing various DL methods to adapt and perform robustly on unknown and out-of-distribution data distributions. This paper comprehensively reviews domain generalization studies specifically tailored for MedIA. We provide a holistic view of how domain generalization techniques interact within the broader MedIA system, going beyond methodologies to consider the operational implications on the entire MedIA workflow. Specifically, we categorize domain generalization methods into data-level, feature-level, model-level, and analysis-level methods. We show how those methods can be used in various stages of the MedIA workflow with DL equipped from data acquisition to model prediction and analysis. Furthermore, we include benchmark datasets and applications used to evaluate these approaches and analyze the strengths and weaknesses of various methods, unveiling future research opportunities.</p>
  </details>
</details>
<details>
  <summary>91. <b>标题：Octopus: Embodied Vision-Language Programmer from Environmental Feedback</b></summary>
  <p><b>编号</b>：[342]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08588</p>
  <p><b>作者</b>：Jingkang Yang,  Yuhao Dong,  Shuai Liu,  Bo Li,  Ziyue Wang,  Chencheng Jiang,  Haoran Tan,  Jiamu Kang,  Yuanhan Zhang,  Kaiyang Zhou,  Ziwei Liu</p>
  <p><b>备注</b>：Project Page: this https URL, Codebase: this https URL</p>
  <p><b>关键词</b>：achieved substantial progress, Large vision-language models, Large vision-language, perception and reasoning, achieved substantial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large vision-language models (VLMs) have achieved substantial progress in multimodal perception and reasoning. Furthermore, when seamlessly integrated into an embodied agent, it signifies a crucial stride towards the creation of autonomous and context-aware systems capable of formulating plans and executing commands with precision. In this paper, we introduce Octopus, a novel VLM designed to proficiently decipher an agent's vision and textual task objectives and to formulate intricate action sequences and generate executable code. Our design allows the agent to adeptly handle a wide spectrum of tasks, ranging from mundane daily chores in simulators to sophisticated interactions in complex video games. Octopus is trained by leveraging GPT-4 to control an explorative agent to generate training data, i.e., action blueprints and the corresponding executable code, within our experimental environment called OctoVerse. We also collect the feedback that allows the enhanced training scheme of Reinforcement Learning with Environmental Feedback (RLEF). Through a series of experiments, we illuminate Octopus's functionality and present compelling results, and the proposed RLEF turns out to refine the agent's decision-making. By open-sourcing our model architecture, simulator, and dataset, we aspire to ignite further innovation and foster collaborative applications within the broader embodied AI community.</p>
  </details>
</details>
<details>
  <summary>92. <b>标题：Is Generalized Dynamic Novel View Synthesis from Monocular Videos  Possible Today?</b></summary>
  <p><b>编号</b>：[343]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08587</p>
  <p><b>作者</b>：Xiaoming Zhao,  Alex Colburn,  Fangchang Ma,  Miguel Angel Bautista,  Joshua M. Susskind,  Alexander G. Schwing</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：Rendering scenes observed, challenging problem, Rendering scenes, scene-specific, optimization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Rendering scenes observed in a monocular video from novel viewpoints is a challenging problem. For static scenes the community has studied both scene-specific optimization techniques, which optimize on every test scene, and generalized techniques, which only run a deep net forward pass on a test scene. In contrast, for dynamic scenes, scene-specific optimization techniques exist, but, to our best knowledge, there is currently no generalized method for dynamic novel view synthesis from a given monocular video. To answer whether generalized dynamic novel view synthesis from monocular videos is possible today, we establish an analysis framework based on existing techniques and work toward the generalized approach. We find a pseudo-generalized process without scene-specific appearance optimization is possible, but geometrically and temporally consistent depth estimates are needed. Despite no scene-specific appearance optimization, the pseudo-generalized approach improves upon some scene-specific methods.</p>
  </details>
</details>
<details>
  <summary>93. <b>标题：PonderV2: Pave the Way for 3D Foundation Model with A Universal  Pre-training Paradigm</b></summary>
  <p><b>编号</b>：[344]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08586</p>
  <p><b>作者</b>：Haoyi Zhu,  Honghui Yang,  Xiaoyang Wu,  Di Huang,  Sha Zhang,  Xianglong He,  Tong He,  Hengshuang Zhao,  Chunhua Shen,  Yu Qiao,  Wanli Ouyang</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2301.00157</p>
  <p><b>关键词</b>：poses considerably greater, computer vision foundational, model poses considerably, numerous NLP, vision foundational models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In contrast to numerous NLP and 2D computer vision foundational models, the learning of a robust and highly generalized 3D foundational model poses considerably greater challenges. This is primarily due to the inherent data variability and the diversity of downstream tasks. In this paper, we introduce a comprehensive 3D pre-training framework designed to facilitate the acquisition of efficient 3D representations, thereby establishing a pathway to 3D foundational models. Motivated by the fact that informative 3D features should be able to encode rich geometry and appearance cues that can be utilized to render realistic images, we propose a novel universal paradigm to learn point cloud representations by differentiable neural rendering, serving as a bridge between 3D and 2D worlds. We train a point cloud encoder within a devised volumetric neural renderer by comparing the rendered images with the real images. Notably, our approach demonstrates the seamless integration of the learned 3D encoder into diverse downstream tasks. These tasks encompass not only high-level challenges such as 3D detection and segmentation but also low-level objectives like 3D reconstruction and image synthesis, spanning both indoor and outdoor scenarios. Besides, we also illustrate the capability of pre-training a 2D backbone using the proposed universal methodology, surpassing conventional pre-training methods by a large margin. For the first time, PonderV2 achieves state-of-the-art performance on 11 indoor and outdoor benchmarks. The consistent improvements in various settings imply the effectiveness of the proposed method. Code and models will be made available at this https URL.</p>
  </details>
</details>
<details>
  <summary>94. <b>标题：Im4D: High-Fidelity and Real-Time Novel View Synthesis for Dynamic  Scenes</b></summary>
  <p><b>编号</b>：[345]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08585</p>
  <p><b>作者</b>：Haotong Lin,  Sida Peng,  Zhen Xu,  Tao Xie,  Xingyi He,  Hujun Bao,  Xiaowei Zhou</p>
  <p><b>备注</b>：SIGGRAPH Asia 2023; Project page: this https URL</p>
  <p><b>关键词</b>：paper aims, aims to tackle, tackle the challenge, dynamic view synthesis, multi-view image-based appearance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper aims to tackle the challenge of dynamic view synthesis from multi-view videos. The key observation is that while previous grid-based methods offer consistent rendering, they fall short in capturing appearance details of a complex dynamic scene, a domain where multi-view image-based rendering methods demonstrate the opposite properties. To combine the best of two worlds, we introduce Im4D, a hybrid scene representation that consists of a grid-based geometry representation and a multi-view image-based appearance representation. Specifically, the dynamic geometry is encoded as a 4D density function composed of spatiotemporal feature planes and a small MLP network, which globally models the scene structure and facilitates the rendering consistency. We represent the scene appearance by the original multi-view videos and a network that learns to predict the color of a 3D point from image features, instead of memorizing detailed appearance totally with networks, thereby naturally making the learning of networks easier. Our method is evaluated on five dynamic view synthesis datasets including DyNeRF, ZJU-MoCap, NHR, DNA-Rendering and ENeRF-Outdoor datasets. The results show that Im4D exhibits state-of-the-art performance in rendering quality and can be trained efficiently, while realizing real-time rendering with a speed of 79.8 FPS for 512x512 images, on a single RTX 3090 GPU.</p>
  </details>
</details>
<details>
  <summary>95. <b>标题：Is ImageNet worth 1 video? Learning strong image encoders from 1 long  unlabelled video</b></summary>
  <p><b>编号</b>：[346]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08584</p>
  <p><b>作者</b>：Shashanka Venkataramanan,  Mamshad Nayeem Rizve,  João Carreira,  Yuki M. Asano,  Yannis Avrithis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：annotation is unnecessary, unlocked the potential, potential of scaling, Walking Tours, Walking Tours video</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Self-supervised learning has unlocked the potential of scaling up pretraining to billions of images, since annotation is unnecessary. But are we making the best use of data? How more economical can we be? In this work, we attempt to answer this question by making two contributions. First, we investigate first-person videos and introduce a "Walking Tours" dataset. These videos are high-resolution, hours-long, captured in a single uninterrupted take, depicting a large number of objects and actions with natural scene transitions. They are unlabeled and uncurated, thus realistic for self-supervision and comparable with human learning.
Second, we introduce a novel self-supervised image pretraining method tailored for learning from continuous videos. Existing methods typically adapt image-based pretraining approaches to incorporate more frames. Instead, we advocate a "tracking to learn to recognize" approach. Our method called DoRA, leads to attention maps that Discover and tRAck objects over time in an end-to-end manner, using transformer cross-attention. We derive multiple views from the tracks and use them in a classical self-supervised distillation loss. Using our novel approach, a single Walking Tours video remarkably becomes a strong competitor to ImageNet for several image and video downstream tasks.</p>
  </details>
</details>
<details>
  <summary>96. <b>标题：Universal Visual Decomposer: Long-Horizon Manipulation Made Easy</b></summary>
  <p><b>编号</b>：[349]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08581</p>
  <p><b>作者</b>：Zichen Zhang,  Yunshuang Li,  Osbert Bastani,  Abhishek Gupta,  Dinesh Jayaraman,  Yecheng Jason Ma,  Luca Weihs</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：encompass multiple stages, multiple stages, encompass multiple, UVD, robotic tasks stretch</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Real-world robotic tasks stretch over extended horizons and encompass multiple stages. Learning long-horizon manipulation tasks, however, is a long-standing challenge, and demands decomposing the overarching task into several manageable subtasks to facilitate policy learning and generalization to unseen tasks. Prior task decomposition methods require task-specific knowledge, are computationally intensive, and cannot readily be applied to new tasks. To address these shortcomings, we propose Universal Visual Decomposer (UVD), an off-the-shelf task decomposition method for visual long horizon manipulation using pre-trained visual representations designed for robotic control. At a high level, UVD discovers subgoals by detecting phase shifts in the embedding space of the pre-trained representation. Operating purely on visual demonstrations without auxiliary information, UVD can effectively extract visual subgoals embedded in the videos, while incurring zero additional training cost on top of standard visuomotor policy training. Goal-conditioned policies learned with UVD-discovered subgoals exhibit significantly improved compositional generalization at test time to unseen tasks. Furthermore, UVD-discovered subgoals can be used to construct goal-based reward shaping that jump-starts temporally extended exploration for reinforcement learning. We extensively evaluate UVD on both simulation and real-world tasks, and in all cases, UVD substantially outperforms baselines across imitation and reinforcement learning settings on in-domain and out-of-domain task sequences alike, validating the clear advantage of automated visual task decomposition within the simple, compact UVD framework.</p>
  </details>
</details>
<details>
  <summary>97. <b>标题：OmniControl: Control Any Joint at Any Time for Human Motion Generation</b></summary>
  <p><b>编号</b>：[350]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08580</p>
  <p><b>作者</b>：Yiming Xie,  Varun Jampani,  Lei Zhong,  Deqing Sun,  Huaizu Jiang</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：generation model based, flexible spatial control, text-conditioned human motion, human motion generation, approach named OmniControl</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a novel approach named OmniControl for incorporating flexible spatial control signals into a text-conditioned human motion generation model based on the diffusion process. Unlike previous methods that can only control the pelvis trajectory, OmniControl can incorporate flexible spatial control signals over different joints at different times with only one model. Specifically, we propose analytic spatial guidance that ensures the generated motion can tightly conform to the input control signals. At the same time, realism guidance is introduced to refine all the joints to generate more coherent motion. Both the spatial and realism guidance are essential and they are highly complementary for balancing control accuracy and motion realism. By combining them, OmniControl generates motions that are realistic, coherent, and consistent with the spatial constraints. Experiments on HumanML3D and KIT-ML datasets show that OmniControl not only achieves significant improvement over state-of-the-art methods on pelvis control but also shows promising results when incorporating the constraints over other joints.</p>
  </details>
</details>
<details>
  <summary>98. <b>标题：HyperHuman: Hyper-Realistic Human Generation with Latent Structural  Diffusion</b></summary>
  <p><b>编号</b>：[351]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08579</p>
  <p><b>作者</b>：Xian Liu,  Jian Ren,  Aliaksandr Siarohin,  Ivan Skorokhodov,  Yanyu Li,  Dahua Lin,  Xihui Liu,  Ziwei Liu,  Sergey Tulyakov</p>
  <p><b>备注</b>：Project Page: this https URL</p>
  <p><b>关键词</b>：unsolved task, human images, significant advances, remains a desirable, desirable yet unsolved</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite significant advances in large-scale text-to-image models, achieving hyper-realistic human image generation remains a desirable yet unsolved task. Existing models like Stable Diffusion and DALL-E 2 tend to generate human images with incoherent parts or unnatural poses. To tackle these challenges, our key insight is that human image is inherently structural over multiple granularities, from the coarse-level body skeleton to fine-grained spatial geometry. Therefore, capturing such correlations between the explicit appearance and latent structure in one model is essential to generate coherent and natural human images. To this end, we propose a unified framework, HyperHuman, that generates in-the-wild human images of high realism and diverse layouts. Specifically, 1) we first build a large-scale human-centric dataset, named HumanVerse, which consists of 340M images with comprehensive annotations like human pose, depth, and surface normal. 2) Next, we propose a Latent Structural Diffusion Model that simultaneously denoises the depth and surface normal along with the synthesized RGB image. Our model enforces the joint learning of image appearance, spatial relationship, and geometry in a unified network, where each branch in the model complements to each other with both structural awareness and textural richness. 3) Finally, to further boost the visual quality, we propose a Structure-Guided Refiner to compose the predicted conditions for more detailed generation of higher resolution. Extensive experiments demonstrate that our framework yields the state-of-the-art performance, generating hyper-realistic human images under diverse scenarios. Project Page: this https URL</p>
  </details>
</details>
<details>
  <summary>99. <b>标题：Visual Data-Type Understanding does not emerge from Scaling  Vision-Language Models</b></summary>
  <p><b>编号</b>：[352]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08577</p>
  <p><b>作者</b>：Vishaal Udandarao,  Max F. Burg,  Samuel Albanie,  Matthias Bethge</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：including impressive instances, yielding remarkable success, textit, Recent advances, including impressive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advances in the development of vision-language models (VLMs) are yielding remarkable success in recognizing visual semantic content, including impressive instances of compositional image understanding. Here, we introduce the novel task of \textit{Visual Data-Type Identification}, a basic perceptual skill with implications for data curation (e.g., noisy data-removal from large datasets, domain-specific retrieval) and autonomous vision (e.g., distinguishing changing weather conditions from camera lens staining). We develop two datasets consisting of animal images altered across a diverse set of 27 visual \textit{data-types}, spanning four broad categories. An extensive zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a nuanced performance landscape. While VLMs are reasonably good at identifying certain stylistic \textit{data-types}, such as cartoons and sketches, they struggle with simpler \textit{data-types} arising from basic manipulations like image rotations or additive noise. Our findings reveal that (i) model scaling alone yields marginal gains for contrastively-trained models like CLIP, and (ii) there is a pronounced drop in performance for the largest auto-regressively trained VLMs like OpenFlamingo. This finding points to a blind spot in current frontier VLMs: they excel in recognizing semantic content but fail to acquire an understanding of visual \textit{data-types} through scaling. By analyzing the pre-training distributions of these models and incorporating \textit{data-type} information into the captions during fine-tuning, we achieve a significant enhancement in performance. By exploring this previously uncharted task, we aim to set the stage for further advancing VLMs to equip them with visual data-type understanding. Code and datasets are released \href{this https URL}{here}.</p>
  </details>
</details>
<details>
  <summary>100. <b>标题：Learning to Act from Actionless Videos through Dense Correspondences</b></summary>
  <p><b>编号</b>：[353]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08576</p>
  <p><b>作者</b>：Po-Chen Ko,  Jiayuan Mao,  Yilun Du,  Shao-Hua Sun,  Joshua B. Tenenbaum</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：reliably executing diverse, construct a video-based, capable of reliably, executing diverse tasks, video-based robot policy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we present an approach to construct a video-based robot policy capable of reliably executing diverse tasks across different robots and environments from few video demonstrations without using any action annotations. Our method leverages images as a task-agnostic representation, encoding both the state and action information, and text as a general representation for specifying robot goals. By synthesizing videos that ``hallucinate'' robot executing actions and in combination with dense correspondences between frames, our approach can infer the closed-formed action to execute to an environment without the need of any explicit action labels. This unique capability allows us to train the policy solely based on RGB videos and deploy learned policies to various robotic tasks. We demonstrate the efficacy of our approach in learning policies on table-top manipulation and navigation tasks. Additionally, we contribute an open-source framework for efficient video modeling, enabling the training of high-fidelity policy models with four GPUs within a single day.</p>
  </details>
</details>
<details>
  <summary>101. <b>标题：Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic  Image Design and Generation</b></summary>
  <p><b>编号</b>：[368]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08541</p>
  <p><b>作者</b>：Zhengyuan Yang,  Jianfeng Wang,  Linjie Li,  Kevin Lin,  Chung-Ching Lin,  Zicheng Liu,  Lijuan Wang</p>
  <p><b>备注</b>：Project page at this https URL</p>
  <p><b>关键词</b>：iterative, iterative self-refinement, self-refinement, ideas, multimodal</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce ``Idea to Image,'' a system that enables multimodal iterative self-refinement with GPT-4V(ision) for automatic image design and generation. Humans can quickly identify the characteristics of different text-to-image (T2I) models via iterative explorations. This enables them to efficiently convert their high-level generation ideas into effective T2I prompts that can produce good images. We investigate if systems based on large multimodal models (LMMs) can develop analogous multimodal self-refinement abilities that enable exploring unknown models or environments via self-refining tries. Idea2Img cyclically generates revised T2I prompts to synthesize draft images, and provides directional feedback for prompt revision, both conditioned on its memory of the probed T2I model's characteristics. The iterative self-refinement brings Idea2Img various advantages over vanilla T2I models. Notably, Idea2Img can process input ideas with interleaved image-text sequences, follow ideas with design instructions, and generate images of better semantic and visual qualities. The user preference study validates the efficacy of multimodal iterative self-refinement on automatic image design and generation.</p>
  </details>
</details>
<details>
  <summary>102. <b>标题：Image2PCI -- A Multitask Learning Framework for Estimating Pavement  Condition Indices Directly from Images</b></summary>
  <p><b>编号</b>：[370]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08538</p>
  <p><b>作者</b>：Neema Jakisa Owor,  Hang Du,  Abdulateef Daud,  Armstrong Aboah,  Yaw Adu-Gyamfi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Pavement Condition Index, Condition Index, evaluating pavement performance, pavement performance based, PCI</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The Pavement Condition Index (PCI) is a widely used metric for evaluating pavement performance based on the type, extent and severity of distresses detected on a pavement surface. In recent times, significant progress has been made in utilizing deep-learning approaches to automate PCI estimation process. However, the current approaches rely on at least two separate models to estimate PCI values -- one model dedicated to determining the type and extent and another for estimating their severity. This approach presents several challenges, including complexities, high computational resource demands, and maintenance burdens that necessitate careful consideration and resolution. To overcome these challenges, the current study develops a unified multi-tasking model that predicts the PCI directly from a top-down pavement image. The proposed architecture is a multi-task model composed of one encoder for feature extraction and four decoders to handle specific tasks: two detection heads, one segmentation head and one PCI estimation head. By multitasking, we are able to extract features from the detection and segmentation heads for automatically estimating the PCI directly from the images. The model performs very well on our benchmarked and open pavement distress dataset that is annotated for multitask learning (the first of its kind). To our best knowledge, this is the first work that can estimate PCI directly from an image at real time speeds while maintaining excellent accuracy on all related tasks for crack detection and segmentation.</p>
  </details>
</details>
<details>
  <summary>103. <b>标题：XAI Benchmark for Visual Explanation</b></summary>
  <p><b>编号</b>：[371]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08537</p>
  <p><b>作者</b>：Yifei Zhang,  Siyi Gu,  James Song,  Bo Pan,  Liang Zhao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：computer vision tasks, deep learning algorithms, black box, Explainable Artificial Intelligence, visual explanation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rise of deep learning algorithms has led to significant advancements in computer vision tasks, but their "black box" nature has raised concerns regarding interpretability. Explainable AI (XAI) has emerged as a critical area of research aiming to open this "black box", and shed light on the decision-making process of AI models. Visual explanations, as a subset of Explainable Artificial Intelligence (XAI), provide intuitive insights into the decision-making processes of AI models handling visual data by highlighting influential areas in an input image. Despite extensive research conducted on visual explanations, most evaluations are model-centered since the availability of corresponding real-world datasets with ground truth explanations is scarce in the context of image data. To bridge this gap, we introduce an XAI Benchmark comprising a dataset collection from diverse topics that provide both class labels and corresponding explanation annotations for images. We have processed data from diverse domains to align with our unified visual explanation framework. We introduce a comprehensive Visual Explanation pipeline, which integrates data loading, preprocessing, experimental setup, and model evaluation processes. This structure enables researchers to conduct fair comparisons of various visual explanation techniques. In addition, we provide a comprehensive review of over 10 evaluation methods for visual explanation to assist researchers in effectively utilizing our dataset collection. To further assess the performance of existing visual explanation methods, we conduct experiments on selected datasets using various model-centered and ground truth-centered evaluation metrics. We envision this benchmark could facilitate the advancement of visual explanation models. The XAI dataset collection and easy-to-use code for evaluation are publicly accessible at this https URL.</p>
  </details>
</details>
<details>
  <summary>104. <b>标题：Animating Street View</b></summary>
  <p><b>编号</b>：[373]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08534</p>
  <p><b>作者</b>：Mengyi Shan,  Brian Curless,  Ira Kemelmacher-Shlizerman,  Steve Seitz</p>
  <p><b>备注</b>：SIGGRAPH Asia 2023 Conference Track</p>
  <p><b>关键词</b>：automatically brings street, brings street view, street view imagery, naturally behaving, animated pedestrians</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a system that automatically brings street view imagery to life by populating it with naturally behaving, animated pedestrians and vehicles. Our approach is to remove existing people and vehicles from the input image, insert moving objects with proper scale, angle, motion, and appearance, plan paths and traffic behavior, as well as render the scene with plausible occlusion and shadowing effects. The system achieves these by reconstructing the still image street scene, simulating crowd behavior, and rendering with consistent lighting, visibility, occlusions, and shadows. We demonstrate results on a diverse range of street scenes including regular still images and panoramas.</p>
  </details>
</details>
<details>
  <summary>105. <b>标题：UniPose: Detecting Any Keypoints</b></summary>
  <p><b>编号</b>：[375]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08530</p>
  <p><b>作者</b>：Jie Yang,  Ailing Zeng,  Ruimao Zhang,  Lei Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：keypoint detection, human and animal, work proposes, prompt-based keypoint detection, Keypoint</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This work proposes a unified framework called UniPose to detect keypoints of any articulated (e.g., human and animal), rigid, and soft objects via visual or textual prompts for fine-grained vision understanding and manipulation. Keypoint is a structure-aware, pixel-level, and compact representation of any object, especially articulated objects. Existing fine-grained promptable tasks mainly focus on object instance detection and segmentation but often fail to identify fine-grained granularity and structured information of image and instance, such as eyes, leg, paw, etc. Meanwhile, prompt-based keypoint detection is still under-explored. To bridge the gap, we make the first attempt to develop an end-to-end prompt-based keypoint detection framework called UniPose to detect keypoints of any objects. As keypoint detection tasks are unified in this framework, we can leverage 13 keypoint detection datasets with 338 keypoints across 1,237 categories over 400K instances to train a generic keypoint detection model. UniPose can effectively align text-to-keypoint and image-to-keypoint due to the mutual enhancement of textual and visual prompts based on the cross-modality contrastive learning optimization objectives. Our experimental results show that UniPose has strong fine-grained localization and generalization abilities across image styles, categories, and poses. Based on UniPose as a generalist keypoint detector, we hope it could serve fine-grained visual perception, understanding, and generation.</p>
  </details>
</details>
<details>
  <summary>106. <b>标题：GaussianDreamer: Fast Generation from Text to 3D Gaussian Splatting with  Point Cloud Priors</b></summary>
  <p><b>编号</b>：[376]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08529</p>
  <p><b>作者</b>：Taoran Yi,  Jiemin Fang,  Guanjun Wu,  Lingxi Xie,  Xiaopeng Zhang,  Wenyu Liu,  Qi Tian,  Xinggang Wang</p>
  <p><b>备注</b>：Work in progress. Project page: this https URL</p>
  <p><b>关键词</b>：shown impressive results, diffusion models, assets from text, impressive results, shown impressive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent times, the generation of 3D assets from text prompts has shown impressive results. Both 2D and 3D diffusion models can generate decent 3D objects based on prompts. 3D diffusion models have good 3D consistency, but their quality and generalization are limited as trainable 3D data is expensive and hard to obtain. 2D diffusion models enjoy strong abilities of generalization and fine generation, but the 3D consistency is hard to guarantee. This paper attempts to bridge the power from the two types of diffusion models via the recent explicit and efficient 3D Gaussian splatting representation. A fast 3D generation framework, named as \name, is proposed, where the 3D diffusion model provides point cloud priors for initialization and the 2D diffusion model enriches the geometry and appearance. Operations of noisy point growing and color perturbation are introduced to enhance the initialized Gaussians. Our \name can generate a high-quality 3D instance within 25 minutes on one GPU, much faster than previous methods, while the generated instances can be directly rendered in real time. Demos and code are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>107. <b>标题：4D Gaussian Splatting for Real-Time Dynamic Scene Rendering</b></summary>
  <p><b>编号</b>：[377]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08528</p>
  <p><b>作者</b>：Guanjun Wu,  Taoran Yi,  Jiemin Fang,  Lingxi Xie,  Xiaopeng Zhang,  Wei Wei,  Wenyu Liu,  Qi Tian,  Xinggang Wang</p>
  <p><b>备注</b>：Work in progress. Project page: this https URL</p>
  <p><b>关键词</b>：challenging task, important but challenging, Gaussian Splatting, rendering dynamic scenes, dynamic scene rendering</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Representing and rendering dynamic scenes has been an important but challenging task. Especially, to accurately model complex motions, high efficiency is usually hard to maintain. We introduce the 4D Gaussian Splatting (4D-GS) to achieve real-time dynamic scene rendering while also enjoying high training and storage efficiency. An efficient deformation field is constructed to model both Gaussian motions and shape deformations. Different adjacent Gaussians are connected via a HexPlane to produce more accurate position and shape deformations. Our 4D-GS method achieves real-time rendering under high resolutions, 70 FPS at a 800$\times$800 resolution on an RTX 3090 GPU, while maintaining comparable or higher quality than previous state-of-the-art methods. More demos and code are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>108. <b>标题：Unsupervised Learning of Object-Centric Embeddings for Cell Instance  Segmentation in Microscopy Images</b></summary>
  <p><b>编号</b>：[383]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08501</p>
  <p><b>作者</b>：Steffen Wolf,  Manan Lalit,  Henry Westmacott,  Katie McDole,  Jan Funke</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：biomedical applications, microscopy images, image patches, method, embed image patches</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Segmentation of objects in microscopy images is required for many biomedical applications. We introduce object-centric embeddings (OCEs), which embed image patches such that the spatial offsets between patches cropped from the same object are preserved. Those learnt embeddings can be used to delineate individual objects and thus obtain instance segmentations. Here, we show theoretically that, under assumptions commonly found in microscopy images, OCEs can be learnt through a self-supervised task that predicts the spatial offset between image patches. Together, this forms an unsupervised cell instance segmentation method which we evaluate on nine diverse large-scale microscopy datasets. Segmentations obtained with our method lead to substantially improved results, compared to state-of-the-art baselines on six out of nine datasets, and perform on par on the remaining three datasets. If ground-truth annotations are available, our method serves as an excellent starting point for supervised training, reducing the required amount of ground-truth needed by one order of magnitude, thus substantially increasing the practical applicability of our method. Source code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>109. <b>标题：Can We Edit Multimodal Large Language Models?</b></summary>
  <p><b>编号</b>：[391]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08475</p>
  <p><b>作者</b>：Siyuan Cheng,  Bozhong Tian,  Qingbin Liu,  Xi Chen,  Yongheng Wang,  Huajun Chen,  Ningyu Zhang</p>
  <p><b>备注</b>：EMNLP 2023</p>
  <p><b>关键词</b>：Large Language Models, Multimodal Large Language, Large Language, editing Multimodal Large, Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in this https URL.</p>
  </details>
</details>
<details>
  <summary>110. <b>标题：WinSyn: A High Resolution Testbed for Synthetic Data</b></summary>
  <p><b>编号</b>：[393]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08471</p>
  <p><b>作者</b>：Tom Kelly,  John Femiani,  Peter Wonka</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：high-resolution photographs, synthetic data, synthetic data generation, synthetic, data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present WinSyn, a dataset consisting of high-resolution photographs and renderings of 3D models as a testbed for synthetic-to-real research. The dataset consists of 75,739 high-resolution photographs of building windows, including traditional and modern designs, captured globally. These include 89,318 cropped subimages of windows, of which 9,002 are semantically labeled. Further, we present our domain-matched photorealistic procedural model which enables experimentation over a variety of parameter distributions and engineering approaches. Our procedural model provides a second corresponding dataset of 21,290 synthetic images. This jointly developed dataset is designed to facilitate research in the field of synthetic-to-real learning and synthetic data generation. WinSyn allows experimentation into the factors that make it challenging for synthetic data to compete with real-world data. We perform ablations using our synthetic model to identify the salient rendering, materials, and geometric factors pertinent to accuracy within the labeling task. We chose windows as a benchmark because they exhibit a large variability of geometry and materials in their design, making them ideal to study synthetic data generation in a constrained setting. We argue that the dataset is a crucial step to enable future research in synthetic data generation for deep learning.</p>
  </details>
</details>
<details>
  <summary>111. <b>标题：MotionDirector: Motion Customization of Text-to-Video Diffusion Models</b></summary>
  <p><b>编号</b>：[395]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08465</p>
  <p><b>作者</b>：Rui Zhao,  Yuchao Gu,  Jay Zhangjie Wu,  David Junhao Zhang,  Jiawei Liu,  Weijia Wu,  Jussi Keppo,  Mike Zheng Shou</p>
  <p><b>备注</b>：Project Page: this https URL</p>
  <p><b>关键词</b>：exhibited remarkable capabilities, Large-scale pre-trained diffusion, pre-trained diffusion models, motion, Large-scale pre-trained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large-scale pre-trained diffusion models have exhibited remarkable capabilities in diverse video generations. Given a set of video clips of the same motion concept, the task of Motion Customization is to adapt existing text-to-video diffusion models to generate videos with this motion. For example, generating a video with a car moving in a prescribed manner under specific camera movements to make a movie, or a video illustrating how a bear would lift weights to inspire creators. Adaptation methods have been developed for customizing appearance like subject or style, yet unexplored for motion. It is straightforward to extend mainstream adaption methods for motion customization, including full model tuning, parameter-efficient tuning of additional layers, and Low-Rank Adaptions (LoRAs). However, the motion concept learned by these methods is often coupled with the limited appearances in the training videos, making it difficult to generalize the customized motion to other appearances. To overcome this challenge, we propose MotionDirector, with a dual-path LoRAs architecture to decouple the learning of appearance and motion. Further, we design a novel appearance-debiased temporal loss to mitigate the influence of appearance on the temporal training objective. Experimental results show the proposed method can generate videos of diverse appearances for the customized motions. Our method also supports various downstream applications, such as the mixing of different videos with their appearance and motion respectively, and animating a single image with customized motions. Our code and model weights will be released.</p>
  </details>
</details>
<details>
  <summary>112. <b>标题：Proving the Potential of Skeleton Based Action Recognition to Automate  the Analysis of Manual Processes</b></summary>
  <p><b>编号</b>：[401]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08451</p>
  <p><b>作者</b>：Marlin Berger,  Frederik Cloppenburg,  Jens Eufinger,  Thomas Gries</p>
  <p><b>备注</b>：16 pages, 6 figures. Find peer-reviewed version in Proceedings of IntelliSys 2023</p>
  <p><b>关键词</b>：textiles and electronics, manual processes, manufacturing sectors, fundamental part, manual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In manufacturing sectors such as textiles and electronics, manual processes are a fundamental part of production. The analysis and monitoring of the processes is necessary for efficient production design. Traditional methods for analyzing manual processes are complex, expensive, and inflexible. Compared to established approaches such as Methods-Time-Measurement (MTM), machine learning (ML) methods promise: Higher flexibility, self-sufficient & permanent use, lower costs. In this work, based on a video stream, the current motion class in a manual assembly process is detected. With information on the current motion, Key-Performance-Indicators (KPIs) can be derived easily. A skeleton-based action recognition approach is taken, as this field recently shows major success in machine vision tasks. For skeleton-based action recognition in manual assembly, no sufficient pre-work could be found. Therefore, a ML pipeline is developed, to enable extensive research on different (pre-) processing methods and neural nets. Suitable well generalizing approaches are found, proving the potential of ML to enhance analyzation of manual processes. Models detect the current motion, performed by an operator in manual assembly, but the results can be transferred to all kinds of manual processes.</p>
  </details>
</details>
<details>
  <summary>113. <b>标题：Debias the Training of Diffusion Models</b></summary>
  <p><b>编号</b>：[407]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08442</p>
  <p><b>作者</b>：Hu Yu,  Li Shen,  Jie Huang,  Man Zhou,  Hongsheng Li,  Feng Zhao</p>
  <p><b>备注</b>：University of Science and Technology of China, Alibaba Group, The Chinese University of Hong Kong</p>
  <p><b>关键词</b>：demonstrated compelling generation, variational lower bound, score matching loss, simple denoising score, denoising score matching</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diffusion models have demonstrated compelling generation quality by optimizing the variational lower bound through a simple denoising score matching loss. In this paper, we provide theoretical evidence that the prevailing practice of using a constant loss weight strategy in diffusion models leads to biased estimation during the training phase. Simply optimizing the denoising network to predict Gaussian noise with constant weighting may hinder precise estimations of original images. To address the issue, we propose an elegant and effective weighting strategy grounded in the theoretically unbiased principle. Moreover, we conduct a comprehensive and systematic exploration to dissect the inherent bias problem deriving from constant weighting loss from the perspectives of its existence, impact and reasons. These analyses are expected to advance our understanding and demystify the inner workings of diffusion models. Through empirical evaluation, we demonstrate that our proposed debiased estimation method significantly enhances sample quality without the reliance on complex techniques, and exhibits improved efficiency compared to the baseline method both in training and sampling processes.</p>
  </details>
</details>
<details>
  <summary>114. <b>标题：Assessing of Soil Erosion Risk Through Geoinformation Sciences and  Remote Sensing -- A Review</b></summary>
  <p><b>编号</b>：[412]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08430</p>
  <p><b>作者</b>：Lachezar Filchev,  Vasil Kolev</p>
  <p><b>备注</b>：Chapter 21 (pages 54)</p>
  <p><b>关键词</b>：Soil Loss Equation, widespread erosion phenomena, Universal Soil Loss, soil erosion risk, Land Degradation Neutrality</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>During past decades a marked manifestation of widespread erosion phenomena was studied worldwide. Global conservation community has launched campaigns at local, regional and continental level in developing countries for preservation of soil resources in order not only to stop or mitigate human impact on nature but also to improve life in rural areas introducing new approaches for soil cultivation. After the adoption of Sustainable Development Goals of UNs and launching several world initiatives such as the Land Degradation Neutrality (LDN) the world came to realize the very importance of the soil resources on which the biosphere relies for its existence. The main goal of the chapter is to review different types and structures erosion models as well as their applications. Several methods using spatial analysis capabilities of geographic information systems (GIS) are in operation for soil erosion risk assessment, such as Universal Soil Loss Equation (USLE), Revised Universal Soil Loss Equation (RUSLE) in operation worldwide and in the USA and MESALES model. These and more models are being discussed in the present work alongside more experimental models and methods for assessing soil erosion risk such as Artificial Intelligence (AI), Machine and Deep Learning, etc. At the end of this work, a prospectus for the future development of soil erosion risk assessment is drawn.</p>
  </details>
</details>
<details>
  <summary>115. <b>标题：Revisiting Data Augmentation for Rotational Invariance in Convolutional  Neural Networks</b></summary>
  <p><b>编号</b>：[413]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08429</p>
  <p><b>作者</b>：Facundo Manuel Quiroga,  Franco Ronchetti,  Laura Lanzarini,  Aurelio Fernandez-Bariviera</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Convolutional Neural Networks, computer vision tasks, Convolutional Neural, Neural Networks, offer state</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Convolutional Neural Networks (CNN) offer state of the art performance in various computer vision tasks. Many of those tasks require different subtypes of affine invariances (scale, rotational, translational) to image transformations. Convolutional layers are translation equivariant by design, but in their basic form lack invariances. In this work we investigate how best to include rotational invariance in a CNN for image classification. Our experiments show that networks trained with data augmentation alone can classify rotated images nearly as well as in the normal unrotated case; this increase in representational power comes only at the cost of training time. We also compare data augmentation versus two modified CNN models for achieving rotational invariance or equivariance, Spatial Transformer Networks and Group Equivariant CNNs, finding no significant accuracy increase with these specialized methods. In the case of data augmented networks, we also analyze which layers help the network to encode the rotational invariance, which is important for understanding its limitations and how to best retrain a network with data augmentation to achieve invariance to rotation.</p>
  </details>
</details>
<details>
  <summary>116. <b>标题："SegLoc": Study on Novel Visual Self-supervised Learning Scheme (Segment  Localization) Tailored for Dense Prediction Tasks of Security Inspection  X-ray Images</b></summary>
  <p><b>编号</b>：[415]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08421</p>
  <p><b>作者</b>：Shervin Halat,  Mohammad Rahmati,  Ehsan Nazerfard</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：self-supervised learning scheme, SSL models, remarkable advancements, advancements of artificial, artificial intelligence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Lately, remarkable advancements of artificial intelligence have been attributed to the integration of self-supervised learning scheme. Despite impressive achievements within NLP, yet SSL in computer vision has not been able to stay on track comparatively. Recently, integration of contrastive learning on top of existing SSL models has established considerable progress in computer vision through which visual SSL models have outperformed their supervised counterparts. Nevertheless, most of these improvements were limited to classification tasks, and also, few works have been dedicated to evaluation of SSL models in real-world scenarios of computer vision, while the majority of works are centered around datasets containing class-wise portrait images, most notably, ImageNet. Consequently, in this work, we have considered dense prediction task of semantic segmentation in security inspection x-ray images to evaluate our proposed model Segmentation Localization. Based upon the model Instance Localization, our model SegLoc has managed to address one of the most challenging downsides of contrastive learning, i.e., false negative pairs of query embeddings. In order to do so, in contrast to baseline model InsLoc, our pretraining dataset is synthesized by cropping, transforming, then pasting already labeled segments from an available labeled dataset, foregrounds, onto instances of an unlabeled dataset, backgrounds. In our case, PIDray and SIXray datasets are considered as labeled and unlabeled datasets, respectively. Moreover, we fully harness labels by avoiding false negative pairs through implementing the idea, one queue per class, in MoCo-v2 whereby negative pairs corresponding to each query are extracted from its corresponding queue within the memory bank. Our approach has outperformed random initialization by 3% to 6%, while having underperformed supervised initialization.</p>
  </details>
</details>
<details>
  <summary>117. <b>标题：Visual Attention-Prompted Prediction and Learning</b></summary>
  <p><b>编号</b>：[416]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08420</p>
  <p><b>作者</b>：Yifei Zhang,  Siyi Gu,  Bo Pan,  Guangji Bai,  Xiaofeng Yang,  Liang Zhao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：incorporating human understanding, visual attention prompt, model predictive power, visual attention, attention</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Explanation(attention)-guided learning is a method that enhances a model's predictive power by incorporating human understanding during the training phase. While attention-guided learning has shown promising results, it often involves time-consuming and computationally expensive model retraining. To address this issue, we introduce the attention-prompted prediction technique, which enables direct prediction guided by the attention prompt without the need for model retraining. However, this approach presents several challenges, including: 1) How to incorporate the visual attention prompt into the model's decision-making process and leverage it for future predictions even in the absence of a prompt? and 2) How to handle the incomplete information from the visual attention prompt? To tackle these challenges, we propose a novel framework called Visual Attention-Prompted Prediction and Learning, which seamlessly integrates visual attention prompts into the model's decision-making process and adapts to images both with and without attention prompts for prediction. To address the incomplete information of the visual attention prompt, we introduce a perturbation-based attention map modification method. Additionally, we propose an optimization-based mask aggregation method with a new weight learning function for adaptive perturbed annotation aggregation in the attention map modification process. Our overall framework is designed to learn in an attention-prompt guided multi-task manner to enhance future predictions even for samples without attention prompts and trained in an alternating manner for better convergence. Extensive experiments conducted on two datasets demonstrate the effectiveness of our proposed framework in enhancing predictions for samples, both with and without provided prompts.</p>
  </details>
</details>
<details>
  <summary>118. <b>标题：Towards Design and Development of an ArUco Markers-Based Quantitative  Surface Tactile Sensor</b></summary>
  <p><b>编号</b>：[424]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08398</p>
  <p><b>作者</b>：Ozdemir Can Kara,  Charles Everson,  Farshid Alambeigi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Vision-based Tactile Sensor, Surface Tactile Sensor, Quantitative Surface Tactile, qualitative image outputs, Vision-based Tactile</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, with the goal of quantifying the qualitative image outputs of a Vision-based Tactile Sensor (VTS), we present the design, fabrication, and characterization of a novel Quantitative Surface Tactile Sensor (called QS-TS). QS-TS directly estimates the sensor's gel layer deformation in real-time enabling safe and autonomous tactile manipulation and servoing of delicate objects using robotic manipulators. The core of the proposed sensor is the utilization of miniature 1.5 mm x 1.5 mm synthetic square markers with inner binary patterns and a broad black border, called ArUco Markers. Each ArUco marker can provide real-time camera pose estimation that, in our design, is used as a quantitative measure for obtaining deformation of the QS-TS gel layer. Moreover, thanks to the use of ArUco markers, we propose a unique fabrication procedure that mitigates various challenges associated with the fabrication of the existing marker-based VTSs and offers an intuitive and less-arduous method for the construction of the VTS. Remarkably, the proposed fabrication facilitates the integration and adherence of markers with the gel layer to robustly and reliably obtain a quantitative measure of deformation in real-time regardless of the orientation of ArUco Markers. The performance and efficacy of the proposed QS-TS in estimating the deformation of the sensor's gel layer were experimentally evaluated and verified. Results demonstrate the phenomenal performance of the QS-TS in estimating the deformation of the gel layer with a relative error of <5%.< p>
  </5%.<></p></details>
</details>
<details>
  <summary>119. <b>标题：Hyp-UML: Hyperbolic Image Retrieval with Uncertainty-aware Metric  Learning</b></summary>
  <p><b>编号</b>：[429]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08390</p>
  <p><b>作者</b>：Shiyang Yan,  Zongxuan Liu,  Lin Xu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Metric learning, uncertainty-aware metric learning, learning, role in training, Metric</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Metric learning plays a critical role in training image retrieval and classification. It is also a key algorithm in representation learning, e.g., for feature learning and its alignment in metric space. Hyperbolic embedding has been recently developed, compared to the conventional Euclidean embedding in most of the previously developed models, and can be more effective in representing the hierarchical data structure. Second, uncertainty estimation/measurement is a long-lasting challenge in artificial intelligence. Successful uncertainty estimation can improve a machine learning model's performance, robustness, and security. In Hyperbolic space, uncertainty measurement is at least with equivalent, if not more, critical importance. In this paper, we develop a Hyperbolic image embedding with uncertainty-aware metric learning for image retrieval. We call our method Hyp-UML: Hyperbolic Uncertainty-aware Metric Learning. Our contribution are threefold: we propose an image embedding algorithm based on Hyperbolic space, with their corresponding uncertainty value; we propose two types of uncertainty-aware metric learning, for the popular Contrastive learning and conventional margin-based metric learning, respectively. We perform extensive experimental validations to prove that the proposed algorithm can achieve state-of-the-art results among related methods. The comprehensive ablation study validates the effectiveness of each component of the proposed algorithm.</p>
  </details>
</details>
<details>
  <summary>120. <b>标题：MeanAP-Guided Reinforced Active Learning for Object Detection</b></summary>
  <p><b>编号</b>：[430]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08387</p>
  <p><b>作者</b>：Zhixuan Liang,  Xingyu Zeng,  Rui Zhao,  Ping Luo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：minimal labeled data, Active learning presents, Active learning, Reinforced Active Learning, active object detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Active learning presents a promising avenue for training high-performance models with minimal labeled data, achieved by judiciously selecting the most informative instances to label and incorporating them into the task learner. Despite notable advancements in active learning for image recognition, metrics devised or learned to gauge the information gain of data, crucial for query strategy design, do not consistently align with task model performance metrics, such as Mean Average Precision (MeanAP) in object detection tasks. This paper introduces MeanAP-Guided Reinforced Active Learning for Object Detection (MAGRAL), a novel approach that directly utilizes the MeanAP metric of the task model to devise a sampling strategy employing a reinforcement learning-based sampling agent. Built upon LSTM architecture, the agent efficiently explores and selects subsequent training instances, and optimizes the process through policy gradient with MeanAP serving as reward. Recognizing the time-intensive nature of MeanAP computation at each step, we propose fast look-up tables to expedite agent training. We assess MAGRAL's efficacy across popular benchmarks, PASCAL VOC and MS COCO, utilizing different backbone architectures. Empirical findings substantiate MAGRAL's superiority over recent state-of-the-art methods, showcasing substantial performance gains. MAGRAL establishes a robust baseline for reinforced active object detection, signifying its potential in advancing the field.</p>
  </details>
</details>
<details>
  <summary>121. <b>标题：AutoVP: An Automated Visual Prompting Framework and Benchmark</b></summary>
  <p><b>编号</b>：[433]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08381</p>
  <p><b>作者</b>：Hsi-Ai Tsao,  Lei Hsiung,  Pin-Yu Chen,  Sijia Liu,  Tsung-Yi Ho</p>
  <p><b>备注</b>：Preprint. The code is available at this https URL</p>
  <p><b>关键词</b>：emerging parameter-efficient fine-tuning, parameter-efficient fine-tuning approach, adapting pre-trained vision, downstream image-classification tasks, Visual prompting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Visual prompting (VP) is an emerging parameter-efficient fine-tuning approach to adapting pre-trained vision models to solve various downstream image-classification tasks. However, there has hitherto been little systematic study of the design space of VP and no clear benchmark for evaluating its performance. To bridge this gap, we propose AutoVP, an end-to-end expandable framework for automating VP design choices, along with 12 downstream image-classification tasks that can serve as a holistic VP-performance benchmark. Our design space covers 1) the joint optimization of the prompts; 2) the selection of pre-trained models, including image classifiers and text-image encoders; and 3) model output mapping strategies, including nonparametric and trainable label mapping. Our extensive experimental results show that AutoVP outperforms the best-known current VP methods by a substantial margin, having up to 6.7% improvement in accuracy; and attains a maximum performance increase of 27.5% compared to linear-probing (LP) baseline. AutoVP thus makes a two-fold contribution: serving both as an efficient tool for hyperparameter tuning on VP design choices, and as a comprehensive benchmark that can reasonably be expected to accelerate VP's development. The source code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>122. <b>标题：Worst-Case Morphs using Wasserstein ALI and Improved MIPGAN</b></summary>
  <p><b>编号</b>：[437]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08371</p>
  <p><b>作者</b>：Una M. Kelly,  Meike Nauta,  Lu Liu,  Luuk J. Spreeuwers,  Raymond N. J. Veldhuis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Generative Adversarial Networks, biometric Face Recognition, Face Recognition, Morphs, Adversarial Networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A morph is a combination of two separate facial images and contains identity information of two different people. When used in an identity document, both people can be authenticated by a biometric Face Recognition (FR) system. Morphs can be generated using either a landmark-based approach or approaches based on deep learning such as Generative Adversarial Networks (GAN). In a recent paper, we introduced a \emph{worst-case} upper bound on how challenging morphing attacks can be for an FR system. The closer morphs are to this upper bound, the bigger the challenge they pose to FR. We introduced an approach with which it was possible to generate morphs that approximate this upper bound for a known FR system (white box), but not for unknown (black box) FR systems.
In this paper, we introduce a morph generation method that can approximate worst-case morphs even when the FR system is not known. A key contribution is that we include the goal of generating difficult morphs \emph{during} training. Our method is based on Adversarially Learned Inference (ALI) and uses concepts from Wasserstein GANs trained with Gradient Penalty, which were introduced to stabilise the training of GANs. We include these concepts to achieve similar improvement in training stability and call the resulting method Wasserstein ALI (WALI). We finetune WALI using loss functions designed specifically to improve the ability to manipulate identity information in facial images and show how it can generate morphs that are more challenging for FR systems than landmark- or GAN-based morphs. We also show how our findings can be used to improve MIPGAN, an existing StyleGAN-based morph generator.</p>
  </details>
</details>
<details>
  <summary>123. <b>标题：UniPAD: A Universal Pre-training Paradigm for Autonomous Driving</b></summary>
  <p><b>编号</b>：[438]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08370</p>
  <p><b>作者</b>：Honghui Yang,  Sha Zhang,  Di Huang,  Xiaoyang Wu,  Haoyi Zhu,  Tong He,  Shixiang Tang,  Hengshuang Zhao,  Qibo Qiu,  Binbin Lin,  Xiaofei He,  Wanli Ouyang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：effective feature learning, autonomous driving, widely acknowledged, context of autonomous, significance of effective</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the context of autonomous driving, the significance of effective feature learning is widely acknowledged. While conventional 3D self-supervised pre-training methods have shown widespread success, most methods follow the ideas originally designed for 2D images. In this paper, we present UniPAD, a novel self-supervised learning paradigm applying 3D volumetric differentiable rendering. UniPAD implicitly encodes 3D space, facilitating the reconstruction of continuous 3D shape structures and the intricate appearance characteristics of their 2D projections. The flexibility of our method enables seamless integration into both 2D and 3D frameworks, enabling a more holistic comprehension of the scenes. We manifest the feasibility and effectiveness of UniPAD by conducting extensive experiments on various downstream 3D tasks. Our method significantly improves lidar-, camera-, and lidar-camera-based baseline by 9.1, 7.7, and 6.9 NDS, respectively. Notably, our pre-training pipeline achieves 73.2 NDS for 3D object detection and 79.4 mIoU for 3D semantic segmentation on the nuScenes validation set, achieving state-of-the-art results in comparison with previous methods. The code will be available at this https URL.</p>
  </details>
</details>
<details>
  <summary>124. <b>标题：Mapping Memes to Words for Multimodal Hateful Meme Classification</b></summary>
  <p><b>编号</b>：[440]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08368</p>
  <p><b>作者</b>：Giovanni Burbi,  Alberto Baldrati,  Lorenzo Agnolucci,  Marco Bertini,  Alberto Del Bimbo</p>
  <p><b>备注</b>：ICCV2023 CLVL Workshop</p>
  <p><b>关键词</b>：convey humor, unique form, form of communication, communication that combines, combines visual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multimodal image-text memes are prevalent on the internet, serving as a unique form of communication that combines visual and textual elements to convey humor, ideas, or emotions. However, some memes take a malicious turn, promoting hateful content and perpetuating discrimination. Detecting hateful memes within this multimodal context is a challenging task that requires understanding the intertwined meaning of text and images. In this work, we address this issue by proposing a novel approach named ISSUES for multimodal hateful meme classification. ISSUES leverages a pre-trained CLIP vision-language model and the textual inversion technique to effectively capture the multimodal semantic content of the memes. The experiments show that our method achieves state-of-the-art results on the Hateful Memes Challenge and HarMeme datasets. The code and the pre-trained models are publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>125. <b>标题：MCU: A Task-centric Framework for Open-ended Agent Evaluation in  Minecraft</b></summary>
  <p><b>编号</b>：[441]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08367</p>
  <p><b>作者</b>：Haowei Lin,  Zihao Wang,  Jianzhu Ma,  Yitao Liang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：framework named MCU, task-centric framework named, MCU framework, open-ended game environment, MCU</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To pursue the goal of creating an open-ended agent in Minecraft, an open-ended game environment with unlimited possibilities, this paper introduces a task-centric framework named MCU for Minecraft agent evaluation. The MCU framework leverages the concept of atom tasks as fundamental building blocks, enabling the generation of diverse or even arbitrary tasks. Within the MCU framework, each task is measured with six distinct difficulty scores (time consumption, operational effort, planning complexity, intricacy, creativity, novelty). These scores offer a multi-dimensional assessment of a task from different angles, and thus can reveal an agent's capability on specific facets. The difficulty scores also serve as the feature of each task, which creates a meaningful task space and unveils the relationship between tasks. For efficient evaluation of Minecraft agents employing the MCU framework, we maintain a unified benchmark, namely SkillForge, which comprises representative tasks with diverse categories and difficulty distribution. We also provide convenient filters for users to select tasks to assess specific capabilities of agents. We show that MCU has the high expressivity to cover all tasks used in recent literature on Minecraft agent, and underscores the need for advancements in areas such as creativity, precise control, and out-of-distribution generalization under the goal of open-ended Minecraft agent development.</p>
  </details>
</details>
<h1>自然语言处理</h1>
<details>
  <summary>1. <b>标题：Tree-Planner: Efficient Close-loop Task Planning with Large Language  Models</b></summary>
  <p><b>编号</b>：[7]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08582</p>
  <p><b>作者</b>：Mengkang Hu,  Yao Mu,  Xinmiao Yu,  Mingyu Ding,  Shiguang Wu,  Wenqi Shao,  Qiguang Chen,  Bin Wang,  Yu Qiao,  Ping Luo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：paper studies close-loop, studies close-loop task, Large Language Models, prompting Large Language, sequence of skills</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper studies close-loop task planning, which refers to the process of generating a sequence of skills (a plan) to accomplish a specific goal while adapting the plan based on real-time observations. Recently, prompting Large Language Models (LLMs) to generate actions iteratively has become a prevalent paradigm due to its superior performance and user-friendliness. However, this paradigm is plagued by two inefficiencies: high token consumption and redundant error correction, both of which hinder its scalability for large-scale testing and applications. To address these issues, we propose Tree-Planner, which reframes task planning with LLMs into three distinct phases: plan sampling, action tree construction, and grounded deciding. Tree-Planner starts by using an LLM to sample a set of potential plans before execution, followed by the aggregation of them to form an action tree. Finally, the LLM performs a top-down decision-making process on the tree, taking into account real-time environmental information. Experiments show that Tree-Planner achieves state-of-the-art performance while maintaining high efficiency. By decomposing LLM queries into a single plan-sampling call and multiple grounded-deciding calls, a considerable part of the prompt are less likely to be repeatedly consumed. As a result, token consumption is reduced by 92.2% compared to the previously best-performing model. Additionally, by enabling backtracking on the action tree as needed, the correction process becomes more flexible, leading to a 40.5% decrease in error corrections. Project page: this https URL</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Visual Data-Type Understanding does not emerge from Scaling  Vision-Language Models</b></summary>
  <p><b>编号</b>：[11]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08577</p>
  <p><b>作者</b>：Vishaal Udandarao,  Max F. Burg,  Samuel Albanie,  Matthias Bethge</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：including impressive instances, yielding remarkable success, textit, Recent advances, including impressive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advances in the development of vision-language models (VLMs) are yielding remarkable success in recognizing visual semantic content, including impressive instances of compositional image understanding. Here, we introduce the novel task of \textit{Visual Data-Type Identification}, a basic perceptual skill with implications for data curation (e.g., noisy data-removal from large datasets, domain-specific retrieval) and autonomous vision (e.g., distinguishing changing weather conditions from camera lens staining). We develop two datasets consisting of animal images altered across a diverse set of 27 visual \textit{data-types}, spanning four broad categories. An extensive zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a nuanced performance landscape. While VLMs are reasonably good at identifying certain stylistic \textit{data-types}, such as cartoons and sketches, they struggle with simpler \textit{data-types} arising from basic manipulations like image rotations or additive noise. Our findings reveal that (i) model scaling alone yields marginal gains for contrastively-trained models like CLIP, and (ii) there is a pronounced drop in performance for the largest auto-regressively trained VLMs like OpenFlamingo. This finding points to a blind spot in current frontier VLMs: they excel in recognizing semantic content but fail to acquire an understanding of visual \textit{data-types} through scaling. By analyzing the pre-training distributions of these models and incorporating \textit{data-type} information into the captions during fine-tuning, we achieve a significant enhancement in performance. By exploring this previously uncharted task, we aim to set the stage for further advancing VLMs to equip them with visual data-type understanding. Code and datasets are released \href{this https URL}{here}.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Transformers as Decision Makers: Provable In-Context Reinforcement  Learning via Supervised Pretraining</b></summary>
  <p><b>编号</b>：[18]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08566</p>
  <p><b>作者</b>：Licong Lin,  Yu Bai,  Song Mei</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：demonstrated remarkable in-context, remarkable in-context reinforcement, make good decisions, unseen environments, datasets have demonstrated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large transformer models pretrained on offline reinforcement learning datasets have demonstrated remarkable in-context reinforcement learning (ICRL) capabilities, where they can make good decisions when prompted with interaction trajectories from unseen environments. However, when and how transformers can be trained to perform ICRL have not been theoretically well-understood. In particular, it is unclear which reinforcement-learning algorithms transformers can perform in context, and how distribution mismatch in offline training data affects the learned algorithms. This paper provides a theoretical framework that analyzes supervised pretraining for ICRL. This includes two recently proposed training methods -- algorithm distillation and decision-pretrained transformers. First, assuming model realizability, we prove the supervised-pretrained transformer will imitate the conditional expectation of the expert algorithm given the observed trajectory. The generalization error will scale with model capacity and a distribution divergence factor between the expert and offline algorithms. Second, we show transformers with ReLU attention can efficiently approximate near-optimal online reinforcement learning algorithms like LinUCB and Thompson sampling for stochastic linear bandits, and UCB-VI for tabular Markov decision processes. This provides the first quantitative analysis of the ICRL capabilities of transformers pretrained from offline trajectories.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of  Language Models with Hypothesis Refinement</b></summary>
  <p><b>编号</b>：[21]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08559</p>
  <p><b>作者</b>：Linlu Qiu,  Liwei Jiang,  Ximing Lu,  Melanie Sclar,  Valentina Pyatkin,  Chandra Bhagavatula,  Bailin Wang,  Yoon Kim,  Yejin Choi,  Nouha Dziri,  Xiang Ren</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：derive underlying principles, inductive reasoning, ability to derive, derive underlying, underlying principles</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The ability to derive underlying principles from a handful of observations and then generalize to novel situations -- known as inductive reasoning -- is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approach achieves strong results across inductive reasoning benchmarks that require inducing causal relations, language-like instructions, and symbolic concepts. However, they also behave as puzzling inductive reasoners, showing notable performance gaps in rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules. Through empirical and human analyses, we further reveal several discrepancies between the inductive reasoning processes of LMs and humans, shedding light on both the potentials and limitations of using LMs in inductive reasoning tasks.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Do pretrained Transformers Really Learn In-context by Gradient Descent?</b></summary>
  <p><b>编号</b>：[28]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08540</p>
  <p><b>作者</b>：Lingfeng Shen,  Aayush Mishra,  Daniel Khashabi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：In-Context Learning, ICL, implicitly equivalent, Gradient Descent, language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Is In-Context Learning (ICL) implicitly equivalent to Gradient Descent (GD)? Several recent works draw analogies between the dynamics of GD and the emergent behavior of ICL in large language models. However, these works make assumptions far from the realistic natural language setting in which language models are trained. Such discrepancies between theory and practice, therefore, necessitate further investigation to validate their applicability.
We start by highlighting the weaknesses in prior works that construct Transformer weights to simulate gradient descent. Their experiments with training Transformers on ICL objective, inconsistencies in the order sensitivity of ICL and GD, sparsity of the constructed weights, and sensitivity to parameter changes are some examples of a mismatch from the real-world setting.
Furthermore, we probe and compare the ICL vs. GD hypothesis in a natural setting. We conduct comprehensive empirical analyses on language models pretrained on natural data (LLaMa-7B). Our comparisons on various performance metrics highlight the inconsistent behavior of ICL and GD as a function of various factors such as datasets, models, and number of demonstrations. We observe that ICL and GD adapt the output distribution of language models differently. These results indicate that the equivalence between ICL and GD is an open hypothesis, requires nuanced considerations and calls for further studies.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Formally Specifying the High-Level Behavior of LLM-Based Agents</b></summary>
  <p><b>编号</b>：[31]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08535</p>
  <p><b>作者</b>：Maxwell Crouse,  Ibrahim Abdelaziz,  Kinjal Basu,  Soham Dan,  Sadhana Kumaravel,  Achille Fokoue,  Pavan Kapanipathi,  Luis Lastras</p>
  <p><b>备注</b>：Preprint under review</p>
  <p><b>关键词</b>：solving challenging problems, task-specific finetuned models, LLM-based agents, expensive to procure, Linear Temporal Logic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>LLM-based agents have recently emerged as promising tools for solving challenging problems without the need for task-specific finetuned models that can be expensive to procure. Currently, the design and implementation of such agents is ad hoc, as the wide variety of tasks that LLM-based agents may be applied to naturally means there can be no one-size-fits-all approach to agent design. In this work we aim to alleviate the difficulty of designing and implementing new agents by proposing a minimalistic, high-level generation framework that simplifies the process of building agents. The framework we introduce allows the user to specify desired agent behaviors in Linear Temporal Logic (LTL). The declarative LTL specification is then used to construct a constrained decoder that guarantees the LLM will produce an output exhibiting the desired behavior. By designing our framework in this way, we obtain several benefits, including the ability to enforce complex agent behavior, the ability to formally validate prompt examples, and the ability to seamlessly incorporate content-focused logical constraints into generation. In particular, our declarative approach, in which the desired behavior is simply described without concern for how it should be implemented or enforced, enables rapid design, implementation and experimentation with different LLM-based agents. We demonstrate how the proposed framework can be used to implement recent LLM-based agents, and show how the guardrails our approach provides can lead to improvements in agent performance. In addition, we release our code for general use.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：LLM-augmented Preference Learning from Natural Language</b></summary>
  <p><b>编号</b>：[37]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08523</p>
  <p><b>作者</b>：Inwon Kang,  Sikai Ruan,  Tyler Ho,  Jui-Chien Lin,  Farhad Mohsin,  Oshani Seneviratne,  Lirong Xia</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Finding preferences expressed, Finding preferences, preferences expressed, expressed in natural, important but challenging</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Finding preferences expressed in natural language is an important but challenging task. State-of-the-art(SotA) methods leverage transformer-based models such as BERT, RoBERTa, etc. and graph neural architectures such as graph attention networks. Since Large Language Models (LLMs) are equipped to deal with larger context lengths and have much larger model sizes than the transformer-based model, we investigate their ability to classify comparative text directly. This work aims to serve as a first step towards using LLMs for the CPC task. We design and conduct a set of experiments that format the classification task into an input prompt for the LLM and a methodology to get a fixed-format response that can be automatically evaluated. Comparing performances with existing methods, we see that pre-trained LLMs are able to outperform the previous SotA models with no fine-tuning involved. Our results show that the LLMs can consistently outperform the SotA when the target text is large -- i.e. composed of multiple sentences --, and are still comparable to the SotA performance in shorter text. We also find that few-shot learning yields better performance than zero-shot learning.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：HoneyBee: Progressive Instruction Finetuning of Large Language Models  for Materials Science</b></summary>
  <p><b>编号</b>：[40]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08511</p>
  <p><b>作者</b>：Yu Song,  Santiago Miret,  Huan Zhang,  Bang Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：materials science, trustworthy data curation, LLaMa-based language model, propose an instruction-based, instruction-based process</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose an instruction-based process for trustworthy data curation in materials science (MatSci-Instruct), which we then apply to finetune a LLaMa-based language model targeted for materials science (HoneyBee). MatSci-Instruct helps alleviate the scarcity of relevant, high-quality materials science textual data available in the open literature, and HoneyBee is the first billion-parameter language model specialized to materials science. In MatSci-Instruct we improve the trustworthiness of generated data by prompting multiple commercially available large language models for generation with an Instructor module (e.g. Chat-GPT) and verification from an independent Verifier module (e.g. Claude). Using MatSci-Instruct, we construct a dataset of multiple tasks and measure the quality of our dataset along multiple dimensions, including accuracy against known facts, relevance to materials science, as well as completeness and reasonableness of the data. Moreover, we iteratively generate more targeted instructions and instruction-data in a finetuning-evaluation-feedback loop leading to progressively better performance for our finetuned HoneyBee models. Our evaluation on the MatSci-NLP benchmark shows HoneyBee's outperformance of existing language models on materials science tasks and iterative improvement in successive stages of instruction-data refinement. We study the quality of HoneyBee's language modeling through automatic evaluation and analyze case studies to further understand the model's capabilities and limitations. Our code and relevant datasets are publicly available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：The Uncertainty-based Retrieval Framework for Ancient Chinese CWS and  POS</b></summary>
  <p><b>编号</b>：[44]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08496</p>
  <p><b>作者</b>：Pengyu Wang,  Zhichen Ren</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ancient Chinese Word, Chinese Word Segmentation, Automatic analysis, related fields, ancient Chinese</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automatic analysis for modern Chinese has greatly improved the accuracy of text mining in related fields, but the study of ancient Chinese is still relatively rare. Ancient text division and lexical annotation are important parts of classical literature comprehension, and previous studies have tried to construct auxiliary dictionary and other fused knowledge to improve the performance. In this paper, we propose a framework for ancient Chinese Word Segmentation and Part-of-Speech Tagging that makes a twofold effort: on the one hand, we try to capture the wordhood semantics; on the other hand, we re-predict the uncertain samples of baseline model by introducing external knowledge. The performance of our architecture outperforms pre-trained BERT with CRF and existing tools such as Jiayan.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Prometheus: Inducing Fine-grained Evaluation Capability in Language  Models</b></summary>
  <p><b>编号</b>：[46]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08491</p>
  <p><b>作者</b>：Seungone Kim,  Jamin Shin,  Yejin Cho,  Joel Jang,  Shayne Longpre,  Hwaran Lee,  Sangdoo Yun,  Seongjin Shin,  Sungdong Kim,  James Thorne,  Minjoon Seo</p>
  <p><b>备注</b>：Work in Progress</p>
  <p><b>关键词</b>：powerful proprietary Large, proprietary Large Language, Large Language Model, Large Language, proprietary Large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, using a powerful proprietary Large Language Model (LLM) (e.g., GPT-4) as an evaluator for long-form responses has become the de facto standard. However, for practitioners with large-scale evaluation tasks and custom criteria in consideration (e.g., child-readability), using proprietary LLMs as an evaluator is unreliable due to the closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose Prometheus, a fully open-source LLM that is on par with GPT-4's evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. We first construct the Feedback Collection, a new dataset that consists of 1K fine-grained score rubrics, 20K instructions, and 100K responses and language feedback generated by GPT-4. Using the Feedback Collection, we train Prometheus, a 13B evaluator LLM that can assess any given long-form text based on customized score rubric provided by the user. Experimental results show that Prometheus scores a Pearson correlation of 0.897 with human evaluators when evaluating with 45 customized score rubrics, which is on par with GPT-4 (0.882), and greatly outperforms ChatGPT (0.392). Furthermore, measuring correlation with GPT-4 with 1222 customized score rubrics across four benchmarks (MT Bench, Vicuna Bench, Feedback Bench, Flask Eval) shows similar trends, bolstering Prometheus's capability as an evaluator LLM. Lastly, Prometheus achieves the highest accuracy on two human preference benchmarks (HHH Alignment & MT Bench Human Judgment) compared to open-sourced reward models explicitly trained on human preference datasets, highlighting its potential as an universal reward model. We open-source our code, dataset, and model at this https URL.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：GraphextQA: A Benchmark for Evaluating Graph-Enhanced Large Language  Models</b></summary>
  <p><b>编号</b>：[48]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08487</p>
  <p><b>作者</b>：Yuanchun Shen,  Ruotong Liao,  Zhen Han,  Yunpu Ma,  Volker Tresp</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：integrating graph modality, successfully integrated information, remains unexplored, audio modalities, successfully integrated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While multi-modal models have successfully integrated information from image, video, and audio modalities, integrating graph modality into large language models (LLMs) remains unexplored. This discrepancy largely stems from the inherent divergence between structured graph data and unstructured text data. Incorporating graph knowledge provides a reliable source of information, enabling potential solutions to address issues in text generation, e.g., hallucination, and lack of domain knowledge. To evaluate the integration of graph knowledge into language models, a dedicated dataset is needed. However, there is currently no benchmark dataset specifically designed for multimodal graph-language models. To address this gap, we propose GraphextQA, a question answering dataset with paired subgraphs, retrieved from Wikidata, to facilitate the evaluation and future development of graph-language models. Additionally, we introduce a baseline model called CrossGNN, which conditions answer generation on the paired graphs by cross-attending question-aware graph features at decoding. The proposed dataset is designed to evaluate graph-language models' ability to understand graphs and make use of it for answer generation. We perform experiments with language-only models and the proposed graph-language model to validate the usefulness of the paired graphs and to demonstrate the difficulty of the task.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Understanding the Humans Behind Online Misinformation: An Observational  Study Through the Lens of the COVID-19 Pandemic</b></summary>
  <p><b>编号</b>：[49]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08483</p>
  <p><b>作者</b>：Mohit Chandra,  Anush Mattapalli,  Munmun De Choudhury</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：threats to society, biggest threats, misinformation, online misinformation, online</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The proliferation of online misinformation has emerged as one of the biggest threats to society. Considerable efforts have focused on building misinformation detection models, still the perils of misinformation remain abound. Mitigating online misinformation and its ramifications requires a holistic approach that encompasses not only an understanding of its intricate landscape in relation to the complex issue and topic-rich information ecosystem online, but also the psychological drivers of individuals behind it. Adopting a time series analytic technique and robust causal inference-based design, we conduct a large-scale observational study analyzing over 32 million COVID-19 tweets and 16 million historical timeline tweets. We focus on understanding the behavior and psychology of users disseminating misinformation during COVID-19 and its relationship with the historical inclinations towards sharing misinformation on Non-COVID topics before the pandemic. Our analysis underscores the intricacies inherent to cross-topic misinformation, and highlights that users' historical inclination toward sharing misinformation is positively associated with their present behavior pertaining to misinformation sharing on emergent topics and beyond. This work may serve as a valuable foundation for designing user-centric inoculation strategies and ecologically-grounded agile interventions for effectively tackling online misinformation.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Can We Edit Multimodal Large Language Models?</b></summary>
  <p><b>编号</b>：[50]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08475</p>
  <p><b>作者</b>：Siyuan Cheng,  Bozhong Tian,  Qingbin Liu,  Xi Chen,  Yongheng Wang,  Huajun Chen,  Ningyu Zhang</p>
  <p><b>备注</b>：EMNLP 2023</p>
  <p><b>关键词</b>：Large Language Models, Multimodal Large Language, Large Language, editing Multimodal Large, Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in this https URL.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：DistillSpec: Improving Speculative Decoding via Knowledge Distillation</b></summary>
  <p><b>编号</b>：[55]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08461</p>
  <p><b>作者</b>：Yongchao Zhou,  Kaifeng Lyu,  Ankit Singh Rawat,  Aditya Krishna Menon,  Afshin Rostamizadeh,  Sanjiv Kumar,  Jean-François Kagy,  Rishabh Agarwal</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：accelerates large language, generating multiple tokens, large language model, language model inference, target model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Speculative decoding (SD) accelerates large language model inference by employing a faster draft model for generating multiple tokens, which are then verified in parallel by the larger target model, resulting in the text generated according to the target model distribution. However, identifying a compact draft model that is well-aligned with the target model is challenging. To tackle this issue, we propose DistillSpec that uses knowledge distillation to better align the draft model with the target model, before applying SD. DistillSpec makes two key design choices, which we demonstrate via systematic study to be crucial to improving the draft and target alignment: utilizing on-policy data generation from the draft model, and tailoring the divergence function to the task and decoding strategy. Notably, DistillSpec yields impressive 10 - 45% speedups over standard SD on a range of standard benchmarks, using both greedy and non-greedy sampling. Furthermore, we combine DistillSpec with lossy SD to achieve fine-grained control over the latency vs. task performance trade-off. Finally, in practical scenarios with models of varying sizes, first using distillation to boost the performance of the target model and then applying DistillSpec to train a well-aligned draft model can reduce decoding latency by 6-10x with minimal performance drop, compared to standard decoding without distillation.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative  Writing</b></summary>
  <p><b>编号</b>：[69]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08433</p>
  <p><b>作者</b>：Carlos Gómez-Rodríguez,  Paul Williams</p>
  <p><b>备注</b>：Accepted for publication in Findings of EMNLP 2023</p>
  <p><b>关键词</b>：English creative writing, English creative, creative writing, requires imagination, evaluate a range</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We evaluate a range of recent LLMs on English creative writing, a challenging and complex task that requires imagination, coherence, and style. We use a difficult, open-ended scenario chosen to avoid training data reuse: an epic narration of a single combat between Ignatius J. Reilly, the protagonist of the Pulitzer Prize-winning novel A Confederacy of Dunces (1980), and a pterodactyl, a prehistoric flying reptile. We ask several LLMs and humans to write such a story and conduct a human evalution involving various criteria such as fluency, coherence, originality, humor, and style. Our results show that some state-of-the-art commercial LLMs match or slightly outperform our writers in most dimensions; whereas open-source LLMs lag behind. Humans retain an edge in creativity, while humor shows a binary divide between LLMs that can handle it comparably to humans and those that fail at it. We discuss the implications and limitations of our study and suggest directions for future research.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Prompting Large Language Models with Chain-of-Thought for Few-Shot  Knowledge Base Question Generation</b></summary>
  <p><b>编号</b>：[85]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08395</p>
  <p><b>作者</b>：Yuanyuan Liang,  Jianing Wang,  Hanlun Zhu,  Lei Wang,  Weining Qian,  Yunshi Lan</p>
  <p><b>备注</b>：Accepted by EMNLP 2023 main conference</p>
  <p><b>关键词</b>：Knowledge Bases, natural language question, Large Language Models, aims to convert, Question Generation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The task of Question Generation over Knowledge Bases (KBQG) aims to convert a logical form into a natural language question. For the sake of expensive cost of large-scale question annotation, the methods of KBQG under low-resource scenarios urgently need to be developed. However, current methods heavily rely on annotated data for fine-tuning, which is not well-suited for few-shot question generation. The emergence of Large Language Models (LLMs) has shown their impressive generalization ability in few-shot tasks. Inspired by Chain-of-Thought (CoT) prompting, which is an in-context learning strategy for reasoning, we formulate KBQG task as a reasoning problem, where the generation of a complete question is splitted into a series of sub-question generation. Our proposed prompting method KQG-CoT first retrieves supportive logical forms from the unlabeled data pool taking account of the characteristics of the logical form. Then, we write a prompt to explicit the reasoning chain of generating complicated questions based on the selected demonstrations. To further ensure prompt quality, we extend KQG-CoT into KQG-CoT+ via sorting the logical forms by their complexity. We conduct extensive experiments over three public KBQG datasets. The results demonstrate that our prompting method consistently outperforms other prompting baselines on the evaluated datasets. Remarkably, our KQG-CoT+ method could surpass existing few-shot SoTA results of the PathQuestions dataset by 18.25, 10.72, and 10.18 absolute points on BLEU-4, METEOR, and ROUGE-L, respectively.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Towards Better Evaluation of Instruction-Following: A Case-Study in  Summarization</b></summary>
  <p><b>编号</b>：[86]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08394</p>
  <p><b>作者</b>：Ondrej Skopek,  Rahul Aralikatte,  Sian Gooding,  Victor Carbune</p>
  <p><b>备注</b>：Accepted to CoNLL 2023</p>
  <p><b>关键词</b>：follow user instructions, user instructions remains, large language models, recent advances, follow user</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite recent advances, evaluating how well large language models (LLMs) follow user instructions remains an open problem. While evaluation methods of language models have seen a rise in prompt-based approaches, limited work on the correctness of these methods has been conducted. In this work, we perform a meta-evaluation of a variety of metrics to quantify how accurately they measure the instruction-following abilities of LLMs. Our investigation is performed on grounded query-based summarization by collecting a new short-form, real-world dataset riSum, containing $300$ document-instruction pairs with $3$ answers each. All $900$ answers are rated by $3$ human annotators. Using riSum, we analyze agreement between evaluation methods and human judgment. Finally, we propose new LLM-based reference-free evaluation methods that improve upon established baselines and perform on-par with costly reference-based metrics which require high-quality summaries.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Reconstructing Materials Tetrahedron: Challenges in Materials  Information Extraction</b></summary>
  <p><b>编号</b>：[91]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08383</p>
  <p><b>作者</b>：Kausik Hira,  Mohd Zaki,  Dhruvil Sheth,  Mausam,  N M Anoop Krishnan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：propelling human progress, documented history, history of propelling, propelling human, human progress</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Discovery of new materials has a documented history of propelling human progress for centuries and more. The behaviour of a material is a function of its composition, structure, and properties, which further depend on its processing and testing conditions. Recent developments in deep learning and natural language processing have enabled information extraction at scale from published literature such as peer-reviewed publications, books, and patents. However, this information is spread in multiple formats, such as tables, text, and images, and with little or no uniformity in reporting style giving rise to several machine learning challenges. Here, we discuss, quantify, and document these outstanding challenges in automated information extraction (IE) from materials science literature towards the creation of a large materials science knowledge base. Specifically, we focus on IE from text and tables and outline several challenges with examples. We hope the present work inspires researchers to address the challenges in a coherent fashion, providing to fillip to IE for the materials knowledge base.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Improving Factual Consistency for Knowledge-Grounded Dialogue Systems  via Knowledge Enhancement and Alignment</b></summary>
  <p><b>编号</b>：[95]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08372</p>
  <p><b>作者</b>：Boyang Xue,  Weichao Wang,  Hongru Wang,  Fei Mi,  Rui Wang,  Yasheng Wang,  Lifeng Shang,  Xin Jiang,  Qun Liu,  Kam-Fai Wong</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Pretrained language models, provided knowledge source, based knowledge-grounded dialogue, Pretrained language, prone to generate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pretrained language models (PLMs) based knowledge-grounded dialogue systems are prone to generate responses that are factually inconsistent with the provided knowledge source. In such inconsistent responses, the dialogue models fail to accurately express the external knowledge they rely upon. Inspired by previous work which identified that feed-forward networks (FFNs) within Transformers are responsible for factual knowledge expressions, we investigate two methods to efficiently improve the factual expression capability {of FFNs} by knowledge enhancement and alignment respectively. We first propose \textsc{K-Dial}, which {explicitly} introduces {extended FFNs in Transformers to enhance factual knowledge expressions} given the specific patterns of knowledge-grounded dialogue inputs. Additionally, we apply the reinforcement learning for factual consistency (RLFC) method to implicitly adjust FFNs' expressions in responses by aligning with gold knowledge for the factual consistency preference. To comprehensively assess the factual consistency and dialogue quality of responses, we employ extensive automatic measures and human evaluations including sophisticated fine-grained NLI-based metrics. Experimental results on WoW and CMU\_DoG datasets demonstrate that our methods efficiently enhance the ability of the FFN module to convey factual knowledge, validating the efficacy of improving factual consistency for knowledge-grounded dialogue systems.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：MCU: A Task-centric Framework for Open-ended Agent Evaluation in  Minecraft</b></summary>
  <p><b>编号</b>：[100]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08367</p>
  <p><b>作者</b>：Haowei Lin,  Zihao Wang,  Jianzhu Ma,  Yitao Liang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：framework named MCU, task-centric framework named, MCU framework, open-ended game environment, MCU</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To pursue the goal of creating an open-ended agent in Minecraft, an open-ended game environment with unlimited possibilities, this paper introduces a task-centric framework named MCU for Minecraft agent evaluation. The MCU framework leverages the concept of atom tasks as fundamental building blocks, enabling the generation of diverse or even arbitrary tasks. Within the MCU framework, each task is measured with six distinct difficulty scores (time consumption, operational effort, planning complexity, intricacy, creativity, novelty). These scores offer a multi-dimensional assessment of a task from different angles, and thus can reveal an agent's capability on specific facets. The difficulty scores also serve as the feature of each task, which creates a meaningful task space and unveils the relationship between tasks. For efficient evaluation of Minecraft agents employing the MCU framework, we maintain a unified benchmark, namely SkillForge, which comprises representative tasks with diverse categories and difficulty distribution. We also provide convenient filters for users to select tasks to assess specific capabilities of agents. We show that MCU has the high expressivity to cover all tasks used in recent literature on Minecraft agent, and underscores the need for advancements in areas such as creativity, precise control, and out-of-distribution generalization under the goal of open-ended Minecraft agent development.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：From Large Language Models to Knowledge Graphs for Biomarker Discovery  in Cancer</b></summary>
  <p><b>编号</b>：[101]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08365</p>
  <p><b>作者</b>：Md. Rezaul Karim,  Lina Molinas Comet,  Md Shajalal,  Oya Beyan,  Dietrich Rebholz-Schuhmann,  Stefan Decker</p>
  <p><b>备注</b>：arXiv admin note: substantial text overlap with arXiv:2302.04737</p>
  <p><b>关键词</b>：disseminating specific biological, specific biological processes, therapeutic decision-making, apprehending and disseminating, disseminating specific</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Domain experts often rely on up-to-date knowledge for apprehending and disseminating specific biological processes that help them design strategies to develop prevention and therapeutic decision-making. A challenging scenario for artificial intelligence (AI) is using biomedical data (e.g., texts, imaging, omics, and clinical) to provide diagnosis and treatment recommendations for cancerous conditions. Data and knowledge about cancer, drugs, genes, proteins, and their mechanism is spread across structured (knowledge bases (KBs)) and unstructured (e.g., scientific articles) sources. A large-scale knowledge graph (KG) can be constructed by integrating these data, followed by extracting facts about semantically interrelated entities and relations. Such KGs not only allow exploration and question answering (QA) but also allow domain experts to deduce new knowledge. However, exploring and querying large-scale KGs is tedious for non-domain users due to a lack of understanding of the underlying data assets and semantic technologies. In this paper, we develop a domain KG to leverage cancer-specific biomarker discovery and interactive QA. For this, a domain ontology called OncoNet Ontology (ONO) is developed to enable semantic reasoning for validating gene-disease relations. The KG is then enriched by harmonizing the ONO, controlled vocabularies, and additional biomedical concepts from scientific articles by employing BioBERT- and SciBERT-based information extraction (IE) methods. Further, since the biomedical domain is evolving, where new findings often replace old ones, without employing up-to-date findings, there is a high chance an AI system exhibits concept drift while providing diagnosis and treatment. Therefore, we finetuned the KG using large language models (LLMs) based on more recent articles and KBs that might not have been seen by the named entity recognition models.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Defending Our Privacy With Backdoors</b></summary>
  <p><b>编号</b>：[121]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08320</p>
  <p><b>作者</b>：Dominik Hintersdorf,  Lukas Struppek,  Daniel Neider,  Kristian Kersting</p>
  <p><b>备注</b>：14 pages, 4 figures</p>
  <p><b>关键词</b>：raised significant privacy, significant privacy concerns, proliferation of large, raised significant, web-scraped data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The proliferation of large AI models trained on uncurated, often sensitive web-scraped data has raised significant privacy concerns. One of the concerns is that adversaries can extract information about the training data using privacy attacks. Unfortunately, the task of removing specific information from the models without sacrificing performance is not straightforward and has proven to be challenging. We propose a rather easy yet effective defense based on backdoor attacks to remove private information such as names of individuals from models, and focus in this work on text encoders. Specifically, through strategic insertion of backdoors, we align the embeddings of sensitive phrases with those of neutral terms-"a person" instead of the person's name. Our empirical results demonstrate the effectiveness of our backdoor-based defense on CLIP by assessing its performance using a specialized privacy attack for zero-shot classifiers. Our approach provides not only a new "dual-use" perspective on backdoor attacks, but also presents a promising avenue to enhance the privacy of individuals within models trained on uncurated web-scraped data.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Not All Demonstration Examples are Equally Beneficial: Reweighting  Demonstration Examples for In-Context Learning</b></summary>
  <p><b>编号</b>：[126]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08309</p>
  <p><b>作者</b>：Zhe Yang,  Damai Dai,  Peiyi Wang,  Zhifang Sui</p>
  <p><b>备注</b>：Findings of EMNLP 2023</p>
  <p><b>关键词</b>：In-Context Learning, Large Language Models, Language Models, input sequence, recently gained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) have recently gained the In-Context Learning (ICL) ability with the models scaling up, allowing them to quickly adapt to downstream tasks with only a few demonstration examples prepended in the input sequence. Nonetheless, the current practice of ICL treats all demonstration examples equally, which still warrants improvement, as the quality of examples is usually uneven. In this paper, we investigate how to determine approximately optimal weights for demonstration examples and how to apply them during ICL. To assess the quality of weights in the absence of additional validation data, we design a masked self-prediction (MSP) score that exhibits a strong correlation with the final ICL performance. To expedite the weight-searching process, we discretize the continuous weight space and adopt beam search. With approximately optimal weights obtained, we further propose two strategies to apply them to demonstrations at different model positions. Experimental results on 8 text classification tasks show that our approach outperforms conventional ICL by a large margin. Our code are publicly available at https:github.com/Zhe-Young/WICL.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：MProto: Multi-Prototype Network with Denoised Optimal Transport for  Distantly Supervised Named Entity Recognition</b></summary>
  <p><b>编号</b>：[130]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08298</p>
  <p><b>作者</b>：Shuhui Wu,  Yongliang Shen,  Zeqi Tan,  Wenqi Ren,  Jietian Guo,  Shiliang Pu,  Weiming Lu</p>
  <p><b>备注</b>：Accepted to EMNLP-2023, camera ready version</p>
  <p><b>关键词</b>：Distantly supervised named, locate entity mentions, Distantly supervised, aims to locate, mentions and classify</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Distantly supervised named entity recognition (DS-NER) aims to locate entity mentions and classify their types with only knowledge bases or gazetteers and unlabeled corpus. However, distant annotations are noisy and degrade the performance of NER models. In this paper, we propose a noise-robust prototype network named MProto for the DS-NER task. Different from previous prototype-based NER methods, MProto represents each entity type with multiple prototypes to characterize the intra-class variance among entity representations. To optimize the classifier, each token should be assigned an appropriate ground-truth prototype and we consider such token-prototype assignment as an optimal transport (OT) problem. Furthermore, to mitigate the noise from incomplete labeling, we propose a novel denoised optimal transport (DOT) algorithm. Specifically, we utilize the assignment result between Other class tokens and all prototypes to distinguish unlabeled entity tokens from true negatives. Experiments on several DS-NER benchmarks demonstrate that our MProto achieves state-of-the-art performance. The source code is now available on Github.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Expanding the Vocabulary of BERT for Knowledge Base Construction</b></summary>
  <p><b>编号</b>：[132]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08291</p>
  <p><b>作者</b>：Dong Yang,  Xu Wang,  Remzi Celebi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Knowledge base construction, acquiring structured information, base construction entails, facilitating question answering, Knowledge base</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Knowledge base construction entails acquiring structured information to create a knowledge base of factual and relational data, facilitating question answering, information retrieval, and semantic understanding. The challenge called "Knowledge Base Construction from Pretrained Language Models" at International Semantic Web Conference 2023 defines tasks focused on constructing knowledge base using language model. Our focus was on Track 1 of the challenge, where the parameters are constrained to a maximum of 1 billion, and the inclusion of entity descriptions within the prompt is prohibited.
Although the masked language model offers sufficient flexibility to extend its vocabulary, it is not inherently designed for multi-token prediction. To address this, we present Vocabulary Expandable BERT for knowledge base construction, which expand the language model's vocabulary while preserving semantic embeddings for newly added words. We adopt task-specific re-pre-training on masked language model to further enhance the language model.
Through experimentation, the results show the effectiveness of our approaches. Our framework achieves F1 score of 0.323 on the hidden test set and 0.362 on the validation set, both data set is provided by the challenge. Notably, our framework adopts a lightweight language model (BERT-base, 0.13 billion parameters) and surpasses the model using prompts directly on large language model (Chatgpt-3, 175 billion parameters). Besides, Token-Recode achieves comparable performances as Re-pretrain. This research advances language understanding models by enabling the direct embedding of multi-token entities, signifying a substantial step forward in link prediction task in knowledge graph and metadata completion in data management.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Optimizing Odia Braille Literacy: The Influence of Speed on Error  Reduction and Enhanced Comprehension</b></summary>
  <p><b>编号</b>：[135]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08280</p>
  <p><b>作者</b>：Monnie Parida,  Manjira Sinha,  Anupam Basu,  Pabitra Mitra</p>
  <p><b>备注</b>：4 Pages, Paper accepted in Diversity and Inclusion track at CODS-COMAD 2024</p>
  <p><b>关键词</b>：Odia Braille reading, extensive detailed analysis, Odia Braille, reading, Braille reading</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This study aims to conduct an extensive detailed analysis of the Odia Braille reading comprehension among students with visual disability. Specifically, the study explores their reading speed and hand or finger movements. The study also aims to investigate any comprehension difficulties and reading errors they may encounter. Six students from the 9th and 10th grades, aged between 14 and 16, participated in the study. We observed participants hand movements to understand how reading errors were connected to hand movement and identify the students reading difficulties. We also evaluated the participants Odia Braille reading skills, including their reading speed (in words per minute), errors, and comprehension. The average speed of Odia Braille reader is 17.64wpm. According to the study, there was a noticeable correlation between reading speed and reading errors. As reading speed decreased, the number of reading errors tended to increase. Moreover, the study established a link between reduced Braille reading errors and improved reading comprehension. In contrast, the study found that better comprehension was associated with increased reading speed. The researchers concluded with some interesting findings about preferred Braille reading patterns. These findings have important theoretical, developmental, and methodological implications for instruction.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：CP-KGC: Constrained-Prompt Knowledge Graph Completion with Large  Language Models</b></summary>
  <p><b>编号</b>：[136]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08279</p>
  <p><b>作者</b>：Rui Yang,  Li Fang,  Yi Zhou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：infer missing connections, Knowledge graph completion, graph completion, deduce and infer, infer missing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Knowledge graph completion (KGC) aims to utilize existing knowledge to deduce and infer missing connections within knowledge graphs. Text-based approaches, like SimKGC, have outperformed graph embedding methods, showcasing the promise of inductive KGC. However, the efficacy of text-based methods hinges on the quality of entity textual descriptions. In this paper, we identify the key issue of whether large language models (LLMs) can generate effective text. To mitigate hallucination in LLM-generated text in this paper, we introduce a constraint-based prompt that utilizes the entity and its textual description as contextual constraints to enhance data quality. Our Constrained-Prompt Knowledge Graph Completion (CP-KGC) method demonstrates effective inference under low resource computing conditions and surpasses prior results on the WN18RR and FB15K237 datasets. This showcases the integration of LLMs in KGC tasks and provides new directions for future research.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Impact of Co-occurrence on Factual Knowledge of Large Language Models</b></summary>
  <p><b>编号</b>：[148]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08256</p>
  <p><b>作者</b>：Cheongwoong Kang,  Jaesik Choi</p>
  <p><b>备注</b>：EMNLP 2023 Findings</p>
  <p><b>关键词</b>：make factually incorrect, factually incorrect responses, make factually, factually incorrect, incorrect responses</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) often make factually incorrect responses despite their success in various applications. In this paper, we hypothesize that relying heavily on simple co-occurrence statistics of the pre-training corpora is one of the main factors that cause factual errors. Our results reveal that LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently co-occurred words over the correct answer. Consequently, LLMs struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they are seen during finetuning. We show that co-occurrence bias remains despite scaling up model sizes or finetuning. Therefore, we suggest finetuning on a debiased dataset to mitigate the bias by filtering out biased samples whose subject-object co-occurrence count is high. Although debiased finetuning allows LLMs to memorize rare facts in the training set, it is not effective in recalling rare facts unseen during finetuning. Further research in mitigation will help build reliable language models by preventing potential errors. The code is available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Who Said That? Benchmarking Social Media AI Detection</b></summary>
  <p><b>编号</b>：[157]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08240</p>
  <p><b>作者</b>：Wanyun Cui,  Linqiu Zhang,  Qianle Wang,  Shuyang Cai</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：posing significant risks, significant risks related, social media platforms, Social media, offering both transformative</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>AI-generated text has proliferated across various online platforms, offering both transformative prospects and posing significant risks related to misinformation and manipulation. Addressing these challenges, this paper introduces SAID (Social media AI Detection), a novel benchmark developed to assess AI-text detection models' capabilities in real social media platforms. It incorporates real AI-generate text from popular social media platforms like Zhihu and Quora. Unlike existing benchmarks, SAID deals with content that reflects the sophisticated strategies employed by real AI users on the Internet which may evade detection or gain visibility, providing a more realistic and challenging evaluation landscape. A notable finding of our study, based on the Zhihu dataset, reveals that annotators can distinguish between AI-generated and human-generated texts with an average accuracy rate of 96.5%. This finding necessitates a re-evaluation of human capability in recognizing AI-generated text in today's widely AI-influenced environment. Furthermore, we present a new user-oriented AI-text detection challenge focusing on the practicality and effectiveness of identifying AI-generated text based on user information and multiple responses. The experimental results demonstrate that conducting detection tasks on actual social media platforms proves to be more challenging compared to traditional simulated AI-text detection, resulting in a decreased accuracy. On the other hand, user-oriented AI-generated text detection significantly improve the accuracy of detection.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Language Models are Universal Embedders</b></summary>
  <p><b>编号</b>：[161]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08232</p>
  <p><b>作者</b>：Xin Zhang,  Zehan Li,  Yanzhao Zhang,  Dingkun Long,  Pengjun Xie,  Meishan Zhang,  Min Zhang</p>
  <p><b>备注</b>：13 pages, in progress</p>
  <p><b>关键词</b>：key component, large language model, English, large language, revolution</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the large language model (LLM) revolution, embedding is a key component of various systems. For example, it is used to retrieve knowledge or memories for LLMs, to build content moderation filters, etc. As such cases span from English to other natural or programming languages, from retrieval to classification and beyond, it is desirable to build a unified embedding model rather than dedicated ones for each scenario. In this work, we make an initial step towards this goal, demonstrating that multiple languages (both natural and programming) pre-trained transformer decoders can embed universally when finetuned on limited English data. We provide a comprehensive practice with thorough evaluations. On English MTEB, our models achieve competitive performance on different embedding tasks by minimal training data. On other benchmarks, such as multilingual classification and code search, our models (without any supervision) perform comparably to, or even surpass heavily supervised baselines and/or APIs. These results provide evidence of a promising path towards building powerful unified embedders that can be applied across tasks and languages.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：SimCKP: Simple Contrastive Learning of Keyphrase Representations</b></summary>
  <p><b>编号</b>：[166]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08221</p>
  <p><b>作者</b>：Minseok Choi,  Chaeheon Gwak,  Seho Kim,  Si Hyeong Kim,  Jaegul Choo</p>
  <p><b>备注</b>：Accepted to Findings of EMNLP 2023</p>
  <p><b>关键词</b>：aims to generate, aims to identify, generate a set, set of summarizing, summarizing words</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Keyphrase generation (KG) aims to generate a set of summarizing words or phrases given a source document, while keyphrase extraction (KE) aims to identify them from the text. Because the search space is much smaller in KE, it is often combined with KG to predict keyphrases that may or may not exist in the corresponding document. However, current unified approaches adopt sequence labeling and maximization-based generation that primarily operate at a token level, falling short in observing and scoring keyphrases as a whole. In this work, we propose SimCKP, a simple contrastive learning framework that consists of two stages: 1) An extractor-generator that extracts keyphrases by learning context-aware phrase-level representations in a contrastive manner while also generating keyphrases that do not appear in the document; 2) A reranker that adapts scores for each generated phrase by likewise aligning their representations with the corresponding document. Experimental results on multiple benchmark datasets demonstrate the effectiveness of our proposed approach, which outperforms the state-of-the-art models by a significant margin.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Visual Question Generation in Bengali</b></summary>
  <p><b>编号</b>：[178]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08187</p>
  <p><b>作者</b>：Mahmud Hasan,  Labiba Islam,  Jannatul Ferdous Ruma,  Tasmiah Tahsin Mayeesha,  Rashedur M. Rahman</p>
  <p><b>备注</b>：19 pages including references, 4 figures and 3 tables. Accepted in the Proceedings of the Workshop on Multimodal, Multilingual Natural Language Generation and Multilingual WebNLG Challenge (MM-NLG 2023)</p>
  <p><b>关键词</b>：Visual Question Generation, Bengali Visual Question, Visual Question, generate human-like questions, Question Generation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The task of Visual Question Generation (VQG) is to generate human-like questions relevant to the given image. As VQG is an emerging research field, existing works tend to focus only on resource-rich language such as English due to the availability of datasets. In this paper, we propose the first Bengali Visual Question Generation task and develop a novel transformer-based encoder-decoder architecture that generates questions in Bengali when given an image. We propose multiple variants of models - (i) image-only: baseline model of generating questions from images without additional information, (ii) image-category and image-answer-category: guided VQG where we condition the model to generate questions based on the answer and the category of expected question. These models are trained and evaluated on the translated VQAv2.0 dataset. Our quantitative and qualitative results establish the first state of the art models for VQG task in Bengali and demonstrate that our models are capable of generating grammatically correct and relevant questions. Our quantitative results show that our image-cat model achieves a BLUE-1 score of 33.12 and BLEU-3 score of 7.56 which is the highest of the other two variants. We also perform a human evaluation to assess the quality of the generation tasks. Human evaluation suggests that image-cat model is capable of generating goal-driven and attribute-specific questions and also stays relevant to the corresponding image.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form  Narrative Text Generation</b></summary>
  <p><b>编号</b>：[179]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08185</p>
  <p><b>作者</b>：Wang You,  Wenshan Wu,  Yaobo Liang,  Shaoguang Mao,  Chenfei Wu,  Maosong Cao,  Yuzhe Cai,  Yiduo Guo,  Yan Xia,  Furu Wei,  Nan Duan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：narrative text generation, plan, Plan Extraction, long-form narrative text, Evaluation-guided Iterative Plan</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Plan-and-Write is a common hierarchical approach in long-form narrative text generation, which first creates a plan to guide the narrative writing. Following this approach, several studies rely on simply prompting large language models for planning, which often yields suboptimal results. In this paper, we propose a new framework called Evaluation-guided Iterative Plan Extraction for long-form narrative text generation (EIPE-text), which extracts plans from the corpus of narratives and utilizes the extracted plans to construct a better planner. EIPE-text has three stages: plan extraction, learning, and inference. In the plan extraction stage, it iteratively extracts and improves plans from the narrative corpus and constructs a plan corpus. We propose a question answer (QA) based evaluation mechanism to automatically evaluate the plans and generate detailed plan refinement instructions to guide the iterative improvement. In the learning stage, we build a better planner by fine-tuning with the plan corpus or in-context learning with examples in the plan corpus. Finally, we leverage a hierarchical approach to generate long-form narratives. We evaluate the effectiveness of EIPE-text in the domains of novels and storytelling. Both GPT-4-based evaluations and human evaluations demonstrate that our method can generate more coherent and relevant long-form narratives. Our code will be released in the future.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Exploring the Cognitive Knowledge Structure of Large Language Models: An  Educational Diagnostic Assessment Approach</b></summary>
  <p><b>编号</b>：[184]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08172</p>
  <p><b>作者</b>：Zheyuan Zhang,  Jifan Yu,  Juanzi Li,  Lei Hou</p>
  <p><b>备注</b>：Findings of EMNLP 2023 (Short Paper)</p>
  <p><b>关键词</b>：Large Language Models, Language Models, exhibited exceptional performance, Large Language, sparks of intelligence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) have not only exhibited exceptional performance across various tasks, but also demonstrated sparks of intelligence. Recent studies have focused on assessing their capabilities on human exams and revealed their impressive competence in different domains. However, cognitive research on the overall knowledge structure of LLMs is still lacking. In this paper, based on educational diagnostic assessment method, we conduct an evaluation using MoocRadar, a meticulously annotated human test dataset based on Bloom Taxonomy. We aim to reveal the knowledge structures of LLMs and gain insights of their cognitive capabilities. This research emphasizes the significance of investigating LLMs' knowledge and understanding the disparate cognitive patterns of LLMs. By shedding light on models' knowledge, researchers can advance development and utilization of LLMs in a more informed and effective manner.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Simplicity Level Estimate (SLE): A Learned Reference-Less Metric for  Sentence Simplification</b></summary>
  <p><b>编号</b>：[185]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08170</p>
  <p><b>作者</b>：Liam Cripwell,  Joël Legrand,  Claire Gardent</p>
  <p><b>备注</b>：Accepted to EMNLP 2023 (Main Conference)</p>
  <p><b>关键词</b>：sentence simplification remains, challenging problem, remains a challenging, Automatic evaluation, sentence simplification</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automatic evaluation for sentence simplification remains a challenging problem. Most popular evaluation metrics require multiple high-quality references -- something not readily available for simplification -- which makes it difficult to test performance on unseen domains. Furthermore, most existing metrics conflate simplicity with correlated attributes such as fluency or meaning preservation. We propose a new learned evaluation metric (SLE) which focuses on simplicity, outperforming almost all existing metrics in terms of correlation with human judgements.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Multiclass Classification of Policy Documents with Large Language Models</b></summary>
  <p><b>编号</b>：[186]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08167</p>
  <p><b>作者</b>：Erkan Gunes,  Christoffer Koch Florczak</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Classifying policy documents, policy issue topics, communication disciplines, Comparative Agendas Project, Large Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Classifying policy documents into policy issue topics has been a long-time effort in political science and communication disciplines. Efforts to automate text classification processes for social science research purposes have so far achieved remarkable results, but there is still a large room for progress. In this work, we test the prediction performance of an alternative strategy, which requires human involvement much less than full manual coding. We use the GPT 3.5 and GPT 4 models of the OpenAI, which are pre-trained instruction-tuned Large Language Models (LLM), to classify congressional bills and congressional hearings into Comparative Agendas Project's 21 major policy issue topics. We propose three use-case scenarios and estimate overall accuracies ranging from %58-83 depending on scenario and GPT model employed. The three scenarios aims at minimal, moderate, and major human interference, respectively. Overall, our results point towards the insufficiency of complete reliance on GPT with minimal human intervention, an increasing accuracy along with the human effort exerted, and a surprisingly high accuracy achieved in the most humanly demanding use-case. However, the superior use-case achieved the %83 accuracy on the %65 of the data in which the two models agreed, suggesting that a similar approach to ours can be relatively easily implemented and allow for mostly automated coding of a majority of a given dataset. This could free up resources allowing manual human coding of the remaining %35 of the data to achieve an overall higher level of accuracy while reducing costs significantly.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Ziya-VL: Bilingual Large Vision-Language Model via Multi-Task  Instruction Tuning</b></summary>
  <p><b>编号</b>：[187]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08166</p>
  <p><b>作者</b>：Junyu Lu,  Dixiang Zhang,  Xiaojun Wu,  Xinyu Gao,  Ruyi Gan,  Jiaxing Zhang,  Yan Song,  Pingjian Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Recent advancements enlarge, integrating multi-modal inputs, large language models, Recent advancements, advancements enlarge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advancements enlarge the capabilities of large language models (LLMs) in zero-shot image-to-text generation and understanding by integrating multi-modal inputs. However, such success is typically limited to English scenarios due to the lack of large-scale and high-quality non-English multi-modal resources, making it extremely difficult to establish competitive counterparts in other languages. In this paper, we introduce the Ziya-VL series, a set of bilingual large-scale vision-language models (LVLMs) designed to incorporate visual semantics into LLM for multi-modal dialogue. Composed of Ziya-VL-Base and Ziya-VL-Chat, our models adopt the Querying Transformer from BLIP-2, further exploring the assistance of optimization schemes such as instruction tuning, multi-stage training and low-rank adaptation module for visual-language alignment. In addition, we stimulate the understanding ability of GPT-4 in multi-modal scenarios, translating our gathered English image-text datasets into Chinese and generating instruction-response through the in-context learning method. The experiment results demonstrate that compared to the existing LVLMs, Ziya-VL achieves competitive performance across a wide range of English-only tasks including zero-shot image-text retrieval, image captioning, and visual question answering. The evaluation leaderboard accessed by GPT-4 also indicates that our models possess satisfactory image-text understanding and generation capabilities in Chinese multi-modal scenario dialogues. Code, demo and models are available at ~\url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Context Compression for Auto-regressive Transformers with Sentinel  Tokens</b></summary>
  <p><b>编号</b>：[192]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08152</p>
  <p><b>作者</b>：Siyu Ren,  Qi Jia,  Kenny Q. Zhu</p>
  <p><b>备注</b>：To appear at EMNLP 2023</p>
  <p><b>关键词</b>：attention module makes, compute in Transformer-based, Transformer-based LLMs, quadratic complexity, module makes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The quadratic complexity of the attention module makes it gradually become the bulk of compute in Transformer-based LLMs during generation. Moreover, the excessive key-value cache that arises when dealing with long inputs also brings severe issues on memory footprint and inference latency. In this work, we propose a plug-and-play approach that is able to incrementally compress the intermediate activation of a specified span of tokens into compact ones, thereby reducing both memory and computational cost when processing subsequent context. Experiments on both in-domain language modeling and zero-shot open-ended document generation demonstrate the advantage of our approach over sparse attention baselines in terms of fluency, n-gram matching, and semantic similarity. At last, we comprehensively profile the benefit of context compression on improving the system throughout. Code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：On the Relevance of Phoneme Duration Variability of Synthesized Training  Data for Automatic Speech Recognition</b></summary>
  <p><b>编号</b>：[202]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08132</p>
  <p><b>作者</b>：Nick Rossenbach,  Benedikt Hilmes,  Ralf Schlüter</p>
  <p><b>备注</b>：To appear at ASRU 2023</p>
  <p><b>关键词</b>：automatic speech recognition, domain mismatch tasks, improve automatic speech, speech recognition, mismatch tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Synthetic data generated by text-to-speech (TTS) systems can be used to improve automatic speech recognition (ASR) systems in low-resource or domain mismatch tasks. It has been shown that TTS-generated outputs still do not have the same qualities as real data. In this work we focus on the temporal structure of synthetic data and its relation to ASR training. By using a novel oracle setup we show how much the degradation of synthetic data quality is influenced by duration modeling in non-autoregressive (NAR) TTS. To get reference phoneme durations we use two common alignment methods, a hidden Markov Gaussian-mixture model (HMM-GMM) aligner and a neural connectionist temporal classification (CTC) aligner. Using a simple algorithm based on random walks we shift phoneme duration distributions of the TTS system closer to real durations, resulting in an improvement of an ASR system using synthetic data in a semi-supervised setting.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Fine-grained Conversational Decoding via Isotropic and Proximal Search</b></summary>
  <p><b>编号</b>：[203]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08130</p>
  <p><b>作者</b>：Yuxuan Yao,  Han Wu,  Qiling Xu,  Linqi Song</p>
  <p><b>备注</b>：To appear in EMNLP 2024</p>
  <p><b>关键词</b>：General-purpose text decoding, text decoding approaches, General-purpose text, dialogue response generation, response generation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>General-purpose text decoding approaches are usually adopted for dialogue response generation. Although the quality of the generated responses can be improved with dialogue-specific encoding methods, conversational decoding methods are still under-explored. Inspired by \citet{wu2023learning} that a good dialogue feature space should follow the rules of locality and isotropy, we present a fine-grained conversational decoding method, termed \textit{isotropic and proximal search (IPS)}. Our method is designed to generate the semantic-concentrated response, while still maintaining informativeness and discrimination against the context. Experiments show that our approach outperforms existing decoding strategies in the dialogue field across both automatic and human evaluation metrics. More in-depth analyses further confirm the effectiveness of our approach.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Who Wrote it and Why? Prompting Large-Language Models for Authorship  Verification</b></summary>
  <p><b>编号</b>：[206]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08123</p>
  <p><b>作者</b>：Chia-Yu Hung,  Zhiqiang Hu,  Yujia Hu,  Roy Ka-Wei Lee</p>
  <p><b>备注</b>：7 pages,1 figure</p>
  <p><b>关键词</b>：natural language processing, Authorship verification, plagiarism detection, language processing, computational linguistics</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Authorship verification (AV) is a fundamental task in natural language processing (NLP) and computational linguistics, with applications in forensic analysis, plagiarism detection, and identification of deceptive content. Existing AV techniques, including traditional stylometric and deep learning approaches, face limitations in terms of data requirements and lack of explainability. To address these limitations, this paper proposes PromptAV, a novel technique that leverages Large-Language Models (LLMs) for AV by providing step-by-step stylometric explanation prompts. PromptAV outperforms state-of-the-art baselines, operates effectively with limited training data, and enhances interpretability through intuitive explanations, showcasing its potential as an effective and interpretable solution for the AV task.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：QASiNa: Religious Domain Question Answering using Sirah Nabawiyah</b></summary>
  <p><b>编号</b>：[213]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08102</p>
  <p><b>作者</b>：Muhammad Razif Rizqullah (1),  Ayu Purwarianti (1),  Alham Fikri Aji (2) ((1) Bandung Institute of Technology, (2) Mohamed bin Zayed University of Artificial Intelligence)</p>
  <p><b>备注</b>：6 Pages. In Proceeding of 10th International Conference on Advanced Informatics: Concepts, Theory and Applications (ICAICTA 2023)</p>
  <p><b>关键词</b>：significant research focus, receive significant research, Large Language Model, development of Large, Substring Match</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Nowadays, Question Answering (QA) tasks receive significant research focus, particularly with the development of Large Language Model (LLM) such as Chat GPT [1]. LLM can be applied to various domains, but it contradicts the principles of information transmission when applied to the Islamic domain. In Islam we strictly regulates the sources of information and who can give interpretations or tafseer for that sources [2]. The approach used by LLM to generate answers based on its own interpretation is similar to the concept of tafseer, LLM is neither an Islamic expert nor a human which is not permitted in Islam. Indonesia is the country with the largest Islamic believer population in the world [3]. With the high influence of LLM, we need to make evaluation of LLM in religious domain. Currently, there is only few religious QA dataset available and none of them using Sirah Nabawiyah especially in Indonesian Language. In this paper, we propose the Question Answering Sirah Nabawiyah (QASiNa) dataset, a novel dataset compiled from Sirah Nabawiyah literatures in Indonesian language. We demonstrate our dataset by using mBERT [4], XLM-R [5], and IndoBERT [6] which fine-tuned with Indonesian translation of SQuAD v2.0 [7]. XLM-R model returned the best performance on QASiNa with EM of 61.20, F1-Score of 75.94, and Substring Match of 70.00. We compare XLM-R performance with Chat GPT-3.5 and GPT-4 [1]. Both Chat GPT version returned lower EM and F1-Score with higher Substring Match, the gap of EM and Substring Match get wider in GPT-4. The experiment indicate that Chat GPT tends to give excessive interpretations as evidenced by its higher Substring Match scores compared to EM and F1-Score, even after providing instruction and context. This concludes Chat GPT is unsuitable for question answering task in religious domain especially for Islamic religion.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：Promptor: A Conversational and Autonomous Prompt Generation Agent for  Intelligent Text Entry Techniques</b></summary>
  <p><b>编号</b>：[214]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08101</p>
  <p><b>作者</b>：Junxiao Shen,  John J. Dudley,  Jingyao Zheng,  Bill Byrne,  Per Ola Kristensson</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：digital interactions, language models, large language models, Text entry, language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Text entry is an essential task in our day-to-day digital interactions. Numerous intelligent features have been developed to streamline this process, making text entry more effective, efficient, and fluid. These improvements include sentence prediction and user personalization. However, as deep learning-based language models become the norm for these advanced features, the necessity for data collection and model fine-tuning increases. These challenges can be mitigated by harnessing the in-context learning capability of large language models such as GPT-3.5. This unique feature allows the language model to acquire new skills through prompts, eliminating the need for data collection and fine-tuning. Consequently, large language models can learn various text prediction techniques. We initially showed that, for a sentence prediction task, merely prompting GPT-3.5 surpassed a GPT-2 backed system and is comparable with a fine-tuned GPT-3.5 model, with the latter two methods requiring costly data collection, fine-tuning and post-processing. However, the task of prompting large language models to specialize in specific text prediction tasks can be challenging, particularly for designers without expertise in prompt engineering. To address this, we introduce Promptor, a conversational prompt generation agent designed to engage proactively with designers. Promptor can automatically generate complex prompts tailored to meet specific needs, thus offering a solution to this challenge. We conducted a user study involving 24 participants creating prompts for three intelligent text entry tasks, half of the participants used Promptor while the other half designed prompts themselves. The results show that Promptor-designed prompts result in a 35% increase in similarity and 22% in coherence over those by designers.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：ClimateNLP: Analyzing Public Sentiment Towards Climate Change Using  Natural Language Processing</b></summary>
  <p><b>编号</b>：[216]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08099</p>
  <p><b>作者</b>：Ajay Krishnan T. K.,  V. S. Anoop</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：health poses unprecedented, human health poses, Climate change, Climate change impact, Climate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Climate change's impact on human health poses unprecedented and diverse challenges. Unless proactive measures based on solid evidence are implemented, these threats will likely escalate and continue to endanger human well-being. The escalating advancements in information and communication technologies have facilitated the widespread availability and utilization of social media platforms. Individuals utilize platforms such as Twitter and Facebook to express their opinions, thoughts, and critiques on diverse subjects, encompassing the pressing issue of climate change. The proliferation of climate change-related content on social media necessitates comprehensive analysis to glean meaningful insights. This paper employs natural language processing (NLP) techniques to analyze climate change discourse and quantify the sentiment of climate change-related tweets. We use ClimateBERT, a pretrained model fine-tuned specifically for the climate change domain. The objective is to discern the sentiment individuals express and uncover patterns in public opinion concerning climate change. Analyzing tweet sentiments allows a deeper comprehension of public perceptions, concerns, and emotions about this critical global challenge. The findings from this experiment unearth valuable insights into public sentiment and the entities associated with climate change discourse. Policymakers, researchers, and organizations can leverage such analyses to understand public perceptions, identify influential actors, and devise informed strategies to address climate change challenges.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Low-Resource Clickbait Spoiling for Indonesian via Question Answering</b></summary>
  <p><b>编号</b>：[225]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08085</p>
  <p><b>作者</b>：Ni Putu Intan Maharani,  Ayu Purwarianti,  Alham Fikri Aji</p>
  <p><b>备注</b>：Accepted in ICAICTA 2023 (10th International Conference on Advanced Informatics: Concepts, Theory and Applications)</p>
  <p><b>关键词</b>：Clickbait spoiling aims, aims to generate, generate a short, short text, text to satisfy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Clickbait spoiling aims to generate a short text to satisfy the curiosity induced by a clickbait post. As it is a newly introduced task, the dataset is only available in English so far. Our contributions include the construction of manually labeled clickbait spoiling corpus in Indonesian and an evaluation on using cross-lingual zero-shot question answering-based models to tackle clikcbait spoiling for low-resource language like Indonesian. We utilize selection of multilingual language models. The experimental results suggest that XLM-RoBERTa (large) model outperforms other models for phrase and passage spoilers, meanwhile, mDeBERTa (base) model outperforms other models for multipart spoilers.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：To token or not to token: A Comparative Study of Text Representations  for Cross-Lingual Transfer</b></summary>
  <p><b>编号</b>：[229]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08078</p>
  <p><b>作者</b>：Md Mushfiqur Rahman,  Fardin Ahsan Sakib,  Fahim Faisal,  Antonios Anastasopoulos</p>
  <p><b>备注</b>：Accepted at 3RD MULTILINGUAL REPRESENTATION LEARNING (MRL) WORKSHOP, 2023</p>
  <p><b>关键词</b>：texttt, bottleneck in low-resource, low-resource cross-lingual transfer, text representation, low-resource cross-lingual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Choosing an appropriate tokenization scheme is often a bottleneck in low-resource cross-lingual transfer. To understand the downstream implications of text representation choices, we perform a comparative analysis on language models having diverse text representation modalities including 2 segmentation-based models (\texttt{BERT}, \texttt{mBERT}), 1 image-based model (\texttt{PIXEL}), and 1 character-level model (\texttt{CANINE}). First, we propose a scoring Language Quotient (LQ) metric capable of providing a weighted representation of both zero-shot and few-shot evaluation combined. Utilizing this metric, we perform experiments comprising 19 source languages and 133 target languages on three tasks (POS tagging, Dependency parsing, and NER). Our analysis reveals that image-based models excel in cross-lingual transfer when languages are closely related and share visually similar scripts. However, for tasks biased toward word meaning (POS, NER), segmentation-based models prove to be superior. Furthermore, in dependency parsing tasks where word relationships play a crucial role, models with their character-level focus, outperform others. Finally, we propose a recommendation scheme based on our findings to guide model selection according to task and language requirements.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Training Generative Question-Answering on Synthetic Data Obtained from  an Instruct-tuned Model</b></summary>
  <p><b>编号</b>：[232]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08072</p>
  <p><b>作者</b>：Kosuke Takahashi,  Takahiro Omi,  Kosuke Arima,  Tatsuya Ishigaki</p>
  <p><b>备注</b>：PACLIC 2023 short paper, 4 pages (6 pages including references), 4 figures</p>
  <p><b>关键词</b>：train question-answering systems, question-answering systems, paper presents, presents a simple, simple and cost-effective</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a simple and cost-effective method for synthesizing data to train question-answering systems. For training, fine-tuning GPT models is a common practice in resource-rich languages like English, however, it becomes challenging for non-English languages due to the scarcity of sufficient question-answer (QA) pairs. Existing approaches use question and answer generators trained on human-authored QA pairs, which involves substantial human expenses. In contrast, we use an instruct-tuned model to generate QA pairs in a zero-shot or few-shot manner. We conduct experiments to compare various strategies for obtaining QA pairs from the instruct-tuned model. The results demonstrate that a model trained on our proposed synthetic data achieves comparable performance to a model trained on manually curated datasets, without incurring human costs.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Rethinking Negative Pairs in Code Search</b></summary>
  <p><b>编号</b>：[235]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08069</p>
  <p><b>作者</b>：Haochen Li,  Xin Zhou,  Luu Anh Tuan,  Chunyan Miao</p>
  <p><b>备注</b>：Accepted to EMNLP 2023</p>
  <p><b>关键词</b>：software development efficiency, fine-tuning code search, negative samples, key component, component in fine-tuning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, contrastive learning has become a key component in fine-tuning code search models for software development efficiency and effectiveness. It pulls together positive code snippets while pushing negative samples away given search queries. Among contrastive learning, InfoNCE is the most widely used loss function due to its better performance. However, the following problems in negative samples of InfoNCE may deteriorate its representation learning: 1) The existence of false negative samples in large code corpora due to duplications. 2). The failure to explicitly differentiate between the potential relevance of negative samples. As an example, a bubble sorting algorithm example is less ``negative'' than a file saving function for the quick sorting algorithm query. In this paper, we tackle the above problems by proposing a simple yet effective Soft-InfoNCE loss that inserts weight terms into InfoNCE. In our proposed loss function, we apply three methods to estimate the weights of negative pairs and show that the vanilla InfoNCE loss is a special case of Soft-InfoNCE. Theoretically, we analyze the effects of Soft-InfoNCE on controlling the distribution of learnt code representations and on deducing a more precise mutual information estimation. We furthermore discuss the superiority of proposed loss functions with other design alternatives. Extensive experiments demonstrate the effectiveness of Soft-InfoNCE and weights estimation methods under state-of-the-art code search models on a large-scale public dataset consisting of six programming languages. Source code is available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large  Language Models</b></summary>
  <p><b>编号</b>：[247]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08041</p>
  <p><b>作者</b>：Jing Liu,  Ruihao Gong,  Xiuying Wei,  Zhiwei Dong,  Jianfei Cai,  Bohan Zhuang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, excel in NLP, Large Language, Language Models, widespread deployment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) excel in NLP, but their demands hinder their widespread deployment. While Quantization-Aware Training (QAT) offers a solution, its extensive training costs make Post-Training Quantization (PTQ) a more practical approach for LLMs. In existing studies, activation outliers in particular channels are identified as the bottleneck to PTQ accuracy. They propose to transform the magnitudes from activations to weights, which however offers limited alleviation or suffers from unstable gradients, resulting in a severe performance drop at low-bitwidth. In this paper, we propose QLLM, an accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM introduces an adaptive channel reassembly technique that reallocates the magnitude of outliers to other channels, thereby mitigating their impact on the quantization range. This is achieved by channel disassembly and channel assembly, which first breaks down the outlier channels into several sub-channels to ensure a more balanced distribution of activation magnitudes. Then similar channels are merged to maintain the original channel number for efficiency. Additionally, an adaptive strategy is designed to autonomously determine the optimal number of sub-channels for channel disassembly. To further compensate for the performance loss caused by quantization, we propose an efficient tuning method that only learns a small number of low-rank weights while freezing the pre-trained quantized model. After training, these low-rank parameters can be fused into the frozen weights without affecting inference. Extensive experiments on LLaMA-1 and LLaMA-2 show that QLLM can obtain accurate quantized models efficiently. For example, QLLM quantizes the 4-bit LLaMA-2-70B within 10 hours on a single A100-80G GPU, outperforming the previous state-of-the-art method by 7.89% on the average accuracy across five zero-shot tasks.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Exploring Large Language Models for Multi-Modal Out-of-Distribution  Detection</b></summary>
  <p><b>编号</b>：[257]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08027</p>
  <p><b>作者</b>：Yi Dai,  Hao Lang,  Kaisheng Zeng,  Fei Huang,  Yongbin Li</p>
  <p><b>备注</b>：EMNLP2023 Findings Long Paper</p>
  <p><b>关键词</b>：trustworthy machine learning, OOD detection, machine learning, OOD, essential for reliable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Out-of-distribution (OOD) detection is essential for reliable and trustworthy machine learning. Recent multi-modal OOD detection leverages textual information from in-distribution (ID) class names for visual OOD detection, yet it currently neglects the rich contextual information of ID classes. Large language models (LLMs) encode a wealth of world knowledge and can be prompted to generate descriptive features for each class. Indiscriminately using such knowledge causes catastrophic damage to OOD detection due to LLMs' hallucinations, as is observed by our analysis. In this paper, we propose to apply world knowledge to enhance OOD detection performance through selective generation from LLMs. Specifically, we introduce a consistency-based uncertainty calibration method to estimate the confidence score of each generation. We further extract visual objects from each image to fully capitalize on the aforementioned world knowledge. Extensive experiments demonstrate that our method consistently outperforms the state-of-the-art.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Harnessing Large Language Models' Empathetic Response Generation  Capabilities for Online Mental Health Counselling Support</b></summary>
  <p><b>编号</b>：[261]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08017</p>
  <p><b>作者</b>：Siyuan Brandon Loh,  Aravind Sesagiri Raamkumar</p>
  <p><b>备注</b>：7 pages, 1 figure</p>
  <p><b>关键词</b>：demonstrated remarkable performance, Large Language Models, reasoning tasks, Pathways Language Model, Large Language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) have demonstrated remarkable performance across various information-seeking and reasoning tasks. These computational systems drive state-of-the-art dialogue systems, such as ChatGPT and Bard. They also carry substantial promise in meeting the growing demands of mental health care, albeit relatively unexplored. As such, this study sought to examine LLMs' capability to generate empathetic responses in conversations that emulate those in a mental health counselling setting. We selected five LLMs: version 3.5 and version 4 of the Generative Pre-training (GPT), Vicuna FastChat-T5, Pathways Language Model (PaLM) version 2, and Falcon-7B-Instruct. Based on a simple instructional prompt, these models responded to utterances derived from the EmpatheticDialogues (ED) dataset. Using three empathy-related metrics, we compared their responses to those from traditional response generation dialogue systems, which were fine-tuned on the ED dataset, along with human-generated responses. Notably, we discovered that responses from the LLMs were remarkably more empathetic in most scenarios. We position our findings in light of catapulting advancements in creating empathetic conversational systems.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Think, Act, and Ask: Open-World Interactive Personalized Robot  Navigation</b></summary>
  <p><b>编号</b>：[288]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07968</p>
  <p><b>作者</b>：Yinpei Dai,  Run Peng,  Sikai Li,  Joyce Chai</p>
  <p><b>备注</b>：Video available at this https URL</p>
  <p><b>关键词</b>：Zero-Shot Object Navigation, Personalized Object Navigation, unknown environments, Object Navigation, Interactive persOnalized Navigation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Zero-Shot Object Navigation (ZSON) enables agents to navigate towards open-vocabulary objects in unknown environments. The existing works of ZSON mainly focus on following individual instructions to find generic object classes, neglecting the utilization of natural language interaction and the complexities of identifying user-specific objects. To address these limitations, we introduce Zero-shot Interactive Personalized Object Navigation (ZIPON), where robots need to navigate to personalized goal objects while engaging in conversations with users. To solve ZIPON, we propose a new framework termed Open-woRld Interactive persOnalized Navigation (ORION), which uses Large Language Models (LLMs) to make sequential decisions to manipulate different modules for perception, navigation and communication. Experimental results show that the performance of interactive agents that can leverage user feedback exhibits significant improvement. However, obtaining a good balance between task completion and the efficiency of navigation and interaction remains challenging for all methods. We further provide more findings on the impact of diverse user feedback forms on the agents' performance.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Clustering of Spell Variations for Proper Nouns Transliterated from the  other languages</b></summary>
  <p><b>编号</b>：[290]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07962</p>
  <p><b>作者</b>：Prathamesh Pawar</p>
  <p><b>备注</b>：3 pages, published Airial Conference 2023</p>
  <p><b>关键词</b>：proper nouns, processing and operating, Proper, text data, nouns</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>One of the prominent problems with processing and operating on text data is the non uniformity of it. Due to the change in the dialects and languages, the caliber of translation is low. This creates a unique problem while using NLP in text data; which is the spell variation arising from the inconsistent translations and transliterations. This problem can also be further aggravated by the human error arising from the various ways to write a Proper Noun from an Indian language into its English equivalent. Translating proper nouns originating from Indian languages can be complicated as some proper nouns are also used as common nouns which might be taken literally. Applications of NLP that require addresses, names and other proper nouns face this problem frequently. We propose a method to cluster these spell variations for proper nouns using ML techniques and mathematical similarity equations. We aimed to use Affinity Propagation to determine relative similarity between the tokens. The results are augmented by filtering the token-variation pair by a similarity threshold. We were able to reduce the spell variations by a considerable amount. This application can significantly reduce the amount of human annotation efforts needed for data cleansing and formatting.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：A New Approach Towards Autoformalization</b></summary>
  <p><b>编号</b>：[292]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07957</p>
  <p><b>作者</b>：Nilay Patel,  Jeffrey Flanigan,  Rahul Saha</p>
  <p><b>备注</b>：Under review at MATHAI 2023 @ NeurIPS 2023</p>
  <p><b>关键词</b>：Verifying mathematical proofs, Verifying mathematical, proofs is difficult, mathematical proofs, mathematics</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Verifying mathematical proofs is difficult, but can be automated with the assistance of a computer. Autoformalization is the task of automatically translating natural language mathematics into a formal language that can be verified by a program. This is a challenging task, and especially for higher-level mathematics found in research papers. Research paper mathematics requires large amounts of background and context. In this paper, we propose an avenue towards tackling autoformalization for research-level mathematics, by breaking the task into easier and more approachable subtasks: unlinked formalization (formalization with unlinked definitions and theorems), entity linking (linking to the proper theorems and definitions), and finally adjusting types so it passes the type checker. In addition, we present arXiv2Formal, a benchmark dataset for unlinked formalization consisting of 50 theorems formalized for the Lean theorem prover sampled from papers on arXiv.org. We welcome any contributions from the community to future versions of this dataset.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：D2 Pruning: Message Passing for Balancing Diversity and Difficulty in  Data Pruning</b></summary>
  <p><b>编号</b>：[300]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07931</p>
  <p><b>作者</b>：Adyasha Maharana,  Prateek Yadav,  Mohit Bansal</p>
  <p><b>备注</b>：17 pages (Our code is available at this https URL)</p>
  <p><b>关键词</b>：Analytical theories suggest, lower test errors, Coreset, Analytical theories, pruning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Analytical theories suggest that higher-quality data can lead to lower test errors in models trained on a fixed data budget. Moreover, a model can be trained on a lower compute budget without compromising performance if a dataset can be stripped of its redundancies. Coreset selection (or data pruning) seeks to select a subset of the training data so as to maximize the performance of models trained on this subset, also referred to as coreset. There are two dominant approaches: (1) geometry-based data selection for maximizing data diversity in the coreset, and (2) functions that assign difficulty scores to samples based on training dynamics. Optimizing for data diversity leads to a coreset that is biased towards easier samples, whereas, selection by difficulty ranking omits easy samples that are necessary for the training of deep learning models. This demonstrates that data diversity and importance scores are two complementary factors that need to be jointly considered during coreset selection. We represent a dataset as an undirected graph and propose a novel pruning algorithm, D2 Pruning, that uses forward and reverse message passing over this dataset graph for coreset selection. D2 Pruning updates the difficulty scores of each example by incorporating the difficulty of its neighboring examples in the dataset graph. Then, these updated difficulty scores direct a graph-based sampling method to select a coreset that encapsulates both diverse and difficult regions of the dataset space. We evaluate supervised and self-supervised versions of our method on various vision and language datasets. Results show that D2 Pruning improves coreset selection over previous state-of-the-art methods for up to 70% pruning rates. Additionally, we find that using D2 Pruning for filtering large multimodal datasets leads to increased diversity in the dataset and improved generalization of pretrained models.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：Tree-Planner: Efficient Close-loop Task Planning with Large Language  Models</b></summary>
  <p><b>编号</b>：[348]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08582</p>
  <p><b>作者</b>：Mengkang Hu,  Yao Mu,  Xinmiao Yu,  Mingyu Ding,  Shiguang Wu,  Wenqi Shao,  Qiguang Chen,  Bin Wang,  Yu Qiao,  Ping Luo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：paper studies close-loop, studies close-loop task, Large Language Models, prompting Large Language, sequence of skills</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper studies close-loop task planning, which refers to the process of generating a sequence of skills (a plan) to accomplish a specific goal while adapting the plan based on real-time observations. Recently, prompting Large Language Models (LLMs) to generate actions iteratively has become a prevalent paradigm due to its superior performance and user-friendliness. However, this paradigm is plagued by two inefficiencies: high token consumption and redundant error correction, both of which hinder its scalability for large-scale testing and applications. To address these issues, we propose Tree-Planner, which reframes task planning with LLMs into three distinct phases: plan sampling, action tree construction, and grounded deciding. Tree-Planner starts by using an LLM to sample a set of potential plans before execution, followed by the aggregation of them to form an action tree. Finally, the LLM performs a top-down decision-making process on the tree, taking into account real-time environmental information. Experiments show that Tree-Planner achieves state-of-the-art performance while maintaining high efficiency. By decomposing LLM queries into a single plan-sampling call and multiple grounded-deciding calls, a considerable part of the prompt are less likely to be repeatedly consumed. As a result, token consumption is reduced by 92.2% compared to the previously best-performing model. Additionally, by enabling backtracking on the action tree as needed, the correction process becomes more flexible, leading to a 40.5% decrease in error corrections. Project page: this https URL</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Visual Data-Type Understanding does not emerge from Scaling  Vision-Language Models</b></summary>
  <p><b>编号</b>：[352]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08577</p>
  <p><b>作者</b>：Vishaal Udandarao,  Max F. Burg,  Samuel Albanie,  Matthias Bethge</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：including impressive instances, yielding remarkable success, textit, Recent advances, including impressive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advances in the development of vision-language models (VLMs) are yielding remarkable success in recognizing visual semantic content, including impressive instances of compositional image understanding. Here, we introduce the novel task of \textit{Visual Data-Type Identification}, a basic perceptual skill with implications for data curation (e.g., noisy data-removal from large datasets, domain-specific retrieval) and autonomous vision (e.g., distinguishing changing weather conditions from camera lens staining). We develop two datasets consisting of animal images altered across a diverse set of 27 visual \textit{data-types}, spanning four broad categories. An extensive zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a nuanced performance landscape. While VLMs are reasonably good at identifying certain stylistic \textit{data-types}, such as cartoons and sketches, they struggle with simpler \textit{data-types} arising from basic manipulations like image rotations or additive noise. Our findings reveal that (i) model scaling alone yields marginal gains for contrastively-trained models like CLIP, and (ii) there is a pronounced drop in performance for the largest auto-regressively trained VLMs like OpenFlamingo. This finding points to a blind spot in current frontier VLMs: they excel in recognizing semantic content but fail to acquire an understanding of visual \textit{data-types} through scaling. By analyzing the pre-training distributions of these models and incorporating \textit{data-type} information into the captions during fine-tuning, we achieve a significant enhancement in performance. By exploring this previously uncharted task, we aim to set the stage for further advancing VLMs to equip them with visual data-type understanding. Code and datasets are released \href{this https URL}{here}.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Transformers as Decision Makers: Provable In-Context Reinforcement  Learning via Supervised Pretraining</b></summary>
  <p><b>编号</b>：[359]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08566</p>
  <p><b>作者</b>：Licong Lin,  Yu Bai,  Song Mei</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：demonstrated remarkable in-context, remarkable in-context reinforcement, make good decisions, unseen environments, datasets have demonstrated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large transformer models pretrained on offline reinforcement learning datasets have demonstrated remarkable in-context reinforcement learning (ICRL) capabilities, where they can make good decisions when prompted with interaction trajectories from unseen environments. However, when and how transformers can be trained to perform ICRL have not been theoretically well-understood. In particular, it is unclear which reinforcement-learning algorithms transformers can perform in context, and how distribution mismatch in offline training data affects the learned algorithms. This paper provides a theoretical framework that analyzes supervised pretraining for ICRL. This includes two recently proposed training methods -- algorithm distillation and decision-pretrained transformers. First, assuming model realizability, we prove the supervised-pretrained transformer will imitate the conditional expectation of the expert algorithm given the observed trajectory. The generalization error will scale with model capacity and a distribution divergence factor between the expert and offline algorithms. Second, we show transformers with ReLU attention can efficiently approximate near-optimal online reinforcement learning algorithms like LinUCB and Thompson sampling for stochastic linear bandits, and UCB-VI for tabular Markov decision processes. This provides the first quantitative analysis of the ICRL capabilities of transformers pretrained from offline trajectories.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of  Language Models with Hypothesis Refinement</b></summary>
  <p><b>编号</b>：[362]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08559</p>
  <p><b>作者</b>：Linlu Qiu,  Liwei Jiang,  Ximing Lu,  Melanie Sclar,  Valentina Pyatkin,  Chandra Bhagavatula,  Bailin Wang,  Yoon Kim,  Yejin Choi,  Nouha Dziri,  Xiang Ren</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：derive underlying principles, inductive reasoning, ability to derive, derive underlying, underlying principles</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The ability to derive underlying principles from a handful of observations and then generalize to novel situations -- known as inductive reasoning -- is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approach achieves strong results across inductive reasoning benchmarks that require inducing causal relations, language-like instructions, and symbolic concepts. However, they also behave as puzzling inductive reasoners, showing notable performance gaps in rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules. Through empirical and human analyses, we further reveal several discrepancies between the inductive reasoning processes of LMs and humans, shedding light on both the potentials and limitations of using LMs in inductive reasoning tasks.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Do pretrained Transformers Really Learn In-context by Gradient Descent?</b></summary>
  <p><b>编号</b>：[369]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08540</p>
  <p><b>作者</b>：Lingfeng Shen,  Aayush Mishra,  Daniel Khashabi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：In-Context Learning, ICL, implicitly equivalent, Gradient Descent, language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Is In-Context Learning (ICL) implicitly equivalent to Gradient Descent (GD)? Several recent works draw analogies between the dynamics of GD and the emergent behavior of ICL in large language models. However, these works make assumptions far from the realistic natural language setting in which language models are trained. Such discrepancies between theory and practice, therefore, necessitate further investigation to validate their applicability.
We start by highlighting the weaknesses in prior works that construct Transformer weights to simulate gradient descent. Their experiments with training Transformers on ICL objective, inconsistencies in the order sensitivity of ICL and GD, sparsity of the constructed weights, and sensitivity to parameter changes are some examples of a mismatch from the real-world setting.
Furthermore, we probe and compare the ICL vs. GD hypothesis in a natural setting. We conduct comprehensive empirical analyses on language models pretrained on natural data (LLaMa-7B). Our comparisons on various performance metrics highlight the inconsistent behavior of ICL and GD as a function of various factors such as datasets, models, and number of demonstrations. We observe that ICL and GD adapt the output distribution of language models differently. These results indicate that the equivalence between ICL and GD is an open hypothesis, requires nuanced considerations and calls for further studies.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：Formally Specifying the High-Level Behavior of LLM-Based Agents</b></summary>
  <p><b>编号</b>：[372]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08535</p>
  <p><b>作者</b>：Maxwell Crouse,  Ibrahim Abdelaziz,  Kinjal Basu,  Soham Dan,  Sadhana Kumaravel,  Achille Fokoue,  Pavan Kapanipathi,  Luis Lastras</p>
  <p><b>备注</b>：Preprint under review</p>
  <p><b>关键词</b>：solving challenging problems, task-specific finetuned models, LLM-based agents, expensive to procure, Linear Temporal Logic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>LLM-based agents have recently emerged as promising tools for solving challenging problems without the need for task-specific finetuned models that can be expensive to procure. Currently, the design and implementation of such agents is ad hoc, as the wide variety of tasks that LLM-based agents may be applied to naturally means there can be no one-size-fits-all approach to agent design. In this work we aim to alleviate the difficulty of designing and implementing new agents by proposing a minimalistic, high-level generation framework that simplifies the process of building agents. The framework we introduce allows the user to specify desired agent behaviors in Linear Temporal Logic (LTL). The declarative LTL specification is then used to construct a constrained decoder that guarantees the LLM will produce an output exhibiting the desired behavior. By designing our framework in this way, we obtain several benefits, including the ability to enforce complex agent behavior, the ability to formally validate prompt examples, and the ability to seamlessly incorporate content-focused logical constraints into generation. In particular, our declarative approach, in which the desired behavior is simply described without concern for how it should be implemented or enforced, enables rapid design, implementation and experimentation with different LLM-based agents. We demonstrate how the proposed framework can be used to implement recent LLM-based agents, and show how the guardrails our approach provides can lead to improvements in agent performance. In addition, we release our code for general use.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：LLM-augmented Preference Learning from Natural Language</b></summary>
  <p><b>编号</b>：[378]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08523</p>
  <p><b>作者</b>：Inwon Kang,  Sikai Ruan,  Tyler Ho,  Jui-Chien Lin,  Farhad Mohsin,  Oshani Seneviratne,  Lirong Xia</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Finding preferences expressed, Finding preferences, preferences expressed, expressed in natural, important but challenging</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Finding preferences expressed in natural language is an important but challenging task. State-of-the-art(SotA) methods leverage transformer-based models such as BERT, RoBERTa, etc. and graph neural architectures such as graph attention networks. Since Large Language Models (LLMs) are equipped to deal with larger context lengths and have much larger model sizes than the transformer-based model, we investigate their ability to classify comparative text directly. This work aims to serve as a first step towards using LLMs for the CPC task. We design and conduct a set of experiments that format the classification task into an input prompt for the LLM and a methodology to get a fixed-format response that can be automatically evaluated. Comparing performances with existing methods, we see that pre-trained LLMs are able to outperform the previous SotA models with no fine-tuning involved. Our results show that the LLMs can consistently outperform the SotA when the target text is large -- i.e. composed of multiple sentences --, and are still comparable to the SotA performance in shorter text. We also find that few-shot learning yields better performance than zero-shot learning.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：HoneyBee: Progressive Instruction Finetuning of Large Language Models  for Materials Science</b></summary>
  <p><b>编号</b>：[381]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08511</p>
  <p><b>作者</b>：Yu Song,  Santiago Miret,  Huan Zhang,  Bang Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：materials science, trustworthy data curation, LLaMa-based language model, propose an instruction-based, instruction-based process</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose an instruction-based process for trustworthy data curation in materials science (MatSci-Instruct), which we then apply to finetune a LLaMa-based language model targeted for materials science (HoneyBee). MatSci-Instruct helps alleviate the scarcity of relevant, high-quality materials science textual data available in the open literature, and HoneyBee is the first billion-parameter language model specialized to materials science. In MatSci-Instruct we improve the trustworthiness of generated data by prompting multiple commercially available large language models for generation with an Instructor module (e.g. Chat-GPT) and verification from an independent Verifier module (e.g. Claude). Using MatSci-Instruct, we construct a dataset of multiple tasks and measure the quality of our dataset along multiple dimensions, including accuracy against known facts, relevance to materials science, as well as completeness and reasonableness of the data. Moreover, we iteratively generate more targeted instructions and instruction-data in a finetuning-evaluation-feedback loop leading to progressively better performance for our finetuned HoneyBee models. Our evaluation on the MatSci-NLP benchmark shows HoneyBee's outperformance of existing language models on materials science tasks and iterative improvement in successive stages of instruction-data refinement. We study the quality of HoneyBee's language modeling through automatic evaluation and analyze case studies to further understand the model's capabilities and limitations. Our code and relevant datasets are publicly available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：The Uncertainty-based Retrieval Framework for Ancient Chinese CWS and  POS</b></summary>
  <p><b>编号</b>：[385]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08496</p>
  <p><b>作者</b>：Pengyu Wang,  Zhichen Ren</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ancient Chinese Word, Chinese Word Segmentation, Automatic analysis, related fields, ancient Chinese</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automatic analysis for modern Chinese has greatly improved the accuracy of text mining in related fields, but the study of ancient Chinese is still relatively rare. Ancient text division and lexical annotation are important parts of classical literature comprehension, and previous studies have tried to construct auxiliary dictionary and other fused knowledge to improve the performance. In this paper, we propose a framework for ancient Chinese Word Segmentation and Part-of-Speech Tagging that makes a twofold effort: on the one hand, we try to capture the wordhood semantics; on the other hand, we re-predict the uncertain samples of baseline model by introducing external knowledge. The performance of our architecture outperforms pre-trained BERT with CRF and existing tools such as Jiayan.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：Prometheus: Inducing Fine-grained Evaluation Capability in Language  Models</b></summary>
  <p><b>编号</b>：[387]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08491</p>
  <p><b>作者</b>：Seungone Kim,  Jamin Shin,  Yejin Cho,  Joel Jang,  Shayne Longpre,  Hwaran Lee,  Sangdoo Yun,  Seongjin Shin,  Sungdong Kim,  James Thorne,  Minjoon Seo</p>
  <p><b>备注</b>：Work in Progress</p>
  <p><b>关键词</b>：powerful proprietary Large, proprietary Large Language, Large Language Model, Large Language, proprietary Large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, using a powerful proprietary Large Language Model (LLM) (e.g., GPT-4) as an evaluator for long-form responses has become the de facto standard. However, for practitioners with large-scale evaluation tasks and custom criteria in consideration (e.g., child-readability), using proprietary LLMs as an evaluator is unreliable due to the closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose Prometheus, a fully open-source LLM that is on par with GPT-4's evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. We first construct the Feedback Collection, a new dataset that consists of 1K fine-grained score rubrics, 20K instructions, and 100K responses and language feedback generated by GPT-4. Using the Feedback Collection, we train Prometheus, a 13B evaluator LLM that can assess any given long-form text based on customized score rubric provided by the user. Experimental results show that Prometheus scores a Pearson correlation of 0.897 with human evaluators when evaluating with 45 customized score rubrics, which is on par with GPT-4 (0.882), and greatly outperforms ChatGPT (0.392). Furthermore, measuring correlation with GPT-4 with 1222 customized score rubrics across four benchmarks (MT Bench, Vicuna Bench, Feedback Bench, Flask Eval) shows similar trends, bolstering Prometheus's capability as an evaluator LLM. Lastly, Prometheus achieves the highest accuracy on two human preference benchmarks (HHH Alignment & MT Bench Human Judgment) compared to open-sourced reward models explicitly trained on human preference datasets, highlighting its potential as an universal reward model. We open-source our code, dataset, and model at this https URL.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：GraphextQA: A Benchmark for Evaluating Graph-Enhanced Large Language  Models</b></summary>
  <p><b>编号</b>：[389]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08487</p>
  <p><b>作者</b>：Yuanchun Shen,  Ruotong Liao,  Zhen Han,  Yunpu Ma,  Volker Tresp</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：integrating graph modality, successfully integrated information, remains unexplored, audio modalities, successfully integrated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While multi-modal models have successfully integrated information from image, video, and audio modalities, integrating graph modality into large language models (LLMs) remains unexplored. This discrepancy largely stems from the inherent divergence between structured graph data and unstructured text data. Incorporating graph knowledge provides a reliable source of information, enabling potential solutions to address issues in text generation, e.g., hallucination, and lack of domain knowledge. To evaluate the integration of graph knowledge into language models, a dedicated dataset is needed. However, there is currently no benchmark dataset specifically designed for multimodal graph-language models. To address this gap, we propose GraphextQA, a question answering dataset with paired subgraphs, retrieved from Wikidata, to facilitate the evaluation and future development of graph-language models. Additionally, we introduce a baseline model called CrossGNN, which conditions answer generation on the paired graphs by cross-attending question-aware graph features at decoding. The proposed dataset is designed to evaluate graph-language models' ability to understand graphs and make use of it for answer generation. We perform experiments with language-only models and the proposed graph-language model to validate the usefulness of the paired graphs and to demonstrate the difficulty of the task.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：Understanding the Humans Behind Online Misinformation: An Observational  Study Through the Lens of the COVID-19 Pandemic</b></summary>
  <p><b>编号</b>：[390]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08483</p>
  <p><b>作者</b>：Mohit Chandra,  Anush Mattapalli,  Munmun De Choudhury</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：threats to society, biggest threats, misinformation, online misinformation, online</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The proliferation of online misinformation has emerged as one of the biggest threats to society. Considerable efforts have focused on building misinformation detection models, still the perils of misinformation remain abound. Mitigating online misinformation and its ramifications requires a holistic approach that encompasses not only an understanding of its intricate landscape in relation to the complex issue and topic-rich information ecosystem online, but also the psychological drivers of individuals behind it. Adopting a time series analytic technique and robust causal inference-based design, we conduct a large-scale observational study analyzing over 32 million COVID-19 tweets and 16 million historical timeline tweets. We focus on understanding the behavior and psychology of users disseminating misinformation during COVID-19 and its relationship with the historical inclinations towards sharing misinformation on Non-COVID topics before the pandemic. Our analysis underscores the intricacies inherent to cross-topic misinformation, and highlights that users' historical inclination toward sharing misinformation is positively associated with their present behavior pertaining to misinformation sharing on emergent topics and beyond. This work may serve as a valuable foundation for designing user-centric inoculation strategies and ecologically-grounded agile interventions for effectively tackling online misinformation.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：Can We Edit Multimodal Large Language Models?</b></summary>
  <p><b>编号</b>：[391]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08475</p>
  <p><b>作者</b>：Siyuan Cheng,  Bozhong Tian,  Qingbin Liu,  Xi Chen,  Yongheng Wang,  Huajun Chen,  Ningyu Zhang</p>
  <p><b>备注</b>：EMNLP 2023</p>
  <p><b>关键词</b>：Large Language Models, Multimodal Large Language, Large Language, editing Multimodal Large, Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in this https URL.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：DistillSpec: Improving Speculative Decoding via Knowledge Distillation</b></summary>
  <p><b>编号</b>：[396]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08461</p>
  <p><b>作者</b>：Yongchao Zhou,  Kaifeng Lyu,  Ankit Singh Rawat,  Aditya Krishna Menon,  Afshin Rostamizadeh,  Sanjiv Kumar,  Jean-François Kagy,  Rishabh Agarwal</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：accelerates large language, generating multiple tokens, large language model, language model inference, target model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Speculative decoding (SD) accelerates large language model inference by employing a faster draft model for generating multiple tokens, which are then verified in parallel by the larger target model, resulting in the text generated according to the target model distribution. However, identifying a compact draft model that is well-aligned with the target model is challenging. To tackle this issue, we propose DistillSpec that uses knowledge distillation to better align the draft model with the target model, before applying SD. DistillSpec makes two key design choices, which we demonstrate via systematic study to be crucial to improving the draft and target alignment: utilizing on-policy data generation from the draft model, and tailoring the divergence function to the task and decoding strategy. Notably, DistillSpec yields impressive 10 - 45% speedups over standard SD on a range of standard benchmarks, using both greedy and non-greedy sampling. Furthermore, we combine DistillSpec with lossy SD to achieve fine-grained control over the latency vs. task performance trade-off. Finally, in practical scenarios with models of varying sizes, first using distillation to boost the performance of the target model and then applying DistillSpec to train a well-aligned draft model can reduce decoding latency by 6-10x with minimal performance drop, compared to standard decoding without distillation.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative  Writing</b></summary>
  <p><b>编号</b>：[410]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08433</p>
  <p><b>作者</b>：Carlos Gómez-Rodríguez,  Paul Williams</p>
  <p><b>备注</b>：Accepted for publication in Findings of EMNLP 2023</p>
  <p><b>关键词</b>：English creative writing, English creative, creative writing, requires imagination, evaluate a range</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We evaluate a range of recent LLMs on English creative writing, a challenging and complex task that requires imagination, coherence, and style. We use a difficult, open-ended scenario chosen to avoid training data reuse: an epic narration of a single combat between Ignatius J. Reilly, the protagonist of the Pulitzer Prize-winning novel A Confederacy of Dunces (1980), and a pterodactyl, a prehistoric flying reptile. We ask several LLMs and humans to write such a story and conduct a human evalution involving various criteria such as fluency, coherence, originality, humor, and style. Our results show that some state-of-the-art commercial LLMs match or slightly outperform our writers in most dimensions; whereas open-source LLMs lag behind. Humans retain an edge in creativity, while humor shows a binary divide between LLMs that can handle it comparably to humans and those that fail at it. We discuss the implications and limitations of our study and suggest directions for future research.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：Prompting Large Language Models with Chain-of-Thought for Few-Shot  Knowledge Base Question Generation</b></summary>
  <p><b>编号</b>：[426]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08395</p>
  <p><b>作者</b>：Yuanyuan Liang,  Jianing Wang,  Hanlun Zhu,  Lei Wang,  Weining Qian,  Yunshi Lan</p>
  <p><b>备注</b>：Accepted by EMNLP 2023 main conference</p>
  <p><b>关键词</b>：Knowledge Bases, natural language question, Large Language Models, aims to convert, Question Generation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The task of Question Generation over Knowledge Bases (KBQG) aims to convert a logical form into a natural language question. For the sake of expensive cost of large-scale question annotation, the methods of KBQG under low-resource scenarios urgently need to be developed. However, current methods heavily rely on annotated data for fine-tuning, which is not well-suited for few-shot question generation. The emergence of Large Language Models (LLMs) has shown their impressive generalization ability in few-shot tasks. Inspired by Chain-of-Thought (CoT) prompting, which is an in-context learning strategy for reasoning, we formulate KBQG task as a reasoning problem, where the generation of a complete question is splitted into a series of sub-question generation. Our proposed prompting method KQG-CoT first retrieves supportive logical forms from the unlabeled data pool taking account of the characteristics of the logical form. Then, we write a prompt to explicit the reasoning chain of generating complicated questions based on the selected demonstrations. To further ensure prompt quality, we extend KQG-CoT into KQG-CoT+ via sorting the logical forms by their complexity. We conduct extensive experiments over three public KBQG datasets. The results demonstrate that our prompting method consistently outperforms other prompting baselines on the evaluated datasets. Remarkably, our KQG-CoT+ method could surpass existing few-shot SoTA results of the PathQuestions dataset by 18.25, 10.72, and 10.18 absolute points on BLEU-4, METEOR, and ROUGE-L, respectively.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：Towards Better Evaluation of Instruction-Following: A Case-Study in  Summarization</b></summary>
  <p><b>编号</b>：[427]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08394</p>
  <p><b>作者</b>：Ondrej Skopek,  Rahul Aralikatte,  Sian Gooding,  Victor Carbune</p>
  <p><b>备注</b>：Accepted to CoNLL 2023</p>
  <p><b>关键词</b>：follow user instructions, user instructions remains, large language models, recent advances, follow user</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite recent advances, evaluating how well large language models (LLMs) follow user instructions remains an open problem. While evaluation methods of language models have seen a rise in prompt-based approaches, limited work on the correctness of these methods has been conducted. In this work, we perform a meta-evaluation of a variety of metrics to quantify how accurately they measure the instruction-following abilities of LLMs. Our investigation is performed on grounded query-based summarization by collecting a new short-form, real-world dataset riSum, containing $300$ document-instruction pairs with $3$ answers each. All $900$ answers are rated by $3$ human annotators. Using riSum, we analyze agreement between evaluation methods and human judgment. Finally, we propose new LLM-based reference-free evaluation methods that improve upon established baselines and perform on-par with costly reference-based metrics which require high-quality summaries.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：Reconstructing Materials Tetrahedron: Challenges in Materials  Information Extraction</b></summary>
  <p><b>编号</b>：[432]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08383</p>
  <p><b>作者</b>：Kausik Hira,  Mohd Zaki,  Dhruvil Sheth,  Mausam,  N M Anoop Krishnan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：propelling human progress, documented history, history of propelling, propelling human, human progress</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Discovery of new materials has a documented history of propelling human progress for centuries and more. The behaviour of a material is a function of its composition, structure, and properties, which further depend on its processing and testing conditions. Recent developments in deep learning and natural language processing have enabled information extraction at scale from published literature such as peer-reviewed publications, books, and patents. However, this information is spread in multiple formats, such as tables, text, and images, and with little or no uniformity in reporting style giving rise to several machine learning challenges. Here, we discuss, quantify, and document these outstanding challenges in automated information extraction (IE) from materials science literature towards the creation of a large materials science knowledge base. Specifically, we focus on IE from text and tables and outline several challenges with examples. We hope the present work inspires researchers to address the challenges in a coherent fashion, providing to fillip to IE for the materials knowledge base.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：Improving Factual Consistency for Knowledge-Grounded Dialogue Systems  via Knowledge Enhancement and Alignment</b></summary>
  <p><b>编号</b>：[436]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08372</p>
  <p><b>作者</b>：Boyang Xue,  Weichao Wang,  Hongru Wang,  Fei Mi,  Rui Wang,  Yasheng Wang,  Lifeng Shang,  Xin Jiang,  Qun Liu,  Kam-Fai Wong</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Pretrained language models, provided knowledge source, based knowledge-grounded dialogue, Pretrained language, prone to generate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pretrained language models (PLMs) based knowledge-grounded dialogue systems are prone to generate responses that are factually inconsistent with the provided knowledge source. In such inconsistent responses, the dialogue models fail to accurately express the external knowledge they rely upon. Inspired by previous work which identified that feed-forward networks (FFNs) within Transformers are responsible for factual knowledge expressions, we investigate two methods to efficiently improve the factual expression capability {of FFNs} by knowledge enhancement and alignment respectively. We first propose \textsc{K-Dial}, which {explicitly} introduces {extended FFNs in Transformers to enhance factual knowledge expressions} given the specific patterns of knowledge-grounded dialogue inputs. Additionally, we apply the reinforcement learning for factual consistency (RLFC) method to implicitly adjust FFNs' expressions in responses by aligning with gold knowledge for the factual consistency preference. To comprehensively assess the factual consistency and dialogue quality of responses, we employ extensive automatic measures and human evaluations including sophisticated fine-grained NLI-based metrics. Experimental results on WoW and CMU\_DoG datasets demonstrate that our methods efficiently enhance the ability of the FFN module to convey factual knowledge, validating the efficacy of improving factual consistency for knowledge-grounded dialogue systems.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：MCU: A Task-centric Framework for Open-ended Agent Evaluation in  Minecraft</b></summary>
  <p><b>编号</b>：[441]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08367</p>
  <p><b>作者</b>：Haowei Lin,  Zihao Wang,  Jianzhu Ma,  Yitao Liang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：framework named MCU, task-centric framework named, MCU framework, open-ended game environment, MCU</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To pursue the goal of creating an open-ended agent in Minecraft, an open-ended game environment with unlimited possibilities, this paper introduces a task-centric framework named MCU for Minecraft agent evaluation. The MCU framework leverages the concept of atom tasks as fundamental building blocks, enabling the generation of diverse or even arbitrary tasks. Within the MCU framework, each task is measured with six distinct difficulty scores (time consumption, operational effort, planning complexity, intricacy, creativity, novelty). These scores offer a multi-dimensional assessment of a task from different angles, and thus can reveal an agent's capability on specific facets. The difficulty scores also serve as the feature of each task, which creates a meaningful task space and unveils the relationship between tasks. For efficient evaluation of Minecraft agents employing the MCU framework, we maintain a unified benchmark, namely SkillForge, which comprises representative tasks with diverse categories and difficulty distribution. We also provide convenient filters for users to select tasks to assess specific capabilities of agents. We show that MCU has the high expressivity to cover all tasks used in recent literature on Minecraft agent, and underscores the need for advancements in areas such as creativity, precise control, and out-of-distribution generalization under the goal of open-ended Minecraft agent development.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：From Large Language Models to Knowledge Graphs for Biomarker Discovery  in Cancer</b></summary>
  <p><b>编号</b>：[442]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08365</p>
  <p><b>作者</b>：Md. Rezaul Karim,  Lina Molinas Comet,  Md Shajalal,  Oya Beyan,  Dietrich Rebholz-Schuhmann,  Stefan Decker</p>
  <p><b>备注</b>：arXiv admin note: substantial text overlap with arXiv:2302.04737</p>
  <p><b>关键词</b>：disseminating specific biological, specific biological processes, therapeutic decision-making, apprehending and disseminating, disseminating specific</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Domain experts often rely on up-to-date knowledge for apprehending and disseminating specific biological processes that help them design strategies to develop prevention and therapeutic decision-making. A challenging scenario for artificial intelligence (AI) is using biomedical data (e.g., texts, imaging, omics, and clinical) to provide diagnosis and treatment recommendations for cancerous conditions. Data and knowledge about cancer, drugs, genes, proteins, and their mechanism is spread across structured (knowledge bases (KBs)) and unstructured (e.g., scientific articles) sources. A large-scale knowledge graph (KG) can be constructed by integrating these data, followed by extracting facts about semantically interrelated entities and relations. Such KGs not only allow exploration and question answering (QA) but also allow domain experts to deduce new knowledge. However, exploring and querying large-scale KGs is tedious for non-domain users due to a lack of understanding of the underlying data assets and semantic technologies. In this paper, we develop a domain KG to leverage cancer-specific biomarker discovery and interactive QA. For this, a domain ontology called OncoNet Ontology (ONO) is developed to enable semantic reasoning for validating gene-disease relations. The KG is then enriched by harmonizing the ONO, controlled vocabularies, and additional biomedical concepts from scientific articles by employing BioBERT- and SciBERT-based information extraction (IE) methods. Further, since the biomedical domain is evolving, where new findings often replace old ones, without employing up-to-date findings, there is a high chance an AI system exhibits concept drift while providing diagnosis and treatment. Therefore, we finetuned the KG using large language models (LLMs) based on more recent articles and KBs that might not have been seen by the named entity recognition models.</p>
  </details>
</details>
<h1>机器学习</h1>
<details>
  <summary>1. <b>标题：Octopus: Embodied Vision-Language Programmer from Environmental Feedback</b></summary>
  <p><b>编号</b>：[1]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08588</p>
  <p><b>作者</b>：Jingkang Yang,  Yuhao Dong,  Shuai Liu,  Bo Li,  Ziyue Wang,  Chencheng Jiang,  Haoran Tan,  Jiamu Kang,  Yuanhan Zhang,  Kaiyang Zhou,  Ziwei Liu</p>
  <p><b>备注</b>：Project Page: this https URL, Codebase: this https URL</p>
  <p><b>关键词</b>：achieved substantial progress, Large vision-language models, Large vision-language, perception and reasoning, achieved substantial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large vision-language models (VLMs) have achieved substantial progress in multimodal perception and reasoning. Furthermore, when seamlessly integrated into an embodied agent, it signifies a crucial stride towards the creation of autonomous and context-aware systems capable of formulating plans and executing commands with precision. In this paper, we introduce Octopus, a novel VLM designed to proficiently decipher an agent's vision and textual task objectives and to formulate intricate action sequences and generate executable code. Our design allows the agent to adeptly handle a wide spectrum of tasks, ranging from mundane daily chores in simulators to sophisticated interactions in complex video games. Octopus is trained by leveraging GPT-4 to control an explorative agent to generate training data, i.e., action blueprints and the corresponding executable code, within our experimental environment called OctoVerse. We also collect the feedback that allows the enhanced training scheme of Reinforcement Learning with Environmental Feedback (RLEF). Through a series of experiments, we illuminate Octopus's functionality and present compelling results, and the proposed RLEF turns out to refine the agent's decision-making. By open-sourcing our model architecture, simulator, and dataset, we aspire to ignite further innovation and foster collaborative applications within the broader embodied AI community.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Tree-Planner: Efficient Close-loop Task Planning with Large Language  Models</b></summary>
  <p><b>编号</b>：[7]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08582</p>
  <p><b>作者</b>：Mengkang Hu,  Yao Mu,  Xinmiao Yu,  Mingyu Ding,  Shiguang Wu,  Wenqi Shao,  Qiguang Chen,  Bin Wang,  Yu Qiao,  Ping Luo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：paper studies close-loop, studies close-loop task, Large Language Models, prompting Large Language, sequence of skills</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper studies close-loop task planning, which refers to the process of generating a sequence of skills (a plan) to accomplish a specific goal while adapting the plan based on real-time observations. Recently, prompting Large Language Models (LLMs) to generate actions iteratively has become a prevalent paradigm due to its superior performance and user-friendliness. However, this paradigm is plagued by two inefficiencies: high token consumption and redundant error correction, both of which hinder its scalability for large-scale testing and applications. To address these issues, we propose Tree-Planner, which reframes task planning with LLMs into three distinct phases: plan sampling, action tree construction, and grounded deciding. Tree-Planner starts by using an LLM to sample a set of potential plans before execution, followed by the aggregation of them to form an action tree. Finally, the LLM performs a top-down decision-making process on the tree, taking into account real-time environmental information. Experiments show that Tree-Planner achieves state-of-the-art performance while maintaining high efficiency. By decomposing LLM queries into a single plan-sampling call and multiple grounded-deciding calls, a considerable part of the prompt are less likely to be repeatedly consumed. As a result, token consumption is reduced by 92.2% compared to the previously best-performing model. Additionally, by enabling backtracking on the action tree as needed, the correction process becomes more flexible, leading to a 40.5% decrease in error corrections. Project page: this https URL</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Visual Data-Type Understanding does not emerge from Scaling  Vision-Language Models</b></summary>
  <p><b>编号</b>：[11]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08577</p>
  <p><b>作者</b>：Vishaal Udandarao,  Max F. Burg,  Samuel Albanie,  Matthias Bethge</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：including impressive instances, yielding remarkable success, textit, Recent advances, including impressive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advances in the development of vision-language models (VLMs) are yielding remarkable success in recognizing visual semantic content, including impressive instances of compositional image understanding. Here, we introduce the novel task of \textit{Visual Data-Type Identification}, a basic perceptual skill with implications for data curation (e.g., noisy data-removal from large datasets, domain-specific retrieval) and autonomous vision (e.g., distinguishing changing weather conditions from camera lens staining). We develop two datasets consisting of animal images altered across a diverse set of 27 visual \textit{data-types}, spanning four broad categories. An extensive zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a nuanced performance landscape. While VLMs are reasonably good at identifying certain stylistic \textit{data-types}, such as cartoons and sketches, they struggle with simpler \textit{data-types} arising from basic manipulations like image rotations or additive noise. Our findings reveal that (i) model scaling alone yields marginal gains for contrastively-trained models like CLIP, and (ii) there is a pronounced drop in performance for the largest auto-regressively trained VLMs like OpenFlamingo. This finding points to a blind spot in current frontier VLMs: they excel in recognizing semantic content but fail to acquire an understanding of visual \textit{data-types} through scaling. By analyzing the pre-training distributions of these models and incorporating \textit{data-type} information into the captions during fine-tuning, we achieve a significant enhancement in performance. By exploring this previously uncharted task, we aim to set the stage for further advancing VLMs to equip them with visual data-type understanding. Code and datasets are released \href{this https URL}{here}.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Learning to Act from Actionless Videos through Dense Correspondences</b></summary>
  <p><b>编号</b>：[12]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08576</p>
  <p><b>作者</b>：Po-Chen Ko,  Jiayuan Mao,  Yilun Du,  Shao-Hua Sun,  Joshua B. Tenenbaum</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：reliably executing diverse, construct a video-based, capable of reliably, executing diverse tasks, video-based robot policy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we present an approach to construct a video-based robot policy capable of reliably executing diverse tasks across different robots and environments from few video demonstrations without using any action annotations. Our method leverages images as a task-agnostic representation, encoding both the state and action information, and text as a general representation for specifying robot goals. By synthesizing videos that ``hallucinate'' robot executing actions and in combination with dense correspondences between frames, our approach can infer the closed-formed action to execute to an environment without the need of any explicit action labels. This unique capability allows us to train the policy solely based on RGB videos and deploy learned policies to various robotic tasks. We demonstrate the efficacy of our approach in learning policies on table-top manipulation and navigation tasks. Additionally, we contribute an open-source framework for efficient video modeling, enabling the training of high-fidelity policy models with four GPUs within a single day.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Jigsaw: Supporting Designers in Prototyping Multimodal Applications by  Assembling AI Foundation Models</b></summary>
  <p><b>编号</b>：[13]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08574</p>
  <p><b>作者</b>：David Chuan-En Lin,  Nikolas Martelaro</p>
  <p><b>备注</b>：Webpage: this https URL</p>
  <p><b>关键词</b>：generating visual prototypes, including ideating design, ideating design concepts, Recent advancements, including ideating</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advancements in AI foundation models have made it possible for them to be utilized off-the-shelf for creative tasks, including ideating design concepts or generating visual prototypes. However, integrating these models into the creative process can be challenging as they often exist as standalone applications tailored to specific tasks. To address this challenge, we introduce Jigsaw, a prototype system that employs puzzle pieces as metaphors to represent foundation models. Jigsaw allows designers to combine different foundation model capabilities across various modalities by assembling compatible puzzle pieces. To inform the design of Jigsaw, we interviewed ten designers and distilled design goals. In a user study, we showed that Jigsaw enhanced designers' understanding of available foundation model capabilities, provided guidance on combining capabilities across different modalities and tasks, and served as a canvas to support design exploration, prototyping, and documentation.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders</b></summary>
  <p><b>编号</b>：[15]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08571</p>
  <p><b>作者</b>：Jan Dubiński,  Stanisław Pawlak,  Franziska Boenisch,  Tomasz Trzciński,  Adam Dziedzic</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Machine Learning, generate vector representations, generate vector, APIs provide, legitimate API users</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine Learning as a Service (MLaaS) APIs provide ready-to-use and high-utility encoders that generate vector representations for given inputs. Since these encoders are very costly to train, they become lucrative targets for model stealing attacks during which an adversary leverages query access to the API to replicate the encoder locally at a fraction of the original training costs. We propose Bucks for Buckets (B4B), the first active defense that prevents stealing while the attack is happening without degrading representation quality for legitimate API users. Our defense relies on the observation that the representations returned to adversaries who try to steal the encoder's functionality cover a significantly larger fraction of the embedding space than representations of legitimate users who utilize the encoder to solve a particular downstream task.vB4B leverages this to adaptively adjust the utility of the returned representations according to a user's coverage of the embedding space. To prevent adaptive adversaries from eluding our defense by simply creating multiple user accounts (sybils), B4B also individually transforms each user's representations. This prevents the adversary from directly aggregating representations over multiple accounts to create their stolen encoder copy. Our active defense opens a new path towards securely sharing and democratizing encoders over public APIs.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Transformers as Decision Makers: Provable In-Context Reinforcement  Learning via Supervised Pretraining</b></summary>
  <p><b>编号</b>：[18]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08566</p>
  <p><b>作者</b>：Licong Lin,  Yu Bai,  Song Mei</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：demonstrated remarkable in-context, remarkable in-context reinforcement, make good decisions, unseen environments, datasets have demonstrated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large transformer models pretrained on offline reinforcement learning datasets have demonstrated remarkable in-context reinforcement learning (ICRL) capabilities, where they can make good decisions when prompted with interaction trajectories from unseen environments. However, when and how transformers can be trained to perform ICRL have not been theoretically well-understood. In particular, it is unclear which reinforcement-learning algorithms transformers can perform in context, and how distribution mismatch in offline training data affects the learned algorithms. This paper provides a theoretical framework that analyzes supervised pretraining for ICRL. This includes two recently proposed training methods -- algorithm distillation and decision-pretrained transformers. First, assuming model realizability, we prove the supervised-pretrained transformer will imitate the conditional expectation of the expert algorithm given the observed trajectory. The generalization error will scale with model capacity and a distribution divergence factor between the expert and offline algorithms. Second, we show transformers with ReLU attention can efficiently approximate near-optimal online reinforcement learning algorithms like LinUCB and Thompson sampling for stochastic linear bandits, and UCB-VI for tabular Markov decision processes. This provides the first quantitative analysis of the ICRL capabilities of transformers pretrained from offline trajectories.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Offline Retraining for Online RL: Decoupled Policy Learning to Mitigate  Exploration Bias</b></summary>
  <p><b>编号</b>：[22]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08558</p>
  <p><b>作者</b>：Max Sobol Mark,  Archit Sharma,  Fahim Tajwar,  Rafael Rafailov,  Sergey Levine,  Chelsea Finn</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：state coverage, optimistically explore, policy, online, offline</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>It is desirable for policies to optimistically explore new states and behaviors during online reinforcement learning (RL) or fine-tuning, especially when prior offline data does not provide enough state coverage. However, exploration bonuses can bias the learned policy, and our experiments find that naive, yet standard use of such bonuses can fail to recover a performant policy. Concurrently, pessimistic training in offline RL has enabled recovery of performant policies from static datasets. Can we leverage offline RL to recover better policies from online interaction? We make a simple observation that a policy can be trained from scratch on all interaction data with pessimistic objectives, thereby decoupling the policies used for data collection and for evaluation. Specifically, we propose offline retraining, a policy extraction step at the end of online fine-tuning in our Offline-to-Online-to-Offline (OOO) framework for reinforcement learning (RL). An optimistic (exploration) policy is used to interact with the environment, and a separate pessimistic (exploitation) policy is trained on all the observed data for evaluation. Such decoupling can reduce any bias from online interaction (intrinsic rewards, primacy bias) in the evaluation policy, and can allow more exploratory behaviors during online interaction which in turn can generate better data for exploitation. OOO is complementary to several offline-to-online RL and online RL methods, and improves their average performance by 14% to 26% in our fine-tuning experiments, achieves state-of-the-art performance on several environments in the D4RL benchmarks, and improves online RL performance by 165% on two OpenAI gym environments. Further, OOO can enable fine-tuning from incomplete offline datasets where prior methods can fail to recover a performant policy. Implementation: this https URL</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Cross-Episodic Curriculum for Transformer Agents</b></summary>
  <p><b>编号</b>：[24]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08549</p>
  <p><b>作者</b>：Lucy Xiaoyang Shi,  Yunfan Jiang,  Jake Grigsby,  Linxi "Jim" Fan,  Yuke Zhu</p>
  <p><b>备注</b>：To appear in NeurIPS 2023; The first two authors contributed equally</p>
  <p><b>关键词</b>：CEC, Transformer, Transformer agent learning, Curriculum, curriculum captures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a new algorithm, Cross-Episodic Curriculum (CEC), to boost the learning efficiency and generalization of Transformer agents. Central to CEC is the placement of cross-episodic experiences into a Transformer's context, which forms the basis of a curriculum. By sequentially structuring online learning trials and mixed-quality demonstrations, CEC constructs curricula that encapsulate learning progression and proficiency increase across episodes. Such synergy combined with the potent pattern recognition capabilities of Transformer models delivers a powerful cross-episodic attention mechanism. The effectiveness of CEC is demonstrated under two representative scenarios: one involving multi-task reinforcement learning with discrete control, such as in DeepMind Lab, where the curriculum captures the learning progression in both individual and progressively complex settings; and the other involving imitation learning with mixed-quality data for continuous control, as seen in RoboMimic, where the curriculum captures the improvement in demonstrators' expertise. In all instances, policies resulting from CEC exhibit superior performance and strong generalization. Code is open-sourced at this https URL to facilitate research on Transformer agent learning.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Stronger Coreset Bounds for Kernel Density Estimators via Chaining</b></summary>
  <p><b>编号</b>：[25]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08548</p>
  <p><b>作者</b>：Rainie Bozzai,  Thomas Rothvoss</p>
  <p><b>备注</b>：23 pages</p>
  <p><b>关键词</b>：big, give improved bounds, frac, sqrt, varepsilon</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We apply the discrepancy method and a chaining approach to give improved bounds on the coreset complexity of a wide class of kernel functions. Our results give randomized polynomial time algorithms to produce coresets of size $O\big(\frac{\sqrt{d}}{\varepsilon}\sqrt{\log\log \frac{1}{\varepsilon}}\big)$ for the Gaussian and Laplacian kernels in the case that the data set is uniformly bounded, an improvement that was not possible with previous techniques. We also obtain coresets of size $O\big(\frac{1}{\varepsilon}\sqrt{\log\log \frac{1}{\varepsilon}}\big)$ for the Laplacian kernel for $d$ constant. Finally, we give the best known bounds of $O\big(\frac{\sqrt{d}}{\varepsilon}\sqrt{\log(2\max\{1,\alpha\})}\big)$ on the coreset complexity of the exponential, Hellinger, and JS Kernels, where $1/\alpha$ is the bandwidth parameter of the kernel.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Do pretrained Transformers Really Learn In-context by Gradient Descent?</b></summary>
  <p><b>编号</b>：[28]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08540</p>
  <p><b>作者</b>：Lingfeng Shen,  Aayush Mishra,  Daniel Khashabi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：In-Context Learning, ICL, implicitly equivalent, Gradient Descent, language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Is In-Context Learning (ICL) implicitly equivalent to Gradient Descent (GD)? Several recent works draw analogies between the dynamics of GD and the emergent behavior of ICL in large language models. However, these works make assumptions far from the realistic natural language setting in which language models are trained. Such discrepancies between theory and practice, therefore, necessitate further investigation to validate their applicability.
We start by highlighting the weaknesses in prior works that construct Transformer weights to simulate gradient descent. Their experiments with training Transformers on ICL objective, inconsistencies in the order sensitivity of ICL and GD, sparsity of the constructed weights, and sensitivity to parameter changes are some examples of a mismatch from the real-world setting.
Furthermore, we probe and compare the ICL vs. GD hypothesis in a natural setting. We conduct comprehensive empirical analyses on language models pretrained on natural data (LLaMa-7B). Our comparisons on various performance metrics highlight the inconsistent behavior of ICL and GD as a function of various factors such as datasets, models, and number of demonstrations. We observe that ICL and GD adapt the output distribution of language models differently. These results indicate that the equivalence between ICL and GD is an open hypothesis, requires nuanced considerations and calls for further studies.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Unsupervised Learning of Object-Centric Embeddings for Cell Instance  Segmentation in Microscopy Images</b></summary>
  <p><b>编号</b>：[42]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08501</p>
  <p><b>作者</b>：Steffen Wolf,  Manan Lalit,  Henry Westmacott,  Katie McDole,  Jan Funke</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：biomedical applications, microscopy images, image patches, method, embed image patches</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Segmentation of objects in microscopy images is required for many biomedical applications. We introduce object-centric embeddings (OCEs), which embed image patches such that the spatial offsets between patches cropped from the same object are preserved. Those learnt embeddings can be used to delineate individual objects and thus obtain instance segmentations. Here, we show theoretically that, under assumptions commonly found in microscopy images, OCEs can be learnt through a self-supervised task that predicts the spatial offset between image patches. Together, this forms an unsupervised cell instance segmentation method which we evaluate on nine diverse large-scale microscopy datasets. Segmentations obtained with our method lead to substantially improved results, compared to state-of-the-art baselines on six out of nine datasets, and perform on par on the remaining three datasets. If ground-truth annotations are available, our method serves as an excellent starting point for supervised training, reducing the required amount of ground-truth needed by one order of magnitude, thus substantially increasing the practical applicability of our method. Source code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Impact of time and note duration tokenizations on deep learning symbolic  music modeling</b></summary>
  <p><b>编号</b>：[43]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08497</p>
  <p><b>作者</b>：Nathan Fradet,  Nicolas Gutowski,  Fabien Chhel,  Jean-Pierre Briot</p>
  <p><b>备注</b>：ISMIR 2023</p>
  <p><b>关键词</b>：Music Information Retrieval, Information Retrieval, MIR, Retrieval, Symbolic music</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Symbolic music is widely used in various deep learning tasks, including generation, transcription, synthesis, and Music Information Retrieval (MIR). It is mostly employed with discrete models like Transformers, which require music to be tokenized, i.e., formatted into sequences of distinct elements called tokens. Tokenization can be performed in different ways. As Transformer can struggle at reasoning, but capture more easily explicit information, it is important to study how the way the information is represented for such model impact their performances. In this work, we analyze the common tokenization methods and experiment with time and note duration representations. We compare the performances of these two impactful criteria on several tasks, including composer and emotion classification, music generation, and sequence representation learning. We demonstrate that explicit information leads to better results depending on the task.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Prometheus: Inducing Fine-grained Evaluation Capability in Language  Models</b></summary>
  <p><b>编号</b>：[46]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08491</p>
  <p><b>作者</b>：Seungone Kim,  Jamin Shin,  Yejin Cho,  Joel Jang,  Shayne Longpre,  Hwaran Lee,  Sangdoo Yun,  Seongjin Shin,  Sungdong Kim,  James Thorne,  Minjoon Seo</p>
  <p><b>备注</b>：Work in Progress</p>
  <p><b>关键词</b>：powerful proprietary Large, proprietary Large Language, Large Language Model, Large Language, proprietary Large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, using a powerful proprietary Large Language Model (LLM) (e.g., GPT-4) as an evaluator for long-form responses has become the de facto standard. However, for practitioners with large-scale evaluation tasks and custom criteria in consideration (e.g., child-readability), using proprietary LLMs as an evaluator is unreliable due to the closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose Prometheus, a fully open-source LLM that is on par with GPT-4's evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. We first construct the Feedback Collection, a new dataset that consists of 1K fine-grained score rubrics, 20K instructions, and 100K responses and language feedback generated by GPT-4. Using the Feedback Collection, we train Prometheus, a 13B evaluator LLM that can assess any given long-form text based on customized score rubric provided by the user. Experimental results show that Prometheus scores a Pearson correlation of 0.897 with human evaluators when evaluating with 45 customized score rubrics, which is on par with GPT-4 (0.882), and greatly outperforms ChatGPT (0.392). Furthermore, measuring correlation with GPT-4 with 1222 customized score rubrics across four benchmarks (MT Bench, Vicuna Bench, Feedback Bench, Flask Eval) shows similar trends, bolstering Prometheus's capability as an evaluator LLM. Lastly, Prometheus achieves the highest accuracy on two human preference benchmarks (HHH Alignment & MT Bench Human Judgment) compared to open-sourced reward models explicitly trained on human preference datasets, highlighting its potential as an universal reward model. We open-source our code, dataset, and model at this https URL.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Can We Edit Multimodal Large Language Models?</b></summary>
  <p><b>编号</b>：[50]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08475</p>
  <p><b>作者</b>：Siyuan Cheng,  Bozhong Tian,  Qingbin Liu,  Xi Chen,  Yongheng Wang,  Huajun Chen,  Ningyu Zhang</p>
  <p><b>备注</b>：EMNLP 2023</p>
  <p><b>关键词</b>：Large Language Models, Multimodal Large Language, Large Language, editing Multimodal Large, Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in this https URL.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Strategies and impact of learning curve estimation for CNN-based image  classification</b></summary>
  <p><b>编号</b>：[53]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08470</p>
  <p><b>作者</b>：Laura Didyk,  Brayden Yarish,  Michael A. Beck,  Christopher P. Bidinosti,  Christopher J. Henry</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：machine learning models, learning models improves, Learning curves, models, Learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Learning curves are a measure for how the performance of machine learning models improves given a certain volume of training data. Over a wide variety of applications and models it was observed that learning curves follow -- to a large extent -- a power law behavior. This makes the performance of different models for a given task somewhat predictable and opens the opportunity to reduce the training time for practitioners, who are exploring the space of possible models and hyperparameters for the problem at hand. By estimating the learning curve of a model from training on small subsets of data only the best models need to be considered for training on the full dataset. How to choose subset sizes and how often to sample models on these to obtain estimates is however not researched. Given that the goal is to reduce overall training time strategies are needed that sample the performance in a time-efficient way and yet leads to accurate learning curve estimates. In this paper we formulate the framework for these strategies and propose several strategies. Further we evaluate the strategies for simulated learning curves and in experiments with popular datasets and models for image classification tasks.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：DistillSpec: Improving Speculative Decoding via Knowledge Distillation</b></summary>
  <p><b>编号</b>：[55]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08461</p>
  <p><b>作者</b>：Yongchao Zhou,  Kaifeng Lyu,  Ankit Singh Rawat,  Aditya Krishna Menon,  Afshin Rostamizadeh,  Sanjiv Kumar,  Jean-François Kagy,  Rishabh Agarwal</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：accelerates large language, generating multiple tokens, large language model, language model inference, target model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Speculative decoding (SD) accelerates large language model inference by employing a faster draft model for generating multiple tokens, which are then verified in parallel by the larger target model, resulting in the text generated according to the target model distribution. However, identifying a compact draft model that is well-aligned with the target model is challenging. To tackle this issue, we propose DistillSpec that uses knowledge distillation to better align the draft model with the target model, before applying SD. DistillSpec makes two key design choices, which we demonstrate via systematic study to be crucial to improving the draft and target alignment: utilizing on-policy data generation from the draft model, and tailoring the divergence function to the task and decoding strategy. Notably, DistillSpec yields impressive 10 - 45% speedups over standard SD on a range of standard benchmarks, using both greedy and non-greedy sampling. Furthermore, we combine DistillSpec with lossy SD to achieve fine-grained control over the latency vs. task performance trade-off. Finally, in practical scenarios with models of varying sizes, first using distillation to boost the performance of the target model and then applying DistillSpec to train a well-aligned draft model can reduce decoding latency by 6-10x with minimal performance drop, compared to standard decoding without distillation.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：A Survey on Heterogeneous Transfer Learning</b></summary>
  <p><b>编号</b>：[56]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08459</p>
  <p><b>作者</b>：Runxue Bao,  Yiming Sun,  Yuhe Gao,  Jindong Wang,  Qiang Yang,  Haifeng Chen,  Zhi-Hong Mao,  Xing Xie,  Ye Ye</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：enhance model performance, transfer learning, underpinning many real-world, approach utilizing knowledge, enhance model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The application of transfer learning, an approach utilizing knowledge from a source domain to enhance model performance in a target domain, has seen a tremendous rise in recent years, underpinning many real-world scenarios. The key to its success lies in the shared common knowledge between the domains, a prerequisite in most transfer learning methodologies. These methods typically presuppose identical feature spaces and label spaces in both domains, known as homogeneous transfer learning, which, however, is not always a practical assumption. Oftentimes, the source and target domains vary in feature spaces, data distributions, and label spaces, making it challenging or costly to secure source domain data with identical feature and label spaces as the target domain. Arbitrary elimination of these differences is not always feasible or optimal. Thus, heterogeneous transfer learning, acknowledging and dealing with such disparities, has emerged as a promising approach for a variety of tasks. Despite the existence of a survey in 2017 on this topic, the fast-paced advances post-2017 necessitate an updated, in-depth review. We therefore present a comprehensive survey of recent developments in heterogeneous transfer learning methods, offering a systematic guide for future research. Our paper reviews methodologies for diverse learning scenarios, discusses the limitations of current studies, and covers various application contexts, including Natural Language Processing, Computer Vision, Multimodality, and Biomedicine, to foster a deeper understanding and spur future research.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Towards Robust Multi-Modal Reasoning via Model Selection</b></summary>
  <p><b>编号</b>：[63]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08446</p>
  <p><b>作者</b>：Xiangyan Liu,  Rongxue Li,  Wei Ji,  Tao Lin</p>
  <p><b>备注</b>：10 pages, 5 figures</p>
  <p><b>关键词</b>：Large Language Model, Large Language, Language Model, model selection, recent research</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The reasoning capabilities of LLM (Large Language Model) are widely acknowledged in recent research, inspiring studies on tool learning and autonomous agents. LLM serves as the "brain" of agent, orchestrating multiple tools for collaborative multi-step task solving. Unlike methods invoking tools like calculators or weather APIs for straightforward tasks, multi-modal agents excel by integrating diverse AI models for complex challenges. However, current multi-modal agents neglect the significance of model selection: they primarily focus on the planning and execution phases, and will only invoke predefined task-specific models for each subtask, making the execution fragile. Meanwhile, other traditional model selection methods are either incompatible with or suboptimal for the multi-modal agent scenarios, due to ignorance of dependencies among subtasks arising by multi-step reasoning.
To this end, we identify the key challenges therein and propose the $\textit{M}^3$ framework as a plug-in with negligible runtime overhead at test-time. This framework improves model selection and bolsters the robustness of multi-modal agents in multi-step reasoning. In the absence of suitable benchmarks, we create MS-GQA, a new dataset specifically designed to investigate the model selection challenge in multi-modal agents. Our experiments reveal that our framework enables dynamic model selection, considering both user inputs and subtask dependencies, thereby robustifying the overall reasoning process. Our code and benchmark: this https URL.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Neural Sampling in Hierarchical Exponential-family Energy-based Models</b></summary>
  <p><b>编号</b>：[70]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08431</p>
  <p><b>作者</b>：Xingsi Dong,  Si Wu</p>
  <p><b>备注</b>：NeurIPS 2023</p>
  <p><b>关键词</b>：Bayesian brain theory, brain theory suggests, brain employs generative, employs generative models, theory suggests</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Bayesian brain theory suggests that the brain employs generative models to understand the external world. The sampling-based perspective posits that the brain infers the posterior distribution through samples of stochastic neuronal responses. Additionally, the brain continually updates its generative model to approach the true distribution of the external world. In this study, we introduce the Hierarchical Exponential-family Energy-based (HEE) model, which captures the dynamics of inference and learning. In the HEE model, we decompose the partition function into individual layers and leverage a group of neurons with shorter time constants to sample the gradient of the decomposed normalization term. This allows our model to estimate the partition function and perform inference simultaneously, circumventing the negative phase encountered in conventional energy-based models (EBMs). As a result, the learning process is localized both in time and space, and the model is easy to converge. To match the brain's rapid computation, we demonstrate that neural adaptation can serve as a momentum term, significantly accelerating the inference process. On natural image datasets, our model exhibits representations akin to those observed in the biological visual system. Furthermore, for the machine learning community, our model can generate observations through joint or marginal generation. We show that marginal generation outperforms joint generation and achieves performance on par with other EBMs.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Differentially Private Non-convex Learning for Multi-layer Neural  Networks</b></summary>
  <p><b>编号</b>：[73]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08425</p>
  <p><b>作者</b>：Hanpu Shen,  Cheng-Long Wang,  Zihang Xiang,  Yiming Ying,  Di Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Differentially Private Stochastic, Private Stochastic Optimization, Differentially Private, Private Stochastic, Stochastic Optimization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper focuses on the problem of Differentially Private Stochastic Optimization for (multi-layer) fully connected neural networks with a single output node. In the first part, we examine cases with no hidden nodes, specifically focusing on Generalized Linear Models (GLMs). We investigate the well-specific model where the random noise possesses a zero mean, and the link function is both bounded and Lipschitz continuous. We propose several algorithms and our analysis demonstrates the feasibility of achieving an excess population risk that remains invariant to the data dimension. We also delve into the scenario involving the ReLU link function, and our findings mirror those of the bounded link function. We conclude this section by contrasting well-specified and misspecified models, using ReLU regression as a representative example.
In the second part of the paper, we extend our ideas to two-layer neural networks with sigmoid or ReLU activation functions in the well-specified model. In the third part, we study the theoretical guarantees of DP-SGD in Abadi et al. (2016) for fully connected multi-layer neural networks. By utilizing recent advances in Neural Tangent Kernel theory, we provide the first excess population risk when both the sample size and the width of the network are sufficiently large. Additionally, we discuss the role of some parameters in DP-SGD regarding their utility, both theoretically and empirically.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Jailbreaking Black Box Large Language Models in Twenty Queries</b></summary>
  <p><b>编号</b>：[76]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08419</p>
  <p><b>作者</b>：Patrick Chao,  Alexander Robey,  Edgar Dobriban,  Hamed Hassani,  George J. Pappas,  Eric Wong</p>
  <p><b>备注</b>：21 pages, 10 figures</p>
  <p><b>关键词</b>：large language models, growing interest, interest in ensuring, ensuring that large, large language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM-2.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Towards Better Evaluation of Instruction-Following: A Case-Study in  Summarization</b></summary>
  <p><b>编号</b>：[86]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08394</p>
  <p><b>作者</b>：Ondrej Skopek,  Rahul Aralikatte,  Sian Gooding,  Victor Carbune</p>
  <p><b>备注</b>：Accepted to CoNLL 2023</p>
  <p><b>关键词</b>：follow user instructions, user instructions remains, large language models, recent advances, follow user</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite recent advances, evaluating how well large language models (LLMs) follow user instructions remains an open problem. While evaluation methods of language models have seen a rise in prompt-based approaches, limited work on the correctness of these methods has been conducted. In this work, we perform a meta-evaluation of a variety of metrics to quantify how accurately they measure the instruction-following abilities of LLMs. Our investigation is performed on grounded query-based summarization by collecting a new short-form, real-world dataset riSum, containing $300$ document-instruction pairs with $3$ answers each. All $900$ answers are rated by $3$ human annotators. Using riSum, we analyze agreement between evaluation methods and human judgment. Finally, we propose new LLM-based reference-free evaluation methods that improve upon established baselines and perform on-par with costly reference-based metrics which require high-quality summaries.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Introducing a Deep Neural Network-based Model Predictive Control  Framework for Rapid Controller Implementation</b></summary>
  <p><b>编号</b>：[87]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08392</p>
  <p><b>作者</b>：David C. Gordon,  Alexander Winkler,  Julian Bedei,  Patrick Schaber,  Jakob Andert,  Charles R. Koch</p>
  <p><b>备注</b>：Submitted to 2024 American Control Conference (ACC), July 8-12, 2024 in Toronto, Canada. ACC is the annual conference of the American Automatic Control Council (AACC), the U.S. national member organization of the International Federation for Automatic Control (IFAC)</p>
  <p><b>关键词</b>：Model Predictive Control, Predictive Control, optimal control solution, Model Predictive, optimal control technique</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Model Predictive Control (MPC) provides an optimal control solution based on a cost function while allowing for the implementation of process constraints. As a model-based optimal control technique, the performance of MPC strongly depends on the model used where a trade-off between model computation time and prediction performance exists. One solution is the integration of MPC with a machine learning (ML) based process model which are quick to evaluate online. This work presents the experimental implementation of a deep neural network (DNN) based nonlinear MPC for Homogeneous Charge Compression Ignition (HCCI) combustion control. The DNN model consists of a Long Short-Term Memory (LSTM) network surrounded by fully connected layers which was trained using experimental engine data and showed acceptable prediction performance with under 5% error for all outputs. Using this model, the MPC is designed to track the Indicated Mean Effective Pressure (IMEP) and combustion phasing trajectories, while minimizing several parameters. Using the acados software package to enable the real-time implementation of the MPC on an ARM Cortex A72, the optimization calculations are completed within 1.4 ms. The external A72 processor is integrated with the prototyping engine controller using a UDP connection allowing for rapid experimental deployment of the NMPC. The IMEP trajectory following of the developed controller was excellent, with a root-mean-square error of 0.133 bar, in addition to observing process constraints.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：MeanAP-Guided Reinforced Active Learning for Object Detection</b></summary>
  <p><b>编号</b>：[89]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08387</p>
  <p><b>作者</b>：Zhixuan Liang,  Xingyu Zeng,  Rui Zhao,  Ping Luo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：minimal labeled data, Active learning presents, Active learning, Reinforced Active Learning, active object detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Active learning presents a promising avenue for training high-performance models with minimal labeled data, achieved by judiciously selecting the most informative instances to label and incorporating them into the task learner. Despite notable advancements in active learning for image recognition, metrics devised or learned to gauge the information gain of data, crucial for query strategy design, do not consistently align with task model performance metrics, such as Mean Average Precision (MeanAP) in object detection tasks. This paper introduces MeanAP-Guided Reinforced Active Learning for Object Detection (MAGRAL), a novel approach that directly utilizes the MeanAP metric of the task model to devise a sampling strategy employing a reinforcement learning-based sampling agent. Built upon LSTM architecture, the agent efficiently explores and selects subsequent training instances, and optimizes the process through policy gradient with MeanAP serving as reward. Recognizing the time-intensive nature of MeanAP computation at each step, we propose fast look-up tables to expedite agent training. We assess MAGRAL's efficacy across popular benchmarks, PASCAL VOC and MS COCO, utilizing different backbone architectures. Empirical findings substantiate MAGRAL's superiority over recent state-of-the-art methods, showcasing substantial performance gains. MAGRAL establishes a robust baseline for reinforced active object detection, signifying its potential in advancing the field.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：AutoVP: An Automated Visual Prompting Framework and Benchmark</b></summary>
  <p><b>编号</b>：[92]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08381</p>
  <p><b>作者</b>：Hsi-Ai Tsao,  Lei Hsiung,  Pin-Yu Chen,  Sijia Liu,  Tsung-Yi Ho</p>
  <p><b>备注</b>：Preprint. The code is available at this https URL</p>
  <p><b>关键词</b>：emerging parameter-efficient fine-tuning, parameter-efficient fine-tuning approach, adapting pre-trained vision, downstream image-classification tasks, Visual prompting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Visual prompting (VP) is an emerging parameter-efficient fine-tuning approach to adapting pre-trained vision models to solve various downstream image-classification tasks. However, there has hitherto been little systematic study of the design space of VP and no clear benchmark for evaluating its performance. To bridge this gap, we propose AutoVP, an end-to-end expandable framework for automating VP design choices, along with 12 downstream image-classification tasks that can serve as a holistic VP-performance benchmark. Our design space covers 1) the joint optimization of the prompts; 2) the selection of pre-trained models, including image classifiers and text-image encoders; and 3) model output mapping strategies, including nonparametric and trainable label mapping. Our extensive experimental results show that AutoVP outperforms the best-known current VP methods by a substantial margin, having up to 6.7% improvement in accuracy; and attains a maximum performance increase of 27.5% compared to linear-probing (LP) baseline. AutoVP thus makes a two-fold contribution: serving both as an efficient tool for hyperparameter tuning on VP design choices, and as a comprehensive benchmark that can reasonably be expected to accelerate VP's development. The source code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：MCU: A Task-centric Framework for Open-ended Agent Evaluation in  Minecraft</b></summary>
  <p><b>编号</b>：[100]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08367</p>
  <p><b>作者</b>：Haowei Lin,  Zihao Wang,  Jianzhu Ma,  Yitao Liang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：framework named MCU, task-centric framework named, MCU framework, open-ended game environment, MCU</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To pursue the goal of creating an open-ended agent in Minecraft, an open-ended game environment with unlimited possibilities, this paper introduces a task-centric framework named MCU for Minecraft agent evaluation. The MCU framework leverages the concept of atom tasks as fundamental building blocks, enabling the generation of diverse or even arbitrary tasks. Within the MCU framework, each task is measured with six distinct difficulty scores (time consumption, operational effort, planning complexity, intricacy, creativity, novelty). These scores offer a multi-dimensional assessment of a task from different angles, and thus can reveal an agent's capability on specific facets. The difficulty scores also serve as the feature of each task, which creates a meaningful task space and unveils the relationship between tasks. For efficient evaluation of Minecraft agents employing the MCU framework, we maintain a unified benchmark, namely SkillForge, which comprises representative tasks with diverse categories and difficulty distribution. We also provide convenient filters for users to select tasks to assess specific capabilities of agents. We show that MCU has the high expressivity to cover all tasks used in recent literature on Minecraft agent, and underscores the need for advancements in areas such as creativity, precise control, and out-of-distribution generalization under the goal of open-ended Minecraft agent development.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Towards Demystifying the Generalization Behaviors When Neural Collapse  Emerges</b></summary>
  <p><b>编号</b>：[105]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08358</p>
  <p><b>作者</b>：Peifeng Gao,  Qianqian Xu,  Yibo Yang,  Peisong Wen,  Huiyang Shao,  Zhiyong Yang,  Bernard Ghanem,  Qingming Huang</p>
  <p><b>备注</b>：20 pages, 6 figures. arXiv admin note: substantial text overlap with arXiv:2304.08914</p>
  <p><b>关键词</b>：deep neural networks, terminal phase, Neural Collapse, deep neural, neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural Collapse (NC) is a well-known phenomenon of deep neural networks in the terminal phase of training (TPT). It is characterized by the collapse of features and classifier into a symmetrical structure, known as simplex equiangular tight frame (ETF). While there have been extensive studies on optimization characteristics showing the global optimality of neural collapse, little research has been done on the generalization behaviors during the occurrence of NC. Particularly, the important phenomenon of generalization improvement during TPT has been remaining in an empirical observation and lacking rigorous theoretical explanation. In this paper, we establish the connection between the minimization of CE and a multi-class SVM during TPT, and then derive a multi-class margin generalization bound, which provides a theoretical explanation for why continuing training can still lead to accuracy improvement on test set, even after the train accuracy has reached 100%. Additionally, our further theoretical results indicate that different alignment between labels and features in a simplex ETF can result in varying degrees of generalization improvement, despite all models reaching NC and demonstrating similar optimization performance on train set. We refer to this newly discovered property as "non-conservative generalization". In experiments, we also provide empirical observations to verify the indications suggested by our theoretical results.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：LightZero: A Unified Benchmark for Monte Carlo Tree Search in General  Sequential Decision Scenarios</b></summary>
  <p><b>编号</b>：[110]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08348</p>
  <p><b>作者</b>：Yazhe Niu,  Yuan Pu,  Zhenjie Yang,  Xueyan Li,  Tong Zhou,  Jiyuan Ren,  Shuai Hu,  Hongsheng Li,  Yu Liu</p>
  <p><b>备注</b>：NeurIPS 2023 Spotlight</p>
  <p><b>关键词</b>：achieved remarkable success, Carlo Tree Search, classic decision-making problems, Monte Carlo Tree, tree-search planning capabilities</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Building agents based on tree-search planning capabilities with learned models has achieved remarkable success in classic decision-making problems, such as Go and Atari. However, it has been deemed challenging or even infeasible to extend Monte Carlo Tree Search (MCTS) based algorithms to diverse real-world applications, especially when these environments involve complex action spaces and significant simulation costs, or inherent stochasticity. In this work, we introduce LightZero, the first unified benchmark for deploying MCTS/MuZero in general sequential decision scenarios. Specificially, we summarize the most critical challenges in designing a general MCTS-style decision-making solver, then decompose the tightly-coupled algorithm and system design of tree-search RL methods into distinct sub-modules. By incorporating more appropriate exploration and optimization strategies, we can significantly enhance these sub-modules and construct powerful LightZero agents to tackle tasks across a wide range of domains, such as board games, Atari, MuJoCo, MiniGrid and GoBigger. Detailed benchmark results reveal the significant potential of such methods in building scalable and efficient decision intelligence. The code is available as part of OpenDILab at this https URL.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：A Generic Software Framework for Distributed Topological Analysis  Pipelines</b></summary>
  <p><b>编号</b>：[113]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08339</p>
  <p><b>作者</b>：Eve Le Guillou,  Michael Will,  Pierre Guillou,  Jonas Lukasczyk,  Pierre Fortin,  Christoph Garth,  Julien Tierny</p>
  <p><b>备注</b>：18 pages, 12 figures</p>
  <p><b>关键词</b>：system paper presents, topological analysis pipelines, topological analysis, analysis pipelines, TTK</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This system paper presents a software framework for the support of topological analysis pipelines in a distributed-memory model. While several recent papers introduced topology-based approaches for distributed-memory environments, these were reporting experiments obtained with tailored, mono-algorithm implementations. In contrast, we describe in this paper a general-purpose, generic framework for topological analysis pipelines, i.e. a sequence of topological algorithms interacting together, possibly on distinct numbers of processes. Specifically, we instantiated our framework with the MPI model, within the Topology ToolKit (TTK). While developing this framework, we faced several algorithmic and software engineering challenges, which we document in this paper. We provide a taxonomy for the distributed-memory topological algorithms supported by TTK, depending on their communication needs and provide examples of hybrid MPI+thread parallelizations. Detailed performance analyses show that parallel efficiencies range from $20\%$ to $80\%$ (depending on the algorithms), and that the MPI-specific preconditioning introduced by our framework induces a negligible computation time overhead. We illustrate the new distributed-memory capabilities of TTK with an example of advanced analysis pipeline, combining multiple algorithms, run on the largest publicly available dataset we have found (120 billion vertices) on a standard cluster with 64 nodes (for a total of 1,536 cores). Finally, we provide a roadmap for the completion of TTK's MPI extension, along with generic recommendations for each algorithm communication category.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Neural Diffusion Models</b></summary>
  <p><b>编号</b>：[114]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08337</p>
  <p><b>作者</b>：Grigory Bartosh,  Dmitry Vetrov,  Christian A. Naesseth</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：shown remarkable performance, Diffusion models, conventional diffusion models, Neural Diffusion Models, shown remarkable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diffusion models have shown remarkable performance on many generative tasks. Despite recent success, most diffusion models are restricted in that they only allow linear transformation of the data distribution. In contrast, broader family of transformations can potentially help train generative distributions more efficiently, simplifying the reverse process and closing the gap between the true negative log-likelihood and the variational approximation. In this paper, we present Neural Diffusion Models (NDMs), a generalization of conventional diffusion models that enables defining and learning time-dependent non-linear transformations of data. We show how to optimise NDMs using a variational bound in a simulation-free setting. Moreover, we derive a time-continuous formulation of NDMs, which allows fast and reliable inference using off-the-shelf numerical ODE and SDE solvers. Finally, we demonstrate the utility of NDMs with learnable transformations through experiments on standard image generation benchmarks, including CIFAR-10, downsampled versions of ImageNet and CelebA-HQ. NDMs outperform conventional diffusion models in terms of likelihood and produce high-quality samples.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Defending Our Privacy With Backdoors</b></summary>
  <p><b>编号</b>：[121]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08320</p>
  <p><b>作者</b>：Dominik Hintersdorf,  Lukas Struppek,  Daniel Neider,  Kristian Kersting</p>
  <p><b>备注</b>：14 pages, 4 figures</p>
  <p><b>关键词</b>：raised significant privacy, significant privacy concerns, proliferation of large, raised significant, web-scraped data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The proliferation of large AI models trained on uncurated, often sensitive web-scraped data has raised significant privacy concerns. One of the concerns is that adversaries can extract information about the training data using privacy attacks. Unfortunately, the task of removing specific information from the models without sacrificing performance is not straightforward and has proven to be challenging. We propose a rather easy yet effective defense based on backdoor attacks to remove private information such as names of individuals from models, and focus in this work on text encoders. Specifically, through strategic insertion of backdoors, we align the embeddings of sensitive phrases with those of neutral terms-"a person" instead of the person's name. Our empirical results demonstrate the effectiveness of our backdoor-based defense on CLIP by assessing its performance using a specialized privacy attack for zero-shot classifiers. Our approach provides not only a new "dual-use" perspective on backdoor attacks, but also presents a promising avenue to enhance the privacy of individuals within models trained on uncurated web-scraped data.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：GePSAn: Generative Procedure Step Anticipation in Cooking Videos</b></summary>
  <p><b>编号</b>：[125]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08312</p>
  <p><b>作者</b>：Mohamed Ashraf Abdelsalam,  Samrudhdhi B. Rangrej,  Isma Hadji,  Nikita Dvornik,  Konstantinos G. Derpanis,  Afsaneh Fazly</p>
  <p><b>备注</b>：published at ICCV 2023</p>
  <p><b>关键词</b>：step, video, future step, future, model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the problem of future step anticipation in procedural videos. Given a video of an ongoing procedural activity, we predict a plausible next procedure step described in rich natural language. While most previous work focus on the problem of data scarcity in procedural video datasets, another core challenge of future anticipation is how to account for multiple plausible future realizations in natural settings. This problem has been largely overlooked in previous work. To address this challenge, we frame future step prediction as modelling the distribution of all possible candidates for the next step. Specifically, we design a generative model that takes a series of video clips as input, and generates multiple plausible and diverse candidates (in natural language) for the next step. Following previous work, we side-step the video annotation scarcity by pretraining our model on a large text-based corpus of procedural activities, and then transfer the model to the video domain. Our experiments, both in textual and video domains, show that our model captures diversity in the next step prediction and generates multiple plausible future predictions. Moreover, our model establishes new state-of-the-art results on YouCookII, where it outperforms existing baselines on the next step anticipation. Finally, we also show that our model can successfully transfer from text to the video domain zero-shot, ie, without fine-tuning or adaptation, and produces good-quality future step predictions from video.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：CHIP: Contrastive Hierarchical Image Pretraining</b></summary>
  <p><b>编号</b>：[128]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08304</p>
  <p><b>作者</b>：Arpit Mittal,  Harshil Jhaveri,  Swapnil Mallick,  Abhishek Ajmera</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：limited number, Few-shot object classification, few-shot classification model, few-shot classification, model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Few-shot object classification is the task of classifying objects in an image with limited number of examples as supervision. We propose a one-shot/few-shot classification model that can classify an object of any unseen class into a relatively general category in an hierarchically based classification. Our model uses a three-level hierarchical contrastive loss based ResNet152 classifier for classifying an object based on its features extracted from Image embedding, not used during the training phase. For our experimentation, we have used a subset of the ImageNet (ILSVRC-12) dataset that contains only the animal classes for training our model and created our own dataset of unseen classes for evaluating our trained model. Our model provides satisfactory results in classifying the unknown objects into a generic category which has been later discussed in greater detail.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Data driven modeling of self-similar dynamics</b></summary>
  <p><b>编号</b>：[134]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08282</p>
  <p><b>作者</b>：Ruyi Tao,  Ningning Tao,  Yizhuang You,  Jiang Zhang</p>
  <p><b>备注</b>：10 pages,4 figures,1 table</p>
  <p><b>关键词</b>：understanding their intricacies, complex systems, crucial for understanding, systems, complex</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multiscale modeling of complex systems is crucial for understanding their intricacies. Data-driven multiscale modeling has emerged as a promising approach to tackle challenges associated with complex systems. On the other hand, self-similarity is prevalent in complex systems, hinting that large-scale complex systems can be modeled at a reduced cost. In this paper, we introduce a multiscale neural network framework that incorporates self-similarity as prior knowledge, facilitating the modeling of self-similar dynamical systems. For deterministic dynamics, our framework can discern whether the dynamics are self-similar. For uncertain dynamics, it can compare and determine which parameter set is closer to self-similarity. The framework allows us to extract scale-invariant kernels from the dynamics for modeling at any scale. Moreover, our method can identify the power law exponents in self-similar systems. Preliminary tests on the Ising model yielded critical exponents consistent with theoretical expectations, providing valuable insights for addressing critical phase transitions in non-equilibrium systems.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Lag-Llama: Towards Foundation Models for Time Series Forecasting</b></summary>
  <p><b>编号</b>：[137]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08278</p>
  <p><b>作者</b>：Kashif Rasul,  Arjun Ashok,  Andrew Robert Williams,  Arian Khorasani,  George Adamopoulos,  Rishika Bhagwatkar,  Marin Biloš,  Hena Ghonia,  Nadhir Vincent Hassen,  Anderson Schneider,  Sahil Garg,  Alexandre Drouin,  Nicolas Chapados,  Yuriy Nevmyvaka,  Irina Rish</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：general-purpose univariate probabilistic, probabilistic time-series forecasting, univariate probabilistic time-series, forecasting model trained, build foundation models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Aiming to build foundation models for time-series forecasting and study their scaling behavior, we present here our work-in-progress on Lag-Llama, a general-purpose univariate probabilistic time-series forecasting model trained on a large collection of time-series data. The model shows good zero-shot prediction capabilities on unseen "out-of-distribution" time-series datasets, outperforming supervised baselines. We use smoothly broken power-laws to fit and predict model scaling behavior. The open source code is made available at this https URL.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Invisible Threats: Backdoor Attack in OCR Systems</b></summary>
  <p><b>编号</b>：[147]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08259</p>
  <p><b>作者</b>：Mauro Conti,  Nicola Farronato,  Stefanos Koffas,  Luca Pajola,  Stjepan Picek</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Optical Character Recognition, Character Recognition, scanned documents, widely used tool, tool to extract</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Optical Character Recognition (OCR) is a widely used tool to extract text from scanned documents. Today, the state-of-the-art is achieved by exploiting deep neural networks. However, the cost of this performance is paid at the price of system vulnerability. For instance, in backdoor attacks, attackers compromise the training phase by inserting a backdoor in the victim's model that will be activated at testing time by specific patterns while leaving the overall model performance intact. This work proposes a backdoor attack for OCR resulting in the injection of non-readable characters from malicious input images. This simple but effective attack exposes the state-of-the-art OCR weakness, making the extracted text correct to human eyes but simultaneously unusable for the NLP application that uses OCR as a preprocessing step. Experimental results show that the attacked models successfully output non-readable characters for around 90% of the poisoned instances without harming their performance for the remaining instances.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Impact of Co-occurrence on Factual Knowledge of Large Language Models</b></summary>
  <p><b>编号</b>：[148]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08256</p>
  <p><b>作者</b>：Cheongwoong Kang,  Jaesik Choi</p>
  <p><b>备注</b>：EMNLP 2023 Findings</p>
  <p><b>关键词</b>：make factually incorrect, factually incorrect responses, make factually, factually incorrect, incorrect responses</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) often make factually incorrect responses despite their success in various applications. In this paper, we hypothesize that relying heavily on simple co-occurrence statistics of the pre-training corpora is one of the main factors that cause factual errors. Our results reveal that LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently co-occurred words over the correct answer. Consequently, LLMs struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they are seen during finetuning. We show that co-occurrence bias remains despite scaling up model sizes or finetuning. Therefore, we suggest finetuning on a debiased dataset to mitigate the bias by filtering out biased samples whose subject-object co-occurrence count is high. Although debiased finetuning allows LLMs to memorize rare facts in the training set, it is not effective in recalling rare facts unseen during finetuning. Further research in mitigation will help build reliable language models by preventing potential errors. The code is available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with  Reinforcement Learning</b></summary>
  <p><b>编号</b>：[150]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08252</p>
  <p><b>作者</b>：Zeyuan Ma,  Hongshu Guo,  Jiacheng Chen,  Zhenrui Li,  Guojun Peng,  Yue-Jiao Gong,  Yining Ma,  Zhiguang Cao</p>
  <p><b>备注</b>：Accepted at NuerIPS 2023</p>
  <p><b>关键词</b>：Optimization with Reinforcement, Reinforcement Learning, mitigate manual fine-tuning, showcased the power, power of leveraging</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, Meta-Black-Box Optimization with Reinforcement Learning (MetaBBO-RL) has showcased the power of leveraging RL at the meta-level to mitigate manual fine-tuning of low-level black-box optimizers. However, this field is hindered by the lack of a unified benchmark. To fill this gap, we introduce MetaBox, the first benchmark platform expressly tailored for developing and evaluating MetaBBO-RL methods. MetaBox offers a flexible algorithmic template that allows users to effortlessly implement their unique designs within the platform. Moreover, it provides a broad spectrum of over 300 problem instances, collected from synthetic to realistic scenarios, and an extensive library of 19 baseline methods, including both traditional black-box optimizers and recent MetaBBO-RL methods. Besides, MetaBox introduces three standardized performance metrics, enabling a more thorough assessment of the methods. In a bid to illustrate the utility of MetaBox for facilitating rigorous evaluation and in-depth analysis, we carry out a wide-ranging benchmarking study on existing MetaBBO-RL methods. Our MetaBox is open-source and accessible at: this https URL.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：GROOT: Learning to Follow Instructions by Watching Gameplay Videos</b></summary>
  <p><b>编号</b>：[158]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08235</p>
  <p><b>作者</b>：Shaofei Cai,  Bowei Zhang,  Zihao Wang,  Xiaojian Ma,  Anji Liu,  Yitao Liang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：follow open-ended instructions, study the problem, problem of building, follow open-ended, open-ended instructions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the problem of building a controller that can follow open-ended instructions in open-world environments. We propose to follow reference videos as instructions, which offer expressive goal specifications while eliminating the need for expensive text-gameplay annotations. A new learning framework is derived to allow learning such instruction-following controllers from gameplay videos while producing a video instruction encoder that induces a structured goal space. We implement our agent GROOT in a simple yet effective encoder-decoder architecture based on causal transformers. We evaluate GROOT against open-world counterparts and human players on a proposed Minecraft SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the human-machine gap as well as exhibiting a 70% winning rate over the best generalist agent baseline. Qualitative analysis of the induced goal space further demonstrates some interesting emergent properties, including the goal composition and complex gameplay behavior synthesis. Code and video can be found on the website this https URL.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Emergence of Latent Binary Encoding in Deep Neural Network Classifiers</b></summary>
  <p><b>编号</b>：[164]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08224</p>
  <p><b>作者</b>：Luigi Sbailò,  Luca Ghiringhelli</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：binary encoding, observe the emergence, latent space, binary encoding accelerates, linear penultimate layer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We observe the emergence of binary encoding within the latent space of deep-neural-network classifiers. Such binary encoding is induced by introducing a linear penultimate layer, which is equipped during training with a loss function that grows as $\exp(\vec{x}^2)$, where $\vec{x}$ are the coordinates in the latent space. The phenomenon we describe represents a specific instance of a well-documented occurrence known as \textit{neural collapse}, which arises in the terminal phase of training and entails the collapse of latent class means to the vertices of a simplex equiangular tight frame (ETF). We show that binary encoding accelerates convergence toward the simplex ETF and enhances classification accuracy.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：SimCKP: Simple Contrastive Learning of Keyphrase Representations</b></summary>
  <p><b>编号</b>：[166]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08221</p>
  <p><b>作者</b>：Minseok Choi,  Chaeheon Gwak,  Seho Kim,  Si Hyeong Kim,  Jaegul Choo</p>
  <p><b>备注</b>：Accepted to Findings of EMNLP 2023</p>
  <p><b>关键词</b>：aims to generate, aims to identify, generate a set, set of summarizing, summarizing words</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Keyphrase generation (KG) aims to generate a set of summarizing words or phrases given a source document, while keyphrase extraction (KE) aims to identify them from the text. Because the search space is much smaller in KE, it is often combined with KG to predict keyphrases that may or may not exist in the corresponding document. However, current unified approaches adopt sequence labeling and maximization-based generation that primarily operate at a token level, falling short in observing and scoring keyphrases as a whole. In this work, we propose SimCKP, a simple contrastive learning framework that consists of two stages: 1) An extractor-generator that extracts keyphrases by learning context-aware phrase-level representations in a contrastive manner while also generating keyphrases that do not appear in the document; 2) A reranker that adapts scores for each generated phrase by likewise aligning their representations with the corresponding document. Experimental results on multiple benchmark datasets demonstrate the effectiveness of our proposed approach, which outperforms the state-of-the-art models by a significant margin.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：TriRE: A Multi-Mechanism Learning Paradigm for Continual Knowledge  Retention and Promotion</b></summary>
  <p><b>编号</b>：[168]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08217</p>
  <p><b>作者</b>：Preetha Vijayan,  Prashant Bhat,  Elahe Arani,  Bahram Zonooz</p>
  <p><b>备注</b>：Accepted at 37th Conference on Neural Information Processing Systems (NeurIPS 2023)</p>
  <p><b>关键词</b>：deep neural networks, neural networks due, previously learned tasks, Continual learning, persistent challenge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Continual learning (CL) has remained a persistent challenge for deep neural networks due to catastrophic forgetting (CF) of previously learned tasks. Several techniques such as weight regularization, experience rehearsal, and parameter isolation have been proposed to alleviate CF. Despite their relative success, these research directions have predominantly remained orthogonal and suffer from several shortcomings, while missing out on the advantages of competing strategies. On the contrary, the brain continually learns, accommodates, and transfers knowledge across tasks by simultaneously leveraging several neurophysiological processes, including neurogenesis, active forgetting, neuromodulation, metaplasticity, experience rehearsal, and context-dependent gating, rarely resulting in CF. Inspired by how the brain exploits multiple mechanisms concurrently, we propose TriRE, a novel CL paradigm that encompasses retaining the most prominent neurons for each task, revising and solidifying the extracted knowledge of current and past tasks, and actively promoting less active neurons for subsequent tasks through rewinding and relearning. Across CL settings, TriRE significantly reduces task interference and surpasses different CL approaches considered in isolation.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Trustworthy Machine Learning</b></summary>
  <p><b>编号</b>：[169]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08215</p>
  <p><b>作者</b>：Bálint Mucsányi,  Michael Kirchhof,  Elisa Nguyen,  Alexander Rubinstein,  Seong Joon Oh</p>
  <p><b>备注</b>：373 pages, textbook at the University of T\"ubingen</p>
  <p><b>关键词</b>：machine learning technology, Trustworthy Machine Learning, machine learning, challenges have emerged, applied to actual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As machine learning technology gets applied to actual products and solutions, new challenges have emerged. Models unexpectedly fail to generalize to small changes in the distribution, tend to be confident on novel data they have never seen, or cannot communicate the rationale behind their decisions effectively with the end users. Collectively, we face a trustworthiness issue with the current machine learning technology. This textbook on Trustworthy Machine Learning (TML) covers a theoretical and technical background of four key topics in TML: Out-of-Distribution Generalization, Explainability, Uncertainty Quantification, and Evaluation of Trustworthiness. We discuss important classical and contemporary research papers of the aforementioned fields and uncover and connect their underlying intuitions. The book evolved from the homonymous course at the University of Tübingen, first offered in the Winter Semester of 2022/23. It is meant to be a stand-alone product accompanied by code snippets and various pointers to further sources on topics of TML. The dedicated website of the book is this https URL.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Lifelong Audio-video Masked Autoencoder with Forget-robust Localized  Alignments</b></summary>
  <p><b>编号</b>：[174]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08204</p>
  <p><b>作者</b>：Jaewoo Lee,  Jaehong Yoon,  Wonjae Kim,  Yunji Kim,  Sung Ju Hwang</p>
  <p><b>备注</b>：Preprint, project page: this https URL</p>
  <p><b>关键词</b>：distribution continually shifts, lifelong audio-video masked, audio-video masked autoencoder, distribution continually, continually shifts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a lifelong audio-video masked autoencoder that continually learns the multimodal representations from a video stream containing audio-video pairs, while its distribution continually shifts over time. Specifically, we propose two novel ideas to tackle the problem: (1) Localized Alignment: We introduce a small trainable multimodal encoder that predicts the audio and video tokens that are well-aligned with each other. This allows the model to learn only the highly correlated audiovisual patches with accurate multimodal relationships. (2) Forget-robust multimodal patch selection: We compare the relative importance of each audio-video patch between the current and past data pair to mitigate unintended drift of the previously learned audio-video representations. Our proposed method, FLAVA (Forget-robust Localized Audio-Video Alignment), therefore, captures the complex relationships between the audio and video modalities during training on a sequence of pre-training tasks while alleviating the forgetting of learned audiovisual correlations. Our experiments validate that FLAVA outperforms the state-of-the-art continual learning methods on several benchmark datasets under continual audio-video representation learning scenarios.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Beyond Traditional DoE: Deep Reinforcement Learning for Optimizing  Experiments in Model Identification of Battery Dynamics</b></summary>
  <p><b>编号</b>：[175]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08198</p>
  <p><b>作者</b>：Gokhan Budan,  Francesca Damiani,  Can Kurtulus,  N. Kemal Ure</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：energy management systems, design processes rely, battery dynamics, energy research, energy management</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Model identification of battery dynamics is a central problem in energy research; many energy management systems and design processes rely on accurate battery models for efficiency optimization. The standard methodology for battery modelling is traditional design of experiments (DoE), where the battery dynamics are excited with many different current profiles and the measured outputs are used to estimate the system dynamics. However, although it is possible to obtain useful models with the traditional approach, the process is time consuming and expensive because of the need to sweep many different current-profile configurations. In the present work, a novel DoE approach is developed based on deep reinforcement learning, which alters the configuration of the experiments on the fly based on the statistics of past experiments. Instead of sticking to a library of predefined current profiles, the proposed approach modifies the current profiles dynamically by updating the output space covered by past measurements, hence only the current profiles that are informative for future experiments are applied. Simulations and real experiments are used to show that the proposed approach gives models that are as accurate as those obtained with traditional DoE but by using 85\% less resources.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Learn From Model Beyond Fine-Tuning: A Survey</b></summary>
  <p><b>编号</b>：[180]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08184</p>
  <p><b>作者</b>：Hongling Zheng,  Li Shen,  Anke Tang,  Yong Luo,  Han Hu,  Bo Du,  Dacheng Tao</p>
  <p><b>备注</b>：20 pages, 9 figures</p>
  <p><b>关键词</b>：natural language processing, demonstrated remarkable performance, model, computer vision, primarily attributed</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Foundation models (FM) have demonstrated remarkable performance across a wide range of tasks (especially in the fields of natural language processing and computer vision), primarily attributed to their ability to comprehend instructions and access extensive, high-quality data. This not only showcases their current effectiveness but also sets a promising trajectory towards the development of artificial general intelligence. Unfortunately, due to multiple constraints, the raw data of the model used for large model training are often inaccessible, so the use of end-to-end models for downstream tasks has become a new research trend, which we call Learn From Model (LFM) in this article. LFM focuses on the research, modification, and design of FM based on the model interface, so as to better understand the model structure and weights (in a black box environment), and to generalize the model to downstream tasks. The study of LFM techniques can be broadly categorized into five major areas: model tuning, model distillation, model reuse, meta learning and model editing. Each category encompasses a repertoire of methods and strategies that aim to enhance the capabilities and performance of FM. This paper gives a comprehensive review of the current methods based on FM from the perspective of LFM, in order to help readers better understand the current research status and ideas. To conclude, we summarize the survey by highlighting several critical areas for future exploration and addressing open issues that require further attention from the research community. The relevant papers we investigated in this article can be accessed at <this https url.< p>
  </this></p></details>
</details>
<details>
  <summary>48. <b>标题：XIMAGENET-12: An Explainable AI Benchmark Dataset for Model Robustness  Evaluation</b></summary>
  <p><b>编号</b>：[181]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08182</p>
  <p><b>作者</b>：Qiang Li,  Dan Zhang,  Shengzhao Lei,  Xun Zhao,  Shuyan Li,  Porawit Kamnoedboon,  WeiWei Li</p>
  <p><b>备注</b>：UnderSubmission</p>
  <p><b>关键词</b>：academically validated robust, numerous unrelated benchmark, problematic practical adoption, unrelated benchmark datasets, standardized robustness metrics</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The lack of standardized robustness metrics and the widespread reliance on numerous unrelated benchmark datasets for testing have created a gap between academically validated robust models and their often problematic practical adoption. To address this, we introduce XIMAGENET-12, an explainable benchmark dataset with over 200K images and 15,600 manual semantic annotations. Covering 12 categories from ImageNet to represent objects commonly encountered in practical life and simulating six diverse scenarios, including overexposure, blurring, color changing, etc., we further propose a novel robustness criterion that extends beyond model generation ability assessment. This benchmark dataset, along with related code, is available at this https URL. Researchers and practitioners can leverage this resource to evaluate the robustness of their visual models under challenging conditions and ultimately benefit from the demands of practical computer vision systems.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Improving Fast Minimum-Norm Attacks with Hyperparameter Optimization</b></summary>
  <p><b>编号</b>：[182]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08177</p>
  <p><b>作者</b>：Giuseppe Floris,  Raffaele Mura,  Luca Scionis,  Giorgio Piras,  Maura Pintor,  Ambra Demontis,  Battista Biggio</p>
  <p><b>备注</b>：Accepted at ESANN23</p>
  <p><b>关键词</b>：machine learning models, fast minimum-norm attacks, Evaluating the adversarial, adversarial robustness, robustness of machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Evaluating the adversarial robustness of machine learning models using gradient-based attacks is challenging. In this work, we show that hyperparameter optimization can improve fast minimum-norm attacks by automating the selection of the loss function, the optimizer and the step-size scheduler, along with the corresponding hyperparameters. Our extensive evaluation involving several robust models demonstrates the improved efficacy of fast minimum-norm attacks when hyper-up with hyperparameter optimization. We release our open-source code at this https URL.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Infinite Width Graph Neural Networks for Node Regression/ Classification</b></summary>
  <p><b>编号</b>：[183]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08176</p>
  <p><b>作者</b>：Yunus Cobanoglu</p>
  <p><b>备注</b>：50 Pages, 2 Figures (with subfigures)o, multiple tables</p>
  <p><b>关键词</b>：Deep Neural Nets, Graph Neural Network, Fully-Connected Deep Neural, Width Neural Networks, Neural Networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This work analyzes Graph Neural Networks, a generalization of Fully-Connected Deep Neural Nets on Graph structured data, when their width, that is the number of nodes in each fullyconnected layer is increasing to infinity. Infinite Width Neural Networks are connecting Deep Learning to Gaussian Processes and Kernels, both Machine Learning Frameworks with long traditions and extensive theoretical foundations. Gaussian Processes and Kernels have much less hyperparameters then Neural Networks and can be used for uncertainty estimation, making them more user friendly for applications. This works extends the increasing amount of research connecting Gaussian Processes and Kernels to Neural Networks. The Kernel and Gaussian Process closed forms are derived for a variety of architectures, namely the standard Graph Neural Network, the Graph Neural Network with Skip-Concatenate Connections and the Graph Attention Neural Network. All architectures are evaluated on a variety of datasets on the task of transductive Node Regression and Classification. Additionally, a Spectral Sparsification method known as Effective Resistance is used to improve runtime and memory requirements. Extending the setting to inductive graph learning tasks (Graph Regression/ Classification) is straightforward and is briefly discussed in 3.5.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse  Autoencoders</b></summary>
  <p><b>编号</b>：[188]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08164</p>
  <p><b>作者</b>：Luke Marks,  Amir Abdullah,  Luna Mendez,  Rauno Arike,  Philip Torr,  Fazl Barez</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large language models, human feedback, Large language, underpin many commercial, human preferences</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) aligned to human preferences via reinforcement learning from human feedback (RLHF) underpin many commercial applications. However, how RLHF impacts LLM internals remains opaque. We propose a novel method to interpret learned reward functions in RLHF-tuned LLMs using sparse autoencoders. Our approach trains autoencoder sets on activations from a base LLM and its RLHF-tuned version. By comparing autoencoder hidden spaces, we identify unique features that reflect the accuracy of the learned reward model. To quantify this, we construct a scenario where the tuned LLM learns token-reward mappings to maximize reward. This is the first application of sparse autoencoders for interpreting learned rewards and broadly inspecting reward learning in LLMs. Our method provides an abstract approximation of reward integrity. This presents a promising technique for ensuring alignment between specified objectives and model behaviors.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Open-Set Knowledge-Based Visual Question Answering with Inference Paths</b></summary>
  <p><b>编号</b>：[193]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08148</p>
  <p><b>作者</b>：Jingru Gan,  Xinzhe Han,  Shuhui Wang,  Qingming Huang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Knowledge-Based Visual Question, Visual Question Answering, Knowledge-Based Visual, external knowledge bases, purpose of Knowledge-Based</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Given an image and an associated textual question, the purpose of Knowledge-Based Visual Question Answering (KB-VQA) is to provide a correct answer to the question with the aid of external knowledge bases. Prior KB-VQA models are usually formulated as a retriever-classifier framework, where a pre-trained retriever extracts textual or visual information from knowledge graphs and then makes a prediction among the candidates. Despite promising progress, there are two drawbacks with existing models. Firstly, modeling question-answering as multi-class classification limits the answer space to a preset corpus and lacks the ability of flexible reasoning. Secondly, the classifier merely consider "what is the answer" without "how to get the answer", which cannot ground the answer to explicit reasoning paths. In this paper, we confront the challenge of \emph{explainable open-set} KB-VQA, where the system is required to answer questions with entities at wild and retain an explainable reasoning path. To resolve the aforementioned issues, we propose a new retriever-ranker paradigm of KB-VQA, Graph pATH rankER (GATHER for brevity). Specifically, it contains graph constructing, pruning, and path-level ranking, which not only retrieves accurate answers but also provides inference paths that explain the reasoning process. To comprehensively evaluate our model, we reformulate the benchmark dataset OK-VQA with manually corrected entity-level annotations and release it as ConceptVQA. Extensive experiments on real-world questions demonstrate that our framework is not only able to perform open-set question answering across the whole knowledge base but provide explicit reasoning path.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Multi-Scale Spatial-Temporal Recurrent Networks for Traffic Flow  Prediction</b></summary>
  <p><b>编号</b>：[199]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08138</p>
  <p><b>作者</b>：Haiyang Liu,  Chunjiang Zhu,  Detian Zhang,  Qing Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：intelligent transportation systems, Traffic flow prediction, Traffic flow, flow prediction, transportation systems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Traffic flow prediction is one of the most fundamental tasks of intelligent transportation systems. The complex and dynamic spatial-temporal dependencies make the traffic flow prediction quite challenging. Although existing spatial-temporal graph neural networks hold prominent, they often encounter challenges such as (1) ignoring the fixed graph that limits the predictive performance of the model, (2) insufficiently capturing complex spatial-temporal dependencies simultaneously, and (3) lacking attention to spatial-temporal information at different time lengths. In this paper, we propose a Multi-Scale Spatial-Temporal Recurrent Network for traffic flow prediction, namely MSSTRN, which consists of two different recurrent neural networks: the single-step gate recurrent unit and the multi-step gate recurrent unit to fully capture the complex spatial-temporal information in the traffic data under different time windows. Moreover, we propose a spatial-temporal synchronous attention mechanism that integrates adaptive position graph convolutions into the self-attention mechanism to achieve synchronous capture of spatial-temporal dependencies. We conducted extensive experiments on four real traffic datasets and demonstrated that our model achieves the best prediction accuracy with non-trivial margins compared to all the twenty baseline methods.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Counterfactual Explanations for Time Series Forecasting</b></summary>
  <p><b>编号</b>：[200]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08137</p>
  <p><b>作者</b>：Zhendong Wang,  Ioanna Miliou,  Isak Samsten,  Panagiotis Papapetrou</p>
  <p><b>备注</b>：10 pages, 6 figures. Accepted by ICDM 2023</p>
  <p><b>关键词</b>：utilize hidden feature, hidden feature patterns, improve forecasting performance, time series, time series forecasting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Among recent developments in time series forecasting methods, deep forecasting models have gained popularity as they can utilize hidden feature patterns in time series to improve forecasting performance. Nevertheless, the majority of current deep forecasting models are opaque, hence making it challenging to interpret the results. While counterfactual explanations have been extensively employed as a post-hoc approach for explaining classification models, their application to forecasting models still remains underexplored. In this paper, we formulate the novel problem of counterfactual generation for time series forecasting, and propose an algorithm, called ForecastCF, that solves the problem by applying gradient-based perturbations to the original time series. ForecastCF guides the perturbations by applying constraints to the forecasted values to obtain desired prediction outcomes. We experimentally evaluate ForecastCF using four state-of-the-art deep model architectures and compare to two baselines. Our results show that ForecastCF outperforms the baseline in terms of counterfactual validity and data manifold closeness. Overall, our findings suggest that ForecastCF can generate meaningful and relevant counterfactual explanations for various forecasting tasks.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Core-sets for Fair and Diverse Data Summarization</b></summary>
  <p><b>编号</b>：[207]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08122</p>
  <p><b>作者</b>：Sepideh Mahabadi,  Stojan Trajanovski</p>
  <p><b>备注</b>：NeurIPS 2023</p>
  <p><b>关键词</b>：core-set construction algorithms, study core-set construction, partition constraint, Diversity Maximization, core-set construction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study core-set construction algorithms for the task of Diversity Maximization under fairness/partition constraint. Given a set of points $P$ in a metric space partitioned into $m$ groups, and given $k_1,\ldots,k_m$, the goal of this problem is to pick $k_i$ points from each group $i$ such that the overall diversity of the $k=\sum_i k_i$ picked points is maximized. We consider two natural diversity measures: sum-of-pairwise distances and sum-of-nearest-neighbor distances, and show improved core-set construction algorithms with respect to these measures. More precisely, we show the first constant factor core-set w.r.t. sum-of-pairwise distances whose size is independent of the size of the dataset and the aspect ratio. Second, we show the first core-set w.r.t. the sum-of-nearest-neighbor distances. Finally, we run several experiments showing the effectiveness of our core-set approach. In particular, we apply constrained diversity maximization to summarize a set of timed messages that takes into account the messages' recency. Specifically, the summary should include more recent messages compared to older ones. This is a real task in one of the largest communication platforms, affecting the experience of hundreds of millions daily active users. By utilizing our core-set method for this task, we achieve a 100x speed-up while losing the diversity by only a few percent. Moreover, our approach allows us to improve the space usage of the algorithm in the streaming setting.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：Generative Intrinsic Optimization: Intrisic Control with Model Learning</b></summary>
  <p><b>编号</b>：[215]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08100</p>
  <p><b>作者</b>：Jianfei Ma</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Future sequence represents, Future sequence, sequence represents, executing the action, maximally informative consequences</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Future sequence represents the outcome after executing the action into the environment. When driven by the information-theoretic concept of mutual information, it seeks maximally informative consequences. Explicit outcomes may vary across state, return, or trajectory serving different purposes such as credit assignment or imitation learning. However, the inherent nature of incorporating intrinsic motivation with reward maximization is often neglected. In this work, we propose a variational approach to jointly learn the necessary quantity for estimating the mutual information and the dynamics model, providing a general framework for incorporating different forms of outcomes of interest. Integrated into a policy iteration scheme, our approach guarantees convergence to the optimal policy. While we mainly focus on theoretical analysis, our approach opens the possibilities of leveraging intrinsic control with model learning to enhance sample efficiency and incorporate uncertainty of the environment into decision-making.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：ClimateBERT-NetZero: Detecting and Assessing Net Zero and Reduction  Targets</b></summary>
  <p><b>编号</b>：[218]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08096</p>
  <p><b>作者</b>：Tobias Schimanski,  Julia Bingler,  Camilla Hyslop,  Mathias Kraus,  Markus Leippold</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：private actors struggle, sustainability commitments made, Public and private, private actors, actors struggle</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Public and private actors struggle to assess the vast amounts of information about sustainability commitments made by various institutions. To address this problem, we create a novel tool for automatically detecting corporate, national, and regional net zero and reduction targets in three steps. First, we introduce an expert-annotated data set with 3.5K text samples. Second, we train and release ClimateBERT-NetZero, a natural language classifier to detect whether a text contains a net zero or reduction target. Third, we showcase its analysis potential with two use cases: We first demonstrate how ClimateBERT-NetZero can be combined with conventional question-answering (Q&A) models to analyze the ambitions displayed in net zero and reduction targets. Furthermore, we employ the ClimateBERT-NetZero model on quarterly earning call transcripts and outline how communication patterns evolve over time. Our experiments demonstrate promising pathways for extracting and analyzing net zero and emission reduction targets at scale.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Discerning Temporal Difference Learning</b></summary>
  <p><b>编号</b>：[222]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08091</p>
  <p><b>作者</b>：Jianfei Ma</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Temporal difference learning, Temporal difference, aimed at efficiently, foundational concept, concept in reinforcement</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Temporal difference learning (TD) is a foundational concept in reinforcement learning (RL), aimed at efficiently assessing a policy's value function. TD($\lambda$), a potent variant, incorporates a memory trace to distribute the prediction error into the historical context. However, this approach often neglects the significance of historical states and the relative importance of propagating the TD error, influenced by challenges such as visitation imbalance or outcome noise. To address this, we propose a novel TD algorithm named discerning TD learning (DTD), which allows flexible emphasis functions$-$predetermined or adapted during training$-$to allocate efforts effectively across states. We establish the convergence properties of our method within a specific class of emphasis functions and showcase its promising potential for adaptation to deep RL contexts. Empirical results underscore that employing a judicious emphasis function not only improves value estimation but also expedites learning across diverse scenarios.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Dealing with zero-inflated data: achieving SOTA with a two-fold machine  learning approach</b></summary>
  <p><b>编号</b>：[224]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08088</p>
  <p><b>作者</b>：Jože M. Rožanec,  Gašper Petelin,  João Costa,  Blaž Bertalanič,  Gregor Cerar,  Marko Guček,  Gregor Papa,  Dunja Mladenić</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：airport shuttle demand, learn to correctly, correctly predict, broader range, shuttle demand prediction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In many cases, a machine learning model must learn to correctly predict a few data points with particular values of interest in a broader range of data where many target values are zero. Zero-inflated data can be found in diverse scenarios, such as lumpy and intermittent demands, power consumption for home appliances being turned on and off, impurities measurement in distillation processes, and even airport shuttle demand prediction. The presence of zeroes affects the models' learning and may result in poor performance. Furthermore, zeroes also distort the metrics used to compute the model's prediction quality. This paper showcases two real-world use cases (home appliances classification and airport shuttle demand prediction) where a hierarchical model applied in the context of zero-inflated data leads to excellent results. In particular, for home appliances classification, the weighted average of Precision, Recall, F1, and AUC ROC was increased by 27%, 34%, 49%, and 27%, respectively. Furthermore, it is estimated that the proposed approach is also four times more energy efficient than the SOTA approach against which it was compared to. Two-fold models performed best in all cases when predicting airport shuttle demand, and the difference against other models has been proven to be statistically significant.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：To token or not to token: A Comparative Study of Text Representations  for Cross-Lingual Transfer</b></summary>
  <p><b>编号</b>：[229]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08078</p>
  <p><b>作者</b>：Md Mushfiqur Rahman,  Fardin Ahsan Sakib,  Fahim Faisal,  Antonios Anastasopoulos</p>
  <p><b>备注</b>：Accepted at 3RD MULTILINGUAL REPRESENTATION LEARNING (MRL) WORKSHOP, 2023</p>
  <p><b>关键词</b>：texttt, bottleneck in low-resource, low-resource cross-lingual transfer, text representation, low-resource cross-lingual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Choosing an appropriate tokenization scheme is often a bottleneck in low-resource cross-lingual transfer. To understand the downstream implications of text representation choices, we perform a comparative analysis on language models having diverse text representation modalities including 2 segmentation-based models (\texttt{BERT}, \texttt{mBERT}), 1 image-based model (\texttt{PIXEL}), and 1 character-level model (\texttt{CANINE}). First, we propose a scoring Language Quotient (LQ) metric capable of providing a weighted representation of both zero-shot and few-shot evaluation combined. Utilizing this metric, we perform experiments comprising 19 source languages and 133 target languages on three tasks (POS tagging, Dependency parsing, and NER). Our analysis reveals that image-based models excel in cross-lingual transfer when languages are closely related and share visually similar scripts. However, for tasks biased toward word meaning (POS, NER), segmentation-based models prove to be superior. Furthermore, in dependency parsing tasks where word relationships play a crucial role, models with their character-level focus, outperform others. Finally, we propose a recommendation scheme based on our findings to guide model selection according to task and language requirements.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：Samples on Thin Ice: Re-Evaluating Adversarial Pruning of Neural  Networks</b></summary>
  <p><b>编号</b>：[231]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08073</p>
  <p><b>作者</b>：Giorgio Piras,  Maura Pintor,  Ambra Demontis,  Battista Biggio</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：trading desirable properties, Neural network pruning, trading desirable, higher sparsity, adversarial pruning methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural network pruning has shown to be an effective technique for reducing the network size, trading desirable properties like generalization and robustness to adversarial attacks for higher sparsity. Recent work has claimed that adversarial pruning methods can produce sparse networks while also preserving robustness to adversarial examples. In this work, we first re-evaluate three state-of-the-art adversarial pruning methods, showing that their robustness was indeed overestimated. We then compare pruned and dense versions of the same models, discovering that samples on thin ice, i.e., closer to the unpruned model's decision boundary, are typically misclassified after pruning. We conclude by discussing how this intuition may lead to designing more effective adversarial pruning methods in future work.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：Learning Transferable Conceptual Prototypes for Interpretable  Unsupervised Domain Adaptation</b></summary>
  <p><b>编号</b>：[233]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08071</p>
  <p><b>作者</b>：Junyu Gao,  Xinhong Ma,  Changsheng Xu</p>
  <p><b>备注</b>：Submitted to IEEE TIP</p>
  <p><b>关键词</b>：deep neural networks, controllable model decisions, current UDA models, unsupervised domain adaptation, neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite the great progress of unsupervised domain adaptation (UDA) with the deep neural networks, current UDA models are opaque and cannot provide promising explanations, limiting their applications in the scenarios that require safe and controllable model decisions. At present, a surge of work focuses on designing deep interpretable methods with adequate data annotations and only a few methods consider the distributional shift problem. Most existing interpretable UDA methods are post-hoc ones, which cannot facilitate the model learning process for performance enhancement. In this paper, we propose an inherently interpretable method, named Transferable Conceptual Prototype Learning (TCPL), which could simultaneously interpret and improve the processes of knowledge transfer and decision-making in UDA. To achieve this goal, we design a hierarchically prototypical module that transfers categorical basic concepts from the source domain to the target domain and learns domain-shared prototypes for explaining the underlying reasoning process. With the learned transferable prototypes, a self-predictive consistent pseudo-label strategy that fuses confidence, predictions, and prototype information, is designed for selecting suitable target samples for pseudo annotations and gradually narrowing down the domain gap. Comprehensive experiments show that the proposed method can not only provide effective and intuitive explanations but also outperform previous state-of-the-arts.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Tight Time-Space Lower Bounds for Constant-Pass Learning</b></summary>
  <p><b>编号</b>：[234]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08070</p>
  <p><b>作者</b>：Xin Lyu,  Avishay Tal,  Hongxun Wu,  Junzhao Yang</p>
  <p><b>备注</b>：To appear at FOCS 2023</p>
  <p><b>关键词</b>：learning, breakthrough paper, exponential number, Raz, samples</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In his breakthrough paper, Raz showed that any parity learning algorithm requires either quadratic memory or an exponential number of samples [FOCS'16, JACM'19]. A line of work that followed extended this result to a large class of learning problems. Until recently, all these results considered learning in the streaming model, where each sample is drawn independently, and the learner is allowed a single pass over the stream of samples. Garg, Raz, and Tal [CCC'19] considered a stronger model, allowing multiple passes over the stream. In the $2$-pass model, they showed that learning parities of size $n$ requires either a memory of size $n^{1.5}$ or at least $2^{\sqrt{n}}$ samples. (Their result also generalizes to other learning problems.)
In this work, for any constant $q$, we prove tight memory-sample lower bounds for any parity learning algorithm that makes $q$ passes over the stream of samples. We show that such a learner requires either $\Omega(n^{2})$ memory size or at least $2^{\Omega(n)}$ samples. Beyond establishing a tight lower bound, this is the first non-trivial lower bound for $q$-pass learning for any $q\ge 3$. Similar to prior work, our results extend to any learning problem with many nearly-orthogonal concepts.
We complement the lower bound with an upper bound, showing that parity learning with $q$ passes can be done efficiently with $O(n^2/\log q)$ memory.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：Rethinking Negative Pairs in Code Search</b></summary>
  <p><b>编号</b>：[235]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08069</p>
  <p><b>作者</b>：Haochen Li,  Xin Zhou,  Luu Anh Tuan,  Chunyan Miao</p>
  <p><b>备注</b>：Accepted to EMNLP 2023</p>
  <p><b>关键词</b>：software development efficiency, fine-tuning code search, negative samples, key component, component in fine-tuning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, contrastive learning has become a key component in fine-tuning code search models for software development efficiency and effectiveness. It pulls together positive code snippets while pushing negative samples away given search queries. Among contrastive learning, InfoNCE is the most widely used loss function due to its better performance. However, the following problems in negative samples of InfoNCE may deteriorate its representation learning: 1) The existence of false negative samples in large code corpora due to duplications. 2). The failure to explicitly differentiate between the potential relevance of negative samples. As an example, a bubble sorting algorithm example is less ``negative'' than a file saving function for the quick sorting algorithm query. In this paper, we tackle the above problems by proposing a simple yet effective Soft-InfoNCE loss that inserts weight terms into InfoNCE. In our proposed loss function, we apply three methods to estimate the weights of negative pairs and show that the vanilla InfoNCE loss is a special case of Soft-InfoNCE. Theoretically, we analyze the effects of Soft-InfoNCE on controlling the distribution of learnt code representations and on deducing a more precise mutual information estimation. We furthermore discuss the superiority of proposed loss functions with other design alternatives. Extensive experiments demonstrate the effectiveness of Soft-InfoNCE and weights estimation methods under state-of-the-art code search models on a large-scale public dataset consisting of six programming languages. Source code is available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：Learning from Label Proportions: Bootstrapping Supervised Learners via  Belief Propagation</b></summary>
  <p><b>编号</b>：[239]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08056</p>
  <p><b>作者</b>：Shreyas Havaldar,  Navodita Sharma,  Shubhi Sareen,  Karthikeyan Shanmugam,  Aravindan Raghuveer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Label Proportions, aggregate level labels, test data, pseudo labels, Belief Propagation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Learning from Label Proportions (LLP) is a learning problem where only aggregate level labels are available for groups of instances, called bags, during training, and the aim is to get the best performance at the instance-level on the test data. This setting arises in domains like advertising and medicine due to privacy considerations. We propose a novel algorithmic framework for this problem that iteratively performs two main steps. For the first step (Pseudo Labeling) in every iteration, we define a Gibbs distribution over binary instance labels that incorporates a) covariate information through the constraint that instances with similar covariates should have similar labels and b) the bag level aggregated label. We then use Belief Propagation (BP) to marginalize the Gibbs distribution to obtain pseudo labels. In the second step (Embedding Refinement), we use the pseudo labels to provide supervision for a learner that yields a better embedding. Further, we iterate on the two steps again by using the second step's embeddings as new covariates for the next iteration. In the final iteration, a classifier is trained using the pseudo labels. Our algorithm displays strong gains against several SOTA baselines (up to 15%) for the LLP Binary Classification problem on various dataset types - tabular and Image. We achieve these improvements with minimal computational overhead above standard supervised learning due to Belief Propagation, for large bag sizes, even for a million samples.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：LGL-BCI: A Lightweight Geometric Learning Framework for Motor  Imagery-Based Brain-Computer Interfaces</b></summary>
  <p><b>编号</b>：[240]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08051</p>
  <p><b>作者</b>：Jianchao Lu,  Yuzhe Tian,  Yang Zhang,  Jiaqi Ge,  Quan Z. Sheng,  Xi Zheng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Brain-Computer Interfaces, based Motor Imagery, Symmetric Positive Definite, brain signals, Geometric Deep Learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Brain-Computer Interfaces (BCIs) are a groundbreaking technology for interacting with external devices using brain signals. Despite advancements, electroencephalogram (EEG)-based Motor Imagery (MI) tasks face challenges like amplitude and phase variability, and complex spatial correlations, with a need for smaller model size and faster inference. This study introduces the LGL-BCI framework, employing a Geometric Deep Learning Framework for EEG processing in non-Euclidean metric spaces, particularly the Symmetric Positive Definite (SPD) Manifold space. LGL-BCI offers robust EEG data representation and captures spatial correlations. We propose an EEG channel selection solution via a feature decomposition algorithm to reduce SPD matrix dimensionality, with a lossless transformation boosting inference speed. Extensive experiments show LGL-BCI's superior accuracy and efficiency compared to current solutions, highlighting geometric deep learning's potential in MI-BCI applications. The efficiency, assessed on two public EEG datasets and two real-world EEG devices, significantly outperforms the state-of-the-art solution in accuracy ($82.54\%$ versus $62.22\%$) with fewer parameters (64.9M compared to 183.7M).</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：Exploring the Relationship Between Model Architecture and In-Context  Learning Ability</b></summary>
  <p><b>编号</b>：[241]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08049</p>
  <p><b>作者</b>：Ivan Lee,  Nan Jiang,  Taylor Berg-Kirkpatrick</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：in-context learning, in-context, learning, architectures, perform in-context learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>What is the relationship between model architecture and the ability to perform in-context learning? In this empirical study, we take the first steps towards answering this question. In particular, we evaluate fifteen model architectures across a suite of synthetic in-context learning tasks. The selected architectures represent a broad range of paradigms, including recurrent and convolution-based neural networks, transformers, and emerging attention alternatives. We discover that all considered architectures can perform in-context learning under certain conditions. However, contemporary architectures are found to be the best performing, especially as task complexity grows. Additionally, our follow-up experiments delve into various factors that influence in-context learning. We observe varied sensitivities among architectures with respect to hyperparameter settings. Our study of training dynamics reveals that certain architectures exhibit a smooth, progressive learning trajectory, while others demonstrate periods of stagnation followed by abrupt mastery of the task. Finally, and somewhat surprisingly, we find that several emerging attention alternatives are more robust in-context learners than transformers; since such approaches have constant-sized memory footprints at inference time, this result opens the future possibility of scaling up in-context learning to vastly larger numbers of in-context examples.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large  Language Models</b></summary>
  <p><b>编号</b>：[247]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08041</p>
  <p><b>作者</b>：Jing Liu,  Ruihao Gong,  Xiuying Wei,  Zhiwei Dong,  Jianfei Cai,  Bohan Zhuang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, excel in NLP, Large Language, Language Models, widespread deployment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) excel in NLP, but their demands hinder their widespread deployment. While Quantization-Aware Training (QAT) offers a solution, its extensive training costs make Post-Training Quantization (PTQ) a more practical approach for LLMs. In existing studies, activation outliers in particular channels are identified as the bottleneck to PTQ accuracy. They propose to transform the magnitudes from activations to weights, which however offers limited alleviation or suffers from unstable gradients, resulting in a severe performance drop at low-bitwidth. In this paper, we propose QLLM, an accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM introduces an adaptive channel reassembly technique that reallocates the magnitude of outliers to other channels, thereby mitigating their impact on the quantization range. This is achieved by channel disassembly and channel assembly, which first breaks down the outlier channels into several sub-channels to ensure a more balanced distribution of activation magnitudes. Then similar channels are merged to maintain the original channel number for efficiency. Additionally, an adaptive strategy is designed to autonomously determine the optimal number of sub-channels for channel disassembly. To further compensate for the performance loss caused by quantization, we propose an efficient tuning method that only learns a small number of low-rank weights while freezing the pre-trained quantized model. After training, these low-rank parameters can be fused into the frozen weights without affecting inference. Extensive experiments on LLaMA-1 and LLaMA-2 show that QLLM can obtain accurate quantized models efficiently. For example, QLLM quantizes the 4-bit LLaMA-2-70B within 10 hours on a single A100-80G GPU, outperforming the previous state-of-the-art method by 7.89% on the average accuracy across five zero-shot tasks.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：SEE-OoD: Supervised Exploration For Enhanced Out-of-Distribution  Detection</b></summary>
  <p><b>编号</b>：[248]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08040</p>
  <p><b>作者</b>：Xiaoyang Song,  Wenbo Sun,  Maher Nouiehed,  Raed Al Kontar,  Judy Jin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：quantifying predictive uncertainty, incorporating model regularization, detection predominantly rely, OoD samples, OoD</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Current techniques for Out-of-Distribution (OoD) detection predominantly rely on quantifying predictive uncertainty and incorporating model regularization during the training phase, using either real or synthetic OoD samples. However, methods that utilize real OoD samples lack exploration and are prone to overfit the OoD samples at hand. Whereas synthetic samples are often generated based on features extracted from training data, rendering them less effective when the training and OoD data are highly overlapped in the feature space. In this work, we propose a Wasserstein-score-based generative adversarial training scheme to enhance OoD detection accuracy, which, for the first time, performs data augmentation and exploration simultaneously under the supervision of limited OoD samples. Specifically, the generator explores OoD spaces and generates synthetic OoD samples using feedback from the discriminator, while the discriminator exploits both the observed and synthesized samples for OoD detection using a predefined Wasserstein score. We provide theoretical guarantees that the optimal solutions of our generative scheme are statistically achievable through adversarial training in empirical settings. We then demonstrate that the proposed method outperforms state-of-the-art techniques on various computer vision datasets and exhibits superior generalizability to unseen OoD data.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：Rethinking Large-scale Pre-ranking System: Entire-chain Cross-domain  Models</b></summary>
  <p><b>编号</b>：[249]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08039</p>
  <p><b>作者</b>：Jinbo Song (1),  Ruoran Huang (1),  Xinyang Wang (1),  Wei Huang (1),  Qian Yu (1),  Mingming Chen (1),  Yafei Yao (1),  Chaosheng Fan (1),  Changping Peng (1),  Zhangang Lin (1),  Jinghe Hu (1),  Jingping Shao (1) ((1) Marketing and Commercialization Center, JD.com)</p>
  <p><b>备注</b>：5 pages, 2 figures</p>
  <p><b>关键词</b>：online advertising, multi-stage architectures, widely equipped, equipped with multi-stage, including matching</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Industrial systems such as recommender systems and online advertising, have been widely equipped with multi-stage architectures, which are divided into several cascaded modules, including matching, pre-ranking, ranking and re-ranking. As a critical bridge between matching and ranking, existing pre-ranking approaches mainly endure sample selection bias (SSB) problem owing to ignoring the entire-chain data dependence, resulting in sub-optimal performances. In this paper, we rethink pre-ranking system from the perspective of the entire sample space, and propose Entire-chain Cross-domain Models (ECM), which leverage samples from the whole cascaded stages to effectively alleviate SSB problem. Besides, we design a fine-grained neural structure named ECMM to further improve the pre-ranking accuracy. Specifically, we propose a cross-domain multi-tower neural network to comprehensively predict for each stage result, and introduce the sub-networking routing strategy with $L0$ regularization to reduce computational costs. Evaluations on real-world large-scale traffic logs demonstrate that our pre-ranking models outperform SOTA methods while time consumption is maintained within an acceptable level, which achieves better trade-off between efficiency and effectiveness.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：Continual Learning via Manifold Expansion Replay</b></summary>
  <p><b>编号</b>：[250]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08038</p>
  <p><b>作者</b>：Zihao Xu,  Xuan Tang,  Yufei Shi,  Jianfeng Zhang,  Jian Yang,  Mingsong Chen,  Xian Wei</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：learner learns multiple, learns multiple tasks, continual learning, learner learns, learns multiple</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In continual learning, the learner learns multiple tasks in sequence, with data being acquired only once for each task. Catastrophic forgetting is a major challenge to continual learning. To reduce forgetting, some existing rehearsal-based methods use episodic memory to replay samples of previous tasks. However, in the process of knowledge integration when learning a new task, this strategy also suffers from catastrophic forgetting due to an imbalance between old and new knowledge. To address this problem, we propose a novel replay strategy called Manifold Expansion Replay (MaER). We argue that expanding the implicit manifold of the knowledge representation in the episodic memory helps to improve the robustness and expressiveness of the model. To this end, we propose a greedy strategy to keep increasing the diameter of the implicit manifold represented by the knowledge in the buffer during memory management. In addition, we introduce Wasserstein distance instead of cross entropy as distillation loss to preserve previous knowledge. With extensive experimental validation on MNIST, CIFAR10, CIFAR100, and TinyImageNet, we show that the proposed method significantly improves the accuracy in continual learning setup, outperforming the state of the arts.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：ZEST: Attention-based Zero-Shot Learning for Unseen IoT Device  Classification</b></summary>
  <p><b>编号</b>：[252]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08036</p>
  <p><b>作者</b>：Binghui Wu,  Philipp Gysel,  Dinil Mon Divakaran,  Mohan Gurusamy</p>
  <p><b>备注</b>：9 pages, 6 figures, 3 tables</p>
  <p><b>关键词</b>：Recent research works, proposed machine learning, Recent research, machine learning models, research works</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent research works have proposed machine learning models for classifying IoT devices connected to a network. However, there is still a practical challenge of not having all devices (and hence their traffic) available during the training of a model. This essentially means, during the operational phase, we need to classify new devices not seen during the training phase. To address this challenge, we propose ZEST -- a ZSL (zero-shot learning) framework based on self-attention for classifying both seen and unseen devices. ZEST consists of i) a self-attention based network feature extractor, termed SANE, for extracting latent space representations of IoT traffic, ii) a generative model that trains a decoder using latent features to generate pseudo data, and iii) a supervised model that is trained on the generated pseudo data for classifying devices. We carry out extensive experiments on real IoT traffic data; our experiments demonstrate i) ZEST achieves significant improvement (in terms of accuracy) over the baselines; ii) ZEST is able to better extract meaningful representations than LSTM which has been commonly used for modeling network traffic.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：Local Graph Clustering with Noisy Labels</b></summary>
  <p><b>编号</b>：[256]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08031</p>
  <p><b>作者</b>：Artur Back de Luca,  Kimon Fountoulakis,  Shenghao Yang</p>
  <p><b>备注</b>：26 pages, 5 figures, 14 tables</p>
  <p><b>关键词</b>：machine learning problems, additional node information, graph, labels, node</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The growing interest in machine learning problems over graphs with additional node information such as texts, images, or labels has popularized methods that require the costly operation of processing the entire graph. Yet, little effort has been made to the development of fast local methods (i.e. without accessing the entire graph) that extract useful information from such data. To that end, we propose a study of local graph clustering using noisy node labels as a proxy for additional node information. In this setting, nodes receive initial binary labels based on cluster affiliation: 1 if they belong to the target cluster and 0 otherwise. Subsequently, a fraction of these labels is flipped. We investigate the benefits of incorporating noisy labels for local graph clustering. By constructing a weighted graph with such labels, we study the performance of graph diffusion-based local clustering method on both the original and the weighted graphs. From a theoretical perspective, we consider recovering an unknown target cluster with a single seed node in a random graph with independent noisy node labels. We provide sufficient conditions on the label noise under which, with high probability, using diffusion in the weighted graph yields a more accurate recovery of the target cluster. This approach proves more effective than using the given labels alone or using diffusion in the label-free original graph. Empirically, we show that reliable node labels can be obtained with just a few samples from an attributed graph. Moreover, utilizing these labels via diffusion in the weighted graph leads to significantly better local clustering performance across several real-world datasets, improving F1 scores by up to 13%.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：Robust 1-bit Compressed Sensing with Iterative Hard Thresholding</b></summary>
  <p><b>编号</b>：[260]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08019</p>
  <p><b>作者</b>：Namiko Matsumoto,  Arya Mazumdar</p>
  <p><b>备注</b>：Accepted to appear in ACM-SIAM Symposium on Discrete Algorithms (SODA) 2024</p>
  <p><b>关键词</b>：compressed sensing, Iterative Hard Thresholding, Binary Iterative Hard, sparse unit vector, measurements</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In 1-bit compressed sensing, the aim is to estimate a $k$-sparse unit vector $x\in S^{n-1}$ within an $\epsilon$ error (in $\ell_2$) from minimal number of linear measurements that are quantized to just their signs, i.e., from measurements of the form $y = \mathrm{Sign}(\langle a, x\rangle).$ In this paper, we study a noisy version where a fraction of the measurements can be flipped, potentially by an adversary. In particular, we analyze the Binary Iterative Hard Thresholding (BIHT) algorithm, a proximal gradient descent on a properly defined loss function used for 1-bit compressed sensing, in this noisy setting. It is known from recent results that, with $\tilde{O}(\frac{k}{\epsilon})$ noiseless measurements, BIHT provides an estimate within $\epsilon$ error. This result is optimal and universal, meaning one set of measurements work for all sparse vectors. In this paper, we show that BIHT also provides better results than all known methods for the noisy setting. We show that when up to $\tau$-fraction of the sign measurements are incorrect (adversarial error), with the same number of measurements as before, BIHT agnostically provides an estimate of $x$ within an $\tilde{O}(\epsilon+\tau)$ error, maintaining the universality of measurements. This establishes stability of iterative hard thresholding in the presence of measurement error. To obtain the result, we use the restricted approximate invertibility of Gaussian matrices, as well as a tight analysis of the high-dimensional geometry of the adversarially corrupted measurements.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：Why Train More? Effective and Efficient Membership Inference via  Memorization</b></summary>
  <p><b>编号</b>：[262]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08015</p>
  <p><b>作者</b>：Jihye Choi,  Shruti Tople,  Varun Chandrasekaran,  Somesh Jha</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Membership Inference Attacks, identify specific data, private training dataset, Membership Inference, machine learning models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Membership Inference Attacks (MIAs) aim to identify specific data samples within the private training dataset of machine learning models, leading to serious privacy violations and other sophisticated threats. Many practical black-box MIAs require query access to the data distribution (the same distribution where the private data is drawn) to train shadow models. By doing so, the adversary obtains models trained "with" or "without" samples drawn from the distribution, and analyzes the characteristics of the samples under consideration. The adversary is often required to train more than hundreds of shadow models to extract the signals needed for MIAs; this becomes the computational overhead of MIAs. In this paper, we propose that by strategically choosing the samples, MI adversaries can maximize their attack success while minimizing the number of shadow models. First, our motivational experiments suggest memorization as the key property explaining disparate sample vulnerability to MIAs. We formalize this through a theoretical bound that connects MI advantage with memorization. Second, we show sample complexity bounds that connect the number of shadow models needed for MIAs with memorization. Lastly, we confirm our theoretical arguments with comprehensive experiments; by utilizing samples with high memorization scores, the adversary can (a) significantly improve its efficacy regardless of the MIA used, and (b) reduce the number of shadow models by nearly two orders of magnitude compared to state-of-the-art approaches.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：AutoFHE: Automated Adaption of CNNs for Efficient Evaluation over FHE</b></summary>
  <p><b>编号</b>：[263]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08012</p>
  <p><b>作者</b>：Wei Ao,  Vishnu Naresh Boddeti</p>
  <p><b>备注</b>：USENIX Security Symposium 2024</p>
  <p><b>关键词</b>：deep convolutional neural, convolutional neural networks, unsupported non-linear activation, involves polynomial approximation, non-linear activation functions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Secure inference of deep convolutional neural networks (CNNs) under RNS-CKKS involves polynomial approximation of unsupported non-linear activation functions. However, existing approaches have three main limitations: 1) Inflexibility: The polynomial approximation and associated homomorphic evaluation architecture are customized manually for each CNN architecture and do not generalize to other networks. 2) Suboptimal Approximation: Each activation function is approximated instead of the function represented by the CNN. 3) Restricted Design: Either high-degree or low-degree polynomial approximations are used. The former retains high accuracy but slows down inference due to bootstrapping operations, while the latter accelerates ciphertext inference but compromises accuracy. To address these limitations, we present AutoFHE, which automatically adapts standard CNNs for secure inference under RNS-CKKS. The key idea is to adopt layerwise mixed-degree polynomial activation functions, which are optimized jointly with the homomorphic evaluation architecture in terms of the placement of bootstrapping operations. The problem is modeled within a multi-objective optimization framework to maximize accuracy and minimize the number of bootstrapping operations. AutoFHE can be applied flexibly on any CNN architecture, and it provides diverse solutions that span the trade-off between accuracy and latency. Experimental evaluation over RNS-CKKS encrypted CIFAR datasets shows that AutoFHE accelerates secure inference by $1.32\times$ to $1.8\times$ compared to methods employing high-degree polynomials. It also improves accuracy by up to 2.56% compared to methods using low-degree polynomials. Lastly, AutoFHE accelerates inference and improves accuracy by $103\times$ and 3.46%, respectively, compared to CNNs under TFHE.</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：LEMON: Lossless model expansion</b></summary>
  <p><b>编号</b>：[269]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07999</p>
  <p><b>作者</b>：Yite Wang,  Jiahao Su,  Hanlin Lu,  Cong Xie,  Tianyi Liu,  Jianbo Yuan,  Haibin Lin,  Ruoyu Sun,  Hongxia Yang</p>
  <p><b>备注</b>：Preprint</p>
  <p><b>关键词</b>：sophisticated reasoning capabilities, deep neural networks, deep neural, surging performance, emergence of sophisticated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Scaling of deep neural networks, especially Transformers, is pivotal for their surging performance and has further led to the emergence of sophisticated reasoning capabilities in foundation models. Such scaling generally requires training large models from scratch with random initialization, failing to leverage the knowledge acquired by their smaller counterparts, which are already resource-intensive to obtain. To tackle this inefficiency, we present $\textbf{L}$ossl$\textbf{E}$ss $\textbf{MO}$del Expansio$\textbf{N}$ (LEMON), a recipe to initialize scaled models using the weights of their smaller but pre-trained counterparts. This is followed by model training with an optimized learning rate scheduler tailored explicitly for the scaled models, substantially reducing the training time compared to training from scratch. Notably, LEMON is versatile, ensuring compatibility with various network structures, including models like Vision Transformers and BERT. Our empirical results demonstrate that LEMON reduces computational costs by 56.7% for Vision Transformers and 33.2% for BERT when compared to training from scratch.</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：Reset It and Forget It: Relearning Last-Layer Weights Improves Continual  and Transfer Learning</b></summary>
  <p><b>编号</b>：[272]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07996</p>
  <p><b>作者</b>：Lapo Frati,  Neil Traft,  Jeff Clune,  Nick Cheney</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：simple pre-training mechanism, continual learning, pre-training mechanism, continual learning settings, simple pre-training</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This work identifies a simple pre-training mechanism that leads to representations exhibiting better continual and transfer learning. This mechanism -- the repeated resetting of weights in the last layer, which we nickname "zapping" -- was originally designed for a meta-continual-learning procedure, yet we show it is surprisingly applicable in many settings beyond both meta-learning and continual learning. In our experiments, we wish to transfer a pre-trained image classifier to a new set of classes, in a few shots. We show that our zapping procedure results in improved transfer accuracy and/or more rapid adaptation in both standard fine-tuning and continual learning settings, while being simple to implement and computationally efficient. In many cases, we achieve performance on par with state of the art meta-learning without needing the expensive higher-order gradients, by using a combination of zapping and sequential learning. An intuitive explanation for the effectiveness of this zapping procedure is that representations trained with repeated zapping learn features that are capable of rapidly adapting to newly initialized classifiers. Such an approach may be considered a computationally cheaper type of, or alternative to, meta-learning rapidly adaptable features with higher-order gradients. This adds to recent work on the usefulness of resetting neural network parameters during training, and invites further investigation of this mechanism.</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：Semantic-Forward Relaying: A Novel Framework Towards 6G Cooperative  Communications</b></summary>
  <p><b>编号</b>：[275]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07987</p>
  <p><b>作者</b>：Wensheng Lin,  Yuna Yan,  Lixin Li,  Zhu Han,  Tad Matsumoto</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：letter proposes, wireless networks, cooperative communications, relaying framework, reduces forwarding payload</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This letter proposes a novel relaying framework, semantic-forward (SF), for cooperative communications towards the sixth-generation (6G) wireless networks. The SF relay extracts and transmits the semantic features, which reduces forwarding payload, and also improves the network robustness against intra-link errors. Based on the theoretical basis for cooperative communications with side information and the turbo principle, we design a joint source-channel coding algorithm to iteratively exchange the extrinsic information for enhancing the decoding gains at the destination. Surprisingly, simulation results indicate that even in bad channel conditions, SF relaying can still effectively improve the recovered information quality.</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：Neural Combinatorial Optimization with Heavy Decoder: Toward Large Scale  Generalization</b></summary>
  <p><b>编号</b>：[276]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07985</p>
  <p><b>作者</b>：Fu Luo,  Xi Lin,  Fei Liu,  Qingfu Zhang,  Zhenkun Wang</p>
  <p><b>备注</b>：Accepted at NeurIPS 2023</p>
  <p><b>关键词</b>：Neural combinatorial optimization, challenging combinatorial optimization, solving challenging combinatorial, promising learning-based approach, specialized algorithm design</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural combinatorial optimization (NCO) is a promising learning-based approach for solving challenging combinatorial optimization problems without specialized algorithm design by experts. However, most constructive NCO methods cannot solve problems with large-scale instance sizes, which significantly diminishes their usefulness for real-world applications. In this work, we propose a novel Light Encoder and Heavy Decoder (LEHD) model with a strong generalization ability to address this critical issue. The LEHD model can learn to dynamically capture the relationships between all available nodes of varying sizes, which is beneficial for model generalization to problems of various scales. Moreover, we develop a data-efficient training scheme and a flexible solution construction mechanism for the proposed LEHD model. By training on small-scale problem instances, the LEHD model can generate nearly optimal solutions for the Travelling Salesman Problem (TSP) and the Capacitated Vehicle Routing Problem (CVRP) with up to 1000 nodes, and also generalizes well to solve real-world TSPLib and CVRPLib problems. These results confirm our proposed LEHD model can significantly improve the state-of-the-art performance for constructive NCO. The code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：RandCom: Random Communication Skipping Method for Decentralized  Stochastic Optimization</b></summary>
  <p><b>编号</b>：[278]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07983</p>
  <p><b>作者</b>：Luyao Guo,  Sulaiman A. Alghunaim,  Kun Yuan,  Laurent Condat,  Jinde Cao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：gaining increasing attention, increasing attention due, accelerating communication complexity, random communication skips, Distributed optimization methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Distributed optimization methods with random communication skips are gaining increasing attention due to their proven benefits in accelerating communication complexity. Nevertheless, existing research mainly focuses on centralized communication protocols for strongly convex deterministic settings. In this work, we provide a decentralized optimization method called RandCom, which incorporates probabilistic local updates. We analyze the performance of RandCom in stochastic non-convex, convex, and strongly convex settings and demonstrate its ability to asymptotically reduce communication overhead by the probability of communication. Additionally, we prove that RandCom achieves linear speedup as the number of nodes increases. In stochastic strongly convex settings, we further prove that RandCom can achieve linear speedup with network-independent stepsizes. Moreover, we apply RandCom to federated learning and provide positive results concerning the potential for achieving linear speedup and the suitability of the probabilistic local update approach for non-convex settings.</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：Reinforcement Learning of Display Transfer Robots in Glass Flow Control  Systems: A Physical Simulation-Based Approach</b></summary>
  <p><b>编号</b>：[279]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07981</p>
  <p><b>作者</b>：Hwajong Lee,  Chan Kim,  Seong-Woo Kim</p>
  <p><b>备注</b>：10 pages, 17 figures</p>
  <p><b>关键词</b>：flow control system, flow control, critical concept, concept for increasing, increasing the production</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A flow control system is a critical concept for increasing the production capacity of manufacturing systems. To solve the scheduling optimization problem related to the flow control with the aim of improving productivity, existing methods depend on a heuristic design by domain human experts. Therefore, the methods require correction, monitoring, and verification by using real equipment. As system designs increase in complexity, the monitoring time increases, which decreases the probability of arriving at the optimal design. As an alternative approach to the heuristic design of flow control systems, the use of deep reinforcement learning to solve the scheduling optimization problem has been considered. Although the existing research on reinforcement learning has yielded excellent performance in some areas, the applicability of the results to actual FAB such as display and semiconductor manufacturing processes is not evident so far. To this end, we propose a method to implement a physical simulation environment and devise a feasible flow control system design using a transfer robot in display manufacturing through reinforcement learning. We present a model and parameter setting to build a virtual environment for different display transfer robots, and training methods of reinforcement learning on the environment to obtain an optimal scheduling of glass flow control systems. Its feasibility was verified by using different types of robots used in the actual process.</p>
  </details>
</details>
<details>
  <summary>83. <b>标题：GRASP: Accelerating Shortest Path Attacks via Graph Attention</b></summary>
  <p><b>编号</b>：[280]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07980</p>
  <p><b>作者</b>：Zohair Shafi. Benjamin A. Miller,  Ayan Chatterjee,  Tina Eliassi-Rad,  Rajmonda S. Caceres</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：accelerating classical combinatorial, Recent advances, machine learning, advances in machine, shown promise</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advances in machine learning (ML) have shown promise in aiding and accelerating classical combinatorial optimization algorithms. ML-based speed ups that aim to learn in an end to end manner (i.e., directly output the solution) tend to trade off run time with solution quality. Therefore, solutions that are able to accelerate existing solvers while maintaining their performance guarantees, are of great interest. We consider an APX-hard problem, where an adversary aims to attack shortest paths in a graph by removing the minimum number of edges. We propose the GRASP algorithm: Graph Attention Accelerated Shortest Path Attack, an ML aided optimization algorithm that achieves run times up to 10x faster, while maintaining the quality of solution generated. GRASP uses a graph attention network to identify a smaller subgraph containing the combinatorial solution, thus effectively reducing the input problem size. Additionally, we demonstrate how careful representation of the input graph, including node features that correlate well with the optimization task, can highlight important structure in the optimization solution.</p>
  </details>
</details>
<details>
  <summary>84. <b>标题：Graph-SCP: Accelerating Set Cover Problems with Graph Neural Networks</b></summary>
  <p><b>编号</b>：[281]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07979</p>
  <p><b>作者</b>：Zohair Shafi,  Benjamin A. Miller,  Tina Eliassi-Rad,  Rajmonda S. Caceres</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：accelerate combinatorial optimization, Set Cover Problem, approaches are increasingly, accelerate combinatorial, Set Cover</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine learning (ML) approaches are increasingly being used to accelerate combinatorial optimization (CO) problems. We look specifically at the Set Cover Problem (SCP) and propose Graph-SCP, a graph neural network method that can augment existing optimization solvers by learning to identify a much smaller sub-problem that contains the solution space. We evaluate the performance of Graph-SCP on synthetic weighted and unweighted SCP instances with diverse problem characteristics and complexities, and on instances from the OR Library, a canonical benchmark for SCP. We show that Graph-SCP reduces the problem size by 30-70% and achieves run time speedups up to~25x when compared to commercial solvers (Gurobi). Given a desired optimality threshold, Graph-SCP will improve upon it or even achieve 100% optimality. This is in contrast to fast greedy solutions that significantly compromise solution quality to achieve guaranteed polynomial run time. Graph-SCP can generalize to larger problem sizes and can be used with other conventional or ML-augmented CO solvers to lead to potential additional run time improvement.</p>
  </details>
</details>
<details>
  <summary>85. <b>标题：Interpretable Diffusion via Information Decomposition</b></summary>
  <p><b>编号</b>：[285]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07972</p>
  <p><b>作者</b>：Xianghao Kong,  Ollie Liu,  Han Li,  Dani Yogatama,  Greg Ver Steeg</p>
  <p><b>备注</b>：32 pages, 18 figures</p>
  <p><b>关键词</b>：enable conditional generation, generation and density, density modeling, modeling of complex, diffusion models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Denoising diffusion models enable conditional generation and density modeling of complex relationships like images and text. However, the nature of the learned relationships is opaque making it difficult to understand precisely what relationships between words and parts of an image are captured, or to predict the effect of an intervention. We illuminate the fine-grained relationships learned by diffusion models by noticing a precise relationship between diffusion and information decomposition. Exact expressions for mutual information and conditional mutual information can be written in terms of the denoising model. Furthermore, pointwise estimates can be easily estimated as well, allowing us to ask questions about the relationships between specific images and captions. Decomposing information even further to understand which variables in a high-dimensional space carry information is a long-standing problem. For diffusion models, we show that a natural non-negative decomposition of mutual information emerges, allowing us to quantify informative relationships between words and pixels in an image. We exploit these new relations to measure the compositional understanding of diffusion models, to do unsupervised localization of objects in images, and to measure effects when selectively editing images through prompt interventions.</p>
  </details>
</details>
<details>
  <summary>86. <b>标题：Hyperparameter Adaptive Search for Surrogate Optimization: A  Self-Adjusting Approach</b></summary>
  <p><b>编号</b>：[286]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07970</p>
  <p><b>作者</b>：Nazanin Nezami,  Hadis Anahideh</p>
  <p><b>备注</b>：2023 Winter Simulation Conference (WSC)</p>
  <p><b>关键词</b>：expensive black-box functions, optimizing expensive black-box, shown promise, expensive black-box, Hyperparameter Adaptive Search</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Surrogate Optimization (SO) algorithms have shown promise for optimizing expensive black-box functions. However, their performance is heavily influenced by hyperparameters related to sampling and surrogate fitting, which poses a challenge to their widespread adoption. We investigate the impact of hyperparameters on various SO algorithms and propose a Hyperparameter Adaptive Search for SO (HASSO) approach. HASSO is not a hyperparameter tuning algorithm, but a generic self-adjusting SO algorithm that dynamically tunes its own hyperparameters while concurrently optimizing the primary objective function, without requiring additional evaluations. The aim is to improve the accessibility, effectiveness, and convergence speed of SO algorithms for practitioners. Our approach identifies and modifies the most influential hyperparameters specific to each problem and SO approach, reducing the need for manual tuning without significantly increasing the computational burden. Experimental results demonstrate the effectiveness of HASSO in enhancing the performance of various SO algorithms across different global optimization test problems.</p>
  </details>
</details>
<details>
  <summary>87. <b>标题：CleftGAN: Adapting A Style-Based Generative Adversarial Network To  Create Images Depicting Cleft Lip Deformity</b></summary>
  <p><b>编号</b>：[287]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07969</p>
  <p><b>作者</b>：Abdullah Hayajneh,  Erchin Serpedin,  Mohammad Shaqfeh,  Graeme Glass,  Mitchell A. Stotland</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ethics board-approved patient, board-approved patient images, machine learning system, ethics board-approved, images</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A major obstacle when attempting to train a machine learning system to evaluate facial clefts is the scarcity of large datasets of high-quality, ethics board-approved patient images. In response, we have built a deep learning-based cleft lip generator designed to produce an almost unlimited number of artificial images exhibiting high-fidelity facsimiles of cleft lip with wide variation. We undertook a transfer learning protocol testing different versions of StyleGAN-ADA (a generative adversarial network image generator incorporating adaptive data augmentation (ADA)) as the base model. Training images depicting a variety of cleft deformities were pre-processed to adjust for rotation, scaling, color adjustment and background blurring. The ADA modification of the primary algorithm permitted construction of our new generative model while requiring input of a relatively small number of training images. Adversarial training was carried out using 514 unique frontal photographs of cleft-affected faces to adapt a pre-trained model based on 70,000 normal faces. The Frechet Inception Distance (FID) was used to measure the similarity of the newly generated facial images to the cleft training dataset, while Perceptual Path Length (PPL) and the novel Divergence Index of Severity Histograms (DISH) measures were also used to assess the performance of the image generator that we dub CleftGAN. We found that StyleGAN3 with translation invariance (StyleGAN3-t) performed optimally as a base model. Generated images achieved a low FID reflecting a close similarity to our training input dataset of genuine cleft images. Low PPL and DISH measures reflected a smooth and semantically valid interpolation of images through the transfer learning process and a similar distribution of severity in the training and generated images, respectively.</p>
  </details>
</details>
<details>
  <summary>88. <b>标题：Towards Causal Deep Learning for Vulnerability Detection</b></summary>
  <p><b>编号</b>：[291]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07958</p>
  <p><b>作者</b>：Md Mahbubur Rahman,  Ira Ceka,  Chengzhi Mao,  Saikat Chakraborty,  Baishakhi Ray,  Wei Le</p>
  <p><b>备注</b>：Accepted at ICSE 2024 (not camera-ready version)</p>
  <p><b>关键词</b>：shown promising results, recent years, shown promising, model, learning vulnerability detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning vulnerability detection has shown promising results in recent years. However, an important challenge that still blocks it from being very useful in practice is that the model is not robust under perturbation and it cannot generalize well over the out-of-distribution (OOD) data, e.g., applying a trained model to unseen projects in real world. We hypothesize that this is because the model learned non-robust features, e.g., variable names, that have spurious correlations with labels. When the perturbed and OOD datasets no longer have the same spurious features, the model prediction fails. To address the challenge, in this paper, we introduced causality into deep learning vulnerability detection. Our approach CausalVul consists of two phases. First, we designed novel perturbations to discover spurious features that the model may use to make predictions. Second, we applied the causal learning algorithms, specifically, do-calculus, on top of existing deep learning models to systematically remove the use of spurious features and thus promote causal based prediction. Our results show that CausalVul consistently improved the model accuracy, robustness and OOD performance for all the state-of-the-art models and datasets we experimented. To the best of our knowledge, this is the first work that introduces do calculus based causal learning to software engineering models and shows it's indeed useful for improving the model accuracy, robustness and generalization. Our replication package is located at this https URL.</p>
  </details>
</details>
<details>
  <summary>89. <b>标题：Cost-Driven Hardware-Software Co-Optimization of Machine Learning  Pipelines</b></summary>
  <p><b>编号</b>：[297]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07940</p>
  <p><b>作者</b>：Ravit Sharma,  Wojciech Romaszkan,  Feiqian Zhu,  Puneet Gupta</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Researchers have long, including smart sensors, long touted, touted a vision, future enabled</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Researchers have long touted a vision of the future enabled by a proliferation of internet-of-things devices, including smart sensors, homes, and cities. Increasingly, embedding intelligence in such devices involves the use of deep neural networks. However, their storage and processing requirements make them prohibitive for cheap, off-the-shelf platforms. Overcoming those requirements is necessary for enabling widely-applicable smart devices. While many ways of making models smaller and more efficient have been developed, there is a lack of understanding of which ones are best suited for particular scenarios. More importantly for edge platforms, those choices cannot be analyzed in isolation from cost and user experience. In this work, we holistically explore how quantization, model scaling, and multi-modality interact with system components such as memory, sensors, and processors. We perform this hardware/software co-design from the cost, latency, and user-experience perspective, and develop a set of guidelines for optimal system design and model deployment for the most cost-constrained platforms. We demonstrate our approach using an end-to-end, on-device, biometric user authentication system using a $20 ESP-EYE board.</p>
  </details>
</details>
<details>
  <summary>90. <b>标题：D2 Pruning: Message Passing for Balancing Diversity and Difficulty in  Data Pruning</b></summary>
  <p><b>编号</b>：[300]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07931</p>
  <p><b>作者</b>：Adyasha Maharana,  Prateek Yadav,  Mohit Bansal</p>
  <p><b>备注</b>：17 pages (Our code is available at this https URL)</p>
  <p><b>关键词</b>：Analytical theories suggest, lower test errors, Coreset, Analytical theories, pruning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Analytical theories suggest that higher-quality data can lead to lower test errors in models trained on a fixed data budget. Moreover, a model can be trained on a lower compute budget without compromising performance if a dataset can be stripped of its redundancies. Coreset selection (or data pruning) seeks to select a subset of the training data so as to maximize the performance of models trained on this subset, also referred to as coreset. There are two dominant approaches: (1) geometry-based data selection for maximizing data diversity in the coreset, and (2) functions that assign difficulty scores to samples based on training dynamics. Optimizing for data diversity leads to a coreset that is biased towards easier samples, whereas, selection by difficulty ranking omits easy samples that are necessary for the training of deep learning models. This demonstrates that data diversity and importance scores are two complementary factors that need to be jointly considered during coreset selection. We represent a dataset as an undirected graph and propose a novel pruning algorithm, D2 Pruning, that uses forward and reverse message passing over this dataset graph for coreset selection. D2 Pruning updates the difficulty scores of each example by incorporating the difficulty of its neighboring examples in the dataset graph. Then, these updated difficulty scores direct a graph-based sampling method to select a coreset that encapsulates both diverse and difficult regions of the dataset space. We evaluate supervised and self-supervised versions of our method on various vision and language datasets. Results show that D2 Pruning improves coreset selection over previous state-of-the-art methods for up to 70% pruning rates. Additionally, we find that using D2 Pruning for filtering large multimodal datasets leads to increased diversity in the dataset and improved generalization of pretrained models.</p>
  </details>
</details>
<details>
  <summary>91. <b>标题：Deep Reinforcement Learning for Autonomous Vehicle Intersection  Navigation</b></summary>
  <p><b>编号</b>：[303]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08595</p>
  <p><b>作者</b>：Badr Ben Elallid,  Hamza El Alaoui,  Nabil Benamar</p>
  <p><b>备注</b>：Accepted for publication in the 2023 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)</p>
  <p><b>关键词</b>：Reinforcement learning algorithms, Reinforcement learning, Deterministic Policy Gradient, Twin Delayed Deep, Delayed Deep Deterministic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we explore the challenges associated with navigating complex T-intersections in dense traffic scenarios for autonomous vehicles (AVs). Reinforcement learning algorithms have emerged as a promising approach to address these challenges by enabling AVs to make safe and efficient decisions in real-time. Here, we address the problem of efficiently and safely navigating T-intersections using a lower-cost, single-agent approach based on the Twin Delayed Deep Deterministic Policy Gradient (TD3) reinforcement learning algorithm. We show that our TD3-based method, when trained and tested in the CARLA simulation platform, demonstrates stable convergence and improved safety performance in various traffic densities. Our results reveal that the proposed approach enables the AV to effectively navigate T-intersections, outperforming previous methods in terms of travel delays, collision minimization, and overall cost. This study contributes to the growing body of knowledge on reinforcement learning applications in autonomous driving and highlights the potential of single-agent, cost-effective methods for addressing more complex driving scenarios and advancing reinforcement learning algorithms in the future.</p>
  </details>
</details>
<details>
  <summary>92. <b>标题：Generative Entropic Neural Optimal Transport To Map Within and Across  Spaces</b></summary>
  <p><b>编号</b>：[304]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.09254</p>
  <p><b>作者</b>：Dominik Klein,  Théo Uscidda,  Fabian Theis,  Marco Cuturi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：machine learning, featured prominently, optimal transport, Learning, neural optimal transport</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Learning measure-to-measure mappings is a crucial task in machine learning, featured prominently in generative modeling. Recent years have witnessed a surge of techniques that draw inspiration from optimal transport (OT) theory. Combined with neural network models, these methods collectively known as \textit{Neural OT} use optimal transport as an inductive bias: such mappings should be optimal w.r.t. a given cost function, in the sense that they are able to move points in a thrifty way, within (by minimizing displacements) or across spaces (by being isometric). This principle, while intuitive, is often confronted with several practical challenges that require adapting the OT toolbox: cost functions other than the squared-Euclidean cost can be challenging to handle, the deterministic formulation of Monge maps leaves little flexibility, mapping across incomparable spaces raises multiple challenges, while the mass conservation constraint inherent to OT can provide too much credit to outliers. While each of these mismatches between practice and theory has been addressed independently in various works, we propose in this work an elegant framework to unify them, called \textit{generative entropic neural optimal transport} (GENOT). GENOT can accommodate any cost function; handles randomness using conditional generative models; can map points across incomparable spaces, and can be used as an \textit{unbalanced} solver. We evaluate our approach through experiments conducted on various synthetic datasets and demonstrate its practicality in single-cell biology. In this domain, GENOT proves to be valuable for tasks such as modeling cell development, predicting cellular responses to drugs, and translating between different data modalities of cells.</p>
  </details>
</details>
<details>
  <summary>93. <b>标题：A Deep Neural Network -- Mechanistic Hybrid Model to Predict  Pharmacokinetics in Rat</b></summary>
  <p><b>编号</b>：[307]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.09167</p>
  <p><b>作者</b>：Florian Führer,  Andrea Gruber,  Holger Diedam,  Andreas H. Göller,  Stephan Menz,  Sebastian Schneckener</p>
  <p><b>备注</b>：Journal of Computer-Aided Molecular Design</p>
  <p><b>关键词</b>：favorable kinetic profile, oral administration.The prediction, systemic availability, drugs or agro-chemicals, drug or agrochemicaldevelopment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>An important aspect in the development of small molecules as drugs or agro-chemicals is their systemic availability after intravenous and oral administration.The prediction of the systemic availability from the chemical structure of a poten-tial candidate is highly desirable, as it allows to focus the drug or agrochemicaldevelopment on compounds with a favorable kinetic profile. However, such pre-dictions are challenging as the availability is the result of the complex interplaybetween molecular properties, biology and physiology and training data is this http URL this work we improve the hybrid model developed earlier [34]. We reducethe median fold change error for the total oral exposure from 2.85 to 2.35 andfor intravenous administration from 1.95 to 1.62. This is achieved by trainingon a larger data set, improving the neural network architecture as well as theparametrization of mechanistic model. Further, we extend our approach to predictadditional endpoints and to handle different covariates, like sex and dosage this http URL contrast to a pure machine learning model, our model is able to predict newend points on which it has not been trained. We demonstrate this feature by1predicting the exposure over the first 24h, while the model has only been trainedon the total exposure.</p>
  </details>
</details>
<details>
  <summary>94. <b>标题：The Computational Complexity of Finding Stationary Points in Non-Convex  Optimization</b></summary>
  <p><b>编号</b>：[308]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.09157</p>
  <p><b>作者</b>：Alexandros Hollender,  Manolis Zampetakis</p>
  <p><b>备注</b>：Full version of COLT 2023 extended abstract</p>
  <p><b>关键词</b>：approximate stationary points, Finding approximate stationary, approximate stationary, stationary points, varepsilon</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Finding approximate stationary points, i.e., points where the gradient is approximately zero, of non-convex but smooth objective functions $f$ over unrestricted $d$-dimensional domains is one of the most fundamental problems in classical non-convex optimization. Nevertheless, the computational and query complexity of this problem are still not well understood when the dimension $d$ of the problem is independent of the approximation error. In this paper, we show the following computational and query complexity results:
1. The problem of finding approximate stationary points over unrestricted domains is PLS-complete.
2. For $d = 2$, we provide a zero-order algorithm for finding $\varepsilon$-approximate stationary points that requires at most $O(1/\varepsilon)$ value queries to the objective function.
3. We show that any algorithm needs at least $\Omega(1/\varepsilon)$ queries to the objective function and/or its gradient to find $\varepsilon$-approximate stationary points when $d=2$. Combined with the above, this characterizes the query complexity of this problem to be $\Theta(1/\varepsilon)$.
4. For $d = 2$, we provide a zero-order algorithm for finding $\varepsilon$-KKT points in constrained optimization problems that requires at most $O(1/\sqrt{\varepsilon})$ value queries to the objective function. This closes the gap between the works of Bubeck and Mikulincer [2020] and Vavasis [1993] and characterizes the query complexity of this problem to be $\Theta(1/\sqrt{\varepsilon})$.
5. Combining our results with the recent result of Fearnley et al. [2022], we show that finding approximate KKT points in constrained optimization is reducible to finding approximate stationary points in unconstrained optimization but the converse is impossible.</p>
  </details>
</details>
<details>
  <summary>95. <b>标题：Lattice Approximations in Wasserstein Space</b></summary>
  <p><b>编号</b>：[309]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.09149</p>
  <p><b>作者</b>：Keaton Hamm,  Varun Khurana</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：scaled Voronoi partition, Wasserstein space, Voronoi partition, piecewise constant measures, constant measures based</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider structured approximation of measures in Wasserstein space $W_p(\mathbb{R}^d)$ for $p\in[1,\infty)$ by discrete and piecewise constant measures based on a scaled Voronoi partition of $\mathbb{R}^d$. We show that if a full rank lattice $\Lambda$ is scaled by a factor of $h\in(0,1]$, then approximation of a measure based on the Voronoi partition of $h\Lambda$ is $O(h)$ regardless of $d$ or $p$. We then use a covering argument to show that $N$-term approximations of compactly supported measures is $O(N^{-\frac1d})$ which matches known rates for optimal quantizers and empirical measure approximation in most instances. Finally, we extend these results to noncompactly supported measures with sufficient decay.</p>
  </details>
</details>
<details>
  <summary>96. <b>标题：Physics-guided Noise Neural Proxy for Low-light Raw Image Denoising</b></summary>
  <p><b>编号</b>：[312]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.09126</p>
  <p><b>作者</b>：Hansen Feng,  Lizhi Wang,  Yiqi Huang,  Yuzhi Wang,  Hua Huang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Low-light raw image, raw image denoising, noise neural proxy, image denoising plays, noise neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Low-light raw image denoising plays a crucial role in mobile photography, and learning-based methods have become the mainstream approach. Training the learning-based methods with synthetic data emerges as an efficient and practical alternative to paired real data. However, the quality of synthetic data is inherently limited by the low accuracy of the noise model, which decreases the performance of low-light raw image denoising. In this paper, we develop a novel framework for accurate noise modeling that learns a physics-guided noise neural proxy (PNNP) from dark frames. PNNP integrates three efficient techniques: physics-guided noise decoupling (PND), physics-guided proxy model (PPM), and differentiable distribution-oriented loss (DDL). The PND decouples the dark frame into different components and handles different levels of noise in a flexible manner, which reduces the complexity of the noise neural proxy. The PPM incorporates physical priors to effectively constrain the generated noise, which promotes the accuracy of the noise neural proxy. The DDL provides explicit and reliable supervision for noise modeling, which promotes the precision of the noise neural proxy. Extensive experiments on public low-light raw image denoising datasets and real low-light imaging scenarios demonstrate the superior performance of our PNNP framework.</p>
  </details>
</details>
<details>
  <summary>97. <b>标题：Automatic Music Playlist Generation via Simulation-based Reinforcement  Learning</b></summary>
  <p><b>编号</b>：[313]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.09123</p>
  <p><b>作者</b>：Federico Tomasi,  Joseph Cauteruccio,  Surya Kanoria,  Kamil Ciosek,  Matteo Rinaldi,  Zhenwen Dai</p>
  <p><b>备注</b>：10 pages. KDD 23</p>
  <p><b>关键词</b>：music streaming services, Personalization of playlists, conventional techniques, collaborative filtering, rely on explicit</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Personalization of playlists is a common feature in music streaming services, but conventional techniques, such as collaborative filtering, rely on explicit assumptions regarding content quality to learn how to make recommendations. Such assumptions often result in misalignment between offline model objectives and online user satisfaction metrics. In this paper, we present a reinforcement learning framework that solves for such limitations by directly optimizing for user satisfaction metrics via the use of a simulated playlist-generation environment. Using this simulator we develop and train a modified Deep Q-Network, the action head DQN (AH-DQN), in a manner that addresses the challenges imposed by the large state and action space of our RL formulation. The resulting policy is capable of making recommendations from large and dynamic sets of candidate items with the expectation of maximizing consumption metrics. We analyze and evaluate agents offline via simulations that use environment models trained on both public and proprietary streaming datasets. We show how these agents lead to better user-satisfaction metrics compared to baseline methods during online A/B tests. Finally, we demonstrate that performance assessments produced from our simulator are strongly correlated with observed online metric results.</p>
  </details>
</details>
<details>
  <summary>98. <b>标题：Self supervised convolutional kernel based handcrafted feature  harmonization: Enhanced left ventricle hypertension disease phenotyping on  echocardiography</b></summary>
  <p><b>编号</b>：[318]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08897</p>
  <p><b>作者</b>：Jina Lee,  Youngtaek Hong,  Dawun Jeong,  Yeonggul Jang,  Sihyeon Jeong,  Taekgeun Jung,  Yeonyee E. Yoon,  Inki Moon,  Seung-Ah Lee,  Hyuk-Jae Chang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：extracts quantitative handcrafted, extracts quantitative, Left Ventricular Hypertrophy, Hypertensive Heart Disease, medical imaging technique</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Radiomics, a medical imaging technique, extracts quantitative handcrafted features from images to predict diseases. Harmonization in those features ensures consistent feature extraction across various imaging devices and protocols. Methods for harmonization include standardized imaging protocols, statistical adjustments, and evaluating feature robustness. Myocardial diseases such as Left Ventricular Hypertrophy (LVH) and Hypertensive Heart Disease (HHD) are diagnosed via echocardiography, but variable imaging settings pose challenges. Harmonization techniques are crucial for applying handcrafted features in disease diagnosis in such scenario. Self-supervised learning (SSL) enhances data understanding within limited datasets and adapts to diverse data settings. ConvNeXt-V2 integrates convolutional layers into SSL, displaying superior performance in various tasks. This study focuses on convolutional filters within SSL, using them as preprocessing to convert images into feature maps for handcrafted feature harmonization. Our proposed method excelled in harmonization evaluation and exhibited superior LVH classification performance compared to existing methods.</p>
  </details>
</details>
<details>
  <summary>99. <b>标题：Adam-family Methods with Decoupled Weight Decay in Deep Learning</b></summary>
  <p><b>编号</b>：[320]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08858</p>
  <p><b>作者</b>：Kuangyu Ding,  Nachuan Xiao,  Kim-Chuan Toh</p>
  <p><b>备注</b>：26 pages</p>
  <p><b>关键词</b>：decoupled weight decay, minimizing quadratically regularized, weight decay, nonconvex optimization problems, quadratically regularized nonsmooth</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we investigate the convergence properties of a wide class of Adam-family methods for minimizing quadratically regularized nonsmooth nonconvex optimization problems, especially in the context of training nonsmooth neural networks with weight decay. Motivated by the AdamW method, we propose a novel framework for Adam-family methods with decoupled weight decay. Within our framework, the estimators for the first-order and second-order moments of stochastic subgradients are updated independently of the weight decay term. Under mild assumptions and with non-diminishing stepsizes for updating the primary optimization variables, we establish the convergence properties of our proposed framework. In addition, we show that our proposed framework encompasses a wide variety of well-known Adam-family methods, hence offering convergence guarantees for these methods in the training of nonsmooth neural networks. More importantly, we show that our proposed framework asymptotically approximates the SGD method, thereby providing an explanation for the empirical observation that decoupled weight decay enhances generalization performance for Adam-family methods. As a practical application of our proposed framework, we propose a novel Adam-family method named Adam with Decoupled Weight Decay (AdamD), and establish its convergence properties under mild conditions. Numerical experiments demonstrate that AdamD outperforms Adam and is comparable to AdamW, in the aspects of both generalization performance and efficiency.</p>
  </details>
</details>
<details>
  <summary>100. <b>标题：A Nonlinear Method for time series forecasting using VMD-GARCH-LSTM  model</b></summary>
  <p><b>编号</b>：[323]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08812</p>
  <p><b>作者</b>：Zhengtao Gui,  Haoyuan Li,  Sijie Xu,  Yu Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Time series forecasting, Time series, series forecasting represents, challenging task, series forecasting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Time series forecasting represents a significant and challenging task across various fields. Recently, methods based on mode decomposition have dominated the forecasting of complex time series because of the advantages of capturing local characteristics and extracting intrinsic modes from data. Unfortunately, most models fail to capture the implied volatilities that contain significant information. To enhance the forecasting of current, rapidly evolving, and volatile time series, we propose a novel decomposition-ensemble paradigm, the VMD-LSTM-GARCH model. The Variational Mode Decomposition algorithm is employed to decompose the time series into K sub-modes. Subsequently, the GARCH model extracts the volatility information from these sub-modes, which serve as the input for the LSTM. The numerical and volatility information of each sub-mode is utilized to train a Long Short-Term Memory network. This network predicts the sub-mode, and then we aggregate the predictions from all sub-modes to produce the output. By integrating econometric and artificial intelligence methods, and taking into account both the numerical and volatility information of the time series, our proposed model demonstrates superior performance in time series forecasting, as evidenced by the significant decrease in MSE, RMSE, and MAPE in our comparative experimental results.</p>
  </details>
</details>
<details>
  <summary>101. <b>标题：PhyloGFN: Phylogenetic inference with generative flow networks</b></summary>
  <p><b>编号</b>：[326]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08774</p>
  <p><b>作者</b>：Mingyang Zhou,  Zichao Yan,  Elliot Layne,  Nikolay Malkin,  Dinghuai Zhang,  Moksh Jain,  Mathieu Blanchette,  Yoshua Bengio</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：biological entities, branch of computational, computational biology, biology that studies, relationships among biological</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Phylogenetics is a branch of computational biology that studies the evolutionary relationships among biological entities. Its long history and numerous applications notwithstanding, inference of phylogenetic trees from sequence data remains challenging: the high complexity of tree space poses a significant obstacle for the current combinatorial and probabilistic techniques. In this paper, we adopt the framework of generative flow networks (GFlowNets) to tackle two core problems in phylogenetics: parsimony-based and Bayesian phylogenetic inference. Because GFlowNets are well-suited for sampling complex combinatorial structures, they are a natural choice for exploring and sampling from the multimodal posterior distribution over tree topologies and evolutionary distances. We demonstrate that our amortized posterior sampler, PhyloGFN, produces diverse and high-quality evolutionary hypotheses on real benchmark datasets. PhyloGFN is competitive with prior works in marginal likelihood estimation and achieves a closer fit to the target distribution than state-of-the-art variational inference methods.</p>
  </details>
</details>
<details>
  <summary>102. <b>标题：Modeling Fission Gas Release at the Mesoscale using Multiscale DenseNet  Regression with Attention Mechanism and Inception Blocks</b></summary>
  <p><b>编号</b>：[327]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08767</p>
  <p><b>作者</b>：Peter Toma,  Md Ali Muntaha,  Joel B. Harley,  Michael R. Tonks</p>
  <p><b>备注</b>：Submitted at Journal of Nuclear Materials, 20 pages, 10 figures, 3 tables</p>
  <p><b>关键词</b>：fission gas release, evolution impacts FGR, microstructure evolution impacts, Mesoscale simulations, gas release</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Mesoscale simulations of fission gas release (FGR) in nuclear fuel provide a powerful tool for understanding how microstructure evolution impacts FGR, but they are computationally intensive. In this study, we present an alternate, data-driven approach, using deep learning to predict instantaneous FGR flux from 2D nuclear fuel microstructure images. Four convolutional neural network (CNN) architectures with multiscale regression are trained and evaluated on simulated FGR data generated using a hybrid phase field/cluster dynamics model. All four networks show high predictive power, with $R^{2}$ values above 98%. The best performing network combine a Convolutional Block Attention Module (CBAM) and InceptionNet mechanisms to provide superior accuracy (mean absolute percentage error of 4.4%), training stability, and robustness on very low instantaneous FGR flux values.</p>
  </details>
</details>
<details>
  <summary>103. <b>标题：Designing Observables for Measurements with Deep Learning</b></summary>
  <p><b>编号</b>：[329]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08717</p>
  <p><b>作者</b>：Owen Long,  Benjamin Nachman</p>
  <p><b>备注</b>：Submitted to EPJC</p>
  <p><b>关键词</b>：infer fundamental, analyses in particle, particle and nuclear, simulations to infer, underlying physics models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Many analyses in particle and nuclear physics use simulations to infer fundamental, effective, or phenomenological parameters of the underlying physics models. When the inference is performed with unfolded cross sections, the observables are designed using physics intuition and heuristics. We propose to design optimal observables with machine learning. Unfolded, differential cross sections in a neural network output contain the most information about parameters of interest and can be well-measured by construction. We demonstrate this idea using two physics models for inclusive measurements in deep inelastic scattering.</p>
  </details>
</details>
<details>
  <summary>104. <b>标题：Machine Learning Who to Nudge: Causal vs Predictive Targeting in a Field  Experiment on Student Financial Aid Renewal</b></summary>
  <p><b>编号</b>：[331]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08672</p>
  <p><b>作者</b>：Susan Athey,  Niall Keleher,  Jann Spiess</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：targeting, low baseline outcomes, baseline outcomes, baseline, students</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In many settings, interventions may be more effective for some individuals than others, so that targeting interventions may be beneficial. We analyze the value of targeting in the context of a large-scale field experiment with over 53,000 college students, where the goal was to use "nudges" to encourage students to renew their financial-aid applications before a non-binding deadline. We begin with baseline approaches to targeting. First, we target based on a causal forest that estimates heterogeneous treatment effects and then assigns students to treatment according to those estimated to have the highest treatment effects. Next, we evaluate two alternative targeting policies, one targeting students with low predicted probability of renewing financial aid in the absence of the treatment, the other targeting those with high probability. The predicted baseline outcome is not the ideal criterion for targeting, nor is it a priori clear whether to prioritize low, high, or intermediate predicted probability. Nonetheless, targeting on low baseline outcomes is common in practice, for example because the relationship between individual characteristics and treatment effects is often difficult or impossible to estimate with historical data. We propose hybrid approaches that incorporate the strengths of both predictive approaches (accurate estimation) and causal approaches (correct criterion); we show that targeting intermediate baseline outcomes is most effective, while targeting based on low baseline outcomes is detrimental. In one year of the experiment, nudging all students improved early filing by an average of 6.4 percentage points over a baseline average of 37% filing, and we estimate that targeting half of the students using our preferred policy attains around 75% of this benefit.</p>
  </details>
</details>
<details>
  <summary>105. <b>标题：Unit Commitment Predictor With a Performance Guarantee: A Support Vector  Machine Classifier</b></summary>
  <p><b>编号</b>：[337]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08601</p>
  <p><b>作者</b>：Farzaneh Pourahmadi,  Jalal Kazempour</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large-scale unit commitment, limited time frame, unit commitment problem, solve large-scale unit, unit commitment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The system operators usually need to solve large-scale unit commitment problems within limited time frame for computation. This paper provides a pragmatic solution, showing how by learning and predicting the on/off commitment decisions of conventional units, there is a potential for system operators to warm start their solver and speed up their computation significantly. For the prediction, we train linear and kernelized support vector machine classifiers, providing an out-of-sample performance guarantee if properly regularized, converting to distributionally robust classifiers. For the unit commitment problem, we solve a mixed-integer second-order cone problem. Our results based on the IEEE 6-bus and 118-bus test systems show that the kernelized SVM with proper regularization outperforms other classifiers, reducing the computational time by a factor of 1.7. In addition, if there is a tight computational limit, while the unit commitment problem without warm start is far away from the optimal solution, its warmly started version can be solved to optimality within the time limit.</p>
  </details>
</details>
<details>
  <summary>106. <b>标题：Octopus: Embodied Vision-Language Programmer from Environmental Feedback</b></summary>
  <p><b>编号</b>：[342]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08588</p>
  <p><b>作者</b>：Jingkang Yang,  Yuhao Dong,  Shuai Liu,  Bo Li,  Ziyue Wang,  Chencheng Jiang,  Haoran Tan,  Jiamu Kang,  Yuanhan Zhang,  Kaiyang Zhou,  Ziwei Liu</p>
  <p><b>备注</b>：Project Page: this https URL, Codebase: this https URL</p>
  <p><b>关键词</b>：achieved substantial progress, Large vision-language models, Large vision-language, perception and reasoning, achieved substantial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large vision-language models (VLMs) have achieved substantial progress in multimodal perception and reasoning. Furthermore, when seamlessly integrated into an embodied agent, it signifies a crucial stride towards the creation of autonomous and context-aware systems capable of formulating plans and executing commands with precision. In this paper, we introduce Octopus, a novel VLM designed to proficiently decipher an agent's vision and textual task objectives and to formulate intricate action sequences and generate executable code. Our design allows the agent to adeptly handle a wide spectrum of tasks, ranging from mundane daily chores in simulators to sophisticated interactions in complex video games. Octopus is trained by leveraging GPT-4 to control an explorative agent to generate training data, i.e., action blueprints and the corresponding executable code, within our experimental environment called OctoVerse. We also collect the feedback that allows the enhanced training scheme of Reinforcement Learning with Environmental Feedback (RLEF). Through a series of experiments, we illuminate Octopus's functionality and present compelling results, and the proposed RLEF turns out to refine the agent's decision-making. By open-sourcing our model architecture, simulator, and dataset, we aspire to ignite further innovation and foster collaborative applications within the broader embodied AI community.</p>
  </details>
</details>
<details>
  <summary>107. <b>标题：Tree-Planner: Efficient Close-loop Task Planning with Large Language  Models</b></summary>
  <p><b>编号</b>：[348]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08582</p>
  <p><b>作者</b>：Mengkang Hu,  Yao Mu,  Xinmiao Yu,  Mingyu Ding,  Shiguang Wu,  Wenqi Shao,  Qiguang Chen,  Bin Wang,  Yu Qiao,  Ping Luo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：paper studies close-loop, studies close-loop task, Large Language Models, prompting Large Language, sequence of skills</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper studies close-loop task planning, which refers to the process of generating a sequence of skills (a plan) to accomplish a specific goal while adapting the plan based on real-time observations. Recently, prompting Large Language Models (LLMs) to generate actions iteratively has become a prevalent paradigm due to its superior performance and user-friendliness. However, this paradigm is plagued by two inefficiencies: high token consumption and redundant error correction, both of which hinder its scalability for large-scale testing and applications. To address these issues, we propose Tree-Planner, which reframes task planning with LLMs into three distinct phases: plan sampling, action tree construction, and grounded deciding. Tree-Planner starts by using an LLM to sample a set of potential plans before execution, followed by the aggregation of them to form an action tree. Finally, the LLM performs a top-down decision-making process on the tree, taking into account real-time environmental information. Experiments show that Tree-Planner achieves state-of-the-art performance while maintaining high efficiency. By decomposing LLM queries into a single plan-sampling call and multiple grounded-deciding calls, a considerable part of the prompt are less likely to be repeatedly consumed. As a result, token consumption is reduced by 92.2% compared to the previously best-performing model. Additionally, by enabling backtracking on the action tree as needed, the correction process becomes more flexible, leading to a 40.5% decrease in error corrections. Project page: this https URL</p>
  </details>
</details>
<details>
  <summary>108. <b>标题：Visual Data-Type Understanding does not emerge from Scaling  Vision-Language Models</b></summary>
  <p><b>编号</b>：[352]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08577</p>
  <p><b>作者</b>：Vishaal Udandarao,  Max F. Burg,  Samuel Albanie,  Matthias Bethge</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：including impressive instances, yielding remarkable success, textit, Recent advances, including impressive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advances in the development of vision-language models (VLMs) are yielding remarkable success in recognizing visual semantic content, including impressive instances of compositional image understanding. Here, we introduce the novel task of \textit{Visual Data-Type Identification}, a basic perceptual skill with implications for data curation (e.g., noisy data-removal from large datasets, domain-specific retrieval) and autonomous vision (e.g., distinguishing changing weather conditions from camera lens staining). We develop two datasets consisting of animal images altered across a diverse set of 27 visual \textit{data-types}, spanning four broad categories. An extensive zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a nuanced performance landscape. While VLMs are reasonably good at identifying certain stylistic \textit{data-types}, such as cartoons and sketches, they struggle with simpler \textit{data-types} arising from basic manipulations like image rotations or additive noise. Our findings reveal that (i) model scaling alone yields marginal gains for contrastively-trained models like CLIP, and (ii) there is a pronounced drop in performance for the largest auto-regressively trained VLMs like OpenFlamingo. This finding points to a blind spot in current frontier VLMs: they excel in recognizing semantic content but fail to acquire an understanding of visual \textit{data-types} through scaling. By analyzing the pre-training distributions of these models and incorporating \textit{data-type} information into the captions during fine-tuning, we achieve a significant enhancement in performance. By exploring this previously uncharted task, we aim to set the stage for further advancing VLMs to equip them with visual data-type understanding. Code and datasets are released \href{this https URL}{here}.</p>
  </details>
</details>
<details>
  <summary>109. <b>标题：Learning to Act from Actionless Videos through Dense Correspondences</b></summary>
  <p><b>编号</b>：[353]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08576</p>
  <p><b>作者</b>：Po-Chen Ko,  Jiayuan Mao,  Yilun Du,  Shao-Hua Sun,  Joshua B. Tenenbaum</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：reliably executing diverse, construct a video-based, capable of reliably, executing diverse tasks, video-based robot policy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we present an approach to construct a video-based robot policy capable of reliably executing diverse tasks across different robots and environments from few video demonstrations without using any action annotations. Our method leverages images as a task-agnostic representation, encoding both the state and action information, and text as a general representation for specifying robot goals. By synthesizing videos that ``hallucinate'' robot executing actions and in combination with dense correspondences between frames, our approach can infer the closed-formed action to execute to an environment without the need of any explicit action labels. This unique capability allows us to train the policy solely based on RGB videos and deploy learned policies to various robotic tasks. We demonstrate the efficacy of our approach in learning policies on table-top manipulation and navigation tasks. Additionally, we contribute an open-source framework for efficient video modeling, enabling the training of high-fidelity policy models with four GPUs within a single day.</p>
  </details>
</details>
<details>
  <summary>110. <b>标题：Jigsaw: Supporting Designers in Prototyping Multimodal Applications by  Assembling AI Foundation Models</b></summary>
  <p><b>编号</b>：[354]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08574</p>
  <p><b>作者</b>：David Chuan-En Lin,  Nikolas Martelaro</p>
  <p><b>备注</b>：Webpage: this https URL</p>
  <p><b>关键词</b>：generating visual prototypes, including ideating design, ideating design concepts, Recent advancements, including ideating</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advancements in AI foundation models have made it possible for them to be utilized off-the-shelf for creative tasks, including ideating design concepts or generating visual prototypes. However, integrating these models into the creative process can be challenging as they often exist as standalone applications tailored to specific tasks. To address this challenge, we introduce Jigsaw, a prototype system that employs puzzle pieces as metaphors to represent foundation models. Jigsaw allows designers to combine different foundation model capabilities across various modalities by assembling compatible puzzle pieces. To inform the design of Jigsaw, we interviewed ten designers and distilled design goals. In a user study, we showed that Jigsaw enhanced designers' understanding of available foundation model capabilities, provided guidance on combining capabilities across different modalities and tasks, and served as a canvas to support design exploration, prototyping, and documentation.</p>
  </details>
</details>
<details>
  <summary>111. <b>标题：Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders</b></summary>
  <p><b>编号</b>：[356]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08571</p>
  <p><b>作者</b>：Jan Dubiński,  Stanisław Pawlak,  Franziska Boenisch,  Tomasz Trzciński,  Adam Dziedzic</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Machine Learning, generate vector representations, generate vector, APIs provide, legitimate API users</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine Learning as a Service (MLaaS) APIs provide ready-to-use and high-utility encoders that generate vector representations for given inputs. Since these encoders are very costly to train, they become lucrative targets for model stealing attacks during which an adversary leverages query access to the API to replicate the encoder locally at a fraction of the original training costs. We propose Bucks for Buckets (B4B), the first active defense that prevents stealing while the attack is happening without degrading representation quality for legitimate API users. Our defense relies on the observation that the representations returned to adversaries who try to steal the encoder's functionality cover a significantly larger fraction of the embedding space than representations of legitimate users who utilize the encoder to solve a particular downstream task.vB4B leverages this to adaptively adjust the utility of the returned representations according to a user's coverage of the embedding space. To prevent adaptive adversaries from eluding our defense by simply creating multiple user accounts (sybils), B4B also individually transforms each user's representations. This prevents the adversary from directly aggregating representations over multiple accounts to create their stolen encoder copy. Our active defense opens a new path towards securely sharing and democratizing encoders over public APIs.</p>
  </details>
</details>
<details>
  <summary>112. <b>标题：Transformers as Decision Makers: Provable In-Context Reinforcement  Learning via Supervised Pretraining</b></summary>
  <p><b>编号</b>：[359]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08566</p>
  <p><b>作者</b>：Licong Lin,  Yu Bai,  Song Mei</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：demonstrated remarkable in-context, remarkable in-context reinforcement, make good decisions, unseen environments, datasets have demonstrated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large transformer models pretrained on offline reinforcement learning datasets have demonstrated remarkable in-context reinforcement learning (ICRL) capabilities, where they can make good decisions when prompted with interaction trajectories from unseen environments. However, when and how transformers can be trained to perform ICRL have not been theoretically well-understood. In particular, it is unclear which reinforcement-learning algorithms transformers can perform in context, and how distribution mismatch in offline training data affects the learned algorithms. This paper provides a theoretical framework that analyzes supervised pretraining for ICRL. This includes two recently proposed training methods -- algorithm distillation and decision-pretrained transformers. First, assuming model realizability, we prove the supervised-pretrained transformer will imitate the conditional expectation of the expert algorithm given the observed trajectory. The generalization error will scale with model capacity and a distribution divergence factor between the expert and offline algorithms. Second, we show transformers with ReLU attention can efficiently approximate near-optimal online reinforcement learning algorithms like LinUCB and Thompson sampling for stochastic linear bandits, and UCB-VI for tabular Markov decision processes. This provides the first quantitative analysis of the ICRL capabilities of transformers pretrained from offline trajectories.</p>
  </details>
</details>
<details>
  <summary>113. <b>标题：Offline Retraining for Online RL: Decoupled Policy Learning to Mitigate  Exploration Bias</b></summary>
  <p><b>编号</b>：[363]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08558</p>
  <p><b>作者</b>：Max Sobol Mark,  Archit Sharma,  Fahim Tajwar,  Rafael Rafailov,  Sergey Levine,  Chelsea Finn</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：state coverage, optimistically explore, policy, online, offline</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>It is desirable for policies to optimistically explore new states and behaviors during online reinforcement learning (RL) or fine-tuning, especially when prior offline data does not provide enough state coverage. However, exploration bonuses can bias the learned policy, and our experiments find that naive, yet standard use of such bonuses can fail to recover a performant policy. Concurrently, pessimistic training in offline RL has enabled recovery of performant policies from static datasets. Can we leverage offline RL to recover better policies from online interaction? We make a simple observation that a policy can be trained from scratch on all interaction data with pessimistic objectives, thereby decoupling the policies used for data collection and for evaluation. Specifically, we propose offline retraining, a policy extraction step at the end of online fine-tuning in our Offline-to-Online-to-Offline (OOO) framework for reinforcement learning (RL). An optimistic (exploration) policy is used to interact with the environment, and a separate pessimistic (exploitation) policy is trained on all the observed data for evaluation. Such decoupling can reduce any bias from online interaction (intrinsic rewards, primacy bias) in the evaluation policy, and can allow more exploratory behaviors during online interaction which in turn can generate better data for exploitation. OOO is complementary to several offline-to-online RL and online RL methods, and improves their average performance by 14% to 26% in our fine-tuning experiments, achieves state-of-the-art performance on several environments in the D4RL benchmarks, and improves online RL performance by 165% on two OpenAI gym environments. Further, OOO can enable fine-tuning from incomplete offline datasets where prior methods can fail to recover a performant policy. Implementation: this https URL</p>
  </details>
</details>
<details>
  <summary>114. <b>标题：Cross-Episodic Curriculum for Transformer Agents</b></summary>
  <p><b>编号</b>：[365]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08549</p>
  <p><b>作者</b>：Lucy Xiaoyang Shi,  Yunfan Jiang,  Jake Grigsby,  Linxi "Jim" Fan,  Yuke Zhu</p>
  <p><b>备注</b>：To appear in NeurIPS 2023; The first two authors contributed equally</p>
  <p><b>关键词</b>：CEC, Transformer, Transformer agent learning, Curriculum, curriculum captures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a new algorithm, Cross-Episodic Curriculum (CEC), to boost the learning efficiency and generalization of Transformer agents. Central to CEC is the placement of cross-episodic experiences into a Transformer's context, which forms the basis of a curriculum. By sequentially structuring online learning trials and mixed-quality demonstrations, CEC constructs curricula that encapsulate learning progression and proficiency increase across episodes. Such synergy combined with the potent pattern recognition capabilities of Transformer models delivers a powerful cross-episodic attention mechanism. The effectiveness of CEC is demonstrated under two representative scenarios: one involving multi-task reinforcement learning with discrete control, such as in DeepMind Lab, where the curriculum captures the learning progression in both individual and progressively complex settings; and the other involving imitation learning with mixed-quality data for continuous control, as seen in RoboMimic, where the curriculum captures the improvement in demonstrators' expertise. In all instances, policies resulting from CEC exhibit superior performance and strong generalization. Code is open-sourced at this https URL to facilitate research on Transformer agent learning.</p>
  </details>
</details>
<details>
  <summary>115. <b>标题：Stronger Coreset Bounds for Kernel Density Estimators via Chaining</b></summary>
  <p><b>编号</b>：[366]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08548</p>
  <p><b>作者</b>：Rainie Bozzai,  Thomas Rothvoss</p>
  <p><b>备注</b>：23 pages</p>
  <p><b>关键词</b>：big, give improved bounds, frac, sqrt, varepsilon</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We apply the discrepancy method and a chaining approach to give improved bounds on the coreset complexity of a wide class of kernel functions. Our results give randomized polynomial time algorithms to produce coresets of size $O\big(\frac{\sqrt{d}}{\varepsilon}\sqrt{\log\log \frac{1}{\varepsilon}}\big)$ for the Gaussian and Laplacian kernels in the case that the data set is uniformly bounded, an improvement that was not possible with previous techniques. We also obtain coresets of size $O\big(\frac{1}{\varepsilon}\sqrt{\log\log \frac{1}{\varepsilon}}\big)$ for the Laplacian kernel for $d$ constant. Finally, we give the best known bounds of $O\big(\frac{\sqrt{d}}{\varepsilon}\sqrt{\log(2\max\{1,\alpha\})}\big)$ on the coreset complexity of the exponential, Hellinger, and JS Kernels, where $1/\alpha$ is the bandwidth parameter of the kernel.</p>
  </details>
</details>
<details>
  <summary>116. <b>标题：Do pretrained Transformers Really Learn In-context by Gradient Descent?</b></summary>
  <p><b>编号</b>：[369]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08540</p>
  <p><b>作者</b>：Lingfeng Shen,  Aayush Mishra,  Daniel Khashabi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：In-Context Learning, ICL, implicitly equivalent, Gradient Descent, language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Is In-Context Learning (ICL) implicitly equivalent to Gradient Descent (GD)? Several recent works draw analogies between the dynamics of GD and the emergent behavior of ICL in large language models. However, these works make assumptions far from the realistic natural language setting in which language models are trained. Such discrepancies between theory and practice, therefore, necessitate further investigation to validate their applicability.
We start by highlighting the weaknesses in prior works that construct Transformer weights to simulate gradient descent. Their experiments with training Transformers on ICL objective, inconsistencies in the order sensitivity of ICL and GD, sparsity of the constructed weights, and sensitivity to parameter changes are some examples of a mismatch from the real-world setting.
Furthermore, we probe and compare the ICL vs. GD hypothesis in a natural setting. We conduct comprehensive empirical analyses on language models pretrained on natural data (LLaMa-7B). Our comparisons on various performance metrics highlight the inconsistent behavior of ICL and GD as a function of various factors such as datasets, models, and number of demonstrations. We observe that ICL and GD adapt the output distribution of language models differently. These results indicate that the equivalence between ICL and GD is an open hypothesis, requires nuanced considerations and calls for further studies.</p>
  </details>
</details>
<details>
  <summary>117. <b>标题：Unsupervised Learning of Object-Centric Embeddings for Cell Instance  Segmentation in Microscopy Images</b></summary>
  <p><b>编号</b>：[383]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08501</p>
  <p><b>作者</b>：Steffen Wolf,  Manan Lalit,  Henry Westmacott,  Katie McDole,  Jan Funke</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：biomedical applications, microscopy images, image patches, method, embed image patches</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Segmentation of objects in microscopy images is required for many biomedical applications. We introduce object-centric embeddings (OCEs), which embed image patches such that the spatial offsets between patches cropped from the same object are preserved. Those learnt embeddings can be used to delineate individual objects and thus obtain instance segmentations. Here, we show theoretically that, under assumptions commonly found in microscopy images, OCEs can be learnt through a self-supervised task that predicts the spatial offset between image patches. Together, this forms an unsupervised cell instance segmentation method which we evaluate on nine diverse large-scale microscopy datasets. Segmentations obtained with our method lead to substantially improved results, compared to state-of-the-art baselines on six out of nine datasets, and perform on par on the remaining three datasets. If ground-truth annotations are available, our method serves as an excellent starting point for supervised training, reducing the required amount of ground-truth needed by one order of magnitude, thus substantially increasing the practical applicability of our method. Source code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>118. <b>标题：Impact of time and note duration tokenizations on deep learning symbolic  music modeling</b></summary>
  <p><b>编号</b>：[384]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08497</p>
  <p><b>作者</b>：Nathan Fradet,  Nicolas Gutowski,  Fabien Chhel,  Jean-Pierre Briot</p>
  <p><b>备注</b>：ISMIR 2023</p>
  <p><b>关键词</b>：Music Information Retrieval, Information Retrieval, MIR, Retrieval, Symbolic music</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Symbolic music is widely used in various deep learning tasks, including generation, transcription, synthesis, and Music Information Retrieval (MIR). It is mostly employed with discrete models like Transformers, which require music to be tokenized, i.e., formatted into sequences of distinct elements called tokens. Tokenization can be performed in different ways. As Transformer can struggle at reasoning, but capture more easily explicit information, it is important to study how the way the information is represented for such model impact their performances. In this work, we analyze the common tokenization methods and experiment with time and note duration representations. We compare the performances of these two impactful criteria on several tasks, including composer and emotion classification, music generation, and sequence representation learning. We demonstrate that explicit information leads to better results depending on the task.</p>
  </details>
</details>
<details>
  <summary>119. <b>标题：Prometheus: Inducing Fine-grained Evaluation Capability in Language  Models</b></summary>
  <p><b>编号</b>：[387]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08491</p>
  <p><b>作者</b>：Seungone Kim,  Jamin Shin,  Yejin Cho,  Joel Jang,  Shayne Longpre,  Hwaran Lee,  Sangdoo Yun,  Seongjin Shin,  Sungdong Kim,  James Thorne,  Minjoon Seo</p>
  <p><b>备注</b>：Work in Progress</p>
  <p><b>关键词</b>：powerful proprietary Large, proprietary Large Language, Large Language Model, Large Language, proprietary Large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, using a powerful proprietary Large Language Model (LLM) (e.g., GPT-4) as an evaluator for long-form responses has become the de facto standard. However, for practitioners with large-scale evaluation tasks and custom criteria in consideration (e.g., child-readability), using proprietary LLMs as an evaluator is unreliable due to the closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose Prometheus, a fully open-source LLM that is on par with GPT-4's evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. We first construct the Feedback Collection, a new dataset that consists of 1K fine-grained score rubrics, 20K instructions, and 100K responses and language feedback generated by GPT-4. Using the Feedback Collection, we train Prometheus, a 13B evaluator LLM that can assess any given long-form text based on customized score rubric provided by the user. Experimental results show that Prometheus scores a Pearson correlation of 0.897 with human evaluators when evaluating with 45 customized score rubrics, which is on par with GPT-4 (0.882), and greatly outperforms ChatGPT (0.392). Furthermore, measuring correlation with GPT-4 with 1222 customized score rubrics across four benchmarks (MT Bench, Vicuna Bench, Feedback Bench, Flask Eval) shows similar trends, bolstering Prometheus's capability as an evaluator LLM. Lastly, Prometheus achieves the highest accuracy on two human preference benchmarks (HHH Alignment & MT Bench Human Judgment) compared to open-sourced reward models explicitly trained on human preference datasets, highlighting its potential as an universal reward model. We open-source our code, dataset, and model at this https URL.</p>
  </details>
</details>
<details>
  <summary>120. <b>标题：Can We Edit Multimodal Large Language Models?</b></summary>
  <p><b>编号</b>：[391]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08475</p>
  <p><b>作者</b>：Siyuan Cheng,  Bozhong Tian,  Qingbin Liu,  Xi Chen,  Yongheng Wang,  Huajun Chen,  Ningyu Zhang</p>
  <p><b>备注</b>：EMNLP 2023</p>
  <p><b>关键词</b>：Large Language Models, Multimodal Large Language, Large Language, editing Multimodal Large, Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in this https URL.</p>
  </details>
</details>
<details>
  <summary>121. <b>标题：Strategies and impact of learning curve estimation for CNN-based image  classification</b></summary>
  <p><b>编号</b>：[394]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08470</p>
  <p><b>作者</b>：Laura Didyk,  Brayden Yarish,  Michael A. Beck,  Christopher P. Bidinosti,  Christopher J. Henry</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：machine learning models, learning models improves, Learning curves, models, Learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Learning curves are a measure for how the performance of machine learning models improves given a certain volume of training data. Over a wide variety of applications and models it was observed that learning curves follow -- to a large extent -- a power law behavior. This makes the performance of different models for a given task somewhat predictable and opens the opportunity to reduce the training time for practitioners, who are exploring the space of possible models and hyperparameters for the problem at hand. By estimating the learning curve of a model from training on small subsets of data only the best models need to be considered for training on the full dataset. How to choose subset sizes and how often to sample models on these to obtain estimates is however not researched. Given that the goal is to reduce overall training time strategies are needed that sample the performance in a time-efficient way and yet leads to accurate learning curve estimates. In this paper we formulate the framework for these strategies and propose several strategies. Further we evaluate the strategies for simulated learning curves and in experiments with popular datasets and models for image classification tasks.</p>
  </details>
</details>
<details>
  <summary>122. <b>标题：DistillSpec: Improving Speculative Decoding via Knowledge Distillation</b></summary>
  <p><b>编号</b>：[396]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08461</p>
  <p><b>作者</b>：Yongchao Zhou,  Kaifeng Lyu,  Ankit Singh Rawat,  Aditya Krishna Menon,  Afshin Rostamizadeh,  Sanjiv Kumar,  Jean-François Kagy,  Rishabh Agarwal</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：accelerates large language, generating multiple tokens, large language model, language model inference, target model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Speculative decoding (SD) accelerates large language model inference by employing a faster draft model for generating multiple tokens, which are then verified in parallel by the larger target model, resulting in the text generated according to the target model distribution. However, identifying a compact draft model that is well-aligned with the target model is challenging. To tackle this issue, we propose DistillSpec that uses knowledge distillation to better align the draft model with the target model, before applying SD. DistillSpec makes two key design choices, which we demonstrate via systematic study to be crucial to improving the draft and target alignment: utilizing on-policy data generation from the draft model, and tailoring the divergence function to the task and decoding strategy. Notably, DistillSpec yields impressive 10 - 45% speedups over standard SD on a range of standard benchmarks, using both greedy and non-greedy sampling. Furthermore, we combine DistillSpec with lossy SD to achieve fine-grained control over the latency vs. task performance trade-off. Finally, in practical scenarios with models of varying sizes, first using distillation to boost the performance of the target model and then applying DistillSpec to train a well-aligned draft model can reduce decoding latency by 6-10x with minimal performance drop, compared to standard decoding without distillation.</p>
  </details>
</details>
<details>
  <summary>123. <b>标题：A Survey on Heterogeneous Transfer Learning</b></summary>
  <p><b>编号</b>：[397]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08459</p>
  <p><b>作者</b>：Runxue Bao,  Yiming Sun,  Yuhe Gao,  Jindong Wang,  Qiang Yang,  Haifeng Chen,  Zhi-Hong Mao,  Xing Xie,  Ye Ye</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：enhance model performance, transfer learning, underpinning many real-world, approach utilizing knowledge, enhance model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The application of transfer learning, an approach utilizing knowledge from a source domain to enhance model performance in a target domain, has seen a tremendous rise in recent years, underpinning many real-world scenarios. The key to its success lies in the shared common knowledge between the domains, a prerequisite in most transfer learning methodologies. These methods typically presuppose identical feature spaces and label spaces in both domains, known as homogeneous transfer learning, which, however, is not always a practical assumption. Oftentimes, the source and target domains vary in feature spaces, data distributions, and label spaces, making it challenging or costly to secure source domain data with identical feature and label spaces as the target domain. Arbitrary elimination of these differences is not always feasible or optimal. Thus, heterogeneous transfer learning, acknowledging and dealing with such disparities, has emerged as a promising approach for a variety of tasks. Despite the existence of a survey in 2017 on this topic, the fast-paced advances post-2017 necessitate an updated, in-depth review. We therefore present a comprehensive survey of recent developments in heterogeneous transfer learning methods, offering a systematic guide for future research. Our paper reviews methodologies for diverse learning scenarios, discusses the limitations of current studies, and covers various application contexts, including Natural Language Processing, Computer Vision, Multimodality, and Biomedicine, to foster a deeper understanding and spur future research.</p>
  </details>
</details>
<details>
  <summary>124. <b>标题：Towards Robust Multi-Modal Reasoning via Model Selection</b></summary>
  <p><b>编号</b>：[404]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08446</p>
  <p><b>作者</b>：Xiangyan Liu,  Rongxue Li,  Wei Ji,  Tao Lin</p>
  <p><b>备注</b>：10 pages, 5 figures</p>
  <p><b>关键词</b>：Large Language Model, Large Language, Language Model, model selection, recent research</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The reasoning capabilities of LLM (Large Language Model) are widely acknowledged in recent research, inspiring studies on tool learning and autonomous agents. LLM serves as the "brain" of agent, orchestrating multiple tools for collaborative multi-step task solving. Unlike methods invoking tools like calculators or weather APIs for straightforward tasks, multi-modal agents excel by integrating diverse AI models for complex challenges. However, current multi-modal agents neglect the significance of model selection: they primarily focus on the planning and execution phases, and will only invoke predefined task-specific models for each subtask, making the execution fragile. Meanwhile, other traditional model selection methods are either incompatible with or suboptimal for the multi-modal agent scenarios, due to ignorance of dependencies among subtasks arising by multi-step reasoning.
To this end, we identify the key challenges therein and propose the $\textit{M}^3$ framework as a plug-in with negligible runtime overhead at test-time. This framework improves model selection and bolsters the robustness of multi-modal agents in multi-step reasoning. In the absence of suitable benchmarks, we create MS-GQA, a new dataset specifically designed to investigate the model selection challenge in multi-modal agents. Our experiments reveal that our framework enables dynamic model selection, considering both user inputs and subtask dependencies, thereby robustifying the overall reasoning process. Our code and benchmark: this https URL.</p>
  </details>
</details>
<details>
  <summary>125. <b>标题：Neural Sampling in Hierarchical Exponential-family Energy-based Models</b></summary>
  <p><b>编号</b>：[411]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08431</p>
  <p><b>作者</b>：Xingsi Dong,  Si Wu</p>
  <p><b>备注</b>：NeurIPS 2023</p>
  <p><b>关键词</b>：Bayesian brain theory, brain theory suggests, brain employs generative, employs generative models, theory suggests</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Bayesian brain theory suggests that the brain employs generative models to understand the external world. The sampling-based perspective posits that the brain infers the posterior distribution through samples of stochastic neuronal responses. Additionally, the brain continually updates its generative model to approach the true distribution of the external world. In this study, we introduce the Hierarchical Exponential-family Energy-based (HEE) model, which captures the dynamics of inference and learning. In the HEE model, we decompose the partition function into individual layers and leverage a group of neurons with shorter time constants to sample the gradient of the decomposed normalization term. This allows our model to estimate the partition function and perform inference simultaneously, circumventing the negative phase encountered in conventional energy-based models (EBMs). As a result, the learning process is localized both in time and space, and the model is easy to converge. To match the brain's rapid computation, we demonstrate that neural adaptation can serve as a momentum term, significantly accelerating the inference process. On natural image datasets, our model exhibits representations akin to those observed in the biological visual system. Furthermore, for the machine learning community, our model can generate observations through joint or marginal generation. We show that marginal generation outperforms joint generation and achieves performance on par with other EBMs.</p>
  </details>
</details>
<details>
  <summary>126. <b>标题：Differentially Private Non-convex Learning for Multi-layer Neural  Networks</b></summary>
  <p><b>编号</b>：[414]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08425</p>
  <p><b>作者</b>：Hanpu Shen,  Cheng-Long Wang,  Zihang Xiang,  Yiming Ying,  Di Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Differentially Private Stochastic, Private Stochastic Optimization, Differentially Private, Private Stochastic, Stochastic Optimization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper focuses on the problem of Differentially Private Stochastic Optimization for (multi-layer) fully connected neural networks with a single output node. In the first part, we examine cases with no hidden nodes, specifically focusing on Generalized Linear Models (GLMs). We investigate the well-specific model where the random noise possesses a zero mean, and the link function is both bounded and Lipschitz continuous. We propose several algorithms and our analysis demonstrates the feasibility of achieving an excess population risk that remains invariant to the data dimension. We also delve into the scenario involving the ReLU link function, and our findings mirror those of the bounded link function. We conclude this section by contrasting well-specified and misspecified models, using ReLU regression as a representative example.
In the second part of the paper, we extend our ideas to two-layer neural networks with sigmoid or ReLU activation functions in the well-specified model. In the third part, we study the theoretical guarantees of DP-SGD in Abadi et al. (2016) for fully connected multi-layer neural networks. By utilizing recent advances in Neural Tangent Kernel theory, we provide the first excess population risk when both the sample size and the width of the network are sufficiently large. Additionally, we discuss the role of some parameters in DP-SGD regarding their utility, both theoretically and empirically.</p>
  </details>
</details>
<details>
  <summary>127. <b>标题：Jailbreaking Black Box Large Language Models in Twenty Queries</b></summary>
  <p><b>编号</b>：[417]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08419</p>
  <p><b>作者</b>：Patrick Chao,  Alexander Robey,  Edgar Dobriban,  Hamed Hassani,  George J. Pappas,  Eric Wong</p>
  <p><b>备注</b>：21 pages, 10 figures</p>
  <p><b>关键词</b>：large language models, growing interest, interest in ensuring, ensuring that large, large language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM-2.</p>
  </details>
</details>
<details>
  <summary>128. <b>标题：Towards Better Evaluation of Instruction-Following: A Case-Study in  Summarization</b></summary>
  <p><b>编号</b>：[427]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08394</p>
  <p><b>作者</b>：Ondrej Skopek,  Rahul Aralikatte,  Sian Gooding,  Victor Carbune</p>
  <p><b>备注</b>：Accepted to CoNLL 2023</p>
  <p><b>关键词</b>：follow user instructions, user instructions remains, large language models, recent advances, follow user</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite recent advances, evaluating how well large language models (LLMs) follow user instructions remains an open problem. While evaluation methods of language models have seen a rise in prompt-based approaches, limited work on the correctness of these methods has been conducted. In this work, we perform a meta-evaluation of a variety of metrics to quantify how accurately they measure the instruction-following abilities of LLMs. Our investigation is performed on grounded query-based summarization by collecting a new short-form, real-world dataset riSum, containing $300$ document-instruction pairs with $3$ answers each. All $900$ answers are rated by $3$ human annotators. Using riSum, we analyze agreement between evaluation methods and human judgment. Finally, we propose new LLM-based reference-free evaluation methods that improve upon established baselines and perform on-par with costly reference-based metrics which require high-quality summaries.</p>
  </details>
</details>
<details>
  <summary>129. <b>标题：Introducing a Deep Neural Network-based Model Predictive Control  Framework for Rapid Controller Implementation</b></summary>
  <p><b>编号</b>：[428]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08392</p>
  <p><b>作者</b>：David C. Gordon,  Alexander Winkler,  Julian Bedei,  Patrick Schaber,  Jakob Andert,  Charles R. Koch</p>
  <p><b>备注</b>：Submitted to 2024 American Control Conference (ACC), July 8-12, 2024 in Toronto, Canada. ACC is the annual conference of the American Automatic Control Council (AACC), the U.S. national member organization of the International Federation for Automatic Control (IFAC)</p>
  <p><b>关键词</b>：Model Predictive Control, Predictive Control, optimal control solution, Model Predictive, optimal control technique</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Model Predictive Control (MPC) provides an optimal control solution based on a cost function while allowing for the implementation of process constraints. As a model-based optimal control technique, the performance of MPC strongly depends on the model used where a trade-off between model computation time and prediction performance exists. One solution is the integration of MPC with a machine learning (ML) based process model which are quick to evaluate online. This work presents the experimental implementation of a deep neural network (DNN) based nonlinear MPC for Homogeneous Charge Compression Ignition (HCCI) combustion control. The DNN model consists of a Long Short-Term Memory (LSTM) network surrounded by fully connected layers which was trained using experimental engine data and showed acceptable prediction performance with under 5% error for all outputs. Using this model, the MPC is designed to track the Indicated Mean Effective Pressure (IMEP) and combustion phasing trajectories, while minimizing several parameters. Using the acados software package to enable the real-time implementation of the MPC on an ARM Cortex A72, the optimization calculations are completed within 1.4 ms. The external A72 processor is integrated with the prototyping engine controller using a UDP connection allowing for rapid experimental deployment of the NMPC. The IMEP trajectory following of the developed controller was excellent, with a root-mean-square error of 0.133 bar, in addition to observing process constraints.</p>
  </details>
</details>
<details>
  <summary>130. <b>标题：MeanAP-Guided Reinforced Active Learning for Object Detection</b></summary>
  <p><b>编号</b>：[430]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08387</p>
  <p><b>作者</b>：Zhixuan Liang,  Xingyu Zeng,  Rui Zhao,  Ping Luo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：minimal labeled data, Active learning presents, Active learning, Reinforced Active Learning, active object detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Active learning presents a promising avenue for training high-performance models with minimal labeled data, achieved by judiciously selecting the most informative instances to label and incorporating them into the task learner. Despite notable advancements in active learning for image recognition, metrics devised or learned to gauge the information gain of data, crucial for query strategy design, do not consistently align with task model performance metrics, such as Mean Average Precision (MeanAP) in object detection tasks. This paper introduces MeanAP-Guided Reinforced Active Learning for Object Detection (MAGRAL), a novel approach that directly utilizes the MeanAP metric of the task model to devise a sampling strategy employing a reinforcement learning-based sampling agent. Built upon LSTM architecture, the agent efficiently explores and selects subsequent training instances, and optimizes the process through policy gradient with MeanAP serving as reward. Recognizing the time-intensive nature of MeanAP computation at each step, we propose fast look-up tables to expedite agent training. We assess MAGRAL's efficacy across popular benchmarks, PASCAL VOC and MS COCO, utilizing different backbone architectures. Empirical findings substantiate MAGRAL's superiority over recent state-of-the-art methods, showcasing substantial performance gains. MAGRAL establishes a robust baseline for reinforced active object detection, signifying its potential in advancing the field.</p>
  </details>
</details>
<details>
  <summary>131. <b>标题：AutoVP: An Automated Visual Prompting Framework and Benchmark</b></summary>
  <p><b>编号</b>：[433]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08381</p>
  <p><b>作者</b>：Hsi-Ai Tsao,  Lei Hsiung,  Pin-Yu Chen,  Sijia Liu,  Tsung-Yi Ho</p>
  <p><b>备注</b>：Preprint. The code is available at this https URL</p>
  <p><b>关键词</b>：emerging parameter-efficient fine-tuning, parameter-efficient fine-tuning approach, adapting pre-trained vision, downstream image-classification tasks, Visual prompting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Visual prompting (VP) is an emerging parameter-efficient fine-tuning approach to adapting pre-trained vision models to solve various downstream image-classification tasks. However, there has hitherto been little systematic study of the design space of VP and no clear benchmark for evaluating its performance. To bridge this gap, we propose AutoVP, an end-to-end expandable framework for automating VP design choices, along with 12 downstream image-classification tasks that can serve as a holistic VP-performance benchmark. Our design space covers 1) the joint optimization of the prompts; 2) the selection of pre-trained models, including image classifiers and text-image encoders; and 3) model output mapping strategies, including nonparametric and trainable label mapping. Our extensive experimental results show that AutoVP outperforms the best-known current VP methods by a substantial margin, having up to 6.7% improvement in accuracy; and attains a maximum performance increase of 27.5% compared to linear-probing (LP) baseline. AutoVP thus makes a two-fold contribution: serving both as an efficient tool for hyperparameter tuning on VP design choices, and as a comprehensive benchmark that can reasonably be expected to accelerate VP's development. The source code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>132. <b>标题：MCU: A Task-centric Framework for Open-ended Agent Evaluation in  Minecraft</b></summary>
  <p><b>编号</b>：[441]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08367</p>
  <p><b>作者</b>：Haowei Lin,  Zihao Wang,  Jianzhu Ma,  Yitao Liang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：framework named MCU, task-centric framework named, MCU framework, open-ended game environment, MCU</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To pursue the goal of creating an open-ended agent in Minecraft, an open-ended game environment with unlimited possibilities, this paper introduces a task-centric framework named MCU for Minecraft agent evaluation. The MCU framework leverages the concept of atom tasks as fundamental building blocks, enabling the generation of diverse or even arbitrary tasks. Within the MCU framework, each task is measured with six distinct difficulty scores (time consumption, operational effort, planning complexity, intricacy, creativity, novelty). These scores offer a multi-dimensional assessment of a task from different angles, and thus can reveal an agent's capability on specific facets. The difficulty scores also serve as the feature of each task, which creates a meaningful task space and unveils the relationship between tasks. For efficient evaluation of Minecraft agents employing the MCU framework, we maintain a unified benchmark, namely SkillForge, which comprises representative tasks with diverse categories and difficulty distribution. We also provide convenient filters for users to select tasks to assess specific capabilities of agents. We show that MCU has the high expressivity to cover all tasks used in recent literature on Minecraft agent, and underscores the need for advancements in areas such as creativity, precise control, and out-of-distribution generalization under the goal of open-ended Minecraft agent development.</p>
  </details>
</details>
<h1>人工智能</h1>
<details>
  <summary>1. <b>标题：Octopus: Embodied Vision-Language Programmer from Environmental Feedback</b></summary>
  <p><b>编号</b>：[1]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08588</p>
  <p><b>作者</b>：Jingkang Yang,  Yuhao Dong,  Shuai Liu,  Bo Li,  Ziyue Wang,  Chencheng Jiang,  Haoran Tan,  Jiamu Kang,  Yuanhan Zhang,  Kaiyang Zhou,  Ziwei Liu</p>
  <p><b>备注</b>：Project Page: this https URL, Codebase: this https URL</p>
  <p><b>关键词</b>：achieved substantial progress, Large vision-language models, Large vision-language, perception and reasoning, achieved substantial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large vision-language models (VLMs) have achieved substantial progress in multimodal perception and reasoning. Furthermore, when seamlessly integrated into an embodied agent, it signifies a crucial stride towards the creation of autonomous and context-aware systems capable of formulating plans and executing commands with precision. In this paper, we introduce Octopus, a novel VLM designed to proficiently decipher an agent's vision and textual task objectives and to formulate intricate action sequences and generate executable code. Our design allows the agent to adeptly handle a wide spectrum of tasks, ranging from mundane daily chores in simulators to sophisticated interactions in complex video games. Octopus is trained by leveraging GPT-4 to control an explorative agent to generate training data, i.e., action blueprints and the corresponding executable code, within our experimental environment called OctoVerse. We also collect the feedback that allows the enhanced training scheme of Reinforcement Learning with Environmental Feedback (RLEF). Through a series of experiments, we illuminate Octopus's functionality and present compelling results, and the proposed RLEF turns out to refine the agent's decision-making. By open-sourcing our model architecture, simulator, and dataset, we aspire to ignite further innovation and foster collaborative applications within the broader embodied AI community.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Tree-Planner: Efficient Close-loop Task Planning with Large Language  Models</b></summary>
  <p><b>编号</b>：[7]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08582</p>
  <p><b>作者</b>：Mengkang Hu,  Yao Mu,  Xinmiao Yu,  Mingyu Ding,  Shiguang Wu,  Wenqi Shao,  Qiguang Chen,  Bin Wang,  Yu Qiao,  Ping Luo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：paper studies close-loop, studies close-loop task, Large Language Models, prompting Large Language, sequence of skills</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper studies close-loop task planning, which refers to the process of generating a sequence of skills (a plan) to accomplish a specific goal while adapting the plan based on real-time observations. Recently, prompting Large Language Models (LLMs) to generate actions iteratively has become a prevalent paradigm due to its superior performance and user-friendliness. However, this paradigm is plagued by two inefficiencies: high token consumption and redundant error correction, both of which hinder its scalability for large-scale testing and applications. To address these issues, we propose Tree-Planner, which reframes task planning with LLMs into three distinct phases: plan sampling, action tree construction, and grounded deciding. Tree-Planner starts by using an LLM to sample a set of potential plans before execution, followed by the aggregation of them to form an action tree. Finally, the LLM performs a top-down decision-making process on the tree, taking into account real-time environmental information. Experiments show that Tree-Planner achieves state-of-the-art performance while maintaining high efficiency. By decomposing LLM queries into a single plan-sampling call and multiple grounded-deciding calls, a considerable part of the prompt are less likely to be repeatedly consumed. As a result, token consumption is reduced by 92.2% compared to the previously best-performing model. Additionally, by enabling backtracking on the action tree as needed, the correction process becomes more flexible, leading to a 40.5% decrease in error corrections. Project page: this https URL</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Jigsaw: Supporting Designers in Prototyping Multimodal Applications by  Assembling AI Foundation Models</b></summary>
  <p><b>编号</b>：[13]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08574</p>
  <p><b>作者</b>：David Chuan-En Lin,  Nikolas Martelaro</p>
  <p><b>备注</b>：Webpage: this https URL</p>
  <p><b>关键词</b>：generating visual prototypes, including ideating design, ideating design concepts, Recent advancements, including ideating</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advancements in AI foundation models have made it possible for them to be utilized off-the-shelf for creative tasks, including ideating design concepts or generating visual prototypes. However, integrating these models into the creative process can be challenging as they often exist as standalone applications tailored to specific tasks. To address this challenge, we introduce Jigsaw, a prototype system that employs puzzle pieces as metaphors to represent foundation models. Jigsaw allows designers to combine different foundation model capabilities across various modalities by assembling compatible puzzle pieces. To inform the design of Jigsaw, we interviewed ten designers and distilled design goals. In a user study, we showed that Jigsaw enhanced designers' understanding of available foundation model capabilities, provided guidance on combining capabilities across different modalities and tasks, and served as a canvas to support design exploration, prototyping, and documentation.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：A Lightweight Calibrated Simulation Enabling Efficient Offline Learning  for Optimal Control of Real Buildings</b></summary>
  <p><b>编号</b>：[16]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08569</p>
  <p><b>作者</b>：Judah Goldfeder,  John Sipple</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Modern commercial Heating, Air Conditioning, commercial Heating, interconnected thermodynamic system, current setpoint control</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Modern commercial Heating, Ventilation, and Air Conditioning (HVAC) devices form a complex and interconnected thermodynamic system with the building and outside weather conditions, and current setpoint control policies are not fully optimized for minimizing energy use and carbon emission. Given a suitable training environment, a Reinforcement Learning (RL) model is able to improve upon these policies, but training such a model, especially in a way that scales to thousands of buildings, presents many real world challenges. We propose a novel simulation-based approach, where a customized simulator is used to train the agent for each building. Our open-source simulator (available online: this https URL) is lightweight and calibrated via telemetry from the building to reach a higher level of fidelity. On a two-story, 68,000 square foot building, with 127 devices, we were able to calibrate our simulator to have just over half a degree of drift from the real world over a six-hour interval. This approach is an important step toward having a real-world RL control system that can be scaled to many buildings, allowing for greater efficiency and resulting in reduced energy consumption and carbon emissions.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Transformers as Decision Makers: Provable In-Context Reinforcement  Learning via Supervised Pretraining</b></summary>
  <p><b>编号</b>：[18]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08566</p>
  <p><b>作者</b>：Licong Lin,  Yu Bai,  Song Mei</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：demonstrated remarkable in-context, remarkable in-context reinforcement, make good decisions, unseen environments, datasets have demonstrated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large transformer models pretrained on offline reinforcement learning datasets have demonstrated remarkable in-context reinforcement learning (ICRL) capabilities, where they can make good decisions when prompted with interaction trajectories from unseen environments. However, when and how transformers can be trained to perform ICRL have not been theoretically well-understood. In particular, it is unclear which reinforcement-learning algorithms transformers can perform in context, and how distribution mismatch in offline training data affects the learned algorithms. This paper provides a theoretical framework that analyzes supervised pretraining for ICRL. This includes two recently proposed training methods -- algorithm distillation and decision-pretrained transformers. First, assuming model realizability, we prove the supervised-pretrained transformer will imitate the conditional expectation of the expert algorithm given the observed trajectory. The generalization error will scale with model capacity and a distribution divergence factor between the expert and offline algorithms. Second, we show transformers with ReLU attention can efficiently approximate near-optimal online reinforcement learning algorithms like LinUCB and Thompson sampling for stochastic linear bandits, and UCB-VI for tabular Markov decision processes. This provides the first quantitative analysis of the ICRL capabilities of transformers pretrained from offline trajectories.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Security Considerations in AI-Robotics: A Survey of Current Methods,  Challenges, and Opportunities</b></summary>
  <p><b>编号</b>：[19]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08565</p>
  <p><b>作者</b>：Subash Neupane,  Shaswata Mitra,  Ivan A. Fernandez,  Swayamjit Saha,  Sudip Mittal,  Jingdao Chen,  Nisha Pillai,  Shahram Rahimi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Artificial Intelligence, AI-Robotics systems, inextricably intertwined, Robotics and Artificial, systems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Robotics and Artificial Intelligence (AI) have been inextricably intertwined since their inception. Today, AI-Robotics systems have become an integral part of our daily lives, from robotic vacuum cleaners to semi-autonomous cars. These systems are built upon three fundamental architectural elements: perception, navigation and planning, and control. However, while the integration of AI-Robotics systems has enhanced the quality our lives, it has also presented a serious problem - these systems are vulnerable to security attacks. The physical components, algorithms, and data that make up AI-Robotics systems can be exploited by malicious actors, potentially leading to dire consequences. Motivated by the need to address the security concerns in AI-Robotics systems, this paper presents a comprehensive survey and taxonomy across three dimensions: attack surfaces, ethical and legal concerns, and Human-Robot Interaction (HRI) security. Our goal is to provide users, developers and other stakeholders with a holistic understanding of these areas to enhance the overall AI-Robotics system security. We begin by surveying potential attack surfaces and provide mitigating defensive strategies. We then delve into ethical issues, such as dependency and psychological impact, as well as the legal concerns regarding accountability for these systems. Besides, emerging trends such as HRI are discussed, considering privacy, integrity, safety, trustworthiness, and explainability concerns. Finally, we present our vision for future research directions in this dynamic and promising field.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：MemGPT: Towards LLMs as Operating Systems</b></summary>
  <p><b>编号</b>：[20]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08560</p>
  <p><b>作者</b>：Charles Packer,  Vivian Fang,  Shishir G. Patil,  Kevin Lin,  Sarah Wooders,  Joseph E. Gonzalez</p>
  <p><b>备注</b>：Code and data available at this https URL</p>
  <p><b>关键词</b>：limited context windows, Large language models, limited context, language models, hindering their utility</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at this https URL.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of  Language Models with Hypothesis Refinement</b></summary>
  <p><b>编号</b>：[21]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08559</p>
  <p><b>作者</b>：Linlu Qiu,  Liwei Jiang,  Ximing Lu,  Melanie Sclar,  Valentina Pyatkin,  Chandra Bhagavatula,  Bailin Wang,  Yoon Kim,  Yejin Choi,  Nouha Dziri,  Xiang Ren</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：derive underlying principles, inductive reasoning, ability to derive, derive underlying, underlying principles</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The ability to derive underlying principles from a handful of observations and then generalize to novel situations -- known as inductive reasoning -- is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approach achieves strong results across inductive reasoning benchmarks that require inducing causal relations, language-like instructions, and symbolic concepts. However, they also behave as puzzling inductive reasoners, showing notable performance gaps in rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules. Through empirical and human analyses, we further reveal several discrepancies between the inductive reasoning processes of LMs and humans, shedding light on both the potentials and limitations of using LMs in inductive reasoning tasks.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Offline Retraining for Online RL: Decoupled Policy Learning to Mitigate  Exploration Bias</b></summary>
  <p><b>编号</b>：[22]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08558</p>
  <p><b>作者</b>：Max Sobol Mark,  Archit Sharma,  Fahim Tajwar,  Rafael Rafailov,  Sergey Levine,  Chelsea Finn</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：state coverage, optimistically explore, policy, online, offline</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>It is desirable for policies to optimistically explore new states and behaviors during online reinforcement learning (RL) or fine-tuning, especially when prior offline data does not provide enough state coverage. However, exploration bonuses can bias the learned policy, and our experiments find that naive, yet standard use of such bonuses can fail to recover a performant policy. Concurrently, pessimistic training in offline RL has enabled recovery of performant policies from static datasets. Can we leverage offline RL to recover better policies from online interaction? We make a simple observation that a policy can be trained from scratch on all interaction data with pessimistic objectives, thereby decoupling the policies used for data collection and for evaluation. Specifically, we propose offline retraining, a policy extraction step at the end of online fine-tuning in our Offline-to-Online-to-Offline (OOO) framework for reinforcement learning (RL). An optimistic (exploration) policy is used to interact with the environment, and a separate pessimistic (exploitation) policy is trained on all the observed data for evaluation. Such decoupling can reduce any bias from online interaction (intrinsic rewards, primacy bias) in the evaluation policy, and can allow more exploratory behaviors during online interaction which in turn can generate better data for exploitation. OOO is complementary to several offline-to-online RL and online RL methods, and improves their average performance by 14% to 26% in our fine-tuning experiments, achieves state-of-the-art performance on several environments in the D4RL benchmarks, and improves online RL performance by 165% on two OpenAI gym environments. Further, OOO can enable fine-tuning from incomplete offline datasets where prior methods can fail to recover a performant policy. Implementation: this https URL</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Cross-Episodic Curriculum for Transformer Agents</b></summary>
  <p><b>编号</b>：[24]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08549</p>
  <p><b>作者</b>：Lucy Xiaoyang Shi,  Yunfan Jiang,  Jake Grigsby,  Linxi "Jim" Fan,  Yuke Zhu</p>
  <p><b>备注</b>：To appear in NeurIPS 2023; The first two authors contributed equally</p>
  <p><b>关键词</b>：CEC, Transformer, Transformer agent learning, Curriculum, curriculum captures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a new algorithm, Cross-Episodic Curriculum (CEC), to boost the learning efficiency and generalization of Transformer agents. Central to CEC is the placement of cross-episodic experiences into a Transformer's context, which forms the basis of a curriculum. By sequentially structuring online learning trials and mixed-quality demonstrations, CEC constructs curricula that encapsulate learning progression and proficiency increase across episodes. Such synergy combined with the potent pattern recognition capabilities of Transformer models delivers a powerful cross-episodic attention mechanism. The effectiveness of CEC is demonstrated under two representative scenarios: one involving multi-task reinforcement learning with discrete control, such as in DeepMind Lab, where the curriculum captures the learning progression in both individual and progressively complex settings; and the other involving imitation learning with mixed-quality data for continuous control, as seen in RoboMimic, where the curriculum captures the improvement in demonstrators' expertise. In all instances, policies resulting from CEC exhibit superior performance and strong generalization. Code is open-sourced at this https URL to facilitate research on Transformer agent learning.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Do pretrained Transformers Really Learn In-context by Gradient Descent?</b></summary>
  <p><b>编号</b>：[28]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08540</p>
  <p><b>作者</b>：Lingfeng Shen,  Aayush Mishra,  Daniel Khashabi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：In-Context Learning, ICL, implicitly equivalent, Gradient Descent, language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Is In-Context Learning (ICL) implicitly equivalent to Gradient Descent (GD)? Several recent works draw analogies between the dynamics of GD and the emergent behavior of ICL in large language models. However, these works make assumptions far from the realistic natural language setting in which language models are trained. Such discrepancies between theory and practice, therefore, necessitate further investigation to validate their applicability.
We start by highlighting the weaknesses in prior works that construct Transformer weights to simulate gradient descent. Their experiments with training Transformers on ICL objective, inconsistencies in the order sensitivity of ICL and GD, sparsity of the constructed weights, and sensitivity to parameter changes are some examples of a mismatch from the real-world setting.
Furthermore, we probe and compare the ICL vs. GD hypothesis in a natural setting. We conduct comprehensive empirical analyses on language models pretrained on natural data (LLaMa-7B). Our comparisons on various performance metrics highlight the inconsistent behavior of ICL and GD as a function of various factors such as datasets, models, and number of demonstrations. We observe that ICL and GD adapt the output distribution of language models differently. These results indicate that the equivalence between ICL and GD is an open hypothesis, requires nuanced considerations and calls for further studies.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Formally Specifying the High-Level Behavior of LLM-Based Agents</b></summary>
  <p><b>编号</b>：[31]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08535</p>
  <p><b>作者</b>：Maxwell Crouse,  Ibrahim Abdelaziz,  Kinjal Basu,  Soham Dan,  Sadhana Kumaravel,  Achille Fokoue,  Pavan Kapanipathi,  Luis Lastras</p>
  <p><b>备注</b>：Preprint under review</p>
  <p><b>关键词</b>：solving challenging problems, task-specific finetuned models, LLM-based agents, expensive to procure, Linear Temporal Logic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>LLM-based agents have recently emerged as promising tools for solving challenging problems without the need for task-specific finetuned models that can be expensive to procure. Currently, the design and implementation of such agents is ad hoc, as the wide variety of tasks that LLM-based agents may be applied to naturally means there can be no one-size-fits-all approach to agent design. In this work we aim to alleviate the difficulty of designing and implementing new agents by proposing a minimalistic, high-level generation framework that simplifies the process of building agents. The framework we introduce allows the user to specify desired agent behaviors in Linear Temporal Logic (LTL). The declarative LTL specification is then used to construct a constrained decoder that guarantees the LLM will produce an output exhibiting the desired behavior. By designing our framework in this way, we obtain several benefits, including the ability to enforce complex agent behavior, the ability to formally validate prompt examples, and the ability to seamlessly incorporate content-focused logical constraints into generation. In particular, our declarative approach, in which the desired behavior is simply described without concern for how it should be implemented or enforced, enables rapid design, implementation and experimentation with different LLM-based agents. We demonstrate how the proposed framework can be used to implement recent LLM-based agents, and show how the guardrails our approach provides can lead to improvements in agent performance. In addition, we release our code for general use.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：How connectivity structure shapes rich and lazy learning in neural  circuits</b></summary>
  <p><b>编号</b>：[39]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08513</p>
  <p><b>作者</b>：Yuhan Helena Liu,  Aristide Baratin,  Jonathan Cornford,  Stefan Mihalas,  Eric Shea-Brown,  Guillaume Lajoie</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：recent work leverages, work leverages deep, attributes critically influence, network attributes critically, leverages deep learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In theoretical neuroscience, recent work leverages deep learning tools to explore how some network attributes critically influence its learning dynamics. Notably, initial weight distributions with small (resp. large) variance may yield a rich (resp. lazy) regime, where significant (resp. minor) changes to network states and representation are observed over the course of learning. However, in biology, neural circuit connectivity generally has a low-rank structure and therefore differs markedly from the random initializations generally used for these studies. As such, here we investigate how the structure of the initial weights, in particular their effective rank, influences the network learning regime. Through both empirical and theoretical analyses, we discover that high-rank initializations typically yield smaller network changes indicative of lazier learning, a finding we also confirm with experimentally-driven initial connectivity in recurrent neural networks. Conversely, low-rank initialization biases learning towards richer learning. Importantly, however, as an exception to this rule, we find lazier learning can still occur with a low-rank initialization that aligns with task and data statistics. Our research highlights the pivotal role of initial weight structures in shaping learning regimes, with implications for metabolic costs of plasticity and risks of catastrophic forgetting.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：HoneyBee: Progressive Instruction Finetuning of Large Language Models  for Materials Science</b></summary>
  <p><b>编号</b>：[40]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08511</p>
  <p><b>作者</b>：Yu Song,  Santiago Miret,  Huan Zhang,  Bang Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：materials science, trustworthy data curation, LLaMa-based language model, propose an instruction-based, instruction-based process</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose an instruction-based process for trustworthy data curation in materials science (MatSci-Instruct), which we then apply to finetune a LLaMa-based language model targeted for materials science (HoneyBee). MatSci-Instruct helps alleviate the scarcity of relevant, high-quality materials science textual data available in the open literature, and HoneyBee is the first billion-parameter language model specialized to materials science. In MatSci-Instruct we improve the trustworthiness of generated data by prompting multiple commercially available large language models for generation with an Instructor module (e.g. Chat-GPT) and verification from an independent Verifier module (e.g. Claude). Using MatSci-Instruct, we construct a dataset of multiple tasks and measure the quality of our dataset along multiple dimensions, including accuracy against known facts, relevance to materials science, as well as completeness and reasonableness of the data. Moreover, we iteratively generate more targeted instructions and instruction-data in a finetuning-evaluation-feedback loop leading to progressively better performance for our finetuned HoneyBee models. Our evaluation on the MatSci-NLP benchmark shows HoneyBee's outperformance of existing language models on materials science tasks and iterative improvement in successive stages of instruction-data refinement. We study the quality of HoneyBee's language modeling through automatic evaluation and analyze case studies to further understand the model's capabilities and limitations. Our code and relevant datasets are publicly available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Impact of time and note duration tokenizations on deep learning symbolic  music modeling</b></summary>
  <p><b>编号</b>：[43]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08497</p>
  <p><b>作者</b>：Nathan Fradet,  Nicolas Gutowski,  Fabien Chhel,  Jean-Pierre Briot</p>
  <p><b>备注</b>：ISMIR 2023</p>
  <p><b>关键词</b>：Music Information Retrieval, Information Retrieval, MIR, Retrieval, Symbolic music</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Symbolic music is widely used in various deep learning tasks, including generation, transcription, synthesis, and Music Information Retrieval (MIR). It is mostly employed with discrete models like Transformers, which require music to be tokenized, i.e., formatted into sequences of distinct elements called tokens. Tokenization can be performed in different ways. As Transformer can struggle at reasoning, but capture more easily explicit information, it is important to study how the way the information is represented for such model impact their performances. In this work, we analyze the common tokenization methods and experiment with time and note duration representations. We compare the performances of these two impactful criteria on several tasks, including composer and emotion classification, music generation, and sequence representation learning. We demonstrate that explicit information leads to better results depending on the task.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Can We Edit Multimodal Large Language Models?</b></summary>
  <p><b>编号</b>：[50]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08475</p>
  <p><b>作者</b>：Siyuan Cheng,  Bozhong Tian,  Qingbin Liu,  Xi Chen,  Yongheng Wang,  Huajun Chen,  Ningyu Zhang</p>
  <p><b>备注</b>：EMNLP 2023</p>
  <p><b>关键词</b>：Large Language Models, Multimodal Large Language, Large Language, editing Multimodal Large, Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in this https URL.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：DistillSpec: Improving Speculative Decoding via Knowledge Distillation</b></summary>
  <p><b>编号</b>：[55]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08461</p>
  <p><b>作者</b>：Yongchao Zhou,  Kaifeng Lyu,  Ankit Singh Rawat,  Aditya Krishna Menon,  Afshin Rostamizadeh,  Sanjiv Kumar,  Jean-François Kagy,  Rishabh Agarwal</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：accelerates large language, generating multiple tokens, large language model, language model inference, target model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Speculative decoding (SD) accelerates large language model inference by employing a faster draft model for generating multiple tokens, which are then verified in parallel by the larger target model, resulting in the text generated according to the target model distribution. However, identifying a compact draft model that is well-aligned with the target model is challenging. To tackle this issue, we propose DistillSpec that uses knowledge distillation to better align the draft model with the target model, before applying SD. DistillSpec makes two key design choices, which we demonstrate via systematic study to be crucial to improving the draft and target alignment: utilizing on-policy data generation from the draft model, and tailoring the divergence function to the task and decoding strategy. Notably, DistillSpec yields impressive 10 - 45% speedups over standard SD on a range of standard benchmarks, using both greedy and non-greedy sampling. Furthermore, we combine DistillSpec with lossy SD to achieve fine-grained control over the latency vs. task performance trade-off. Finally, in practical scenarios with models of varying sizes, first using distillation to boost the performance of the target model and then applying DistillSpec to train a well-aligned draft model can reduce decoding latency by 6-10x with minimal performance drop, compared to standard decoding without distillation.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：A Survey on Heterogeneous Transfer Learning</b></summary>
  <p><b>编号</b>：[56]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08459</p>
  <p><b>作者</b>：Runxue Bao,  Yiming Sun,  Yuhe Gao,  Jindong Wang,  Qiang Yang,  Haifeng Chen,  Zhi-Hong Mao,  Xing Xie,  Ye Ye</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：enhance model performance, transfer learning, underpinning many real-world, approach utilizing knowledge, enhance model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The application of transfer learning, an approach utilizing knowledge from a source domain to enhance model performance in a target domain, has seen a tremendous rise in recent years, underpinning many real-world scenarios. The key to its success lies in the shared common knowledge between the domains, a prerequisite in most transfer learning methodologies. These methods typically presuppose identical feature spaces and label spaces in both domains, known as homogeneous transfer learning, which, however, is not always a practical assumption. Oftentimes, the source and target domains vary in feature spaces, data distributions, and label spaces, making it challenging or costly to secure source domain data with identical feature and label spaces as the target domain. Arbitrary elimination of these differences is not always feasible or optimal. Thus, heterogeneous transfer learning, acknowledging and dealing with such disparities, has emerged as a promising approach for a variety of tasks. Despite the existence of a survey in 2017 on this topic, the fast-paced advances post-2017 necessitate an updated, in-depth review. We therefore present a comprehensive survey of recent developments in heterogeneous transfer learning methods, offering a systematic guide for future research. Our paper reviews methodologies for diverse learning scenarios, discusses the limitations of current studies, and covers various application contexts, including Natural Language Processing, Computer Vision, Multimodality, and Biomedicine, to foster a deeper understanding and spur future research.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Metrics for popularity bias in dynamic recommender systems</b></summary>
  <p><b>编号</b>：[57]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08455</p>
  <p><b>作者</b>：Valentijn Braun,  Debarati Bhaumik,  Diptish Dey</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Albeit the widespread, daily lives, limited research, recommender systems, systems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Albeit the widespread application of recommender systems (RecSys) in our daily lives, rather limited research has been done on quantifying unfairness and biases present in such systems. Prior work largely focuses on determining whether a RecSys is discriminating or not but does not compute the amount of bias present in these systems. Biased recommendations may lead to decisions that can potentially have adverse effects on individuals, sensitive user groups, and society. Hence, it is important to quantify these biases for fair and safe commercial applications of these systems. This paper focuses on quantifying popularity bias that stems directly from the output of RecSys models, leading to over recommendation of popular items that are likely to be misaligned with user preferences. Four metrics to quantify popularity bias in RescSys over time in dynamic setting across different sensitive user groups have been proposed. These metrics have been demonstrated for four collaborative filtering based RecSys algorithms trained on two commonly used benchmark datasets in the literature. Results obtained show that the metrics proposed provide a comprehensive understanding of growing disparities in treatment between sensitive groups over time when used conjointly.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Towards Robust Multi-Modal Reasoning via Model Selection</b></summary>
  <p><b>编号</b>：[63]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08446</p>
  <p><b>作者</b>：Xiangyan Liu,  Rongxue Li,  Wei Ji,  Tao Lin</p>
  <p><b>备注</b>：10 pages, 5 figures</p>
  <p><b>关键词</b>：Large Language Model, Large Language, Language Model, model selection, recent research</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The reasoning capabilities of LLM (Large Language Model) are widely acknowledged in recent research, inspiring studies on tool learning and autonomous agents. LLM serves as the "brain" of agent, orchestrating multiple tools for collaborative multi-step task solving. Unlike methods invoking tools like calculators or weather APIs for straightforward tasks, multi-modal agents excel by integrating diverse AI models for complex challenges. However, current multi-modal agents neglect the significance of model selection: they primarily focus on the planning and execution phases, and will only invoke predefined task-specific models for each subtask, making the execution fragile. Meanwhile, other traditional model selection methods are either incompatible with or suboptimal for the multi-modal agent scenarios, due to ignorance of dependencies among subtasks arising by multi-step reasoning.
To this end, we identify the key challenges therein and propose the $\textit{M}^3$ framework as a plug-in with negligible runtime overhead at test-time. This framework improves model selection and bolsters the robustness of multi-modal agents in multi-step reasoning. In the absence of suitable benchmarks, we create MS-GQA, a new dataset specifically designed to investigate the model selection challenge in multi-modal agents. Our experiments reveal that our framework enables dynamic model selection, considering both user inputs and subtask dependencies, thereby robustifying the overall reasoning process. Our code and benchmark: this https URL.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Debias the Training of Diffusion Models</b></summary>
  <p><b>编号</b>：[66]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08442</p>
  <p><b>作者</b>：Hu Yu,  Li Shen,  Jie Huang,  Man Zhou,  Hongsheng Li,  Feng Zhao</p>
  <p><b>备注</b>：University of Science and Technology of China, Alibaba Group, The Chinese University of Hong Kong</p>
  <p><b>关键词</b>：demonstrated compelling generation, variational lower bound, score matching loss, simple denoising score, denoising score matching</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diffusion models have demonstrated compelling generation quality by optimizing the variational lower bound through a simple denoising score matching loss. In this paper, we provide theoretical evidence that the prevailing practice of using a constant loss weight strategy in diffusion models leads to biased estimation during the training phase. Simply optimizing the denoising network to predict Gaussian noise with constant weighting may hinder precise estimations of original images. To address the issue, we propose an elegant and effective weighting strategy grounded in the theoretically unbiased principle. Moreover, we conduct a comprehensive and systematic exploration to dissect the inherent bias problem deriving from constant weighting loss from the perspectives of its existence, impact and reasons. These analyses are expected to advance our understanding and demystify the inner workings of diffusion models. Through empirical evaluation, we demonstrate that our proposed debiased estimation method significantly enhances sample quality without the reliance on complex techniques, and exhibits improved efficiency compared to the baseline method both in training and sampling processes.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Neural Sampling in Hierarchical Exponential-family Energy-based Models</b></summary>
  <p><b>编号</b>：[70]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08431</p>
  <p><b>作者</b>：Xingsi Dong,  Si Wu</p>
  <p><b>备注</b>：NeurIPS 2023</p>
  <p><b>关键词</b>：Bayesian brain theory, brain theory suggests, brain employs generative, employs generative models, theory suggests</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Bayesian brain theory suggests that the brain employs generative models to understand the external world. The sampling-based perspective posits that the brain infers the posterior distribution through samples of stochastic neuronal responses. Additionally, the brain continually updates its generative model to approach the true distribution of the external world. In this study, we introduce the Hierarchical Exponential-family Energy-based (HEE) model, which captures the dynamics of inference and learning. In the HEE model, we decompose the partition function into individual layers and leverage a group of neurons with shorter time constants to sample the gradient of the decomposed normalization term. This allows our model to estimate the partition function and perform inference simultaneously, circumventing the negative phase encountered in conventional energy-based models (EBMs). As a result, the learning process is localized both in time and space, and the model is easy to converge. To match the brain's rapid computation, we demonstrate that neural adaptation can serve as a momentum term, significantly accelerating the inference process. On natural image datasets, our model exhibits representations akin to those observed in the biological visual system. Furthermore, for the machine learning community, our model can generate observations through joint or marginal generation. We show that marginal generation outperforms joint generation and achieves performance on par with other EBMs.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题："SegLoc": Study on Novel Visual Self-supervised Learning Scheme (Segment  Localization) Tailored for Dense Prediction Tasks of Security Inspection  X-ray Images</b></summary>
  <p><b>编号</b>：[74]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08421</p>
  <p><b>作者</b>：Shervin Halat,  Mohammad Rahmati,  Ehsan Nazerfard</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：self-supervised learning scheme, SSL models, remarkable advancements, advancements of artificial, artificial intelligence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Lately, remarkable advancements of artificial intelligence have been attributed to the integration of self-supervised learning scheme. Despite impressive achievements within NLP, yet SSL in computer vision has not been able to stay on track comparatively. Recently, integration of contrastive learning on top of existing SSL models has established considerable progress in computer vision through which visual SSL models have outperformed their supervised counterparts. Nevertheless, most of these improvements were limited to classification tasks, and also, few works have been dedicated to evaluation of SSL models in real-world scenarios of computer vision, while the majority of works are centered around datasets containing class-wise portrait images, most notably, ImageNet. Consequently, in this work, we have considered dense prediction task of semantic segmentation in security inspection x-ray images to evaluate our proposed model Segmentation Localization. Based upon the model Instance Localization, our model SegLoc has managed to address one of the most challenging downsides of contrastive learning, i.e., false negative pairs of query embeddings. In order to do so, in contrast to baseline model InsLoc, our pretraining dataset is synthesized by cropping, transforming, then pasting already labeled segments from an available labeled dataset, foregrounds, onto instances of an unlabeled dataset, backgrounds. In our case, PIDray and SIXray datasets are considered as labeled and unlabeled datasets, respectively. Moreover, we fully harness labels by avoiding false negative pairs through implementing the idea, one queue per class, in MoCo-v2 whereby negative pairs corresponding to each query are extracted from its corresponding queue within the memory bank. Our approach has outperformed random initialization by 3% to 6%, while having underperformed supervised initialization.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Jailbreaking Black Box Large Language Models in Twenty Queries</b></summary>
  <p><b>编号</b>：[76]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08419</p>
  <p><b>作者</b>：Patrick Chao,  Alexander Robey,  Edgar Dobriban,  Hamed Hassani,  George J. Pappas,  Eric Wong</p>
  <p><b>备注</b>：21 pages, 10 figures</p>
  <p><b>关键词</b>：large language models, growing interest, interest in ensuring, ensuring that large, large language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM-2.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Tightening Bounds on Probabilities of Causation By Merging Datasets</b></summary>
  <p><b>编号</b>：[80]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08406</p>
  <p><b>作者</b>：Numair Sani,  Atalanti A. Mastakouri</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Probabilities of Causation, play a fundamental, decision-making in law, health care, bounds</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Probabilities of Causation (PoC) play a fundamental role in decision-making in law, health care and public policy. Nevertheless, their point identification is challenging, requiring strong assumptions, in the absence of which only bounds can be derived. Existing work to further tighten these bounds by leveraging extra information either provides numerical bounds, symbolic bounds for fixed dimensionality, or requires access to multiple datasets that contain the same treatment and outcome variables. However, in many clinical, epidemiological and public policy applications, there exist external datasets that examine the effect of different treatments on the same outcome variable, or study the association between covariates and the outcome variable. These external datasets cannot be used in conjunction with the aforementioned bounds, since the former may entail different treatment assignment mechanisms, or even obey different causal structures. Here, we provide symbolic bounds on the PoC for this challenging scenario. We focus on combining either two randomized experiments studying different treatments, or a randomized experiment and an observational study, assuming causal sufficiency. Our symbolic bounds work for arbitrary dimensionality of covariates and treatment, and we discuss the conditions under which these bounds are tighter than existing bounds in literature. Finally, our bounds parameterize the difference in treatment assignment mechanism across datasets, allowing the mechanisms to vary across datasets while still allowing causal information to be transferred from the external dataset to the target dataset.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Performance/power assessment of CNN packages on embedded automotive  platforms</b></summary>
  <p><b>编号</b>：[82]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08401</p>
  <p><b>作者</b>：Paolo Burgio,  Gianluca Brilli</p>
  <p><b>备注</b>：14 pages; 17 figures, 10 tables</p>
  <p><b>关键词</b>：highly-parallel accelerators opens, power-efficient embedded computers, embedded computers based, edge computers based, rise of power-efficient</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rise of power-efficient embedded computers based on highly-parallel accelerators opens a number of opportunities and challenges for researchers and engineers, and paved the way to the era of edge computing. At the same time, advances in embedded AI for object detection and categorization such as YOLO, GoogleNet and AlexNet reached an unprecedented level of accuracy (mean-Average Precision - mAP) and performance (Frames-Per-Second - FPS). Today, edge computers based on heterogeneous many-core systems are a predominant choice to deploy such systems in industry 4.0, wearable devices, and - our focus - autonomous driving systems. In these latter systems, engineers struggle to make reduced automotive power and size budgets co-exist with the accuracy and performance targets requested by autonomous driving. We aim at validating the effectiveness and efficiency of most recent networks on state-of-the-art platforms with embedded commercial-off-the-shelf System-on-Chips, such as Xavier AGX, Tegra X2 and Nano for NVIDIA and XCZU9EG and XCZU3EG of the Zynq UltraScale+ family, for the Xilinx counterpart. Our work aims at supporting engineers in choosing the most appropriate CNN package and computing system for their designs, and deriving guidelines for adequately sizing their systems.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Prompting Large Language Models with Chain-of-Thought for Few-Shot  Knowledge Base Question Generation</b></summary>
  <p><b>编号</b>：[85]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08395</p>
  <p><b>作者</b>：Yuanyuan Liang,  Jianing Wang,  Hanlun Zhu,  Lei Wang,  Weining Qian,  Yunshi Lan</p>
  <p><b>备注</b>：Accepted by EMNLP 2023 main conference</p>
  <p><b>关键词</b>：Knowledge Bases, natural language question, Large Language Models, aims to convert, Question Generation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The task of Question Generation over Knowledge Bases (KBQG) aims to convert a logical form into a natural language question. For the sake of expensive cost of large-scale question annotation, the methods of KBQG under low-resource scenarios urgently need to be developed. However, current methods heavily rely on annotated data for fine-tuning, which is not well-suited for few-shot question generation. The emergence of Large Language Models (LLMs) has shown their impressive generalization ability in few-shot tasks. Inspired by Chain-of-Thought (CoT) prompting, which is an in-context learning strategy for reasoning, we formulate KBQG task as a reasoning problem, where the generation of a complete question is splitted into a series of sub-question generation. Our proposed prompting method KQG-CoT first retrieves supportive logical forms from the unlabeled data pool taking account of the characteristics of the logical form. Then, we write a prompt to explicit the reasoning chain of generating complicated questions based on the selected demonstrations. To further ensure prompt quality, we extend KQG-CoT into KQG-CoT+ via sorting the logical forms by their complexity. We conduct extensive experiments over three public KBQG datasets. The results demonstrate that our prompting method consistently outperforms other prompting baselines on the evaluated datasets. Remarkably, our KQG-CoT+ method could surpass existing few-shot SoTA results of the PathQuestions dataset by 18.25, 10.72, and 10.18 absolute points on BLEU-4, METEOR, and ROUGE-L, respectively.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Towards Better Evaluation of Instruction-Following: A Case-Study in  Summarization</b></summary>
  <p><b>编号</b>：[86]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08394</p>
  <p><b>作者</b>：Ondrej Skopek,  Rahul Aralikatte,  Sian Gooding,  Victor Carbune</p>
  <p><b>备注</b>：Accepted to CoNLL 2023</p>
  <p><b>关键词</b>：follow user instructions, user instructions remains, large language models, recent advances, follow user</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite recent advances, evaluating how well large language models (LLMs) follow user instructions remains an open problem. While evaluation methods of language models have seen a rise in prompt-based approaches, limited work on the correctness of these methods has been conducted. In this work, we perform a meta-evaluation of a variety of metrics to quantify how accurately they measure the instruction-following abilities of LLMs. Our investigation is performed on grounded query-based summarization by collecting a new short-form, real-world dataset riSum, containing $300$ document-instruction pairs with $3$ answers each. All $900$ answers are rated by $3$ human annotators. Using riSum, we analyze agreement between evaluation methods and human judgment. Finally, we propose new LLM-based reference-free evaluation methods that improve upon established baselines and perform on-par with costly reference-based metrics which require high-quality summaries.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Do Not Marginalize Mechanisms, Rather Consolidate!</b></summary>
  <p><b>编号</b>：[93]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08377</p>
  <p><b>作者</b>：Moritz Willig (1),  Matej Zečević (1),  Devendra Singh Dhami (4),  Kristian Kersting (1,2,3) (Technical University of Darmstadt, (2) Hessian Center for AI, (3) German Research Center for AI (4) Eindhoven University of Technology)</p>
  <p><b>备注</b>：19 pages, 8 figures</p>
  <p><b>关键词</b>：complex causal relationships, Structural causal models, tool for understanding, understanding the complex, relationships that underlie</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Structural causal models (SCMs) are a powerful tool for understanding the complex causal relationships that underlie many real-world systems. As these systems grow in size, the number of variables and complexity of interactions between them does, too. Thus, becoming convoluted and difficult to analyze. This is particularly true in the context of machine learning and artificial intelligence, where an ever increasing amount of data demands for new methods to simplify and compress large scale SCM. While methods for marginalizing and abstracting SCM already exist today, they may destroy the causality of the marginalized model. To alleviate this, we introduce the concept of consolidating causal mechanisms to transform large-scale SCM while preserving consistent interventional behaviour. We show consolidation is a powerful method for simplifying SCM, discuss reduction of computational complexity and give a perspective on generalizing abilities of consolidated SCM.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：MCU: A Task-centric Framework for Open-ended Agent Evaluation in  Minecraft</b></summary>
  <p><b>编号</b>：[100]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08367</p>
  <p><b>作者</b>：Haowei Lin,  Zihao Wang,  Jianzhu Ma,  Yitao Liang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：framework named MCU, task-centric framework named, MCU framework, open-ended game environment, MCU</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To pursue the goal of creating an open-ended agent in Minecraft, an open-ended game environment with unlimited possibilities, this paper introduces a task-centric framework named MCU for Minecraft agent evaluation. The MCU framework leverages the concept of atom tasks as fundamental building blocks, enabling the generation of diverse or even arbitrary tasks. Within the MCU framework, each task is measured with six distinct difficulty scores (time consumption, operational effort, planning complexity, intricacy, creativity, novelty). These scores offer a multi-dimensional assessment of a task from different angles, and thus can reveal an agent's capability on specific facets. The difficulty scores also serve as the feature of each task, which creates a meaningful task space and unveils the relationship between tasks. For efficient evaluation of Minecraft agents employing the MCU framework, we maintain a unified benchmark, namely SkillForge, which comprises representative tasks with diverse categories and difficulty distribution. We also provide convenient filters for users to select tasks to assess specific capabilities of agents. We show that MCU has the high expressivity to cover all tasks used in recent literature on Minecraft agent, and underscores the need for advancements in areas such as creativity, precise control, and out-of-distribution generalization under the goal of open-ended Minecraft agent development.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：2SFGL: A Simple And Robust Protocol For Graph-Based Fraud Detection</b></summary>
  <p><b>编号</b>：[115]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08335</p>
  <p><b>作者</b>：Zhirui Pan,  Guangzhong Wang,  Zhaoning Li,  Lifeng Chen,  Yang Bian,  Zhongyuan Lai</p>
  <p><b>备注</b>：IEEE</p>
  <p><b>关键词</b>：improves financial safety, learning improves financial, graph learning improves, safety and efficiency, graph learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Financial crime detection using graph learning improves financial safety and efficiency. However, criminals may commit financial crimes across different institutions to avoid detection, which increases the difficulty of detection for financial institutions which use local data for graph learning. As most financial institutions are subject to strict regulations in regards to data privacy protection, the training data is often isolated and conventional learning technology cannot handle the problem. Federated learning (FL) allows multiple institutions to train a model without revealing their datasets to each other, hence ensuring data privacy protection. In this paper, we proposes a novel two-stage approach to federated graph learning (2SFGL): The first stage of 2SFGL involves the virtual fusion of multiparty graphs, and the second involves model training and inference on the virtual graph. We evaluate our framework on a conventional fraud detection task based on the FraudAmazonDataset and FraudYelpDataset. Experimental results show that integrating and applying a GCN (Graph Convolutional Network) with our 2SFGL framework to the same task results in a 17.6\%-30.2\% increase in performance on several typical metrics compared to the case only using FedAvg, while integrating GraphSAGE with 2SFGL results in a 6\%-16.2\% increase in performance compared to the case only using FedAvg. We conclude that our proposed framework is a robust and simple protocol which can be simply integrated to pre-existing graph-based fraud detection methods.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Transport-Hub-Aware Spatial-Temporal Adaptive Graph Transformer for  Traffic Flow Prediction</b></summary>
  <p><b>编号</b>：[117]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08328</p>
  <p><b>作者</b>：Xiao Xu,  Lei Zhang,  Bailong Liu,  Zhizhen Liang,  Xuefei Zhang</p>
  <p><b>备注</b>：11 pages, 4 figures. arXiv admin note: text overlap with arXiv:2301.07945 by other authors</p>
  <p><b>关键词</b>：Intelligent Transportation System, traffic flow prediction, Transportation System, Intelligent Transportation, traffic flow</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As a core technology of Intelligent Transportation System (ITS), traffic flow prediction has a wide range of applications. Traffic flow data are spatial-temporal, which are not only correlated to spatial locations in road networks, but also vary with temporal time indices. Existing methods have solved the challenges in traffic flow prediction partly, focusing on modeling spatial-temporal dependencies effectively, while not all intrinsic properties of traffic flow data are utilized fully. Besides, there are very few attempts at incremental learning of spatial-temporal data mining, and few previous works can be easily transferred to the traffic flow prediction task. Motivated by the challenge of incremental learning methods for traffic flow prediction and the underutilization of intrinsic properties of road networks, we propose a Transport-Hub-aware Spatial-Temporal adaptive graph transFormer (H-STFormer) for traffic flow prediction. Specifically, we first design a novel spatial self-attention module to capture the dynamic spatial dependencies. Three graph masking matrices are integrated into spatial self-attentions to highlight both short- and long-term dependences. Additionally, we employ a temporal self-attention module to detect dynamic temporal patterns in the traffic flow data. Finally, we design an extra spatial-temporal knowledge distillation module for incremental learning of traffic flow prediction tasks. Through extensive experiments, we show the effectiveness of H-STFormer in normal and incremental traffic flow prediction tasks. The code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：CHIP: Contrastive Hierarchical Image Pretraining</b></summary>
  <p><b>编号</b>：[128]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08304</p>
  <p><b>作者</b>：Arpit Mittal,  Harshil Jhaveri,  Swapnil Mallick,  Abhishek Ajmera</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：limited number, Few-shot object classification, few-shot classification model, few-shot classification, model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Few-shot object classification is the task of classifying objects in an image with limited number of examples as supervision. We propose a one-shot/few-shot classification model that can classify an object of any unseen class into a relatively general category in an hierarchically based classification. Our model uses a three-level hierarchical contrastive loss based ResNet152 classifier for classifying an object based on its features extracted from Image embedding, not used during the training phase. For our experimentation, we have used a subset of the ImageNet (ILSVRC-12) dataset that contains only the animal classes for training our model and created our own dataset of unseen classes for evaluating our trained model. Our model provides satisfactory results in classifying the unknown objects into a generic category which has been later discussed in greater detail.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：If our aim is to build morality into an artificial agent, how might we  begin to go about doing so?</b></summary>
  <p><b>编号</b>：[131]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08295</p>
  <p><b>作者</b>：Reneira Seeamber,  Cosmin Badea</p>
  <p><b>备注</b>：12 pages, 1 figure,</p>
  <p><b>关键词</b>：Artificial Intelligence, autonomous driving, healthcare to autonomous, find successful, Intelligence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As Artificial Intelligence (AI) becomes pervasive in most fields, from healthcare to autonomous driving, it is essential that we find successful ways of building morality into our machines, especially for decision-making. However, the question of what it means to be moral is still debated, particularly in the context of AI. In this paper, we highlight the different aspects that should be considered when building moral agents, including the most relevant moral paradigms and challenges. We also discuss the top-down and bottom-up approaches to design and the role of emotion and sentience in morality. We then propose solutions including a hybrid approach to design and a hierarchical approach to combining moral paradigms. We emphasize how governance and policy are becoming ever more critical in AI Ethics and in ensuring that the tasks we set for moral agents are attainable, that ethical behavior is achieved, and that we obtain good AI.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Expanding the Vocabulary of BERT for Knowledge Base Construction</b></summary>
  <p><b>编号</b>：[132]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08291</p>
  <p><b>作者</b>：Dong Yang,  Xu Wang,  Remzi Celebi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Knowledge base construction, acquiring structured information, base construction entails, facilitating question answering, Knowledge base</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Knowledge base construction entails acquiring structured information to create a knowledge base of factual and relational data, facilitating question answering, information retrieval, and semantic understanding. The challenge called "Knowledge Base Construction from Pretrained Language Models" at International Semantic Web Conference 2023 defines tasks focused on constructing knowledge base using language model. Our focus was on Track 1 of the challenge, where the parameters are constrained to a maximum of 1 billion, and the inclusion of entity descriptions within the prompt is prohibited.
Although the masked language model offers sufficient flexibility to extend its vocabulary, it is not inherently designed for multi-token prediction. To address this, we present Vocabulary Expandable BERT for knowledge base construction, which expand the language model's vocabulary while preserving semantic embeddings for newly added words. We adopt task-specific re-pre-training on masked language model to further enhance the language model.
Through experimentation, the results show the effectiveness of our approaches. Our framework achieves F1 score of 0.323 on the hidden test set and 0.362 on the validation set, both data set is provided by the challenge. Notably, our framework adopts a lightweight language model (BERT-base, 0.13 billion parameters) and surpasses the model using prompts directly on large language model (Chatgpt-3, 175 billion parameters). Besides, Token-Recode achieves comparable performances as Re-pretrain. This research advances language understanding models by enabling the direct embedding of multi-token entities, signifying a substantial step forward in link prediction task in knowledge graph and metadata completion in data management.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：CP-KGC: Constrained-Prompt Knowledge Graph Completion with Large  Language Models</b></summary>
  <p><b>编号</b>：[136]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08279</p>
  <p><b>作者</b>：Rui Yang,  Li Fang,  Yi Zhou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：infer missing connections, Knowledge graph completion, graph completion, deduce and infer, infer missing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Knowledge graph completion (KGC) aims to utilize existing knowledge to deduce and infer missing connections within knowledge graphs. Text-based approaches, like SimKGC, have outperformed graph embedding methods, showcasing the promise of inductive KGC. However, the efficacy of text-based methods hinges on the quality of entity textual descriptions. In this paper, we identify the key issue of whether large language models (LLMs) can generate effective text. To mitigate hallucination in LLM-generated text in this paper, we introduce a constraint-based prompt that utilizes the entity and its textual description as contextual constraints to enhance data quality. Our Constrained-Prompt Knowledge Graph Completion (CP-KGC) method demonstrates effective inference under low resource computing conditions and surpasses prior results on the WN18RR and FB15K237 datasets. This showcases the integration of LLMs in KGC tasks and provides new directions for future research.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Lag-Llama: Towards Foundation Models for Time Series Forecasting</b></summary>
  <p><b>编号</b>：[137]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08278</p>
  <p><b>作者</b>：Kashif Rasul,  Arjun Ashok,  Andrew Robert Williams,  Arian Khorasani,  George Adamopoulos,  Rishika Bhagwatkar,  Marin Biloš,  Hena Ghonia,  Nadhir Vincent Hassen,  Anderson Schneider,  Sahil Garg,  Alexandre Drouin,  Nicolas Chapados,  Yuriy Nevmyvaka,  Irina Rish</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：general-purpose univariate probabilistic, probabilistic time-series forecasting, univariate probabilistic time-series, forecasting model trained, build foundation models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Aiming to build foundation models for time-series forecasting and study their scaling behavior, we present here our work-in-progress on Lag-Llama, a general-purpose univariate probabilistic time-series forecasting model trained on a large collection of time-series data. The model shows good zero-shot prediction capabilities on unseen "out-of-distribution" time-series datasets, outperforming supervised baselines. We use smoothly broken power-laws to fit and predict model scaling behavior. The open source code is made available at this https URL.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Direction-Oriented Visual-semantic Embedding Model for Remote Sensing  Image-text Retrieval</b></summary>
  <p><b>编号</b>：[138]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08276</p>
  <p><b>作者</b>：Qing Ma,  Jiancheng Pan,  Cong Bai</p>
  <p><b>备注</b>：13 pages, 11 figures</p>
  <p><b>关键词</b>：Image-text retrieval, recent years, retrieval has developed, developed rapidly, rapidly in recent</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image-text retrieval has developed rapidly in recent years. However, it is still a challenge in remote sensing due to visual-semantic imbalance, which leads to incorrect matching of non-semantic visual and textual features. To solve this problem, we propose a novel Direction-Oriented Visual-semantic Embedding Model (DOVE) to mine the relationship between vision and language. Concretely, a Regional-Oriented Attention Module (ROAM) adaptively adjusts the distance between the final visual and textual embeddings in the latent semantic space, oriented by regional visual features. Meanwhile, a lightweight Digging Text Genome Assistant (DTGA) is designed to expand the range of tractable textual representation and enhance global word-level semantic connections using less attention operations. Ultimately, we exploit a global visual-semantic constraint to reduce single visual dependency and serve as an external constraint for the final visual and textual representations. The effectiveness and superiority of our method are verified by extensive experiments including parameter evaluation, quantitative comparison, ablation studies and visual analysis, on two benchmark datasets, RSICD and RSITMD.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：Impact of Co-occurrence on Factual Knowledge of Large Language Models</b></summary>
  <p><b>编号</b>：[148]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08256</p>
  <p><b>作者</b>：Cheongwoong Kang,  Jaesik Choi</p>
  <p><b>备注</b>：EMNLP 2023 Findings</p>
  <p><b>关键词</b>：make factually incorrect, factually incorrect responses, make factually, factually incorrect, incorrect responses</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) often make factually incorrect responses despite their success in various applications. In this paper, we hypothesize that relying heavily on simple co-occurrence statistics of the pre-training corpora is one of the main factors that cause factual errors. Our results reveal that LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently co-occurred words over the correct answer. Consequently, LLMs struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they are seen during finetuning. We show that co-occurrence bias remains despite scaling up model sizes or finetuning. Therefore, we suggest finetuning on a debiased dataset to mitigate the bias by filtering out biased samples whose subject-object co-occurrence count is high. Although debiased finetuning allows LLMs to memorize rare facts in the training set, it is not effective in recalling rare facts unseen during finetuning. Further research in mitigation will help build reliable language models by preventing potential errors. The code is available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with  Reinforcement Learning</b></summary>
  <p><b>编号</b>：[150]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08252</p>
  <p><b>作者</b>：Zeyuan Ma,  Hongshu Guo,  Jiacheng Chen,  Zhenrui Li,  Guojun Peng,  Yue-Jiao Gong,  Yining Ma,  Zhiguang Cao</p>
  <p><b>备注</b>：Accepted at NuerIPS 2023</p>
  <p><b>关键词</b>：Optimization with Reinforcement, Reinforcement Learning, mitigate manual fine-tuning, showcased the power, power of leveraging</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, Meta-Black-Box Optimization with Reinforcement Learning (MetaBBO-RL) has showcased the power of leveraging RL at the meta-level to mitigate manual fine-tuning of low-level black-box optimizers. However, this field is hindered by the lack of a unified benchmark. To fill this gap, we introduce MetaBox, the first benchmark platform expressly tailored for developing and evaluating MetaBBO-RL methods. MetaBox offers a flexible algorithmic template that allows users to effortlessly implement their unique designs within the platform. Moreover, it provides a broad spectrum of over 300 problem instances, collected from synthetic to realistic scenarios, and an extensive library of 19 baseline methods, including both traditional black-box optimizers and recent MetaBBO-RL methods. Besides, MetaBox introduces three standardized performance metrics, enabling a more thorough assessment of the methods. In a bid to illustrate the utility of MetaBox for facilitating rigorous evaluation and in-depth analysis, we carry out a wide-ranging benchmarking study on existing MetaBBO-RL methods. Our MetaBox is open-source and accessible at: this https URL.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：GROOT: Learning to Follow Instructions by Watching Gameplay Videos</b></summary>
  <p><b>编号</b>：[158]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08235</p>
  <p><b>作者</b>：Shaofei Cai,  Bowei Zhang,  Zihao Wang,  Xiaojian Ma,  Anji Liu,  Yitao Liang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：follow open-ended instructions, study the problem, problem of building, follow open-ended, open-ended instructions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the problem of building a controller that can follow open-ended instructions in open-world environments. We propose to follow reference videos as instructions, which offer expressive goal specifications while eliminating the need for expensive text-gameplay annotations. A new learning framework is derived to allow learning such instruction-following controllers from gameplay videos while producing a video instruction encoder that induces a structured goal space. We implement our agent GROOT in a simple yet effective encoder-decoder architecture based on causal transformers. We evaluate GROOT against open-world counterparts and human players on a proposed Minecraft SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the human-machine gap as well as exhibiting a 70% winning rate over the best generalist agent baseline. Qualitative analysis of the induced goal space further demonstrates some interesting emergent properties, including the goal composition and complex gameplay behavior synthesis. Code and video can be found on the website this https URL.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：The Impact of Time Step Frequency on the Realism of Robotic Manipulation  Simulation for Objects of Different Scales</b></summary>
  <p><b>编号</b>：[160]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08233</p>
  <p><b>作者</b>：Minh Q. Ta,  Holly Dinkel,  Hameed Abdul-Rashid,  Yangfei Dai,  Jessica Myers,  Tan Chen,  Junyi Geng,  Timothy Bretl</p>
  <p><b>备注</b>：3 pages, 3 figures, Best Poster Finalist at the 2023 Robotics and AI in Future Factory Workshop at the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Video presentation [this https URL]. Robotics and AI in Future Factory workshop [this https URL]</p>
  <p><b>关键词</b>：time step frequency, manipulation simulation accuracy, simulation accuracy, time step, step frequency</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This work evaluates the impact of time step frequency and component scale on robotic manipulation simulation accuracy. Increasing the time step frequency for small-scale objects is shown to improve simulation accuracy. This simulation, demonstrating pre-assembly part picking for two object geometries, serves as a starting point for discussing how to improve Sim2Real transfer in robotic assembly processes.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：SimCKP: Simple Contrastive Learning of Keyphrase Representations</b></summary>
  <p><b>编号</b>：[166]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08221</p>
  <p><b>作者</b>：Minseok Choi,  Chaeheon Gwak,  Seho Kim,  Si Hyeong Kim,  Jaegul Choo</p>
  <p><b>备注</b>：Accepted to Findings of EMNLP 2023</p>
  <p><b>关键词</b>：aims to generate, aims to identify, generate a set, set of summarizing, summarizing words</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Keyphrase generation (KG) aims to generate a set of summarizing words or phrases given a source document, while keyphrase extraction (KE) aims to identify them from the text. Because the search space is much smaller in KE, it is often combined with KG to predict keyphrases that may or may not exist in the corresponding document. However, current unified approaches adopt sequence labeling and maximization-based generation that primarily operate at a token level, falling short in observing and scoring keyphrases as a whole. In this work, we propose SimCKP, a simple contrastive learning framework that consists of two stages: 1) An extractor-generator that extracts keyphrases by learning context-aware phrase-level representations in a contrastive manner while also generating keyphrases that do not appear in the document; 2) A reranker that adapts scores for each generated phrase by likewise aligning their representations with the corresponding document. Experimental results on multiple benchmark datasets demonstrate the effectiveness of our proposed approach, which outperforms the state-of-the-art models by a significant margin.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：TriRE: A Multi-Mechanism Learning Paradigm for Continual Knowledge  Retention and Promotion</b></summary>
  <p><b>编号</b>：[168]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08217</p>
  <p><b>作者</b>：Preetha Vijayan,  Prashant Bhat,  Elahe Arani,  Bahram Zonooz</p>
  <p><b>备注</b>：Accepted at 37th Conference on Neural Information Processing Systems (NeurIPS 2023)</p>
  <p><b>关键词</b>：deep neural networks, neural networks due, previously learned tasks, Continual learning, persistent challenge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Continual learning (CL) has remained a persistent challenge for deep neural networks due to catastrophic forgetting (CF) of previously learned tasks. Several techniques such as weight regularization, experience rehearsal, and parameter isolation have been proposed to alleviate CF. Despite their relative success, these research directions have predominantly remained orthogonal and suffer from several shortcomings, while missing out on the advantages of competing strategies. On the contrary, the brain continually learns, accommodates, and transfers knowledge across tasks by simultaneously leveraging several neurophysiological processes, including neurogenesis, active forgetting, neuromodulation, metaplasticity, experience rehearsal, and context-dependent gating, rarely resulting in CF. Inspired by how the brain exploits multiple mechanisms concurrently, we propose TriRE, a novel CL paradigm that encompasses retaining the most prominent neurons for each task, revising and solidifying the extracted knowledge of current and past tasks, and actively promoting less active neurons for subsequent tasks through rewinding and relearning. Across CL settings, TriRE significantly reduces task interference and surpasses different CL approaches considered in isolation.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Trustworthy Machine Learning</b></summary>
  <p><b>编号</b>：[169]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08215</p>
  <p><b>作者</b>：Bálint Mucsányi,  Michael Kirchhof,  Elisa Nguyen,  Alexander Rubinstein,  Seong Joon Oh</p>
  <p><b>备注</b>：373 pages, textbook at the University of T\"ubingen</p>
  <p><b>关键词</b>：machine learning technology, Trustworthy Machine Learning, machine learning, challenges have emerged, applied to actual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As machine learning technology gets applied to actual products and solutions, new challenges have emerged. Models unexpectedly fail to generalize to small changes in the distribution, tend to be confident on novel data they have never seen, or cannot communicate the rationale behind their decisions effectively with the end users. Collectively, we face a trustworthiness issue with the current machine learning technology. This textbook on Trustworthy Machine Learning (TML) covers a theoretical and technical background of four key topics in TML: Out-of-Distribution Generalization, Explainability, Uncertainty Quantification, and Evaluation of Trustworthiness. We discuss important classical and contemporary research papers of the aforementioned fields and uncover and connect their underlying intuitions. The book evolved from the homonymous course at the University of Tübingen, first offered in the Winter Semester of 2022/23. It is meant to be a stand-alone product accompanied by code snippets and various pointers to further sources on topics of TML. The dedicated website of the book is this https URL.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Long-Tailed Classification Based on Coarse-Grained Leading Forest and  Multi-Center Loss</b></summary>
  <p><b>编号</b>：[172]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08206</p>
  <p><b>作者</b>：Jinye Yang,  Ji Xu</p>
  <p><b>备注</b>：This is another research work to apply leading tree structure along with deep learning architecture</p>
  <p><b>关键词</b>：long-tailed classification methods, long-tailed classification, real world, existing long-tailed classification, unavoidable and challenging</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Long-tailed(LT) classification is an unavoidable and challenging problem in the real world. Most of the existing long-tailed classification methods focus only on solving the inter-class imbalance in which there are more samples in the head class than in the tail class, while ignoring the intra-lass imbalance in which the number of samples of the head attribute within the same class is much larger than the number of samples of the tail attribute. The deviation in the model is caused by both of these factors, and due to the fact that attributes are implicit in most datasets and the combination of attributes is very complex, the intra-class imbalance is more difficult to handle. For this purpose, we proposed a long-tailed classification framework, known as \textbf{\textsc{Cognisance}}, which is founded on Coarse-Grained Leading Forest (CLF) and Multi-Center Loss (MCL), aiming to build a multi-granularity joint solution model by means of invariant feature learning. In this method, we designed an unsupervised learning method, i.e., CLF, to better characterize the distribution of attributes within a class. Depending on the distribution of attributes, we can flexibly construct sampling strategies suitable for different environments. In addition, we introduce a new metric learning loss (MCL), which aims to gradually eliminate confusing attributes during the feature learning process. More importantly, this approach does not depend on a specific model structure and can be integrated with existing LT methods as an independent component. We have conducted extensive experiments and our approach has state-of-the-art performance in both existing benchmarks ImageNet-GLT and MSCOCO-GLT, and can improve the performance of existing LT methods. Our codes are available on GitHub: \url{this https URL}</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Beyond Traditional DoE: Deep Reinforcement Learning for Optimizing  Experiments in Model Identification of Battery Dynamics</b></summary>
  <p><b>编号</b>：[175]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08198</p>
  <p><b>作者</b>：Gokhan Budan,  Francesca Damiani,  Can Kurtulus,  N. Kemal Ure</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：energy management systems, design processes rely, battery dynamics, energy research, energy management</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Model identification of battery dynamics is a central problem in energy research; many energy management systems and design processes rely on accurate battery models for efficiency optimization. The standard methodology for battery modelling is traditional design of experiments (DoE), where the battery dynamics are excited with many different current profiles and the measured outputs are used to estimate the system dynamics. However, although it is possible to obtain useful models with the traditional approach, the process is time consuming and expensive because of the need to sweep many different current-profile configurations. In the present work, a novel DoE approach is developed based on deep reinforcement learning, which alters the configuration of the experiments on the fly based on the statistics of past experiments. Instead of sticking to a library of predefined current profiles, the proposed approach modifies the current profiles dynamically by updating the output space covered by past measurements, hence only the current profiles that are informative for future experiments are applied. Simulations and real experiments are used to show that the proposed approach gives models that are as accurate as those obtained with traditional DoE but by using 85\% less resources.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form  Narrative Text Generation</b></summary>
  <p><b>编号</b>：[179]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08185</p>
  <p><b>作者</b>：Wang You,  Wenshan Wu,  Yaobo Liang,  Shaoguang Mao,  Chenfei Wu,  Maosong Cao,  Yuzhe Cai,  Yiduo Guo,  Yan Xia,  Furu Wei,  Nan Duan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：narrative text generation, plan, Plan Extraction, long-form narrative text, Evaluation-guided Iterative Plan</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Plan-and-Write is a common hierarchical approach in long-form narrative text generation, which first creates a plan to guide the narrative writing. Following this approach, several studies rely on simply prompting large language models for planning, which often yields suboptimal results. In this paper, we propose a new framework called Evaluation-guided Iterative Plan Extraction for long-form narrative text generation (EIPE-text), which extracts plans from the corpus of narratives and utilizes the extracted plans to construct a better planner. EIPE-text has three stages: plan extraction, learning, and inference. In the plan extraction stage, it iteratively extracts and improves plans from the narrative corpus and constructs a plan corpus. We propose a question answer (QA) based evaluation mechanism to automatically evaluate the plans and generate detailed plan refinement instructions to guide the iterative improvement. In the learning stage, we build a better planner by fine-tuning with the plan corpus or in-context learning with examples in the plan corpus. Finally, we leverage a hierarchical approach to generate long-form narratives. We evaluate the effectiveness of EIPE-text in the domains of novels and storytelling. Both GPT-4-based evaluations and human evaluations demonstrate that our method can generate more coherent and relevant long-form narratives. Our code will be released in the future.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Learn From Model Beyond Fine-Tuning: A Survey</b></summary>
  <p><b>编号</b>：[180]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08184</p>
  <p><b>作者</b>：Hongling Zheng,  Li Shen,  Anke Tang,  Yong Luo,  Han Hu,  Bo Du,  Dacheng Tao</p>
  <p><b>备注</b>：20 pages, 9 figures</p>
  <p><b>关键词</b>：natural language processing, demonstrated remarkable performance, model, computer vision, primarily attributed</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Foundation models (FM) have demonstrated remarkable performance across a wide range of tasks (especially in the fields of natural language processing and computer vision), primarily attributed to their ability to comprehend instructions and access extensive, high-quality data. This not only showcases their current effectiveness but also sets a promising trajectory towards the development of artificial general intelligence. Unfortunately, due to multiple constraints, the raw data of the model used for large model training are often inaccessible, so the use of end-to-end models for downstream tasks has become a new research trend, which we call Learn From Model (LFM) in this article. LFM focuses on the research, modification, and design of FM based on the model interface, so as to better understand the model structure and weights (in a black box environment), and to generalize the model to downstream tasks. The study of LFM techniques can be broadly categorized into five major areas: model tuning, model distillation, model reuse, meta learning and model editing. Each category encompasses a repertoire of methods and strategies that aim to enhance the capabilities and performance of FM. This paper gives a comprehensive review of the current methods based on FM from the perspective of LFM, in order to help readers better understand the current research status and ideas. To conclude, we summarize the survey by highlighting several critical areas for future exploration and addressing open issues that require further attention from the research community. The relevant papers we investigated in this article can be accessed at <this https url.< p>
  </this></p></details>
</details>
<details>
  <summary>50. <b>标题：Multi-Scale Spatial-Temporal Recurrent Networks for Traffic Flow  Prediction</b></summary>
  <p><b>编号</b>：[199]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08138</p>
  <p><b>作者</b>：Haiyang Liu,  Chunjiang Zhu,  Detian Zhang,  Qing Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：intelligent transportation systems, Traffic flow prediction, Traffic flow, flow prediction, transportation systems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Traffic flow prediction is one of the most fundamental tasks of intelligent transportation systems. The complex and dynamic spatial-temporal dependencies make the traffic flow prediction quite challenging. Although existing spatial-temporal graph neural networks hold prominent, they often encounter challenges such as (1) ignoring the fixed graph that limits the predictive performance of the model, (2) insufficiently capturing complex spatial-temporal dependencies simultaneously, and (3) lacking attention to spatial-temporal information at different time lengths. In this paper, we propose a Multi-Scale Spatial-Temporal Recurrent Network for traffic flow prediction, namely MSSTRN, which consists of two different recurrent neural networks: the single-step gate recurrent unit and the multi-step gate recurrent unit to fully capture the complex spatial-temporal information in the traffic data under different time windows. Moreover, we propose a spatial-temporal synchronous attention mechanism that integrates adaptive position graph convolutions into the self-attention mechanism to achieve synchronous capture of spatial-temporal dependencies. We conducted extensive experiments on four real traffic datasets and demonstrated that our model achieves the best prediction accuracy with non-trivial margins compared to all the twenty baseline methods.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Can Large Language Models Really Improve by Self-critiquing Their Own  Plans?</b></summary>
  <p><b>编号</b>：[208]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08118</p>
  <p><b>作者</b>：Karthik Valmeekam,  Matthew Marquez,  Subbarao Kambhampati</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Large Language, Language Models, plan generation, successfully verify</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>There have been widespread claims about Large Language Models (LLMs) being able to successfully verify or self-critique their candidate solutions in reasoning problems in an iterative mode. Intrigued by those claims, in this paper we set out to investigate the verification/self-critiquing abilities of large language models in the context of planning. We evaluate a planning system that employs LLMs for both plan generation and verification. We assess the verifier LLM's performance against ground-truth verification, the impact of self-critiquing on plan generation, and the influence of varying feedback levels on system performance. Using GPT-4, a state-of-the-art LLM, for both generation and verification, our findings reveal that self-critiquing appears to diminish plan generation performance, especially when compared to systems with external, sound verifiers and the LLM verifiers in that system produce a notable number of false positives, compromising the system's reliability. Additionally, the nature of feedback, whether binary or detailed, showed minimal impact on plan generation. Collectively, our results cast doubt on the effectiveness of LLMs in a self-critiquing, iterative framework for planning tasks.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：DUSA: Decoupled Unsupervised Sim2Real Adaptation for  Vehicle-to-Everything Collaborative Perception</b></summary>
  <p><b>编号</b>：[209]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08117</p>
  <p><b>作者</b>：Xianghao Kong,  Wentao Jiang,  Jinrang Jia,  Yifeng Shi,  Runsheng Xu,  Si Liu</p>
  <p><b>备注</b>：ACM MM 2023</p>
  <p><b>关键词</b>：real-world data, data, Simulated data, adaptation, real-world</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vehicle-to-Everything (V2X) collaborative perception is crucial for autonomous driving. However, achieving high-precision V2X perception requires a significant amount of annotated real-world data, which can always be expensive and hard to acquire. Simulated data have raised much attention since they can be massively produced at an extremely low cost. Nevertheless, the significant domain gap between simulated and real-world data, including differences in sensor type, reflectance patterns, and road surroundings, often leads to poor performance of models trained on simulated data when evaluated on real-world data. In addition, there remains a domain gap between real-world collaborative agents, e.g. different types of sensors may be installed on autonomous vehicles and roadside infrastructures with different extrinsics, further increasing the difficulty of sim2real generalization. To take full advantage of simulated data, we present a new unsupervised sim2real domain adaptation method for V2X collaborative detection named Decoupled Unsupervised Sim2Real Adaptation (DUSA). Our new method decouples the V2X collaborative sim2real domain adaptation problem into two sub-problems: sim2real adaptation and inter-agent adaptation. For sim2real adaptation, we design a Location-adaptive Sim2Real Adapter (LSA) module to adaptively aggregate features from critical locations of the feature map and align the features between simulated data and real-world data via a sim/real discriminator on the aggregated global feature. For inter-agent adaptation, we further devise a Confidence-aware Inter-agent Adapter (CIA) module to align the fine-grained features from heterogeneous agents under the guidance of agent-wise confidence maps. Experiments demonstrate the effectiveness of the proposed DUSA approach on unsupervised sim2real adaptation from the simulated V2XSet dataset to the real-world DAIR-V2X-C dataset.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Promptor: A Conversational and Autonomous Prompt Generation Agent for  Intelligent Text Entry Techniques</b></summary>
  <p><b>编号</b>：[214]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08101</p>
  <p><b>作者</b>：Junxiao Shen,  John J. Dudley,  Jingyao Zheng,  Bill Byrne,  Per Ola Kristensson</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：digital interactions, language models, large language models, Text entry, language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Text entry is an essential task in our day-to-day digital interactions. Numerous intelligent features have been developed to streamline this process, making text entry more effective, efficient, and fluid. These improvements include sentence prediction and user personalization. However, as deep learning-based language models become the norm for these advanced features, the necessity for data collection and model fine-tuning increases. These challenges can be mitigated by harnessing the in-context learning capability of large language models such as GPT-3.5. This unique feature allows the language model to acquire new skills through prompts, eliminating the need for data collection and fine-tuning. Consequently, large language models can learn various text prediction techniques. We initially showed that, for a sentence prediction task, merely prompting GPT-3.5 surpassed a GPT-2 backed system and is comparable with a fine-tuned GPT-3.5 model, with the latter two methods requiring costly data collection, fine-tuning and post-processing. However, the task of prompting large language models to specialize in specific text prediction tasks can be challenging, particularly for designers without expertise in prompt engineering. To address this, we introduce Promptor, a conversational prompt generation agent designed to engage proactively with designers. Promptor can automatically generate complex prompts tailored to meet specific needs, thus offering a solution to this challenge. We conducted a user study involving 24 participants creating prompts for three intelligent text entry tasks, half of the participants used Promptor while the other half designed prompts themselves. The results show that Promptor-designed prompts result in a 35% increase in similarity and 22% in coherence over those by designers.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Sentinel: An Aggregation Function to Secure Decentralized Federated  Learning</b></summary>
  <p><b>编号</b>：[217]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08097</p>
  <p><b>作者</b>：Chao Feng,  Alberto Huertas Celdran,  Janosch Baltensperger,  Enrique Tomas Matınez Bertran,  Gerome Bovet,  Burkhard Stiller</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Decentralized Federated Learning, Federated Learning, preserving data privacy, quality of service, Decentralized Federated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rapid integration of Federated Learning (FL) into networking encompasses various aspects such as network management, quality of service, and cybersecurity while preserving data privacy. In this context, Decentralized Federated Learning (DFL) emerges as an innovative paradigm to train collaborative models, addressing the single point of failure limitation. However, the security and trustworthiness of FL and DFL are compromised by poisoning attacks, negatively impacting its performance. Existing defense mechanisms have been designed for centralized FL and they do not adequately exploit the particularities of DFL. Thus, this work introduces Sentinel, a defense strategy to counteract poisoning attacks in DFL. Sentinel leverages the accessibility of local data and defines a three-step aggregation protocol consisting of similarity filtering, bootstrap validation, and normalization to safeguard against malicious model updates. Sentinel has been evaluated with diverse datasets and various poisoning attack types and threat levels, improving the state-of-the-art performance against both untargeted and targeted poisoning attacks.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Discerning Temporal Difference Learning</b></summary>
  <p><b>编号</b>：[222]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08091</p>
  <p><b>作者</b>：Jianfei Ma</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Temporal difference learning, Temporal difference, aimed at efficiently, foundational concept, concept in reinforcement</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Temporal difference learning (TD) is a foundational concept in reinforcement learning (RL), aimed at efficiently assessing a policy's value function. TD($\lambda$), a potent variant, incorporates a memory trace to distribute the prediction error into the historical context. However, this approach often neglects the significance of historical states and the relative importance of propagating the TD error, influenced by challenges such as visitation imbalance or outcome noise. To address this, we propose a novel TD algorithm named discerning TD learning (DTD), which allows flexible emphasis functions$-$predetermined or adapted during training$-$to allocate efforts effectively across states. We establish the convergence properties of our method within a specific class of emphasis functions and showcase its promising potential for adaptation to deep RL contexts. Empirical results underscore that employing a judicious emphasis function not only improves value estimation but also expedites learning across diverse scenarios.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：Low-Resource Clickbait Spoiling for Indonesian via Question Answering</b></summary>
  <p><b>编号</b>：[225]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08085</p>
  <p><b>作者</b>：Ni Putu Intan Maharani,  Ayu Purwarianti,  Alham Fikri Aji</p>
  <p><b>备注</b>：Accepted in ICAICTA 2023 (10th International Conference on Advanced Informatics: Concepts, Theory and Applications)</p>
  <p><b>关键词</b>：Clickbait spoiling aims, aims to generate, generate a short, short text, text to satisfy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Clickbait spoiling aims to generate a short text to satisfy the curiosity induced by a clickbait post. As it is a newly introduced task, the dataset is only available in English so far. Our contributions include the construction of manually labeled clickbait spoiling corpus in Indonesian and an evaluation on using cross-lingual zero-shot question answering-based models to tackle clikcbait spoiling for low-resource language like Indonesian. We utilize selection of multilingual language models. The experimental results suggest that XLM-RoBERTa (large) model outperforms other models for phrase and passage spoilers, meanwhile, mDeBERTa (base) model outperforms other models for multipart spoilers.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：GameGPT: Multi-agent Collaborative Framework for Game Development</b></summary>
  <p><b>编号</b>：[236]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08067</p>
  <p><b>作者</b>：Dake Chen,  Hanbin Wang,  Yunhao Huo,  Yuzhao Li,  Haoyang Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language model, software development processes, expedite software development, language model, based agents</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The large language model (LLM) based agents have demonstrated their capacity to automate and expedite software development processes. In this paper, we focus on game development and propose a multi-agent collaborative framework, dubbed GameGPT, to automate game development. While many studies have pinpointed hallucination as a primary roadblock for deploying LLMs in production, we identify another concern: redundancy. Our framework presents a series of methods to mitigate both concerns. These methods include dual collaboration and layered approaches with several in-house lexicons, to mitigate the hallucination and redundancy in the planning, task identification, and implementation phases. Furthermore, a decoupling approach is also introduced to achieve code generation with better precision.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：The Search-and-Mix Paradigm in Approximate Nash Equilibrium Algorithms</b></summary>
  <p><b>编号</b>：[237]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08066</p>
  <p><b>作者</b>：Xiaotie Deng,  Dongchen Li,  Hanyu Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Math deals, reasoning becomes automated, deals with mathematics, constructive manner, approximate Nash equilibria</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>AI in Math deals with mathematics in a constructive manner so that reasoning becomes automated, less laborious, and less error-prone. For algorithms, the question becomes how to automate analyses for specific problems. For the first time, this work provides an automatic method for approximation analysis on a well-studied problem in theoretical computer science: computing approximate Nash equilibria in two-player games. We observe that such algorithms can be reformulated into a search-and-mix paradigm, which involves a search phase followed by a mixing phase. By doing so, we are able to fully automate the procedure of designing and analyzing the mixing phase. For example, we illustrate how to perform our method with a program to analyze the approximation bounds of all the algorithms in the literature. Same approximation bounds are computed without any hand-written proof. Our automatic method heavily relies on the LP-relaxation structure in approximate Nash equilibria. Since many approximation algorithms and online algorithms adopt the LP relaxation, our approach may be extended to automate the analysis of other algorithms.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Learning from Label Proportions: Bootstrapping Supervised Learners via  Belief Propagation</b></summary>
  <p><b>编号</b>：[239]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08056</p>
  <p><b>作者</b>：Shreyas Havaldar,  Navodita Sharma,  Shubhi Sareen,  Karthikeyan Shanmugam,  Aravindan Raghuveer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Label Proportions, aggregate level labels, test data, pseudo labels, Belief Propagation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Learning from Label Proportions (LLP) is a learning problem where only aggregate level labels are available for groups of instances, called bags, during training, and the aim is to get the best performance at the instance-level on the test data. This setting arises in domains like advertising and medicine due to privacy considerations. We propose a novel algorithmic framework for this problem that iteratively performs two main steps. For the first step (Pseudo Labeling) in every iteration, we define a Gibbs distribution over binary instance labels that incorporates a) covariate information through the constraint that instances with similar covariates should have similar labels and b) the bag level aggregated label. We then use Belief Propagation (BP) to marginalize the Gibbs distribution to obtain pseudo labels. In the second step (Embedding Refinement), we use the pseudo labels to provide supervision for a learner that yields a better embedding. Further, we iterate on the two steps again by using the second step's embeddings as new covariates for the next iteration. In the final iteration, a classifier is trained using the pseudo labels. Our algorithm displays strong gains against several SOTA baselines (up to 15%) for the LLP Binary Classification problem on various dataset types - tabular and Image. We achieve these improvements with minimal computational overhead above standard supervised learning due to Belief Propagation, for large bag sizes, even for a million samples.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Understanding and Controlling a Maze-Solving Policy Network</b></summary>
  <p><b>编号</b>：[245]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08043</p>
  <p><b>作者</b>：Ulisse Mini,  Peli Grietzer,  Mrinank Sharma,  Austin Meek,  Monte MacDiarmid,  Alexander Matt Turner</p>
  <p><b>备注</b>：46 pages</p>
  <p><b>关键词</b>：pretrained reinforcement learning, reinforcement learning policy, target squares, carefully study, study a pretrained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To understand the goals and goal representations of AI systems, we carefully study a pretrained reinforcement learning policy that solves mazes by navigating to a range of target squares. We find this network pursues multiple context-dependent goals, and we further identify circuits within the network that correspond to one of these goals. In particular, we identified eleven channels that track the location of the goal. By modifying these channels, either with hand-designed interventions or by combining forward passes, we can partially control the policy. We show that this network contains redundant, distributed, and retargetable goal representations, shedding light on the nature of goal-direction in trained policy networks.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large  Language Models</b></summary>
  <p><b>编号</b>：[247]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08041</p>
  <p><b>作者</b>：Jing Liu,  Ruihao Gong,  Xiuying Wei,  Zhiwei Dong,  Jianfei Cai,  Bohan Zhuang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, excel in NLP, Large Language, Language Models, widespread deployment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) excel in NLP, but their demands hinder their widespread deployment. While Quantization-Aware Training (QAT) offers a solution, its extensive training costs make Post-Training Quantization (PTQ) a more practical approach for LLMs. In existing studies, activation outliers in particular channels are identified as the bottleneck to PTQ accuracy. They propose to transform the magnitudes from activations to weights, which however offers limited alleviation or suffers from unstable gradients, resulting in a severe performance drop at low-bitwidth. In this paper, we propose QLLM, an accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM introduces an adaptive channel reassembly technique that reallocates the magnitude of outliers to other channels, thereby mitigating their impact on the quantization range. This is achieved by channel disassembly and channel assembly, which first breaks down the outlier channels into several sub-channels to ensure a more balanced distribution of activation magnitudes. Then similar channels are merged to maintain the original channel number for efficiency. Additionally, an adaptive strategy is designed to autonomously determine the optimal number of sub-channels for channel disassembly. To further compensate for the performance loss caused by quantization, we propose an efficient tuning method that only learns a small number of low-rank weights while freezing the pre-trained quantized model. After training, these low-rank parameters can be fused into the frozen weights without affecting inference. Extensive experiments on LLaMA-1 and LLaMA-2 show that QLLM can obtain accurate quantized models efficiently. For example, QLLM quantizes the 4-bit LLaMA-2-70B within 10 hours on a single A100-80G GPU, outperforming the previous state-of-the-art method by 7.89% on the average accuracy across five zero-shot tasks.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：Rethinking Large-scale Pre-ranking System: Entire-chain Cross-domain  Models</b></summary>
  <p><b>编号</b>：[249]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08039</p>
  <p><b>作者</b>：Jinbo Song (1),  Ruoran Huang (1),  Xinyang Wang (1),  Wei Huang (1),  Qian Yu (1),  Mingming Chen (1),  Yafei Yao (1),  Chaosheng Fan (1),  Changping Peng (1),  Zhangang Lin (1),  Jinghe Hu (1),  Jingping Shao (1) ((1) Marketing and Commercialization Center, JD.com)</p>
  <p><b>备注</b>：5 pages, 2 figures</p>
  <p><b>关键词</b>：online advertising, multi-stage architectures, widely equipped, equipped with multi-stage, including matching</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Industrial systems such as recommender systems and online advertising, have been widely equipped with multi-stage architectures, which are divided into several cascaded modules, including matching, pre-ranking, ranking and re-ranking. As a critical bridge between matching and ranking, existing pre-ranking approaches mainly endure sample selection bias (SSB) problem owing to ignoring the entire-chain data dependence, resulting in sub-optimal performances. In this paper, we rethink pre-ranking system from the perspective of the entire sample space, and propose Entire-chain Cross-domain Models (ECM), which leverage samples from the whole cascaded stages to effectively alleviate SSB problem. Besides, we design a fine-grained neural structure named ECMM to further improve the pre-ranking accuracy. Specifically, we propose a cross-domain multi-tower neural network to comprehensively predict for each stage result, and introduce the sub-networking routing strategy with $L0$ regularization to reduce computational costs. Evaluations on real-world large-scale traffic logs demonstrate that our pre-ranking models outperform SOTA methods while time consumption is maintained within an acceptable level, which achieves better trade-off between efficiency and effectiveness.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Receive, Reason, and React: Drive as You Say with Large Language Models  in Autonomous Vehicles</b></summary>
  <p><b>编号</b>：[254]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08034</p>
  <p><b>作者</b>：Can Cui,  Yunsheng Ma,  Xu Cao,  Wenqian Ye,  Ziran Wang</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2309.10228</p>
  <p><b>关键词</b>：next-generation autonomous vehicles, Large Language Models, autonomous vehicles, artificial intelligence, fusion of human-centric</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The fusion of human-centric design and artificial intelligence (AI) capabilities has opened up new possibilities for next-generation autonomous vehicles that go beyond transportation. These vehicles can dynamically interact with passengers and adapt to their preferences. This paper proposes a novel framework that leverages Large Language Models (LLMs) to enhance the decision-making process in autonomous vehicles. By utilizing LLMs' linguistic and contextual understanding abilities with specialized tools, we aim to integrate the language and reasoning capabilities of LLMs into autonomous vehicles. Our research includes experiments in HighwayEnv, a collection of environments for autonomous driving and tactical decision-making tasks, to explore LLMs' interpretation, interaction, and reasoning in various scenarios. We also examine real-time personalization, demonstrating how LLMs can influence driving behaviors based on verbal commands. Our empirical results highlight the substantial advantages of utilizing chain-of-thought prompting, leading to improved driving decisions, and showing the potential for LLMs to enhance personalized driving experiences through ongoing verbal feedback. The proposed framework aims to transform autonomous vehicle operations, offering personalized support, transparent decision-making, and continuous learning to enhance safety and effectiveness. We achieve user-centric, transparent, and adaptive autonomous driving ecosystems supported by the integration of LLMs into autonomous vehicles.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：Incorporating Domain Knowledge Graph into Multimodal Movie Genre  Classification with Self-Supervised Attention and Contrastive Learning</b></summary>
  <p><b>编号</b>：[255]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08032</p>
  <p><b>作者</b>：Jiaqi Li,  Guilin Qi,  Chuanyi Zhang,  Yongrui Chen,  Yiming Tan,  Chenlong Xia,  Ye Tian</p>
  <p><b>备注</b>：Accepted by ACM MM 2023</p>
  <p><b>关键词</b>：demanding multi-label classification, multi-label classification task, classification task due, knowledge graph, Multimodal movie genre</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multimodal movie genre classification has always been regarded as a demanding multi-label classification task due to the diversity of multimodal data such as posters, plot summaries, trailers and metadata. Although existing works have made great progress in modeling and combining each modality, they still face three issues: 1) unutilized group relations in metadata, 2) unreliable attention allocation, and 3) indiscriminative fused features. Given that the knowledge graph has been proven to contain rich information, we present a novel framework that exploits the knowledge graph from various perspectives to address the above problems. As a preparation, the metadata is processed into a domain knowledge graph. A translate model for knowledge graph embedding is adopted to capture the relations between entities. Firstly we retrieve the relevant embedding from the knowledge graph by utilizing group relations in metadata and then integrate it with other modalities. Next, we introduce an Attention Teacher module for reliable attention allocation based on self-supervised learning. It learns the distribution of the knowledge graph and produces rational attention weights. Finally, a Genre-Centroid Anchored Contrastive Learning module is proposed to strengthen the discriminative ability of fused features. The embedding space of anchors is initialized from the genre entities in the knowledge graph. To verify the effectiveness of our framework, we collect a larger and more challenging dataset named MM-IMDb 2.0 compared with the MM-IMDb dataset. The experimental results on two datasets demonstrate that our model is superior to the state-of-the-art methods. We will release the code in the near future.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：Beyond Sharing Weights in Decoupling Feature Learning Network for UAV  RGB-Infrared Vehicle Re-Identification</b></summary>
  <p><b>编号</b>：[258]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08026</p>
  <p><b>作者</b>：Xingyue Liu,  Jiahao Qi,  Chen Chen,  Kangcheng Bin,  Ping Zhong</p>
  <p><b>备注</b>：13 pages, 10 figures, 64 citations, submitted to TMM</p>
  <p><b>关键词</b>：full-time target search, performing full-time target, unmanned aerial vehicle, cross-modality vehicle re-identification, cross-modality vehicle Re-ID</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Owing to the capacity of performing full-time target search, cross-modality vehicle re-identification (Re-ID) based on unmanned aerial vehicle (UAV) is gaining more attention in both video surveillance and public security. However, this promising and innovative research has not been studied sufficiently due to the data inadequacy issue. Meanwhile, the cross-modality discrepancy and orientation discrepancy challenges further aggravate the difficulty of this task. To this end, we pioneer a cross-modality vehicle Re-ID benchmark named UAV Cross-Modality Vehicle Re-ID (UCM-VeID), containing 753 identities with 16015 RGB and 13913 infrared images. Moreover, to meet cross-modality discrepancy and orientation discrepancy challenges, we present a hybrid weights decoupling network (HWDNet) to learn the shared discriminative orientation-invariant features. For the first challenge, we proposed a hybrid weights siamese network with a well-designed weight restrainer and its corresponding objective function to learn both modality-specific and modality shared information. In terms of the second challenge, three effective decoupling structures with two pretext tasks are investigated to learn orientation-invariant feature. Comprehensive experiments are carried out to validate the effectiveness of the proposed method. The dataset and codes will be released at this https URL.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：Effects of Human Adversarial and Affable Samples on BERT  Generalizability</b></summary>
  <p><b>编号</b>：[266]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08008</p>
  <p><b>作者</b>：Aparna Elangovan,  Jiayuan He,  Yuan Li,  Karin Verspoor</p>
  <p><b>备注</b>：To appear at EMNLP 2023</p>
  <p><b>关键词</b>：real-world settings requiring, settings requiring generalization, training data, demonstrably worse, worse in real-world</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>BERT-based models have had strong performance on leaderboards, yet have been demonstrably worse in real-world settings requiring generalization. Limited quantities of training data is considered a key impediment to achieving generalizability in machine learning. In this paper, we examine the impact of training data quality, not quantity, on a model's generalizability. We consider two characteristics of training data: the portion of human-adversarial (h-adversarial), i.e., sample pairs with seemingly minor differences but different ground-truth labels, and human-affable (h-affable) training samples, i.e., sample pairs with minor differences but the same ground-truth label. We find that for a fixed size of training samples, as a rule of thumb, having 10-30% h-adversarial instances improves the precision, and therefore F1, by up to 20 points in the tasks of text classification and relation extraction. Increasing h-adversarials beyond this range can result in performance plateaus or even degradation. In contrast, h-affables may not contribute to a model's generalizability and may even degrade generalization performance.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：A Novel Statistical Measure for Out-of-Distribution Detection in Data  Quality Assurance</b></summary>
  <p><b>编号</b>：[270]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07998</p>
  <p><b>作者</b>：Tinghui Ouyang,  Isao Echizen,  Yoshiki Seo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：AI-based intelligent systems, poses significant threats, problem domain poses, domain poses significant, OOD detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Data outside the problem domain poses significant threats to the security of AI-based intelligent systems. Aiming to investigate the data domain and out-of-distribution (OOD) data in AI quality management (AIQM) study, this paper proposes to use deep learning techniques for feature representation and develop a novel statistical measure for OOD detection. First, to extract low-dimensional representative features distinguishing normal and OOD data, the proposed research combines the deep auto-encoder (AE) architecture and neuron activation status for feature engineering. Then, using local conditional probability (LCP) in data reconstruction, a novel and superior statistical measure is developed to calculate the score of OOD detection. Experiments and evaluations are conducted on image benchmark datasets and an industrial dataset. Through comparative analysis with other common statistical measures in OOD detection, the proposed research is validated as feasible and effective in OOD and AIQM studies.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：Point-NeuS: Point-Guided Neural Implicit Surface Reconstruction by  Volume Rendering</b></summary>
  <p><b>编号</b>：[271]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07997</p>
  <p><b>作者</b>：Chen Zhang,  Wanjuan Su,  Wenbing Tao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：learning neural implicit, volume rendering, multi-view reconstruction, learning neural, neural implicit surface</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, learning neural implicit surface by volume rendering has been a promising way for multi-view reconstruction. However, limited accuracy and excessive time complexity remain bottlenecks that current methods urgently need to overcome. To address these challenges, we propose a new method called Point-NeuS, utilizing point-guided mechanisms to achieve accurate and efficient reconstruction. Point modeling is organically embedded into the volume rendering to enhance and regularize the representation of implicit surface. Specifically, to achieve precise point guidance and noise robustness, aleatoric uncertainty of the point cloud is modeled to capture the distribution of noise and estimate the reliability of points. Additionally, a Neural Projection module connecting points and images is introduced to add geometric constraints to the Signed Distance Function (SDF). To better compensate for geometric bias between volume rendering and point modeling, high-fidelity points are filtered into an Implicit Displacement Network to improve the representation of SDF. Benefiting from our effective point guidance, lightweight networks are employed to achieve an impressive 11x speedup compared to NeuS. Extensive experiments show that our method yields high-quality surfaces, especially for fine-grained details and smooth regions. Moreover, it exhibits strong robustness to both noisy and sparse data.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：HeightFormer: A Multilevel Interaction and Image-adaptive  Classification-regression Network for Monocular Height Estimation with Aerial  Images</b></summary>
  <p><b>编号</b>：[273]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07995</p>
  <p><b>作者</b>：Zhan Chen,  Yidan Zhang,  Xiyu Qi,  Yongqiang Mao,  Xin Zhou,  Lulu Niu,  Hui Wu,  Lei Wang,  Yunping Ge</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：urban modelling, proving critical, autonomous driving, pivotal topic, topic within measurement</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Height estimation has long been a pivotal topic within measurement and remote sensing disciplines, proving critical for endeavours such as 3D urban modelling, MR and autonomous driving. Traditional methods utilise stereo matching or multisensor fusion, both well-established techniques that typically necessitate multiple images from varying perspectives and adjunct sensors like SAR, leading to substantial deployment costs. Single image height estimation has emerged as an attractive alternative, boasting a larger data source variety and simpler deployment. However, current methods suffer from limitations such as fixed receptive fields, a lack of global information interaction, leading to noticeable instance-level height deviations. The inherent complexity of height prediction can result in a blurry estimation of object edge depth when using mainstream regression methods based on fixed height division. This paper presents a comprehensive solution for monocular height estimation in remote sensing, termed HeightFormer, combining multilevel interactions and image-adaptive classification-regression. It features the Multilevel Interaction Backbone (MIB) and Image-adaptive Classification-regression Height Generator (ICG). MIB supplements the fixed sample grid in CNN of the conventional backbone network with tokens of different interaction ranges. It is complemented by a pixel-, patch-, and feature map-level hierarchical interaction mechanism, designed to relay spatial geometry information across different scales and introducing a global receptive field to enhance the quality of instance-level height estimation. The ICG dynamically generates height partition for each image and reframes the traditional regression task, using a refinement from coarse to fine classification-regression that significantly mitigates the innate ill-posedness issue and drastically improves edge sharpness.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：Large Language Models for Scientific Synthesis, Inference and  Explanation</b></summary>
  <p><b>编号</b>：[277]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07984</p>
  <p><b>作者</b>：Yizhen Zheng,  Huan Yee Koh,  Jiaxin Ju,  Anh T.N. Nguyen,  Lauren T. May,  Geoffrey I. Webb,  Shirui Pan</p>
  <p><b>备注</b>：Supplementary Information: this https URL Github Repo: this https URL</p>
  <p><b>关键词</b>：Large language models, Large language, artificial intelligence systems, primary knowledge consists, language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models are a form of artificial intelligence systems whose primary knowledge consists of the statistical patterns, semantic relationships, and syntactical structures of language1. Despite their limited forms of "knowledge", these systems are adept at numerous complex tasks including creative writing, storytelling, translation, question-answering, summarization, and computer code generation. However, they have yet to demonstrate advanced applications in natural science. Here we show how large language models can perform scientific synthesis, inference, and explanation. We present a method for using general-purpose large language models to make inferences from scientific datasets of the form usually associated with special-purpose machine learning algorithms. We show that the large language model can augment this "knowledge" by synthesizing from the scientific literature. When a conventional machine learning system is augmented with this synthesized and inferred knowledge it can outperform the current state of the art across a range of benchmark tasks for predicting molecular properties. This approach has the further advantage that the large language model can explain the machine learning system's predictions. We anticipate that our framework will open new avenues for AI to accelerate the pace of scientific discovery.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：Self-supervised visual learning for analyzing firearms trafficking  activities on the Web</b></summary>
  <p><b>编号</b>：[283]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07975</p>
  <p><b>作者</b>：Sotirios Konstantakos,  Despina Ioanna Chalkiadaki,  Ioannis Mademlis,  Adamantia Anna Rebolledo Chrysochoou,  Georgios Th. Papadopoulos</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：public space security, law enforcement investigations, World Wide Web, RGB images, Convolutional Neural Networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automated visual firearms classification from RGB images is an important real-world task with applications in public space security, intelligence gathering and law enforcement investigations. When applied to images massively crawled from the World Wide Web (including social media and dark Web sites), it can serve as an important component of systems that attempt to identify criminal firearms trafficking networks, by analyzing Big Data from open-source intelligence. Deep Neural Networks (DNN) are the state-of-the-art methodology for achieving this, with Convolutional Neural Networks (CNN) being typically employed. The common transfer learning approach consists of pretraining on a large-scale, generic annotated dataset for whole-image classification, such as ImageNet-1k, and then finetuning the DNN on a smaller, annotated, task-specific, downstream dataset for visual firearms classification. Neither Visual Transformer (ViT) neural architectures nor Self-Supervised Learning (SSL) approaches have been so far evaluated on this critical task. SSL essentially consists of replacing the traditional supervised pretraining objective with an unsupervised pretext task that does not require ground-truth labels..</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：Interpretable Diffusion via Information Decomposition</b></summary>
  <p><b>编号</b>：[285]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07972</p>
  <p><b>作者</b>：Xianghao Kong,  Ollie Liu,  Han Li,  Dani Yogatama,  Greg Ver Steeg</p>
  <p><b>备注</b>：32 pages, 18 figures</p>
  <p><b>关键词</b>：enable conditional generation, generation and density, density modeling, modeling of complex, diffusion models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Denoising diffusion models enable conditional generation and density modeling of complex relationships like images and text. However, the nature of the learned relationships is opaque making it difficult to understand precisely what relationships between words and parts of an image are captured, or to predict the effect of an intervention. We illuminate the fine-grained relationships learned by diffusion models by noticing a precise relationship between diffusion and information decomposition. Exact expressions for mutual information and conditional mutual information can be written in terms of the denoising model. Furthermore, pointwise estimates can be easily estimated as well, allowing us to ask questions about the relationships between specific images and captions. Decomposing information even further to understand which variables in a high-dimensional space carry information is a long-standing problem. For diffusion models, we show that a natural non-negative decomposition of mutual information emerges, allowing us to quantify informative relationships between words and pixels in an image. We exploit these new relations to measure the compositional understanding of diffusion models, to do unsupervised localization of objects in images, and to measure effects when selectively editing images through prompt interventions.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：A New Approach Towards Autoformalization</b></summary>
  <p><b>编号</b>：[292]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07957</p>
  <p><b>作者</b>：Nilay Patel,  Jeffrey Flanigan,  Rahul Saha</p>
  <p><b>备注</b>：Under review at MATHAI 2023 @ NeurIPS 2023</p>
  <p><b>关键词</b>：Verifying mathematical proofs, Verifying mathematical, proofs is difficult, mathematical proofs, mathematics</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Verifying mathematical proofs is difficult, but can be automated with the assistance of a computer. Autoformalization is the task of automatically translating natural language mathematics into a formal language that can be verified by a program. This is a challenging task, and especially for higher-level mathematics found in research papers. Research paper mathematics requires large amounts of background and context. In this paper, we propose an avenue towards tackling autoformalization for research-level mathematics, by breaking the task into easier and more approachable subtasks: unlinked formalization (formalization with unlinked definitions and theorems), entity linking (linking to the proper theorems and definitions), and finally adjusting types so it passes the type checker. In addition, we present arXiv2Formal, a benchmark dataset for unlinked formalization consisting of 50 theorems formalized for the Lean theorem prover sampled from papers on arXiv.org. We welcome any contributions from the community to future versions of this dataset.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：AutoRepo: A general framework for multi-modal LLM-based automated  construction reporting</b></summary>
  <p><b>编号</b>：[295]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07944</p>
  <p><b>作者</b>：Hongxu Pu,  Xincong Yang,  Jing Li,  Runhao Guo,  Heng Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：projects is paramount, timely completion, vital instrument, inspection reports, construction inspections serving</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Ensuring the safety, quality, and timely completion of construction projects is paramount, with construction inspections serving as a vital instrument towards these goals. Nevertheless, the predominantly manual approach of present-day inspections frequently results in inefficiencies and inadequate information management. Such methods often fall short of providing holistic, exhaustive assessments, consequently engendering regulatory oversights and potential safety hazards. To address this issue, this paper presents a novel framework named AutoRepo for automated generation of construction inspection reports. The unmanned vehicles efficiently perform construction inspections and collect scene information, while the multimodal large language models (LLMs) are leveraged to automatically generate the inspection reports. The framework was applied and tested on a real-world construction site, demonstrating its potential to expedite the inspection process, significantly reduce resource allocation, and produce high-quality, regulatory standard-compliant inspection reports. This research thus underscores the immense potential of multimodal large language models in revolutionizing construction inspection practices, signaling a significant leap forward towards a more efficient and safer construction management paradigm.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation using  Large Language Models</b></summary>
  <p><b>编号</b>：[298]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07937</p>
  <p><b>作者</b>：Bangguo Yu,  Hamidreza Kasaei,  Ming Cao</p>
  <p><b>备注</b>：7 pages, 4 figures, conference</p>
  <p><b>关键词</b>：human-robot interaction tasks, advanced human-robot interaction, navigating unknown environments, autonomous robots navigating, robots navigating unknown</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In advanced human-robot interaction tasks, visual target navigation is crucial for autonomous robots navigating unknown environments. While numerous approaches have been developed in the past, most are designed for single-robot operations, which often suffer from reduced efficiency and robustness due to environmental complexities. Furthermore, learning policies for multi-robot collaboration are resource-intensive. To address these challenges, we propose Co-NavGPT, an innovative framework that integrates Large Language Models (LLMs) as a global planner for multi-robot cooperative visual target navigation. Co-NavGPT encodes the explored environment data into prompts, enhancing LLMs' scene comprehension. It then assigns exploration frontiers to each robot for efficient target search. Experimental results on Habitat-Matterport 3D (HM3D) demonstrate that Co-NavGPT surpasses existing models in success rates and efficiency without any learning process, demonstrating the vast potential of LLMs in multi-robot collaboration domains. The supplementary video, prompts, and code can be accessed via the following link: \href{this https URL}{this https URL}.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：What Matters to You? Towards Visual Representation Alignment for Robot  Learning</b></summary>
  <p><b>编号</b>：[299]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07932</p>
  <p><b>作者</b>：Ran Tian,  Chenfeng Xu,  Masayoshi Tomizuka,  Jitendra Malik,  Andrea Bajcsy</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：optimize rewards aligned, service of people, operating in service, visual, RGB images</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>When operating in service of people, robots need to optimize rewards aligned with end-user preferences. Since robots will rely on raw perceptual inputs like RGB images, their rewards will inevitably use visual representations. Recently there has been excitement in using representations from pre-trained visual models, but key to making these work in robotics is fine-tuning, which is typically done via proxy tasks like dynamics prediction or enforcing temporal cycle-consistency. However, all these proxy tasks bypass the human's input on what matters to them, exacerbating spurious correlations and ultimately leading to robot behaviors that are misaligned with user preferences. In this work, we propose that robots should leverage human feedback to align their visual representations with the end-user and disentangle what matters for the task. We propose Representation-Aligned Preference-based Learning (RAPL), a method for solving the visual representation alignment problem and visual reward learning problem through the lens of preference-based learning and optimal transport. Across experiments in X-MAGICAL and in robotic manipulation, we find that RAPL's reward consistently generates preferred robot behaviors with high sample efficiency, and shows strong zero-shot generalization when the visual representation is learned from a different embodiment than the robot's.</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：D2 Pruning: Message Passing for Balancing Diversity and Difficulty in  Data Pruning</b></summary>
  <p><b>编号</b>：[300]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.07931</p>
  <p><b>作者</b>：Adyasha Maharana,  Prateek Yadav,  Mohit Bansal</p>
  <p><b>备注</b>：17 pages (Our code is available at this https URL)</p>
  <p><b>关键词</b>：Analytical theories suggest, lower test errors, Coreset, Analytical theories, pruning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Analytical theories suggest that higher-quality data can lead to lower test errors in models trained on a fixed data budget. Moreover, a model can be trained on a lower compute budget without compromising performance if a dataset can be stripped of its redundancies. Coreset selection (or data pruning) seeks to select a subset of the training data so as to maximize the performance of models trained on this subset, also referred to as coreset. There are two dominant approaches: (1) geometry-based data selection for maximizing data diversity in the coreset, and (2) functions that assign difficulty scores to samples based on training dynamics. Optimizing for data diversity leads to a coreset that is biased towards easier samples, whereas, selection by difficulty ranking omits easy samples that are necessary for the training of deep learning models. This demonstrates that data diversity and importance scores are two complementary factors that need to be jointly considered during coreset selection. We represent a dataset as an undirected graph and propose a novel pruning algorithm, D2 Pruning, that uses forward and reverse message passing over this dataset graph for coreset selection. D2 Pruning updates the difficulty scores of each example by incorporating the difficulty of its neighboring examples in the dataset graph. Then, these updated difficulty scores direct a graph-based sampling method to select a coreset that encapsulates both diverse and difficult regions of the dataset space. We evaluate supervised and self-supervised versions of our method on various vision and language datasets. Results show that D2 Pruning improves coreset selection over previous state-of-the-art methods for up to 70% pruning rates. Additionally, we find that using D2 Pruning for filtering large multimodal datasets leads to increased diversity in the dataset and improved generalization of pretrained models.</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：Deep Reinforcement Learning for Autonomous Vehicle Intersection  Navigation</b></summary>
  <p><b>编号</b>：[303]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08595</p>
  <p><b>作者</b>：Badr Ben Elallid,  Hamza El Alaoui,  Nabil Benamar</p>
  <p><b>备注</b>：Accepted for publication in the 2023 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)</p>
  <p><b>关键词</b>：Reinforcement learning algorithms, Reinforcement learning, Deterministic Policy Gradient, Twin Delayed Deep, Delayed Deep Deterministic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we explore the challenges associated with navigating complex T-intersections in dense traffic scenarios for autonomous vehicles (AVs). Reinforcement learning algorithms have emerged as a promising approach to address these challenges by enabling AVs to make safe and efficient decisions in real-time. Here, we address the problem of efficiently and safely navigating T-intersections using a lower-cost, single-agent approach based on the Twin Delayed Deep Deterministic Policy Gradient (TD3) reinforcement learning algorithm. We show that our TD3-based method, when trained and tested in the CARLA simulation platform, demonstrates stable convergence and improved safety performance in various traffic densities. Our results reveal that the proposed approach enables the AV to effectively navigate T-intersections, outperforming previous methods in terms of travel delays, collision minimization, and overall cost. This study contributes to the growing body of knowledge on reinforcement learning applications in autonomous driving and highlights the potential of single-agent, cost-effective methods for addressing more complex driving scenarios and advancing reinforcement learning algorithms in the future.</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：Adam-family Methods with Decoupled Weight Decay in Deep Learning</b></summary>
  <p><b>编号</b>：[320]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08858</p>
  <p><b>作者</b>：Kuangyu Ding,  Nachuan Xiao,  Kim-Chuan Toh</p>
  <p><b>备注</b>：26 pages</p>
  <p><b>关键词</b>：decoupled weight decay, minimizing quadratically regularized, weight decay, nonconvex optimization problems, quadratically regularized nonsmooth</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we investigate the convergence properties of a wide class of Adam-family methods for minimizing quadratically regularized nonsmooth nonconvex optimization problems, especially in the context of training nonsmooth neural networks with weight decay. Motivated by the AdamW method, we propose a novel framework for Adam-family methods with decoupled weight decay. Within our framework, the estimators for the first-order and second-order moments of stochastic subgradients are updated independently of the weight decay term. Under mild assumptions and with non-diminishing stepsizes for updating the primary optimization variables, we establish the convergence properties of our proposed framework. In addition, we show that our proposed framework encompasses a wide variety of well-known Adam-family methods, hence offering convergence guarantees for these methods in the training of nonsmooth neural networks. More importantly, we show that our proposed framework asymptotically approximates the SGD method, thereby providing an explanation for the empirical observation that decoupled weight decay enhances generalization performance for Adam-family methods. As a practical application of our proposed framework, we propose a novel Adam-family method named Adam with Decoupled Weight Decay (AdamD), and establish its convergence properties under mild conditions. Numerical experiments demonstrate that AdamD outperforms Adam and is comparable to AdamW, in the aspects of both generalization performance and efficiency.</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：Domain Generalization for Medical Image Analysis: A Survey</b></summary>
  <p><b>编号</b>：[339]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08598</p>
  <p><b>作者</b>：Jee Seok Yoon,  Kwanseok Oh,  Yooseung Shin,  Maciej A. Mazurowski,  Heung-Il Suk</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Medical Image Analysis, made significant contributions, Medical Image, Image Analysis, medicine and healthcare</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Medical Image Analysis (MedIA) has become an essential tool in medicine and healthcare, aiding in disease diagnosis, prognosis, and treatment planning, and recent successes in deep learning (DL) have made significant contributions to its advances. However, DL models for MedIA remain challenging to deploy in real-world situations, failing for generalization under the distributional gap between training and testing samples, known as a distribution shift problem. Researchers have dedicated their efforts to developing various DL methods to adapt and perform robustly on unknown and out-of-distribution data distributions. This paper comprehensively reviews domain generalization studies specifically tailored for MedIA. We provide a holistic view of how domain generalization techniques interact within the broader MedIA system, going beyond methodologies to consider the operational implications on the entire MedIA workflow. Specifically, we categorize domain generalization methods into data-level, feature-level, model-level, and analysis-level methods. We show how those methods can be used in various stages of the MedIA workflow with DL equipped from data acquisition to model prediction and analysis. Furthermore, we include benchmark datasets and applications used to evaluate these approaches and analyze the strengths and weaknesses of various methods, unveiling future research opportunities.</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：Octopus: Embodied Vision-Language Programmer from Environmental Feedback</b></summary>
  <p><b>编号</b>：[342]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08588</p>
  <p><b>作者</b>：Jingkang Yang,  Yuhao Dong,  Shuai Liu,  Bo Li,  Ziyue Wang,  Chencheng Jiang,  Haoran Tan,  Jiamu Kang,  Yuanhan Zhang,  Kaiyang Zhou,  Ziwei Liu</p>
  <p><b>备注</b>：Project Page: this https URL, Codebase: this https URL</p>
  <p><b>关键词</b>：achieved substantial progress, Large vision-language models, Large vision-language, perception and reasoning, achieved substantial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large vision-language models (VLMs) have achieved substantial progress in multimodal perception and reasoning. Furthermore, when seamlessly integrated into an embodied agent, it signifies a crucial stride towards the creation of autonomous and context-aware systems capable of formulating plans and executing commands with precision. In this paper, we introduce Octopus, a novel VLM designed to proficiently decipher an agent's vision and textual task objectives and to formulate intricate action sequences and generate executable code. Our design allows the agent to adeptly handle a wide spectrum of tasks, ranging from mundane daily chores in simulators to sophisticated interactions in complex video games. Octopus is trained by leveraging GPT-4 to control an explorative agent to generate training data, i.e., action blueprints and the corresponding executable code, within our experimental environment called OctoVerse. We also collect the feedback that allows the enhanced training scheme of Reinforcement Learning with Environmental Feedback (RLEF). Through a series of experiments, we illuminate Octopus's functionality and present compelling results, and the proposed RLEF turns out to refine the agent's decision-making. By open-sourcing our model architecture, simulator, and dataset, we aspire to ignite further innovation and foster collaborative applications within the broader embodied AI community.</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：Tree-Planner: Efficient Close-loop Task Planning with Large Language  Models</b></summary>
  <p><b>编号</b>：[348]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08582</p>
  <p><b>作者</b>：Mengkang Hu,  Yao Mu,  Xinmiao Yu,  Mingyu Ding,  Shiguang Wu,  Wenqi Shao,  Qiguang Chen,  Bin Wang,  Yu Qiao,  Ping Luo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：paper studies close-loop, studies close-loop task, Large Language Models, prompting Large Language, sequence of skills</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper studies close-loop task planning, which refers to the process of generating a sequence of skills (a plan) to accomplish a specific goal while adapting the plan based on real-time observations. Recently, prompting Large Language Models (LLMs) to generate actions iteratively has become a prevalent paradigm due to its superior performance and user-friendliness. However, this paradigm is plagued by two inefficiencies: high token consumption and redundant error correction, both of which hinder its scalability for large-scale testing and applications. To address these issues, we propose Tree-Planner, which reframes task planning with LLMs into three distinct phases: plan sampling, action tree construction, and grounded deciding. Tree-Planner starts by using an LLM to sample a set of potential plans before execution, followed by the aggregation of them to form an action tree. Finally, the LLM performs a top-down decision-making process on the tree, taking into account real-time environmental information. Experiments show that Tree-Planner achieves state-of-the-art performance while maintaining high efficiency. By decomposing LLM queries into a single plan-sampling call and multiple grounded-deciding calls, a considerable part of the prompt are less likely to be repeatedly consumed. As a result, token consumption is reduced by 92.2% compared to the previously best-performing model. Additionally, by enabling backtracking on the action tree as needed, the correction process becomes more flexible, leading to a 40.5% decrease in error corrections. Project page: this https URL</p>
  </details>
</details>
<details>
  <summary>83. <b>标题：Jigsaw: Supporting Designers in Prototyping Multimodal Applications by  Assembling AI Foundation Models</b></summary>
  <p><b>编号</b>：[354]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08574</p>
  <p><b>作者</b>：David Chuan-En Lin,  Nikolas Martelaro</p>
  <p><b>备注</b>：Webpage: this https URL</p>
  <p><b>关键词</b>：generating visual prototypes, including ideating design, ideating design concepts, Recent advancements, including ideating</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advancements in AI foundation models have made it possible for them to be utilized off-the-shelf for creative tasks, including ideating design concepts or generating visual prototypes. However, integrating these models into the creative process can be challenging as they often exist as standalone applications tailored to specific tasks. To address this challenge, we introduce Jigsaw, a prototype system that employs puzzle pieces as metaphors to represent foundation models. Jigsaw allows designers to combine different foundation model capabilities across various modalities by assembling compatible puzzle pieces. To inform the design of Jigsaw, we interviewed ten designers and distilled design goals. In a user study, we showed that Jigsaw enhanced designers' understanding of available foundation model capabilities, provided guidance on combining capabilities across different modalities and tasks, and served as a canvas to support design exploration, prototyping, and documentation.</p>
  </details>
</details>
<details>
  <summary>84. <b>标题：A Lightweight Calibrated Simulation Enabling Efficient Offline Learning  for Optimal Control of Real Buildings</b></summary>
  <p><b>编号</b>：[357]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08569</p>
  <p><b>作者</b>：Judah Goldfeder,  John Sipple</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Modern commercial Heating, Air Conditioning, commercial Heating, interconnected thermodynamic system, current setpoint control</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Modern commercial Heating, Ventilation, and Air Conditioning (HVAC) devices form a complex and interconnected thermodynamic system with the building and outside weather conditions, and current setpoint control policies are not fully optimized for minimizing energy use and carbon emission. Given a suitable training environment, a Reinforcement Learning (RL) model is able to improve upon these policies, but training such a model, especially in a way that scales to thousands of buildings, presents many real world challenges. We propose a novel simulation-based approach, where a customized simulator is used to train the agent for each building. Our open-source simulator (available online: this https URL) is lightweight and calibrated via telemetry from the building to reach a higher level of fidelity. On a two-story, 68,000 square foot building, with 127 devices, we were able to calibrate our simulator to have just over half a degree of drift from the real world over a six-hour interval. This approach is an important step toward having a real-world RL control system that can be scaled to many buildings, allowing for greater efficiency and resulting in reduced energy consumption and carbon emissions.</p>
  </details>
</details>
<details>
  <summary>85. <b>标题：Transformers as Decision Makers: Provable In-Context Reinforcement  Learning via Supervised Pretraining</b></summary>
  <p><b>编号</b>：[359]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08566</p>
  <p><b>作者</b>：Licong Lin,  Yu Bai,  Song Mei</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：demonstrated remarkable in-context, remarkable in-context reinforcement, make good decisions, unseen environments, datasets have demonstrated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large transformer models pretrained on offline reinforcement learning datasets have demonstrated remarkable in-context reinforcement learning (ICRL) capabilities, where they can make good decisions when prompted with interaction trajectories from unseen environments. However, when and how transformers can be trained to perform ICRL have not been theoretically well-understood. In particular, it is unclear which reinforcement-learning algorithms transformers can perform in context, and how distribution mismatch in offline training data affects the learned algorithms. This paper provides a theoretical framework that analyzes supervised pretraining for ICRL. This includes two recently proposed training methods -- algorithm distillation and decision-pretrained transformers. First, assuming model realizability, we prove the supervised-pretrained transformer will imitate the conditional expectation of the expert algorithm given the observed trajectory. The generalization error will scale with model capacity and a distribution divergence factor between the expert and offline algorithms. Second, we show transformers with ReLU attention can efficiently approximate near-optimal online reinforcement learning algorithms like LinUCB and Thompson sampling for stochastic linear bandits, and UCB-VI for tabular Markov decision processes. This provides the first quantitative analysis of the ICRL capabilities of transformers pretrained from offline trajectories.</p>
  </details>
</details>
<details>
  <summary>86. <b>标题：Security Considerations in AI-Robotics: A Survey of Current Methods,  Challenges, and Opportunities</b></summary>
  <p><b>编号</b>：[360]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08565</p>
  <p><b>作者</b>：Subash Neupane,  Shaswata Mitra,  Ivan A. Fernandez,  Swayamjit Saha,  Sudip Mittal,  Jingdao Chen,  Nisha Pillai,  Shahram Rahimi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Artificial Intelligence, AI-Robotics systems, inextricably intertwined, Robotics and Artificial, systems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Robotics and Artificial Intelligence (AI) have been inextricably intertwined since their inception. Today, AI-Robotics systems have become an integral part of our daily lives, from robotic vacuum cleaners to semi-autonomous cars. These systems are built upon three fundamental architectural elements: perception, navigation and planning, and control. However, while the integration of AI-Robotics systems has enhanced the quality our lives, it has also presented a serious problem - these systems are vulnerable to security attacks. The physical components, algorithms, and data that make up AI-Robotics systems can be exploited by malicious actors, potentially leading to dire consequences. Motivated by the need to address the security concerns in AI-Robotics systems, this paper presents a comprehensive survey and taxonomy across three dimensions: attack surfaces, ethical and legal concerns, and Human-Robot Interaction (HRI) security. Our goal is to provide users, developers and other stakeholders with a holistic understanding of these areas to enhance the overall AI-Robotics system security. We begin by surveying potential attack surfaces and provide mitigating defensive strategies. We then delve into ethical issues, such as dependency and psychological impact, as well as the legal concerns regarding accountability for these systems. Besides, emerging trends such as HRI are discussed, considering privacy, integrity, safety, trustworthiness, and explainability concerns. Finally, we present our vision for future research directions in this dynamic and promising field.</p>
  </details>
</details>
<details>
  <summary>87. <b>标题：MemGPT: Towards LLMs as Operating Systems</b></summary>
  <p><b>编号</b>：[361]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08560</p>
  <p><b>作者</b>：Charles Packer,  Vivian Fang,  Shishir G. Patil,  Kevin Lin,  Sarah Wooders,  Joseph E. Gonzalez</p>
  <p><b>备注</b>：Code and data available at this https URL</p>
  <p><b>关键词</b>：limited context windows, Large language models, limited context, language models, hindering their utility</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at this https URL.</p>
  </details>
</details>
<details>
  <summary>88. <b>标题：Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of  Language Models with Hypothesis Refinement</b></summary>
  <p><b>编号</b>：[362]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08559</p>
  <p><b>作者</b>：Linlu Qiu,  Liwei Jiang,  Ximing Lu,  Melanie Sclar,  Valentina Pyatkin,  Chandra Bhagavatula,  Bailin Wang,  Yoon Kim,  Yejin Choi,  Nouha Dziri,  Xiang Ren</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：derive underlying principles, inductive reasoning, ability to derive, derive underlying, underlying principles</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The ability to derive underlying principles from a handful of observations and then generalize to novel situations -- known as inductive reasoning -- is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approach achieves strong results across inductive reasoning benchmarks that require inducing causal relations, language-like instructions, and symbolic concepts. However, they also behave as puzzling inductive reasoners, showing notable performance gaps in rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules. Through empirical and human analyses, we further reveal several discrepancies between the inductive reasoning processes of LMs and humans, shedding light on both the potentials and limitations of using LMs in inductive reasoning tasks.</p>
  </details>
</details>
<details>
  <summary>89. <b>标题：Offline Retraining for Online RL: Decoupled Policy Learning to Mitigate  Exploration Bias</b></summary>
  <p><b>编号</b>：[363]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08558</p>
  <p><b>作者</b>：Max Sobol Mark,  Archit Sharma,  Fahim Tajwar,  Rafael Rafailov,  Sergey Levine,  Chelsea Finn</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：state coverage, optimistically explore, policy, online, offline</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>It is desirable for policies to optimistically explore new states and behaviors during online reinforcement learning (RL) or fine-tuning, especially when prior offline data does not provide enough state coverage. However, exploration bonuses can bias the learned policy, and our experiments find that naive, yet standard use of such bonuses can fail to recover a performant policy. Concurrently, pessimistic training in offline RL has enabled recovery of performant policies from static datasets. Can we leverage offline RL to recover better policies from online interaction? We make a simple observation that a policy can be trained from scratch on all interaction data with pessimistic objectives, thereby decoupling the policies used for data collection and for evaluation. Specifically, we propose offline retraining, a policy extraction step at the end of online fine-tuning in our Offline-to-Online-to-Offline (OOO) framework for reinforcement learning (RL). An optimistic (exploration) policy is used to interact with the environment, and a separate pessimistic (exploitation) policy is trained on all the observed data for evaluation. Such decoupling can reduce any bias from online interaction (intrinsic rewards, primacy bias) in the evaluation policy, and can allow more exploratory behaviors during online interaction which in turn can generate better data for exploitation. OOO is complementary to several offline-to-online RL and online RL methods, and improves their average performance by 14% to 26% in our fine-tuning experiments, achieves state-of-the-art performance on several environments in the D4RL benchmarks, and improves online RL performance by 165% on two OpenAI gym environments. Further, OOO can enable fine-tuning from incomplete offline datasets where prior methods can fail to recover a performant policy. Implementation: this https URL</p>
  </details>
</details>
<details>
  <summary>90. <b>标题：Cross-Episodic Curriculum for Transformer Agents</b></summary>
  <p><b>编号</b>：[365]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08549</p>
  <p><b>作者</b>：Lucy Xiaoyang Shi,  Yunfan Jiang,  Jake Grigsby,  Linxi "Jim" Fan,  Yuke Zhu</p>
  <p><b>备注</b>：To appear in NeurIPS 2023; The first two authors contributed equally</p>
  <p><b>关键词</b>：CEC, Transformer, Transformer agent learning, Curriculum, curriculum captures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a new algorithm, Cross-Episodic Curriculum (CEC), to boost the learning efficiency and generalization of Transformer agents. Central to CEC is the placement of cross-episodic experiences into a Transformer's context, which forms the basis of a curriculum. By sequentially structuring online learning trials and mixed-quality demonstrations, CEC constructs curricula that encapsulate learning progression and proficiency increase across episodes. Such synergy combined with the potent pattern recognition capabilities of Transformer models delivers a powerful cross-episodic attention mechanism. The effectiveness of CEC is demonstrated under two representative scenarios: one involving multi-task reinforcement learning with discrete control, such as in DeepMind Lab, where the curriculum captures the learning progression in both individual and progressively complex settings; and the other involving imitation learning with mixed-quality data for continuous control, as seen in RoboMimic, where the curriculum captures the improvement in demonstrators' expertise. In all instances, policies resulting from CEC exhibit superior performance and strong generalization. Code is open-sourced at this https URL to facilitate research on Transformer agent learning.</p>
  </details>
</details>
<details>
  <summary>91. <b>标题：Do pretrained Transformers Really Learn In-context by Gradient Descent?</b></summary>
  <p><b>编号</b>：[369]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08540</p>
  <p><b>作者</b>：Lingfeng Shen,  Aayush Mishra,  Daniel Khashabi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：In-Context Learning, ICL, implicitly equivalent, Gradient Descent, language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Is In-Context Learning (ICL) implicitly equivalent to Gradient Descent (GD)? Several recent works draw analogies between the dynamics of GD and the emergent behavior of ICL in large language models. However, these works make assumptions far from the realistic natural language setting in which language models are trained. Such discrepancies between theory and practice, therefore, necessitate further investigation to validate their applicability.
We start by highlighting the weaknesses in prior works that construct Transformer weights to simulate gradient descent. Their experiments with training Transformers on ICL objective, inconsistencies in the order sensitivity of ICL and GD, sparsity of the constructed weights, and sensitivity to parameter changes are some examples of a mismatch from the real-world setting.
Furthermore, we probe and compare the ICL vs. GD hypothesis in a natural setting. We conduct comprehensive empirical analyses on language models pretrained on natural data (LLaMa-7B). Our comparisons on various performance metrics highlight the inconsistent behavior of ICL and GD as a function of various factors such as datasets, models, and number of demonstrations. We observe that ICL and GD adapt the output distribution of language models differently. These results indicate that the equivalence between ICL and GD is an open hypothesis, requires nuanced considerations and calls for further studies.</p>
  </details>
</details>
<details>
  <summary>92. <b>标题：Formally Specifying the High-Level Behavior of LLM-Based Agents</b></summary>
  <p><b>编号</b>：[372]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08535</p>
  <p><b>作者</b>：Maxwell Crouse,  Ibrahim Abdelaziz,  Kinjal Basu,  Soham Dan,  Sadhana Kumaravel,  Achille Fokoue,  Pavan Kapanipathi,  Luis Lastras</p>
  <p><b>备注</b>：Preprint under review</p>
  <p><b>关键词</b>：solving challenging problems, task-specific finetuned models, LLM-based agents, expensive to procure, Linear Temporal Logic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>LLM-based agents have recently emerged as promising tools for solving challenging problems without the need for task-specific finetuned models that can be expensive to procure. Currently, the design and implementation of such agents is ad hoc, as the wide variety of tasks that LLM-based agents may be applied to naturally means there can be no one-size-fits-all approach to agent design. In this work we aim to alleviate the difficulty of designing and implementing new agents by proposing a minimalistic, high-level generation framework that simplifies the process of building agents. The framework we introduce allows the user to specify desired agent behaviors in Linear Temporal Logic (LTL). The declarative LTL specification is then used to construct a constrained decoder that guarantees the LLM will produce an output exhibiting the desired behavior. By designing our framework in this way, we obtain several benefits, including the ability to enforce complex agent behavior, the ability to formally validate prompt examples, and the ability to seamlessly incorporate content-focused logical constraints into generation. In particular, our declarative approach, in which the desired behavior is simply described without concern for how it should be implemented or enforced, enables rapid design, implementation and experimentation with different LLM-based agents. We demonstrate how the proposed framework can be used to implement recent LLM-based agents, and show how the guardrails our approach provides can lead to improvements in agent performance. In addition, we release our code for general use.</p>
  </details>
</details>
<details>
  <summary>93. <b>标题：How connectivity structure shapes rich and lazy learning in neural  circuits</b></summary>
  <p><b>编号</b>：[380]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08513</p>
  <p><b>作者</b>：Yuhan Helena Liu,  Aristide Baratin,  Jonathan Cornford,  Stefan Mihalas,  Eric Shea-Brown,  Guillaume Lajoie</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：recent work leverages, work leverages deep, attributes critically influence, network attributes critically, leverages deep learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In theoretical neuroscience, recent work leverages deep learning tools to explore how some network attributes critically influence its learning dynamics. Notably, initial weight distributions with small (resp. large) variance may yield a rich (resp. lazy) regime, where significant (resp. minor) changes to network states and representation are observed over the course of learning. However, in biology, neural circuit connectivity generally has a low-rank structure and therefore differs markedly from the random initializations generally used for these studies. As such, here we investigate how the structure of the initial weights, in particular their effective rank, influences the network learning regime. Through both empirical and theoretical analyses, we discover that high-rank initializations typically yield smaller network changes indicative of lazier learning, a finding we also confirm with experimentally-driven initial connectivity in recurrent neural networks. Conversely, low-rank initialization biases learning towards richer learning. Importantly, however, as an exception to this rule, we find lazier learning can still occur with a low-rank initialization that aligns with task and data statistics. Our research highlights the pivotal role of initial weight structures in shaping learning regimes, with implications for metabolic costs of plasticity and risks of catastrophic forgetting.</p>
  </details>
</details>
<details>
  <summary>94. <b>标题：HoneyBee: Progressive Instruction Finetuning of Large Language Models  for Materials Science</b></summary>
  <p><b>编号</b>：[381]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08511</p>
  <p><b>作者</b>：Yu Song,  Santiago Miret,  Huan Zhang,  Bang Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：materials science, trustworthy data curation, LLaMa-based language model, propose an instruction-based, instruction-based process</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose an instruction-based process for trustworthy data curation in materials science (MatSci-Instruct), which we then apply to finetune a LLaMa-based language model targeted for materials science (HoneyBee). MatSci-Instruct helps alleviate the scarcity of relevant, high-quality materials science textual data available in the open literature, and HoneyBee is the first billion-parameter language model specialized to materials science. In MatSci-Instruct we improve the trustworthiness of generated data by prompting multiple commercially available large language models for generation with an Instructor module (e.g. Chat-GPT) and verification from an independent Verifier module (e.g. Claude). Using MatSci-Instruct, we construct a dataset of multiple tasks and measure the quality of our dataset along multiple dimensions, including accuracy against known facts, relevance to materials science, as well as completeness and reasonableness of the data. Moreover, we iteratively generate more targeted instructions and instruction-data in a finetuning-evaluation-feedback loop leading to progressively better performance for our finetuned HoneyBee models. Our evaluation on the MatSci-NLP benchmark shows HoneyBee's outperformance of existing language models on materials science tasks and iterative improvement in successive stages of instruction-data refinement. We study the quality of HoneyBee's language modeling through automatic evaluation and analyze case studies to further understand the model's capabilities and limitations. Our code and relevant datasets are publicly available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>95. <b>标题：Impact of time and note duration tokenizations on deep learning symbolic  music modeling</b></summary>
  <p><b>编号</b>：[384]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08497</p>
  <p><b>作者</b>：Nathan Fradet,  Nicolas Gutowski,  Fabien Chhel,  Jean-Pierre Briot</p>
  <p><b>备注</b>：ISMIR 2023</p>
  <p><b>关键词</b>：Music Information Retrieval, Information Retrieval, MIR, Retrieval, Symbolic music</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Symbolic music is widely used in various deep learning tasks, including generation, transcription, synthesis, and Music Information Retrieval (MIR). It is mostly employed with discrete models like Transformers, which require music to be tokenized, i.e., formatted into sequences of distinct elements called tokens. Tokenization can be performed in different ways. As Transformer can struggle at reasoning, but capture more easily explicit information, it is important to study how the way the information is represented for such model impact their performances. In this work, we analyze the common tokenization methods and experiment with time and note duration representations. We compare the performances of these two impactful criteria on several tasks, including composer and emotion classification, music generation, and sequence representation learning. We demonstrate that explicit information leads to better results depending on the task.</p>
  </details>
</details>
<details>
  <summary>96. <b>标题：Can We Edit Multimodal Large Language Models?</b></summary>
  <p><b>编号</b>：[391]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08475</p>
  <p><b>作者</b>：Siyuan Cheng,  Bozhong Tian,  Qingbin Liu,  Xi Chen,  Yongheng Wang,  Huajun Chen,  Ningyu Zhang</p>
  <p><b>备注</b>：EMNLP 2023</p>
  <p><b>关键词</b>：Large Language Models, Multimodal Large Language, Large Language, editing Multimodal Large, Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in this https URL.</p>
  </details>
</details>
<details>
  <summary>97. <b>标题：DistillSpec: Improving Speculative Decoding via Knowledge Distillation</b></summary>
  <p><b>编号</b>：[396]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08461</p>
  <p><b>作者</b>：Yongchao Zhou,  Kaifeng Lyu,  Ankit Singh Rawat,  Aditya Krishna Menon,  Afshin Rostamizadeh,  Sanjiv Kumar,  Jean-François Kagy,  Rishabh Agarwal</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：accelerates large language, generating multiple tokens, large language model, language model inference, target model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Speculative decoding (SD) accelerates large language model inference by employing a faster draft model for generating multiple tokens, which are then verified in parallel by the larger target model, resulting in the text generated according to the target model distribution. However, identifying a compact draft model that is well-aligned with the target model is challenging. To tackle this issue, we propose DistillSpec that uses knowledge distillation to better align the draft model with the target model, before applying SD. DistillSpec makes two key design choices, which we demonstrate via systematic study to be crucial to improving the draft and target alignment: utilizing on-policy data generation from the draft model, and tailoring the divergence function to the task and decoding strategy. Notably, DistillSpec yields impressive 10 - 45% speedups over standard SD on a range of standard benchmarks, using both greedy and non-greedy sampling. Furthermore, we combine DistillSpec with lossy SD to achieve fine-grained control over the latency vs. task performance trade-off. Finally, in practical scenarios with models of varying sizes, first using distillation to boost the performance of the target model and then applying DistillSpec to train a well-aligned draft model can reduce decoding latency by 6-10x with minimal performance drop, compared to standard decoding without distillation.</p>
  </details>
</details>
<details>
  <summary>98. <b>标题：A Survey on Heterogeneous Transfer Learning</b></summary>
  <p><b>编号</b>：[397]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08459</p>
  <p><b>作者</b>：Runxue Bao,  Yiming Sun,  Yuhe Gao,  Jindong Wang,  Qiang Yang,  Haifeng Chen,  Zhi-Hong Mao,  Xing Xie,  Ye Ye</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：enhance model performance, transfer learning, underpinning many real-world, approach utilizing knowledge, enhance model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The application of transfer learning, an approach utilizing knowledge from a source domain to enhance model performance in a target domain, has seen a tremendous rise in recent years, underpinning many real-world scenarios. The key to its success lies in the shared common knowledge between the domains, a prerequisite in most transfer learning methodologies. These methods typically presuppose identical feature spaces and label spaces in both domains, known as homogeneous transfer learning, which, however, is not always a practical assumption. Oftentimes, the source and target domains vary in feature spaces, data distributions, and label spaces, making it challenging or costly to secure source domain data with identical feature and label spaces as the target domain. Arbitrary elimination of these differences is not always feasible or optimal. Thus, heterogeneous transfer learning, acknowledging and dealing with such disparities, has emerged as a promising approach for a variety of tasks. Despite the existence of a survey in 2017 on this topic, the fast-paced advances post-2017 necessitate an updated, in-depth review. We therefore present a comprehensive survey of recent developments in heterogeneous transfer learning methods, offering a systematic guide for future research. Our paper reviews methodologies for diverse learning scenarios, discusses the limitations of current studies, and covers various application contexts, including Natural Language Processing, Computer Vision, Multimodality, and Biomedicine, to foster a deeper understanding and spur future research.</p>
  </details>
</details>
<details>
  <summary>99. <b>标题：Metrics for popularity bias in dynamic recommender systems</b></summary>
  <p><b>编号</b>：[398]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08455</p>
  <p><b>作者</b>：Valentijn Braun,  Debarati Bhaumik,  Diptish Dey</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Albeit the widespread, daily lives, limited research, recommender systems, systems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Albeit the widespread application of recommender systems (RecSys) in our daily lives, rather limited research has been done on quantifying unfairness and biases present in such systems. Prior work largely focuses on determining whether a RecSys is discriminating or not but does not compute the amount of bias present in these systems. Biased recommendations may lead to decisions that can potentially have adverse effects on individuals, sensitive user groups, and society. Hence, it is important to quantify these biases for fair and safe commercial applications of these systems. This paper focuses on quantifying popularity bias that stems directly from the output of RecSys models, leading to over recommendation of popular items that are likely to be misaligned with user preferences. Four metrics to quantify popularity bias in RescSys over time in dynamic setting across different sensitive user groups have been proposed. These metrics have been demonstrated for four collaborative filtering based RecSys algorithms trained on two commonly used benchmark datasets in the literature. Results obtained show that the metrics proposed provide a comprehensive understanding of growing disparities in treatment between sensitive groups over time when used conjointly.</p>
  </details>
</details>
<details>
  <summary>100. <b>标题：Towards Robust Multi-Modal Reasoning via Model Selection</b></summary>
  <p><b>编号</b>：[404]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08446</p>
  <p><b>作者</b>：Xiangyan Liu,  Rongxue Li,  Wei Ji,  Tao Lin</p>
  <p><b>备注</b>：10 pages, 5 figures</p>
  <p><b>关键词</b>：Large Language Model, Large Language, Language Model, model selection, recent research</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The reasoning capabilities of LLM (Large Language Model) are widely acknowledged in recent research, inspiring studies on tool learning and autonomous agents. LLM serves as the "brain" of agent, orchestrating multiple tools for collaborative multi-step task solving. Unlike methods invoking tools like calculators or weather APIs for straightforward tasks, multi-modal agents excel by integrating diverse AI models for complex challenges. However, current multi-modal agents neglect the significance of model selection: they primarily focus on the planning and execution phases, and will only invoke predefined task-specific models for each subtask, making the execution fragile. Meanwhile, other traditional model selection methods are either incompatible with or suboptimal for the multi-modal agent scenarios, due to ignorance of dependencies among subtasks arising by multi-step reasoning.
To this end, we identify the key challenges therein and propose the $\textit{M}^3$ framework as a plug-in with negligible runtime overhead at test-time. This framework improves model selection and bolsters the robustness of multi-modal agents in multi-step reasoning. In the absence of suitable benchmarks, we create MS-GQA, a new dataset specifically designed to investigate the model selection challenge in multi-modal agents. Our experiments reveal that our framework enables dynamic model selection, considering both user inputs and subtask dependencies, thereby robustifying the overall reasoning process. Our code and benchmark: this https URL.</p>
  </details>
</details>
<details>
  <summary>101. <b>标题：Debias the Training of Diffusion Models</b></summary>
  <p><b>编号</b>：[407]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08442</p>
  <p><b>作者</b>：Hu Yu,  Li Shen,  Jie Huang,  Man Zhou,  Hongsheng Li,  Feng Zhao</p>
  <p><b>备注</b>：University of Science and Technology of China, Alibaba Group, The Chinese University of Hong Kong</p>
  <p><b>关键词</b>：demonstrated compelling generation, variational lower bound, score matching loss, simple denoising score, denoising score matching</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diffusion models have demonstrated compelling generation quality by optimizing the variational lower bound through a simple denoising score matching loss. In this paper, we provide theoretical evidence that the prevailing practice of using a constant loss weight strategy in diffusion models leads to biased estimation during the training phase. Simply optimizing the denoising network to predict Gaussian noise with constant weighting may hinder precise estimations of original images. To address the issue, we propose an elegant and effective weighting strategy grounded in the theoretically unbiased principle. Moreover, we conduct a comprehensive and systematic exploration to dissect the inherent bias problem deriving from constant weighting loss from the perspectives of its existence, impact and reasons. These analyses are expected to advance our understanding and demystify the inner workings of diffusion models. Through empirical evaluation, we demonstrate that our proposed debiased estimation method significantly enhances sample quality without the reliance on complex techniques, and exhibits improved efficiency compared to the baseline method both in training and sampling processes.</p>
  </details>
</details>
<details>
  <summary>102. <b>标题：Neural Sampling in Hierarchical Exponential-family Energy-based Models</b></summary>
  <p><b>编号</b>：[411]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08431</p>
  <p><b>作者</b>：Xingsi Dong,  Si Wu</p>
  <p><b>备注</b>：NeurIPS 2023</p>
  <p><b>关键词</b>：Bayesian brain theory, brain theory suggests, brain employs generative, employs generative models, theory suggests</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Bayesian brain theory suggests that the brain employs generative models to understand the external world. The sampling-based perspective posits that the brain infers the posterior distribution through samples of stochastic neuronal responses. Additionally, the brain continually updates its generative model to approach the true distribution of the external world. In this study, we introduce the Hierarchical Exponential-family Energy-based (HEE) model, which captures the dynamics of inference and learning. In the HEE model, we decompose the partition function into individual layers and leverage a group of neurons with shorter time constants to sample the gradient of the decomposed normalization term. This allows our model to estimate the partition function and perform inference simultaneously, circumventing the negative phase encountered in conventional energy-based models (EBMs). As a result, the learning process is localized both in time and space, and the model is easy to converge. To match the brain's rapid computation, we demonstrate that neural adaptation can serve as a momentum term, significantly accelerating the inference process. On natural image datasets, our model exhibits representations akin to those observed in the biological visual system. Furthermore, for the machine learning community, our model can generate observations through joint or marginal generation. We show that marginal generation outperforms joint generation and achieves performance on par with other EBMs.</p>
  </details>
</details>
<details>
  <summary>103. <b>标题："SegLoc": Study on Novel Visual Self-supervised Learning Scheme (Segment  Localization) Tailored for Dense Prediction Tasks of Security Inspection  X-ray Images</b></summary>
  <p><b>编号</b>：[415]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08421</p>
  <p><b>作者</b>：Shervin Halat,  Mohammad Rahmati,  Ehsan Nazerfard</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：self-supervised learning scheme, SSL models, remarkable advancements, advancements of artificial, artificial intelligence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Lately, remarkable advancements of artificial intelligence have been attributed to the integration of self-supervised learning scheme. Despite impressive achievements within NLP, yet SSL in computer vision has not been able to stay on track comparatively. Recently, integration of contrastive learning on top of existing SSL models has established considerable progress in computer vision through which visual SSL models have outperformed their supervised counterparts. Nevertheless, most of these improvements were limited to classification tasks, and also, few works have been dedicated to evaluation of SSL models in real-world scenarios of computer vision, while the majority of works are centered around datasets containing class-wise portrait images, most notably, ImageNet. Consequently, in this work, we have considered dense prediction task of semantic segmentation in security inspection x-ray images to evaluate our proposed model Segmentation Localization. Based upon the model Instance Localization, our model SegLoc has managed to address one of the most challenging downsides of contrastive learning, i.e., false negative pairs of query embeddings. In order to do so, in contrast to baseline model InsLoc, our pretraining dataset is synthesized by cropping, transforming, then pasting already labeled segments from an available labeled dataset, foregrounds, onto instances of an unlabeled dataset, backgrounds. In our case, PIDray and SIXray datasets are considered as labeled and unlabeled datasets, respectively. Moreover, we fully harness labels by avoiding false negative pairs through implementing the idea, one queue per class, in MoCo-v2 whereby negative pairs corresponding to each query are extracted from its corresponding queue within the memory bank. Our approach has outperformed random initialization by 3% to 6%, while having underperformed supervised initialization.</p>
  </details>
</details>
<details>
  <summary>104. <b>标题：Jailbreaking Black Box Large Language Models in Twenty Queries</b></summary>
  <p><b>编号</b>：[417]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08419</p>
  <p><b>作者</b>：Patrick Chao,  Alexander Robey,  Edgar Dobriban,  Hamed Hassani,  George J. Pappas,  Eric Wong</p>
  <p><b>备注</b>：21 pages, 10 figures</p>
  <p><b>关键词</b>：large language models, growing interest, interest in ensuring, ensuring that large, large language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM-2.</p>
  </details>
</details>
<details>
  <summary>105. <b>标题：Tightening Bounds on Probabilities of Causation By Merging Datasets</b></summary>
  <p><b>编号</b>：[421]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08406</p>
  <p><b>作者</b>：Numair Sani,  Atalanti A. Mastakouri</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Probabilities of Causation, play a fundamental, decision-making in law, health care, bounds</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Probabilities of Causation (PoC) play a fundamental role in decision-making in law, health care and public policy. Nevertheless, their point identification is challenging, requiring strong assumptions, in the absence of which only bounds can be derived. Existing work to further tighten these bounds by leveraging extra information either provides numerical bounds, symbolic bounds for fixed dimensionality, or requires access to multiple datasets that contain the same treatment and outcome variables. However, in many clinical, epidemiological and public policy applications, there exist external datasets that examine the effect of different treatments on the same outcome variable, or study the association between covariates and the outcome variable. These external datasets cannot be used in conjunction with the aforementioned bounds, since the former may entail different treatment assignment mechanisms, or even obey different causal structures. Here, we provide symbolic bounds on the PoC for this challenging scenario. We focus on combining either two randomized experiments studying different treatments, or a randomized experiment and an observational study, assuming causal sufficiency. Our symbolic bounds work for arbitrary dimensionality of covariates and treatment, and we discuss the conditions under which these bounds are tighter than existing bounds in literature. Finally, our bounds parameterize the difference in treatment assignment mechanism across datasets, allowing the mechanisms to vary across datasets while still allowing causal information to be transferred from the external dataset to the target dataset.</p>
  </details>
</details>
<details>
  <summary>106. <b>标题：Performance/power assessment of CNN packages on embedded automotive  platforms</b></summary>
  <p><b>编号</b>：[423]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08401</p>
  <p><b>作者</b>：Paolo Burgio,  Gianluca Brilli</p>
  <p><b>备注</b>：14 pages; 17 figures, 10 tables</p>
  <p><b>关键词</b>：highly-parallel accelerators opens, power-efficient embedded computers, embedded computers based, edge computers based, rise of power-efficient</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rise of power-efficient embedded computers based on highly-parallel accelerators opens a number of opportunities and challenges for researchers and engineers, and paved the way to the era of edge computing. At the same time, advances in embedded AI for object detection and categorization such as YOLO, GoogleNet and AlexNet reached an unprecedented level of accuracy (mean-Average Precision - mAP) and performance (Frames-Per-Second - FPS). Today, edge computers based on heterogeneous many-core systems are a predominant choice to deploy such systems in industry 4.0, wearable devices, and - our focus - autonomous driving systems. In these latter systems, engineers struggle to make reduced automotive power and size budgets co-exist with the accuracy and performance targets requested by autonomous driving. We aim at validating the effectiveness and efficiency of most recent networks on state-of-the-art platforms with embedded commercial-off-the-shelf System-on-Chips, such as Xavier AGX, Tegra X2 and Nano for NVIDIA and XCZU9EG and XCZU3EG of the Zynq UltraScale+ family, for the Xilinx counterpart. Our work aims at supporting engineers in choosing the most appropriate CNN package and computing system for their designs, and deriving guidelines for adequately sizing their systems.</p>
  </details>
</details>
<details>
  <summary>107. <b>标题：Prompting Large Language Models with Chain-of-Thought for Few-Shot  Knowledge Base Question Generation</b></summary>
  <p><b>编号</b>：[426]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08395</p>
  <p><b>作者</b>：Yuanyuan Liang,  Jianing Wang,  Hanlun Zhu,  Lei Wang,  Weining Qian,  Yunshi Lan</p>
  <p><b>备注</b>：Accepted by EMNLP 2023 main conference</p>
  <p><b>关键词</b>：Knowledge Bases, natural language question, Large Language Models, aims to convert, Question Generation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The task of Question Generation over Knowledge Bases (KBQG) aims to convert a logical form into a natural language question. For the sake of expensive cost of large-scale question annotation, the methods of KBQG under low-resource scenarios urgently need to be developed. However, current methods heavily rely on annotated data for fine-tuning, which is not well-suited for few-shot question generation. The emergence of Large Language Models (LLMs) has shown their impressive generalization ability in few-shot tasks. Inspired by Chain-of-Thought (CoT) prompting, which is an in-context learning strategy for reasoning, we formulate KBQG task as a reasoning problem, where the generation of a complete question is splitted into a series of sub-question generation. Our proposed prompting method KQG-CoT first retrieves supportive logical forms from the unlabeled data pool taking account of the characteristics of the logical form. Then, we write a prompt to explicit the reasoning chain of generating complicated questions based on the selected demonstrations. To further ensure prompt quality, we extend KQG-CoT into KQG-CoT+ via sorting the logical forms by their complexity. We conduct extensive experiments over three public KBQG datasets. The results demonstrate that our prompting method consistently outperforms other prompting baselines on the evaluated datasets. Remarkably, our KQG-CoT+ method could surpass existing few-shot SoTA results of the PathQuestions dataset by 18.25, 10.72, and 10.18 absolute points on BLEU-4, METEOR, and ROUGE-L, respectively.</p>
  </details>
</details>
<details>
  <summary>108. <b>标题：Towards Better Evaluation of Instruction-Following: A Case-Study in  Summarization</b></summary>
  <p><b>编号</b>：[427]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08394</p>
  <p><b>作者</b>：Ondrej Skopek,  Rahul Aralikatte,  Sian Gooding,  Victor Carbune</p>
  <p><b>备注</b>：Accepted to CoNLL 2023</p>
  <p><b>关键词</b>：follow user instructions, user instructions remains, large language models, recent advances, follow user</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite recent advances, evaluating how well large language models (LLMs) follow user instructions remains an open problem. While evaluation methods of language models have seen a rise in prompt-based approaches, limited work on the correctness of these methods has been conducted. In this work, we perform a meta-evaluation of a variety of metrics to quantify how accurately they measure the instruction-following abilities of LLMs. Our investigation is performed on grounded query-based summarization by collecting a new short-form, real-world dataset riSum, containing $300$ document-instruction pairs with $3$ answers each. All $900$ answers are rated by $3$ human annotators. Using riSum, we analyze agreement between evaluation methods and human judgment. Finally, we propose new LLM-based reference-free evaluation methods that improve upon established baselines and perform on-par with costly reference-based metrics which require high-quality summaries.</p>
  </details>
</details>
<details>
  <summary>109. <b>标题：Do Not Marginalize Mechanisms, Rather Consolidate!</b></summary>
  <p><b>编号</b>：[434]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08377</p>
  <p><b>作者</b>：Moritz Willig (1),  Matej Zečević (1),  Devendra Singh Dhami (4),  Kristian Kersting (1,2,3) (Technical University of Darmstadt, (2) Hessian Center for AI, (3) German Research Center for AI (4) Eindhoven University of Technology)</p>
  <p><b>备注</b>：19 pages, 8 figures</p>
  <p><b>关键词</b>：complex causal relationships, Structural causal models, tool for understanding, understanding the complex, relationships that underlie</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Structural causal models (SCMs) are a powerful tool for understanding the complex causal relationships that underlie many real-world systems. As these systems grow in size, the number of variables and complexity of interactions between them does, too. Thus, becoming convoluted and difficult to analyze. This is particularly true in the context of machine learning and artificial intelligence, where an ever increasing amount of data demands for new methods to simplify and compress large scale SCM. While methods for marginalizing and abstracting SCM already exist today, they may destroy the causality of the marginalized model. To alleviate this, we introduce the concept of consolidating causal mechanisms to transform large-scale SCM while preserving consistent interventional behaviour. We show consolidation is a powerful method for simplifying SCM, discuss reduction of computational complexity and give a perspective on generalizing abilities of consolidated SCM.</p>
  </details>
</details>
<details>
  <summary>110. <b>标题：MCU: A Task-centric Framework for Open-ended Agent Evaluation in  Minecraft</b></summary>
  <p><b>编号</b>：[441]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2310.08367</p>
  <p><b>作者</b>：Haowei Lin,  Zihao Wang,  Jianzhu Ma,  Yitao Liang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：framework named MCU, task-centric framework named, MCU framework, open-ended game environment, MCU</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To pursue the goal of creating an open-ended agent in Minecraft, an open-ended game environment with unlimited possibilities, this paper introduces a task-centric framework named MCU for Minecraft agent evaluation. The MCU framework leverages the concept of atom tasks as fundamental building blocks, enabling the generation of diverse or even arbitrary tasks. Within the MCU framework, each task is measured with six distinct difficulty scores (time consumption, operational effort, planning complexity, intricacy, creativity, novelty). These scores offer a multi-dimensional assessment of a task from different angles, and thus can reveal an agent's capability on specific facets. The difficulty scores also serve as the feature of each task, which creates a meaningful task space and unveils the relationship between tasks. For efficient evaluation of Minecraft agents employing the MCU framework, we maintain a unified benchmark, namely SkillForge, which comprises representative tasks with diverse categories and difficulty distribution. We also provide convenient filters for users to select tasks to assess specific capabilities of agents. We show that MCU has the high expressivity to cover all tasks used in recent literature on Minecraft agent, and underscores the need for advancements in areas such as creativity, precise control, and out-of-distribution generalization under the goal of open-ended Minecraft agent development.</p>
  </details>
</details>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">徐耀彬</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://louishsu.xyz/2023/10/16/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">http://louishsu.xyz/2023/10/16/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://louishsu.xyz" target="_blank">LOUIS' BLOG</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2023/09/22/vLLM%EF%BC%9A%E5%88%A9%E7%94%A8%E5%88%86%E9%A1%B5%E7%BC%93%E5%AD%98%E5%92%8C%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E6%8F%90%E9%AB%98%E5%A4%A7%E6%A8%A1%E5%9E%8B2~4x%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6.html"><img class="next-cover" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww5.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">徐耀彬</div><div class="author-info__description">💭这个人很懒，什么都没有留下</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/isLouisHsu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/isLouisHsu" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:is.louishsu@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">记录和分享一些学习和开源内容，若有问题可通过邮箱is.louishsu@foxmail.com联系，欢迎交流！！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">统计</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">计算机视觉</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">自然语言处理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">机器学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">人工智能</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/10/16/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2023-10-16)"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv每日速递(2023-10-16)"/></a><div class="content"><a class="title" href="/2023/10/16/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2023-10-16)">Arxiv每日速递(2023-10-16)</a><time datetime="2023-10-16T15:18:17.921Z" title="发表于 2023-10-16 23:18:17">2023-10-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/09/22/vLLM%EF%BC%9A%E5%88%A9%E7%94%A8%E5%88%86%E9%A1%B5%E7%BC%93%E5%AD%98%E5%92%8C%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E6%8F%90%E9%AB%98%E5%A4%A7%E6%A8%A1%E5%9E%8B2~4x%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6.html" title="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww5.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度"/></a><div class="content"><a class="title" href="/2023/09/22/vLLM%EF%BC%9A%E5%88%A9%E7%94%A8%E5%88%86%E9%A1%B5%E7%BC%93%E5%AD%98%E5%92%8C%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E6%8F%90%E9%AB%98%E5%A4%A7%E6%A8%A1%E5%9E%8B2~4x%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6.html" title="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度">vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度</a><time datetime="2023-09-22T14:55:45.000Z" title="发表于 2023-09-22 22:55:45">2023-09-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/09/06/Prompt%EF%BC%9A%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%89%A7%E8%A1%8C%E6%8C%87%E5%8D%97.html" title="Prompt：大语言模型的执行指南"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww6.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Prompt：大语言模型的执行指南"/></a><div class="content"><a class="title" href="/2023/09/06/Prompt%EF%BC%9A%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%89%A7%E8%A1%8C%E6%8C%87%E5%8D%97.html" title="Prompt：大语言模型的执行指南">Prompt：大语言模型的执行指南</a><time datetime="2023-09-06T14:45:45.000Z" title="发表于 2023-09-06 22:45:45">2023-09-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/09/03/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9C%A81688%E7%94%B5%E5%95%86%E5%9C%BA%E6%99%AF%E7%9A%84%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5.html" title="【转载】大语言模型在1688电商场景的算法实践"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【转载】大语言模型在1688电商场景的算法实践"/></a><div class="content"><a class="title" href="/2023/09/03/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9C%A81688%E7%94%B5%E5%95%86%E5%9C%BA%E6%99%AF%E7%9A%84%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5.html" title="【转载】大语言模型在1688电商场景的算法实践">【转载】大语言模型在1688电商场景的算法实践</a><time datetime="2023-09-03T15:35:45.000Z" title="发表于 2023-09-03 23:35:45">2023-09-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/07/%E3%80%90%E6%A2%B3%E7%90%86%E3%80%91%E9%99%86%E5%A5%87%E6%9C%80%E6%96%B0%E6%BC%94%E8%AE%B2%E5%AE%9E%E5%BD%95%EF%BC%9A%E6%88%91%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%96%E7%95%8C%E8%A7%82%20.html" title="【梳理】陆奇最新演讲实录：我的大模型世界观"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【梳理】陆奇最新演讲实录：我的大模型世界观"/></a><div class="content"><a class="title" href="/2023/05/07/%E3%80%90%E6%A2%B3%E7%90%86%E3%80%91%E9%99%86%E5%A5%87%E6%9C%80%E6%96%B0%E6%BC%94%E8%AE%B2%E5%AE%9E%E5%BD%95%EF%BC%9A%E6%88%91%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%96%E7%95%8C%E8%A7%82%20.html" title="【梳理】陆奇最新演讲实录：我的大模型世界观">【梳理】陆奇最新演讲实录：我的大模型世界观</a><time datetime="2023-05-07T11:07:45.000Z" title="发表于 2023-05-07 19:07:45">2023-05-07</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2023 By 徐耀彬</div><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender"title="主题采用butterfly"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr"title="本站使用JsDelivr为静态资源提供CDN加速"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub"title="本站项目由Gtihub托管"alt="img"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"alt="img"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></br></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'default',
      })
      false && mermaid.init()
    })
  }
}</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    let initData = {
      el: '#twikoo-wrap',
      envId: 'blog-',
      region: ''
    }

    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    twikoo.init(initData)
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'blog-',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/机器学习/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐱 机器学习 (3)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/自然语言处理/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 自然语言处理 (7)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/竞赛相关/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 竞赛相关 (3)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/阅读笔记/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 阅读笔记 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="http://louishsu.xyz/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt=""><img width="48" height="48" src="https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161037709574435991610377095138.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2021-05-19</span><a class="blog-slider__title" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt="">全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/cail2021.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2021-10-22</span><a class="blog-slider__title" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt="">中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt=""><img width="48" height="48" src="https://cdn.kesci.com/upload/image/r7j60un866.png?imageView2/2/w/2500/h/2500" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-11-17</span><a class="blog-slider__title" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt="">2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww5.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-09-22</span><a class="blog-slider__title" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt="">vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww6.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-09-06</span><a class="blog-slider__title" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt="">Prompt：大语言模型的执行指南</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/11/26/升级深度学习开发环境全攻略.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/default.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-11-26</span><a class="blog-slider__title" href="2022/11/26/升级深度学习开发环境全攻略.html" alt="">升级深度学习开发环境全攻略</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2022/11/26/升级深度学习开发环境全攻略.html" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>