<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Arxiv每日速递(2024-01-11) | LOUIS' BLOG</title><meta name="author" content="徐耀彬"><meta name="copyright" content="徐耀彬"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以自然语言处理、计算机视觉、机器学习、人工智能等大方向进行划分。 统计 今日共更新254篇论文，其中：  38篇自然语言处理（cs.CL） 57篇计算机视觉（cs.CV） 74篇机器学习（cs.LG） 67篇人工智能（cs.AI）  自然语言处理    1. 标题：Model Editing Can Hurt General Abilit">
<meta property="og:type" content="article">
<meta property="og:title" content="Arxiv每日速递(2024-01-11)">
<meta property="og:url" content="http://louishsu.xyz/2024/01/11/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">
<meta property="og:site_name" content="LOUIS&#39; BLOG">
<meta property="og:description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以自然语言处理、计算机视觉、机器学习、人工智能等大方向进行划分。 统计 今日共更新254篇论文，其中：  38篇自然语言处理（cs.CL） 57篇计算机视觉（cs.CV） 74篇机器学习（cs.LG） 67篇人工智能（cs.AI）  自然语言处理    1. 标题：Model Editing Can Hurt General Abilit">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png">
<meta property="article:published_time" content="2024-01-11T00:36:57.870Z">
<meta property="article:modified_time" content="2024-01-11T00:38:34.035Z">
<meta property="article:author" content="徐耀彬">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://louishsu.xyz/2024/01/11/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2024-01-11 08:38:34'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/background.css"><script src="https://cdn.jsdelivr.net/npm/echarts@4.7.0/dist/echarts.min.js"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-brands fa-app-store"></i><span> 利器</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://code.visualstudio.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> VSCode：微软旗下的跨平台代码编辑软件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mobaxterm.mobatek.net/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> MobaXterm：超好用的全能远程终端</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.zotero.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zotero：便于收集、组织、引用、共享的文献管理工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/CopyTranslator/CopyTranslator"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> CopyTranslator：“复制即翻译”的外文辅助阅读翻译解决方案</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ditto-cp.sourceforge.io/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Ditto：强大的Windows剪贴板增强工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/indiff/qttabbar"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> QtTabBar：在Windows资源管理器中使用多标签功能扩展工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/sentialx/multrin"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Multrin：“窗口合并”辅助小工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/Wox-launcher/Wox"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Wox &amp; Everything：基于名称快速定位文件和文件夹的搜索工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/NickeManarin/ScreenToGif"><i class="fa-fw fa-regular fa-star"></i><span> ScreenToGif：快速录制屏幕指定区域并保存为动图文件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mathpix.com/"><i class="fa-fw fa-regular fa-star"></i><span> Mathpix Snipping：识别数学公式并转换成LaTeX</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">LOUIS' BLOG</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-brands fa-app-store"></i><span> 利器</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://code.visualstudio.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> VSCode：微软旗下的跨平台代码编辑软件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mobaxterm.mobatek.net/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> MobaXterm：超好用的全能远程终端</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.zotero.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zotero：便于收集、组织、引用、共享的文献管理工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/CopyTranslator/CopyTranslator"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> CopyTranslator：“复制即翻译”的外文辅助阅读翻译解决方案</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ditto-cp.sourceforge.io/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Ditto：强大的Windows剪贴板增强工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/indiff/qttabbar"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> QtTabBar：在Windows资源管理器中使用多标签功能扩展工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/sentialx/multrin"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Multrin：“窗口合并”辅助小工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/Wox-launcher/Wox"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Wox &amp; Everything：基于名称快速定位文件和文件夹的搜索工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/NickeManarin/ScreenToGif"><i class="fa-fw fa-regular fa-star"></i><span> ScreenToGif：快速录制屏幕指定区域并保存为动图文件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mathpix.com/"><i class="fa-fw fa-regular fa-star"></i><span> Mathpix Snipping：识别数学公式并转换成LaTeX</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Arxiv每日速递(2024-01-11)<a class="post-edit-link" href="https://github.com/isLouisHsu/blog/tree/master/source_posts/Arxiv每日速递.md" title="编辑" target="_blank"><i class="fas fa-pencil-square"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-01-11T00:36:57.870Z" title="发表于 2024-01-11 08:36:57">2024-01-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-01-11T00:38:34.035Z" title="更新于 2024-01-11 08:38:34">2024-01-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">阅读笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">8.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>52分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2024/01/11/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以自然语言处理、计算机视觉、机器学习、人工智能等大方向进行划分。</p>
<h1>统计</h1>
<p>今日共更新254篇论文，其中：</p>
<ul>
<li><a href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86">38篇自然语言处理（cs.CL）</a></li>
<li><a href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89">57篇计算机视觉（cs.CV）</a></li>
<li><a href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">74篇机器学习（cs.LG）</a></li>
<li><a href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD">67篇人工智能（cs.AI）</a></li>
</ul>
<h1>自然语言处理</h1>
<details>
  <summary>1. <b>标题：Model Editing Can Hurt General Abilities of Large Language Models</b></summary>
  <p><b>编号</b>：[12]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04700</p>
  <p><b>作者</b>：Jia-Chen Gu,  Hao-Xiang Xu,  Jun-Yu Ma,  Pan Lu,  Zhen-Hua Ling,  Kai-Wei Chang,  Nanyun Peng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, Recent advances, advances in large, large language, paradigms for accessing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advances in large language models (LLMs) have opened up new paradigms for accessing the knowledge stored in their parameters. One critical challenge that has emerged is the presence of hallucinations in LLM outputs due to false or outdated knowledge. Since retraining LLMs with updated information is resource-intensive, there has been a growing interest in model editing. However, many model editing methods, while effective in various scenarios, tend to overemphasize aspects such as efficacy, generalization, and locality in editing performance, often overlooking potential side effects on the general abilities of LLMs. In this paper, we raise concerns that the improvement of model factuality may come at the cost of a significant degradation of these general abilities, which is not conducive to the sustainable development of LLMs. Systematically, we analyze side effects by evaluating four popular editing methods on two LLMs across eight representative task categories. Extensive empirical research reveals that model editing does improve model factuality but at the expense of substantially impairing general abilities. Therefore, we advocate for more research efforts to minimize the loss of general abilities acquired during LLM pre-training and to ultimately preserve them during model editing.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering  with Multi-Granularity Answers</b></summary>
  <p><b>编号</b>：[13]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04695</p>
  <p><b>作者</b>：Gal Yona,  Roee Aharoni,  Mor Geva</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Factual questions typically, Barack Obama born, answered correctly, Barack Obama, Factual questions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Factual questions typically can be answered correctly at different levels of granularity. For example, both ``August 4, 1961'' and ``1961'' are correct answers to the question ``When was Barack Obama born?''. Standard question answering (QA) evaluation protocols, however, do not explicitly take this into account and compare a predicted answer against answers of a single granularity level. In this work, we propose GRANOLA QA, a novel evaluation setting where a predicted answer is evaluated in terms of accuracy and informativeness against a set of multi-granularity answers. We present a simple methodology for enriching existing datasets with multi-granularity answers, and create GRANOLA-EQ, a multi-granularity version of the EntityQuestions dataset. We evaluate a range of decoding methods on GRANOLA-EQ, including a new algorithm, called Decoding with Response Aggregation (DRAG), that is geared towards aligning the response granularity with the model's uncertainty. Our experiments show that large language models with standard decoding tend to generate specific answers, which are often incorrect. In contrast, when evaluated on multi-granularity answers, DRAG yields a nearly 20 point increase in accuracy on average, which further increases for rare entities. Overall, this reveals that standard evaluation and decoding schemes may significantly underestimate the knowledge encapsulated in LMs.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation</b></summary>
  <p><b>编号</b>：[19]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04679</p>
  <p><b>作者</b>：Mahdi Nikdan,  Soroush Tabesh,  Dan Alistarh</p>
  <p><b>备注</b>：Preliminary version</p>
  <p><b>关键词</b>：large language models, investigate parameter-efficient fine-tuning, PEFT method called, language models, called Robust Adaptation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memory- and computationally-efficient training. Our code will be made available at this https URL}{\texttt{this https URL</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence  Lengths in Large Language Models</b></summary>
  <p><b>编号</b>：[27]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04658</p>
  <p><b>作者</b>：Zhen Qin,  Weigao Sun,  Dong Li,  Xuyang Shen,  Weixuan Sun,  Yiran Zhong</p>
  <p><b>备注</b>：Technical Report. Yiran Zhong is the corresponding author. The source code is available at this https URL</p>
  <p><b>关键词</b>：Linear attention, attention, Linear, recently emerged, promising alternative</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：DepressionEmo: A novel dataset for multilabel classification of  depression emotions</b></summary>
  <p><b>编号</b>：[28]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04655</p>
  <p><b>作者</b>：Abu Bakar Siddiqur Rahman,  Hoang-Thang Ta,  Lotfollah Najjar,  Azad Azadmanesh,  Ali Saffet Gönül</p>
  <p><b>备注</b>：18 pages</p>
  <p><b>关键词</b>：human social interactions, diverse responses elicited, social interactions, situational contexts, integral to human</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Emotions are integral to human social interactions, with diverse responses elicited by various situational contexts. Particularly, the prevalence of negative emotional states has been correlated with negative outcomes for mental health, necessitating a comprehensive analysis of their occurrence and impact on individuals. In this paper, we introduce a novel dataset named DepressionEmo designed to detect 8 emotions associated with depression by 6037 examples of long Reddit user posts. This dataset was created through a majority vote over inputs by zero-shot classifications from pre-trained models and validating the quality by annotators and ChatGPT, exhibiting an acceptable level of interrater reliability between annotators. The correlation between emotions, their distribution over time, and linguistic analysis are conducted on DepressionEmo. Besides, we provide several text classification methods classified into two groups: machine learning methods such as SVM, XGBoost, and Light GBM; and deep learning methods such as BERT, GAN-BERT, and BART. The pretrained BART model, bart-base allows us to obtain the highest F1- Macro of 0.76, showing its outperformance compared to other methods evaluated in our analysis. Across all emotions, the highest F1-Macro value is achieved by suicide intent, indicating a certain value of our dataset in identifying emotions in individuals with depression symptoms through text analysis. The curated dataset is publicly available at: this https URL.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Applying Large Language Models API to Issue Classification Problem</b></summary>
  <p><b>编号</b>：[35]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04637</p>
  <p><b>作者</b>：Gabriel Aracena,  Kyle Luster,  Fabio Santos,  Igor Steinmacher,  Marco A. Gerosa</p>
  <p><b>备注</b>：4 pages, 1 figure, NLBSE and ICSE conference submission, ACM formatted, pre print</p>
  <p><b>关键词</b>：critical problems promptly, optimize resource allocation, address critical problems, problems promptly, optimize resource</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Effective prioritization of issue reports is crucial in software engineering to optimize resource allocation and address critical problems promptly. However, the manual classification of issue reports for prioritization is laborious and lacks scalability. Alternatively, many open source software (OSS) projects employ automated processes for this task, albeit relying on substantial datasets for adequate training. This research seeks to devise an automated approach that ensures reliability in issue prioritization, even when trained on smaller datasets. Our proposed methodology harnesses the power of Generative Pre-trained Transformers (GPT), recognizing their potential to efficiently handle this task. By leveraging the capabilities of such models, we aim to develop a robust system for prioritizing issue reports accurately, mitigating the necessity for extensive training data while maintaining reliability. In our research, we have developed a reliable GPT-based approach to accurately label and prioritize issue reports with a reduced training dataset. By reducing reliance on massive data requirements and focusing on few-shot fine-tuning, our methodology offers a more accessible and efficient solution for issue prioritization in software engineering. Our model predicted issue types in individual projects up to 93.2% in precision, 95% in recall, and 89.3% in F1-score.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：DebugBench: Evaluating Debugging Capability of Large Language Models</b></summary>
  <p><b>编号</b>：[41]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04621</p>
  <p><b>作者</b>：Runchu Tian,  Yining Ye,  Yujia Qin,  Xin Cong,  Yankai Lin,  Zhiyuan Liu,  Maosong Sun</p>
  <p><b>备注</b>：in progress</p>
  <p><b>关键词</b>：Large Language Models, demonstrated exceptional coding, Large Language, exceptional coding capability, Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and three open-source models in a zero-shot scenario. We find that (1) while closed-source models like GPT-4 exhibit inferior debugging performance compared to humans, open-source models such as Code Llama fail to attain any pass rate scores; (2) the complexity of debugging notably fluctuates depending on the bug category; (3) incorporating runtime feedback has a clear impact on debugging performance which is not always helpful. As an extension, we also compare LLM debugging and code generation, revealing a strong correlation between them for closed-source models. These findings will benefit the development of LLMs in debugging.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Agent Alignment in Evolving Social Norms</b></summary>
  <p><b>编号</b>：[42]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04620</p>
  <p><b>作者</b>：Shimin Li,  Tianxiang Sun,  Xipeng Qiu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, based on Large, production and life</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Agents based on Large Language Models (LLMs) are increasingly permeating various domains of human production and life, highlighting the importance of aligning them with human values. The current alignment of AI systems primarily focuses on passively aligning LLMs through human intervention. However, agents possess characteristics like receiving environmental feedback and self-evolution, rendering the LLM alignment methods inadequate. In response, we propose an evolutionary framework for agent evolution and alignment, named EvolutionaryAgent, which transforms agent alignment into a process of evolution and selection under the principle of survival of the fittest. In an environment where social norms continuously evolve, agents better adapted to the current social norms will have a higher probability of survival and proliferation, while those inadequately aligned dwindle over time. Experimental results assessing the agents from multiple perspectives in aligning with social norms demonstrate that EvolutionaryAgent possesses the capability to align progressively better with the evolving social norms while maintaining its proficiency in general tasks. Effectiveness tests conducted on various open and closed-source LLMs as the foundation for agents also prove the applicability of our approach.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Language Detection for Transliterated Content</b></summary>
  <p><b>编号</b>：[43]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04619</p>
  <p><b>作者</b>：Selva Kumar S,  Afifah Khan Mohammed Ajmal Khan,  Chirag Manjeshwar,  Imadh Ajaz Banday</p>
  <p><b>备注</b>：4 Pages, 6 diagrams</p>
  <p><b>关键词</b>：Internet functions, contemporary digital era, unparalleled catalyst, dismantling geographical, evident in texting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the contemporary digital era, the Internet functions as an unparalleled catalyst, dismantling geographical and linguistic barriers particularly evident in texting. This evolution facilitates global communication, transcending physical distances and fostering dynamic cultural exchange. A notable trend is the widespread use of transliteration, where the English alphabet is employed to convey messages in native languages, posing a unique challenge for language technology in accurately detecting the source language. This paper addresses this challenge through a dataset of phone text messages in Hindi and Russian transliterated into English utilizing BERT for language classification and Google Translate API for transliteration conversion. The research pioneers innovative approaches to identify and convert transliterated text, navigating challenges in the diverse linguistic landscape of digital communication. Emphasizing the pivotal role of comprehensive datasets for training Large Language Models LLMs like BERT, our model showcases exceptional proficiency in accurately identifying and classifying languages from transliterated text. With a validation accuracy of 99% our models robust performance underscores its reliability. The comprehensive exploration of transliteration dynamics supported by innovative approaches and cutting edge technologies like BERT, positions our research at the forefront of addressing unique challenges in the linguistic landscape of digital communication. Beyond contributing to language identification and transliteration capabilities this work holds promise for applications in content moderation, analytics and fostering a globally connected community engaged in meaningful dialogue.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：An Assessment on Comprehending Mental Health through Large Language  Models</b></summary>
  <p><b>编号</b>：[52]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04592</p>
  <p><b>作者</b>：Mihael Arcan,  Paul-David Niland,  Fionn Delahunty</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：challenges pose considerable, pose considerable global, considerable global burdens, large language models, health challenges pose</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Mental health challenges pose considerable global burdens on individuals and communities. Recent data indicates that more than 20% of adults may encounter at least one mental disorder in their lifetime. On the one hand, the advancements in large language models have facilitated diverse applications, yet a significant research gap persists in understanding and enhancing the potential of large language models within the domain of mental health. On the other hand, across various applications, an outstanding question involves the capacity of large language models to comprehend expressions of human mental health conditions in natural language. This study presents an initial evaluation of large language models in addressing this gap. Due to this, we compare the performance of Llama-2 and ChatGPT with classical Machine as well as Deep learning models. Our results on the DAIC-WOZ dataset show that transformer-based models, like BERT or XLNet, outperform the large language models.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Evaluating Language Model Agency through Negotiations</b></summary>
  <p><b>编号</b>：[67]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04536</p>
  <p><b>作者</b>：Tim R. Davidson,  Veniamin Veselovsky,  Martin Josifoski,  Maxime Peyrard,  Antoine Bosselut,  Michal Kosinski,  Robert West</p>
  <p><b>备注</b>：Code and link to project data are made available at this https URL</p>
  <p><b>关键词</b>：exploit Language Models', increasingly exploit Language, Language Models', display agent-like behavior, governments increasingly exploit</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Companies, organizations, and governments increasingly exploit Language Models' (LM) remarkable capability to display agent-like behavior. As LMs are adopted to perform tasks with growing autonomy, there exists an urgent need for reliable and scalable evaluation benchmarks. Current, predominantly static LM benchmarks are ill-suited to evaluate such dynamic applications. Thus, we propose jointly evaluating LM performance and alignment through the lenses of negotiation games. We argue that this common task better reflects real-world deployment conditions while offering insights into LMs' decision-making processes. Crucially, negotiation games allow us to study multi-turn, and cross-model interactions, modulate complexity, and side-step accidental data leakage in evaluation. We report results for six publicly accessible LMs from several major providers on a variety of negotiation games, evaluating both self-play and cross-play performance. Noteworthy findings include: (i) open-source models are currently unable to complete these tasks; (ii) cooperative bargaining games prove challenging; and (iii) the most powerful models do not always "win".</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：MERA: A Comprehensive LLM Evaluation in Russian</b></summary>
  <p><b>编号</b>：[69]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04531</p>
  <p><b>作者</b>：Alena Fenogenova,  Artem Chervyakov,  Nikita Martynov,  Anastasia Kozlova,  Maria Tikhonova,  Albina Akhmetgareeva,  Anton Emelyanov,  Denis Shevelev,  Pavel Lebedev,  Leonid Sinev,  Ulyana Isaeva,  Katerina Kolomeytseva,  Daniil Moskovskiy,  Elizaveta Goncharova,  Nikita Savushkin,  Polina Mikhailova,  Denis Dimitrov,  Alexander Panchenko,  Sergei Markov</p>
  <p><b>备注</b>：the paper version comparable with the release code v.1.1.0 of the benchmark; this https URL</p>
  <p><b>关键词</b>：past few years, notable advancements, foundation models, open Multimodal Evaluation, models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Over the past few years, one of the most notable advancements in AI research has been in foundation models (FMs), headlined by the rise of language models (LMs). As the models' size increases, LMs demonstrate enhancements in measurable aspects and the development of new qualitative features. However, despite researchers' attention and the rapid growth in LM application, the capabilities, limitations, and associated risks still need to be better understood. To address these issues, we introduce an open Multimodal Evaluation of Russian-language Architectures (MERA), a new instruction benchmark for evaluating foundation models oriented towards the Russian language. The benchmark encompasses 21 evaluation tasks for generative models in 11 skill domains and is designed as a black-box test to ensure the exclusion of data leakage. The paper introduces a methodology to evaluate FMs and LMs in zero- and few-shot fixed instruction settings that can be extended to other modalities. We propose an evaluation methodology, an open-source code base for the MERA assessment, and a leaderboard with a submission system. We evaluate open LMs as baselines and find that they are still far behind the human level. We publicly release MERA to guide forthcoming research, anticipate groundbreaking model features, standardize the evaluation procedure, and address potential societal drawbacks.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：LUNA: A Framework for Language Understanding and Naturalness Assessment</b></summary>
  <p><b>编号</b>：[71]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04522</p>
  <p><b>作者</b>：Marat Saidov,  Aleksandra Bakalova,  Ekaterina Taktasheva,  Vladislav Mikhailov,  Ekaterina Artemova</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Natural Language Generation, gained increased attention, NLG evaluation metrics, Language Generation, Natural Language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The evaluation of Natural Language Generation (NLG) models has gained increased attention, urging the development of metrics that evaluate various aspects of generated text. LUNA addresses this challenge by introducing a unified interface for 20 NLG evaluation metrics. These metrics are categorized based on their reference-dependence and the type of text representation they employ, from string-based n-gram overlap to the utilization of static embeddings and pre-trained language models.
The straightforward design of LUNA allows for easy extension with novel metrics, requiring just a few lines of code. LUNA offers a user-friendly tool for evaluating generated texts.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：The Critique of Critique</b></summary>
  <p><b>编号</b>：[73]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04518</p>
  <p><b>作者</b>：Shichao Sun,  Junlong Li,  Weizhe Yuan,  Ruifeng Yuan,  Wenjie Li,  Pengfei Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, model-generated content, natural language description</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Critique, as a natural language description for assessing the quality of model-generated content, has been proven to play an essential role in the training, evaluation, and refinement of Large Language Models (LLMs). However, there is a lack of principled understanding in evaluating the quality of the critique itself. In this paper, we pioneer the critique of critique, termed MetaCritique, which is a framework to evaluate the critique from two aspects, i.e., factuality as precision score and comprehensiveness as recall score. We calculate the harmonic mean of precision and recall as the overall rating called F1 score. To obtain a reliable evaluation outcome, we propose Atomic Information Units (AIUs), which describe the critique in a more fine-grained manner. MetaCritique takes each AIU into account and aggregates each AIU's judgment for the overall score. Moreover, given the evaluation process involves intricate reasoning, our MetaCritique provides a natural language rationale to support each judgment. We construct a meta-evaluation dataset containing 300 critiques (2653 AIUs) across four tasks (question answering, reasoning, entailment, and summarization), and we conduct a comparative study to demonstrate the feasibility and effectiveness. Experiments also show superior critique judged by MetaCritique leads to better refinement, indicating generative artificial intelligence indeed has the potential to be significantly advanced with our MetaCritique. We will release relevant code and meta-evaluation datasets at this https URL.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Exploring Prompt-Based Methods for Zero-Shot Hypernym Prediction with  Large Language Models</b></summary>
  <p><b>编号</b>：[74]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04515</p>
  <p><b>作者</b>：Mikhail Tikhomirov,  Natalia Loukachevitch</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, article investigates, investigates a zero-shot, large language, LLMs</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This article investigates a zero-shot approach to hypernymy prediction using large language models (LLMs). The study employs a method based on text probability calculation, applying it to various generated prompts. The experiments demonstrate a strong correlation between the effectiveness of language model prompts and classic patterns, indicating that preliminary prompt selection can be carried out using smaller models before moving to larger ones. We also explore prompts for predicting co-hyponyms and improving hypernymy predictions by augmenting prompts with additional information through automatically identified co-hyponyms. An iterative approach is developed for predicting higher-level concepts, which further improves the quality on the BLESS dataset (MAP = 0.8).</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Rewriting the Code: A Simple Method for Large Language Model Augmented  Code Search</b></summary>
  <p><b>编号</b>：[75]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04514</p>
  <p><b>作者</b>：Haochen Li,  Xin Zhou,  Zhiqi Shen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, natural language queries, Language Models, Large Language, exemplar code snippets</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In code search, the Generation-Augmented Retrieval (GAR) framework, which generates exemplar code snippets to augment queries, has emerged as a promising strategy to address the principal challenge of modality misalignment between code snippets and natural language queries, particularly with the demonstrated code generation capabilities of Large Language Models (LLMs). Nevertheless, our preliminary investigations indicate that the improvements conferred by such an LLM-augmented framework are somewhat constrained. This limitation could potentially be ascribed to the fact that the generated codes, albeit functionally accurate, frequently display a pronounced stylistic deviation from the ground truth code in the codebase. In this paper, we extend the foundational GAR framework and propose a simple yet effective method that additionally Rewrites the Code (ReCo) within the codebase for style normalization. Experimental results demonstrate that ReCo significantly boosts retrieval accuracy across sparse (up to 35.7%), zero-shot dense (up to 27.6%), and fine-tuned dense (up to 23.6%) retrieval settings in diverse search scenarios. To further elucidate the advantages of ReCo and stimulate research in code style normalization, we introduce Code Style Similarity, the first metric tailored to quantify stylistic similarities in code. Notably, our empirical findings reveal the inadequacy of existing metrics in capturing stylistic nuances.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：TechGPT-2.0: A large language model project to solve the task of  knowledge graph construction</b></summary>
  <p><b>编号</b>：[78]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04507</p>
  <p><b>作者</b>：Jiaqi Wang,  Yuying Chang,  Zhong Li,  Ning An,  Qi Ma,  Lei Hei,  Haibo Luo,  Yifei Lu,  Feiliang Ren</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：exhibited robust performance, Large language models, Large language, diverse natural language, performance across diverse</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models have exhibited robust performance across diverse natural language processing tasks. This report introduces TechGPT-2.0, a project designed to enhance the capabilities of large language models specifically in knowledge graph construction tasks, including named entity recognition (NER) and relationship triple extraction (RTE) tasks in NLP applications. Additionally, it serves as a LLM accessible for research within the Chinese open-source model community. We offer two 7B large language model weights and a QLoRA weight specialized for processing lengthy texts.Notably, TechGPT-2.0 is trained on Huawei's Ascend server. Inheriting all functionalities from TechGPT-1.0, it exhibits robust text processing capabilities, particularly in the domains of medicine and law. Furthermore, we introduce new capabilities to the model, enabling it to process texts in various domains such as geographical areas, transportation, organizations, literary works, biology, natural sciences, astronomical objects, and architecture. These enhancements also fortified the model's adeptness in handling hallucinations, unanswerable queries, and lengthy texts. This report provides a comprehensive and detailed introduction to the full fine-tuning process on Huawei's Ascend servers, encompassing experiences in Ascend server debugging, instruction fine-tuning data processing, and model training. Our code is available at this https URL</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Continuously Learning New Words in Automatic Speech Recognition</b></summary>
  <p><b>编号</b>：[87]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04482</p>
  <p><b>作者</b>：Christian Huber,  Alexander Waibel</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Automatic Speech Recognition, Automatic Speech, Speech Recognition, recent advances, Automatic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite recent advances, Automatic Speech Recognition (ASR) systems are still far from perfect. Typical errors include acronyms, named entities and domain-specific special words for which little or no data is available. To address the problem of recognizing these words, we propose an self-supervised continual learning approach. Given the audio of a lecture talk with corresponding slides, we bias the model towards decoding new words from the slides by using a memory-enhanced ASR model from previous work. Then, we perform inference on the talk, collecting utterances that contain detected new words into an adaptation dataset. Continual learning is then performed on this set by adapting low-rank matrix weights added to each weight matrix of the model. The whole procedure is iterated for many talks. We show that with this approach, we obtain increasing performance on the new words when they occur more frequently (more than 80% recall) while preserving the general performance of the model.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Fighting Fire with Fire: Adversarial Prompting to Generate a  Misinformation Detection Dataset</b></summary>
  <p><b>编号</b>：[88]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04481</p>
  <p><b>作者</b>：Shrey Satapara,  Parth Mehta,  Debasis Ganguly,  Sandip Modha</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：language generation capabilities, inducing mass agitation, Llama etc., large language models, language generation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The recent success in language generation capabilities of large language models (LLMs), such as GPT, Bard, Llama etc., can potentially lead to concerns about their possible misuse in inducing mass agitation and communal hatred via generating fake news and spreading misinformation. Traditional means of developing a misinformation ground-truth dataset does not scale well because of the extensive manual effort required to annotate the data. In this paper, we propose an LLM-based approach of creating silver-standard ground-truth datasets for identifying misinformation. Specifically speaking, given a trusted news article, our proposed approach involves prompting LLMs to automatically generate a summarised version of the original article. The prompts in our proposed approach act as a controlling mechanism to generate specific types of factual incorrectness in the generated summaries, e.g., incorrect quantities, false attributions etc. To investigate the usefulness of this dataset, we conduct a set of experiments where we train a range of supervised models for the task of misinformation detection.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：TransportationGames: Benchmarking Transportation Knowledge of  (Multimodal) Large Language Models</b></summary>
  <p><b>编号</b>：[91]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04471</p>
  <p><b>作者</b>：Xue Zhang,  Xiangyu Shi,  Xinyue Lou,  Rui Qi,  Yufeng Chen,  Jinan Xu,  Wenjuan Han</p>
  <p><b>备注</b>：Work in Progress</p>
  <p><b>关键词</b>：multimodal large language, Large language models, excellent general capabilities, shown excellent general, Large language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) and multimodal large language models (MLLMs) have shown excellent general capabilities, even exhibiting adaptability in many professional domains such as law, economics, transportation, and medicine. Currently, many domain-specific benchmarks have been proposed to verify the performance of (M)LLMs in specific fields. Among various domains, transportation plays a crucial role in modern society as it impacts the economy, the environment, and the quality of life for billions of people. However, it is unclear how much traffic knowledge (M)LLMs possess and whether they can reliably perform transportation-related tasks. To address this gap, we propose TransportationGames, a carefully designed and thorough evaluation benchmark for assessing (M)LLMs in the transportation domain. By comprehensively considering the applications in real-world scenarios and referring to the first three levels in Bloom's Taxonomy, we test the performance of various (M)LLMs in memorizing, understanding, and applying transportation knowledge by the selected tasks. The experimental results show that although some models perform well in some tasks, there is still much room for improvement overall. We hope the release of TransportationGames can serve as a foundation for future research, thereby accelerating the implementation and application of (M)LLMs in the transportation domain.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Estimating Text Similarity based on Semantic Concept Embeddings</b></summary>
  <p><b>编号</b>：[107]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04422</p>
  <p><b>作者</b>：Tim vor der Brück,  Marc Pouly</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：enjoy great success, semantic similarity estimation, embeddings enjoy great, similarity estimation, word embeddings enjoy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Due to their ease of use and high accuracy, Word2Vec (W2V) word embeddings enjoy great success in the semantic representation of words, sentences, and whole documents as well as for semantic similarity estimation. However, they have the shortcoming that they are directly extracted from a surface representation, which does not adequately represent human thought processes and also performs poorly for highly ambiguous words. Therefore, we propose Semantic Concept Embeddings (CE) based on the MultiNet Semantic Network (SN) formalism, which addresses both shortcomings. The evaluation on a marketing target group distribution task showed that the accuracy of predicted target groups can be increased by combining traditional word embeddings with semantic CEs.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Chain-of-Table: Evolving Tables in the Reasoning Chain for Table  Understanding</b></summary>
  <p><b>编号</b>：[114]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04398</p>
  <p><b>作者</b>：Zilong Wang,  Hao Zhang,  Chun-Liang Li,  Julian Martin Eisenschlos,  Vincent Perot,  Zifeng Wang,  Lesly Miculicich,  Yasuhisa Fujii,  Jingbo Shang,  Chen-Yu Lee,  Tomas Pfister</p>
  <p><b>备注</b>：Preprint</p>
  <p><b>关键词</b>：large language models, table-based question answering, table understanding tasks, reasoning chain, Table-based reasoning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Table-based reasoning with large language models (LLMs) is a promising direction to tackle many table understanding tasks, such as table-based question answering and fact verification. Compared with generic reasoning, table-based reasoning requires the extraction of underlying semantics from both free-form questions and semi-structured tabular data. Chain-of-Thought and its similar approaches incorporate the reasoning chain in the form of textual context, but it is still an open question how to effectively leverage tabular data in the reasoning chain. We propose the Chain-of-Table framework, where tabular data is explicitly used in the reasoning chain as a proxy for intermediate thoughts. Specifically, we guide LLMs using in-context learning to iteratively generate operations and update the table to represent a tabular reasoning chain. LLMs can therefore dynamically plan the next operation based on the results of the previous ones. This continuous evolution of the table forms a chain, showing the reasoning process for a given tabular problem. The chain carries structured information of the intermediate results, enabling more accurate and reliable predictions. Chain-of-Table achieves new state-of-the-art performance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM choices.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Probabilistic emotion and sentiment modelling of patient-reported  experiences</b></summary>
  <p><b>编号</b>：[126]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04367</p>
  <p><b>作者</b>：Curtis Murray,  Lewis Mitchell,  Jonathan Tuke,  Mark Mackay</p>
  <p><b>备注</b>：23 pages, 10 figures, 5 tables</p>
  <p><b>关键词</b>：patient experience narratives, online patient experience, study introduces, experience narratives, network topic modelling</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This study introduces a novel methodology for modelling patient emotions from online patient experience narratives. We employed metadata network topic modelling to analyse patient-reported experiences from Care Opinion, revealing key emotional themes linked to patient-caregiver interactions and clinical outcomes. We develop a probabilistic, context-specific emotion recommender system capable of predicting both multilabel emotions and binary sentiments using a naive Bayes classifier using contextually meaningful topics as predictors. The superior performance of our predicted emotions under this model compared to baseline models was assessed using the information retrieval metrics nDCG and Q-measure, and our predicted sentiments achieved an F1 score of 0.921, significantly outperforming standard sentiment lexicons. This method offers a transparent, cost-effective way to understand patient feedback, enhancing traditional collection methods and informing individualised patient care. Our findings are accessible via an R package and interactive dashboard, providing valuable tools for healthcare researchers and practitioners.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Improving the Robustness of Knowledge-Grounded Dialogue via Contrastive  Learning</b></summary>
  <p><b>编号</b>：[129]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04361</p>
  <p><b>作者</b>：Jiaan Wang,  Jianfeng Qu,  Kexin Wang,  Zhixu Li,  Wen Hua,  Ximing Li,  An Liu</p>
  <p><b>备注</b>：Accepted by AAAI 2024</p>
  <p><b>关键词</b>：knowledge graphs, external knowledge, Knowledge-grounded dialogue, KGD, dialogue context</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Knowledge-grounded dialogue (KGD) learns to generate an informative response based on a given dialogue context and external knowledge (\emph{e.g.}, knowledge graphs; KGs). Recently, the emergence of large language models (LLMs) and pre-training techniques has brought great success to knowledge-grounded dialogue. However, when building KGD systems in real applications, there are various real-world noises that are inevitable to face. For example, the dialogue context might involve perturbations such as misspellings and abbreviations. In addition, KGs typically suffer from incompletion and also might contain erroneous and outdated facts. Such real-world noises pose a challenge to the robustness of KGD systems and hinder their applications in the real world. In this paper, we propose an entity-based contrastive learning framework for improving the robustness of KGD. Specifically, we make use of the entity information in a KGD sample to create both its positive and negative samples which involve semantic-irrelevant and semantic-relevant perturbations, respectively. The contrastive learning framework ensures the KGD model is aware of these two types of perturbations, thus generating informative responses with the potentially noisy inputs in real applications. Experimental results on three benchmark datasets show that our method achieves new state-of-the-art performance in terms of automatic evaluation scores, verifying its effectiveness and potentiality. Furthermore, we show that our method can generate better responses than comparison models in both the noisy and the few-shot settings.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：LAMPAT: Low-Rank Adaption for Multilingual Paraphrasing Using  Adversarial Training</b></summary>
  <p><b>编号</b>：[137]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04348</p>
  <p><b>作者</b>：Khoi M.Le,  Trinh Pham,  Tho Quan,  Anh Tuan Luu</p>
  <p><b>备注</b>：Accepted at AAAI 2024. Data and code are available at this https URL</p>
  <p><b>关键词</b>：textbf, Natural Language Processing, texts that convey, Language Processing tasks, sentence structures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Paraphrases are texts that convey the same meaning while using different words or sentence structures. It can be used as an automatic data augmentation tool for many Natural Language Processing tasks, especially when dealing with low-resource languages, where data shortage is a significant problem. To generate a paraphrase in multilingual settings, previous studies have leveraged the knowledge from the machine translation field, i.e., forming a paraphrase through zero-shot machine translation in the same language. Despite good performance on human evaluation, those methods still require parallel translation datasets, thus making them inapplicable to languages that do not have parallel corpora. To mitigate that problem, we proposed the first unsupervised multilingual paraphrasing model, LAMPAT ($\textbf{L}$ow-rank $\textbf{A}$daptation for $\textbf{M}$ultilingual $\textbf{P}$araphrasing using $\textbf{A}$dversarial $\textbf{T}$raining), by which monolingual dataset is sufficient enough to generate a human-like and diverse sentence. Throughout the experiments, we found out that our method not only works well for English but can generalize on unseen languages as well. Data and code are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Private Fine-tuning of Large Language Models with Zeroth-order  Optimization</b></summary>
  <p><b>编号</b>：[139]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04343</p>
  <p><b>作者</b>：Xinyu Tang,  Ashwinee Panda,  Milad Nasr,  Saeed Mahloujifar,  Prateek Mittal</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：datasets may run, Fine-tuning large pretrained, large pretrained models, private datasets, privacy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Fine-tuning large pretrained models on private datasets may run the risk of violating privacy. Differential privacy is a framework for mitigating privacy risks by enforcing algorithmic stability. DP-SGD enables training models with private data in a privacy-preserving manner, but raises new obstacles in the form of performance loss and significant engineering challenges. We introduce DP-ZO, a new method for fine-tuning large language models that preserves the privacy of training data by privatizing zeroth-order optimization. A key insight into the design of our method is that the direction of the gradient in SPSA, the zeroth-order algorithm we use, is always random and the only information that depends on private data is the step size, i.e., a scalar. Therefore, we only need to privatize the scalar step size, which is memory-efficient. DP-ZO, which can be instantiated with either Laplace or Gaussian noise, provides a strong privacy-utility trade-off across different tasks, and model sizes, under conservative privacy budgets. One noteworthy result is that DP-ZO exhibits just $1.86\%$ performance degradation due to privacy at $(1,10^{-5})$-DP when fine-tuning OPT-66B on 1000 training samples from SQuAD.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Know Your Needs Better: Towards Structured Understanding of Marketer  Demands with Analogical Reasoning Augmented LLMs</b></summary>
  <p><b>编号</b>：[152]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04319</p>
  <p><b>作者</b>：Junjie Wang,  Dan Yang,  Binbin Hu,  Yue Shen,  Ziqi Liu,  Wen Zhang,  Jinjie Gu,  Zhiqiang Zhang</p>
  <p><b>备注</b>：Under review</p>
  <p><b>关键词</b>：target users solely, natural language form, user targeting, target users, users solely</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we explore a new way for user targeting, where non-expert marketers could select their target users solely given demands in natural language form. The key to this issue is how to transform natural languages into practical structured logical languages, i.e., the structured understanding of marketer demands. Considering the impressive natural language processing ability of large language models (LLMs), we try to leverage LLMs to solve this issue. Past research indicates that the reasoning ability of LLMs can be effectively enhanced through chain-of-thought (CoT) prompting. But existing methods still have some limitations: (1) Previous methods either use simple "Let's think step by step" spells or provide fixed examples in demonstrations without considering compatibility between prompts and questions, making LLMs ineffective in some complex reasoning tasks such as structured language transformation. (2) Previous methods are often implemented in closed-source models or excessively large models, which is not suitable in industrial practical scenarios. Based on these, we propose ARALLM (i.e., Analogical Reasoning Augmented Large Language Models) consisting of two modules: Analogical Reasoning based Prompting and Reasoning-Augmented Multi-Task Model Distillation.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Vision Reimagined: AI-Powered Breakthroughs in WiFi Indoor Imaging</b></summary>
  <p><b>编号</b>：[154]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04317</p>
  <p><b>作者</b>：Jianyang Shi,  Bowen Zhang,  Amartansh Dubey,  Ross Murch,  Liwen Jing</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：WiFi indoor imaging, Frechet Inception Distance, Indoor imaging, imaging, critical task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Indoor imaging is a critical task for robotics and internet-of-things. WiFi as an omnipresent signal is a promising candidate for carrying out passive imaging and synchronizing the up-to-date information to all connected devices. This is the first research work to consider WiFi indoor imaging as a multi-modal image generation task that converts the measured WiFi power into a high-resolution indoor image. Our proposed WiFi-GEN network achieves a shape reconstruction accuracy that is 275% of that achieved by physical model-based inversion methods. Additionally, the Frechet Inception Distance score has been significantly reduced by 82%. To examine the effectiveness of models for this task, the first large-scale dataset is released containing 80,000 pairs of WiFi signal and imaging target. Our model absorbs challenges for the model-based methods including the non-linearity, ill-posedness and non-certainty into massive parameters of our generative AI network. The network is also designed to best fit measured WiFi signals and the desired imaging output. For reproducibility, we will release the data and code upon acceptance.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：MARG: Multi-Agent Review Generation for Scientific Papers</b></summary>
  <p><b>编号</b>：[172]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04259</p>
  <p><b>作者</b>：Mike D'Arcy,  Tom Hope,  Larry Birnbaum,  Doug Downey</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：multiple LLM instances, feedback generation approach, develop MARG, internal discussion, multiple LLM</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the ability of LLMs to generate feedback for scientific papers and develop MARG, a feedback generation approach using multiple LLM instances that engage in internal discussion. By distributing paper text across agents, MARG can consume the full text of papers beyond the input length limitations of the base LLM, and by specializing agents and incorporating sub-tasks tailored to different comment types (experiments, clarity, impact) it improves the helpfulness and specificity of feedback. In a user study, baseline methods using GPT-4 were rated as producing generic or very generic comments more than half the time, and only 1.7 comments per paper were rated as good overall in the best baseline. Our system substantially improves the ability of GPT-4 to generate specific and helpful feedback, reducing the rate of generic comments from 60% to 29% and generating 3.7 good comments per paper (a 2.2x improvement).</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：High-precision Voice Search Query Correction via Retrievable Speech-text  Embedings</b></summary>
  <p><b>编号</b>：[180]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04235</p>
  <p><b>作者</b>：Christopher Li,  Gary Wang,  Kyle Kastner,  Heng Su,  Allen Chen,  Andrew Rosenberg,  Zhehuai Chen,  Zelin Wu,  Leonid Velikovich,  Pat Rondon,  Diamantino Caseiro,  Petar Aleksic</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Automatic speech recognition, sufficient training data, ASR hypothesis text, Automatic speech, speech recognition</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automatic speech recognition (ASR) systems can suffer from poor recall for various reasons, such as noisy audio, lack of sufficient training data, etc.
Previous work has shown that recall can be improved by retrieving rewrite candidates from a large database of likely, contextually-relevant alternatives to the hypothesis text using nearest-neighbors search over embeddings of the ASR hypothesis text to correct and candidate corrections.
However, ASR-hypothesis-based retrieval can yield poor precision if the textual hypotheses are too phonetically dissimilar to the transcript truth. In this paper, we eliminate the hypothesis-audio mismatch problem by querying the correction database directly using embeddings derived from the utterance audio; the embeddings of the utterance audio and candidate corrections are produced by multimodal speech-text embedding networks trained to place the embedding of the audio of an utterance and the embedding of its corresponding textual transcript close together.
After locating an appropriate correction candidate using nearest-neighbor search, we score the candidate with its speech-text embedding distance before adding the candidate to the original n-best list.
We show a relative word error rate (WER) reduction of 6% on utterances whose transcripts appear in the candidate set, without increasing WER on general utterances.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Distortions in Judged Spatial Relations in Large Language Models: The  Dawn of Natural Language Geographic Data?</b></summary>
  <p><b>编号</b>：[184]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04218</p>
  <p><b>作者</b>：Nir Fulman,  Abdulkadir Memduhoğlu,  Alexander Zipf</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Large Language, capability of Large, discern intercardinal directions, Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a benchmark for assessing the capability of Large Language Models (LLMs) to discern intercardinal directions between geographic locations and apply it to three prominent LLMs: GPT-3.5, GPT-4, and Llama-2. This benchmark specifically evaluates whether LLMs exhibit a hierarchical spatial bias similar to humans, where judgments about individual locations' spatial relationships are influenced by the perceived relationships of the larger groups that contain them. To investigate this, we formulated 14 questions focusing on well-known American cities. Seven questions were designed to challenge the LLMs with scenarios potentially influenced by the orientation of larger geographical units, such as states or countries, while the remaining seven targeted locations less susceptible to such hierarchical categorization. Among the tested models, GPT-4 exhibited superior performance with 55.3% accuracy, followed by GPT-3.5 at 47.3%, and Llama-2 at 44.7%. The models showed significantly reduced accuracy on tasks with suspected hierarchical bias. For example, GPT-4's accuracy dropped to 32.9% on these tasks, compared to 85.7% on others. Despite these inaccuracies, the models identified the nearest cardinal direction in most cases, suggesting associative learning, embodying human-like misconceptions. We discuss the potential of text-based data representing geographic relationships directly to improve the spatial reasoning capabilities of LLMs.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：FunnyNet-W: Multimodal Learning of Funny Moments in Videos in the Wild</b></summary>
  <p><b>编号</b>：[185]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04210</p>
  <p><b>作者</b>：Zhi-Song Liu,  Robin Courant,  Vicky Kalogeiton</p>
  <p><b>备注</b>：22 pages, 14 figures</p>
  <p><b>关键词</b>：make people laugh, funny moments, Large Language Model, dialogues and culture, predict funny moments</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automatically understanding funny moments (i.e., the moments that make people laugh) when watching comedy is challenging, as they relate to various features, such as body language, dialogues and culture. In this paper, we propose FunnyNet-W, a model that relies on cross- and self-attention for visual, audio and text data to predict funny moments in videos. Unlike most methods that rely on ground truth data in the form of subtitles, in this work we exploit modalities that come naturally with videos: (a) video frames as they contain visual information indispensable for scene understanding, (b) audio as it contains higher-level cues associated with funny moments, such as intonation, pitch and pauses and (c) text automatically extracted with a speech-to-text model as it can provide rich information when processed by a Large Language Model. To acquire labels for training, we propose an unsupervised approach that spots and labels funny audio moments. We provide experiments on five datasets: the sitcoms TBBT, MHD, MUStARD, Friends, and the TED talk UR-Funny. Extensive experiments and analysis show that FunnyNet-W successfully exploits visual, auditory and textual cues to identify funny moments, while our findings reveal FunnyNet-W's ability to predict funny moments in the wild. FunnyNet-W sets the new state of the art for funny moment detection with multimodal cues on all datasets with and without using ground truth information.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Cross-Speaker Encoding Network for Multi-Talker Speech Recognition</b></summary>
  <p><b>编号</b>：[193]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04152</p>
  <p><b>作者</b>：Jiawen Kang,  Lingwei Meng,  Mingyu Cui,  Haohan Guo,  Xixin Wu,  Xunying Liu,  Helen Meng</p>
  <p><b>备注</b>：Accepted by ICASSP2024</p>
  <p><b>关键词</b>：garnered great interest, directly transcribe overlapped, transcribe overlapped speech, multiple speakers, garnered great</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>End-to-end multi-talker speech recognition has garnered great interest as an effective approach to directly transcribe overlapped speech from multiple speakers. Current methods typically adopt either 1) single-input multiple-output (SIMO) models with a branched encoder, or 2) single-input single-output (SISO) models based on attention-based encoder-decoder architecture with serialized output training (SOT). In this work, we propose a Cross-Speaker Encoding (CSE) network to address the limitations of SIMO models by aggregating cross-speaker representations. Furthermore, the CSE model is integrated with SOT to leverage both the advantages of SIMO and SISO while mitigating their drawbacks. To the best of our knowledge, this work represents an early effort to integrate SIMO and SISO for multi-talker speech recognition. Experiments on the two-speaker LibrispeechMix dataset show that the CES model reduces word error rate (WER) by 8% over the SIMO baseline. The CSE-SOT model reduces WER by 10% overall and by 16% on high-overlap speech compared to the SOT model.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Chain of LoRA: Efficient Fine-tuning of Language Models via Residual  Learning</b></summary>
  <p><b>编号</b>：[194]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04151</p>
  <p><b>作者</b>：Wenhan Xia,  Chengwei Qin,  Elad Hazan</p>
  <p><b>备注</b>：Work in progress</p>
  <p><b>关键词</b>：tailoring pre-trained large, primary methodology, methodology for tailoring, LoRA, large language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Fine-tuning is the primary methodology for tailoring pre-trained large language models to specific tasks. As the model's scale and the diversity of tasks expand, parameter-efficient fine-tuning methods are of paramount importance. One of the most widely used family of methods is low-rank adaptation (LoRA) and its variants. LoRA encodes weight update as the product of two low-rank matrices. Despite its advantages, LoRA falls short of full-parameter fine-tuning in terms of generalization error for certain tasks.
We introduce Chain of LoRA (COLA), an iterative optimization framework inspired by the Frank-Wolfe algorithm, to bridge the gap between LoRA and full parameter fine-tuning, without incurring additional computational costs or memory overheads. COLA employs a residual learning procedure where it merges learned LoRA modules into the pre-trained language model parameters and re-initilize optimization for new born LoRA modules. We provide theoretical convergence guarantees as well as empirical results to validate the effectiveness of our algorithm. Across various models (OPT and llama-2) and seven benchmarking tasks, we demonstrate that COLA can consistently outperform LoRA without additional computational or memory costs.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Generation Z's Ability to Discriminate Between AI-generated and  Human-Authored Text on Discord</b></summary>
  <p><b>编号</b>：[212]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04120</p>
  <p><b>作者</b>：Dhruv Ramu,  Rishab Jain,  Aditya Jain</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：growing popularity, popularity of generative, transformative effects, generative artificial intelligence, social media</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The growing popularity of generative artificial intelligence (AI) chatbots such as ChatGPT is having transformative effects on social media. As the prevalence of AI-generated content grows, concerns have been raised regarding privacy and misinformation online. Among social media platforms, Discord enables AI integrations -- making their primarily "Generation Z" userbase particularly exposed to AI-generated content. We surveyed Generation Z aged individuals (n = 335) to evaluate their proficiency in discriminating between AI-generated and human-authored text on Discord. The investigation employed one-shot prompting of ChatGPT, disguised as a text message received on the this http URL platform. We explore the influence of demographic factors on ability, as well as participants' familiarity with Discord and artificial intelligence technologies. We find that Generation Z individuals are unable to discern between AI and human-authored text (p = 0.011), and that those with lower self-reported familiarity with Discord demonstrated an improved ability in identifying human-authored compared to those with self-reported experience with AI (p << 0.0001). Our results suggest that there is a nuanced relationship between AI technology and popular modes of communication for Generation Z, contributing valuable insights into human-computer interactions, digital communication, and artificial intelligence literacy.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Working with Trouble and Failures in Conversation between Humans and  Robots (WTF 2023) & Is CUI Design Ready Yet?</b></summary>
  <p><b>编号</b>：[221]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04108</p>
  <p><b>作者</b>：Frank Förster,  Marta Romeo,  Patrick Holthaus,  Maria Jose Galvez Trigo,  Joel E. Fischer,  Birthe Nesset,  Christian Dondrup,  Christine Murad,  Cosmin Munteanu,  Benjamin R. Cowan,  Leigh Clark,  Martin Porcheron,  Heloisa Candello,  Raina Langevin</p>
  <p><b>备注</b>：WTF 2023 & 'Is CUI Design Ready Yet?' workshop proceedings including 10 extended abstracts and articles</p>
  <p><b>关键词</b>：Humans and Robots, ACM conference, CUI Design Ready, conversational user interfaces, CUI Design</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Workshop proceedings of two co-located workshops "Working with Troubles and Failures in Conversation with Humans and Robots" (WTF 2023) and "Is CUI Design Ready Yet?", both of which were part of the ACM conference on conversational user interfaces 2023.
WTF 23 aimed at bringing together researchers from human-robot interaction, dialogue systems, human-computer interaction, and conversation analysis. Despite all progress, robotic speech interfaces continue to be brittle in a number of ways and the experience of failure of such interfaces is commonplace amongst roboticists. However, the technical literature is positively skewed toward their good performance. The workshop aims to provide a platform for discussing communicative troubles and failures in human-robot interactions and related failures in non-robotic speech interfaces. Aims include a scrupulous investigation into communicative failures, to begin working on a taxonomy of such failures, and enable a preliminary discussion on possible mitigating strategies. Workshop website: this https URL
Is CUI Design Ready Yet? As CUIs become more prevalent in both academic research and the commercial market, it becomes more essential to design usable and adoptable CUIs. While research has been growing on the methods for designing CUIs for commercial use, there has been little discussion on the overall community practice of developing design resources to aid in practical CUI design. The aim of this workshop, therefore, is to bring the CUI community together to discuss the current practices for developing tools and resources for practical CUI design, the adoption (or non-adoption) of these tools and resources, and how these resources are utilized in the training and education of new CUI designers entering the field. Workshop website: this https URL</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：TwinBooster: Synergising Large Language Models with Barlow Twins and  Gradient Boosting for Enhanced Molecular Property Prediction</b></summary>
  <p><b>编号</b>：[233]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04478</p>
  <p><b>作者</b>：Maximilian G. Schuh,  Davide Boldini,  Stephan A. Sieber</p>
  <p><b>备注</b>：20 pages, 4 figures, 10 tables</p>
  <p><b>关键词</b>：molecular activities, precise prediction, Barlow Twins, molecular, molecular property prediction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The success of drug discovery and development relies on the precise prediction of molecular activities and properties. While in silico molecular property prediction has shown remarkable potential, its use has been limited so far to assays for which large amounts of data are available. In this study, we use a fine-tuned large language model to integrate biological assays based on their textual information, coupled with Barlow Twins, a Siamese neural network using a novel self-supervised learning approach. This architecture uses both assay information and molecular fingerprints to extract the true molecular information. TwinBooster enables the prediction of properties of unseen bioassays and molecules by providing state-of-the-art zero-shot learning tasks. Remarkably, our artificial intelligence pipeline shows excellent performance on the FS-Mol benchmark. This breakthrough demonstrates the application of deep learning to critical property prediction tasks where data is typically scarce. By accelerating the early identification of active molecules in drug discovery and development, this method has the potential to help streamline the identification of novel therapeutics.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Large language models in bioinformatics: applications and perspectives</b></summary>
  <p><b>编号</b>：[249]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04155</p>
  <p><b>作者</b>：Jiajia Liu,  Mengyuan Yang,  Yankai Yu,  Haixia Xu,  Kang Li,  Xiaobo Zhou</p>
  <p><b>备注</b>：7 figures</p>
  <p><b>关键词</b>：Large language models, Large language, language models, natural language processing, intelligence models based</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) are a class of artificial intelligence models based on deep learning, which have great performance in various tasks, especially in natural language processing (NLP). Large language models typically consist of artificial neural networks with numerous parameters, trained on large amounts of unlabeled input using self-supervised or semi-supervised learning. However, their potential for solving bioinformatics problems may even exceed their proficiency in modeling human language. In this review, we will present a summary of the prominent large language models used in natural language processing, such as BERT and GPT, and focus on exploring the applications of large language models at different omics levels in bioinformatics, mainly including applications of large language models in genomics, transcriptomics, proteomics, drug discovery and single cell analysis. Finally, this review summarizes the potential and prospects of large language models in solving bioinformatic problems.</p>
  </details>
</details>
<h1>计算机视觉</h1>
<details>
  <summary>1. <b>标题：A Simple Baseline for Spoken Language to Sign Language Translation with  3D Avatars</b></summary>
  <p><b>编号</b>：[1]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04730</p>
  <p><b>作者</b>：Ronglai Zuo,  Fangyun Wei,  Zenggui Chen,  Brian Mak,  Jiaolong Yang,  Xin Tong</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：translating spoken languages, develop a functional, functional system, system for translating, sign</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The objective of this paper is to develop a functional system for translating spoken languages into sign languages, referred to as Spoken2Sign translation. The Spoken2Sign task is orthogonal and complementary to traditional sign language to spoken language (Sign2Spoken) translation. To enable Spoken2Sign translation, we present a simple baseline consisting of three steps: 1) creating a gloss-video dictionary using existing Sign2Spoken benchmarks; 2) estimating a 3D sign for each sign video in the dictionary; 3) training a Spoken2Sign model, which is composed of a Text2Gloss translator, a sign connector, and a rendering module, with the aid of the yielded gloss-3D sign dictionary. The translation results are then displayed through a sign avatar. As far as we know, we are the first to present the Spoken2Sign task in an output format of 3D signs. In addition to its capability of Spoken2Sign translation, we also demonstrate that two by-products of our approach-3D keypoint augmentation and multi-view understanding-can assist in keypoint-based sign language understanding. Code and models will be available at this https URL</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar  Creation</b></summary>
  <p><b>编号</b>：[3]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04728</p>
  <p><b>作者</b>：Xiyi Chen,  Marko Mihajlovic,  Shaofei Wang,  Sergey Prokudin,  Siyu Tang</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：previously unfeasible capability, Recent advances, capability of generating, text prompt, single input image</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advances in generative diffusion models have enabled the previously unfeasible capability of generating 3D assets from a single input image or a text prompt. In this work, we aim to enhance the quality and functionality of these models for the task of creating controllable, photorealistic human avatars. We achieve this by integrating a 3D morphable model into the state-of-the-art multiview-consistent diffusion approach. We demonstrate that accurate conditioning of a generative pipeline on the articulated 3D model enhances the baseline model performance on the task of novel view synthesis from a single image. More importantly, this integration facilitates a seamless and accurate incorporation of facial expression and body pose control into the generation process. To the best of our knowledge, our proposed framework is the first diffusion model to enable the creation of fully 3D-consistent, animatable, and photorealistic human avatars from a single image of an unseen subject; extensive quantitative and qualitative evaluations demonstrate the advantages of our approach over existing state-of-the-art avatar creation models on both novel view and novel expression synthesis tasks.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Revisiting Adversarial Training at Scale</b></summary>
  <p><b>编号</b>：[4]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04727</p>
  <p><b>作者</b>：Zeyu Wang,  Xianhang Li,  Hongru Zhu,  Cihang Xie</p>
  <p><b>备注</b>：tech report</p>
  <p><b>关键词</b>：machine learning community, machine learning, learning community, community has witnessed, witnessed a drastic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The machine learning community has witnessed a drastic change in the training pipeline, pivoted by those ''foundation models'' with unprecedented scales. However, the field of adversarial training is lagging behind, predominantly centered around small model sizes like ResNet-50, and tiny and low-resolution datasets like CIFAR-10. To bridge this transformation gap, this paper provides a modern re-examination with adversarial training, investigating its potential benefits when applied at scale. Additionally, we introduce an efficient and effective training strategy to enable adversarial training with giant models and web-scale data at an affordable computing cost. We denote this newly introduced framework as AdvXL.
Empirical results demonstrate that AdvXL establishes new state-of-the-art robust accuracy records under AutoAttack on ImageNet-1K. For example, by training on DataComp-1B dataset, our AdvXL empowers a vanilla ViT-g model to substantially surpass the previous records of $l_{\infty}$-, $l_{2}$-, and $l_{1}$-robust accuracy by margins of 11.4%, 14.2% and 12.9%, respectively. This achievement posits AdvXL as a pioneering approach, charting a new trajectory for the efficient training of robust visual representations at significantly larger scales. Our code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Low-resource finetuning of foundation models beats state-of-the-art in  histopathology</b></summary>
  <p><b>编号</b>：[5]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04720</p>
  <p><b>作者</b>：Benedikt Roth,  Valentin Koch,  Sophia J. Wagner,  Julia A. Schnabel,  Carsten Marr,  Tingying Peng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：smaller patches, slide images, approaches first tessellate, finally aggregate, vectors with weakly-supervised</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To handle the large scale of whole slide images in computational pathology, most approaches first tessellate the images into smaller patches, extract features from these patches, and finally aggregate the feature vectors with weakly-supervised learning. The performance of this workflow strongly depends on the quality of the extracted features. Recently, foundation models in computer vision showed that leveraging huge amounts of data through supervised or self-supervised learning improves feature quality and generalizability for a variety of tasks. In this study, we benchmark the most popular vision foundation models as feature extractors for histopathology data. We evaluate the models in two settings: slide-level classification and patch-level classification. We show that foundation models are a strong baseline. Our experiments demonstrate that by finetuning a foundation model on a single GPU for only two hours or three days depending on the dataset, we can match or outperform state-of-the-art feature extractors for computational pathology. These findings imply that even with little resources one can finetune a feature extractor tailored towards a specific downstream task and dataset. This is a considerable shift from the current state, where only few institutions with large amounts of resources and datasets are able to train a feature extractor. We publish all code used for training and evaluation as well as the finetuned models.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Jump Cut Smoothing for Talking Heads</b></summary>
  <p><b>编号</b>：[6]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04718</p>
  <p><b>作者</b>：Xiaojuan Wang,  Taesung Park,  Yang Zhou,  Eli Shechtman,  Richard Zhang</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：jump cut offers, offers an abrupt, viewing experience, unwanted change, talking head videos</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A jump cut offers an abrupt, sometimes unwanted change in the viewing experience. We present a novel framework for smoothing these jump cuts, in the context of talking head videos. We leverage the appearance of the subject from the other source frames in the video, fusing it with a mid-level representation driven by DensePose keypoints and face landmarks. To achieve motion, we interpolate the keypoints and landmarks between the end frames around the cut. We then use an image translation network from the keypoints and source frames, to synthesize pixels. Because keypoints can contain errors, we propose a cross-modal attention scheme to select and pick the most appropriate source amongst multiple options for each key point. By leveraging this mid-level representation, our method can achieve stronger results than a strong video interpolation baseline. We demonstrate our method on various jump cuts in the talking head videos, such as cutting filler words, pauses, and even random cuts. Our experiments show that we can achieve seamless transitions, even in the challenging cases where the talking head rotates or moves drastically in the jump cut.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Low-Resource Vision Challenges for Foundation Models</b></summary>
  <p><b>编号</b>：[7]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04716</p>
  <p><b>作者</b>：Yunhua Zhang,  Hazel Doughty,  Cees G.M. Snoek</p>
  <p><b>备注</b>：Under Review</p>
  <p><b>关键词</b>：languages lack sufficient, natural language processing, language processing, lack sufficient data, languages lack</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Low-resource settings are well-established in natural language processing, where many languages lack sufficient data for machine learning at scale. However, low-resource problems are under-explored in computer vision. In this paper, we strive to address this gap and explore the challenges of low-resource image tasks with vision foundation models. Thus, we first collect a benchmark of genuinely low-resource image data, covering historic maps, circuit diagrams, and mechanical drawings. These low-resource settings all share the three challenges of data scarcity, fine-grained differences, and the distribution shift from natural images to the specialized domain of interest. While existing foundation models have shown impressive generalizability, we find they cannot transfer well to our low-resource tasks. To begin to tackle the challenges of low-resource vision, we introduce one simple baseline per challenge. Specifically, we propose to i) enlarge the data space by generative models, ii) adopt the best sub-kernels to encode local regions for fine-grained difference discovery and iii) learn attention for specialized domains. Experiments on the three low-resource data sources in our benchmark demonstrate our proposals already provide a better baseline than common transfer learning, data augmentation, and fine-grained methods. This highlights the unique characteristics and challenges of low-resource vision for foundation models that warrant further investigation. Project website: this https URL.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：CoordGate: Efficiently Computing Spatially-Varying Convolutions in  Convolutional Neural Networks</b></summary>
  <p><b>编号</b>：[18]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04680</p>
  <p><b>作者</b>：Sunny Howard,  Peter Norreys,  Andreas Döpp</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Optical imaging systems, point spread function, Optical imaging, spread function, applies a static</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Optical imaging systems are inherently limited in their resolution due to the point spread function (PSF), which applies a static, yet spatially-varying, convolution to the image. This degradation can be addressed via Convolutional Neural Networks (CNNs), particularly through deblurring techniques. However, current solutions face certain limitations in efficiently computing spatially-varying convolutions. In this paper we propose CoordGate, a novel lightweight module that uses a multiplicative gate and a coordinate encoding network to enable efficient computation of spatially-varying convolutions in CNNs. CoordGate allows for selective amplification or attenuation of filters based on their spatial position, effectively acting like a locally connected neural network. The effectiveness of the CoordGate solution is demonstrated within the context of U-Nets and applied to the challenging problem of image deblurring. The experimental results show that CoordGate outperforms conventional approaches, offering a more robust and spatially aware solution for CNNs in various computer vision applications.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Benchmark Analysis of Various Pre-trained Deep Learning Models on ASSIRA  Cats and Dogs Dataset</b></summary>
  <p><b>编号</b>：[23]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04666</p>
  <p><b>作者</b>：Galib Muhammad Shahriar Himel,  Md. Masudul Islam</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：NVIDIA GeForce RTX, image classification, grown in popularity, basic application, application and implementation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As the most basic application and implementation of deep learning, image classification has grown in popularity. Various datasets are provided by renowned data science communities for benchmarking machine learning algorithms and pre-trained models. The ASSIRA Cats & Dogs dataset is one of them and is being used in this research for its overall acceptance and benchmark standards. A comparison of various pre-trained models is demonstrated by using different types of optimizers and loss functions. Hyper-parameters are changed to gain the best result from a model. By applying this approach, we have got higher accuracy without major changes in the training model. To run the experiment, we used three different computer architectures: a laptop equipped with NVIDIA GeForce GTX 1070, a laptop equipped with NVIDIA GeForce RTX 3080Ti, and a desktop equipped with NVIDIA GeForce RTX 3090. The acquired results demonstrate supremacy in terms of accuracy over the previously done experiments on this dataset. From this experiment, the highest accuracy which is 99.65% is gained using the NASNet Large.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Learning to Prompt Segment Anything Models</b></summary>
  <p><b>编号</b>：[29]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04651</p>
  <p><b>作者</b>：Jiaxing Huang,  Kai Jiang,  Jingyi Zhang,  Han Qiu,  Lewei Lu,  Shijian Lu,  Eric Xing</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：demonstrated great potential, Segment Anything Models, prompts, SAMs, demonstrated great</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Segment Anything Models (SAMs) like SEEM and SAM have demonstrated great potential in learning to segment anything. The core design of SAMs lies with Promptable Segmentation, which takes a handcrafted prompt as input and returns the expected segmentation mask. SAMs work with two types of prompts including spatial prompts (e.g., points) and semantic prompts (e.g., texts), which work together to prompt SAMs to segment anything on downstream datasets. Despite the important role of prompts, how to acquire suitable prompts for SAMs is largely under-explored. In this work, we examine the architecture of SAMs and identify two challenges for learning effective prompts for SAMs. To this end, we propose spatial-semantic prompt learning (SSPrompt) that learns effective semantic and spatial prompts for better SAMs. Specifically, SSPrompt introduces spatial prompt learning and semantic prompt learning, which optimize spatial prompts and semantic prompts directly over the embedding space and selectively leverage the knowledge encoded in pre-trained prompt encoders. Extensive experiments show that SSPrompt achieves superior image segmentation performance consistently across multiple widely adopted datasets.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Advancing Ante-Hoc Explainable Models through Generative Adversarial  Networks</b></summary>
  <p><b>编号</b>：[33]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04647</p>
  <p><b>作者</b>：Tanmay Garg,  Deepika Vemuri,  Vineeth N Balasubramanian</p>
  <p><b>备注</b>：Paper accepted in Human-Centric Representation Learning workshop at AAAI 2024 (this https URL)</p>
  <p><b>关键词</b>：enhancing model interpretability, concept learning framework, learning framework, framework for enhancing, interpretability and performance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a novel concept learning framework for enhancing model interpretability and performance in visual classification tasks. Our approach appends an unsupervised explanation generator to the primary classifier network and makes use of adversarial training. During training, the explanation module is optimized to extract visual concepts from the classifier's latent representations, while the GAN-based module aims to discriminate images generated from concepts, from true images. This joint training scheme enables the model to implicitly align its internally learned concepts with human-interpretable visual properties. Comprehensive experiments demonstrate the robustness of our approach, while producing coherent concept activations. We analyse the learned concepts, showing their semantic concordance with object parts and visual attributes. We also study how perturbations in the adversarial training protocol impact both classification and concept acquisition. In summary, this work presents a significant step towards building inherently interpretable deep vision models with task-aligned concept representations - a key enabler for developing trustworthy AI for real-world perception tasks.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Generic Knowledge Boosted Pre-training For Remote Sensing Images</b></summary>
  <p><b>编号</b>：[44]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04614</p>
  <p><b>作者</b>：Ziyue Huang,  Mingming Zhang,  Yuan Gong,  Qingjie Liu,  Yunhong Wang</p>
  <p><b>备注</b>：13 pages, 6 figures</p>
  <p><b>关键词</b>：remote sensing, remote sensing pre-training, remote sensing images, sensing, Deep learning models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning models are essential for scene classification, change detection, land cover segmentation, and other remote sensing image understanding tasks. Most backbones of existing remote sensing deep learning models are typically initialized by pre-trained weights obtained from ImageNet pre-training (IMP). However, domain gaps exist between remote sensing images and natural images (e.g., ImageNet), making deep learning models initialized by pre-trained weights of IMP perform poorly for remote sensing image understanding. Although some pre-training methods are studied in the remote sensing community, current remote sensing pre-training methods face the problem of vague generalization by only using remote sensing images. In this paper, we propose a novel remote sensing pre-training framework, Generic Knowledge Boosted Remote Sensing Pre-training (GeRSP), to learn robust representations from remote sensing and natural images for remote sensing understanding tasks. GeRSP contains two pre-training branches: (1) A self-supervised pre-training branch is adopted to learn domain-related representations from unlabeled remote sensing images. (2) A supervised pre-training branch is integrated into GeRSP for general knowledge learning from labeled natural images. Moreover, GeRSP combines two pre-training branches using a teacher-student architecture to simultaneously learn representations with general and special knowledge, which generates a powerful pre-trained model for deep learning model initialization. Finally, we evaluate GeRSP and other remote sensing pre-training methods on three downstream tasks, i.e., object detection, semantic segmentation, and scene classification. The extensive experimental results consistently demonstrate that GeRSP can effectively learn robust representations in a unified manner, improving the performance of remote sensing downstream tasks.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：EmoGen: Emotional Image Content Generation with Text-to-Image Diffusion  Models</b></summary>
  <p><b>编号</b>：[47]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04608</p>
  <p><b>作者</b>：Jingyuan Yang,  Jiawei Feng,  Hui Huang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：witnessed remarkable progress, create visually astonishing, visually astonishing images, Recent years, years have witnessed</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent years have witnessed remarkable progress in image generation task, where users can create visually astonishing images with high-quality. However, existing text-to-image diffusion models are proficient in generating concrete concepts (dogs) but encounter challenges with more abstract ones (emotions). Several efforts have been made to modify image emotions with color and style adjustments, facing limitations in effectively conveying emotions with fixed image contents. In this work, we introduce Emotional Image Content Generation (EICG), a new task to generate semantic-clear and emotion-faithful images given emotion categories. Specifically, we propose an emotion space and construct a mapping network to align it with the powerful Contrastive Language-Image Pre-training (CLIP) space, providing a concrete interpretation of abstract emotions. Attribute loss and emotion confidence are further proposed to ensure the semantic diversity and emotion fidelity of the generated images. Our method outperforms the state-of-the-art text-to-image approaches both quantitatively and qualitatively, where we derive three custom metrics, i.e., emotion accuracy, semantic clarity and semantic diversity. In addition to generation, our method can help emotion understanding and inspire emotional art design.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Enhanced Distribution Alignment for Post-Training Quantization of  Diffusion Models</b></summary>
  <p><b>编号</b>：[53]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04585</p>
  <p><b>作者</b>：Xuewen Liu,  Zhikai Li,  Junrui Xiao,  Qingyi Gu</p>
  <p><b>备注</b>：16 pages, 15 figures</p>
  <p><b>关键词</b>：iterative noise estimation, achieved great success, image generation tasks, Diffusion models, noise estimation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diffusion models have achieved great success in image generation tasks through iterative noise estimation. However, the heavy denoising process and complex neural networks hinder their low-latency applications in real-world scenarios. Quantization can effectively reduce model complexity, and post-training quantization (PTQ), which does not require fine-tuning, is highly promising in accelerating the denoising process. Unfortunately, we find that due to the highly dynamic distribution of activations in different denoising steps, existing PTQ methods for diffusion models suffer from distribution mismatch issues at both calibration sample level and reconstruction output level, which makes the performance far from satisfactory, especially in low-bit cases. In this paper, we propose Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models (EDA-DM) to address the above issues. Specifically, at the calibration sample level, we select calibration samples based on the density and diversity in the latent space, thus facilitating the alignment of their distribution with the overall samples; and at the reconstruction output level, we propose Fine-grained Block Reconstruction, which can align the outputs of the quantized model and the full-precision model at different network granularity. Extensive experiments demonstrate that EDA-DM outperforms the existing post-training quantization frameworks in both unconditional and conditional generation scenarios. At low-bit precision, the quantized models with our method even outperform the full-precision models on most datasets.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Effective pruning of web-scale datasets based on complexity of concept  clusters</b></summary>
  <p><b>编号</b>：[54]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04578</p>
  <p><b>作者</b>：Amro Abbas,  Evgenia Rusak,  Kushal Tirumala,  Wieland Brendel,  Kamalika Chaudhuri,  Ari S. Morcos</p>
  <p><b>备注</b>：Oral at the DataComp Workshop, ICCV 2023</p>
  <p><b>关键词</b>：Utilizing massive web-scale, massive web-scale datasets, machine learning models, unprecedented performance gains, imposes outlandish compute</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Utilizing massive web-scale datasets has led to unprecedented performance gains in machine learning models, but also imposes outlandish compute requirements for their training. In order to improve training and data efficiency, we here push the limits of pruning large-scale multimodal datasets for training CLIP-style models. Today's most effective pruning method on ImageNet clusters data samples into separate concepts according to their embedding and prunes away the most prototypical samples. We scale this approach to LAION and improve it by noting that the pruning rate should be concept-specific and adapted to the complexity of the concept. Using a simple and intuitive complexity measure, we are able to reduce the training cost to a quarter of regular training. By filtering from the LAION dataset, we find that training on a smaller set of high-quality data can lead to higher performance with significantly lower training costs. More specifically, we are able to outperform the LAION-trained OpenCLIP-ViT-B32 model on ImageNet zero-shot accuracy by 1.1p.p. while only using 27.7% of the data and training compute. Despite a strong reduction in training cost, we also see improvements on ImageNet dist. shifts, retrieval tasks and VTAB. On the DataComp Medium benchmark, we achieve a new state-of-the-art ImageNet zero-shot accuracy and a competitive average zero-shot accuracy on 38 evaluation tasks.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Let's Go Shopping (LGS) -- Web-Scale Image-Text Dataset for Visual  Concept Understanding</b></summary>
  <p><b>编号</b>：[56]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04575</p>
  <p><b>作者</b>：Yatong Bai,  Utsav Garg,  Apaar Shanker,  Haoming Zhang,  Samyak Parajuli,  Erhan Bas,  Isidora Filipovic,  Amelia N. Chu,  Eugenia D Fomitcheva,  Elliot Branson,  Aerin Kim,  Somayeh Sojoudi,  Kyunghyun Cho</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：non-trivial data-collecting processes, require non-trivial data-collecting, neural networks, classification and captioning, data-collecting processes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision and vision-language applications of neural networks, such as image classification and captioning, rely on large-scale annotated datasets that require non-trivial data-collecting processes. This time-consuming endeavor hinders the emergence of large-scale datasets, limiting researchers and practitioners to a small number of choices. Therefore, we seek more efficient ways to collect and annotate images. Previous initiatives have gathered captions from HTML alt-texts and crawled social media postings, but these data sources suffer from noise, sparsity, or subjectivity. For this reason, we turn to commercial shopping websites whose data meet three criteria: cleanliness, informativeness, and fluency. We introduce the Let's Go Shopping (LGS) dataset, a large-scale public dataset with 15 million image-caption pairs from publicly available e-commerce websites. When compared with existing general-domain datasets, the LGS images focus on the foreground object and have less complex backgrounds. Our experiments on LGS show that the classifiers trained on existing benchmark datasets do not readily generalize to e-commerce data, while specific self-supervised visual feature extractors can better generalize. Furthermore, LGS's high-quality e-commerce-focused images and bimodal nature make it advantageous for vision-language bi-modal tasks: LGS enables image-captioning models to generate richer captions and helps text-to-image generation models achieve e-commerce style transfer.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Phase-shifted remote photoplethysmography for estimating heart rate and  blood pressure from facial video</b></summary>
  <p><b>编号</b>：[59]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04560</p>
  <p><b>作者</b>：Gyutae Hwang,  Sang Jun Lee</p>
  <p><b>备注</b>：32 pages, 7 figures</p>
  <p><b>关键词</b>：Heart rate, blood pressure, cardiovascular diseases, estimating the heart, Heart</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Human health can be critically affected by cardiovascular diseases, such as hypertension, arrhythmias, and stroke. Heart rate and blood pressure are important biometric information for the monitoring of cardiovascular system and early diagnosis of cardiovascular diseases. Existing methods for estimating the heart rate are based on electrocardiography and photoplethyomography, which require contacting the sensor to the skin surface. Moreover, catheter and cuff-based methods for measuring blood pressure cause inconvenience and have limited applicability. Therefore, in this thesis, we propose a vision-based method for estimating the heart rate and blood pressure. This thesis proposes a 2-stage deep learning framework consisting of a dual remote photoplethysmography network (DRP-Net) and bounded blood pressure network (BBP-Net). In the first stage, DRP-Net infers remote photoplethysmography (rPPG) signals for the acral and facial regions, and these phase-shifted rPPG signals are utilized to estimate the heart rate. In the second stage, BBP-Net integrates temporal features and analyzes phase discrepancy between the acral and facial rPPG signals to estimate SBP and DBP values. To improve the accuracy of estimating the heart rate, we employed a data augmentation method based on a frame interpolation model. Moreover, we designed BBP-Net to infer blood pressure within a predefined range by incorporating a scaled sigmoid function. Our method resulted in estimating the heart rate with the mean absolute error (MAE) of 1.78 BPM, reducing the MAE by 34.31 % compared to the recent method, on the MMSE-HR dataset. The MAE for estimating the systolic blood pressure (SBP) and diastolic blood pressure (DBP) were 10.19 mmHg and 7.09 mmHg. On the V4V dataset, the MAE for the heart rate, SBP, and DBP were 3.83 BPM, 13.64 mmHg, and 9.4 mmHg, respectively.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：WaveletFormerNet: A Transformer-based Wavelet Network for Real-world  Non-homogeneous and Dense Fog Removal</b></summary>
  <p><b>编号</b>：[62]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04550</p>
  <p><b>作者</b>：Shengli Zhang,  Zhiyong Tao,  Sen Lin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：removing synthetic fog, achieved remarkable success, deep convolutional neural, complex foggy conditions, convolutional neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Although deep convolutional neural networks have achieved remarkable success in removing synthetic fog, it is essential to be able to process images taken in complex foggy conditions, such as dense or non-homogeneous fog, in the real world. However, the haze distribution in the real world is complex, and downsampling can lead to color distortion or loss of detail in the output results as the resolution of a feature map or image resolution decreases. In addition to the challenges of obtaining sufficient training data, overfitting can also arise in deep learning techniques for foggy image processing, which can limit the generalization abilities of the model, posing challenges for its practical applications in real-world scenarios. Considering these issues, this paper proposes a Transformer-based wavelet network (WaveletFormerNet) for real-world foggy image recovery. We embed the discrete wavelet transform into the Vision Transformer by proposing the WaveletFormer and IWaveletFormer blocks, aiming to alleviate texture detail loss and color distortion in the image due to downsampling. We introduce parallel convolution in the Transformer block, which allows for the capture of multi-frequency information in a lightweight mechanism. Additionally, we have implemented a feature aggregation module (FAM) to maintain image resolution and enhance the feature extraction capacity of our model, further contributing to its impressive performance in real-world foggy image recovery tasks. Extensive experiments demonstrate that our WaveletFormerNet performs better than state-of-the-art methods, as shown through quantitative and qualitative evaluations of minor model complexity. Additionally, our satisfactory results on real-world dust removal and application tests showcase the superior generalization ability and improved performance of WaveletFormerNet in computer vision-related applications.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Take A Shortcut Back: Mitigating the Gradient Vanishing for Training  Spiking Neural Networks</b></summary>
  <p><b>编号</b>：[85]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04486</p>
  <p><b>作者</b>：Yufei Guo,  Yuanpei Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Spiking Neural Network, biologically inspired neural, Spiking Neural, garnered significant attention, neural network infrastructure</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The Spiking Neural Network (SNN) is a biologically inspired neural network infrastructure that has recently garnered significant attention. It utilizes binary spike activations to transmit information, thereby replacing multiplications with additions and resulting in high energy efficiency. However, training an SNN directly poses a challenge due to the undefined gradient of the firing spike process. Although prior works have employed various surrogate gradient training methods that use an alternative function to replace the firing process during back-propagation, these approaches ignore an intrinsic problem: gradient vanishing. To address this issue, we propose a shortcut back-propagation method in our paper, which advocates for transmitting the gradient directly from the loss to the shallow layers. This enables us to present the gradient to the shallow layers directly, thereby significantly mitigating the gradient vanishing problem. Additionally, this method does not introduce any burden during the inference phase. To strike a balance between final accuracy and ease of training, we also propose an evolutionary training framework and implement it by inducing a balance coefficient that dynamically changes with the training epoch, which further improves the network's performance. Extensive experiments conducted over static and dynamic datasets using several popular network structures reveal that our method consistently outperforms state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation</b></summary>
  <p><b>编号</b>：[92]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04468</p>
  <p><b>作者</b>：Weimin Wang,  Jiawei Liu,  Zhijie Lin,  Jiangqiao Yan,  Shuo Chen,  Chetwin Low,  Tuyen Hoang,  Jie Wu,  Jun Hao Liew,  Hanshu Yan,  Daquan Zhou,  Jiashi Feng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：catalyzed significant research, high-fidelity video generation, growing demand, demand for high-fidelity, textual descriptions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The growing demand for high-fidelity video generation from textual descriptions has catalyzed significant research in this field. In this work, we introduce MagicVideo-V2 that integrates the text-to-image model, video motion generator, reference image embedding module and frame interpolation module into an end-to-end video generation pipeline. Benefiting from these architecture designs, MagicVideo-V2 can generate an aesthetically pleasing, high-resolution video with remarkable fidelity and smoothness. It demonstrates superior performance over leading Text-to-Video systems such as Runway, Pika 1.0, Morph, Moon Valley and Stable Video Diffusion model via user evaluation at large scale.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：PhilEO Bench: Evaluating Geo-Spatial Foundation Models</b></summary>
  <p><b>编号</b>：[93]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04464</p>
  <p><b>作者</b>：Casper Fibaek,  Luke Camilleri,  Andreas Luyts,  Nikolaos Dionelis,  Bertrand Le Saux</p>
  <p><b>备注</b>：6 pages, 5 figures, Submitted to IGARSS 2024</p>
  <p><b>关键词</b>：Earth Observation, captured by Earth, Foundation Models, constellation generating, makes Remote Sensing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Massive amounts of unlabelled data are captured by Earth Observation (EO) satellites, with the Sentinel-2 constellation generating 1.6 TB of data daily. This makes Remote Sensing a data-rich domain well suited to Machine Learning (ML) solutions. However, a bottleneck in applying ML models to EO is the lack of annotated data as annotation is a labour-intensive and costly process. As a result, research in this domain has focused on Self-Supervised Learning and Foundation Model approaches. This paper addresses the need to evaluate different Foundation Models on a fair and uniform benchmark by introducing the PhilEO Bench, a novel evaluation framework for EO Foundation Models. The framework comprises of a testbed and a novel 400 GB Sentinel-2 dataset containing labels for three downstream tasks, building density estimation, road segmentation, and land cover classification. We present experiments using our framework evaluating different Foundation Models, including Prithvi and SatMAE, at multiple n-shots and convergence rates.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：D3AD: Dynamic Denoising Diffusion Probabilistic Model for Anomaly  Detection</b></summary>
  <p><b>编号</b>：[94]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04463</p>
  <p><b>作者</b>：Justin Tebbe,  Jawad Tayyub</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：found valuable applications, nominal data distribution, found valuable, valuable applications, capturing the nominal</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diffusion models have found valuable applications in anomaly detection by capturing the nominal data distribution and identifying anomalies via reconstruction. Despite their merits, they struggle to localize anomalies of varying scales, especially larger anomalies like entire missing components. Addressing this, we present a novel framework that enhances the capability of diffusion models, by extending the previous introduced implicit conditioning approach Meng et al. (2022) in three significant ways. First, we incorporate a dynamic step size computation that allows for variable noising steps in the forward process guided by an initial anomaly prediction. Second, we demonstrate that denoising an only scaled input, without any added noise, outperforms conventional denoising process. Third, we project images in a latent space to abstract away from fine details that interfere with reconstruction of large missing components. Additionally, we propose a fine-tuning mechanism that facilitates the model to effectively grasp the nuances of the target domain. Our method undergoes rigorous evaluation on two prominent anomaly detection datasets VISA and BTAD, yielding state-of-the-art performance. Importantly, our framework effectively localizes anomalies regardless of their scale, marking a pivotal advancement in diffusion-based anomaly detection.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：A Novel Dataset for Non-Destructive Inspection of Handwritten Documents</b></summary>
  <p><b>编号</b>：[99]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04448</p>
  <p><b>作者</b>：Eleonora Breci (1),  Luca Guarnera (1),  Sebastiano Battiato (1) ((1) University of Catania)</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Science that aims, Forensic Science, aims to examine, order to properly, properly define</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Forensic handwriting examination is a branch of Forensic Science that aims to examine handwritten documents in order to properly define or hypothesize the manuscript's author. These analysis involves comparing two or more (digitized) documents through a comprehensive comparison of intrinsic local and global features. If a correlation exists and specific best practices are satisfied, then it will be possible to affirm that the documents under analysis were written by the same individual. The need to create sophisticated tools capable of extracting and comparing significant features has led to the development of cutting-edge software with almost entirely automated processes, improving the forensic examination of handwriting and achieving increasingly objective evaluations. This is made possible by algorithmic solutions based on purely mathematical concepts. Machine Learning and Deep Learning models trained with specific datasets could turn out to be the key elements to best solve the task at hand. In this paper, we proposed a new and challenging dataset consisting of two subsets: the first consists of 21 documents written either by the classic ``pen and paper" approach (and later digitized) and directly acquired on common devices such as tablets; the second consists of 362 handwritten manuscripts by 124 different people, acquired following a specific pipeline. Our study pioneered a comparison between traditionally handwritten documents and those produced with digital tools (e.g., tablets). Preliminary results on the proposed datasets show that 90% classification accuracy can be achieved on the first subset (documents written on both paper and pen and later digitized and on tablets) and 96% on the second portion of the data. The datasets are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Image classification network enhancement methods based on knowledge  injection</b></summary>
  <p><b>编号</b>：[101]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04441</p>
  <p><b>作者</b>：Yishuang Tian,  Ning Wang,  Liang Zhang</p>
  <p><b>备注</b>：7 pages, 3 figures, 5 tables</p>
  <p><b>关键词</b>：deep neural network, neural network training, deep neural, neural network, network training model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The current deep neural network algorithm still stays in the end-to-end training supervision method like Image-Label pairs, which makes traditional algorithm is difficult to explain the reason for the results, and the prediction logic is difficult to understand and analyze. The current algorithm does not use the existing human knowledge information, which makes the model not in line with the human cognition model and makes the model not suitable for human use. In order to solve the above problems, the present invention provides a deep neural network training method based on the human knowledge, which uses the human cognition model to construct the deep neural network training model, and uses the existing human knowledge information to construct the deep neural network training model. This paper proposes a multi-level hierarchical deep learning algorithm, which is composed of multi-level hierarchical deep neural network architecture and multi-level hierarchical deep learning framework. The experimental results show that the proposed algorithm can effectively explain the hidden information of the neural network. The goal of our study is to improve the interpretability of deep neural networks (DNNs) by providing an analysis of the impact of knowledge injection on the classification task. We constructed a knowledge injection dataset with matching knowledge data and image classification data. The knowledge injection dataset is the benchmark dataset for the experiments in the paper. Our model expresses the improvement in interpretability and classification task performance of hidden layers at different scales.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Empirical Analysis of Anomaly Detection on Hyperspectral Imaging Using  Dimension Reduction Methods</b></summary>
  <p><b>编号</b>：[102]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04437</p>
  <p><b>作者</b>：Dongeon Kim,  YeongHyeon Park</p>
  <p><b>备注</b>：4 pages, 4 figures, 3 tables</p>
  <p><b>关键词</b>：detect foreign matters, invisible wavelengths including, wavelengths including ultraviolet, Recent studies, hyperspectral imaging</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent studies try to use hyperspectral imaging (HSI) to detect foreign matters in products because it enables to visualize the invisible wavelengths including ultraviolet and infrared. Considering the enormous image channels of the HSI, several dimension reduction methods-e.g., PCA or UMAP-can be considered to reduce but those cannot ease the fundamental limitations, as follows: (1) latency of HSI capturing. (2) less explanation ability of the important channels. In this paper, to circumvent the aforementioned methods, one of the ways to channel reduction, on anomaly detection proposed HSI. Different from feature extraction methods (i.e., PCA or UMAP), feature selection can sort the feature by impact and show better explainability so we might redesign the task-optimized and cost-effective spectroscopic camera. Via the extensive experiment results with synthesized MVTec AD dataset, we confirm that the feature selection method shows 6.90x faster at the inference phase compared with feature extraction-based approaches while preserving anomaly detection performance. Ultimately, we conclude the advantage of feature selection which is effective yet fast.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Uncertainty-aware Sampling for Long-tailed Semi-supervised Learning</b></summary>
  <p><b>编号</b>：[103]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04435</p>
  <p><b>作者</b>：Kuo Yang,  Duo Li,  Menghan Hu,  Guangtao Zhai,  Xiaokang Yang,  Xiao-Ping Zhang</p>
  <p><b>备注</b>：Submitted to TPAMI</p>
  <p><b>关键词</b>：model prediction bias, training stages, semi-supervised learning, learning with imbalance, long-tailed distribution</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>For semi-supervised learning with imbalance classes, the long-tailed distribution of data will increase the model prediction bias toward dominant classes, undermining performance on less frequent classes. Existing methods also face challenges in ensuring the selection of sufficiently reliable pseudo-labels for model training and there is a lack of mechanisms to adjust the selection of more reliable pseudo-labels based on different training stages. To mitigate this issue, we introduce uncertainty into the modeling process for pseudo-label sampling, taking into account that the model performance on the tailed classes varies over different training stages. For example, at the early stage of model training, the limited predictive accuracy of model results in a higher rate of uncertain pseudo-labels. To counter this, we propose an Uncertainty-Aware Dynamic Threshold Selection (UDTS) approach. This approach allows the model to perceive the uncertainty of pseudo-labels at different training stages, thereby adaptively adjusting the selection thresholds for different classes. Compared to other methods such as the baseline method FixMatch, UDTS achieves an increase in accuracy of at least approximately 5.26%, 1.75%, 9.96%, and 1.28% on the natural scene image datasets CIFAR10-LT, CIFAR100-LT, STL-10-LT, and the medical image dataset TissueMNIST, respectively. The source code of UDTS is publicly available at: this https URL.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Meta-forests: Domain generalization on random forests with meta-learning</b></summary>
  <p><b>编号</b>：[105]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04425</p>
  <p><b>作者</b>：Yuyang Sun,  Panagiotis Kosmas</p>
  <p><b>备注</b>：This paper is accepted by ACML2023</p>
  <p><b>关键词</b>：popular machine learning, machine learning technique, unseen target domain, multiple source domains, machine learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Domain generalization is a popular machine learning technique that enables models to perform well on the unseen target domain, by learning from multiple source domains. Domain generalization is useful in cases where data is limited, difficult, or expensive to collect, such as in object recognition and biomedicine. In this paper, we propose a novel domain generalization algorithm called "meta-forests", which builds upon the basic random forests model by incorporating the meta-learning strategy and maximum mean discrepancy measure. The aim of meta-forests is to enhance the generalization ability of classifiers by reducing the correlation among trees and increasing their strength. More specifically, meta-forests conducts meta-learning optimization during each meta-task, while also utilizing the maximum mean discrepancy as a regularization term to penalize poor generalization performance in the meta-test process. To evaluate the effectiveness of our algorithm, we test it on two publicly object recognition datasets and a glucose monitoring dataset that we have used in a previous study. Our results show that meta-forests outperforms state-of-the-art approaches in terms of generalization performance on both object recognition and glucose monitoring datasets.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：MapAI: Precision in Building Segmentation</b></summary>
  <p><b>编号</b>：[110]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04406</p>
  <p><b>作者</b>：Sander Riisøen Jyhne,  Morten Goodwin,  Per Arne Andersen,  Ivar Oveland,  Alexander Salveson Nossum,  Karianne Ormseth,  Mathilde Ørstavik,  Andrew C. Flatman</p>
  <p><b>备注</b>：5 pages, 4 figures, competition</p>
  <p><b>关键词</b>：Artificial Intelligence Research, Norwegian Artificial Intelligence, Intelligence Research Consortium, Norwegian Mapping Authority, Artificial Intelligence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>MapAI: Precision in Building Segmentation is a competition arranged with the Norwegian Artificial Intelligence Research Consortium (NORA) in collaboration with Centre for Artificial Intelligence Research at the University of Agder (CAIR), the Norwegian Mapping Authority, AI:Hub, Norkart, and the Danish Agency for Data Supply and Infrastructure. The competition will be held in the fall of 2022. It will be concluded at the Northern Lights Deep Learning conference focusing on the segmentation of buildings using aerial images and laser data. We propose two different tasks to segment buildings, where the first task can only utilize aerial images, while the second must use laser data (LiDAR) with or without aerial images. Furthermore, we use IoU and Boundary IoU to properly evaluate the precision of the models, with the latter being an IoU measure that evaluates the results' boundaries. We provide the participants with a training dataset and keep a test dataset for evaluation.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Optimal Transcoding Resolution Prediction for Efficient Per-Title  Bitrate Ladder Estimation</b></summary>
  <p><b>编号</b>：[111]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04405</p>
  <p><b>作者</b>：Jinhai Yang,  Mengxi Guo,  Shijie Zhao,  Junlin Li,  Li Zhang</p>
  <p><b>备注</b>：Accepted by the 2024 Data Compression Conference (DCC) for presentation as a poster. This is the full paper</p>
  <p><b>关键词</b>：Adaptive video streaming, meet heterogeneous network, heterogeneous network conditions, end-user demands, video streaming requires</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Adaptive video streaming requires efficient bitrate ladder construction to meet heterogeneous network conditions and end-user demands. Per-title optimized encoding typically traverses numerous encoding parameters to search the Pareto-optimal operating points for each video. Recently, researchers have attempted to predict the content-optimized bitrate ladder for pre-encoding overhead reduction. However, existing methods commonly estimate the encoding parameters on the Pareto front and still require subsequent pre-encodings. In this paper, we propose to directly predict the optimal transcoding resolution at each preset bitrate for efficient bitrate ladder construction. We adopt a Temporal Attentive Gated Recurrent Network to capture spatial-temporal features and predict transcoding resolutions as a multi-task classification problem. We demonstrate that content-optimized bitrate ladders can thus be efficiently determined without any pre-encoding. Our method well approximates the ground-truth bitrate-resolution pairs with a slight Bjøntegaard Delta rate loss of 1.21% and significantly outperforms the state-of-the-art fixed ladder.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：MST: Adaptive Multi-Scale Tokens Guided Interactive Segmentation</b></summary>
  <p><b>编号</b>：[112]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04403</p>
  <p><b>作者</b>：Long Xu,  Shanghong Li,  Yongquan Chen,  Jun Luo</p>
  <p><b>备注</b>：12 pages, 10 figures</p>
  <p><b>关键词</b>：Industrial Informatics, gained significant attention, field of Industrial, data annotation, gained significant</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the field of Industrial Informatics, interactive segmentation has gained significant attention for its application in human-computer interaction and data annotation. Existing algorithms, however, face challenges in balancing the segmentation accuracy between large and small targets, often leading to an increased number of user interactions. To tackle this, a novel multi-scale token adaptation algorithm, leveraging token similarity, has been devised to enhance segmentation across varying target sizes. This algorithm utilizes a differentiable top-k tokens selection mechanism, allowing for fewer tokens to be used while maintaining efficient multi-scale token interaction. Furthermore, a contrastive loss is introduced to better discriminate between target and background tokens, improving the correctness and robustness of the tokens similar to the target. Extensive benchmarking shows that the algorithm achieves state-of-the-art (SOTA) performance compared to current methods. An interactive demo and all reproducible codes will be released at this https URL.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Learning with Noisy Labels: Interconnection of Two  Expectation-Maximizations</b></summary>
  <p><b>编号</b>：[117]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04390</p>
  <p><b>作者</b>：Heewon Kim,  Hyun Sung Chang,  Kiho Cho,  Jaeyun Lee,  Bohyung Han</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：developing computer vision, computer vision algorithms, Labor-intensive labeling, bottleneck in developing, developing computer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Labor-intensive labeling becomes a bottleneck in developing computer vision algorithms based on deep learning. For this reason, dealing with imperfect labels has increasingly gained attention and has become an active field of study. We address learning with noisy labels (LNL) problem, which is formalized as a task of finding a structured manifold in the midst of noisy data. In this framework, we provide a proper objective function and an optimization algorithm based on two expectation-maximization (EM) cycles. The separate networks associated with the two EM cycles collaborate to optimize the objective function, where one model is for distinguishing clean labels from corrupted ones while the other is for refurbishing the corrupted labels. This approach results in a non-collapsing LNL-flywheel model in the end. Experiments show that our algorithm achieves state-of-the-art performance in multiple standard benchmarks with substantial margins under various types of label noise.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Towards Real-World Aerial Vision Guidance with Categorical 6D Pose  Tracker</b></summary>
  <p><b>编号</b>：[121]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04377</p>
  <p><b>作者</b>：Jingtao Sun,  Yaonan Wang,  Danwei Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：downstream robot tasks, downstream robot, real-world robot task, aerial, pose</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Tracking the object 6-DoF pose is crucial for various downstream robot tasks and real-world applications. In this paper, we investigate the real-world robot task of aerial vision guidance for aerial robotics manipulation, utilizing category-level 6-DoF pose tracking. Aerial conditions inevitably introduce special challenges, such as rapid viewpoint changes in pitch and roll. To support this task and challenge, we firstly introduce a robust category-level 6-DoF pose tracker (Robust6DoF). This tracker leverages shape and temporal prior knowledge to explore optimal inter-frame keypoint pairs, generated under a priori structural adaptive supervision in a coarse-to-fine manner. Notably, our Robust6DoF employs a Spatial-Temporal Augmentation module to deal with the problems of the inter-frame differences and intra-class shape variations through both temporal dynamic filtering and shape-similarity filtering. We further present a Pose-Aware Discrete Servo strategy (PAD-Servo), serving as a decoupling approach to implement the final aerial vision guidance task. It contains two servo action policies to better accommodate the structural properties of aerial robotics manipulation. Exhaustive experiments on four well-known public benchmarks demonstrate the superiority of our Robust6DoF. Real-world tests directly verify that our Robust6DoF along with PAD-Servo can be readily used in real-world aerial robotic applications.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：SoK: Facial Deepfake Detectors</b></summary>
  <p><b>编号</b>：[127]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04364</p>
  <p><b>作者</b>：Binh M. Le,  Jiwon Kim,  Shahroz Tariq,  Kristen Moore,  Alsharif Abuadbba,  Simon S. Woo</p>
  <p><b>备注</b>：18 pages, 6 figures, 5 table, under peer-review</p>
  <p><b>关键词</b>：threat to society, primarily due, creation and dissemination, rapidly emerged, ease of creation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deepfakes have rapidly emerged as a profound and serious threat to society, primarily due to their ease of creation and dissemination. This situation has triggered an accelerated development of deepfake detection technologies. However, many existing detectors rely heavily on lab-generated datasets for validation, which may not effectively prepare them for novel, emerging, and real-world deepfake techniques. In this paper, we conduct an extensive and comprehensive review and analysis of the latest state-of-the-art deepfake detectors, evaluating them against several critical criteria. These criteria facilitate the categorization of these detectors into 4 high-level groups and 13 fine-grained sub-groups, all aligned with a unified standard conceptual framework. This classification and framework offer deep and practical insights into the factors that affect detector efficacy. We assess the generalizability of 16 leading detectors across various standard attack scenarios, including black-box, white-box, and gray-box settings. Our systematized analysis and experimentation lay the groundwork for a deeper understanding of deepfake detectors and their generalizability, paving the way for future research focused on creating detectors adept at countering various attack scenarios. Additionally, this work offers insights for developing more proactive defenses against deepfakes.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Representative Feature Extraction During Diffusion Process for Sketch  Extraction with One Example</b></summary>
  <p><b>编号</b>：[128]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04362</p>
  <p><b>作者</b>：Kwan Yun,  Youngseo Kim,  Kwanggyoon Seo,  Chang Wook Seo,  Junyong Noh</p>
  <p><b>备注</b>：8 pages(main paper), 8 pages(supplementary material)</p>
  <p><b>关键词</b>：generating a variety, variety of stylized, features, introduce DiffSketch, stylized sketches</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce DiffSketch, a method for generating a variety of stylized sketches from images. Our approach focuses on selecting representative features from the rich semantics of deep features within a pretrained diffusion model. This novel sketch generation method can be trained with one manual drawing. Furthermore, efficient sketch extraction is ensured by distilling a trained generator into a streamlined extractor. We select denoising diffusion features through analysis and integrate these selected features with VAE features to produce sketches. Additionally, we propose a sampling scheme for training models using a conditional generative approach. Through a series of comparisons, we verify that distilled DiffSketch not only outperforms existing state-of-the-art sketch extraction methods but also surpasses diffusion-based stylization methods in the task of extracting sketches.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Iterative Feedback Network for Unsupervised Point Cloud Registration</b></summary>
  <p><b>编号</b>：[132]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04357</p>
  <p><b>作者</b>：Yifan Xie,  Boyu Wang,  Shiqi Li,  Jihua Zhu</p>
  <p><b>备注</b>：8 pages, accepted by RAL 2024</p>
  <p><b>关键词</b>：point cloud registration, cloud registration aims, computer vision, feedback high-level features, fundamental problem</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As a fundamental problem in computer vision, point cloud registration aims to seek the optimal transformation for aligning a pair of point clouds. In most existing methods, the information flows are usually forward transferring, thus lacking the guidance from high-level information to low-level information. Besides, excessive high-level information may be overly redundant, and directly using it may conflict with the original low-level information. In this paper, we propose a novel Iterative Feedback Network (IFNet) for unsupervised point cloud registration, in which the representation of low-level features is efficiently enriched by rerouting subsequent high-level features. Specifically, our IFNet is built upon a series of Feedback Registration Block (FRB) modules, with each module responsible for generating the feedforward rigid transformation and feedback high-level features. These FRB modules are cascaded and recurrently unfolded over time. Further, the Feedback Transformer is designed to efficiently select relevant information from feedback high-level features, which is utilized to refine the low-level features. What's more, we incorporate a geometry-awareness descriptor to empower the network for making full use of most geometric information, which leads to more precise registration results. Extensive experiments on various benchmark datasets demonstrate the superior registration performance of our IFNet.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Knowledge-enhanced Multi-perspective Video Representation Learning for  Scene Recognition</b></summary>
  <p><b>编号</b>：[133]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04354</p>
  <p><b>作者</b>：Xuzheng Yu,  Chen Jiang,  Wei Zhang,  Tian Gan,  Linlin Chao,  Jianan Zhao,  Yuan Cheng,  Qingpei Guo,  Wei Chu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：increasingly important, explosive growth, video scene recognition, video, comprehensive representation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the explosive growth of video data in real-world applications, a comprehensive representation of videos becomes increasingly important. In this paper, we address the problem of video scene recognition, whose goal is to learn a high-level video representation to classify scenes in videos. Due to the diversity and complexity of video contents in realistic scenarios, this task remains a challenge. Most existing works identify scenes for videos only from visual or textual information in a temporal perspective, ignoring the valuable information hidden in single frames, while several earlier studies only recognize scenes for separate images in a non-temporal perspective. We argue that these two perspectives are both meaningful for this task and complementary to each other, meanwhile, externally introduced knowledge can also promote the comprehension of videos. We propose a novel two-stream framework to model video representations from multiple perspectives, i.e. temporal and non-temporal perspectives, and integrate the two perspectives in an end-to-end manner by self-distillation. Besides, we design a knowledge-enhanced feature fusion and label prediction method that contributes to naturally introducing knowledge into the task of video scene recognition. Experiments conducted on a real-world dataset demonstrate the effectiveness of our proposed method.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Pre-trained Model Guided Fine-Tuning for Zero-Shot Adversarial  Robustness</b></summary>
  <p><b>编号</b>：[135]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04350</p>
  <p><b>作者</b>：Sibo Wang,  Jie Zhang,  Zheng Yuan,  Shiguang Shan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：demonstrated impressive performance, Large-scale pre-trained vision-language, exhibit remarkable zero-shot, pre-trained vision-language models, Pre-trained Model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large-scale pre-trained vision-language models like CLIP have demonstrated impressive performance across various tasks, and exhibit remarkable zero-shot generalization capability, while they are also vulnerable to imperceptible adversarial examples. Existing works typically employ adversarial training (fine-tuning) as a defense method against adversarial examples. However, direct application to the CLIP model may result in overfitting, compromising the model's capacity for generalization. In this paper, we propose Pre-trained Model Guided Adversarial Fine-Tuning (PMG-AFT) method, which leverages supervision from the original pre-trained model by carefully designing an auxiliary branch, to enhance the model's zero-shot adversarial robustness. Specifically, PMG-AFT minimizes the distance between the features of adversarial examples in the target model and those in the pre-trained model, aiming to preserve the generalization features already captured by the pre-trained model. Extensive Experiments on 15 zero-shot datasets demonstrate that PMG-AFT significantly outperforms the state-of-the-art method, improving the top-1 robust accuracy by an average of 4.99%. Furthermore, our approach consistently improves clean accuracy by an average of 8.72%.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：RomniStereo: Recurrent Omnidirectional Stereo Matching</b></summary>
  <p><b>编号</b>：[138]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04345</p>
  <p><b>作者</b>：Hualie Jiang,  Rui Xu,  Minglang Tan,  Wenjie Jiang</p>
  <p><b>备注</b>：accepted by IEEE RA-L, this https URL</p>
  <p><b>关键词</b>：Omnidirectional stereo matching, stereo matching, conventional stereo matching, depth sensing, Recurrent All-pairs Field</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Omnidirectional stereo matching (OSM) is an essential and reliable means for $360^{\circ}$ depth sensing. However, following earlier works on conventional stereo matching, prior state-of-the-art (SOTA) methods rely on a 3D encoder-decoder block to regularize the cost volume, causing the whole system complicated and sub-optimal results. Recently, the Recurrent All-pairs Field Transforms (RAFT) based approach employs the recurrent update in 2D and has efficiently improved image-matching tasks, \ie, optical flow, and stereo matching. To bridge the gap between OSM and RAFT, we mainly propose an opposite adaptive weighting scheme to seamlessly transform the outputs of spherical sweeping of OSM into the required inputs for the recurrent update, thus creating a recurrent omnidirectional stereo matching (RomniStereo) algorithm. Furthermore, we introduce two techniques, \ie, grid embedding and adaptive context feature generation, which also contribute to RomniStereo's performance. Our best model improves the average MAE metric by 40.7\% over the previous SOTA baseline across five datasets. When visualizing the results, our models demonstrate clear advantages on both synthetic and realistic examples. The code is available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Memory-Efficient Personalization using Quantized Diffusion Model</b></summary>
  <p><b>编号</b>：[141]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04339</p>
  <p><b>作者</b>：Hyogon Ryu,  Seohyun Lim,  Hyunjung Shim</p>
  <p><b>备注</b>：20 pages</p>
  <p><b>关键词</b>：Stable Diffusion, markedly advances, billion-parameter diffusion models, rise of billion-parameter, advances the field</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rise of billion-parameter diffusion models like Stable Diffusion XL, Imagen, and Dall-E3 markedly advances the field of generative AI. However, their large-scale nature poses challenges in fine-tuning and deployment due to high resource demands and slow inference speed. This paper ventures into the relatively unexplored yet promising realm of fine-tuning quantized diffusion models. We establish a strong baseline by customizing three models: PEQA for fine-tuning quantization parameters, Q-Diffusion for post-training quantization, and DreamBooth for personalization. Our analysis reveals a notable trade-off between subject and prompt fidelity within the baseline model. To address these issues, we introduce two strategies, inspired by the distinct roles of different timesteps in diffusion models: S1 optimizing a single set of fine-tuning parameters exclusively at selected intervals, and S2 creating multiple fine-tuning parameter sets, each specialized for different timestep intervals. Our approach not only enhances personalization but also upholds prompt fidelity and image quality, significantly outperforming the baseline qualitatively and quantitatively. The code will be made publicly available.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：Mix-GENEO: A flexible filtration for multiparameter persistent homology  detects digital images</b></summary>
  <p><b>编号</b>：[145]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04332</p>
  <p><b>作者</b>：Jiaxing He,  Bingzhe Hou,  Tieru Wu,  Yue Xin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Topological Data Analysis, Data Analysis, defining practical multifiltrations, Analysis are defining, Topological Data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Two important problems in the field of Topological Data Analysis are defining practical multifiltrations on objects and showing ability of TDA to detect the geometry. Motivated by the problems, we constuct three multifiltrations named multi-GENEO, multi-DGENEO and mix-GENEO, and prove the stability of both the interleaving distance and multiparameter persistence landscape of multi-GENEO with respect to the pseudometric of the subspace of bounded functions. We also give the estimations of upper bound for multi-DGENEO and mix-GENEO. Finally, we provide experiment results on MNIST dataset to demonstrate our bifiltrations have ability to detect geometric and topological differences of digital images.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：BD-MSA: Body decouple VHR Remote Sensing Image Change Detection method  guided by multi-scale feature information aggregation</b></summary>
  <p><b>编号</b>：[147]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04330</p>
  <p><b>作者</b>：Yonghui Tan,  Xiaolong Li,  Yishu Chen,  Jinquan Ai</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：sensing image change, image change detection, remote sensing image, bi-temporal images, Aggregation change detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The purpose of remote sensing image change detection (RSCD) is to detect differences between bi-temporal images taken at the same place. Deep learning has been extensively used to RSCD tasks, yielding significant results in terms of result recognition. However, due to the shooting angle of the satellite, the impacts of thin clouds, and certain lighting conditions, the problem of fuzzy edges in the change region in some remote sensing photographs cannot be properly handled using current RSCD algorithms. To solve this issue, we proposed a Body Decouple Multi-Scale by fearure Aggregation change detection (BD-MSA), a novel model that collects both global and local feature map information in the channel and space dimensions of the feature map during the training and prediction phases. This approach allows us to successfully extract the change region's boundary information while also divorcing the change region's main body from its boundary. Numerous studies have shown that the assessment metrics and evaluation effects of the model described in this paper on the publicly available datasets DSIFN-CD and S2Looking are the best when compared to other models.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：RadarCam-Depth: Radar-Camera Fusion for Depth Estimation with Learned  Metric Scale</b></summary>
  <p><b>编号</b>：[149]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04325</p>
  <p><b>作者</b>：Han Li,  Yukai Ma,  Yaqing Gu,  Kewei Hu,  Yong Liu,  Xingxing Zuo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Radar point cloud, noisy Radar data, noisy Radar point, dense depth, dense depth estimation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a novel approach for metric dense depth estimation based on the fusion of a single-view image and a sparse, noisy Radar point cloud. The direct fusion of heterogeneous Radar and image data, or their encodings, tends to yield dense depth maps with significant artifacts, blurred boundaries, and suboptimal accuracy. To circumvent this issue, we learn to augment versatile and robust monocular depth prediction with the dense metric scale induced from sparse and noisy Radar data. We propose a Radar-Camera framework for highly accurate and fine-detailed dense depth estimation with four stages, including monocular depth prediction, global scale alignment of monocular depth with sparse Radar points, quasi-dense scale estimation through learning the association between Radar points and image patches, and local scale refinement of dense depth using a scale map learner. Our proposed method significantly outperforms the state-of-the-art Radar-Camera depth estimation methods by reducing the mean absolute error (MAE) of depth estimation by 25.6% and 40.2% on the challenging nuScenes dataset and our self-collected ZJU-4DRadarCam dataset, respectively.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Vision Reimagined: AI-Powered Breakthroughs in WiFi Indoor Imaging</b></summary>
  <p><b>编号</b>：[154]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04317</p>
  <p><b>作者</b>：Jianyang Shi,  Bowen Zhang,  Amartansh Dubey,  Ross Murch,  Liwen Jing</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：WiFi indoor imaging, Frechet Inception Distance, Indoor imaging, imaging, critical task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Indoor imaging is a critical task for robotics and internet-of-things. WiFi as an omnipresent signal is a promising candidate for carrying out passive imaging and synchronizing the up-to-date information to all connected devices. This is the first research work to consider WiFi indoor imaging as a multi-modal image generation task that converts the measured WiFi power into a high-resolution indoor image. Our proposed WiFi-GEN network achieves a shape reconstruction accuracy that is 275% of that achieved by physical model-based inversion methods. Additionally, the Frechet Inception Distance score has been significantly reduced by 82%. To examine the effectiveness of models for this task, the first large-scale dataset is released containing 80,000 pairs of WiFi signal and imaging target. Our model absorbs challenges for the model-based methods including the non-linearity, ill-posedness and non-certainty into massive parameters of our generative AI network. The network is also designed to best fit measured WiFi signals and the desired imaging output. For reproducibility, we will release the data and code upon acceptance.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：StarCraftImage: A Dataset For Prototyping Spatial Reasoning Methods For  Multi-Agent Environments</b></summary>
  <p><b>编号</b>：[162]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04290</p>
  <p><b>作者</b>：Sean Kulinski,  Nicholas R. Waytowich,  James Z. Hare,  David I. Inouye</p>
  <p><b>备注</b>：Published in CVPR 23'</p>
  <p><b>关键词</b>：missing data imputation, agent type identification, event prediction, multiple applications, autonomous surveillance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Spatial reasoning tasks in multi-agent environments such as event prediction, agent type identification, or missing data imputation are important for multiple applications (e.g., autonomous surveillance over sensor networks and subtasks for reinforcement learning (RL)). StarCraft II game replays encode intelligent (and adversarial) multi-agent behavior and could provide a testbed for these tasks; however, extracting simple and standardized representations for prototyping these tasks is laborious and hinders reproducibility. In contrast, MNIST and CIFAR10, despite their extreme simplicity, have enabled rapid prototyping and reproducibility of ML methods. Following the simplicity of these datasets, we construct a benchmark spatial reasoning dataset based on StarCraft II replays that exhibit complex multi-agent behaviors, while still being as easy to use as MNIST and CIFAR10. Specifically, we carefully summarize a window of 255 consecutive game states to create 3.6 million summary images from 60,000 replays, including all relevant metadata such as game outcome and player races. We develop three formats of decreasing complexity: Hyperspectral images that include one channel for every unit type (similar to multispectral geospatial images), RGB images that mimic CIFAR10, and grayscale images that mimic MNIST. We show how this dataset can be used for prototyping spatial reasoning methods. All datasets, code for extraction, and code for dataset loading can be found at this https URL</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Detecting Face Synthesis Using a Concealed Fusion Model</b></summary>
  <p><b>编号</b>：[173]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04257</p>
  <p><b>作者</b>：Roberto Leyva,  Victor Sanchez,  Gregory Epiphaniou,  Carsten Maple</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：potential negative impacts, computer security due, Face image synthesis, negative impacts, including those related</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Face image synthesis is gaining more attention in computer security due to concerns about its potential negative impacts, including those related to fake biometrics. Hence, building models that can detect the synthesized face images is an important challenge to tackle. In this paper, we propose a fusion-based strategy to detect face image synthesis while providing resiliency to several attacks. The proposed strategy uses a late fusion of the outputs computed by several undisclosed models by relying on random polynomial coefficients and exponents to conceal a new feature space. Unlike existing concealing solutions, our strategy requires no quantization, which helps to preserve the feature space. Our experiments reveal that our strategy achieves state-of-the-art performance while providing protection against poisoning, perturbation, backdoor, and reverse model attacks.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Robust Image Watermarking using Stable Diffusion</b></summary>
  <p><b>编号</b>：[177]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04247</p>
  <p><b>作者</b>：Lijun Zhang,  Xiao Liu,  Antoni Viros Martin,  Cindy Xiong Bearfield,  Yuriy Brun,  Hui Guan</p>
  <p><b>备注</b>：15 pages, 14 figures</p>
  <p><b>关键词</b>：tracking image provenance, claiming ownership, critical for tracking, provenance and claiming, stable diffusion</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Watermarking images is critical for tracking image provenance and claiming ownership. With the advent of generative models, such as stable diffusion, able to create fake but realistic images, watermarking has become particularly important, e.g., to make generated images reliably identifiable. Unfortunately, the very same stable diffusion technology can remove watermarks injected using existing methods. To address this problem, we present a ZoDiac, which uses a pre-trained stable diffusion model to inject a watermark into the trainable latent space, resulting in watermarks that can be reliably detected in the latent vector, even when attacked. We evaluate ZoDiac on three benchmarks, MS-COCO, DiffusionDB, and WikiArt, and find that ZoDiac is robust against state-of-the-art watermark attacks, with a watermark detection rate over 98% and a false positive rate below 6.4%, outperforming state-of-the-art watermarking methods. Our research demonstrates that stable diffusion is a promising approach to robust watermarking, able to withstand even stable-diffusion-based attacks.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Data-Agnostic Face Image Synthesis Detection Using Bayesian CNNs</b></summary>
  <p><b>编号</b>：[179]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04241</p>
  <p><b>作者</b>：Roberto Leyva,  Victor Sanchez,  Gregory Epiphaniou,  Carsten Maple</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：considerably gaining attention, potential negative impact, Face image synthesis, image synthesis process, image synthesis detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Face image synthesis detection is considerably gaining attention because of the potential negative impact on society that this type of synthetic data brings. In this paper, we propose a data-agnostic solution to detect the face image synthesis process. Specifically, our solution is based on an anomaly detection framework that requires only real data to learn the inference process. It is therefore data-agnostic in the sense that it requires no synthetic face images. The solution uses the posterior probability with respect to the reference data to determine if new samples are synthetic or not. Our evaluation results using different synthesizers show that our solution is very competitive against the state-of-the-art, which requires synthetic data for training.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：SOAP: Cross-sensor Domain Adaptation for 3D Object Detection Using  Stationary Object Aggregation Pseudo-labelling</b></summary>
  <p><b>编号</b>：[181]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04230</p>
  <p><b>作者</b>：Chengjie Huang,  Vahdat Abdelzad,  Sean Sedwards,  Krzysztof Czarnecki</p>
  <p><b>备注</b>：Accepted by WACV 2024</p>
  <p><b>关键词</b>：Object Aggregation Pseudo-labelling, Stationary Object Aggregation, Aggregation Pseudo-labelling, propose Stationary Object, high quality pseudo-labels</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider the problem of cross-sensor domain adaptation in the context of LiDAR-based 3D object detection and propose Stationary Object Aggregation Pseudo-labelling (SOAP) to generate high quality pseudo-labels for stationary objects. In contrast to the current state-of-the-art in-domain practice of aggregating just a few input scans, SOAP aggregates entire sequences of point clouds at the input level to reduce the sensor domain gap. Then, by means of what we call quasi-stationary training and spatial consistency post-processing, the SOAP model generates accurate pseudo-labels for stationary objects, closing a minimum of 30.3% domain gap compared to few-frame detectors. Our results also show that state-of-the-art domain adaptation approaches can achieve even greater performance in combination with SOAP, in both the unsupervised and semi-supervised settings.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：FunnyNet-W: Multimodal Learning of Funny Moments in Videos in the Wild</b></summary>
  <p><b>编号</b>：[185]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04210</p>
  <p><b>作者</b>：Zhi-Song Liu,  Robin Courant,  Vicky Kalogeiton</p>
  <p><b>备注</b>：22 pages, 14 figures</p>
  <p><b>关键词</b>：make people laugh, funny moments, Large Language Model, dialogues and culture, predict funny moments</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automatically understanding funny moments (i.e., the moments that make people laugh) when watching comedy is challenging, as they relate to various features, such as body language, dialogues and culture. In this paper, we propose FunnyNet-W, a model that relies on cross- and self-attention for visual, audio and text data to predict funny moments in videos. Unlike most methods that rely on ground truth data in the form of subtitles, in this work we exploit modalities that come naturally with videos: (a) video frames as they contain visual information indispensable for scene understanding, (b) audio as it contains higher-level cues associated with funny moments, such as intonation, pitch and pauses and (c) text automatically extracted with a speech-to-text model as it can provide rich information when processed by a Large Language Model. To acquire labels for training, we propose an unsupervised approach that spots and labels funny audio moments. We provide experiments on five datasets: the sitcoms TBBT, MHD, MUStARD, Friends, and the TED talk UR-Funny. Extensive experiments and analysis show that FunnyNet-W successfully exploits visual, auditory and textual cues to identify funny moments, while our findings reveal FunnyNet-W's ability to predict funny moments in the wild. FunnyNet-W sets the new state of the art for funny moment detection with multimodal cues on all datasets with and without using ground truth information.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Language-Conditioned Robotic Manipulation with Fast and Slow Thinking</b></summary>
  <p><b>编号</b>：[190]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04181</p>
  <p><b>作者</b>：Minjie Zhu,  Yichen Zhu,  Jinming Li,  Junjie Wen,  Zhiyuan Xu,  Zhengping Che,  Chaomin Shen,  Yaxin Peng,  Dong Liu,  Feifei Feng,  Jian Tang</p>
  <p><b>备注</b>：submitted to ICRA2024</p>
  <p><b>关键词</b>：language-conditioned robotic manipulation, robotic manipulation aims, transfer natural language, fast and slow, slow thinking</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The language-conditioned robotic manipulation aims to transfer natural language instructions into executable actions, from simple pick-and-place to tasks requiring intent recognition and visual reasoning. Inspired by the dual process theory in cognitive science, which suggests two parallel systems of fast and slow thinking in human decision-making, we introduce Robotics with Fast and Slow Thinking (RFST), a framework that mimics human cognitive architecture to classify tasks and makes decisions on two systems based on instruction types. Our RFST consists of two key components: 1) an instruction discriminator to determine which system should be activated based on the current user instruction, and 2) a slow-thinking system that is comprised of a fine-tuned vision language model aligned with the policy networks, which allows the robot to recognize user intention or perform reasoning tasks. To assess our methodology, we built a dataset featuring real-world trajectories, capturing actions ranging from spontaneous impulses to tasks requiring deliberate contemplation. Our results, both in simulation and real-world scenarios, confirm that our approach adeptly manages intricate tasks that demand intent recognition and reasoning. The project is available at this https URL</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Efficient Selective Audio Masked Multimodal Bottleneck Transformer for  Audio-Video Classification</b></summary>
  <p><b>编号</b>：[192]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04154</p>
  <p><b>作者</b>：Wentao Zhu</p>
  <p><b>备注</b>：Accepted by WACV 2024; well-formatted PDF is in this https URL arXiv admin note: text overlap with arXiv:2401.04023</p>
  <p><b>关键词</b>：mainstream media platforms, AVT, Transformer, video Transformer, media platforms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Audio and video are two most common modalities in the mainstream media platforms, e.g., YouTube. To learn from multimodal videos effectively, in this work, we propose a novel audio-video recognition approach termed audio video Transformer, AVT, leveraging the effective spatio-temporal representation by the video Transformer to improve action recognition accuracy. For multimodal fusion, simply concatenating multimodal tokens in a cross-modal Transformer requires large computational and memory resources, instead we reduce the cross-modality complexity through an audio-video bottleneck Transformer. To improve the learning efficiency of multimodal Transformer, we integrate self-supervised objectives, i.e., audio-video contrastive learning, audio-video matching, and masked audio and video learning, into AVT training, which maps diverse audio and video representations into a common multimodal representation space. We further propose a masked audio segment loss to learn semantic audio activities in AVT. Extensive experiments and ablation studies on three public datasets and two in-house datasets consistently demonstrate the effectiveness of the proposed AVT. Specifically, AVT outperforms its previous state-of-the-art counterparts on Kinetics-Sounds by 8%. AVT also surpasses one of the previous state-of-the-art video Transformers [25] by 10% on VGGSound by leveraging the audio signal. Compared to one of the previous state-of-the-art multimodal methods, MBT [32], AVT is 1.3% more efficient in terms of FLOPs and improves the accuracy by 3.8% on Epic-Kitchens-100.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Two-stream joint matching method based on contrastive learning for  few-shot action recognition</b></summary>
  <p><b>编号</b>：[195]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04150</p>
  <p><b>作者</b>：Long Deng,  Ziqiang Li,  Bingxin Zhou,  Zhongming Chen,  Ao Li,  Yongxin Ge</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Joint Matching Module, achieved significant success, video matching problems, metric learning paradigm, Contrastive Learning Module</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Although few-shot action recognition based on metric learning paradigm has achieved significant success, it fails to address the following issues: (1) inadequate action relation modeling and underutilization of multi-modal information; (2) challenges in handling video matching problems with different lengths and speeds, and video matching problems with misalignment of video sub-actions. To address these issues, we propose a Two-Stream Joint Matching method based on contrastive learning (TSJM), which consists of two modules: Multi-modal Contrastive Learning Module (MCL) and Joint Matching Module (JMM). The objective of the MCL is to extensively investigate the inter-modal mutual information relationships, thereby thoroughly extracting modal information to enhance the modeling of action relationships. The JMM aims to simultaneously address the aforementioned video matching problems. The effectiveness of the proposed method is evaluated on two widely used few shot action recognition datasets, namely, SSv2 and Kinetics. Comprehensive ablation experiments are also conducted to substantiate the efficacy of our proposed approach.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：RHOBIN Challenge: Reconstruction of Human Object Interaction</b></summary>
  <p><b>编号</b>：[199]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04143</p>
  <p><b>作者</b>：Xianghui Xie,  Xi Wang,  Nikos Athanasiou,  Bharat Lal Bhatnagar,  Chun-Hao P. Huang,  Kaichun Mo,  Hao Chen,  Xia Jia,  Zerui Zhang,  Liangxian Cui,  Xiao Lin,  Bingqiao Qian,  Jie Xiao,  Wenfei Yang,  Hyeongjin Nam,  Daniel Sungho Jung,  Kihoon Kim,  Kyoung Mu Lee,  Otmar Hilliges,  Gerard Pons-Moll</p>
  <p><b>备注</b>：14 pages, 5 tables, 7 figure. Technical report of the CVPR'23 workshop: RHOBIN challenge (this https URL)</p>
  <p><b>关键词</b>：recent years, interaction, Reconstruction, research, object pose</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Modeling the interaction between humans and objects has been an emerging research direction in recent years. Capturing human-object interaction is however a very challenging task due to heavy occlusion and complex dynamics, which requires understanding not only 3D human pose, and object pose but also the interaction between them. Reconstruction of 3D humans and objects has been two separate research fields in computer vision for a long time. We hence proposed the first RHOBIN challenge: reconstruction of human-object interactions in conjunction with the RHOBIN workshop. It was aimed at bringing the research communities of human and object reconstruction as well as interaction modeling together to discuss techniques and exchange ideas. Our challenge consists of three tracks of 3D reconstruction from monocular RGB images with a focus on dealing with challenging interaction scenarios. Our challenge attracted more than 100 participants with more than 300 submissions, indicating the broad interest in the research communities. This paper describes the settings of our challenge and discusses the winning methods of each track in more detail. We observe that the human reconstruction task is becoming mature even under heavy occlusion settings while object pose estimation and joint reconstruction remain challenging tasks. With the growing interest in interaction modeling, we hope this report can provide useful insights and foster future research in this direction. Our workshop website can be found at \href{this https URL}{this https URL}.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Semantic Draw Engineering for Text-to-Image Creation</b></summary>
  <p><b>编号</b>：[216]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04116</p>
  <p><b>作者</b>：Yang Li,  Huaqiang Jiang,  Yangkai Wu</p>
  <p><b>备注</b>：6pages, 5 figures</p>
  <p><b>关键词</b>：Generative Adversarial Networks, Adversarial Networks, Generative Adversarial, conducted through Generative, Networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Text-to-image generation is conducted through Generative Adversarial Networks (GANs) or transformer models. However, the current challenge lies in accurately generating images based on textual descriptions, especially in scenarios where the content and theme of the target image are ambiguous. In this paper, we propose a method that utilizes artificial intelligence models for thematic creativity, followed by a classification modeling of the actual painting process. The method involves converting all visual elements into quantifiable data structures before creating images. We evaluate the effectiveness of this approach in terms of semantic accuracy, image reproducibility, and computational efficiency, in comparison with existing image generation algorithms.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Timeline-based Process Discovery</b></summary>
  <p><b>编号</b>：[217]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04114</p>
  <p><b>作者</b>：Harleen Kaur,  Jan Mendling,  Christoffer Rubensson,  Timotheus Kampik</p>
  <p><b>备注</b>：8 pages</p>
  <p><b>关键词</b>：business processes, key concern, provide insights, insights into performance, performance aspects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A key concern of automatic process discovery is to provide insights into performance aspects of business processes. Waiting times are of particular importance in this context. For that reason, it is surprising that current techniques for automatic process discovery generate directly-follows graphs and comparable process models, but often miss the opportunity to explicitly represent the time axis. In this paper, we present an approach for automatically constructing process models that explicitly align with a time axis. We exemplify our approach for directly-follows graphs. Our evaluation using two BPIC datasets and a proprietary dataset highlight the benefits of this representation in comparison to standard layout techniques.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：U-Mamba: Enhancing Long-range Dependency for Biomedical Image  Segmentation</b></summary>
  <p><b>编号</b>：[223]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04722</p>
  <p><b>作者</b>：Jun Ma,  Feifei Li,  Bo Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Convolutional Neural Networks, handle long-range dependencies, Neural Networks, biomedical image segmentation, Convolutional Neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Convolutional Neural Networks (CNNs) and Transformers have been the most popular architectures for biomedical image segmentation, but both of them have limited ability to handle long-range dependencies because of inherent locality or computational complexity. To address this challenge, we introduce U-Mamba, a general-purpose network for biomedical image segmentation. Inspired by the State Space Sequence Models (SSMs), a new family of deep sequence models known for their strong capability in handling long sequences, we design a hybrid CNN-SSM block that integrates the local feature extraction power of convolutional layers with the abilities of SSMs for capturing the long-range dependency. Moreover, U-Mamba enjoys a self-configuring mechanism, allowing it to automatically adapt to various datasets without manual intervention. We conduct extensive experiments on four diverse tasks, including the 3D abdominal organ segmentation in CT and MR images, instrument segmentation in endoscopy images, and cell segmentation in microscopy images. The results reveal that U-Mamba outperforms state-of-the-art CNN-based and Transformer-based segmentation networks across all tasks. This opens new avenues for efficient long-range dependency modeling in biomedical image analysis. The code, models, and data are publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：An Automatic Cascaded Model for Hemorrhagic Stroke Segmentation and  Hemorrhagic Volume Estimation</b></summary>
  <p><b>编号</b>：[227]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04570</p>
  <p><b>作者</b>：Weijin Xu,  Zhuang Sha,  Huihua Yang,  Rongcai Jiang,  Zhanying Li,  Wentao Liu,  Ruisheng Su</p>
  <p><b>备注</b>：Accepted by SWITCH2023: Stroke Workshop on Imaging and Treatment CHallenges, a workshop at MICCAI 2023</p>
  <p><b>关键词</b>：great health threat, health threat, rapid onset, condition that poses, poses a great</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Hemorrhagic Stroke (HS) has a rapid onset and is a serious condition that poses a great health threat. Promptly and accurately delineating the bleeding region and estimating the volume of bleeding in Computer Tomography (CT) images can assist clinicians in treatment planning, leading to improved treatment outcomes for patients. In this paper, a cascaded 3D model is constructed based on UNet to perform a two-stage segmentation of the hemorrhage area in CT images from rough to fine, and the hemorrhage volume is automatically calculated from the segmented area. On a dataset with 341 cases of hemorrhagic stroke CT scans, the proposed model provides high-quality segmentation outcome with higher accuracy (DSC 85.66%) and better computation efficiency (6.2 second per sample) when compared to the traditional Tada formula with respect to hemorrhage volume estimation.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Spatio-Temporal Turbulence Mitigation: A Translational Perspective</b></summary>
  <p><b>编号</b>：[241]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04244</p>
  <p><b>作者</b>：Xingguang Zhang,  Nicholas Chimitt,  Yiheng Chi,  Zhiyuan Mao,  Stanley H. Chan</p>
  <p><b>备注</b>：project page this https URL</p>
  <p><b>关键词</b>：Recovering images distorted, challenging inverse problem, inverse problem due, Recovering images, Atmospheric TUrbulence Mitigation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recovering images distorted by atmospheric turbulence is a challenging inverse problem due to the stochastic nature of turbulence. Although numerous turbulence mitigation (TM) algorithms have been proposed, their efficiency and generalization to real-world dynamic scenarios remain severely limited. Building upon the intuitions of classical TM algorithms, we present the Deep Atmospheric TUrbulence Mitigation network (DATUM). DATUM aims to overcome major challenges when transitioning from classical to deep learning approaches. By carefully integrating the merits of classical multi-frame TM methods into a deep network structure, we demonstrate that DATUM can efficiently perform long-range temporal aggregation using a recurrent fashion, while deformable attention and temporal-channel attention seamlessly facilitate pixel registration and lucky imaging. With additional supervision, tilt and blur degradation can be jointly mitigated. These inductive biases empower DATUM to significantly outperform existing methods while delivering a tenfold increase in processing speed. A large-scale training dataset, ATSyn, is presented as a co-invention to enable generalization in real turbulence. Our code and datasets will be available at \href{this https URL}{\textcolor{pink}{this https URL}}</p>
  </details>
</details>
<h1>机器学习</h1>
<details>
  <summary>1. <b>标题：On the Effect of Contextual Information on Human Delegation Behavior in  Human-AI collaboration</b></summary>
  <p><b>编号</b>：[2]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04729</p>
  <p><b>作者</b>：Philipp Spitzer,  Joshua Holstein,  Patrick Hemmer,  Michael Vössing,  Niklas Kühl,  Dominik Martin,  Gerhard Satzger</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：constantly increasing capabilities, artificial intelligence, open new possibilities, constantly increasing, delegate instances</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The constantly increasing capabilities of artificial intelligence (AI) open new possibilities for human-AI collaboration. One promising approach to leverage existing complementary capabilities is allowing humans to delegate individual instances to the AI. However, enabling humans to delegate instances effectively requires them to assess both their own and the AI's capabilities in the context of the given task. In this work, we explore the effects of providing contextual information on human decisions to delegate instances to an AI. We find that providing participants with contextual information significantly improves the human-AI team performance. Additionally, we show that the delegation behavior changes significantly when participants receive varying types of contextual information. Overall, this research advances the understanding of human-AI interaction in human delegation and provides actionable insights for designing more effective collaborative systems.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：AI-based Mapping of the Conservation Status of Orchid Assemblages at  Global Scale</b></summary>
  <p><b>编号</b>：[16]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04691</p>
  <p><b>作者</b>：Joaquim Estopinan,  Maximilien Servajean,  Pierre Bonnet,  Alexis Joly,  François Munoz</p>
  <p><b>备注</b>：21 pages, 4 figures. Website URL: this https URL Data and code: this https URL</p>
  <p><b>关键词</b>：Species Distribution Model, conservation status, accurate global maps, global maps showing, widely recognised</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Although increasing threats on biodiversity are now widely recognised, there are no accurate global maps showing whether and where species assemblages are at risk. We hereby assess and map at kilometre resolution the conservation status of the iconic orchid family, and discuss the insights conveyed at multiple scales. We introduce a new Deep Species Distribution Model trained on 1M occurrences of 14K orchid species to predict their assemblages at global scale and at kilometre resolution. We propose two main indicators of the conservation status of the assemblages: (i) the proportion of threatened species, and (ii) the status of the most threatened species in the assemblage. We show and analyze the variation of these indicators at World scale and in relation to currently protected areas in Sumatra island. Global and interactive maps available online show the indicators of conservation status of orchid assemblages, with sharp spatial variations at all scales. The highest level of threat is found at Madagascar and the neighbouring islands. In Sumatra, we found good correspondence of protected areas with our indicators, but supplementing current IUCN assessments with status predictions results in alarming levels of species threat across the island. Recent advances in deep learning enable reliable mapping of the conservation status of species assemblages on a global scale. As an umbrella taxon, orchid family provides a reference for identifying vulnerable ecosystems worldwide, and prioritising conservation actions both at international and local levels.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Mixture of multilayer stochastic block models for multiview clustering</b></summary>
  <p><b>编号</b>：[17]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04682</p>
  <p><b>作者</b>：Kylliann De Santiago,  Marie Szafranski,  Christophe Ambroise</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：aggregating multiple clustering, multiple clustering coming, propose an original, aggregating multiple, Stochastic Block Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we propose an original method for aggregating multiple clustering coming from different sources of information. Each partition is encoded by a co-membership matrix between observations. Our approach uses a mixture of multilayer Stochastic Block Models (SBM) to group co-membership matrices with similar information into components and to partition observations into different clusters, taking into account their specificities within the components. The identifiability of the model parameters is established and a variational Bayesian EM algorithm is proposed for the estimation of these parameters. The Bayesian framework allows for selecting an optimal number of clusters and components. The proposed approach is compared using synthetic data with consensus clustering and tensor-based algorithms for community detection in large-scale complex networks. Finally, the method is utilized to analyze global food trading networks, leading to structures of interest.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation</b></summary>
  <p><b>编号</b>：[19]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04679</p>
  <p><b>作者</b>：Mahdi Nikdan,  Soroush Tabesh,  Dan Alistarh</p>
  <p><b>备注</b>：Preliminary version</p>
  <p><b>关键词</b>：large language models, investigate parameter-efficient fine-tuning, PEFT method called, language models, called Robust Adaptation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memory- and computationally-efficient training. Our code will be made available at this https URL}{\texttt{this https URL</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Transfer-Learning-Based Autotuning Using Gaussian Copula</b></summary>
  <p><b>编号</b>：[22]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04669</p>
  <p><b>作者</b>：Thomas Randall (1),  Jaehoon Koo (2),  Brice Videau (3),  Michael Kruse (3),  Xingfu Wu (3),  Paul Hovland (3),  Mary Hall (4),  Rong Ge (1),  Prasanna Balaprakash (5) ((1) Clemson University, (2) Hanyang University, (3) Argonne National Laboratory, (4) University of Utah, (5) Oak Ridge National Laboratory)</p>
  <p><b>备注</b>：13 pages, 5 figures, 7 tables, the definitive version of this work is published in the Proceedings of the ACM International Conference on Supercomputing 2023, available at this https URL</p>
  <p><b>关键词</b>：diverse high-performance computing, solve larger problems, high-performance computing, HPC systems, diverse high-performance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As diverse high-performance computing (HPC) systems are built, many opportunities arise for applications to solve larger problems than ever before. Given the significantly increased complexity of these HPC systems and application tuning, empirical performance tuning, such as autotuning, has emerged as a promising approach in recent years. Despite its effectiveness, autotuning is often a computationally expensive approach. Transfer learning (TL)-based autotuning seeks to address this issue by leveraging the data from prior tuning. Current TL methods for autotuning spend significant time modeling the relationship between parameter configurations and performance, which is ineffective for few-shot (that is, few empirical evaluations) tuning on new tasks. We introduce the first generative TL-based autotuning approach based on the Gaussian copula (GC) to model the high-performing regions of the search space from prior data and then generate high-performing configurations for new tasks. This allows a sampling-based approach that maximizes few-shot performance and provides the first probabilistic estimation of the few-shot budget for effective TL-based autotuning. We compare our generative TL approach with state-of-the-art autotuning techniques on several benchmarks. We find that the GC is capable of achieving 64.37% of peak few-shot performance in its first evaluation. Furthermore, the GC model can determine a few-shot transfer budget that yields up to 33.39$\times$ speedup, a dramatic improvement over the 20.58$\times$ speedup using prior techniques.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Benchmark Analysis of Various Pre-trained Deep Learning Models on ASSIRA  Cats and Dogs Dataset</b></summary>
  <p><b>编号</b>：[23]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04666</p>
  <p><b>作者</b>：Galib Muhammad Shahriar Himel,  Md. Masudul Islam</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：NVIDIA GeForce RTX, image classification, grown in popularity, basic application, application and implementation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As the most basic application and implementation of deep learning, image classification has grown in popularity. Various datasets are provided by renowned data science communities for benchmarking machine learning algorithms and pre-trained models. The ASSIRA Cats & Dogs dataset is one of them and is being used in this research for its overall acceptance and benchmark standards. A comparison of various pre-trained models is demonstrated by using different types of optimizers and loss functions. Hyper-parameters are changed to gain the best result from a model. By applying this approach, we have got higher accuracy without major changes in the training model. To run the experiment, we used three different computer architectures: a laptop equipped with NVIDIA GeForce GTX 1070, a laptop equipped with NVIDIA GeForce RTX 3080Ti, and a desktop equipped with NVIDIA GeForce RTX 3090. The acquired results demonstrate supremacy in terms of accuracy over the previously done experiments on this dataset. From this experiment, the highest accuracy which is 99.65% is gained using the NASNet Large.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：A novel framework for generalization of deep hidden physics models</b></summary>
  <p><b>编号</b>：[32]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04648</p>
  <p><b>作者</b>：Vijay Kag,  Birupaksha Pal</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：oft encountered problem, complex physics involved, full system information, information is unknown, oft encountered</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Modelling of systems where the full system information is unknown is an oft encountered problem for various engineering and industrial applications, as it's either impossible to consider all the complex physics involved or simpler models are considered to keep within the limits of the available resources. Recent advances in greybox modelling like the deep hidden physics models address this space by combining data and physics. However, for most real-life applications, model generalizability is a key issue, as retraining a model for every small change in system inputs and parameters or modification in domain configuration can render the model economically unviable. In this work we present a novel enhancement to the idea of hidden physics models which can generalize for changes in system inputs, parameters and domains. We also show that this approach holds promise in system discovery as well and helps learn the hidden physics for the changed system inputs, parameters and domain configuration.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Advancing Ante-Hoc Explainable Models through Generative Adversarial  Networks</b></summary>
  <p><b>编号</b>：[33]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04647</p>
  <p><b>作者</b>：Tanmay Garg,  Deepika Vemuri,  Vineeth N Balasubramanian</p>
  <p><b>备注</b>：Paper accepted in Human-Centric Representation Learning workshop at AAAI 2024 (this https URL)</p>
  <p><b>关键词</b>：enhancing model interpretability, concept learning framework, learning framework, framework for enhancing, interpretability and performance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a novel concept learning framework for enhancing model interpretability and performance in visual classification tasks. Our approach appends an unsupervised explanation generator to the primary classifier network and makes use of adversarial training. During training, the explanation module is optimized to extract visual concepts from the classifier's latent representations, while the GAN-based module aims to discriminate images generated from concepts, from true images. This joint training scheme enables the model to implicitly align its internally learned concepts with human-interpretable visual properties. Comprehensive experiments demonstrate the robustness of our approach, while producing coherent concept activations. We analyse the learned concepts, showing their semantic concordance with object parts and visual attributes. We also study how perturbations in the adversarial training protocol impact both classification and concept acquisition. In summary, this work presents a significant step towards building inherently interpretable deep vision models with task-aligned concept representations - a key enabler for developing trustworthy AI for real-world perception tasks.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Applying Large Language Models API to Issue Classification Problem</b></summary>
  <p><b>编号</b>：[35]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04637</p>
  <p><b>作者</b>：Gabriel Aracena,  Kyle Luster,  Fabio Santos,  Igor Steinmacher,  Marco A. Gerosa</p>
  <p><b>备注</b>：4 pages, 1 figure, NLBSE and ICSE conference submission, ACM formatted, pre print</p>
  <p><b>关键词</b>：critical problems promptly, optimize resource allocation, address critical problems, problems promptly, optimize resource</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Effective prioritization of issue reports is crucial in software engineering to optimize resource allocation and address critical problems promptly. However, the manual classification of issue reports for prioritization is laborious and lacks scalability. Alternatively, many open source software (OSS) projects employ automated processes for this task, albeit relying on substantial datasets for adequate training. This research seeks to devise an automated approach that ensures reliability in issue prioritization, even when trained on smaller datasets. Our proposed methodology harnesses the power of Generative Pre-trained Transformers (GPT), recognizing their potential to efficiently handle this task. By leveraging the capabilities of such models, we aim to develop a robust system for prioritizing issue reports accurately, mitigating the necessity for extensive training data while maintaining reliability. In our research, we have developed a reliable GPT-based approach to accurately label and prioritize issue reports with a reduced training dataset. By reducing reliance on massive data requirements and focusing on few-shot fine-tuning, our methodology offers a more accessible and efficient solution for issue prioritization in software engineering. Our model predicted issue types in individual projects up to 93.2% in precision, 95% in recall, and 89.3% in F1-score.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Hypercomplex neural network in time series forecasting of stock data</b></summary>
  <p><b>编号</b>：[37]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04632</p>
  <p><b>作者</b>：Radosław Kycia,  Agnieszka Niemczynowicz</p>
  <p><b>备注</b>：15 pages</p>
  <p><b>关键词</b>：Stock Market time, Market time series, time series, related Stock Market, time series prediction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The three classes of architectures for time series prediction were tested. They differ by input layers which contain either convolutional, LSTM, or dense hypercomplex layers for 4D algebras. The input was four related Stock Market time series, and the prediction of one of them is expected. The optimization of hyperparameters related to the classes of architectures was performed in order to compare the best neural networks within the class. The results show that in most cases, the architecture with a hypercomplex dense layer provides similar MAE accuracy to other architectures, however, with considerably less trainable parameters. Thanks to it, hypercomplex neural networks can be learned and process data faster than the other tested architectures. Moreover, the order of the input time series has an impact on effectively.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Deep Reinforcement Multi-agent Learning framework for Information  Gathering with Local Gaussian Processes for Water Monitoring</b></summary>
  <p><b>编号</b>：[38]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04631</p>
  <p><b>作者</b>：Samuel Yanes Luis,  Dmitriy Shutin,  Juan Marchal Gómez,  Daniel Gutiérrez Reina,  Sergio Toral Marín</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：hydrological resources involves, resources involves continuously, Local Gaussian Processes, involves continuously monitoring, Gaussian Processes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The conservation of hydrological resources involves continuously monitoring their contamination. A multi-agent system composed of autonomous surface vehicles is proposed in this paper to efficiently monitor the water quality. To achieve a safe control of the fleet, the fleet policy should be able to act based on measurements and to the the fleet state. It is proposed to use Local Gaussian Processes and Deep Reinforcement Learning to jointly obtain effective monitoring policies. Local Gaussian processes, unlike classical global Gaussian processes, can accurately model the information in a dissimilar spatial correlation which captures more accurately the water quality information. A Deep convolutional policy is proposed, that bases the decisions on the observation on the mean and variance of this model, by means of an information gain reward. Using a Double Deep Q-Learning algorithm, agents are trained to minimize the estimation error in a safe manner thanks to a Consensus-based heuristic. Simulation results indicate an improvement of up to 24% in terms of the mean absolute error with the proposed models. Also, training results with 1-3 agents indicate that our proposed approach returns 20% and 24% smaller average estimation errors for, respectively, monitoring water quality variables and monitoring algae blooms, as compared to state-of-the-art approaches</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Distribution-Free Conformal Joint Prediction Regions for Neural Marked  Temporal Point Processes</b></summary>
  <p><b>编号</b>：[45]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04612</p>
  <p><b>作者</b>：Victor Dheur,  Tanguy Bosser,  Rafael Izbicki,  Souhaib Ben Taieb</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Temporal Point Processes, labeled events observed, arrival time, event arrival time, prediction regions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sequences of labeled events observed at irregular intervals in continuous time are ubiquitous across various fields. Temporal Point Processes (TPPs) provide a mathematical framework for modeling these sequences, enabling inferences such as predicting the arrival time of future events and their associated label, called mark. However, due to model misspecification or lack of training data, these probabilistic models may provide a poor approximation of the true, unknown underlying process, with prediction regions extracted from them being unreliable estimates of the underlying uncertainty. This paper develops more reliable methods for uncertainty quantification in neural TPP models via the framework of conformal prediction. A primary objective is to generate a distribution-free joint prediction region for the arrival time and mark, with a finite-sample marginal coverage guarantee. A key challenge is to handle both a strictly positive, continuous response and a categorical response, without distributional assumptions. We first consider a simple but overly conservative approach that combines individual prediction regions for the event arrival time and mark. Then, we introduce a more effective method based on bivariate highest density regions derived from the joint predictive density of event arrival time and mark. By leveraging the dependencies between these two variables, this method exclude unlikely combinations of the two, resulting in sharper prediction regions while still attaining the pre-specified coverage level. We also explore the generation of individual univariate prediction regions for arrival times and marks through conformal regression and classification techniques. Moreover, we investigate the stronger notion of conditional coverage. Finally, through extensive experimentation on both simulated and real-world datasets, we assess the validity and efficiency of these methods.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Enhanced Distribution Alignment for Post-Training Quantization of  Diffusion Models</b></summary>
  <p><b>编号</b>：[53]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04585</p>
  <p><b>作者</b>：Xuewen Liu,  Zhikai Li,  Junrui Xiao,  Qingyi Gu</p>
  <p><b>备注</b>：16 pages, 15 figures</p>
  <p><b>关键词</b>：iterative noise estimation, achieved great success, image generation tasks, Diffusion models, noise estimation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diffusion models have achieved great success in image generation tasks through iterative noise estimation. However, the heavy denoising process and complex neural networks hinder their low-latency applications in real-world scenarios. Quantization can effectively reduce model complexity, and post-training quantization (PTQ), which does not require fine-tuning, is highly promising in accelerating the denoising process. Unfortunately, we find that due to the highly dynamic distribution of activations in different denoising steps, existing PTQ methods for diffusion models suffer from distribution mismatch issues at both calibration sample level and reconstruction output level, which makes the performance far from satisfactory, especially in low-bit cases. In this paper, we propose Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models (EDA-DM) to address the above issues. Specifically, at the calibration sample level, we select calibration samples based on the density and diversity in the latent space, thus facilitating the alignment of their distribution with the overall samples; and at the reconstruction output level, we propose Fine-grained Block Reconstruction, which can align the outputs of the quantized model and the full-precision model at different network granularity. Extensive experiments demonstrate that EDA-DM outperforms the existing post-training quantization frameworks in both unconditional and conditional generation scenarios. At low-bit precision, the quantized models with our method even outperform the full-precision models on most datasets.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Masked Audio Generation using a Single Non-Autoregressive Transformer</b></summary>
  <p><b>编号</b>：[55]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04577</p>
  <p><b>作者</b>：Alon Ziv,  Itai Gat,  Gael Le Lan,  Tal Remez,  Felix Kreuk,  Alexandre Défossez,  Jade Copet,  Gabriel Synnaeve,  Yossi Adi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：masked generative sequence, operates directly, generative sequence modeling, masked generative, MAGNeT</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce MAGNeT, a masked generative sequence modeling method that operates directly over several streams of audio tokens. Unlike prior work, MAGNeT is comprised of a single-stage, non-autoregressive transformer. During training, we predict spans of masked tokens obtained from a masking scheduler, while during inference we gradually construct the output sequence using several decoding steps. To further enhance the quality of the generated audio, we introduce a novel rescoring method in which, we leverage an external pre-trained model to rescore and rank predictions from MAGNeT, which will be then used for later decoding steps. Lastly, we explore a hybrid version of MAGNeT, in which we fuse between autoregressive and non-autoregressive models to generate the first few seconds in an autoregressive manner while the rest of the sequence is being decoded in parallel. We demonstrate the efficiency of MAGNeT for the task of text-to-music and text-to-audio generation and conduct an extensive empirical evaluation, considering both objective metrics and human studies. The proposed approach is comparable to the evaluated baselines, while being significantly faster (x7 faster than the autoregressive baseline). Through ablation studies and analysis, we shed light on the importance of each of the components comprising MAGNeT, together with pointing to the trade-offs between autoregressive and non-autoregressive modeling, considering latency, throughput, and generation quality. Samples are available on our demo page this https URL.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Robust Imitation Learning for Automated Game Testing</b></summary>
  <p><b>编号</b>：[57]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04572</p>
  <p><b>作者</b>：Pierluigi Vito Amadori,  Timothy Bradley,  Ryan Spick,  Guy Moss</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：long process, process that involves, involves many stages, product is ready, EVOLUTE</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Game development is a long process that involves many stages before a product is ready for the market. Human play testing is among the most time consuming, as testers are required to repeatedly perform tasks in the search for errors in the code. Therefore, automated testing is seen as a key technology for the gaming industry, as it would dramatically improve development costs and efficiency. Toward this end, we propose EVOLUTE, a novel imitation learning-based architecture that combines behavioural cloning (BC) with energy based models (EBMs). EVOLUTE is a two-stream ensemble model that splits the action space of autonomous agents into continuous and discrete tasks. The EBM stream handles the continuous tasks, to have a more refined and adaptive control, while the BC stream handles discrete actions, to ease training. We evaluate the performance of EVOLUTE in a shooting-and-driving game, where the agent is required to navigate and continuously identify targets to attack. The proposed model has higher generalisation capabilities than standard BC approaches, showing a wider range of behaviours and higher performances. Also, EVOLUTE is easier to train than a pure end-to-end EBM model, as discrete tasks can be quite sparse in the dataset and cause model training to explore a much wider set of possible actions while training.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：HyperGANStrument: Instrument Sound Synthesis and Editing with  Pitch-Invariant Hypernetworks</b></summary>
  <p><b>编号</b>：[60]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04558</p>
  <p><b>作者</b>：Zhe Zhang,  Taketo Akama</p>
  <p><b>备注</b>：5 pages, 3 figures, Accepted for 2024 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Audio examples: this https URL</p>
  <p><b>关键词</b>：instance conditioning technique, shown remarkable capabilities, synthesizing realistic instrument, pitch-invariant feature extractor, realistic instrument sounds</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>GANStrument, exploiting GANs with a pitch-invariant feature extractor and instance conditioning technique, has shown remarkable capabilities in synthesizing realistic instrument sounds. To further improve the reconstruction ability and pitch accuracy to enhance the editability of user-provided sound, we propose HyperGANStrument, which introduces a pitch-invariant hypernetwork to modulate the weights of a pre-trained GANStrument generator, given a one-shot sound as input. The hypernetwork modulation provides feedback for the generator in the reconstruction of the input sound. In addition, we take advantage of an adversarial fine-tuning scheme for the hypernetwork to improve the reconstruction fidelity and generation diversity of the generator. Experimental results show that the proposed model not only enhances the generation capability of GANStrument but also significantly improves the editability of synthesized sounds. Audio examples are available at the online demo page.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Evaluating Language Model Agency through Negotiations</b></summary>
  <p><b>编号</b>：[67]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04536</p>
  <p><b>作者</b>：Tim R. Davidson,  Veniamin Veselovsky,  Martin Josifoski,  Maxime Peyrard,  Antoine Bosselut,  Michal Kosinski,  Robert West</p>
  <p><b>备注</b>：Code and link to project data are made available at this https URL</p>
  <p><b>关键词</b>：exploit Language Models', increasingly exploit Language, Language Models', display agent-like behavior, governments increasingly exploit</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Companies, organizations, and governments increasingly exploit Language Models' (LM) remarkable capability to display agent-like behavior. As LMs are adopted to perform tasks with growing autonomy, there exists an urgent need for reliable and scalable evaluation benchmarks. Current, predominantly static LM benchmarks are ill-suited to evaluate such dynamic applications. Thus, we propose jointly evaluating LM performance and alignment through the lenses of negotiation games. We argue that this common task better reflects real-world deployment conditions while offering insights into LMs' decision-making processes. Crucially, negotiation games allow us to study multi-turn, and cross-model interactions, modulate complexity, and side-step accidental data leakage in evaluation. We report results for six publicly accessible LMs from several major providers on a variety of negotiation games, evaluating both self-play and cross-play performance. Noteworthy findings include: (i) open-source models are currently unable to complete these tasks; (ii) cooperative bargaining games prove challenging; and (iii) the most powerful models do not always "win".</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Rewriting the Code: A Simple Method for Large Language Model Augmented  Code Search</b></summary>
  <p><b>编号</b>：[75]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04514</p>
  <p><b>作者</b>：Haochen Li,  Xin Zhou,  Zhiqi Shen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, natural language queries, Language Models, Large Language, exemplar code snippets</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In code search, the Generation-Augmented Retrieval (GAR) framework, which generates exemplar code snippets to augment queries, has emerged as a promising strategy to address the principal challenge of modality misalignment between code snippets and natural language queries, particularly with the demonstrated code generation capabilities of Large Language Models (LLMs). Nevertheless, our preliminary investigations indicate that the improvements conferred by such an LLM-augmented framework are somewhat constrained. This limitation could potentially be ascribed to the fact that the generated codes, albeit functionally accurate, frequently display a pronounced stylistic deviation from the ground truth code in the codebase. In this paper, we extend the foundational GAR framework and propose a simple yet effective method that additionally Rewrites the Code (ReCo) within the codebase for style normalization. Experimental results demonstrate that ReCo significantly boosts retrieval accuracy across sparse (up to 35.7%), zero-shot dense (up to 27.6%), and fine-tuned dense (up to 23.6%) retrieval settings in diverse search scenarios. To further elucidate the advantages of ReCo and stimulate research in code style normalization, we introduce Code Style Similarity, the first metric tailored to quantify stylistic similarities in code. Notably, our empirical findings reveal the inadequacy of existing metrics in capturing stylistic nuances.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Data-driven Nonlinear Model Reduction using Koopman Theory: Integrated  Control Form and NMPC Case Study</b></summary>
  <p><b>编号</b>：[77]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04508</p>
  <p><b>作者</b>：Jan C. Schulze,  Alexander Mitsos</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：nonlinear dynamical systems, data-driven model reduction, Koopman theory, theory for data-driven, dynamical systems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We use Koopman theory for data-driven model reduction of nonlinear dynamical systems with controls. We propose generic model structures combining delay-coordinate encoding of measurements and full-state decoding to integrate reduced Koopman modeling and state estimation. We present a deep-learning approach to train the proposed models. A case study demonstrates that our approach provides accurate control models and enables real-time capable nonlinear model predictive control of a high-purity cryogenic distillation column.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：SpiNNaker2: A Large-Scale Neuromorphic System for Event-Based and  Asynchronous Machine Learning</b></summary>
  <p><b>编号</b>：[82]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04491</p>
  <p><b>作者</b>：Hector A. Gonzalez,  Jiaxin Huang,  Florian Kelber,  Khaleelulla Khan Nazeer,  Tim Langer,  Chen Liu,  Matthias Lohrmann,  Amirhossein Rostami,  Mark Schöne,  Bernhard Vogginger,  Timo C. Wunderlich,  Yexin Yan,  Mahmoud Akl,  Christian Mayr</p>
  <p><b>备注</b>：Submitted at the Workshop on Machine Learning with New Compute Paradigms at NeurIPS 2023 (MLNPCP 2023)</p>
  <p><b>关键词</b>：domain specific hardware, specific hardware accelerators, machine learning research, machine learning, domain specific</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The joint progress of artificial neural networks (ANNs) and domain specific hardware accelerators such as GPUs and TPUs took over many domains of machine learning research. This development is accompanied by a rapid growth of the required computational demands for larger models and more data. Concurrently, emerging properties of foundation models such as in-context learning drive new opportunities for machine learning applications. However, the computational cost of such applications is a limiting factor of the technology in data centers, and more importantly in mobile devices and edge systems. To mediate the energy footprint and non-trivial latency of contemporary systems, neuromorphic computing systems deeply integrate computational principles of neurobiological systems by leveraging low-power analog and digital technologies. SpiNNaker2 is a digital neuromorphic chip developed for scalable machine learning. The event-based and asynchronous design of SpiNNaker2 allows the composition of large-scale systems involving thousands of chips. This work features the operating principles of SpiNNaker2 systems, outlining the prototype of novel machine learning applications. These applications range from ANNs over bio-inspired spiking neural networks to generalized event-based neural networks. With the successful development and deployment of SpiNNaker2, we aim to facilitate the advancement of event-based and asynchronous algorithms for future generations of machine learning systems.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Optimal Survival Trees: A Dynamic Programming Approach</b></summary>
  <p><b>编号</b>：[83]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04489</p>
  <p><b>作者</b>：Tim Huisman,  Jacobus G. M. van der Linden,  Emir Demirović</p>
  <p><b>备注</b>：Published at AAAI-24</p>
  <p><b>关键词</b>：singular unrepeated events, Survival analysis studies, unrepeated events, based on historical, historical data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Survival analysis studies and predicts the time of death, or other singular unrepeated events, based on historical data, while the true time of death for some instances is unknown. Survival trees enable the discovery of complex nonlinear relations in a compact human comprehensible model, by recursively splitting the population and predicting a distinct survival distribution in each leaf node. We use dynamic programming to provide the first survival tree method with optimality guarantees, enabling the assessment of the optimality gap of heuristics. We improve the scalability of our method through a special algorithm for computing trees up to depth two. The experiments show that our method's run time even outperforms some heuristics for realistic cases while obtaining similar out-of-sample performance with the state-of-the-art.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Continuously Learning New Words in Automatic Speech Recognition</b></summary>
  <p><b>编号</b>：[87]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04482</p>
  <p><b>作者</b>：Christian Huber,  Alexander Waibel</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Automatic Speech Recognition, Automatic Speech, Speech Recognition, recent advances, Automatic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite recent advances, Automatic Speech Recognition (ASR) systems are still far from perfect. Typical errors include acronyms, named entities and domain-specific special words for which little or no data is available. To address the problem of recognizing these words, we propose an self-supervised continual learning approach. Given the audio of a lecture talk with corresponding slides, we bias the model towards decoding new words from the slides by using a memory-enhanced ASR model from previous work. Then, we perform inference on the talk, collecting utterances that contain detected new words into an adaptation dataset. Continual learning is then performed on this set by adapting low-rank matrix weights added to each weight matrix of the model. The whole procedure is iterated for many talks. We show that with this approach, we obtain increasing performance on the new words when they occur more frequently (more than 80% recall) while preserving the general performance of the model.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：A Survey on Efficient Federated Learning Methods for Foundation Model  Training</b></summary>
  <p><b>编号</b>：[90]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04472</p>
  <p><b>作者</b>：Herbert Woisetschläger,  Alexander Isenko,  Shiqiang Wang,  Ruben Mayer,  Hans-Arno Jacobsen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：facilitate privacy-preserving collaborative, Federated Learning, privacy-preserving collaborative training, established technique, technique to facilitate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Federated Learning (FL) has become an established technique to facilitate privacy-preserving collaborative training. However, new approaches to FL often discuss their contributions involving small deep-learning models only. With the tremendous success of transformer models, the following question arises: What is necessary to operationalize foundation models in an FL application? Knowing that computation and communication often take up similar amounts of time in FL, we introduce a novel taxonomy focused on computational and communication efficiency methods in FL applications. This said, these methods aim to optimize the training time and reduce communication between clients and the server. We also look at the current state of widely used FL frameworks and discuss future research potentials based on existing approaches in FL research and beyond.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：PhilEO Bench: Evaluating Geo-Spatial Foundation Models</b></summary>
  <p><b>编号</b>：[93]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04464</p>
  <p><b>作者</b>：Casper Fibaek,  Luke Camilleri,  Andreas Luyts,  Nikolaos Dionelis,  Bertrand Le Saux</p>
  <p><b>备注</b>：6 pages, 5 figures, Submitted to IGARSS 2024</p>
  <p><b>关键词</b>：Earth Observation, captured by Earth, Foundation Models, constellation generating, makes Remote Sensing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Massive amounts of unlabelled data are captured by Earth Observation (EO) satellites, with the Sentinel-2 constellation generating 1.6 TB of data daily. This makes Remote Sensing a data-rich domain well suited to Machine Learning (ML) solutions. However, a bottleneck in applying ML models to EO is the lack of annotated data as annotation is a labour-intensive and costly process. As a result, research in this domain has focused on Self-Supervised Learning and Foundation Model approaches. This paper addresses the need to evaluate different Foundation Models on a fair and uniform benchmark by introducing the PhilEO Bench, a novel evaluation framework for EO Foundation Models. The framework comprises of a testbed and a novel 400 GB Sentinel-2 dataset containing labels for three downstream tasks, building density estimation, road segmentation, and land cover classification. We present experiments using our framework evaluating different Foundation Models, including Prithvi and SatMAE, at multiple n-shots and convergence rates.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：AI Competitions and Benchmarks, Practical issues: Proposals, grant  money, sponsors, prizes, dissemination, publicity</b></summary>
  <p><b>编号</b>：[98]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04452</p>
  <p><b>作者</b>：Magali Richard (TIMC-MAGe),  Yuna Blum (IGDR),  Justin Guinney,  Gustavo Stolovitzky,  Adrien Pavão (LRI)</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：pragmatic aspects involved, comprehensive overview, pragmatic aspects, aspects involved, involved in organizing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This chapter provides a comprehensive overview of the pragmatic aspects involved in organizing AI competitions. We begin by discussing strategies to incentivize participation, touching upon effective communication techniques, aligning with trending topics in the field, structuring awards, potential recruitment opportunities, and more. We then shift to the essence of community engagement, and into organizational best practices and effective means of disseminating challenge outputs. Lastly, the chapter addresses the logistics, exposing on costs, required manpower, and resource allocation for effectively managing and executing a challenge. By examining these practical problems, readers will gain actionable insights to navigate the multifaceted landscape of AI competition organization, from inception to completion.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Meta-forests: Domain generalization on random forests with meta-learning</b></summary>
  <p><b>编号</b>：[105]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04425</p>
  <p><b>作者</b>：Yuyang Sun,  Panagiotis Kosmas</p>
  <p><b>备注</b>：This paper is accepted by ACML2023</p>
  <p><b>关键词</b>：popular machine learning, machine learning technique, unseen target domain, multiple source domains, machine learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Domain generalization is a popular machine learning technique that enables models to perform well on the unseen target domain, by learning from multiple source domains. Domain generalization is useful in cases where data is limited, difficult, or expensive to collect, such as in object recognition and biomedicine. In this paper, we propose a novel domain generalization algorithm called "meta-forests", which builds upon the basic random forests model by incorporating the meta-learning strategy and maximum mean discrepancy measure. The aim of meta-forests is to enhance the generalization ability of classifiers by reducing the correlation among trees and increasing their strength. More specifically, meta-forests conducts meta-learning optimization during each meta-task, while also utilizing the maximum mean discrepancy as a regularization term to penalize poor generalization performance in the meta-test process. To evaluate the effectiveness of our algorithm, we test it on two publicly object recognition datasets and a glucose monitoring dataset that we have used in a previous study. Our results show that meta-forests outperforms state-of-the-art approaches in terms of generalization performance on both object recognition and glucose monitoring datasets.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Fine-Grained Embedding Dimension Optimization During Training for  Recommender Systems</b></summary>
  <p><b>编号</b>：[109]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04408</p>
  <p><b>作者</b>：Qinyi Luo,  Penghan Wang,  Wei Zhang,  Fan Lai,  Jiachen Mao,  Xiaohan Wei,  Jun Song,  Wei-Yu Tsai,  Shuai Yang,  Yuxi Hu,  Xuehai Qian</p>
  <p><b>备注</b>：16 pages, 9 figures</p>
  <p><b>关键词</b>：Deep Learning Recommender, modern Deep Learning, Learning Recommender Models, Deep Learning, Learning Recommender</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Huge embedding tables in modern Deep Learning Recommender Models (DLRM) require prohibitively large memory during training and inference. Aiming to reduce the memory footprint of training, this paper proposes FIne-grained In-Training Embedding Dimension optimization (FIITED). Given the observation that embedding vectors are not equally important, FIITED adjusts the dimension of each individual embedding vector continuously during training, assigning longer dimensions to more important embeddings while adapting to dynamic changes in data. A novel embedding storage system based on virtually-hashed physically-indexed hash tables is designed to efficiently implement the embedding dimension adjustment and effectively enable memory saving. Experiments on two industry models show that FIITED is able to reduce the size of embeddings by more than 65% while maintaining the trained model's quality, saving significantly more memory than a state-of-the-art in-training embedding pruning method. On public click-through rate prediction datasets, FIITED is able to prune up to 93.75%-99.75% embeddings without significant accuracy loss.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：IGNITE: Individualized GeNeration of Imputations in Time-series  Electronic health records</b></summary>
  <p><b>编号</b>：[113]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04402</p>
  <p><b>作者</b>：Ghadeer O. Ghosheh,  Jin Li,  Tingting Zhu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：fit individual-level differences, Electronic Health Records, Health Records present, individual-level differences, present a valuable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Electronic Health Records present a valuable modality for driving personalized medicine, where treatment is tailored to fit individual-level differences. For this purpose, many data-driven machine learning and statistical models rely on the wealth of longitudinal EHRs to study patients' physiological and treatment effects. However, longitudinal EHRs tend to be sparse and highly missing, where missingness could also be informative and reflect the underlying patient's health status. Therefore, the success of data-driven models for personalized medicine highly depends on how the EHR data is represented from physiological data, treatments, and the missing values in the data. To this end, we propose a novel deep-learning model that learns the underlying patient dynamics over time across multivariate data to generate personalized realistic values conditioning on an individual's demographic characteristics and treatments. Our proposed model, IGNITE (Individualized GeNeration of Imputations in Time-series Electronic health records), utilises a conditional dual-variational autoencoder augmented with dual-stage attention to generate missing values for an individual. In IGNITE, we further propose a novel individualized missingness mask (IMM), which helps our model generate values based on the individual's observed data and missingness patterns. We further extend the use of IGNITE from imputing missingness to a personalized data synthesizer, where it generates missing EHRs that were never observed prior or even generates new patients for various applications. We validate our model on three large publicly available datasets and show that IGNITE outperforms state-of-the-art approaches in missing data reconstruction and task prediction.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：The Role of Higher-Order Cognitive Models in Active Learning</b></summary>
  <p><b>编号</b>：[115]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04397</p>
  <p><b>作者</b>：Oskar Keurulainen,  Gokhan Alcan,  Ville Kyrki</p>
  <p><b>备注</b>：7 pages, to appear in the CAIHu bridge program at AAAI 2024</p>
  <p><b>关键词</b>：Building machines capable, Building machines, machines capable, capable of efficiently, efficiently collaborating</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Building machines capable of efficiently collaborating with humans has been a longstanding goal in artificial intelligence. Especially in the presence of uncertainties, optimal cooperation often requires that humans and artificial agents model each other's behavior and use these models to infer underlying goals, beliefs or intentions, potentially involving multiple levels of recursion. Empirical evidence for such higher-order cognition in human behavior is also provided by previous works in cognitive science, linguistics, and robotics. We advocate for a new paradigm for active learning for human feedback that utilises humans as active data sources while accounting for their higher levels of agency. In particular, we discuss how increasing level of agency results in qualitatively different forms of rational communication between an active learning system and a teacher. Additionally, we provide a practical example of active learning using a higher-order cognitive model. This is accompanied by a computational study that underscores the unique behaviors that this model produces.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Machine unlearning through fine-grained model parameters perturbation</b></summary>
  <p><b>编号</b>：[119]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04385</p>
  <p><b>作者</b>：Zhiwei Zuo,  Zhuo Tang,  Kenli Li,  Anwitaman Datta</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：unlearning, retracting data records, Machine unlearning, involve retracting data, inexact machine unlearning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine unlearning techniques, which involve retracting data records and reducing influence of said data on trained models, help with the user privacy protection objective but incur significant computational costs. Weight perturbation-based unlearning is a general approach, but it typically involves globally modifying the parameters. We propose fine-grained Top-K and Random-k parameters perturbed inexact machine unlearning strategies that address the privacy needs while keeping the computational costs tractable.
In order to demonstrate the efficacy of our strategies we also tackle the challenge of evaluating the effectiveness of machine unlearning by considering the model's generalization performance across both unlearning and remaining data. To better assess the unlearning effect and model generalization, we propose novel metrics, namely, the forgetting rate and memory retention rate. However, for inexact machine unlearning, current metrics are inadequate in quantifying the degree of forgetting that occurs after unlearning strategies are applied. To address this, we introduce SPD-GAN, which subtly perturbs the distribution of data targeted for unlearning. Then, we evaluate the degree of unlearning by measuring the performance difference of the models on the perturbed unlearning data before and after the unlearning process. By implementing these innovative techniques and metrics, we achieve computationally efficacious privacy protection in machine learning applications without significant sacrifice of model performance. Furthermore, this approach provides a novel method for evaluating the degree of unlearning.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Towards Explainable Artificial Intelligence (XAI): A Data Mining  Perspective</b></summary>
  <p><b>编号</b>：[122]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04374</p>
  <p><b>作者</b>：Haoyi Xiong,  Xuhong L,  Xiaofei Zhang,  Jiamin Chen,  Xinhao Sun,  Yuchen Li,  Zeyi Sun,  Mengnan Du</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：deep neural networks, neural networks, extensive efforts, accessible terms, complexity and lack</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Given the complexity and lack of transparency in deep neural networks (DNNs), extensive efforts have been made to make these systems more interpretable or explain their behaviors in accessible terms. Unlike most reviews, which focus on algorithmic and model-centric perspectives, this work takes a "data-centric" view, examining how data collection, processing, and analysis contribute to explainable AI (XAI). We categorize existing work into three categories subject to their purposes: interpretations of deep models, referring to feature attributions and reasoning processes that correlate data points with model outputs; influences of training data, examining the impact of training data nuances, such as data valuation and sample anomalies, on decision-making processes; and insights of domain knowledge, discovering latent patterns and fostering new knowledge from data and models to advance social values and scientific discovery. Specifically, we distill XAI methodologies into data mining operations on training and testing data across modalities, such as images, text, and tabular data, as well as on training logs, checkpoints, models and other DNN behavior descriptors. In this way, our study offers a comprehensive, data-centric examination of XAI from a lens of data mining methods and applications.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Air Quality Forecasting Using Machine Learning: A Global perspective  with Relevance to Low-Resource Settings</b></summary>
  <p><b>编号</b>：[124]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04369</p>
  <p><b>作者</b>：Mulomba Mukendi Christian,  Hyebong Choi</p>
  <p><b>备注</b>：16 pages. This is a conference proceeding Presented at: SIBR 2024 (Seoul) Conference on Interdisciplinary Business and Economics Research, 5th-6th January 2024, Seoul, South Korea</p>
  <p><b>关键词</b>：Air pollution stands, air quality, death globally, pollution stands, fourth leading</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Air pollution stands as the fourth leading cause of death globally. While extensive research has been conducted in this domain, most approaches rely on large datasets when it comes to prediction. This limits their applicability in low-resource settings though more vulnerable. This study addresses this gap by proposing a novel machine learning approach for accurate air quality prediction using two months of air quality data. By leveraging the World Weather Repository, the meteorological, air pollutant, and Air Quality Index features from 197 capital cities were considered to predict air quality for the next day. The evaluation of several machine learning models demonstrates the effectiveness of the Random Forest algorithm in generating reliable predictions, particularly when applied to classification rather than regression, approach which enhances the model's generalizability by 42%, achieving a cross-validation score of 0.38 for regression and 0.89 for classification. To instill confidence in the predictions, interpretable machine learning was considered. Finally, a cost estimation comparing the implementation of this solution in high-resource and low-resource settings is presented including a tentative of technology licensing business model. This research highlights the potential for resource-limited countries to independently predict air quality while awaiting larger datasets to further refine their predictions.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Enhancing Acute Kidney Injury Prediction through Integration of Drug  Features in Intensive Care Units</b></summary>
  <p><b>编号</b>：[125]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04368</p>
  <p><b>作者</b>：Gabriel D. M. Manalu,  Mulomba Mukendi Christian,  Songhee You,  Hyebong Choi</p>
  <p><b>备注</b>：9 pages, 2 tables</p>
  <p><b>关键词</b>：acute kidney injury, affect kidney function, adversely affect kidney, AKI prediction, kidney injury</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The relationship between acute kidney injury (AKI) prediction and nephrotoxic drugs, or drugs that adversely affect kidney function, is one that has yet to be explored in the critical care setting. One contributing factor to this gap in research is the limited investigation of drug modalities in the intensive care unit (ICU) context, due to the challenges of processing prescription data into the corresponding drug representations and a lack in the comprehensive understanding of these drug representations. This study addresses this gap by proposing a novel approach that leverages patient prescription data as a modality to improve existing models for AKI prediction. We base our research on Electronic Health Record (EHR) data, extracting the relevant patient prescription information and converting it into the selected drug representation for our research, the extended-connectivity fingerprint (ECFP). Furthermore, we adopt a unique multimodal approach, developing machine learning models and 1D Convolutional Neural Networks (CNN) applied to clinical drug representations, establishing a procedure which has not been used by any previous studies predicting AKI. The findings showcase a notable improvement in AKI prediction through the integration of drug embeddings and other patient cohort features. By using drug features represented as ECFP molecular fingerprints along with common cohort features such as demographics and lab test values, we achieved a considerable improvement in model performance for the AKI prediction task over the baseline model which does not include the drug representations as features, indicating that our distinct approach enhances existing baseline techniques and highlights the relevance of drug data in predicting AKI in the ICU setting</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：SoK: Facial Deepfake Detectors</b></summary>
  <p><b>编号</b>：[127]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04364</p>
  <p><b>作者</b>：Binh M. Le,  Jiwon Kim,  Shahroz Tariq,  Kristen Moore,  Alsharif Abuadbba,  Simon S. Woo</p>
  <p><b>备注</b>：18 pages, 6 figures, 5 table, under peer-review</p>
  <p><b>关键词</b>：threat to society, primarily due, creation and dissemination, rapidly emerged, ease of creation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deepfakes have rapidly emerged as a profound and serious threat to society, primarily due to their ease of creation and dissemination. This situation has triggered an accelerated development of deepfake detection technologies. However, many existing detectors rely heavily on lab-generated datasets for validation, which may not effectively prepare them for novel, emerging, and real-world deepfake techniques. In this paper, we conduct an extensive and comprehensive review and analysis of the latest state-of-the-art deepfake detectors, evaluating them against several critical criteria. These criteria facilitate the categorization of these detectors into 4 high-level groups and 13 fine-grained sub-groups, all aligned with a unified standard conceptual framework. This classification and framework offer deep and practical insights into the factors that affect detector efficacy. We assess the generalizability of 16 leading detectors across various standard attack scenarios, including black-box, white-box, and gray-box settings. Our systematized analysis and experimentation lay the groundwork for a deeper understanding of deepfake detectors and their generalizability, paving the way for future research focused on creating detectors adept at countering various attack scenarios. Additionally, this work offers insights for developing more proactive defenses against deepfakes.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：A Change Point Detection Integrated Remaining Useful Life Estimation  Model under Variable Operating Conditions</b></summary>
  <p><b>编号</b>：[134]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04351</p>
  <p><b>作者</b>：Anushiya Arunan,  Yan Qin,  Xiaoli Li,  Chau Yuen</p>
  <p><b>备注</b>：Accepted in Control Engineering Practice Journal with DOI: this https URL</p>
  <p><b>关键词</b>：health status evaluation, status evaluation serves, significant preliminary step, RUL estimation, change points</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>By informing the onset of the degradation process, health status evaluation serves as a significant preliminary step for reliable remaining useful life (RUL) estimation of complex equipment. This paper proposes a novel temporal dynamics learning-based model for detecting change points of individual devices, even under variable operating conditions, and utilises the learnt change points to improve the RUL estimation accuracy. During offline model development, the multivariate sensor data are decomposed to learn fused temporal correlation features that are generalisable and representative of normal operation dynamics across multiple operating conditions. Monitoring statistics and control limit thresholds for normal behaviour are dynamically constructed from these learnt temporal features for the unsupervised detection of device-level change points. The detected change points then inform the degradation data labelling for training a long short-term memory (LSTM)-based RUL estimation model. During online monitoring, the temporal correlation dynamics of a query device is monitored for breach of the control limit derived in offline training. If a change point is detected, the device's RUL is estimated with the well-trained offline model for early preventive action. Using C-MAPSS turbofan engines as the case study, the proposed method improved the accuracy by 5.6\% and 7.5\% for two scenarios with six operating conditions, when compared to existing LSTM-based RUL estimation models that do not consider heterogeneous change points.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Private Fine-tuning of Large Language Models with Zeroth-order  Optimization</b></summary>
  <p><b>编号</b>：[139]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04343</p>
  <p><b>作者</b>：Xinyu Tang,  Ashwinee Panda,  Milad Nasr,  Saeed Mahloujifar,  Prateek Mittal</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：datasets may run, Fine-tuning large pretrained, large pretrained models, private datasets, privacy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Fine-tuning large pretrained models on private datasets may run the risk of violating privacy. Differential privacy is a framework for mitigating privacy risks by enforcing algorithmic stability. DP-SGD enables training models with private data in a privacy-preserving manner, but raises new obstacles in the form of performance loss and significant engineering challenges. We introduce DP-ZO, a new method for fine-tuning large language models that preserves the privacy of training data by privatizing zeroth-order optimization. A key insight into the design of our method is that the direction of the gradient in SPSA, the zeroth-order algorithm we use, is always random and the only information that depends on private data is the step size, i.e., a scalar. Therefore, we only need to privatize the scalar step size, which is memory-efficient. DP-ZO, which can be instantiated with either Laplace or Gaussian noise, provides a strong privacy-utility trade-off across different tasks, and model sizes, under conservative privacy budgets. One noteworthy result is that DP-ZO exhibits just $1.86\%$ performance degradation due to privacy at $(1,10^{-5})$-DP when fine-tuning OPT-66B on 1000 training samples from SQuAD.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：G-Meta: Distributed Meta Learning in GPU Clusters for Large-Scale  Recommender Systems</b></summary>
  <p><b>编号</b>：[142]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04338</p>
  <p><b>作者</b>：Youshao Xiao,  Shangchun Zhao,  Zhenglei Zhou,  Zhaoxin Huan,  Lin Ju,  Xiaolu Zhang,  Lin Wang,  Jun Zhou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Deep Learning Recommendation, Learning Recommendation Models, Optimization-based Meta DLRM, Learning Recommendation, Meta DLRM models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, a new paradigm, meta learning, has been widely applied to Deep Learning Recommendation Models (DLRM) and significantly improves statistical performance, especially in cold-start scenarios. However, the existing systems are not tailored for meta learning based DLRM models and have critical problems regarding efficiency in distributed training in the GPU cluster. It is because the conventional deep learning pipeline is not optimized for two task-specific datasets and two update loops in meta learning. This paper provides a high-performance framework for large-scale training for Optimization-based Meta DLRM models over the \textbf{G}PU cluster, namely \textbf{G}-Meta. Firstly, G-Meta utilizes both data parallelism and model parallelism with careful orchestration regarding computation and communication efficiency, to enable high-speed distributed training. Secondly, it proposes a Meta-IO pipeline for efficient data ingestion to alleviate the I/O bottleneck. Various experimental results show that G-Meta achieves notable training speed without loss of statistical performance. Since early 2022, G-Meta has been deployed in Alipay's core advertising and recommender system, shrinking the continuous delivery of models by four times. It also obtains 6.48\% improvement in Conversion Rate (CVR) and 1.06\% increase in CPM (Cost Per Mille) in Alipay's homepage display advertising, with the benefit of larger training samples and tasks.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Deep Efficient Private Neighbor Generation for Subgraph Federated  Learning</b></summary>
  <p><b>编号</b>：[143]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04336</p>
  <p><b>作者</b>：Ke Zhang,  Lichao Sun,  Bolin Ding,  Siu Ming Yiu,  Carl Yang</p>
  <p><b>备注</b>：Accepted to SDM 2024</p>
  <p><b>关键词</b>：multiple data owners, realistic applications, fragmented and separately, separately stored, stored by multiple</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Behemoth graphs are often fragmented and separately stored by multiple data owners as distributed subgraphs in many realistic applications. Without harming data privacy, it is natural to consider the subgraph federated learning (subgraph FL) scenario, where each local client holds a subgraph of the entire global graph, to obtain globally generalized graph mining models. To overcome the unique challenge of incomplete information propagation on local subgraphs due to missing cross-subgraph neighbors, previous works resort to the augmentation of local neighborhoods through the joint FL of missing neighbor generators and GNNs. Yet their technical designs have profound limitations regarding the utility, efficiency, and privacy goals of FL. In this work, we propose FedDEP to comprehensively tackle these challenges in subgraph FL. FedDEP consists of a series of novel technical designs: (1) Deep neighbor generation through leveraging the GNN embeddings of potential missing neighbors; (2) Efficient pseudo-FL for neighbor generation through embedding prototyping; and (3) Privacy protection through noise-less edge-local-differential-privacy.
We analyze the correctness and efficiency of FedDEP, and provide theoretical guarantees on its privacy.
Empirical results on four real-world datasets justify the clear benefits of proposed techniques.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：Coupling Graph Neural Networks with Fractional Order Continuous  Dynamics: A Robustness Study</b></summary>
  <p><b>编号</b>：[146]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04331</p>
  <p><b>作者</b>：Qiyu Kang,  Kai Zhao,  Yang Song,  Yihang Xie,  Yanan Zhao,  Sijie Wang,  Rui She,  Wee Peng Tay</p>
  <p><b>备注</b>：in Proc. AAAI Conference on Artificial Intelligence, Vancouver, Canada, Feb. 2024</p>
  <p><b>关键词</b>：graph neural FDE, neural FDE models, graph neural, neural FDE, traditional graph neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we rigorously investigate the robustness of graph neural fractional-order differential equation (FDE) models. This framework extends beyond traditional graph neural (integer-order) ordinary differential equation (ODE) models by implementing the time-fractional Caputo derivative. Utilizing fractional calculus allows our model to consider long-term memory during the feature updating process, diverging from the memoryless Markovian updates seen in traditional graph neural ODE models. The superiority of graph neural FDE models over graph neural ODE models has been established in environments free from attacks or perturbations. While traditional graph neural ODE models have been verified to possess a degree of stability and resilience in the presence of adversarial attacks in existing literature, the robustness of graph neural FDE models, especially under adversarial conditions, remains largely unexplored. This paper undertakes a detailed assessment of the robustness of graph neural FDE models. We establish a theoretical foundation outlining the robustness characteristics of graph neural FDE models, highlighting that they maintain more stringent output perturbation bounds in the face of input and graph topology disturbances, compared to their integer-order counterparts. Our empirical evaluations further confirm the enhanced robustness of graph neural FDE models, highlighting their potential in adversarially robust applications.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Private Truly-Everlasting Robust-Prediction</b></summary>
  <p><b>编号</b>：[157]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04311</p>
  <p><b>作者</b>：Uri Stemmer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：introduced by Naor, PEP, Private Everlasting Prediction, recently introduced, sample complexity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Private Everlasting Prediction (PEP), recently introduced by Naor et al. [2023], is a model for differentially private learning in which the learner never publicly releases a hypothesis. Instead, it provides black-box access to a "prediction oracle" that can predict the labels of an endless stream of unlabeled examples drawn from the underlying distribution. Importantly, PEP provides privacy both for the initial training set and for the endless stream of classification queries. We present two conceptual modifications to the definition of PEP, as well as new constructions exhibiting significant improvements over prior work. Specifically,
(1) Robustness: PEP only guarantees accuracy provided that all the classification queries are drawn from the correct underlying distribution. A few out-of-distribution queries might break the validity of the prediction oracle for future queries, even for future queries which are sampled from the correct distribution. We incorporate robustness against such poisoning attacks into the definition of PEP, and show how to obtain it.
(2) Dependence of the privacy parameter $\delta$ in the time horizon: We present a relaxed privacy definition, suitable for PEP, that allows us to disconnect the privacy parameter $\delta$ from the number of total time steps $T$. This allows us to obtain algorithms for PEP whose sample complexity is independent from $T$, thereby making them "truly everlasting". This is in contrast to prior work where the sample complexity grows with $polylog(T)$.
(3) New constructions: Prior constructions for PEP exhibit sample complexity that is quadratic in the VC dimension of the target class. We present new constructions of PEP for axis-aligned rectangles and for decision-stumps that exhibit sample complexity linear in the dimension (instead of quadratic). We show that our constructions satisfy very strong robustness properties.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Advancing Deep Active Learning & Data Subset Selection: Unifying  Principles with Information-Theory Intuitions</b></summary>
  <p><b>编号</b>：[159]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04305</p>
  <p><b>作者</b>：Andreas Kirsch</p>
  <p><b>备注</b>：PhD thesis</p>
  <p><b>关键词</b>：data subset selection, deep learning, data subset, subset selection, learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>At its core, this thesis aims to enhance the practicality of deep learning by improving the label and training efficiency of deep learning models. To this end, we investigate data subset selection techniques, specifically active learning and active sampling, grounded in information-theoretic principles. Active learning improves label efficiency, while active sampling enhances training efficiency. Supervised deep learning models often require extensive training with labeled data. Label acquisition can be expensive and time-consuming, and training large models is resource-intensive, hindering the adoption outside academic research and ``big tech.'' Existing methods for data subset selection in deep learning often rely on heuristics or lack a principled information-theoretic foundation. In contrast, this thesis examines several objectives for data subset selection and their applications within deep learning, striving for a more principled approach inspired by information theory. We begin by disentangling epistemic and aleatoric uncertainty in single forward-pass deep neural networks, which provides helpful intuitions and insights into different forms of uncertainty and their relevance for data subset selection. We then propose and investigate various approaches for active learning and data subset selection in (Bayesian) deep learning. Finally, we relate various existing and proposed approaches to approximations of information quantities in weight or prediction space. Underpinning this work is a principled and practical notation for information-theoretic quantities that includes both random variables and observed outcomes. This thesis demonstrates the benefits of working from a unified perspective and highlights the potential impact of our contributions to the practical application of deep learning.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Setting the Record Straight on Transformer Oversmoothing</b></summary>
  <p><b>编号</b>：[161]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04301</p>
  <p><b>作者</b>：Gbètondji J-S Dovonon,  Michael M. Bronstein,  Matt J. Kusner</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：set of domains, recently become wildly, diverse set, inherently low-pass filters, Transformers</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transformer-based models have recently become wildly successful across a diverse set of domains. At the same time, recent work has shown that Transformers are inherently low-pass filters that gradually oversmooth the inputs, reducing the expressivity of their representations. A natural question is: How can Transformers achieve these successes given this shortcoming? In this work we show that in fact Transformers are not inherently low-pass filters. Instead, whether Transformers oversmooth or not depends on the eigenspectrum of their update equations. Our analysis extends prior work in oversmoothing and in the closely-related phenomenon of rank collapse. We show that many successful Transformer models have attention and weights which satisfy conditions that avoid oversmoothing. Based on this analysis, we derive a simple way to parameterize the weights of the Transformer update equations that allows for control over its spectrum, ensuring that oversmoothing does not occur. Compared to a recent solution for oversmoothing, our approach improves generalization, even when training with more layers, fewer datapoints, and data that is corrupted.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：A Fast Graph Search Algorithm with Dynamic Optimization and Reduced  Histogram for Discrimination of Binary Classification Problem</b></summary>
  <p><b>编号</b>：[166]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04282</p>
  <p><b>作者</b>：Qinwu Xu</p>
  <p><b>备注</b>：14 pages, 8 figures, 1 table</p>
  <p><b>关键词</b>：optimal discrimination path, study develops, binary classification problem, find the optimal, Support Vector Machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This study develops a graph search algorithm to find the optimal discrimination path for the binary classification problem. The objective function is defined as the difference of variations between the true positive (TP) and false positive (FP). It uses the depth first search (DFS) algorithm to find the top-down paths for discrimination. It proposes a dynamic optimization procedure to optimize TP at the upper levels and then reduce FP at the lower levels. To accelerate computing speed with improving accuracy, it proposes a reduced histogram algorithm with variable bin size instead of looping over all data points, to find the feature threshold of discrimination. The algorithm is applied on top of a Support Vector Machine (SVM) model for a binary classification problem on whether a person is fit or unfit. It significantly improves TP and reduces FP of the SVM results (e.g., reduced FP by 90% with a loss of only\ 5% TP). The graph search auto-generates 39 ranked discrimination paths within 9 seconds on an input of total 328,464 objects, using a dual-core Laptop computer with a processor of 2.59 GHz.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Predicting the structure of dynamic graphs</b></summary>
  <p><b>编号</b>：[167]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04280</p>
  <p><b>作者</b>：Sevvandi Kandanaarachchi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：incremental learning facilitate, Dynamic graph embeddings, learning facilitate predictive, facilitate predictive tasks, inductive and incremental</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Dynamic graph embeddings, inductive and incremental learning facilitate predictive tasks such as node classification and link prediction. However, predicting the structure of a graph at a future time step from a time series of graphs, allowing for new nodes has not gained much attention. In this paper, we present such an approach. We use time series methods to predict the node degree at future time points and combine it with flux balance analysis -- a linear programming method used in biochemistry -- to obtain the structure of future graphs. Furthermore, we explore the predictive graph distribution for different parameter values. We evaluate this method using synthetic and real datasets and demonstrate its utility and applicability.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Attention versus Contrastive Learning of Tabular Data -- A Data-centric  Benchmarking</b></summary>
  <p><b>编号</b>：[169]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04266</p>
  <p><b>作者</b>：Shourav B. Rabbani,  Ivan V. Medri,  Manar D. Samad</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：tabular data sets, data sets, tabular data, data, learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite groundbreaking success in image and text learning, deep learning has not achieved significant improvements against traditional machine learning (ML) when it comes to tabular data. This performance gap underscores the need for data-centric treatment and benchmarking of learning algorithms. Recently, attention and contrastive learning breakthroughs have shifted computer vision and natural language processing paradigms. However, the effectiveness of these advanced deep models on tabular data is sparsely studied using a few data sets with very large sample sizes, reporting mixed findings after benchmarking against a limited number of baselines. We argue that the heterogeneity of tabular data sets and selective baselines in the literature can bias the benchmarking outcomes. This article extensively evaluates state-of-the-art attention and contrastive learning methods on a wide selection of 28 tabular data sets (14 easy and 14 hard-to-classify) against traditional deep and machine learning. Our data-centric benchmarking demonstrates when traditional ML is preferred over deep learning and vice versa because no best learning method exists for all tabular data sets. Combining between-sample and between-feature attentions conquers the invincible traditional ML on tabular data sets by a significant margin but fails on high dimensional data, where contrastive learning takes a robust lead. While a hybrid attention-contrastive learning strategy mostly wins on hard-to-classify data sets, traditional methods are frequently superior on easy-to-classify data sets with presumably simpler decision boundaries. To the best of our knowledge, this is the first benchmarking paper with statistical analyses of attention and contrastive learning performances on a diverse selection of tabular data sets against traditional deep and machine learning baselines to facilitate further advances in this field.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Explaining the Power of Topological Data Analysis in Graph Machine  Learning</b></summary>
  <p><b>编号</b>：[174]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04250</p>
  <p><b>作者</b>：Funmilola Mary Taiwo,  Umar Islambekov,  Cuneyt Gurcan Akcora</p>
  <p><b>备注</b>：17 pages, 12 figures</p>
  <p><b>关键词</b>：Topological Data Analysis, Data Analysis, capture intricate shapes, Topological Data, TDA</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Topological Data Analysis (TDA) has been praised by researchers for its ability to capture intricate shapes and structures within data. TDA is considered robust in handling noisy and high-dimensional datasets, and its interpretability is believed to promote an intuitive understanding of model behavior. However, claims regarding the power and usefulness of TDA have only been partially tested in application domains where TDA-based models are compared to other graph machine learning approaches, such as graph neural networks. We meticulously test claims on TDA through a comprehensive set of experiments and validate their merits. Our results affirm TDA's robustness against outliers and its interpretability, aligning with proponents' arguments. However, we find that TDA does not significantly enhance the predictive power of existing methods in our specific experiments, while incurring significant computational costs. We investigate phenomena related to graph characteristics, such as small diameters and high clustering coefficients, to mitigate the computational expenses of TDA computations. Our results offer valuable perspectives on integrating TDA into graph machine learning tasks.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Scalable Normalizing Flows Enable Boltzmann Generators for  Macromolecules</b></summary>
  <p><b>编号</b>：[178]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04246</p>
  <p><b>作者</b>：Joseph C. Kim,  David Bloore,  Karan Kapoor,  Jun Feng,  Ming-Hong Hao,  Mengdi Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：functional states, Boltzmann distribution, Boltzmann Generators, training, distribution</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The Boltzmann distribution of a protein provides a roadmap to all of its functional states. Normalizing flows are a promising tool for modeling this distribution, but current methods are intractable for typical pharmacological targets; they become computationally intractable due to the size of the system, heterogeneity of intra-molecular potential energy, and long-range interactions. To remedy these issues, we present a novel flow architecture that utilizes split channels and gated attention to efficiently learn the conformational distribution of proteins defined by internal coordinates. We show that by utilizing a 2-Wasserstein loss, one can smooth the transition from maximum likelihood training to energy-based training, enabling the training of Boltzmann Generators for macromolecules. We evaluate our model and training strategy on villin headpiece HP35(nle-nle), a 35-residue subdomain, and protein G, a 56-residue protein. We demonstrate that standard architectures and training strategies, such as maximum likelihood alone, fail while our novel architecture and multi-stage training strategy are able to model the conformational distributions of protein G and HP35.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Curiosity & Entropy Driven Unsupervised RL in Multiple Environments</b></summary>
  <p><b>编号</b>：[187]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04198</p>
  <p><b>作者</b>：Shaurya Dewan,  Anisha Jain,  Zoe LaLena,  Lifan Yu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Unsupervised Reinforcement Learning, Unsupervised Reinforcement, Multiple environments' propose, Multiple environments', tackle unsupervised</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The authors of 'Unsupervised Reinforcement Learning in Multiple environments' propose a method, alpha-MEPOL, to tackle unsupervised RL across multiple environments. They pre-train a task-agnostic exploration policy using interactions from an entire environment class and then fine-tune this policy for various tasks using supervision. We expanded upon this work, with the goal of improving performance. We primarily propose and experiment with five new modifications to the original work: sampling trajectories using an entropy-based probability distribution, dynamic alpha, higher KL Divergence threshold, curiosity-driven exploration, and alpha-percentile sampling on curiosity. Dynamic alpha and higher KL-Divergence threshold both provided a significant improvement over the baseline from the earlier work. PDF-sampling failed to provide any improvement due to it being approximately equivalent to the baseline method when the sample space is small. In high-dimensional environments, the addition of curiosity-driven exploration enhances learning by encouraging the agent to seek diverse experiences and explore the unknown more. However, its benefits are limited in low-dimensional and simpler environments where exploration possibilities are constrained and there is little that is truly unknown to the agent. Overall, some of our experiments did boost performance over the baseline and there are a few directions that seem promising for further research.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Efficient Selective Audio Masked Multimodal Bottleneck Transformer for  Audio-Video Classification</b></summary>
  <p><b>编号</b>：[192]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04154</p>
  <p><b>作者</b>：Wentao Zhu</p>
  <p><b>备注</b>：Accepted by WACV 2024; well-formatted PDF is in this https URL arXiv admin note: text overlap with arXiv:2401.04023</p>
  <p><b>关键词</b>：mainstream media platforms, AVT, Transformer, video Transformer, media platforms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Audio and video are two most common modalities in the mainstream media platforms, e.g., YouTube. To learn from multimodal videos effectively, in this work, we propose a novel audio-video recognition approach termed audio video Transformer, AVT, leveraging the effective spatio-temporal representation by the video Transformer to improve action recognition accuracy. For multimodal fusion, simply concatenating multimodal tokens in a cross-modal Transformer requires large computational and memory resources, instead we reduce the cross-modality complexity through an audio-video bottleneck Transformer. To improve the learning efficiency of multimodal Transformer, we integrate self-supervised objectives, i.e., audio-video contrastive learning, audio-video matching, and masked audio and video learning, into AVT training, which maps diverse audio and video representations into a common multimodal representation space. We further propose a masked audio segment loss to learn semantic audio activities in AVT. Extensive experiments and ablation studies on three public datasets and two in-house datasets consistently demonstrate the effectiveness of the proposed AVT. Specifically, AVT outperforms its previous state-of-the-art counterparts on Kinetics-Sounds by 8%. AVT also surpasses one of the previous state-of-the-art video Transformers [25] by 10% on VGGSound by leveraging the audio signal. Compared to one of the previous state-of-the-art multimodal methods, MBT [32], AVT is 1.3% more efficient in terms of FLOPs and improves the accuracy by 3.8% on Epic-Kitchens-100.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Chain of LoRA: Efficient Fine-tuning of Language Models via Residual  Learning</b></summary>
  <p><b>编号</b>：[194]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04151</p>
  <p><b>作者</b>：Wenhan Xia,  Chengwei Qin,  Elad Hazan</p>
  <p><b>备注</b>：Work in progress</p>
  <p><b>关键词</b>：tailoring pre-trained large, primary methodology, methodology for tailoring, LoRA, large language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Fine-tuning is the primary methodology for tailoring pre-trained large language models to specific tasks. As the model's scale and the diversity of tasks expand, parameter-efficient fine-tuning methods are of paramount importance. One of the most widely used family of methods is low-rank adaptation (LoRA) and its variants. LoRA encodes weight update as the product of two low-rank matrices. Despite its advantages, LoRA falls short of full-parameter fine-tuning in terms of generalization error for certain tasks.
We introduce Chain of LoRA (COLA), an iterative optimization framework inspired by the Frank-Wolfe algorithm, to bridge the gap between LoRA and full parameter fine-tuning, without incurring additional computational costs or memory overheads. COLA employs a residual learning procedure where it merges learned LoRA modules into the pre-trained language model parameters and re-initilize optimization for new born LoRA modules. We provide theoretical convergence guarantees as well as empirical results to validate the effectiveness of our algorithm. Across various models (OPT and llama-2) and seven benchmarking tasks, we demonstrate that COLA can consistently outperform LoRA without additional computational or memory costs.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Online Test-Time Adaptation of Spatial-Temporal Traffic Flow Forecasting</b></summary>
  <p><b>编号</b>：[196]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04148</p>
  <p><b>作者</b>：Pengxin Guo,  Pengrong Jin,  Ziyue Li,  Lei Bai,  Yu Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：traffic flow forecasting, optimal travel routes, implementing control measures, selecting optimal travel, aiding traffic managers</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Accurate spatial-temporal traffic flow forecasting is crucial in aiding traffic managers in implementing control measures and assisting drivers in selecting optimal travel routes. Traditional deep-learning based methods for traffic flow forecasting typically rely on historical data to train their models, which are then used to make predictions on future data. However, the performance of the trained model usually degrades due to the temporal drift between the historical and future data. To make the model trained on historical data better adapt to future data in a fully online manner, this paper conducts the first study of the online test-time adaptation techniques for spatial-temporal traffic flow forecasting problems. To this end, we propose an Adaptive Double Correction by Series Decomposition (ADCSD) method, which first decomposes the output of the trained model into seasonal and trend-cyclical parts and then corrects them by two separate modules during the testing phase using the latest observed data entry by entry. In the proposed ADCSD method, instead of fine-tuning the whole trained model during the testing phase, a lite network is attached after the trained model, and only the lite network is fine-tuned in the testing process each time a data entry is observed. Moreover, to satisfy that different time series variables may have different levels of temporal drift, two adaptive vectors are adopted to provide different weights for different time series variables. Extensive experiments on four real-world traffic flow forecasting datasets demonstrate the effectiveness of the proposed ADCSD method. The code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Learn Once Plan Arbitrarily (LOPA): Attention-Enhanced Deep  Reinforcement Learning Method for Global Path Planning</b></summary>
  <p><b>编号</b>：[197]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04145</p>
  <p><b>作者</b>：Guoming Huang,  Mingxin Hou,  Xiaofang Yuan,  Shuqiao Huang,  Yaonan Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Deep reinforcement learning, recently shown promise, Deep reinforcement, reinforcement learning, recently shown</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep reinforcement learning (DRL) methods have recently shown promise in path planning tasks. However, when dealing with global planning tasks, these methods face serious challenges such as poor convergence and generalization. To this end, we propose an attention-enhanced DRL method called LOPA (Learn Once Plan Arbitrarily) in this paper. Firstly, we analyze the reasons of these problems from the perspective of DRL's observation, revealing that the traditional design causes DRL to be interfered by irrelevant map information. Secondly, we develop the LOPA which utilizes a novel attention-enhanced mechanism to attain an improved attention capability towards the key information of the observation. Such a mechanism is realized by two steps: (1) an attention model is built to transform the DRL's observation into two dynamic views: local and global, significantly guiding the LOPA to focus on the key information on the given maps; (2) a dual-channel network is constructed to process these two views and integrate them to attain an improved reasoning capability. The LOPA is validated via multi-objective global path planning experiments. The result suggests the LOPA has improved convergence and generalization performance as well as great path planning efficiency.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Robust Calibration For Improved Weather Prediction Under Distributional  Shift</b></summary>
  <p><b>编号</b>：[198]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04144</p>
  <p><b>作者</b>：Sankalp Gilda,  Neel Bhandari,  Wendy Mak,  Andrea Panizza</p>
  <p><b>备注</b>：Presented at the Bayesian Deep Learning workshop at NeurIPS 2021</p>
  <p><b>关键词</b>：Real-World Distributional Shift, Distributional Shift, Robustness and Uncertainty, Shifts Challenge, Real-World Distributional</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we present results on improving out-of-domain weather prediction and uncertainty estimation as part of the \texttt{Shifts Challenge on Robustness and Uncertainty under Real-World Distributional Shift} challenge. We find that by leveraging a mixture of experts in conjunction with an advanced data augmentation technique borrowed from the computer vision domain, in conjunction with robust \textit{post-hoc} calibration of predictive uncertainties, we can potentially achieve more accurate and better-calibrated results with deep neural networks than with boosted tree models for tabular data. We quantify our predictions using several metrics and propose several future lines of inquiry and experimentation to boost performance.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：On The Potential of The Fractal Geometry and The CNNs Ability to Encode  it</b></summary>
  <p><b>编号</b>：[200]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04141</p>
  <p><b>作者</b>：Julia El Zini,  Bassel Musharrafieh,  Mariette Awad</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：measuring scale, fractal, fractal features, fractal dimension, statistical index</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The fractal dimension provides a statistical index of object complexity by studying how the pattern changes with the measuring scale. Although useful in several classification tasks, the fractal dimension is under-explored in deep learning applications. In this work, we investigate the features that are learned by deep models and we study whether these deep networks are able to encode features as complex and high-level as the fractal dimensions. Specifically, we conduct a correlation analysis experiment to show that deep networks are not able to extract such a feature in none of their layers. We combine our analytical study with a human evaluation to investigate the differences between deep learning networks and models that operate on the fractal feature solely. Moreover, we show the effectiveness of fractal features in applications where the object structure is crucial for the classification task. We empirically show that training a shallow network on fractal features achieves performance comparable, even superior in specific cases, to that of deep networks trained on raw data while requiring less computational resources. Fractals improved the accuracy of the classification by 30% on average while requiring up to 84% less time to train. We couple our empirical study with a complexity analysis of the computational cost of extracting the proposed fractal features, and we study its limitation.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：CCNETS: A Novel Brain-Inspired Approach for Enhanced Pattern Recognition  in Imbalanced Datasets</b></summary>
  <p><b>编号</b>：[201]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04139</p>
  <p><b>作者</b>：Hanbeot Park (1),  Yunjeong Cho (2),  Hoon-Hee Kim (3)</p>
  <p><b>备注</b>：31 pages, authors (3) is Corresponding Author</p>
  <p><b>关键词</b>：Causal Cooperative Nets, Cooperative Nets, Causal Cooperative, model-based classifier designed, Causal Learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This study introduces CCNETS (Causal Learning with Causal Cooperative Nets), a novel generative model-based classifier designed to tackle the challenge of generating data for imbalanced datasets in pattern recognition. CCNETS is uniquely crafted to emulate brain-like information processing and comprises three main components: Explainer, Producer, and Reasoner. Each component is designed to mimic specific brain functions, which aids in generating high-quality datasets and enhancing classification performance.
The model is particularly focused on addressing the common and significant challenge of handling imbalanced datasets in machine learning. CCNETS's effectiveness is demonstrated through its application to a "fraud dataset," where normal transactions significantly outnumber fraudulent ones (99.83% vs. 0.17%). Traditional methods often struggle with such imbalances, leading to skewed performance metrics. However, CCNETS exhibits superior classification ability, as evidenced by its performance metrics. Specifically, it achieved an F1-score of 0.7992, outperforming traditional models like Autoencoders and Multi-layer Perceptrons (MLP) in the same context. This performance indicates CCNETS's proficiency in more accurately distinguishing between normal and fraudulent patterns.
The innovative structure of CCNETS enhances the coherence between generative and classification models, helping to overcome the limitations of pattern recognition that rely solely on generative models. This study emphasizes CCNETS's potential in diverse applications, especially where quality data generation and pattern recognition are key. It proves effective in machine learning, particularly for imbalanced datasets. CCNETS overcomes current challenges in these datasets and advances machine learning with brain-inspired approaches.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：Global-Aware Enhanced Spatial-Temporal Graph Recurrent Networks: A New  Framework For Traffic Flow Prediction</b></summary>
  <p><b>编号</b>：[204]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04135</p>
  <p><b>作者</b>：Haiyang Liu,  Chunjiang Zhu,  Detian Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：enhancing transport efficiency, flow prediction plays, alleviating traffic congestion, transport efficiency, recurrent neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Traffic flow prediction plays a crucial role in alleviating traffic congestion and enhancing transport efficiency. While combining graph convolution networks with recurrent neural networks for spatial-temporal modeling is a common strategy in this realm, the restricted structure of recurrent neural networks limits their ability to capture global information. For spatial modeling, many prior studies learn a graph structure that is assumed to be fixed and uniform at all time steps, which may not be true. This paper introduces a novel traffic prediction framework, Global-Aware Enhanced Spatial-Temporal Graph Recurrent Network (GA-STGRN), comprising two core components: a spatial-temporal graph recurrent neural network and a global awareness layer. Within this framework, three innovative prediction models are formulated. A sequence-aware graph neural network is proposed and integrated into the Gated Recurrent Unit (GRU) to learn non-fixed graphs at different time steps and capture local temporal relationships. To enhance the model's global perception, three distinct global spatial-temporal transformer-like architectures (GST^2) are devised for the global awareness layer. We conduct extensive experiments on four real traffic datasets and the results demonstrate the superiority of our framework and the three concrete models.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：SynHIN: Generating Synthetic Heterogeneous Information Network for  Explainable AI</b></summary>
  <p><b>编号</b>：[206]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04133</p>
  <p><b>作者</b>：Ming-Yi Hong,  Yi-Hsiang Huang,  You-Chen Teng,  Chih-Yu Wang,  Che Lin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：detecting e-commerce spam, Graph, detecting e-commerce, e-commerce spam, spam to social</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Graph Neural Networks (GNNs) excel in various domains, from detecting e-commerce spam to social network classification problems. However, the lack of public graph datasets hampers research progress, particularly in heterogeneous information networks (HIN). The demand for datasets for fair HIN comparisons is growing due to advancements in GNN interpretation models. In response, we propose SynHIN, a unique method for generating synthetic heterogeneous information networks. SynHIN identifies motifs in real-world datasets, summarizes graph statistics, and constructs a synthetic network. Our approach utilizes In-Cluster and Out-Cluster Merge modules to build the synthetic HIN from primary motif clusters. After In/Our-Cluster mergers and a post-pruning process fitting the real dataset constraints, we ensure the synthetic graph statistics align closely with the reference one. SynHIN generates a synthetic heterogeneous graph dataset for node classification tasks, using the primary motif as the explanation ground truth. It can adapt and address the lack of heterogeneous graph datasets and motif ground truths, proving beneficial for assessing heterogeneous graph neural network explainers. We further present a benchmark dataset for future heterogeneous graph explainer model research. Our work marks a significant step towards explainable AI in HGNNs.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Unsupervised Test-Time Adaptation via Plug-and-Play Transformer Modules</b></summary>
  <p><b>编号</b>：[208]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04130</p>
  <p><b>作者</b>：Xiangyu Chang,  Sk Miraj Ahmed,  Basak Guler,  Srikanth V. Krishnamurthy,  Ananthram Swami,  Samet Oymak,  Amit K. Roy-Chowdhury</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Visual Prompt Tuning, Visual Prompt, Prompt Tuning, found success, success in enabling</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Parameter-efficient tuning (PET) methods such as LoRA, Adapter, and Visual Prompt Tuning (VPT) have found success in enabling adaptation to new domains by tuning small modules within a transformer model. However, the number of domains encountered during test time can be very large, and the data is usually unlabeled. Thus, adaptation to new domains is challenging; it is also impractical to generate customized tuned modules for each such domain. Toward addressing these challenges, this work introduces PLUTO: a Plug-and-pLay modUlar Test-time domain adaptatiOn strategy. We pre-train a large set of modules, each specialized for different source domains, effectively creating a ``module store''. Given a target domain with few-shot unlabeled data, we introduce an unsupervised test-time adaptation (TTA) method to (1) select a sparse subset of relevant modules from this store and (2) create a weighted combination of selected modules without tuning their weights. This plug-and-play nature enables us to harness multiple most-relevant source domains in a single inference call. Comprehensive evaluations demonstrate that PLUTO uniformly outperforms alternative TTA methods and that selecting $\leq$5 modules suffice to extract most of the benefit. At a high level, our method equips pre-trained transformers with the capability to dynamically adapt to new domains, motivating a new paradigm for efficient and scalable domain adaptation.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Why is the User Interface a Dark Pattern? : Explainable Auto-Detection  and its Analysis</b></summary>
  <p><b>编号</b>：[213]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04119</p>
  <p><b>作者</b>：Yuki Yada,  Tsuneo Matsumoto,  Fuyuko Kido,  Hayato Yamana</p>
  <p><b>备注</b>：IEEE International Conference on Big Data (IEEE BigData 2022)</p>
  <p><b>关键词</b>：make users behave, Dark patterns, Dark, designs for online, behave in unintended</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Dark patterns are deceptive user interface designs for online services that make users behave in unintended ways. Dark patterns, such as privacy invasion, financial loss, and emotional distress, can harm users. These issues have been the subject of considerable debate in recent years. In this paper, we study interpretable dark pattern auto-detection, that is, why a particular user interface is detected as having dark patterns. First, we trained a model using transformer-based pre-trained language models, BERT, on a text-based dataset for the automatic detection of dark patterns in e-commerce. Then, we applied post-hoc explanation techniques, including local interpretable model agnostic explanation (LIME) and Shapley additive explanations (SHAP), to the trained model, which revealed which terms influence each prediction as a dark pattern. In addition, we extracted and analyzed terms that affected the dark patterns. Our findings may prevent users from being manipulated by dark patterns, and aid in the construction of more equitable internet services. Our code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Timeline-based Process Discovery</b></summary>
  <p><b>编号</b>：[217]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04114</p>
  <p><b>作者</b>：Harleen Kaur,  Jan Mendling,  Christoffer Rubensson,  Timotheus Kampik</p>
  <p><b>备注</b>：8 pages</p>
  <p><b>关键词</b>：business processes, key concern, provide insights, insights into performance, performance aspects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A key concern of automatic process discovery is to provide insights into performance aspects of business processes. Waiting times are of particular importance in this context. For that reason, it is surprising that current techniques for automatic process discovery generate directly-follows graphs and comparable process models, but often miss the opportunity to explicitly represent the time axis. In this paper, we present an approach for automatically constructing process models that explicitly align with a time axis. We exemplify our approach for directly-follows graphs. Our evaluation using two BPIC datasets and a proprietary dataset highlight the benefits of this representation in comparison to standard layout techniques.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：U-Mamba: Enhancing Long-range Dependency for Biomedical Image  Segmentation</b></summary>
  <p><b>编号</b>：[223]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04722</p>
  <p><b>作者</b>：Jun Ma,  Feifei Li,  Bo Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Convolutional Neural Networks, handle long-range dependencies, Neural Networks, biomedical image segmentation, Convolutional Neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Convolutional Neural Networks (CNNs) and Transformers have been the most popular architectures for biomedical image segmentation, but both of them have limited ability to handle long-range dependencies because of inherent locality or computational complexity. To address this challenge, we introduce U-Mamba, a general-purpose network for biomedical image segmentation. Inspired by the State Space Sequence Models (SSMs), a new family of deep sequence models known for their strong capability in handling long sequences, we design a hybrid CNN-SSM block that integrates the local feature extraction power of convolutional layers with the abilities of SSMs for capturing the long-range dependency. Moreover, U-Mamba enjoys a self-configuring mechanism, allowing it to automatically adapt to various datasets without manual intervention. We conduct extensive experiments on four diverse tasks, including the 3D abdominal organ segmentation in CT and MR images, instrument segmentation in endoscopy images, and cell segmentation in microscopy images. The results reveal that U-Mamba outperforms state-of-the-art CNN-based and Transformer-based segmentation networks across all tasks. This opens new avenues for efficient long-range dependency modeling in biomedical image analysis. The code, models, and data are publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：Linear Recursive Feature Machines provably recover low-rank matrices</b></summary>
  <p><b>编号</b>：[230]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04553</p>
  <p><b>作者</b>：Adityanarayanan Radhakrishnan,  Mikhail Belkin,  Dmitriy Drusvyatskiy</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：networks make accurate, make accurate predictions, feature learning, neural networks make, Recursive Feature Machines</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A fundamental problem in machine learning is to understand how neural networks make accurate predictions, while seemingly bypassing the curse of dimensionality. A possible explanation is that common training algorithms for neural networks implicitly perform dimensionality reduction - a process called feature learning. Recent work posited that the effects of feature learning can be elicited from a classical statistical estimator called the average gradient outer product (AGOP). The authors proposed Recursive Feature Machines (RFMs) as an algorithm that explicitly performs feature learning by alternating between (1) reweighting the feature vectors by the AGOP and (2) learning the prediction function in the transformed space. In this work, we develop the first theoretical guarantees for how RFM performs dimensionality reduction by focusing on the class of overparametrized problems arising in sparse linear regression and low-rank matrix recovery. Specifically, we show that RFM restricted to linear models (lin-RFM) generalizes the well-studied Iteratively Reweighted Least Squares (IRLS) algorithm. Our results shed light on the connection between feature learning in neural networks and classical sparse recovery algorithms. In addition, we provide an implementation of lin-RFM that scales to matrices with millions of missing entries. Our implementation is faster than the standard IRLS algorithm as it is SVD-free. It also outperforms deep linear networks for sparse linear regression and low-rank matrix completion.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Semi-Supervised Deep Sobolev Regression: Estimation, Variable Selection  and Beyond</b></summary>
  <p><b>编号</b>：[231]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04535</p>
  <p><b>作者</b>：Zhao Ding,  Chenguang Duan,  Yuling Jiao,  Jerry Zhijian Yang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：deep Sobolev regressor, semi-supervised deep Sobolev, Sobolev regressor, underlying regression function, deep Sobolev</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose SDORE, a semi-supervised deep Sobolev regressor, for the nonparametric estimation of the underlying regression function and its gradient. SDORE employs deep neural networks to minimize empirical risk with gradient norm regularization, allowing computation of the gradient norm on unlabeled data. We conduct a comprehensive analysis of the convergence rates of SDORE and establish a minimax optimal rate for the regression function. Crucially, we also derive a convergence rate for the associated plug-in gradient estimator, even in the presence of significant domain shift. These theoretical findings offer valuable prior guidance for selecting regularization parameters and determining the size of the neural network, while showcasing the provable advantage of leveraging unlabeled data in semi-supervised learning. To the best of our knowledge, SDORE is the first provable neural network-based approach that simultaneously estimates the regression function and its gradient, with diverse applications including nonparametric variable selection and inverse problems. The effectiveness of SDORE is validated through an extensive range of numerical simulations and real data analysis.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：Zero Shot Audio to Audio Emotion Transfer With Speaker Disentanglement</b></summary>
  <p><b>编号</b>：[232]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04511</p>
  <p><b>作者</b>：Soumya Dutta,  Sriram Ganapathy</p>
  <p><b>备注</b>：5 pages, 3 figures, accepted at ICASSP 2024</p>
  <p><b>关键词</b>：style transfer involves, transfer involves replacing, content related attributes, Emotion Style Transfer, Zero-shot Emotion Style</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The problem of audio-to-audio (A2A) style transfer involves replacing the style features of the source audio with those from the target audio while preserving the content related attributes of the source audio. In this paper, we propose an efficient approach, termed as Zero-shot Emotion Style Transfer (ZEST), that allows the transfer of emotional content present in the given source audio with the one embedded in the target audio while retaining the speaker and speech content from the source. The proposed system builds upon decomposing speech into semantic tokens, speaker representations and emotion embeddings. Using these factors, we propose a framework to reconstruct the pitch contour of the given speech signal and train a decoder that reconstructs the speech signal. The model is trained using a self-supervision based reconstruction loss. During conversion, the emotion embedding is alone derived from the target audio, while rest of the factors are derived from the source audio. In our experiments, we show that, even without using parallel training data or labels from the source or target audio, we illustrate zero shot emotion transfer capabilities of the proposed ZEST model using objective and subjective quality evaluations.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：TwinBooster: Synergising Large Language Models with Barlow Twins and  Gradient Boosting for Enhanced Molecular Property Prediction</b></summary>
  <p><b>编号</b>：[233]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04478</p>
  <p><b>作者</b>：Maximilian G. Schuh,  Davide Boldini,  Stephan A. Sieber</p>
  <p><b>备注</b>：20 pages, 4 figures, 10 tables</p>
  <p><b>关键词</b>：molecular activities, precise prediction, Barlow Twins, molecular, molecular property prediction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The success of drug discovery and development relies on the precise prediction of molecular activities and properties. While in silico molecular property prediction has shown remarkable potential, its use has been limited so far to assays for which large amounts of data are available. In this study, we use a fine-tuned large language model to integrate biological assays based on their textual information, coupled with Barlow Twins, a Siamese neural network using a novel self-supervised learning approach. This architecture uses both assay information and molecular fingerprints to extract the true molecular information. TwinBooster enables the prediction of properties of unseen bioassays and molecules by providing state-of-the-art zero-shot learning tasks. Remarkably, our artificial intelligence pipeline shows excellent performance on the FS-Mol benchmark. This breakthrough demonstrates the application of deep learning to critical property prediction tasks where data is typically scarce. By accelerating the early identification of active molecules in drug discovery and development, this method has the potential to help streamline the identification of novel therapeutics.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：Sea wave data reconstruction using micro-seismic measurements and  machine learning methods</b></summary>
  <p><b>编号</b>：[236]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04431</p>
  <p><b>作者</b>：Lorenzo Iafolla,  Emiliano Fiorenza,  Massimo Chiappini,  Cosmo Carmisciano,  Valerio Antonio Iafolla</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：machine learning algorithm, machine learning, Sea wave monitoring, learning algorithm, monitoring is key</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sea wave monitoring is key in many applications in oceanography such as the validation of weather and wave models. Conventional in situ solutions are based on moored buoys whose measurements are often recognized as a standard. However, being exposed to a harsh environment, they are not reliable, need frequent maintenance, and the datasets feature many gaps. To overcome the previous limitations, we propose a system including a buoy, a micro-seismic measuring station, and a machine learning algorithm. The working principle is based on measuring the micro-seismic signals generated by the sea waves. Thus, the machine learning algorithm will be trained to reconstruct the missing buoy data from the micro-seismic data. As the micro-seismic station can be installed indoor, it assures high reliability while the machine learning algorithm provides accurate reconstruction of the missing buoy data. In this work, we present the methods to process the data, develop and train the machine learning algorithm, and assess the reconstruction accuracy. As a case of study, we used experimental data collected in 2014 from the Northern Tyrrhenian Sea demonstrating that the data reconstruction can be done both for significant wave height and wave period. The proposed approach was inspired from Data Science, whose methods were the foundation for the new solutions presented in this work. For example, estimating the period of the sea waves, often not discussed in previous works, was relatively simple with machine learning. In conclusion, the experimental results demonstrated that the new system can overcome the reliability issues of the buoy keeping the same accuracy.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：Stable generative modeling using diffusion maps</b></summary>
  <p><b>编号</b>：[238]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04372</p>
  <p><b>作者</b>：Georg Gottwald,  Fengyi Li,  Youssef Marzouk,  Sebastian Reich</p>
  <p><b>备注</b>：23 pages, 25 figures</p>
  <p><b>关键词</b>：sufficiently large number, unknown distribution, sufficiently large, large number, training samples</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider the problem of sampling from an unknown distribution for which only a sufficiently large number of training samples are available. Such settings have recently drawn considerable interest in the context of generative modelling. In this paper, we propose a generative model combining diffusion maps and Langevin dynamics. Diffusion maps are used to approximate the drift term from the available training samples, which is then implemented in a discrete-time Langevin sampler to generate new samples. By setting the kernel bandwidth to match the time step size used in the unadjusted Langevin algorithm, our method effectively circumvents any stability issues typically associated with time-stepping stiff stochastic differential equations. More precisely, we introduce a novel split-step scheme, ensuring that the generated samples remain within the convex hull of the training samples. Our framework can be naturally extended to generate conditional samples. We demonstrate the performance of our proposed scheme through experiments on synthetic datasets with increasing dimensions and on a stochastic subgrid-scale parametrization conditional sampling problem.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：Universal Consistency of Wide and Deep ReLU Neural Networks and Minimax  Optimal Convergence Rates for Kolmogorov-Donoho Optimal Function Classes</b></summary>
  <p><b>编号</b>：[239]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04286</p>
  <p><b>作者</b>：Hyunouk Ko,  Xiaoming Huo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：prove universal consistency, classification rule based, ReLU neural networks, neural networks, deep ReLU neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we first extend the result of FL93 and prove universal consistency for a classification rule based on wide and deep ReLU neural networks trained on the logistic loss. Unlike the approach in FL93 that decomposes the estimation and empirical error, we directly analyze the classification risk based on the observation that a realization of a neural network that is wide enough is capable of interpolating an arbitrary number of points. Secondly, we give sufficient conditions for a class of probability measures under which classifiers based on neural networks achieve minimax optimal rates of convergence. Our result is motivated from the practitioner's observation that neural networks are often trained to achieve 0 training error, which is the case for our proposed neural network classifiers. Our proofs hinge on recent developments in empirical risk minimization and on approximation rates of deep ReLU neural networks for various function classes of interest. Applications to classical function spaces of smoothness illustrate the usefulness of our result.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：A learning-based mathematical programming formulation for the automatic  configuration of optimization solvers</b></summary>
  <p><b>编号</b>：[243]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04237</p>
  <p><b>作者</b>：Gabriele Iommazzo,  Claudia D'Ambrosio,  Antonio Frangioni,  Leo Liberti</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：propose a methodology, machine learning, solver configuration, based on machine, performance function</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a methodology, based on machine learning and optimization, for selecting a solver configuration for a given instance. First, we employ a set of solved instances and configurations in order to learn a performance function of the solver. Secondly, we formulate a mixed-integer nonlinear program where the objective/constraints explicitly encode the learnt information, and which we solve, upon the arrival of an unknown instance, to find the best solver configuration for that instance, based on the performance function. The main novelty of our approach lies in the fact that the configuration set search problem is formulated as a mathematical program, which allows us to a) enforce hard dependence and compatibility constraints on the configurations, and b) solve it efficiently with off-the-shelf optimization tools.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：Towards a Machine Learning-Based Approach to Predict Space Object  Density Distributions</b></summary>
  <p><b>编号</b>：[245]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04212</p>
  <p><b>作者</b>：Victor Rodriguez-Fernandez,  Sumiyajav Sarangerel,  Peng Mun Siew,  Pablo Machuca,  Daniel Jang,  Richard Linares</p>
  <p><b>备注</b>：2024 AIAA SciTech Forum, 8-12 January 2024, Orlando, FL, USA</p>
  <p><b>关键词</b>：Low Earth Orbit, Anthropogenic Space Objects, facing significant congestion, Low Earth, Earth Orbit</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the rapid increase in the number of Anthropogenic Space Objects (ASOs), Low Earth Orbit (LEO) is facing significant congestion, thereby posing challenges to space operators and risking the viability of the space environment for varied uses. Current models for examining this evolution, while detailed, are computationally demanding. To address these issues, we propose a novel machine learning-based model, as an extension of the MIT Orbital Capacity Tool (MOCAT). This advanced model is designed to accelerate the propagation of ASO density distributions, and it is trained on hundreds of simulations generated by an established and accurate model of the space environment evolution. We study how different deep learning-based solutions can potentially be good candidates for ASO propagation and manage the high-dimensionality of the data. To assess the model's capabilities, we conduct experiments in long term forecasting scenarios (around 100 years), analyze how and why the performance degrades over time, and discuss potential solutions to make this solution better.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：Dense Hopfield Networks in the Teacher-Student Setting</b></summary>
  <p><b>编号</b>：[246]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04191</p>
  <p><b>作者</b>：Robin Thériault,  Daniele Tantari</p>
  <p><b>备注</b>：34 pages, 9 figures</p>
  <p><b>关键词</b>：Dense Hopfield networks, Hopfield networks, Dense Hopfield, p-body Hopfield networks, Nishimori line</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Dense Hopfield networks are known for their feature to prototype transition and adversarial robustness. However, previous theoretical studies have been mostly concerned with their storage capacity. We bridge this gap by studying the phase diagram of p-body Hopfield networks in the teacher-student setting of an unsupervised learning problem, uncovering ferromagnetic phases reminiscent of the prototype and feature learning regimes. On the Nishimori line, we find the critical size of the training set necessary for efficient pattern retrieval. Interestingly, we find that that the paramagnetic to ferromagnetic transition of the teacher-student setting coincides with the paramagnetic to spin-glass transition of the direct model, i.e. with random patterns. Outside of the Nishimori line, we investigate the learning performance in relation to the inference temperature and dataset noise. Moreover, we show that using a larger p for the student than the teacher gives the student an extensive tolerance to noise. We then derive a closed-form expression measuring the adversarial robustness of such a student at zero temperature, corroborating the positive correlation between number of parameters and robustness observed in large neural networks. We also use our model to clarify why the prototype phase of modern Hopfield networks is adversarially robust.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：FlopPITy: Enabling self-consistent exoplanet atmospheric retrievals with  machine learning</b></summary>
  <p><b>编号</b>：[248]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04168</p>
  <p><b>作者</b>：Francisco Ardévol Martínez,  Michiel Min,  Daniela Huppenkothen,  Inga Kamp,  Paul I. Palmer</p>
  <p><b>备注</b>：Accepted for publication at A&A</p>
  <p><b>关键词</b>：Bayesian retrieval techniques, Bayesian retrieval, atmospheres to constrain, properties is typically, exoplanet atmospheric retrievals</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Interpreting the observations of exoplanet atmospheres to constrain physical and chemical properties is typically done using Bayesian retrieval techniques. Because these methods require many model computations, a compromise is made between model complexity and run time. Reaching this compromise leads to the simplification of many physical and chemical processes (e.g. parameterised temperature structure). Here we implement and test sequential neural posterior estimation (SNPE), a machine learning inference algorithm, for exoplanet atmospheric retrievals. The goal is to speed up retrievals so they can be run with more computationally expensive atmospheric models, such as those computing the temperature structure using radiative transfer. We generate 100 synthetic observations using ARCiS (ARtful Modeling Code for exoplanet Science, an atmospheric modelling code with the flexibility to compute models in varying degrees of complexity) and perform retrievals on them to test the faithfulness of the SNPE posteriors. The faithfulness quantifies whether the posteriors contain the ground truth as often as we expect. We also generate a synthetic observation of a cool brown dwarf using the self-consistent capabilities of ARCiS and run a retrieval with self-consistent models to showcase the possibilities that SNPE opens. We find that SNPE provides faithful posteriors and is therefore a reliable tool for exoplanet atmospheric retrievals. We are able to run a self-consistent retrieval of a synthetic brown dwarf spectrum using only 50,000 forward model evaluations. We find that SNPE can speed up retrievals between $\sim2\times$ and $\geq10\times$ depending on the computational load of the forward model, the dimensionality of the observation, and the signal-to-noise ratio of the observation. We make the code publicly available for the community on Github.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：DeepPhysiNet: Bridging Deep Learning and Atmospheric Physics for  Accurate and Continuous Weather Modeling</b></summary>
  <p><b>编号</b>：[252]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04125</p>
  <p><b>作者</b>：Wenyuan Li,  Zili Liu,  Keyan Chen,  Hao Chen,  Shunlin Liang,  Zhengxia Zou,  Zhenwei Shi</p>
  <p><b>备注</b>：18 pages, 9 figures</p>
  <p><b>关键词</b>：holds significant importance, Numerical Weather Prediction, forecasting holds significant, Deep Learning-based Prediction, weather forecasting holds</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Accurate weather forecasting holds significant importance to human activities. Currently, there are two paradigms for weather forecasting: Numerical Weather Prediction (NWP) and Deep Learning-based Prediction (DLP). NWP utilizes atmospheric physics for weather modeling but suffers from poor data utilization and high computational costs, while DLP can learn weather patterns from vast amounts of data directly but struggles to incorporate physical laws. Both paradigms possess their respective strengths and weaknesses, and are incompatible, because physical laws adopted in NWP describe the relationship between coordinates and meteorological variables, while DLP directly learns the relationships between meteorological variables without consideration of coordinates. To address these problems, we introduce the DeepPhysiNet framework, incorporating physical laws into deep learning models for accurate and continuous weather system modeling. First, we construct physics networks based on multilayer perceptrons (MLPs) for individual meteorological variable, such as temperature, pressure, and wind speed. Physics networks establish relationships between variables and coordinates by taking coordinates as input and producing variable values as output. The physical laws in the form of Partial Differential Equations (PDEs) can be incorporated as a part of loss function. Next, we construct hyper-networks based on deep learning methods to directly learn weather patterns from a large amount of meteorological data. The output of hyper-networks constitutes a part of the weights for the physics networks. Experimental results demonstrate that, upon successful integration of physical laws, DeepPhysiNet can accomplish multiple tasks simultaneously, not only enhancing forecast accuracy but also obtaining continuous spatiotemporal resolution results, which is unattainable by either the NWP or DLP.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：Isolated pulsar population synthesis with simulation-based inference</b></summary>
  <p><b>编号</b>：[253]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2312.14848</p>
  <p><b>作者</b>：Vanessa Graber,  Michele Ronchi,  Celsa Pardo-Araujo,  Nanda Rea</p>
  <p><b>备注</b>：30 pages, 14 figures, 5 tables, 2 appendices, comments welcome</p>
  <p><b>关键词</b>：log, isolated Galactic radio, constrain the magneto-rotational, power law, magneto-rotational properties</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We combine pulsar population synthesis with simulation-based inference to constrain the magneto-rotational properties of isolated Galactic radio pulsars. We first develop a flexible framework to model neutron-star birth properties and evolution, focusing on their dynamical, rotational and magnetic characteristics. In particular, we sample initial magnetic-field strengths, $B$, and spin periods, $P$, from log-normal distributions and capture the late-time magnetic-field decay with a power law. Each log-normal is described by a mean, $\mu_{\log B}, \mu_{\log P}$, and standard deviation, $\sigma_{\log B}, \sigma_{\log P}$, while the power law is characterized by the index, $a_{\rm late}$, resulting in five free parameters. We subsequently model the stars' radio emission and observational biases to mimic detections with three radio surveys, and produce a large database of synthetic $P$-$\dot{P}$ diagrams by varying our input parameters. We then follow a simulation-based inference approach that focuses on neural posterior estimation and employ this database to train deep neural networks to directly infer the posterior distributions of the five model parameters. After successfully validating these individual neural density estimators on simulated data, we use an ensemble of networks to infer the posterior distributions for the observed pulsar population. We obtain $\mu_{\log B} = 13.10^{+0.08}_{-0.10}$, $\sigma_{\log B} = 0.45^{+0.05}_{-0.05}$ and $\mu_{\log P} = -1.00^{+0.26}_{-0.21}$, $\sigma_{\log P} = 0.38^{+0.33}_{-0.18}$ for the log-normal distributions, and $a_{\rm late} = -1.80^{+0.65}_{-0.61}$ for the power law at $95\%$ credible interval. Our approach represents a crucial step towards robust statistical inference for complex population-synthesis frameworks and forms the basis for future multi-wavelength analyses of Galactic pulsars.</p>
  </details>
</details>
<h1>人工智能</h1>
<details>
  <summary>1. <b>标题：Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar  Creation</b></summary>
  <p><b>编号</b>：[3]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04728</p>
  <p><b>作者</b>：Xiyi Chen,  Marko Mihajlovic,  Shaofei Wang,  Sergey Prokudin,  Siyu Tang</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：previously unfeasible capability, Recent advances, capability of generating, text prompt, single input image</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advances in generative diffusion models have enabled the previously unfeasible capability of generating 3D assets from a single input image or a text prompt. In this work, we aim to enhance the quality and functionality of these models for the task of creating controllable, photorealistic human avatars. We achieve this by integrating a 3D morphable model into the state-of-the-art multiview-consistent diffusion approach. We demonstrate that accurate conditioning of a generative pipeline on the articulated 3D model enhances the baseline model performance on the task of novel view synthesis from a single image. More importantly, this integration facilitates a seamless and accurate incorporation of facial expression and body pose control into the generation process. To the best of our knowledge, our proposed framework is the first diffusion model to enable the creation of fully 3D-consistent, animatable, and photorealistic human avatars from a single image of an unseen subject; extensive quantitative and qualitative evaluations demonstrate the advantages of our approach over existing state-of-the-art avatar creation models on both novel view and novel expression synthesis tasks.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation</b></summary>
  <p><b>编号</b>：[19]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04679</p>
  <p><b>作者</b>：Mahdi Nikdan,  Soroush Tabesh,  Dan Alistarh</p>
  <p><b>备注</b>：Preliminary version</p>
  <p><b>关键词</b>：large language models, investigate parameter-efficient fine-tuning, PEFT method called, language models, called Robust Adaptation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memory- and computationally-efficient training. Our code will be made available at this https URL}{\texttt{this https URL</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Benchmark Analysis of Various Pre-trained Deep Learning Models on ASSIRA  Cats and Dogs Dataset</b></summary>
  <p><b>编号</b>：[23]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04666</p>
  <p><b>作者</b>：Galib Muhammad Shahriar Himel,  Md. Masudul Islam</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：NVIDIA GeForce RTX, image classification, grown in popularity, basic application, application and implementation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As the most basic application and implementation of deep learning, image classification has grown in popularity. Various datasets are provided by renowned data science communities for benchmarking machine learning algorithms and pre-trained models. The ASSIRA Cats & Dogs dataset is one of them and is being used in this research for its overall acceptance and benchmark standards. A comparison of various pre-trained models is demonstrated by using different types of optimizers and loss functions. Hyper-parameters are changed to gain the best result from a model. By applying this approach, we have got higher accuracy without major changes in the training model. To run the experiment, we used three different computer architectures: a laptop equipped with NVIDIA GeForce GTX 1070, a laptop equipped with NVIDIA GeForce RTX 3080Ti, and a desktop equipped with NVIDIA GeForce RTX 3090. The acquired results demonstrate supremacy in terms of accuracy over the previously done experiments on this dataset. From this experiment, the highest accuracy which is 99.65% is gained using the NASNet Large.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence  Lengths in Large Language Models</b></summary>
  <p><b>编号</b>：[27]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04658</p>
  <p><b>作者</b>：Zhen Qin,  Weigao Sun,  Dong Li,  Xuyang Shen,  Weixuan Sun,  Yiran Zhong</p>
  <p><b>备注</b>：Technical Report. Yiran Zhong is the corresponding author. The source code is available at this https URL</p>
  <p><b>关键词</b>：Linear attention, attention, Linear, recently emerged, promising alternative</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：A novel framework for generalization of deep hidden physics models</b></summary>
  <p><b>编号</b>：[32]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04648</p>
  <p><b>作者</b>：Vijay Kag,  Birupaksha Pal</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：oft encountered problem, complex physics involved, full system information, information is unknown, oft encountered</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Modelling of systems where the full system information is unknown is an oft encountered problem for various engineering and industrial applications, as it's either impossible to consider all the complex physics involved or simpler models are considered to keep within the limits of the available resources. Recent advances in greybox modelling like the deep hidden physics models address this space by combining data and physics. However, for most real-life applications, model generalizability is a key issue, as retraining a model for every small change in system inputs and parameters or modification in domain configuration can render the model economically unviable. In this work we present a novel enhancement to the idea of hidden physics models which can generalize for changes in system inputs, parameters and domains. We also show that this approach holds promise in system discovery as well and helps learn the hidden physics for the changed system inputs, parameters and domain configuration.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Advancing Ante-Hoc Explainable Models through Generative Adversarial  Networks</b></summary>
  <p><b>编号</b>：[33]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04647</p>
  <p><b>作者</b>：Tanmay Garg,  Deepika Vemuri,  Vineeth N Balasubramanian</p>
  <p><b>备注</b>：Paper accepted in Human-Centric Representation Learning workshop at AAAI 2024 (this https URL)</p>
  <p><b>关键词</b>：enhancing model interpretability, concept learning framework, learning framework, framework for enhancing, interpretability and performance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a novel concept learning framework for enhancing model interpretability and performance in visual classification tasks. Our approach appends an unsupervised explanation generator to the primary classifier network and makes use of adversarial training. During training, the explanation module is optimized to extract visual concepts from the classifier's latent representations, while the GAN-based module aims to discriminate images generated from concepts, from true images. This joint training scheme enables the model to implicitly align its internally learned concepts with human-interpretable visual properties. Comprehensive experiments demonstrate the robustness of our approach, while producing coherent concept activations. We analyse the learned concepts, showing their semantic concordance with object parts and visual attributes. We also study how perturbations in the adversarial training protocol impact both classification and concept acquisition. In summary, this work presents a significant step towards building inherently interpretable deep vision models with task-aligned concept representations - a key enabler for developing trustworthy AI for real-world perception tasks.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Applying Large Language Models API to Issue Classification Problem</b></summary>
  <p><b>编号</b>：[35]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04637</p>
  <p><b>作者</b>：Gabriel Aracena,  Kyle Luster,  Fabio Santos,  Igor Steinmacher,  Marco A. Gerosa</p>
  <p><b>备注</b>：4 pages, 1 figure, NLBSE and ICSE conference submission, ACM formatted, pre print</p>
  <p><b>关键词</b>：critical problems promptly, optimize resource allocation, address critical problems, problems promptly, optimize resource</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Effective prioritization of issue reports is crucial in software engineering to optimize resource allocation and address critical problems promptly. However, the manual classification of issue reports for prioritization is laborious and lacks scalability. Alternatively, many open source software (OSS) projects employ automated processes for this task, albeit relying on substantial datasets for adequate training. This research seeks to devise an automated approach that ensures reliability in issue prioritization, even when trained on smaller datasets. Our proposed methodology harnesses the power of Generative Pre-trained Transformers (GPT), recognizing their potential to efficiently handle this task. By leveraging the capabilities of such models, we aim to develop a robust system for prioritizing issue reports accurately, mitigating the necessity for extensive training data while maintaining reliability. In our research, we have developed a reliable GPT-based approach to accurately label and prioritize issue reports with a reduced training dataset. By reducing reliance on massive data requirements and focusing on few-shot fine-tuning, our methodology offers a more accessible and efficient solution for issue prioritization in software engineering. Our model predicted issue types in individual projects up to 93.2% in precision, 95% in recall, and 89.3% in F1-score.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Deep Reinforcement Multi-agent Learning framework for Information  Gathering with Local Gaussian Processes for Water Monitoring</b></summary>
  <p><b>编号</b>：[38]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04631</p>
  <p><b>作者</b>：Samuel Yanes Luis,  Dmitriy Shutin,  Juan Marchal Gómez,  Daniel Gutiérrez Reina,  Sergio Toral Marín</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：hydrological resources involves, resources involves continuously, Local Gaussian Processes, involves continuously monitoring, Gaussian Processes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The conservation of hydrological resources involves continuously monitoring their contamination. A multi-agent system composed of autonomous surface vehicles is proposed in this paper to efficiently monitor the water quality. To achieve a safe control of the fleet, the fleet policy should be able to act based on measurements and to the the fleet state. It is proposed to use Local Gaussian Processes and Deep Reinforcement Learning to jointly obtain effective monitoring policies. Local Gaussian processes, unlike classical global Gaussian processes, can accurately model the information in a dissimilar spatial correlation which captures more accurately the water quality information. A Deep convolutional policy is proposed, that bases the decisions on the observation on the mean and variance of this model, by means of an information gain reward. Using a Double Deep Q-Learning algorithm, agents are trained to minimize the estimation error in a safe manner thanks to a Consensus-based heuristic. Simulation results indicate an improvement of up to 24% in terms of the mean absolute error with the proposed models. Also, training results with 1-3 agents indicate that our proposed approach returns 20% and 24% smaller average estimation errors for, respectively, monitoring water quality variables and monitoring algae blooms, as compared to state-of-the-art approaches</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：DebugBench: Evaluating Debugging Capability of Large Language Models</b></summary>
  <p><b>编号</b>：[41]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04621</p>
  <p><b>作者</b>：Runchu Tian,  Yining Ye,  Yujia Qin,  Xin Cong,  Yankai Lin,  Zhiyuan Liu,  Maosong Sun</p>
  <p><b>备注</b>：in progress</p>
  <p><b>关键词</b>：Large Language Models, demonstrated exceptional coding, Large Language, exceptional coding capability, Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and three open-source models in a zero-shot scenario. We find that (1) while closed-source models like GPT-4 exhibit inferior debugging performance compared to humans, open-source models such as Code Llama fail to attain any pass rate scores; (2) the complexity of debugging notably fluctuates depending on the bug category; (3) incorporating runtime feedback has a clear impact on debugging performance which is not always helpful. As an extension, we also compare LLM debugging and code generation, revealing a strong correlation between them for closed-source models. These findings will benefit the development of LLMs in debugging.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Agent Alignment in Evolving Social Norms</b></summary>
  <p><b>编号</b>：[42]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04620</p>
  <p><b>作者</b>：Shimin Li,  Tianxiang Sun,  Xipeng Qiu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, based on Large, production and life</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Agents based on Large Language Models (LLMs) are increasingly permeating various domains of human production and life, highlighting the importance of aligning them with human values. The current alignment of AI systems primarily focuses on passively aligning LLMs through human intervention. However, agents possess characteristics like receiving environmental feedback and self-evolution, rendering the LLM alignment methods inadequate. In response, we propose an evolutionary framework for agent evolution and alignment, named EvolutionaryAgent, which transforms agent alignment into a process of evolution and selection under the principle of survival of the fittest. In an environment where social norms continuously evolve, agents better adapted to the current social norms will have a higher probability of survival and proliferation, while those inadequately aligned dwindle over time. Experimental results assessing the agents from multiple perspectives in aligning with social norms demonstrate that EvolutionaryAgent possesses the capability to align progressively better with the evolving social norms while maintaining its proficiency in general tasks. Effectiveness tests conducted on various open and closed-source LLMs as the foundation for agents also prove the applicability of our approach.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Masked Audio Generation using a Single Non-Autoregressive Transformer</b></summary>
  <p><b>编号</b>：[55]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04577</p>
  <p><b>作者</b>：Alon Ziv,  Itai Gat,  Gael Le Lan,  Tal Remez,  Felix Kreuk,  Alexandre Défossez,  Jade Copet,  Gabriel Synnaeve,  Yossi Adi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：masked generative sequence, operates directly, generative sequence modeling, masked generative, MAGNeT</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce MAGNeT, a masked generative sequence modeling method that operates directly over several streams of audio tokens. Unlike prior work, MAGNeT is comprised of a single-stage, non-autoregressive transformer. During training, we predict spans of masked tokens obtained from a masking scheduler, while during inference we gradually construct the output sequence using several decoding steps. To further enhance the quality of the generated audio, we introduce a novel rescoring method in which, we leverage an external pre-trained model to rescore and rank predictions from MAGNeT, which will be then used for later decoding steps. Lastly, we explore a hybrid version of MAGNeT, in which we fuse between autoregressive and non-autoregressive models to generate the first few seconds in an autoregressive manner while the rest of the sequence is being decoded in parallel. We demonstrate the efficiency of MAGNeT for the task of text-to-music and text-to-audio generation and conduct an extensive empirical evaluation, considering both objective metrics and human studies. The proposed approach is comparable to the evaluated baselines, while being significantly faster (x7 faster than the autoregressive baseline). Through ablation studies and analysis, we shed light on the importance of each of the components comprising MAGNeT, together with pointing to the trade-offs between autoregressive and non-autoregressive modeling, considering latency, throughput, and generation quality. Samples are available on our demo page this https URL.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Let's Go Shopping (LGS) -- Web-Scale Image-Text Dataset for Visual  Concept Understanding</b></summary>
  <p><b>编号</b>：[56]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04575</p>
  <p><b>作者</b>：Yatong Bai,  Utsav Garg,  Apaar Shanker,  Haoming Zhang,  Samyak Parajuli,  Erhan Bas,  Isidora Filipovic,  Amelia N. Chu,  Eugenia D Fomitcheva,  Elliot Branson,  Aerin Kim,  Somayeh Sojoudi,  Kyunghyun Cho</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：non-trivial data-collecting processes, require non-trivial data-collecting, neural networks, classification and captioning, data-collecting processes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision and vision-language applications of neural networks, such as image classification and captioning, rely on large-scale annotated datasets that require non-trivial data-collecting processes. This time-consuming endeavor hinders the emergence of large-scale datasets, limiting researchers and practitioners to a small number of choices. Therefore, we seek more efficient ways to collect and annotate images. Previous initiatives have gathered captions from HTML alt-texts and crawled social media postings, but these data sources suffer from noise, sparsity, or subjectivity. For this reason, we turn to commercial shopping websites whose data meet three criteria: cleanliness, informativeness, and fluency. We introduce the Let's Go Shopping (LGS) dataset, a large-scale public dataset with 15 million image-caption pairs from publicly available e-commerce websites. When compared with existing general-domain datasets, the LGS images focus on the foreground object and have less complex backgrounds. Our experiments on LGS show that the classifiers trained on existing benchmark datasets do not readily generalize to e-commerce data, while specific self-supervised visual feature extractors can better generalize. Furthermore, LGS's high-quality e-commerce-focused images and bimodal nature make it advantageous for vision-language bi-modal tasks: LGS enables image-captioning models to generate richer captions and helps text-to-image generation models achieve e-commerce style transfer.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Evaluating Language Model Agency through Negotiations</b></summary>
  <p><b>编号</b>：[67]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04536</p>
  <p><b>作者</b>：Tim R. Davidson,  Veniamin Veselovsky,  Martin Josifoski,  Maxime Peyrard,  Antoine Bosselut,  Michal Kosinski,  Robert West</p>
  <p><b>备注</b>：Code and link to project data are made available at this https URL</p>
  <p><b>关键词</b>：exploit Language Models', increasingly exploit Language, Language Models', display agent-like behavior, governments increasingly exploit</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Companies, organizations, and governments increasingly exploit Language Models' (LM) remarkable capability to display agent-like behavior. As LMs are adopted to perform tasks with growing autonomy, there exists an urgent need for reliable and scalable evaluation benchmarks. Current, predominantly static LM benchmarks are ill-suited to evaluate such dynamic applications. Thus, we propose jointly evaluating LM performance and alignment through the lenses of negotiation games. We argue that this common task better reflects real-world deployment conditions while offering insights into LMs' decision-making processes. Crucially, negotiation games allow us to study multi-turn, and cross-model interactions, modulate complexity, and side-step accidental data leakage in evaluation. We report results for six publicly accessible LMs from several major providers on a variety of negotiation games, evaluating both self-play and cross-play performance. Noteworthy findings include: (i) open-source models are currently unable to complete these tasks; (ii) cooperative bargaining games prove challenging; and (iii) the most powerful models do not always "win".</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：MERA: A Comprehensive LLM Evaluation in Russian</b></summary>
  <p><b>编号</b>：[69]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04531</p>
  <p><b>作者</b>：Alena Fenogenova,  Artem Chervyakov,  Nikita Martynov,  Anastasia Kozlova,  Maria Tikhonova,  Albina Akhmetgareeva,  Anton Emelyanov,  Denis Shevelev,  Pavel Lebedev,  Leonid Sinev,  Ulyana Isaeva,  Katerina Kolomeytseva,  Daniil Moskovskiy,  Elizaveta Goncharova,  Nikita Savushkin,  Polina Mikhailova,  Denis Dimitrov,  Alexander Panchenko,  Sergei Markov</p>
  <p><b>备注</b>：the paper version comparable with the release code v.1.1.0 of the benchmark; this https URL</p>
  <p><b>关键词</b>：past few years, notable advancements, foundation models, open Multimodal Evaluation, models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Over the past few years, one of the most notable advancements in AI research has been in foundation models (FMs), headlined by the rise of language models (LMs). As the models' size increases, LMs demonstrate enhancements in measurable aspects and the development of new qualitative features. However, despite researchers' attention and the rapid growth in LM application, the capabilities, limitations, and associated risks still need to be better understood. To address these issues, we introduce an open Multimodal Evaluation of Russian-language Architectures (MERA), a new instruction benchmark for evaluating foundation models oriented towards the Russian language. The benchmark encompasses 21 evaluation tasks for generative models in 11 skill domains and is designed as a black-box test to ensure the exclusion of data leakage. The paper introduces a methodology to evaluate FMs and LMs in zero- and few-shot fixed instruction settings that can be extended to other modalities. We propose an evaluation methodology, an open-source code base for the MERA assessment, and a leaderboard with a submission system. We evaluate open LMs as baselines and find that they are still far behind the human level. We publicly release MERA to guide forthcoming research, anticipate groundbreaking model features, standardize the evaluation procedure, and address potential societal drawbacks.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：The Critique of Critique</b></summary>
  <p><b>编号</b>：[73]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04518</p>
  <p><b>作者</b>：Shichao Sun,  Junlong Li,  Weizhe Yuan,  Ruifeng Yuan,  Wenjie Li,  Pengfei Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, model-generated content, natural language description</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Critique, as a natural language description for assessing the quality of model-generated content, has been proven to play an essential role in the training, evaluation, and refinement of Large Language Models (LLMs). However, there is a lack of principled understanding in evaluating the quality of the critique itself. In this paper, we pioneer the critique of critique, termed MetaCritique, which is a framework to evaluate the critique from two aspects, i.e., factuality as precision score and comprehensiveness as recall score. We calculate the harmonic mean of precision and recall as the overall rating called F1 score. To obtain a reliable evaluation outcome, we propose Atomic Information Units (AIUs), which describe the critique in a more fine-grained manner. MetaCritique takes each AIU into account and aggregates each AIU's judgment for the overall score. Moreover, given the evaluation process involves intricate reasoning, our MetaCritique provides a natural language rationale to support each judgment. We construct a meta-evaluation dataset containing 300 critiques (2653 AIUs) across four tasks (question answering, reasoning, entailment, and summarization), and we conduct a comparative study to demonstrate the feasibility and effectiveness. Experiments also show superior critique judged by MetaCritique leads to better refinement, indicating generative artificial intelligence indeed has the potential to be significantly advanced with our MetaCritique. We will release relevant code and meta-evaluation datasets at this https URL.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Exploring Prompt-Based Methods for Zero-Shot Hypernym Prediction with  Large Language Models</b></summary>
  <p><b>编号</b>：[74]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04515</p>
  <p><b>作者</b>：Mikhail Tikhomirov,  Natalia Loukachevitch</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, article investigates, investigates a zero-shot, large language, LLMs</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This article investigates a zero-shot approach to hypernymy prediction using large language models (LLMs). The study employs a method based on text probability calculation, applying it to various generated prompts. The experiments demonstrate a strong correlation between the effectiveness of language model prompts and classic patterns, indicating that preliminary prompt selection can be carried out using smaller models before moving to larger ones. We also explore prompts for predicting co-hyponyms and improving hypernymy predictions by augmenting prompts with additional information through automatically identified co-hyponyms. An iterative approach is developed for predicting higher-level concepts, which further improves the quality on the BLESS dataset (MAP = 0.8).</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：TechGPT-2.0: A large language model project to solve the task of  knowledge graph construction</b></summary>
  <p><b>编号</b>：[78]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04507</p>
  <p><b>作者</b>：Jiaqi Wang,  Yuying Chang,  Zhong Li,  Ning An,  Qi Ma,  Lei Hei,  Haibo Luo,  Yifei Lu,  Feiliang Ren</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：exhibited robust performance, Large language models, Large language, diverse natural language, performance across diverse</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models have exhibited robust performance across diverse natural language processing tasks. This report introduces TechGPT-2.0, a project designed to enhance the capabilities of large language models specifically in knowledge graph construction tasks, including named entity recognition (NER) and relationship triple extraction (RTE) tasks in NLP applications. Additionally, it serves as a LLM accessible for research within the Chinese open-source model community. We offer two 7B large language model weights and a QLoRA weight specialized for processing lengthy texts.Notably, TechGPT-2.0 is trained on Huawei's Ascend server. Inheriting all functionalities from TechGPT-1.0, it exhibits robust text processing capabilities, particularly in the domains of medicine and law. Furthermore, we introduce new capabilities to the model, enabling it to process texts in various domains such as geographical areas, transportation, organizations, literary works, biology, natural sciences, astronomical objects, and architecture. These enhancements also fortified the model's adeptness in handling hallucinations, unanswerable queries, and lengthy texts. This report provides a comprehensive and detailed introduction to the full fine-tuning process on Huawei's Ascend servers, encompassing experiences in Ascend server debugging, instruction fine-tuning data processing, and model training. Our code is available at this https URL</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Optimal Survival Trees: A Dynamic Programming Approach</b></summary>
  <p><b>编号</b>：[83]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04489</p>
  <p><b>作者</b>：Tim Huisman,  Jacobus G. M. van der Linden,  Emir Demirović</p>
  <p><b>备注</b>：Published at AAAI-24</p>
  <p><b>关键词</b>：singular unrepeated events, Survival analysis studies, unrepeated events, based on historical, historical data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Survival analysis studies and predicts the time of death, or other singular unrepeated events, based on historical data, while the true time of death for some instances is unknown. Survival trees enable the discovery of complex nonlinear relations in a compact human comprehensible model, by recursively splitting the population and predicting a distinct survival distribution in each leaf node. We use dynamic programming to provide the first survival tree method with optimality guarantees, enabling the assessment of the optimality gap of heuristics. We improve the scalability of our method through a special algorithm for computing trees up to depth two. The experiments show that our method's run time even outperforms some heuristics for realistic cases while obtaining similar out-of-sample performance with the state-of-the-art.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Fighting Fire with Fire: Adversarial Prompting to Generate a  Misinformation Detection Dataset</b></summary>
  <p><b>编号</b>：[88]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04481</p>
  <p><b>作者</b>：Shrey Satapara,  Parth Mehta,  Debasis Ganguly,  Sandip Modha</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：language generation capabilities, inducing mass agitation, Llama etc., large language models, language generation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The recent success in language generation capabilities of large language models (LLMs), such as GPT, Bard, Llama etc., can potentially lead to concerns about their possible misuse in inducing mass agitation and communal hatred via generating fake news and spreading misinformation. Traditional means of developing a misinformation ground-truth dataset does not scale well because of the extensive manual effort required to annotate the data. In this paper, we propose an LLM-based approach of creating silver-standard ground-truth datasets for identifying misinformation. Specifically speaking, given a trusted news article, our proposed approach involves prompting LLMs to automatically generate a summarised version of the original article. The prompts in our proposed approach act as a controlling mechanism to generate specific types of factual incorrectness in the generated summaries, e.g., incorrect quantities, false attributions etc. To investigate the usefulness of this dataset, we conduct a set of experiments where we train a range of supervised models for the task of misinformation detection.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Combining Embedding-Based and Semantic-Based Models for Post-hoc  Explanations in Recommender Systems</b></summary>
  <p><b>编号</b>：[89]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04474</p>
  <p><b>作者</b>：Ngoc Luyen Le,  Marie-Hélène Abel,  Philippe Gouspillou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：today data-rich environment, decision support systems, data-rich environment, recommender systems play, today data-rich</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In today's data-rich environment, recommender systems play a crucial role in decision support systems. They provide to users personalized recommendations and explanations about these recommendations. Embedding-based models, despite their widespread use, often suffer from a lack of interpretability, which can undermine trust and user engagement. This paper presents an approach that combines embedding-based and semantic-based models to generate post-hoc explanations in recommender systems, leveraging ontology-based knowledge graphs to improve interpretability and explainability. By organizing data within a structured framework, ontologies enable the modeling of intricate relationships between entities, which is essential for generating explanations. By combining embedding-based and semantic based models for post-hoc explanations in recommender systems, the framework we defined aims at producing meaningful and easy-to-understand explanations, enhancing user trust and satisfaction, and potentially promoting the adoption of recommender systems across the e-commerce sector.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：A Survey on Efficient Federated Learning Methods for Foundation Model  Training</b></summary>
  <p><b>编号</b>：[90]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04472</p>
  <p><b>作者</b>：Herbert Woisetschläger,  Alexander Isenko,  Shiqiang Wang,  Ruben Mayer,  Hans-Arno Jacobsen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：facilitate privacy-preserving collaborative, Federated Learning, privacy-preserving collaborative training, established technique, technique to facilitate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Federated Learning (FL) has become an established technique to facilitate privacy-preserving collaborative training. However, new approaches to FL often discuss their contributions involving small deep-learning models only. With the tremendous success of transformer models, the following question arises: What is necessary to operationalize foundation models in an FL application? Knowing that computation and communication often take up similar amounts of time in FL, we introduce a novel taxonomy focused on computational and communication efficiency methods in FL applications. This said, these methods aim to optimize the training time and reduce communication between clients and the server. We also look at the current state of widely used FL frameworks and discuss future research potentials based on existing approaches in FL research and beyond.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation</b></summary>
  <p><b>编号</b>：[92]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04468</p>
  <p><b>作者</b>：Weimin Wang,  Jiawei Liu,  Zhijie Lin,  Jiangqiao Yan,  Shuo Chen,  Chetwin Low,  Tuyen Hoang,  Jie Wu,  Jun Hao Liew,  Hanshu Yan,  Daquan Zhou,  Jiashi Feng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：catalyzed significant research, high-fidelity video generation, growing demand, demand for high-fidelity, textual descriptions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The growing demand for high-fidelity video generation from textual descriptions has catalyzed significant research in this field. In this work, we introduce MagicVideo-V2 that integrates the text-to-image model, video motion generator, reference image embedding module and frame interpolation module into an end-to-end video generation pipeline. Benefiting from these architecture designs, MagicVideo-V2 can generate an aesthetically pleasing, high-resolution video with remarkable fidelity and smoothness. It demonstrates superior performance over leading Text-to-Video systems such as Runway, Pika 1.0, Morph, Moon Valley and Stable Video Diffusion model via user evaluation at large scale.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Image classification network enhancement methods based on knowledge  injection</b></summary>
  <p><b>编号</b>：[101]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04441</p>
  <p><b>作者</b>：Yishuang Tian,  Ning Wang,  Liang Zhang</p>
  <p><b>备注</b>：7 pages, 3 figures, 5 tables</p>
  <p><b>关键词</b>：deep neural network, neural network training, deep neural, neural network, network training model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The current deep neural network algorithm still stays in the end-to-end training supervision method like Image-Label pairs, which makes traditional algorithm is difficult to explain the reason for the results, and the prediction logic is difficult to understand and analyze. The current algorithm does not use the existing human knowledge information, which makes the model not in line with the human cognition model and makes the model not suitable for human use. In order to solve the above problems, the present invention provides a deep neural network training method based on the human knowledge, which uses the human cognition model to construct the deep neural network training model, and uses the existing human knowledge information to construct the deep neural network training model. This paper proposes a multi-level hierarchical deep learning algorithm, which is composed of multi-level hierarchical deep neural network architecture and multi-level hierarchical deep learning framework. The experimental results show that the proposed algorithm can effectively explain the hidden information of the neural network. The goal of our study is to improve the interpretability of deep neural networks (DNNs) by providing an analysis of the impact of knowledge injection on the classification task. We constructed a knowledge injection dataset with matching knowledge data and image classification data. The knowledge injection dataset is the benchmark dataset for the experiments in the paper. Our model expresses the improvement in interpretability and classification task performance of hidden layers at different scales.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Empirical Analysis of Anomaly Detection on Hyperspectral Imaging Using  Dimension Reduction Methods</b></summary>
  <p><b>编号</b>：[102]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04437</p>
  <p><b>作者</b>：Dongeon Kim,  YeongHyeon Park</p>
  <p><b>备注</b>：4 pages, 4 figures, 3 tables</p>
  <p><b>关键词</b>：detect foreign matters, invisible wavelengths including, wavelengths including ultraviolet, Recent studies, hyperspectral imaging</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent studies try to use hyperspectral imaging (HSI) to detect foreign matters in products because it enables to visualize the invisible wavelengths including ultraviolet and infrared. Considering the enormous image channels of the HSI, several dimension reduction methods-e.g., PCA or UMAP-can be considered to reduce but those cannot ease the fundamental limitations, as follows: (1) latency of HSI capturing. (2) less explanation ability of the important channels. In this paper, to circumvent the aforementioned methods, one of the ways to channel reduction, on anomaly detection proposed HSI. Different from feature extraction methods (i.e., PCA or UMAP), feature selection can sort the feature by impact and show better explainability so we might redesign the task-optimized and cost-effective spectroscopic camera. Via the extensive experiment results with synthesized MVTec AD dataset, we confirm that the feature selection method shows 6.90x faster at the inference phase compared with feature extraction-based approaches while preserving anomaly detection performance. Ultimately, we conclude the advantage of feature selection which is effective yet fast.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：i-Rebalance: Personalized Vehicle Repositioning for Supply Demand  Balance</b></summary>
  <p><b>编号</b>：[104]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04429</p>
  <p><b>作者</b>：Haoyang Chen,  Peiyan Sun,  Qiyuan Song,  Wanyuan Wang,  Weiwei Wu,  Wencan Zhang,  Guanyu Gao,  Yan Lyu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Ride-hailing platforms, demand and supply, facing the challenge, challenge of balancing, balancing demand</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Ride-hailing platforms have been facing the challenge of balancing demand and supply. Existing vehicle reposition techniques often treat drivers as homogeneous agents and relocate them deterministically, assuming compliance with the reposition. In this paper, we consider a more realistic and driver-centric scenario where drivers have unique cruising preferences and can decide whether to take the recommendation or not on their own. We propose i-Rebalance, a personalized vehicle reposition technique with deep reinforcement learning (DRL). i-Rebalance estimates drivers' decisions on accepting reposition recommendations through an on-field user study involving 99 real drivers. To optimize supply-demand balance and enhance preference satisfaction simultaneously, i-Rebalance has a sequential reposition strategy with dual DRL agents: Grid Agent to determine the reposition order of idle vehicles, and Vehicle Agent to provide personalized recommendations to each vehicle in the pre-defined order. This sequential learning strategy facilitates more effective policy training within a smaller action space compared to traditional joint-action methods. Evaluation of real-world trajectory data shows that i-Rebalance improves driver acceptance rate by 38.07% and total driver income by 9.97%.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Estimating Text Similarity based on Semantic Concept Embeddings</b></summary>
  <p><b>编号</b>：[107]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04422</p>
  <p><b>作者</b>：Tim vor der Brück,  Marc Pouly</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：enjoy great success, semantic similarity estimation, embeddings enjoy great, similarity estimation, word embeddings enjoy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Due to their ease of use and high accuracy, Word2Vec (W2V) word embeddings enjoy great success in the semantic representation of words, sentences, and whole documents as well as for semantic similarity estimation. However, they have the shortcoming that they are directly extracted from a surface representation, which does not adequately represent human thought processes and also performs poorly for highly ambiguous words. Therefore, we propose Semantic Concept Embeddings (CE) based on the MultiNet Semantic Network (SN) formalism, which addresses both shortcomings. The evaluation on a marketing target group distribution task showed that the accuracy of predicted target groups can be increased by combining traditional word embeddings with semantic CEs.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Optimal Transcoding Resolution Prediction for Efficient Per-Title  Bitrate Ladder Estimation</b></summary>
  <p><b>编号</b>：[111]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04405</p>
  <p><b>作者</b>：Jinhai Yang,  Mengxi Guo,  Shijie Zhao,  Junlin Li,  Li Zhang</p>
  <p><b>备注</b>：Accepted by the 2024 Data Compression Conference (DCC) for presentation as a poster. This is the full paper</p>
  <p><b>关键词</b>：Adaptive video streaming, meet heterogeneous network, heterogeneous network conditions, end-user demands, video streaming requires</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Adaptive video streaming requires efficient bitrate ladder construction to meet heterogeneous network conditions and end-user demands. Per-title optimized encoding typically traverses numerous encoding parameters to search the Pareto-optimal operating points for each video. Recently, researchers have attempted to predict the content-optimized bitrate ladder for pre-encoding overhead reduction. However, existing methods commonly estimate the encoding parameters on the Pareto front and still require subsequent pre-encodings. In this paper, we propose to directly predict the optimal transcoding resolution at each preset bitrate for efficient bitrate ladder construction. We adopt a Temporal Attentive Gated Recurrent Network to capture spatial-temporal features and predict transcoding resolutions as a multi-task classification problem. We demonstrate that content-optimized bitrate ladders can thus be efficiently determined without any pre-encoding. Our method well approximates the ground-truth bitrate-resolution pairs with a slight Bjøntegaard Delta rate loss of 1.21% and significantly outperforms the state-of-the-art fixed ladder.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：IGNITE: Individualized GeNeration of Imputations in Time-series  Electronic health records</b></summary>
  <p><b>编号</b>：[113]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04402</p>
  <p><b>作者</b>：Ghadeer O. Ghosheh,  Jin Li,  Tingting Zhu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：fit individual-level differences, Electronic Health Records, Health Records present, individual-level differences, present a valuable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Electronic Health Records present a valuable modality for driving personalized medicine, where treatment is tailored to fit individual-level differences. For this purpose, many data-driven machine learning and statistical models rely on the wealth of longitudinal EHRs to study patients' physiological and treatment effects. However, longitudinal EHRs tend to be sparse and highly missing, where missingness could also be informative and reflect the underlying patient's health status. Therefore, the success of data-driven models for personalized medicine highly depends on how the EHR data is represented from physiological data, treatments, and the missing values in the data. To this end, we propose a novel deep-learning model that learns the underlying patient dynamics over time across multivariate data to generate personalized realistic values conditioning on an individual's demographic characteristics and treatments. Our proposed model, IGNITE (Individualized GeNeration of Imputations in Time-series Electronic health records), utilises a conditional dual-variational autoencoder augmented with dual-stage attention to generate missing values for an individual. In IGNITE, we further propose a novel individualized missingness mask (IMM), which helps our model generate values based on the individual's observed data and missingness patterns. We further extend the use of IGNITE from imputing missingness to a personalized data synthesizer, where it generates missing EHRs that were never observed prior or even generates new patients for various applications. We validate our model on three large publicly available datasets and show that IGNITE outperforms state-of-the-art approaches in missing data reconstruction and task prediction.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Machine unlearning through fine-grained model parameters perturbation</b></summary>
  <p><b>编号</b>：[119]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04385</p>
  <p><b>作者</b>：Zhiwei Zuo,  Zhuo Tang,  Kenli Li,  Anwitaman Datta</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：unlearning, retracting data records, Machine unlearning, involve retracting data, inexact machine unlearning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine unlearning techniques, which involve retracting data records and reducing influence of said data on trained models, help with the user privacy protection objective but incur significant computational costs. Weight perturbation-based unlearning is a general approach, but it typically involves globally modifying the parameters. We propose fine-grained Top-K and Random-k parameters perturbed inexact machine unlearning strategies that address the privacy needs while keeping the computational costs tractable.
In order to demonstrate the efficacy of our strategies we also tackle the challenge of evaluating the effectiveness of machine unlearning by considering the model's generalization performance across both unlearning and remaining data. To better assess the unlearning effect and model generalization, we propose novel metrics, namely, the forgetting rate and memory retention rate. However, for inexact machine unlearning, current metrics are inadequate in quantifying the degree of forgetting that occurs after unlearning strategies are applied. To address this, we introduce SPD-GAN, which subtly perturbs the distribution of data targeted for unlearning. Then, we evaluate the degree of unlearning by measuring the performance difference of the models on the perturbed unlearning data before and after the unlearning process. By implementing these innovative techniques and metrics, we achieve computationally efficacious privacy protection in machine learning applications without significant sacrifice of model performance. Furthermore, this approach provides a novel method for evaluating the degree of unlearning.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Towards Explainable Artificial Intelligence (XAI): A Data Mining  Perspective</b></summary>
  <p><b>编号</b>：[122]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04374</p>
  <p><b>作者</b>：Haoyi Xiong,  Xuhong L,  Xiaofei Zhang,  Jiamin Chen,  Xinhao Sun,  Yuchen Li,  Zeyi Sun,  Mengnan Du</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：deep neural networks, neural networks, extensive efforts, accessible terms, complexity and lack</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Given the complexity and lack of transparency in deep neural networks (DNNs), extensive efforts have been made to make these systems more interpretable or explain their behaviors in accessible terms. Unlike most reviews, which focus on algorithmic and model-centric perspectives, this work takes a "data-centric" view, examining how data collection, processing, and analysis contribute to explainable AI (XAI). We categorize existing work into three categories subject to their purposes: interpretations of deep models, referring to feature attributions and reasoning processes that correlate data points with model outputs; influences of training data, examining the impact of training data nuances, such as data valuation and sample anomalies, on decision-making processes; and insights of domain knowledge, discovering latent patterns and fostering new knowledge from data and models to advance social values and scientific discovery. Specifically, we distill XAI methodologies into data mining operations on training and testing data across modalities, such as images, text, and tabular data, as well as on training logs, checkpoints, models and other DNN behavior descriptors. In this way, our study offers a comprehensive, data-centric examination of XAI from a lens of data mining methods and applications.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Representative Feature Extraction During Diffusion Process for Sketch  Extraction with One Example</b></summary>
  <p><b>编号</b>：[128]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04362</p>
  <p><b>作者</b>：Kwan Yun,  Youngseo Kim,  Kwanggyoon Seo,  Chang Wook Seo,  Junyong Noh</p>
  <p><b>备注</b>：8 pages(main paper), 8 pages(supplementary material)</p>
  <p><b>关键词</b>：generating a variety, variety of stylized, features, introduce DiffSketch, stylized sketches</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce DiffSketch, a method for generating a variety of stylized sketches from images. Our approach focuses on selecting representative features from the rich semantics of deep features within a pretrained diffusion model. This novel sketch generation method can be trained with one manual drawing. Furthermore, efficient sketch extraction is ensured by distilling a trained generator into a streamlined extractor. We select denoising diffusion features through analysis and integrate these selected features with VAE features to produce sketches. Additionally, we propose a sampling scheme for training models using a conditional generative approach. Through a series of comparisons, we verify that distilled DiffSketch not only outperforms existing state-of-the-art sketch extraction methods but also surpasses diffusion-based stylization methods in the task of extracting sketches.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Improving the Robustness of Knowledge-Grounded Dialogue via Contrastive  Learning</b></summary>
  <p><b>编号</b>：[129]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04361</p>
  <p><b>作者</b>：Jiaan Wang,  Jianfeng Qu,  Kexin Wang,  Zhixu Li,  Wen Hua,  Ximing Li,  An Liu</p>
  <p><b>备注</b>：Accepted by AAAI 2024</p>
  <p><b>关键词</b>：knowledge graphs, external knowledge, Knowledge-grounded dialogue, KGD, dialogue context</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Knowledge-grounded dialogue (KGD) learns to generate an informative response based on a given dialogue context and external knowledge (\emph{e.g.}, knowledge graphs; KGs). Recently, the emergence of large language models (LLMs) and pre-training techniques has brought great success to knowledge-grounded dialogue. However, when building KGD systems in real applications, there are various real-world noises that are inevitable to face. For example, the dialogue context might involve perturbations such as misspellings and abbreviations. In addition, KGs typically suffer from incompletion and also might contain erroneous and outdated facts. Such real-world noises pose a challenge to the robustness of KGD systems and hinder their applications in the real world. In this paper, we propose an entity-based contrastive learning framework for improving the robustness of KGD. Specifically, we make use of the entity information in a KGD sample to create both its positive and negative samples which involve semantic-irrelevant and semantic-relevant perturbations, respectively. The contrastive learning framework ensures the KGD model is aware of these two types of perturbations, thus generating informative responses with the potentially noisy inputs in real applications. Experimental results on three benchmark datasets show that our method achieves new state-of-the-art performance in terms of automatic evaluation scores, verifying its effectiveness and potentiality. Furthermore, we show that our method can generate better responses than comparison models in both the noisy and the few-shot settings.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Iterative Feedback Network for Unsupervised Point Cloud Registration</b></summary>
  <p><b>编号</b>：[132]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04357</p>
  <p><b>作者</b>：Yifan Xie,  Boyu Wang,  Shiqi Li,  Jihua Zhu</p>
  <p><b>备注</b>：8 pages, accepted by RAL 2024</p>
  <p><b>关键词</b>：point cloud registration, cloud registration aims, computer vision, feedback high-level features, fundamental problem</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As a fundamental problem in computer vision, point cloud registration aims to seek the optimal transformation for aligning a pair of point clouds. In most existing methods, the information flows are usually forward transferring, thus lacking the guidance from high-level information to low-level information. Besides, excessive high-level information may be overly redundant, and directly using it may conflict with the original low-level information. In this paper, we propose a novel Iterative Feedback Network (IFNet) for unsupervised point cloud registration, in which the representation of low-level features is efficiently enriched by rerouting subsequent high-level features. Specifically, our IFNet is built upon a series of Feedback Registration Block (FRB) modules, with each module responsible for generating the feedforward rigid transformation and feedback high-level features. These FRB modules are cascaded and recurrently unfolded over time. Further, the Feedback Transformer is designed to efficiently select relevant information from feedback high-level features, which is utilized to refine the low-level features. What's more, we incorporate a geometry-awareness descriptor to empower the network for making full use of most geometric information, which leads to more precise registration results. Extensive experiments on various benchmark datasets demonstrate the superior registration performance of our IFNet.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：A Change Point Detection Integrated Remaining Useful Life Estimation  Model under Variable Operating Conditions</b></summary>
  <p><b>编号</b>：[134]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04351</p>
  <p><b>作者</b>：Anushiya Arunan,  Yan Qin,  Xiaoli Li,  Chau Yuen</p>
  <p><b>备注</b>：Accepted in Control Engineering Practice Journal with DOI: this https URL</p>
  <p><b>关键词</b>：health status evaluation, status evaluation serves, significant preliminary step, RUL estimation, change points</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>By informing the onset of the degradation process, health status evaluation serves as a significant preliminary step for reliable remaining useful life (RUL) estimation of complex equipment. This paper proposes a novel temporal dynamics learning-based model for detecting change points of individual devices, even under variable operating conditions, and utilises the learnt change points to improve the RUL estimation accuracy. During offline model development, the multivariate sensor data are decomposed to learn fused temporal correlation features that are generalisable and representative of normal operation dynamics across multiple operating conditions. Monitoring statistics and control limit thresholds for normal behaviour are dynamically constructed from these learnt temporal features for the unsupervised detection of device-level change points. The detected change points then inform the degradation data labelling for training a long short-term memory (LSTM)-based RUL estimation model. During online monitoring, the temporal correlation dynamics of a query device is monitored for breach of the control limit derived in offline training. If a change point is detected, the device's RUL is estimated with the well-trained offline model for early preventive action. Using C-MAPSS turbofan engines as the case study, the proposed method improved the accuracy by 5.6\% and 7.5\% for two scenarios with six operating conditions, when compared to existing LSTM-based RUL estimation models that do not consider heterogeneous change points.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Memory-Efficient Personalization using Quantized Diffusion Model</b></summary>
  <p><b>编号</b>：[141]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04339</p>
  <p><b>作者</b>：Hyogon Ryu,  Seohyun Lim,  Hyunjung Shim</p>
  <p><b>备注</b>：20 pages</p>
  <p><b>关键词</b>：Stable Diffusion, markedly advances, billion-parameter diffusion models, rise of billion-parameter, advances the field</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rise of billion-parameter diffusion models like Stable Diffusion XL, Imagen, and Dall-E3 markedly advances the field of generative AI. However, their large-scale nature poses challenges in fine-tuning and deployment due to high resource demands and slow inference speed. This paper ventures into the relatively unexplored yet promising realm of fine-tuning quantized diffusion models. We establish a strong baseline by customizing three models: PEQA for fine-tuning quantization parameters, Q-Diffusion for post-training quantization, and DreamBooth for personalization. Our analysis reveals a notable trade-off between subject and prompt fidelity within the baseline model. To address these issues, we introduce two strategies, inspired by the distinct roles of different timesteps in diffusion models: S1 optimizing a single set of fine-tuning parameters exclusively at selected intervals, and S2 creating multiple fine-tuning parameter sets, each specialized for different timestep intervals. Our approach not only enhances personalization but also upholds prompt fidelity and image quality, significantly outperforming the baseline qualitatively and quantitatively. The code will be made publicly available.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Deep Efficient Private Neighbor Generation for Subgraph Federated  Learning</b></summary>
  <p><b>编号</b>：[143]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04336</p>
  <p><b>作者</b>：Ke Zhang,  Lichao Sun,  Bolin Ding,  Siu Ming Yiu,  Carl Yang</p>
  <p><b>备注</b>：Accepted to SDM 2024</p>
  <p><b>关键词</b>：multiple data owners, realistic applications, fragmented and separately, separately stored, stored by multiple</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Behemoth graphs are often fragmented and separately stored by multiple data owners as distributed subgraphs in many realistic applications. Without harming data privacy, it is natural to consider the subgraph federated learning (subgraph FL) scenario, where each local client holds a subgraph of the entire global graph, to obtain globally generalized graph mining models. To overcome the unique challenge of incomplete information propagation on local subgraphs due to missing cross-subgraph neighbors, previous works resort to the augmentation of local neighborhoods through the joint FL of missing neighbor generators and GNNs. Yet their technical designs have profound limitations regarding the utility, efficiency, and privacy goals of FL. In this work, we propose FedDEP to comprehensively tackle these challenges in subgraph FL. FedDEP consists of a series of novel technical designs: (1) Deep neighbor generation through leveraging the GNN embeddings of potential missing neighbors; (2) Efficient pseudo-FL for neighbor generation through embedding prototyping; and (3) Privacy protection through noise-less edge-local-differential-privacy.
We analyze the correctness and efficiency of FedDEP, and provide theoretical guarantees on its privacy.
Empirical results on four real-world datasets justify the clear benefits of proposed techniques.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Large Language Models for Robotics: Opportunities, Challenges, and  Perspectives</b></summary>
  <p><b>编号</b>：[144]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04334</p>
  <p><b>作者</b>：Jiaqi Wang,  Zihao Wu,  Yiwei Li,  Hanqi Jiang,  Peng Shu,  Enze Shi,  Huawen Hu,  Chong Ma,  Yiheng Liu,  Xuhui Wang,  Yincheng Yao,  Xuan Liu,  Huaqin Zhao,  Zhengliang Liu,  Haixing Dai,  Lin Zhao,  Bao Ge,  Xiang Li,  Tianming Liu,  Shu Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：undergone significant expansion, Large language models, undergone significant, significant expansion, increasingly integrated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have undergone significant expansion and have been increasingly integrated across various domains. Notably, in the realm of robot task planning, LLMs harness their advanced reasoning and language comprehension capabilities to formulate precise and efficient action plans based on natural language instructions. However, for embodied tasks, where robots interact with complex environments, text-only LLMs often face challenges due to a lack of compatibility with robotic visual perception. This study provides a comprehensive overview of the emerging integration of LLMs and multimodal LLMs into various robotic tasks. Additionally, we propose a framework that utilizes multimodal GPT-4V to enhance embodied task planning through the combination of natural language instructions and robot visual perceptions. Our results, based on diverse datasets, indicate that GPT-4V effectively enhances robot performance in embodied tasks. This extensive survey and evaluation of LLMs and multimodal LLMs across a variety of robotic tasks enriches the understanding of LLM-centric embodied intelligence and provides forward-looking insights toward bridging the gap in Human-Robot-Environment interaction.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Coupling Graph Neural Networks with Fractional Order Continuous  Dynamics: A Robustness Study</b></summary>
  <p><b>编号</b>：[146]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04331</p>
  <p><b>作者</b>：Qiyu Kang,  Kai Zhao,  Yang Song,  Yihang Xie,  Yanan Zhao,  Sijie Wang,  Rui She,  Wee Peng Tay</p>
  <p><b>备注</b>：in Proc. AAAI Conference on Artificial Intelligence, Vancouver, Canada, Feb. 2024</p>
  <p><b>关键词</b>：graph neural FDE, neural FDE models, graph neural, neural FDE, traditional graph neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we rigorously investigate the robustness of graph neural fractional-order differential equation (FDE) models. This framework extends beyond traditional graph neural (integer-order) ordinary differential equation (ODE) models by implementing the time-fractional Caputo derivative. Utilizing fractional calculus allows our model to consider long-term memory during the feature updating process, diverging from the memoryless Markovian updates seen in traditional graph neural ODE models. The superiority of graph neural FDE models over graph neural ODE models has been established in environments free from attacks or perturbations. While traditional graph neural ODE models have been verified to possess a degree of stability and resilience in the presence of adversarial attacks in existing literature, the robustness of graph neural FDE models, especially under adversarial conditions, remains largely unexplored. This paper undertakes a detailed assessment of the robustness of graph neural FDE models. We establish a theoretical foundation outlining the robustness characteristics of graph neural FDE models, highlighting that they maintain more stringent output perturbation bounds in the face of input and graph topology disturbances, compared to their integer-order counterparts. Our empirical evaluations further confirm the enhanced robustness of graph neural FDE models, highlighting their potential in adversarially robust applications.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：BD-MSA: Body decouple VHR Remote Sensing Image Change Detection method  guided by multi-scale feature information aggregation</b></summary>
  <p><b>编号</b>：[147]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04330</p>
  <p><b>作者</b>：Yonghui Tan,  Xiaolong Li,  Yishu Chen,  Jinquan Ai</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：sensing image change, image change detection, remote sensing image, bi-temporal images, Aggregation change detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The purpose of remote sensing image change detection (RSCD) is to detect differences between bi-temporal images taken at the same place. Deep learning has been extensively used to RSCD tasks, yielding significant results in terms of result recognition. However, due to the shooting angle of the satellite, the impacts of thin clouds, and certain lighting conditions, the problem of fuzzy edges in the change region in some remote sensing photographs cannot be properly handled using current RSCD algorithms. To solve this issue, we proposed a Body Decouple Multi-Scale by fearure Aggregation change detection (BD-MSA), a novel model that collects both global and local feature map information in the channel and space dimensions of the feature map during the training and prediction phases. This approach allows us to successfully extract the change region's boundary information while also divorcing the change region's main body from its boundary. Numerous studies have shown that the assessment metrics and evaluation effects of the model described in this paper on the publicly available datasets DSIFN-CD and S2Looking are the best when compared to other models.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Know Your Needs Better: Towards Structured Understanding of Marketer  Demands with Analogical Reasoning Augmented LLMs</b></summary>
  <p><b>编号</b>：[152]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04319</p>
  <p><b>作者</b>：Junjie Wang,  Dan Yang,  Binbin Hu,  Yue Shen,  Ziqi Liu,  Wen Zhang,  Jinjie Gu,  Zhiqiang Zhang</p>
  <p><b>备注</b>：Under review</p>
  <p><b>关键词</b>：target users solely, natural language form, user targeting, target users, users solely</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we explore a new way for user targeting, where non-expert marketers could select their target users solely given demands in natural language form. The key to this issue is how to transform natural languages into practical structured logical languages, i.e., the structured understanding of marketer demands. Considering the impressive natural language processing ability of large language models (LLMs), we try to leverage LLMs to solve this issue. Past research indicates that the reasoning ability of LLMs can be effectively enhanced through chain-of-thought (CoT) prompting. But existing methods still have some limitations: (1) Previous methods either use simple "Let's think step by step" spells or provide fixed examples in demonstrations without considering compatibility between prompts and questions, making LLMs ineffective in some complex reasoning tasks such as structured language transformation. (2) Previous methods are often implemented in closed-source models or excessively large models, which is not suitable in industrial practical scenarios. Based on these, we propose ARALLM (i.e., Analogical Reasoning Augmented Large Language Models) consisting of two modules: Analogical Reasoning based Prompting and Reasoning-Augmented Multi-Task Model Distillation.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：StarCraftImage: A Dataset For Prototyping Spatial Reasoning Methods For  Multi-Agent Environments</b></summary>
  <p><b>编号</b>：[162]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04290</p>
  <p><b>作者</b>：Sean Kulinski,  Nicholas R. Waytowich,  James Z. Hare,  David I. Inouye</p>
  <p><b>备注</b>：Published in CVPR 23'</p>
  <p><b>关键词</b>：missing data imputation, agent type identification, event prediction, multiple applications, autonomous surveillance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Spatial reasoning tasks in multi-agent environments such as event prediction, agent type identification, or missing data imputation are important for multiple applications (e.g., autonomous surveillance over sensor networks and subtasks for reinforcement learning (RL)). StarCraft II game replays encode intelligent (and adversarial) multi-agent behavior and could provide a testbed for these tasks; however, extracting simple and standardized representations for prototyping these tasks is laborious and hinders reproducibility. In contrast, MNIST and CIFAR10, despite their extreme simplicity, have enabled rapid prototyping and reproducibility of ML methods. Following the simplicity of these datasets, we construct a benchmark spatial reasoning dataset based on StarCraft II replays that exhibit complex multi-agent behaviors, while still being as easy to use as MNIST and CIFAR10. Specifically, we carefully summarize a window of 255 consecutive game states to create 3.6 million summary images from 60,000 replays, including all relevant metadata such as game outcome and player races. We develop three formats of decreasing complexity: Hyperspectral images that include one channel for every unit type (similar to multispectral geospatial images), RGB images that mimic CIFAR10, and grayscale images that mimic MNIST. We show how this dataset can be used for prototyping spatial reasoning methods. All datasets, code for extraction, and code for dataset loading can be found at this https URL</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Robust Image Watermarking using Stable Diffusion</b></summary>
  <p><b>编号</b>：[177]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04247</p>
  <p><b>作者</b>：Lijun Zhang,  Xiao Liu,  Antoni Viros Martin,  Cindy Xiong Bearfield,  Yuriy Brun,  Hui Guan</p>
  <p><b>备注</b>：15 pages, 14 figures</p>
  <p><b>关键词</b>：tracking image provenance, claiming ownership, critical for tracking, provenance and claiming, stable diffusion</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Watermarking images is critical for tracking image provenance and claiming ownership. With the advent of generative models, such as stable diffusion, able to create fake but realistic images, watermarking has become particularly important, e.g., to make generated images reliably identifiable. Unfortunately, the very same stable diffusion technology can remove watermarks injected using existing methods. To address this problem, we present a ZoDiac, which uses a pre-trained stable diffusion model to inject a watermark into the trainable latent space, resulting in watermarks that can be reliably detected in the latent vector, even when attacked. We evaluate ZoDiac on three benchmarks, MS-COCO, DiffusionDB, and WikiArt, and find that ZoDiac is robust against state-of-the-art watermark attacks, with a watermark detection rate over 98% and a false positive rate below 6.4%, outperforming state-of-the-art watermarking methods. Our research demonstrates that stable diffusion is a promising approach to robust watermarking, able to withstand even stable-diffusion-based attacks.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：FunnyNet-W: Multimodal Learning of Funny Moments in Videos in the Wild</b></summary>
  <p><b>编号</b>：[185]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04210</p>
  <p><b>作者</b>：Zhi-Song Liu,  Robin Courant,  Vicky Kalogeiton</p>
  <p><b>备注</b>：22 pages, 14 figures</p>
  <p><b>关键词</b>：make people laugh, funny moments, Large Language Model, dialogues and culture, predict funny moments</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automatically understanding funny moments (i.e., the moments that make people laugh) when watching comedy is challenging, as they relate to various features, such as body language, dialogues and culture. In this paper, we propose FunnyNet-W, a model that relies on cross- and self-attention for visual, audio and text data to predict funny moments in videos. Unlike most methods that rely on ground truth data in the form of subtitles, in this work we exploit modalities that come naturally with videos: (a) video frames as they contain visual information indispensable for scene understanding, (b) audio as it contains higher-level cues associated with funny moments, such as intonation, pitch and pauses and (c) text automatically extracted with a speech-to-text model as it can provide rich information when processed by a Large Language Model. To acquire labels for training, we propose an unsupervised approach that spots and labels funny audio moments. We provide experiments on five datasets: the sitcoms TBBT, MHD, MUStARD, Friends, and the TED talk UR-Funny. Extensive experiments and analysis show that FunnyNet-W successfully exploits visual, auditory and textual cues to identify funny moments, while our findings reveal FunnyNet-W's ability to predict funny moments in the wild. FunnyNet-W sets the new state of the art for funny moment detection with multimodal cues on all datasets with and without using ground truth information.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Learning Racing From an AI Coach: Effects of Multimodal Autonomous  Driving Explanations on Driving Performance, Cognitive Load, Expertise, and  Trust</b></summary>
  <p><b>编号</b>：[186]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04206</p>
  <p><b>作者</b>：Robert Kaufman,  Jean Costa,  Everlyne Kimani</p>
  <p><b>备注</b>：15 pages</p>
  <p><b>关键词</b>：explanatory communications modeled, pre-post experiment, Coach explanatory communications, human driving experts, Coach explanatory</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In a pre-post experiment (n = 41), we test the impact of an AI Coach's explanatory communications modeled after the instructions of human driving experts. Participants were divided into four (4) groups to assess two (2) dimensions of the AI coach's explanations: information type ('what' and 'why'-type explanations) and presentation modality (auditory and visual). We directly compare how AI Coaching sessions employing these techniques impact driving performance, cognitive load, confidence, expertise, and trust in an observation learning context. Through interviews, we delineate the learning process of our participants. Results show that an AI driving coach can be useful for teaching performance driving skills to novices. Comparing between groups, we find the type and modality of information influences performance outcomes. We attribute differences to how information directed attention, mitigated uncertainty, and influenced overload experienced by participants. These, in turn, affected how successfully participants were able to learn. Results suggest efficient, modality-appropriate explanations should be opted for when designing effective HMI communications that can instruct without overwhelming. Further, they support the need to align communications with human learning and cognitive processes. Results are synthesized into eight design implications for future autonomous vehicle HMI and AI coach design.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Curiosity & Entropy Driven Unsupervised RL in Multiple Environments</b></summary>
  <p><b>编号</b>：[187]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04198</p>
  <p><b>作者</b>：Shaurya Dewan,  Anisha Jain,  Zoe LaLena,  Lifan Yu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Unsupervised Reinforcement Learning, Unsupervised Reinforcement, Multiple environments' propose, Multiple environments', tackle unsupervised</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The authors of 'Unsupervised Reinforcement Learning in Multiple environments' propose a method, alpha-MEPOL, to tackle unsupervised RL across multiple environments. They pre-train a task-agnostic exploration policy using interactions from an entire environment class and then fine-tune this policy for various tasks using supervision. We expanded upon this work, with the goal of improving performance. We primarily propose and experiment with five new modifications to the original work: sampling trajectories using an entropy-based probability distribution, dynamic alpha, higher KL Divergence threshold, curiosity-driven exploration, and alpha-percentile sampling on curiosity. Dynamic alpha and higher KL-Divergence threshold both provided a significant improvement over the baseline from the earlier work. PDF-sampling failed to provide any improvement due to it being approximately equivalent to the baseline method when the sample space is small. In high-dimensional environments, the addition of curiosity-driven exploration enhances learning by encouraging the agent to seek diverse experiences and explore the unknown more. However, its benefits are limited in low-dimensional and simpler environments where exploration possibilities are constrained and there is little that is truly unknown to the agent. Overall, some of our experiments did boost performance over the baseline and there are a few directions that seem promising for further research.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Interactive Multi-Objective Evolutionary Optimization of Software  Architectures</b></summary>
  <p><b>编号</b>：[189]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04192</p>
  <p><b>作者</b>：Aurora Ramírez,  José Raúl Romero,  Sebastián Ventura</p>
  <p><b>备注</b>：41 pages, 5 figures, journal "Information Sciences"</p>
  <p><b>关键词</b>：software specification, multiple software metrics, criteria are met, architectural alternatives, software</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While working on a software specification, designers usually need to evaluate different architectural alternatives to be sure that quality criteria are met. Even when these quality aspects could be expressed in terms of multiple software metrics, other qualitative factors cannot be numerically measured, but they are extracted from the engineer's know-how and prior experiences. In fact, detecting not only strong but also weak points in the different solutions seems to fit better with the way humans make their decisions. Putting the human in the loop brings new challenges to the search-based software engineering field, especially for those human-centered activities within the early analysis phase. This paper explores how the interactive evolutionary computation can serve as a basis for integrating the human's judgment into the search process. An interactive approach is proposed to discover software architectures, in which both quantitative and qualitative criteria are applied to guide a multi-objective evolutionary algorithm. The obtained feedback is incorporated into the fitness function using architectural preferences allowing the algorithm to discern between promising and poor solutions. Experimentation with real users has revealed that the proposed interaction mechanism can effectively guide the search towards those regions of the search space that are of real interest to the expert.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Efficient Selective Audio Masked Multimodal Bottleneck Transformer for  Audio-Video Classification</b></summary>
  <p><b>编号</b>：[192]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04154</p>
  <p><b>作者</b>：Wentao Zhu</p>
  <p><b>备注</b>：Accepted by WACV 2024; well-formatted PDF is in this https URL arXiv admin note: text overlap with arXiv:2401.04023</p>
  <p><b>关键词</b>：mainstream media platforms, AVT, Transformer, video Transformer, media platforms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Audio and video are two most common modalities in the mainstream media platforms, e.g., YouTube. To learn from multimodal videos effectively, in this work, we propose a novel audio-video recognition approach termed audio video Transformer, AVT, leveraging the effective spatio-temporal representation by the video Transformer to improve action recognition accuracy. For multimodal fusion, simply concatenating multimodal tokens in a cross-modal Transformer requires large computational and memory resources, instead we reduce the cross-modality complexity through an audio-video bottleneck Transformer. To improve the learning efficiency of multimodal Transformer, we integrate self-supervised objectives, i.e., audio-video contrastive learning, audio-video matching, and masked audio and video learning, into AVT training, which maps diverse audio and video representations into a common multimodal representation space. We further propose a masked audio segment loss to learn semantic audio activities in AVT. Extensive experiments and ablation studies on three public datasets and two in-house datasets consistently demonstrate the effectiveness of the proposed AVT. Specifically, AVT outperforms its previous state-of-the-art counterparts on Kinetics-Sounds by 8%. AVT also surpasses one of the previous state-of-the-art video Transformers [25] by 10% on VGGSound by leveraging the audio signal. Compared to one of the previous state-of-the-art multimodal methods, MBT [32], AVT is 1.3% more efficient in terms of FLOPs and improves the accuracy by 3.8% on Epic-Kitchens-100.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Cross-Speaker Encoding Network for Multi-Talker Speech Recognition</b></summary>
  <p><b>编号</b>：[193]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04152</p>
  <p><b>作者</b>：Jiawen Kang,  Lingwei Meng,  Mingyu Cui,  Haohan Guo,  Xixin Wu,  Xunying Liu,  Helen Meng</p>
  <p><b>备注</b>：Accepted by ICASSP2024</p>
  <p><b>关键词</b>：garnered great interest, directly transcribe overlapped, transcribe overlapped speech, multiple speakers, garnered great</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>End-to-end multi-talker speech recognition has garnered great interest as an effective approach to directly transcribe overlapped speech from multiple speakers. Current methods typically adopt either 1) single-input multiple-output (SIMO) models with a branched encoder, or 2) single-input single-output (SISO) models based on attention-based encoder-decoder architecture with serialized output training (SOT). In this work, we propose a Cross-Speaker Encoding (CSE) network to address the limitations of SIMO models by aggregating cross-speaker representations. Furthermore, the CSE model is integrated with SOT to leverage both the advantages of SIMO and SISO while mitigating their drawbacks. To the best of our knowledge, this work represents an early effort to integrate SIMO and SISO for multi-talker speech recognition. Experiments on the two-speaker LibrispeechMix dataset show that the CES model reduces word error rate (WER) by 8% over the SIMO baseline. The CSE-SOT model reduces WER by 10% overall and by 16% on high-overlap speech compared to the SOT model.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Online Test-Time Adaptation of Spatial-Temporal Traffic Flow Forecasting</b></summary>
  <p><b>编号</b>：[196]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04148</p>
  <p><b>作者</b>：Pengxin Guo,  Pengrong Jin,  Ziyue Li,  Lei Bai,  Yu Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：traffic flow forecasting, optimal travel routes, implementing control measures, selecting optimal travel, aiding traffic managers</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Accurate spatial-temporal traffic flow forecasting is crucial in aiding traffic managers in implementing control measures and assisting drivers in selecting optimal travel routes. Traditional deep-learning based methods for traffic flow forecasting typically rely on historical data to train their models, which are then used to make predictions on future data. However, the performance of the trained model usually degrades due to the temporal drift between the historical and future data. To make the model trained on historical data better adapt to future data in a fully online manner, this paper conducts the first study of the online test-time adaptation techniques for spatial-temporal traffic flow forecasting problems. To this end, we propose an Adaptive Double Correction by Series Decomposition (ADCSD) method, which first decomposes the output of the trained model into seasonal and trend-cyclical parts and then corrects them by two separate modules during the testing phase using the latest observed data entry by entry. In the proposed ADCSD method, instead of fine-tuning the whole trained model during the testing phase, a lite network is attached after the trained model, and only the lite network is fine-tuned in the testing process each time a data entry is observed. Moreover, to satisfy that different time series variables may have different levels of temporal drift, two adaptive vectors are adopted to provide different weights for different time series variables. Extensive experiments on four real-world traffic flow forecasting datasets demonstrate the effectiveness of the proposed ADCSD method. The code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Learn Once Plan Arbitrarily (LOPA): Attention-Enhanced Deep  Reinforcement Learning Method for Global Path Planning</b></summary>
  <p><b>编号</b>：[197]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04145</p>
  <p><b>作者</b>：Guoming Huang,  Mingxin Hou,  Xiaofang Yuan,  Shuqiao Huang,  Yaonan Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Deep reinforcement learning, recently shown promise, Deep reinforcement, reinforcement learning, recently shown</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep reinforcement learning (DRL) methods have recently shown promise in path planning tasks. However, when dealing with global planning tasks, these methods face serious challenges such as poor convergence and generalization. To this end, we propose an attention-enhanced DRL method called LOPA (Learn Once Plan Arbitrarily) in this paper. Firstly, we analyze the reasons of these problems from the perspective of DRL's observation, revealing that the traditional design causes DRL to be interfered by irrelevant map information. Secondly, we develop the LOPA which utilizes a novel attention-enhanced mechanism to attain an improved attention capability towards the key information of the observation. Such a mechanism is realized by two steps: (1) an attention model is built to transform the DRL's observation into two dynamic views: local and global, significantly guiding the LOPA to focus on the key information on the given maps; (2) a dual-channel network is constructed to process these two views and integrate them to attain an improved reasoning capability. The LOPA is validated via multi-objective global path planning experiments. The result suggests the LOPA has improved convergence and generalization performance as well as great path planning efficiency.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Robust Calibration For Improved Weather Prediction Under Distributional  Shift</b></summary>
  <p><b>编号</b>：[198]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04144</p>
  <p><b>作者</b>：Sankalp Gilda,  Neel Bhandari,  Wendy Mak,  Andrea Panizza</p>
  <p><b>备注</b>：Presented at the Bayesian Deep Learning workshop at NeurIPS 2021</p>
  <p><b>关键词</b>：Real-World Distributional Shift, Distributional Shift, Robustness and Uncertainty, Shifts Challenge, Real-World Distributional</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we present results on improving out-of-domain weather prediction and uncertainty estimation as part of the \texttt{Shifts Challenge on Robustness and Uncertainty under Real-World Distributional Shift} challenge. We find that by leveraging a mixture of experts in conjunction with an advanced data augmentation technique borrowed from the computer vision domain, in conjunction with robust \textit{post-hoc} calibration of predictive uncertainties, we can potentially achieve more accurate and better-calibrated results with deep neural networks than with boosted tree models for tabular data. We quantify our predictions using several metrics and propose several future lines of inquiry and experimentation to boost performance.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：On The Potential of The Fractal Geometry and The CNNs Ability to Encode  it</b></summary>
  <p><b>编号</b>：[200]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04141</p>
  <p><b>作者</b>：Julia El Zini,  Bassel Musharrafieh,  Mariette Awad</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：measuring scale, fractal, fractal features, fractal dimension, statistical index</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The fractal dimension provides a statistical index of object complexity by studying how the pattern changes with the measuring scale. Although useful in several classification tasks, the fractal dimension is under-explored in deep learning applications. In this work, we investigate the features that are learned by deep models and we study whether these deep networks are able to encode features as complex and high-level as the fractal dimensions. Specifically, we conduct a correlation analysis experiment to show that deep networks are not able to extract such a feature in none of their layers. We combine our analytical study with a human evaluation to investigate the differences between deep learning networks and models that operate on the fractal feature solely. Moreover, we show the effectiveness of fractal features in applications where the object structure is crucial for the classification task. We empirically show that training a shallow network on fractal features achieves performance comparable, even superior in specific cases, to that of deep networks trained on raw data while requiring less computational resources. Fractals improved the accuracy of the classification by 30% on average while requiring up to 84% less time to train. We couple our empirical study with a complexity analysis of the computational cost of extracting the proposed fractal features, and we study its limitation.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Expanding Horizons in HCI Research Through LLM-Driven Qualitative  Analysis</b></summary>
  <p><b>编号</b>：[202]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04138</p>
  <p><b>作者</b>：Maya Grace Torii,  Takahito Murakami,  Yoichi Ochiai</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Language Models, papers typed, Large Language, send</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>How would research be like if we still needed to "send" papers typed with a typewriter? Our life and research environment have continually evolved, often accompanied by controversial opinions about new methodologies. In this paper, we embrace this change by introducing a new approach to qualitative analysis in HCI using Large Language Models (LLMs). We detail a method that uses LLMs for qualitative data analysis and present a quantitative framework using SBART cosine similarity for performance evaluation. Our findings indicate that LLMs not only match the efficacy of traditional analysis methods but also offer unique insights. Through a novel dataset and benchmark, we explore LLMs' characteristics in HCI research, suggesting potential avenues for further exploration and application in the field.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：The Stronger the Diffusion Model, the Easier the Backdoor: Data  Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline</b></summary>
  <p><b>编号</b>：[203]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04136</p>
  <p><b>作者</b>：Haonan Wang,  Qianli Shen,  Yao Tong,  Yang Zhang,  Kenji Kawaguchi</p>
  <p><b>备注</b>：This study reveals that by subtly inserting non-copyright-infringing poisoning data into a diffusion model's training dataset, it's possible to trigger the model to generate copyrighted content, highlighting vulnerabilities in current copyright protection strategies</p>
  <p><b>关键词</b>：generate high-quality images, ability to generate, generate high-quality, indistinguishable from real, potential copyright concerns</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The commercialization of diffusion models, renowned for their ability to generate high-quality images that are often indistinguishable from real ones, brings forth potential copyright concerns. Although attempts have been made to impede unauthorized access to copyrighted material during training and to subsequently prevent DMs from generating copyrighted images, the effectiveness of these solutions remains unverified. This study explores the vulnerabilities associated with copyright protection in DMs by introducing a backdoor data poisoning attack (SilentBadDiffusion) against text-to-image diffusion models. Our attack method operates without requiring access to or control over the diffusion model's training or fine-tuning processes; it merely involves the insertion of poisoning data into the clean training dataset. This data, comprising poisoning images equipped with prompts, is generated by leveraging the powerful capabilities of multimodal large language models and text-guided image inpainting techniques. Our experimental results and analysis confirm the method's effectiveness. By integrating a minor portion of non-copyright-infringing stealthy poisoning data into the clean dataset-rendering it free from suspicion-we can prompt the finetuned diffusion models to produce copyrighted content when activated by specific trigger prompts. These findings underline potential pitfalls in the prevailing copyright protection strategies and underscore the necessity for increased scrutiny and preventative measures against the misuse of DMs.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Global-Aware Enhanced Spatial-Temporal Graph Recurrent Networks: A New  Framework For Traffic Flow Prediction</b></summary>
  <p><b>编号</b>：[204]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04135</p>
  <p><b>作者</b>：Haiyang Liu,  Chunjiang Zhu,  Detian Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：enhancing transport efficiency, flow prediction plays, alleviating traffic congestion, transport efficiency, recurrent neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Traffic flow prediction plays a crucial role in alleviating traffic congestion and enhancing transport efficiency. While combining graph convolution networks with recurrent neural networks for spatial-temporal modeling is a common strategy in this realm, the restricted structure of recurrent neural networks limits their ability to capture global information. For spatial modeling, many prior studies learn a graph structure that is assumed to be fixed and uniform at all time steps, which may not be true. This paper introduces a novel traffic prediction framework, Global-Aware Enhanced Spatial-Temporal Graph Recurrent Network (GA-STGRN), comprising two core components: a spatial-temporal graph recurrent neural network and a global awareness layer. Within this framework, three innovative prediction models are formulated. A sequence-aware graph neural network is proposed and integrated into the Gated Recurrent Unit (GRU) to learn non-fixed graphs at different time steps and capture local temporal relationships. To enhance the model's global perception, three distinct global spatial-temporal transformer-like architectures (GST^2) are devised for the global awareness layer. We conduct extensive experiments on four real traffic datasets and the results demonstrate the superiority of our framework and the three concrete models.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：Web Neural Network with Complete DiGraphs</b></summary>
  <p><b>编号</b>：[205]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04134</p>
  <p><b>作者</b>：Frank Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：complete directed graph, processes continuous data, closely by structuring, complete directed, directed graph</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper introduces a new neural network model that aims to mimic the biological brain more closely by structuring the network as a complete directed graph that processes continuous data for each timestep. Current neural networks have structures that vaguely mimic the brain structure, such as neurons, convolutions, and recurrence. The model proposed in this paper adds additional structural properties by introducing cycles into the neuron connections and removing the sequential nature commonly seen in other network layers. Furthermore, the model has continuous input and output, inspired by spiking neural networks, which allows the network to learn a process of classification, rather than simply returning the final result.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：SynHIN: Generating Synthetic Heterogeneous Information Network for  Explainable AI</b></summary>
  <p><b>编号</b>：[206]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04133</p>
  <p><b>作者</b>：Ming-Yi Hong,  Yi-Hsiang Huang,  You-Chen Teng,  Chih-Yu Wang,  Che Lin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：detecting e-commerce spam, Graph, detecting e-commerce, e-commerce spam, spam to social</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Graph Neural Networks (GNNs) excel in various domains, from detecting e-commerce spam to social network classification problems. However, the lack of public graph datasets hampers research progress, particularly in heterogeneous information networks (HIN). The demand for datasets for fair HIN comparisons is growing due to advancements in GNN interpretation models. In response, we propose SynHIN, a unique method for generating synthetic heterogeneous information networks. SynHIN identifies motifs in real-world datasets, summarizes graph statistics, and constructs a synthetic network. Our approach utilizes In-Cluster and Out-Cluster Merge modules to build the synthetic HIN from primary motif clusters. After In/Our-Cluster mergers and a post-pruning process fitting the real dataset constraints, we ensure the synthetic graph statistics align closely with the reference one. SynHIN generates a synthetic heterogeneous graph dataset for node classification tasks, using the primary motif as the explanation ground truth. It can adapt and address the lack of heterogeneous graph datasets and motif ground truths, proving beneficial for assessing heterogeneous graph neural network explainers. We further present a benchmark dataset for future heterogeneous graph explainer model research. Our work marks a significant step towards explainable AI in HGNNs.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Unsupervised Test-Time Adaptation via Plug-and-Play Transformer Modules</b></summary>
  <p><b>编号</b>：[208]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04130</p>
  <p><b>作者</b>：Xiangyu Chang,  Sk Miraj Ahmed,  Basak Guler,  Srikanth V. Krishnamurthy,  Ananthram Swami,  Samet Oymak,  Amit K. Roy-Chowdhury</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Visual Prompt Tuning, Visual Prompt, Prompt Tuning, found success, success in enabling</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Parameter-efficient tuning (PET) methods such as LoRA, Adapter, and Visual Prompt Tuning (VPT) have found success in enabling adaptation to new domains by tuning small modules within a transformer model. However, the number of domains encountered during test time can be very large, and the data is usually unlabeled. Thus, adaptation to new domains is challenging; it is also impractical to generate customized tuned modules for each such domain. Toward addressing these challenges, this work introduces PLUTO: a Plug-and-pLay modUlar Test-time domain adaptatiOn strategy. We pre-train a large set of modules, each specialized for different source domains, effectively creating a ``module store''. Given a target domain with few-shot unlabeled data, we introduce an unsupervised test-time adaptation (TTA) method to (1) select a sparse subset of relevant modules from this store and (2) create a weighted combination of selected modules without tuning their weights. This plug-and-play nature enables us to harness multiple most-relevant source domains in a single inference call. Comprehensive evaluations demonstrate that PLUTO uniformly outperforms alternative TTA methods and that selecting $\leq$5 modules suffice to extract most of the benefit. At a high level, our method equips pre-trained transformers with the capability to dynamically adapt to new domains, motivating a new paradigm for efficient and scalable domain adaptation.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：The Concept of the Tactile Signature System for Individuals with Visual  Impairments</b></summary>
  <p><b>编号</b>：[209]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04126</p>
  <p><b>作者</b>：Anatoliy Kremenchutskiy,  Galymzhan Gabdreshov</p>
  <p><b>备注</b>：9 pages, 14 references</p>
  <p><b>关键词</b>：Tactile Signature System, handwritten signatures presents, create handwritten signatures, aspects of life, Tactile Signature</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The lack of an accessible and effective system for blind individuals to create handwritten signatures presents a significant barrier to their independence and full participation in various aspects of life. This research introduces the Tactile Signature System, a groundbreaking approach that empowers individuals with visual impairments to form their unique handwritten signatures. Key features of the system include: Personalized customization: Through tactile interaction and voice algorithmic guidance, individuals create signatures reflecting their preferences and natural writing style. Real-time feedback: AI-powered voice prompts and analysis ensure accuracy and consistency in signature formation. Accessibility: Installation in local service centers provides a secure and supervised environment for signature creation. The system's impact reaches beyond the individual level: Promotes inclusivity and independence: Blind individuals can engage in legal and financial transactions without relying on others. Empowers and fosters equal opportunities: Participation in education, employment, and civic engagement becomes more accessible. Aligns with international conventions: Upholds the right of persons with disabilities to participate fully in society. The Tactile Signature System represents a significant step towards an inclusive and accessible future for individuals with visual impairments.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：MobileAgent: enhancing mobile control via human-machine interaction and  SOP integration</b></summary>
  <p><b>编号</b>：[210]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04124</p>
  <p><b>作者</b>：Tinghe Ding</p>
  <p><b>备注</b>：agent, mobile control, SOP, human-machine interaction</p>
  <p><b>关键词</b>：Large Language Models, Large Language, centered around Large, Language Models, capable of automating</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Agents centered around Large Language Models (LLMs) are now capable of automating mobile device operations for users. After fine-tuning to learn a user's mobile operations, these agents can adhere to high-level user instructions online. They execute tasks such as goal decomposition, sequencing of sub-goals, and interactive environmental exploration, until the final objective is achieved. However, privacy concerns related to personalized user data arise during mobile operations, requiring user confirmation. Moreover, users' real-world operations are exploratory, with action data being complex and redundant, posing challenges for agent learning. To address these issues, in our practical application, we have designed interactive tasks between agents and humans to identify sensitive information and align with personalized user needs. Additionally, we integrated Standard Operating Procedure (SOP) information within the model's in-context learning to enhance the agent's comprehension of complex task execution. Our approach is evaluated on the new device control benchmark AitW, which encompasses 30K unique instructions across multi-step tasks, including application operation, web searching, and web shopping. Experimental results show that the SOP-based agent achieves state-of-the-art performance without incurring additional inference costs, boasting an overall action success rate of 66.92%.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：From Prompt Engineering to Prompt Science With Human in the Loop</b></summary>
  <p><b>编号</b>：[211]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04122</p>
  <p><b>作者</b>：Chirag Shah</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：warrants increased scrutiny, place that warrants, warrants increased, increased scrutiny, LLMs make</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As LLMs make their way into many aspects of our lives, one place that warrants increased scrutiny with LLM usage is scientific research. Using LLMs for generating or analyzing data for research purposes is gaining popularity. But when such application is marred with ad-hoc decisions and engineering solutions, we need to be concerned about how it may affect that research, its findings, or any future works based on that research. We need a more scientific approach to using LLMs in our research. While there are several active efforts to support more systematic construction of prompts, they are often focused more on achieving desirable outcomes rather than producing replicable and generalizable knowledge with sufficient transparency, objectivity, or rigor. This article presents a new methodology inspired by codebook construction through qualitative methods to address that. Using humans in the loop and a multi-phase verification processes, this methodology lays a foundation for more systematic, objective, and trustworthy way of applying LLMs for analyzing data. Specifically, we show how a set of researchers can work through a rigorous process of labeling, deliberating, and documenting to remove subjectivity and bring transparency and replicability to prompt generation process. A set of experiments are presented to show how this methodology can be put in practice.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：Generation Z's Ability to Discriminate Between AI-generated and  Human-Authored Text on Discord</b></summary>
  <p><b>编号</b>：[212]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04120</p>
  <p><b>作者</b>：Dhruv Ramu,  Rishab Jain,  Aditya Jain</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：growing popularity, popularity of generative, transformative effects, generative artificial intelligence, social media</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The growing popularity of generative artificial intelligence (AI) chatbots such as ChatGPT is having transformative effects on social media. As the prevalence of AI-generated content grows, concerns have been raised regarding privacy and misinformation online. Among social media platforms, Discord enables AI integrations -- making their primarily "Generation Z" userbase particularly exposed to AI-generated content. We surveyed Generation Z aged individuals (n = 335) to evaluate their proficiency in discriminating between AI-generated and human-authored text on Discord. The investigation employed one-shot prompting of ChatGPT, disguised as a text message received on the this http URL platform. We explore the influence of demographic factors on ability, as well as participants' familiarity with Discord and artificial intelligence technologies. We find that Generation Z individuals are unable to discern between AI and human-authored text (p = 0.011), and that those with lower self-reported familiarity with Discord demonstrated an improved ability in identifying human-authored compared to those with self-reported experience with AI (p << 0.0001). Our results suggest that there is a nuanced relationship between AI technology and popular modes of communication for Generation Z, contributing valuable insights into human-computer interactions, digital communication, and artificial intelligence literacy.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Why is the User Interface a Dark Pattern? : Explainable Auto-Detection  and its Analysis</b></summary>
  <p><b>编号</b>：[213]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04119</p>
  <p><b>作者</b>：Yuki Yada,  Tsuneo Matsumoto,  Fuyuko Kido,  Hayato Yamana</p>
  <p><b>备注</b>：IEEE International Conference on Big Data (IEEE BigData 2022)</p>
  <p><b>关键词</b>：make users behave, Dark patterns, Dark, designs for online, behave in unintended</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Dark patterns are deceptive user interface designs for online services that make users behave in unintended ways. Dark patterns, such as privacy invasion, financial loss, and emotional distress, can harm users. These issues have been the subject of considerable debate in recent years. In this paper, we study interpretable dark pattern auto-detection, that is, why a particular user interface is detected as having dark patterns. First, we trained a model using transformer-based pre-trained language models, BERT, on a text-based dataset for the automatic detection of dark patterns in e-commerce. Then, we applied post-hoc explanation techniques, including local interpretable model agnostic explanation (LIME) and Shapley additive explanations (SHAP), to the trained model, which revealed which terms influence each prediction as a dark pattern. In addition, we extracted and analyzed terms that affected the dark patterns. Our findings may prevent users from being manipulated by dark patterns, and aid in the construction of more equitable internet services. Our code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：Working with Trouble and Failures in Conversation between Humans and  Robots (WTF 2023) & Is CUI Design Ready Yet?</b></summary>
  <p><b>编号</b>：[221]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04108</p>
  <p><b>作者</b>：Frank Förster,  Marta Romeo,  Patrick Holthaus,  Maria Jose Galvez Trigo,  Joel E. Fischer,  Birthe Nesset,  Christian Dondrup,  Christine Murad,  Cosmin Munteanu,  Benjamin R. Cowan,  Leigh Clark,  Martin Porcheron,  Heloisa Candello,  Raina Langevin</p>
  <p><b>备注</b>：WTF 2023 & 'Is CUI Design Ready Yet?' workshop proceedings including 10 extended abstracts and articles</p>
  <p><b>关键词</b>：Humans and Robots, ACM conference, CUI Design Ready, conversational user interfaces, CUI Design</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Workshop proceedings of two co-located workshops "Working with Troubles and Failures in Conversation with Humans and Robots" (WTF 2023) and "Is CUI Design Ready Yet?", both of which were part of the ACM conference on conversational user interfaces 2023.
WTF 23 aimed at bringing together researchers from human-robot interaction, dialogue systems, human-computer interaction, and conversation analysis. Despite all progress, robotic speech interfaces continue to be brittle in a number of ways and the experience of failure of such interfaces is commonplace amongst roboticists. However, the technical literature is positively skewed toward their good performance. The workshop aims to provide a platform for discussing communicative troubles and failures in human-robot interactions and related failures in non-robotic speech interfaces. Aims include a scrupulous investigation into communicative failures, to begin working on a taxonomy of such failures, and enable a preliminary discussion on possible mitigating strategies. Workshop website: this https URL
Is CUI Design Ready Yet? As CUIs become more prevalent in both academic research and the commercial market, it becomes more essential to design usable and adoptable CUIs. While research has been growing on the methods for designing CUIs for commercial use, there has been little discussion on the overall community practice of developing design resources to aid in practical CUI design. The aim of this workshop, therefore, is to bring the CUI community together to discuss the current practices for developing tools and resources for practical CUI design, the adoption (or non-adoption) of these tools and resources, and how these resources are utilized in the training and education of new CUI designers entering the field. Workshop website: this https URL</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：A Deep Network for Explainable Prediction of Non-Imaging Phenotypes  using Anatomical Multi-View Data</b></summary>
  <p><b>编号</b>：[226]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04579</p>
  <p><b>作者</b>：Yuxiang Wei,  Yuqian Chen,  Tengfei Xue,  Leo Zekelman,  Nikos Makris,  Yogesh Rathi,  Weidong Cai,  Fan Zhang,  Lauren J. O' Donnell</p>
  <p><b>备注</b>：2023 The Medical Image Computing and Computer Assisted Intervention Society workshop</p>
  <p><b>关键词</b>：Large datasets, distinct feature sets, multiple distinct feature, sets, multiple feature sets</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large datasets often contain multiple distinct feature sets, or views, that offer complementary information that can be exploited by multi-view learning methods to improve results. We investigate anatomical multi-view data, where each brain anatomical structure is described with multiple feature sets. In particular, we focus on sets of white matter microstructure and connectivity features from diffusion MRI, as well as sets of gray matter area and thickness features from structural MRI. We investigate machine learning methodology that applies multi-view approaches to improve the prediction of non-imaging phenotypes, including demographics (age), motor (strength), and cognition (picture vocabulary). We present an explainable multi-view network (EMV-Net) that can use different anatomical views to improve prediction performance. In this network, each individual anatomical view is processed by a view-specific feature extractor and the extracted information from each view is fused using a learnable weight. This is followed by a wavelet transform-based module to obtain complementary information across views which is then applied to calibrate the view-specific information. Additionally, the calibrator produces an attention-based calibration score to indicate anatomical structures' importance for interpretation.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：TwinBooster: Synergising Large Language Models with Barlow Twins and  Gradient Boosting for Enhanced Molecular Property Prediction</b></summary>
  <p><b>编号</b>：[233]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04478</p>
  <p><b>作者</b>：Maximilian G. Schuh,  Davide Boldini,  Stephan A. Sieber</p>
  <p><b>备注</b>：20 pages, 4 figures, 10 tables</p>
  <p><b>关键词</b>：molecular activities, precise prediction, Barlow Twins, molecular, molecular property prediction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The success of drug discovery and development relies on the precise prediction of molecular activities and properties. While in silico molecular property prediction has shown remarkable potential, its use has been limited so far to assays for which large amounts of data are available. In this study, we use a fine-tuned large language model to integrate biological assays based on their textual information, coupled with Barlow Twins, a Siamese neural network using a novel self-supervised learning approach. This architecture uses both assay information and molecular fingerprints to extract the true molecular information. TwinBooster enables the prediction of properties of unseen bioassays and molecules by providing state-of-the-art zero-shot learning tasks. Remarkably, our artificial intelligence pipeline shows excellent performance on the FS-Mol benchmark. This breakthrough demonstrates the application of deep learning to critical property prediction tasks where data is typically scarce. By accelerating the early identification of active molecules in drug discovery and development, this method has the potential to help streamline the identification of novel therapeutics.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：DeepPhysiNet: Bridging Deep Learning and Atmospheric Physics for  Accurate and Continuous Weather Modeling</b></summary>
  <p><b>编号</b>：[252]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.04125</p>
  <p><b>作者</b>：Wenyuan Li,  Zili Liu,  Keyan Chen,  Hao Chen,  Shunlin Liang,  Zhengxia Zou,  Zhenwei Shi</p>
  <p><b>备注</b>：18 pages, 9 figures</p>
  <p><b>关键词</b>：holds significant importance, Numerical Weather Prediction, forecasting holds significant, Deep Learning-based Prediction, weather forecasting holds</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Accurate weather forecasting holds significant importance to human activities. Currently, there are two paradigms for weather forecasting: Numerical Weather Prediction (NWP) and Deep Learning-based Prediction (DLP). NWP utilizes atmospheric physics for weather modeling but suffers from poor data utilization and high computational costs, while DLP can learn weather patterns from vast amounts of data directly but struggles to incorporate physical laws. Both paradigms possess their respective strengths and weaknesses, and are incompatible, because physical laws adopted in NWP describe the relationship between coordinates and meteorological variables, while DLP directly learns the relationships between meteorological variables without consideration of coordinates. To address these problems, we introduce the DeepPhysiNet framework, incorporating physical laws into deep learning models for accurate and continuous weather system modeling. First, we construct physics networks based on multilayer perceptrons (MLPs) for individual meteorological variable, such as temperature, pressure, and wind speed. Physics networks establish relationships between variables and coordinates by taking coordinates as input and producing variable values as output. The physical laws in the form of Partial Differential Equations (PDEs) can be incorporated as a part of loss function. Next, we construct hyper-networks based on deep learning methods to directly learn weather patterns from a large amount of meteorological data. The output of hyper-networks constitutes a part of the weights for the physics networks. Experimental results demonstrate that, upon successful integration of physical laws, DeepPhysiNet can accomplish multiple tasks simultaneously, not only enhancing forecast accuracy but also obtaining continuous spatiotemporal resolution results, which is unattainable by either the NWP or DLP.</p>
  </details>
</details>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">徐耀彬</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://louishsu.xyz/2024/01/11/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">http://louishsu.xyz/2024/01/11/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://louishsu.xyz" target="_blank">LOUIS' BLOG</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2023/09/22/vLLM%EF%BC%9A%E5%88%A9%E7%94%A8%E5%88%86%E9%A1%B5%E7%BC%93%E5%AD%98%E5%92%8C%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E6%8F%90%E9%AB%98%E5%A4%A7%E6%A8%A1%E5%9E%8B2~4x%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6.html"><img class="next-cover" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww5.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">徐耀彬</div><div class="author-info__description">💭这个人很懒，什么都没有留下</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/isLouisHsu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/isLouisHsu" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:is.louishsu@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">记录和分享一些学习和开源内容，若有问题可通过邮箱is.louishsu@foxmail.com联系，欢迎交流！！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">统计</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">自然语言处理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">计算机视觉</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">机器学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">人工智能</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/01/11/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2024-01-11)"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv每日速递(2024-01-11)"/></a><div class="content"><a class="title" href="/2024/01/11/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2024-01-11)">Arxiv每日速递(2024-01-11)</a><time datetime="2024-01-11T00:36:57.870Z" title="发表于 2024-01-11 08:36:57">2024-01-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/09/22/vLLM%EF%BC%9A%E5%88%A9%E7%94%A8%E5%88%86%E9%A1%B5%E7%BC%93%E5%AD%98%E5%92%8C%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E6%8F%90%E9%AB%98%E5%A4%A7%E6%A8%A1%E5%9E%8B2~4x%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6.html" title="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww5.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度"/></a><div class="content"><a class="title" href="/2023/09/22/vLLM%EF%BC%9A%E5%88%A9%E7%94%A8%E5%88%86%E9%A1%B5%E7%BC%93%E5%AD%98%E5%92%8C%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E6%8F%90%E9%AB%98%E5%A4%A7%E6%A8%A1%E5%9E%8B2~4x%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6.html" title="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度">vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度</a><time datetime="2023-09-22T14:55:45.000Z" title="发表于 2023-09-22 22:55:45">2023-09-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/09/06/Prompt%EF%BC%9A%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%89%A7%E8%A1%8C%E6%8C%87%E5%8D%97.html" title="Prompt：大语言模型的执行指南"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww6.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Prompt：大语言模型的执行指南"/></a><div class="content"><a class="title" href="/2023/09/06/Prompt%EF%BC%9A%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%89%A7%E8%A1%8C%E6%8C%87%E5%8D%97.html" title="Prompt：大语言模型的执行指南">Prompt：大语言模型的执行指南</a><time datetime="2023-09-06T14:45:45.000Z" title="发表于 2023-09-06 22:45:45">2023-09-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/09/03/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9C%A81688%E7%94%B5%E5%95%86%E5%9C%BA%E6%99%AF%E7%9A%84%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5.html" title="【转载】大语言模型在1688电商场景的算法实践"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【转载】大语言模型在1688电商场景的算法实践"/></a><div class="content"><a class="title" href="/2023/09/03/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9C%A81688%E7%94%B5%E5%95%86%E5%9C%BA%E6%99%AF%E7%9A%84%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5.html" title="【转载】大语言模型在1688电商场景的算法实践">【转载】大语言模型在1688电商场景的算法实践</a><time datetime="2023-09-03T15:35:45.000Z" title="发表于 2023-09-03 23:35:45">2023-09-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/07/%E3%80%90%E6%A2%B3%E7%90%86%E3%80%91%E9%99%86%E5%A5%87%E6%9C%80%E6%96%B0%E6%BC%94%E8%AE%B2%E5%AE%9E%E5%BD%95%EF%BC%9A%E6%88%91%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%96%E7%95%8C%E8%A7%82%20.html" title="【梳理】陆奇最新演讲实录：我的大模型世界观"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【梳理】陆奇最新演讲实录：我的大模型世界观"/></a><div class="content"><a class="title" href="/2023/05/07/%E3%80%90%E6%A2%B3%E7%90%86%E3%80%91%E9%99%86%E5%A5%87%E6%9C%80%E6%96%B0%E6%BC%94%E8%AE%B2%E5%AE%9E%E5%BD%95%EF%BC%9A%E6%88%91%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%96%E7%95%8C%E8%A7%82%20.html" title="【梳理】陆奇最新演讲实录：我的大模型世界观">【梳理】陆奇最新演讲实录：我的大模型世界观</a><time datetime="2023-05-07T11:07:45.000Z" title="发表于 2023-05-07 19:07:45">2023-05-07</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By 徐耀彬</div><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender"title="主题采用butterfly"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr"title="本站使用JsDelivr为静态资源提供CDN加速"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub"title="本站项目由Gtihub托管"alt="img"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"alt="img"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></br></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'default',
      })
      false && mermaid.init()
    })
  }
}</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    let initData = {
      el: '#twikoo-wrap',
      envId: 'blog-',
      region: ''
    }

    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    twikoo.init(initData)
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'blog-',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/机器学习/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐱 机器学习 (3)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/自然语言处理/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 自然语言处理 (7)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/竞赛相关/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 竞赛相关 (3)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/阅读笔记/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 阅读笔记 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="http://louishsu.xyz/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt=""><img width="48" height="48" src="https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161037709574435991610377095138.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2021-05-19</span><a class="blog-slider__title" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt="">全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/cail2021.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2021-10-22</span><a class="blog-slider__title" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt="">中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt=""><img width="48" height="48" src="https://cdn.kesci.com/upload/image/r7j60un866.png?imageView2/2/w/2500/h/2500" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-11-17</span><a class="blog-slider__title" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt="">2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww5.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-09-22</span><a class="blog-slider__title" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt="">vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww6.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-09-06</span><a class="blog-slider__title" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt="">Prompt：大语言模型的执行指南</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/11/26/升级深度学习开发环境全攻略.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/default.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-11-26</span><a class="blog-slider__title" href="2022/11/26/升级深度学习开发环境全攻略.html" alt="">升级深度学习开发环境全攻略</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2022/11/26/升级深度学习开发环境全攻略.html" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>