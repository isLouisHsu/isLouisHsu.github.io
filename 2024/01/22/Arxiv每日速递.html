<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Arxiv每日速递(2024-01-22) | LOUIS' BLOG</title><meta name="author" content="徐耀彬"><meta name="copyright" content="徐耀彬"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以自然语言处理、计算机视觉、机器学习、人工智能等大方向进行划分。 统计 今日共更新365篇论文，其中：  49篇自然语言处理（cs.CL） 103篇计算机视觉（cs.CV） 116篇机器学习（cs.LG） 91篇人工智能（cs.AI）  自然语言处理    1. 标题：ChatQA: Building GPT-4 Level Conver">
<meta property="og:type" content="article">
<meta property="og:title" content="Arxiv每日速递(2024-01-22)">
<meta property="og:url" content="http://louishsu.xyz/2024/01/22/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">
<meta property="og:site_name" content="LOUIS&#39; BLOG">
<meta property="og:description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以自然语言处理、计算机视觉、机器学习、人工智能等大方向进行划分。 统计 今日共更新365篇论文，其中：  49篇自然语言处理（cs.CL） 103篇计算机视觉（cs.CV） 116篇机器学习（cs.LG） 91篇人工智能（cs.AI）  自然语言处理    1. 标题：ChatQA: Building GPT-4 Level Conver">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png">
<meta property="article:published_time" content="2024-01-22T00:40:44.003Z">
<meta property="article:modified_time" content="2024-01-22T00:42:27.671Z">
<meta property="article:author" content="徐耀彬">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://louishsu.xyz/2024/01/22/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2024-01-22 08:42:27'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/background.css"><script src="https://cdn.jsdelivr.net/npm/echarts@4.7.0/dist/echarts.min.js"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-brands fa-app-store"></i><span> 利器</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://code.visualstudio.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> VSCode：微软旗下的跨平台代码编辑软件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mobaxterm.mobatek.net/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> MobaXterm：超好用的全能远程终端</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.zotero.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zotero：便于收集、组织、引用、共享的文献管理工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/CopyTranslator/CopyTranslator"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> CopyTranslator：“复制即翻译”的外文辅助阅读翻译解决方案</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ditto-cp.sourceforge.io/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Ditto：强大的Windows剪贴板增强工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/indiff/qttabbar"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> QtTabBar：在Windows资源管理器中使用多标签功能扩展工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/sentialx/multrin"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Multrin：“窗口合并”辅助小工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/Wox-launcher/Wox"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Wox &amp; Everything：基于名称快速定位文件和文件夹的搜索工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/NickeManarin/ScreenToGif"><i class="fa-fw fa-regular fa-star"></i><span> ScreenToGif：快速录制屏幕指定区域并保存为动图文件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mathpix.com/"><i class="fa-fw fa-regular fa-star"></i><span> Mathpix Snipping：识别数学公式并转换成LaTeX</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">LOUIS' BLOG</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-brands fa-app-store"></i><span> 利器</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://code.visualstudio.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> VSCode：微软旗下的跨平台代码编辑软件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mobaxterm.mobatek.net/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> MobaXterm：超好用的全能远程终端</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.zotero.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zotero：便于收集、组织、引用、共享的文献管理工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/CopyTranslator/CopyTranslator"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> CopyTranslator：“复制即翻译”的外文辅助阅读翻译解决方案</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ditto-cp.sourceforge.io/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Ditto：强大的Windows剪贴板增强工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/indiff/qttabbar"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> QtTabBar：在Windows资源管理器中使用多标签功能扩展工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/sentialx/multrin"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Multrin：“窗口合并”辅助小工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/Wox-launcher/Wox"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Wox &amp; Everything：基于名称快速定位文件和文件夹的搜索工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/NickeManarin/ScreenToGif"><i class="fa-fw fa-regular fa-star"></i><span> ScreenToGif：快速录制屏幕指定区域并保存为动图文件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mathpix.com/"><i class="fa-fw fa-regular fa-star"></i><span> Mathpix Snipping：识别数学公式并转换成LaTeX</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Arxiv每日速递(2024-01-22)<a class="post-edit-link" href="https://github.com/isLouisHsu/blog/tree/master/source_posts/Arxiv每日速递.md" title="编辑" target="_blank"><i class="fas fa-pencil-square"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-01-22T00:40:44.003Z" title="发表于 2024-01-22 08:40:44">2024-01-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-01-22T00:42:27.671Z" title="更新于 2024-01-22 08:42:27">2024-01-22</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">阅读笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">89.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>535分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2024/01/22/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以自然语言处理、计算机视觉、机器学习、人工智能等大方向进行划分。</p>
<h1>统计</h1>
<p>今日共更新365篇论文，其中：</p>
<ul>
<li><a href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86">49篇自然语言处理（cs.CL）</a></li>
<li><a href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89">103篇计算机视觉（cs.CV）</a></li>
<li><a href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">116篇机器学习（cs.LG）</a></li>
<li><a href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD">91篇人工智能（cs.AI）</a></li>
</ul>
<h1>自然语言处理</h1>
<details>
  <summary>1. <b>标题：ChatQA: Building GPT-4 Level Conversational QA Models</b></summary>
  <p><b>编号</b>：[7]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10225</p>
  <p><b>作者</b>：Zihan Liu,  Wei Ping,  Rajarshi Roy,  Peng Xu,  Mohammad Shoeybi,  Bryan Catanzaro</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：conversational question answering, level accuracies, introduce ChatQA, question answering, conversational question</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：MM-Interleaved: Interleaved Image-Text Generative Modeling via  Multi-modal Feature Synchronizer</b></summary>
  <p><b>编号</b>：[19]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10208</p>
  <p><b>作者</b>：Changyao Tian,  Xizhou Zhu,  Yuwen Xiong,  Weiyun Wang,  Zhe Chen,  Wenhai Wang,  Yuntao Chen,  Lewei Lu,  Tong Lu,  Jie Zhou,  Hongsheng Li,  Yu Qiao,  Jifeng Dai</p>
  <p><b>备注</b>：20 pages, 9 figures, 17 tables</p>
  <p><b>关键词</b>：interleaved image-text data, Developing generative models, research and practical, interleaved image-text, Developing generative</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Developing generative models for interleaved image-text data has both research and practical value. It requires models to understand the interleaved sequences and subsequently generate images and text. However, existing attempts are limited by the issue that the fixed number of visual tokens cannot efficiently capture image details, which is particularly problematic in the multi-image scenarios. To address this, this paper presents MM-Interleaved, an end-to-end generative model for interleaved image-text data. It introduces a multi-scale and multi-image feature synchronizer module, allowing direct access to fine-grained image features in the previous context during the generation process. MM-Interleaved is end-to-end pre-trained on both paired and interleaved image-text corpora. It is further enhanced through a supervised fine-tuning phase, wherein the model improves its ability to follow complex multi-modal instructions. Experiments demonstrate the versatility of MM-Interleaved in recognizing visual details following multi-modal instructions and generating consistent images following both textual and visual conditions. Code and models are available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through  Text Reconstruction</b></summary>
  <p><b>编号</b>：[25]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10189</p>
  <p><b>作者</b>：Qingyun Wang,  Zixuan Zhang,  Hongxiang Li,  Xuan Liu,  Jiawei Han,  Heng Ji,  Huimin Zhao</p>
  <p><b>备注</b>：16 pages. Accepted by Findings of the Association for Computational Linguistics: EACL 2024. Code and resources are available at this https URL</p>
  <p><b>关键词</b>：entity extraction, entity, extraction, chemical domain faces, faces two unique</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Fine-grained few-shot entity extraction in the chemical domain faces two unique challenges. First, compared with entity extraction tasks in the general domain, sentences from chemical papers usually contain more entities. Moreover, entity extraction models usually have difficulty extracting entities of long-tailed types. In this paper, we propose Chem-FINESE, a novel sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to address these two challenges. Our Chem-FINESE has two components: a seq2seq entity extractor to extract named entities from the input sentence and a seq2seq self-validation module to reconstruct the original input sentence from extracted entities. Inspired by the fact that a good entity extraction system needs to extract entities faithfully, our new self-validation module leverages entity extraction results to reconstruct the original input sentence. Besides, we design a new contrastive loss to reduce excessive copying during the extraction process. Finally, we release ChemNER+, a new fine-grained chemical entity extraction dataset that is annotated by domain experts with the ChemNER schema. Experiments in few-shot settings with both ChemNER+ and CHEMET datasets show that our newly proposed framework has contributed up to 8.26% and 6.84% absolute F1-score gains respectively.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Beyond Reference-Based Metrics: Analyzing Behaviors of Open LLMs on  Data-to-Text Generation</b></summary>
  <p><b>编号</b>：[27]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10186</p>
  <p><b>作者</b>：Zdeněk Kasner,  Ondřej Dušek</p>
  <p><b>备注</b>：26 pages</p>
  <p><b>关键词</b>：large language models, extent open large, open large language, large language, structured data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We investigate to which extent open large language models (LLMs) can generate coherent and relevant text from structured data. To prevent bias from benchmarks leaked into LLM training data, we collect Quintd-1: an ad-hoc benchmark for five data-to-text (D2T) generation tasks, consisting of structured data records in standard formats gathered from public APIs. We leverage reference-free evaluation metrics and LLMs' in-context learning capabilities, allowing us to test the models with no human-written references. Our evaluation focuses on annotating semantic accuracy errors on token-level, combining human annotators and a metric based on GPT-4. Our systematic examination of the models' behavior across domains and tasks suggests that state-of-the-art open LLMs with 7B parameters can generate fluent and coherent text from various standard data formats in zero-shot settings. However, we also show that semantic accuracy of the outputs remains a major issue: on our benchmark, 80% of outputs of open LLMs contain a semantic error according to human annotators (91% according to GPT-4). Our code, data, and model outputs are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Spatial-Temporal Large Language Model for Traffic Prediction</b></summary>
  <p><b>编号</b>：[46]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10134</p>
  <p><b>作者</b>：Chenxi Liu,  Sun Yang,  Qianxiong Xu,  Zhishuai Li,  Cheng Long,  Ziyue Li,  Rui Zhao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：intelligent transportation systems, foresee future traffic, Large Language, transportation systems, endeavors to foresee</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Traffic prediction, a critical component for intelligent transportation systems, endeavors to foresee future traffic at specific locations using historical data. Although existing traffic prediction models often emphasize developing complex neural network structures, their accuracy has not seen improvements accordingly. Recently, Large Language Models (LLMs) have shown outstanding capabilities in time series analysis. Differing from existing models, LLMs progress mainly through parameter expansion and extensive pre-training while maintaining their fundamental structures. In this paper, we propose a Spatial-Temporal Large Language Model (ST-LLM) for traffic prediction. Specifically, ST-LLM redefines the timesteps at each location as tokens and incorporates a spatial-temporal embedding module to learn the spatial location and global temporal representations of tokens. Then these representations are fused to provide each token with unified spatial and temporal information. Furthermore, we propose a novel partially frozen attention strategy of the LLM, which is designed to capture spatial-temporal dependencies for traffic prediction. Comprehensive experiments on real traffic datasets offer evidence that ST-LLM outperforms state-of-the-art models. Notably, the ST-LLM also exhibits robust performance in both few-shot and zero-shot prediction scenarios.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Marrying Adapters and Mixup to Efficiently Enhance the Adversarial  Robustness of Pre-Trained Language Models for Text Classification</b></summary>
  <p><b>编号</b>：[52]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10111</p>
  <p><b>作者</b>：Tuc Nguyen,  Thai Le</p>
  <p><b>备注</b>：10 pages and 2 figures</p>
  <p><b>关键词</b>：Existing works show, Existing works, neural networks, enhance their generalizability, Pre-trained Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing works show that augmenting training data of neural networks using both clean and adversarial examples can enhance their generalizability under adversarial attacks. However, this training approach often leads to performance degradation on clean inputs. Additionally, it requires frequent re-training of the entire model to account for new attack types, resulting in significant and costly computations. Such limitations make adversarial training mechanisms less practical, particularly for complex Pre-trained Language Models (PLMs) with millions or even billions of parameters. To overcome these challenges while still harnessing the theoretical benefits of adversarial training, this study combines two concepts: (1) adapters, which enable parameter-efficient fine-tuning, and (2) Mixup, which train NNs via convex combinations of pairs data pairs. Intuitively, we propose to fine-tune PLMs through convex combinations of non-data pairs of fine-tuned adapters, one trained with clean and another trained with adversarial examples. Our experiments show that the proposed method achieves the best trade-off between training efficiency and predictive performance, both with and without attacks compared to other baselines on a variety of downstream tasks.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Power in Numbers: Robust reading comprehension by finetuning with four  adversarial sentences per example</b></summary>
  <p><b>编号</b>：[56]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10091</p>
  <p><b>作者</b>：Ariel Marcus</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Stanford Question Answering, Question Answering Dataset, Stanford Question, Question Answering, achieved human level</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent models have achieved human level performance on the Stanford Question Answering Dataset when using F1 scores to evaluate the reading comprehension task. Yet, teaching machines to comprehend text has not been solved in the general case. By appending one adversarial sentence to the context paragraph, past research has shown that the F1 scores from reading comprehension models drop almost in half. In this paper, I replicate past adversarial research with a new model, ELECTRA-Small, and demonstrate that the new model's F1 score drops from 83.9% to 29.2%. To improve ELECTRA-Small's resistance to this attack, I finetune the model on SQuAD v1.1 training examples with one to five adversarial sentences appended to the context paragraph. Like past research, I find that the finetuned model on one adversarial sentence does not generalize well across evaluation datasets. However, when finetuned on four or five adversarial sentences the model attains an F1 score of more than 70% on most evaluation datasets with multiple appended and prepended adversarial sentences. The results suggest that with enough examples we can make models robust to adversarial attacks.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Communication-Efficient Personalized Federated Learning for  Speech-to-Text Tasks</b></summary>
  <p><b>编号</b>：[63]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10070</p>
  <p><b>作者</b>：Yichao Du,  Zhirui Zhang,  Linan Yue,  Xu Huang,  Yuqing Zhang,  Tong Xu,  Linli Xu,  Enhong Chen</p>
  <p><b>备注</b>：ICASSP 2024</p>
  <p><b>关键词</b>：automatic speech recognition, including automatic speech, meet legal regulations, gained significant attention, overcome data heterogeneity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To protect privacy and meet legal regulations, federated learning (FL) has gained significant attention for training speech-to-text (S2T) systems, including automatic speech recognition (ASR) and speech translation (ST). However, the commonly used FL approach (i.e., \textsc{FedAvg}) in S2T tasks typically suffers from extensive communication overhead due to multi-round interactions based on the whole model and performance degradation caused by data heterogeneity among this http URL address these issues, we propose a personalized federated S2T framework that introduces \textsc{FedLoRA}, a lightweight LoRA module for client-side tuning and interaction with the server to minimize communication overhead, and \textsc{FedMem}, a global model equipped with a $k$-nearest-neighbor ($k$NN) classifier that captures client-specific distributional shifts to achieve personalization and overcome data heterogeneity. Extensive experiments based on Conformer and Whisper backbone models on CoVoST and GigaSpeech benchmarks show that our approach significantly reduces the communication overhead on all S2T tasks and effectively personalizes the global model to overcome data heterogeneity.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs</b></summary>
  <p><b>编号</b>：[65]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10065</p>
  <p><b>作者</b>：Haritz Puerto,  Martin Tutek,  Somak Aditya,  Xiaodan Zhu,  Iryna Gurevych</p>
  <p><b>备注</b>：Code, prompt templates, prompts, and outputs are publicly available at this https URL</p>
  <p><b>关键词</b>：Reasoning, code, fundamental component, component for achieving, prompts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reasoning is a fundamental component for achieving language understanding. Among the multiple types of reasoning, conditional reasoning, the ability to draw different conclusions depending on some condition, has been understudied in large language models (LLMs). Recent prompting methods, such as chain of thought, have significantly improved LLMs on reasoning tasks. Nevertheless, there is still little understanding of what triggers reasoning abilities in LLMs. We hypothesize that code prompts can trigger conditional reasoning in LLMs trained on text and code. We propose a chain of prompts that transforms a natural language problem into code and prompts the LLM with the generated code. Our experiments find that code prompts exhibit a performance boost between 2.6 and 7.7 points on GPT 3.5 across multiple datasets requiring conditional reasoning. We then conduct experiments to discover how code prompts elicit conditional reasoning abilities and through which features. We observe that prompts need to contain natural language text accompanied by high-quality code that closely represents the semantics of the instance text. Furthermore, we show that code prompts are more efficient, requiring fewer demonstrations, and that they trigger superior state tracking of variables or key entities.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Antonym vs Synonym Distinction using InterlaCed Encoder NETworks  (ICE-NET)</b></summary>
  <p><b>编号</b>：[69]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10045</p>
  <p><b>作者</b>：Muhammad Asif Ali,  Yan Hu,  Jianbin Qin,  Di Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：lexical resource construction, automated lexical resource, resource construction, core challenge, challenge in lexico-semantic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Antonyms vs synonyms distinction is a core challenge in lexico-semantic analysis and automated lexical resource construction. These pairs share a similar distributional context which makes it harder to distinguish them. Leading research in this regard attempts to capture the properties of the relation pairs, i.e., symmetry, transitivity, and trans-transitivity. However, the inability of existing research to appropriately model the relation-specific properties limits their end performance. In this paper, we propose InterlaCed Encoder NETworks (i.e., ICE-NET) for antonym vs synonym distinction, that aim to capture and model the relation-specific properties of the antonyms and synonyms pairs in order to perform the classification task in a performance-enhanced manner. Experimental evaluation using the benchmark datasets shows that ICE-NET outperforms the existing research by a relative score of upto 1.8% in F1-measure. We release the codes for ICE-NET at this https URL.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Large Language Models for Scientific Information Extraction: An  Empirical Study for Virology</b></summary>
  <p><b>编号</b>：[73]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10040</p>
  <p><b>作者</b>：Mahsa Shamsabadi,  Jennifer D'Souza,  Sören Auer</p>
  <p><b>备注</b>：8 pages, 6 figures, Accepted as Findings of the ACL: EACL 2024</p>
  <p><b>关键词</b>：Amazon product descriptions, structured Amazon product, semantic content representation, discourse-based scholarly communication, tools like Wikipedia</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we champion the use of structured and semantic content representation of discourse-based scholarly communication, inspired by tools like Wikipedia infoboxes or structured Amazon product descriptions. These representations provide users with a concise overview, aiding scientists in navigating the dense academic landscape. Our novel automated approach leverages the robust text generation capabilities of LLMs to produce structured scholarly contribution summaries, offering both a practical solution and insights into LLMs' emergent abilities.
For LLMs, the prime focus is on improving their general intelligence as conversational agents. We argue that these models can also be applied effectively in information extraction (IE), specifically in complex IE tasks within terse domains like Science. This paradigm shift replaces the traditional modular, pipelined machine learning approach with a simpler objective expressed through instructions. Our results show that finetuned FLAN-T5 with 1000x fewer parameters than the state-of-the-art GPT-davinci is competitive for the task.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Evolutionary Computation in the Era of Large Language Model: Survey and  Roadmap</b></summary>
  <p><b>编号</b>：[77]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10034</p>
  <p><b>作者</b>：Xingyu Wu,  Sheng-hao Wu,  Jibin Wu,  Liang Feng,  Kay Chen Tan</p>
  <p><b>备注</b>：evolutionary algorithm (EA), large language model (LLM), optimization problem, prompt optimization, architecture search, code generation</p>
  <p><b>关键词</b>：Large Language Models, revolutionized natural language, Language Models, artificial general intelligence, natural language processing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs), built upon Transformer-based architectures with massive pretraining on diverse data, have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence. The interplay between LLMs and Evolutionary Algorithms (EAs), despite differing in objectives and methodologies, reveals intriguing parallels, especially in their shared optimization nature, black-box characteristics, and proficiency in handling complex problems. Meanwhile, EA can not only provide an optimization framework for LLM's further enhancement under black-box settings but also empower LLM with flexible global search and iterative mechanism in applications. On the other hand, LLM's abundant domain knowledge enables EA to perform smarter searches, while its text processing capability assist in deploying EA across various tasks. Based on their complementary advantages, this paper presents a comprehensive review and forward-looking roadmap, categorizing their mutual inspiration into LLM-enhanced evolutionary optimization and EA-enhanced LLM. Some integrated synergy methods are further introduced to exemplify the amalgamation of LLMs and EAs in various application scenarios, including neural architecture search, code generation, software engineering, and text generation. As the first comprehensive review specifically focused on the EA research in the era of LLMs, this paper provides a foundational stepping stone for understanding and harnessing the collaborative potential of LLMs and EAs. By presenting a comprehensive review, categorization, and critical analysis, we contribute to the ongoing discourse on the cross-disciplinary study of these two powerful paradigms. The identified challenges and future directions offer guidance to unlock the full potential of this innovative collaboration.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Framing Analysis of Health-Related Narratives: Conspiracy versus  Mainstream Media</b></summary>
  <p><b>编号</b>：[78]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10030</p>
  <p><b>作者</b>：Markus Reiter-Haas,  Beate Klösch,  Markus Hadler,  Elisabeth Lex</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Understanding how online, public opinion, crucial due, impact on public, framing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Understanding how online media frame issues is crucial due to their impact on public opinion. Research on framing using natural language processing techniques mainly focuses on specific content features in messages and neglects their narrative elements. Also, the distinction between framing in different sources remains an understudied problem. We address those issues and investigate how the framing of health-related topics, such as COVID-19 and other diseases, differs between conspiracy and mainstream websites. We incorporate narrative information into the framing analysis by introducing a novel frame extraction approach based on semantic graphs. We find that health-related narratives in conspiracy media are predominantly framed in terms of beliefs, while mainstream media tend to present them in terms of science. We hope our work offers new ways for a more nuanced frame analysis.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Self-Rewarding Language Models</b></summary>
  <p><b>编号</b>：[80]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10020</p>
  <p><b>作者</b>：Weizhe Yuan,  Richard Yuanzhe Pang,  Kyunghyun Cho,  Sainbayar Sukhbaatar,  Jing Xu,  Jason Weston</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：achieve superhuman agents, require superhuman feedback, future models require, models require superhuman, adequate training signal</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While only a preliminary study, this work opens the door to the possibility of models that can continually improve in both axes.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：R-Judge: Benchmarking Safety Risk Awareness for LLM Agents</b></summary>
  <p><b>编号</b>：[81]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10019</p>
  <p><b>作者</b>：Tongxin Yuan,  Zhiwei He,  Lingzhong Dong,  Yiming Wang,  Ruijie Zhao,  Tian Xia,  Lizhen Xu,  Binglin Zhou,  Fangqi Li,  Zhuosheng Zhang,  Rui Wang,  Gongshen Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：exhibited great potential, autonomously completing tasks, Large language models, Large language, safety</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have exhibited great potential in autonomously completing tasks across real-world applications. Despite this, these LLM agents introduce unexpected safety risks when operating in interactive environments. Instead of centering on LLM-generated content safety in most prior studies, this work addresses the imperative need for benchmarking the behavioral safety of LLM agents within diverse environments. We introduce R-Judge, a benchmark crafted to evaluate the proficiency of LLMs in judging safety risks given agent interaction records. R-Judge comprises 162 agent interaction records, encompassing 27 key risk scenarios among 7 application categories and 10 risk types. It incorporates human consensus on safety with annotated safety risk labels and high-quality risk descriptions. Utilizing R-Judge, we conduct a comprehensive evaluation of 8 prominent LLMs commonly employed as the backbone for agents. The best-performing model, GPT-4, achieves 72.29% in contrast to the human score of 89.38%, showing considerable room for enhancing the risk awareness of LLMs. Notably, leveraging risk descriptions as environment feedback significantly improves model performance, revealing the importance of salient safety risk feedback. Furthermore, we design an effective chain of safety analysis technique to help the judgment of safety risks and conduct an in-depth case study to facilitate future research. R-Judge is publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Gender Bias in Machine Translation and The Era of Large Language Models</b></summary>
  <p><b>编号</b>：[83]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10016</p>
  <p><b>作者</b>：Eva Vanmassenhove</p>
  <p><b>备注</b>：24 pages</p>
  <p><b>关键词</b>：Neural Machine Translation, Machine Translation systems, Machine Translation, Generative Pretrained Transformer, Machine Translation approaches</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This chapter examines the role of Machine Translation in perpetuating gender bias, highlighting the challenges posed by cross-linguistic settings and statistical dependencies. A comprehensive overview of relevant existing work related to gender bias in both conventional Neural Machine Translation approaches and Generative Pretrained Transformer models employed as Machine Translation systems is provided. Through an experiment using ChatGPT (based on GPT-3.5) in an English-Italian translation context, we further assess ChatGPT's current capacity to address gender bias. The findings emphasize the ongoing need for advancements in mitigating bias in Machine Translation systems and underscore the importance of fostering fairness and inclusivity in language technologies.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Towards Hierarchical Spoken Language Dysfluency Modeling</b></summary>
  <p><b>编号</b>：[84]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10015</p>
  <p><b>作者</b>：Jiachen Lian,  Gopala Anumanchipalli</p>
  <p><b>备注</b>：2024 EACL Long (main conference). arXiv admin note: substantial text overlap with arXiv:2312.12810</p>
  <p><b>关键词</b>：language learning, therapy and language, Hierarchical Unconstrained Dysfluency, Unconstrained Dysfluency Modeling, dysfluency modeling</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Speech dysfluency modeling is the bottleneck for both speech therapy and language learning. However, there is no AI solution to systematically tackle this problem. We first propose to define the concept of dysfluent speech and dysfluent speech modeling. We then present Hierarchical Unconstrained Dysfluency Modeling (H-UDM) approach that addresses both dysfluency transcription and detection to eliminate the need for extensive manual annotation. Furthermore, we introduce a simulated dysfluent dataset called VCTK++ to enhance the capabilities of H-UDM in phonetic transcription. Our experimental results demonstrate the effectiveness and robustness of our proposed methods in both transcription and detection tasks.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and  Visual Question Generation</b></summary>
  <p><b>编号</b>：[89]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10005</p>
  <p><b>作者</b>：Kohei Uehara,  Nabarun Goswami,  Hanqin Wang,  Toshiaki Baba,  Kohtaro Tanaka,  Tomohiro Hashimoto,  Kai Wang,  Rei Ito,  Takagi Naoya,  Ryo Umagami,  Yingyi Wen,  Tanachai Anakewat,  Tatsuya Harada</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Multi-Modal Models, visual content requires, Large Language Model, increasing demand, demand for intelligent</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The increasing demand for intelligent systems capable of interpreting and reasoning about visual content requires the development of Large Multi-Modal Models (LMMs) that are not only accurate but also have explicit reasoning capabilities. This paper presents a novel approach to imbue an LMM with the ability to conduct explicit reasoning based on visual content and textual instructions. We introduce a system that can ask a question to acquire necessary knowledge, thereby enhancing the robustness and explicability of the reasoning process. Our method comprises the development of a novel dataset generated by a Large Language Model (LLM), designed to promote chain-of-thought reasoning combined with a question-asking mechanism. We designed an LMM, which has high capabilities on region awareness to address the intricate requirements of image-text alignment. The model undergoes a three-stage training phase, starting with large-scale image-text alignment using a large-scale datasets, followed by instruction tuning, and fine-tuning with a focus on chain-of-thought reasoning. The results demonstrate a stride toward a more robust, accurate, and interpretable LMM, capable of reasoning explicitly and seeking information proactively when confronted with ambiguous visual input.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Distantly Supervised Morpho-Syntactic Model for Relation Extraction</b></summary>
  <p><b>编号</b>：[90]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10002</p>
  <p><b>作者</b>：Nicolas Gutehrlé,  Iana Atanassova</p>
  <p><b>备注</b>：Preprint</p>
  <p><b>关键词</b>：involves automatically converting, automatically converting unstructured, converting unstructured textual, unstructured textual content, involves automatically</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The task of Information Extraction (IE) involves automatically converting unstructured textual content into structured data. Most research in this field concentrates on extracting all facts or a specific set of relationships from documents. In this paper, we present a method for the extraction and categorisation of an unrestricted set of relationships from text. Our method relies on morpho-syntactic extraction patterns obtained by a distant supervision method, and creates Syntactic and Semantic Indices to extract and classify candidate graphs. We evaluate our approach on six datasets built on Wikidata and Wikipedia. The evaluation shows that our approach can achieve Precision scores of up to 0.85, but with lower Recall and F1 scores. Our approach allows to quickly create rule-based systems for Information Extraction and to build annotated datasets to train machine-learning and deep-learning based classifiers.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Gradable ChatGPT Translation Evaluation</b></summary>
  <p><b>编号</b>：[97]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09984</p>
  <p><b>作者</b>：Hui Jiao,  Bei Peng,  Lu Zong,  Xiaojun Zhang,  Xinwei Li</p>
  <p><b>备注</b>：Under review in the journal Procesamiento del Lenguaje Natural</p>
  <p><b>关键词</b>：language model based, large-scale pre-training, based on large-scale, exerted a profound, domain of machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>ChatGPT, as a language model based on large-scale pre-training, has exerted a profound influence on the domain of machine translation. In ChatGPT, a "Prompt" refers to a segment of text or instruction employed to steer the model towards generating a specific category of response. The design of the translation prompt emerges as a key aspect that can wield influence over factors such as the style, precision and accuracy of the translation to a certain extent. However, there is a lack of a common standard and methodology on how to design and select a translation prompt. Accordingly, this paper proposes a generic taxonomy, which defines gradable translation prompts in terms of expression type, translation style, POS information and explicit statement, thus facilitating the construction of prompts endowed with distinct attributes tailored for various translation tasks. Specific experiments and cases are selected to validate and illustrate the effectiveness of the method.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Better Explain Transformers by Illuminating Important Information</b></summary>
  <p><b>编号</b>：[101]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09972</p>
  <p><b>作者</b>：Linxin Song,  Yan Cui,  Ao Luo,  Freddy Lecue,  Irene Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Transformer-based models excel, natural language processing, attracting countless efforts, Transformer-based models, language processing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transformer-based models excel in various natural language processing (NLP) tasks, attracting countless efforts to explain their inner workings. Prior methods explain Transformers by focusing on the raw gradient and attention as token attribution scores, where non-relevant information is often considered during explanation computation, resulting in confusing results. In this work, we propose highlighting the important information and eliminating irrelevant information by a refined information flow on top of the layer-wise relevance propagation (LRP) method. Specifically, we consider identifying syntactic and positional heads as important attention heads and focus on the relevance obtained from these important heads. Experimental results demonstrate that irrelevant information does distort output attribution scores and then should be masked during explanation computation. Compared to eight baselines on both classification and question-answering datasets, our method consistently outperforms with over 3\% to 33\% improvement on explanation metrics, providing superior explanation performance. Our anonymous code repository is available at: this https URL</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Sketch-Guided Constrained Decoding for Boosting Blackbox Large Language  Models without Logit Access</b></summary>
  <p><b>编号</b>：[102]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09967</p>
  <p><b>作者</b>：Saibo Geng,  Berkay Döner,  Chris Wendler,  Martin Josifoski,  Robert West</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：control text generation, architectural modifications, Constrained decoding, enforcing constraints, control text</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Constrained decoding, a technique for enforcing constraints on language model outputs, offers a way to control text generation without retraining or architectural modifications. Its application is, however, typically restricted to models that give users access to next-token distributions (usually via softmax logits), which poses a limitation with blackbox large language models (LLMs). This paper introduces sketch-guided constrained decoding (SGCD), a novel approach to constrained decoding for blackbox LLMs, which operates without access to the logits of the blackbox LLM. SGCD utilizes a locally hosted auxiliary model to refine the output of an unconstrained blackbox LLM, effectively treating this initial output as a "sketch" for further elaboration. This approach is complementary to traditional logit-based techniques and enables the application of constrained decoding in settings where full model transparency is unavailable. We demonstrate the efficacy of SGCD through experiments in closed information extraction and constituency parsing, showing how it enhances the utility and flexibility of blackbox LLMs for complex NLP tasks.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Meme-ingful Analysis: Enhanced Understanding of Cyberbullying in Memes  Through Multimodal Explanations</b></summary>
  <p><b>编号</b>：[126]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09899</p>
  <p><b>作者</b>：Prince Jha,  Krishanu Maity,  Raghav Jain,  Apoorv Verma,  Sriparna Saha,  Pushpak Bhattacharyya</p>
  <p><b>备注</b>：EACL2024</p>
  <p><b>关键词</b>：gained significant influence, communicating political, sociocultural ideas, gained significant, significant influence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Internet memes have gained significant influence in communicating political, psychological, and sociocultural ideas. While memes are often humorous, there has been a rise in the use of memes for trolling and cyberbullying. Although a wide variety of effective deep learning-based models have been developed for detecting offensive multimodal memes, only a few works have been done on explainability aspect. Recent laws like "right to explanations" of General Data Protection Regulation, have spurred research in developing interpretable models rather than only focusing on performance. Motivated by this, we introduce {\em MultiBully-Ex}, the first benchmark dataset for multimodal explanation from code-mixed cyberbullying memes. Here, both visual and textual modalities are highlighted to explain why a given meme is cyberbullying. A Contrastive Language-Image Pretraining (CLIP) projection-based multimodal shared-private multitask approach has been proposed for visual and textual explanation of a meme. Experimental results demonstrate that training with multimodal explanations improves performance in generating textual justifications and more accurately identifying the visual evidence supporting a decision with reliable performance improvements.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：A Survey on Hardware Accelerators for Large Language Models</b></summary>
  <p><b>编号</b>：[129]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09890</p>
  <p><b>作者</b>：Christoforos Kachris</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, generate human-like text, language processing tasks, natural language processing, Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) have emerged as powerful tools for natural language processing tasks, revolutionizing the field with their ability to understand and generate human-like text. As the demand for more sophisticated LLMs continues to grow, there is a pressing need to address the computational challenges associated with their scale and complexity. This paper presents a comprehensive survey on hardware accelerators designed to enhance the performance and energy efficiency of Large Language Models. By examining a diverse range of accelerators, including GPUs, FPGAs, and custom-designed architectures, we explore the landscape of hardware solutions tailored to meet the unique computational demands of LLMs. The survey encompasses an in-depth analysis of architecture, performance metrics, and energy efficiency considerations, providing valuable insights for researchers, engineers, and decision-makers aiming to optimize the deployment of LLMs in real-world applications.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Attention-Based Recurrent Neural Network For Automatic Behavior Laying  Hen Recognition</b></summary>
  <p><b>编号</b>：[134]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09880</p>
  <p><b>作者</b>：Fréjus A. A. Laleye,  Mikaël A. Mousse</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：modern poultry farming, laying hens, laying hen behavior, frequency domain features, laying hen call</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>One of the interests of modern poultry farming is the vocalization of laying hens which contain very useful information on health behavior. This information is used as health and well-being indicators that help breeders better monitor laying hens, which involves early detection of problems for rapid and more effective intervention. In this work, we focus on the sound analysis for the recognition of the types of calls of the laying hens in order to propose a robust system of characterization of their behavior for a better monitoring. To do this, we first collected and annotated laying hen call signals, then designed an optimal acoustic characterization based on the combination of time and frequency domain features. We then used these features to build the multi-label classification models based on recurrent neural network to assign a semantic class to the vocalization that characterize the laying hen behavior. The results show an overall performance with our model based on the combination of time and frequency domain features that obtained the highest F1-score (F1=92.75) with a gain of 17% on the models using the frequency domain features and of 8% on the compared approaches from the litterature.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Evolutionary Multi-Objective Optimization of Large Language Model  Prompts for Balancing Sentiments</b></summary>
  <p><b>编号</b>：[140]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09862</p>
  <p><b>作者</b>：Jill Baumann,  Oliver Kramer</p>
  <p><b>备注</b>：Accepted in EvoApps at EvoStar 2024</p>
  <p><b>关键词</b>：attracted considerable attention, large language models, advent of large, large language, ChatGPT has attracted</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The advent of large language models (LLMs) such as ChatGPT has attracted considerable attention in various domains due to their remarkable performance and versatility. As the use of these models continues to grow, the importance of effective prompt engineering has come to the fore. Prompt optimization emerges as a crucial challenge, as it has a direct impact on model performance and the extraction of relevant information. Recently, evolutionary algorithms (EAs) have shown promise in addressing this issue, paving the way for novel optimization strategies. In this work, we propose a evolutionary multi-objective (EMO) approach specifically tailored for prompt optimization called EMO-Prompts, using sentiment analysis as a case study. We use sentiment analysis capabilities as our experimental targets. Our results demonstrate that EMO-Prompts effectively generates prompts capable of guiding the LLM to produce texts embodying two conflicting emotions simultaneously.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：MatSciRE: Leveraging Pointer Networks to Automate Entity and Relation  Extraction for Material Science Knowledge-base Construction</b></summary>
  <p><b>编号</b>：[150]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09839</p>
  <p><b>作者</b>：Ankan Mullick,  Akash Ghosh,  G Sai Chaitanya,  Samir Ghui,  Tapas Nayak,  Seung-Cheol Lee,  Satadeep Bhattacharjee,  Pawan Goyal</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Material science, Material science literature, Science Relation Extractor, rich source, source of factual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Material science literature is a rich source of factual information about various categories of entities (like materials and compositions) and various relations between these entities, such as conductivity, voltage, etc. Automatically extracting this information to generate a material science knowledge base is a challenging task. In this paper, we propose MatSciRE (Material Science Relation Extractor), a Pointer Network-based encoder-decoder framework, to jointly extract entities and relations from material science articles as a triplet ($entity1, relation, entity2$). Specifically, we target the battery materials and identify five relations to work on - conductivity, coulombic efficiency, capacity, voltage, and energy. Our proposed approach achieved a much better F1-score (0.771) than a previous attempt using ChemDataExtractor (0.716). The overall graphical framework of MatSciRE is shown in Fig 1. The material information is extracted from material science literature in the form of entity-relation triplets using MatSciRE.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Simple and effective data augmentation for compositional generalization</b></summary>
  <p><b>编号</b>：[160]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09815</p>
  <p><b>作者</b>：Yuekun Yao,  Alexander Koller</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：predict complex meanings, Compositional generalization, simpler sentences, poses challenges, powerful pretrained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Compositional generalization, the ability to predict complex meanings from training on simpler sentences, poses challenges for powerful pretrained seq2seq models. In this paper, we show that data augmentation methods that sample MRs and backtranslate them can be effective for compositional generalization, but only if we sample from the right distribution. Remarkably, sampling from a uniform distribution performs almost as well as sampling from the test distribution, and greatly outperforms earlier methods that sampled from the training distribution. We further conduct experiments to investigate the reason why this happens and where the benefit of such data augmentation methods come from.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks</b></summary>
  <p><b>编号</b>：[164]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09798</p>
  <p><b>作者</b>：Kazuhiro Takemoto</p>
  <p><b>备注</b>：11 pages, 3 figures, 2 tables</p>
  <p><b>关键词</b>：Large Language Models, Large Language, produce ethically harmful, Language Models, ethically harmful prompts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) like ChatGPT face `jailbreak' challenges, where safeguards are bypassed to produce ethically harmful prompts. This study introduces a simple black-box method to effectively generate jailbreak prompts, overcoming the limitations of high complexity and computational costs associated with existing methods. The proposed technique iteratively rewrites harmful prompts into non-harmful expressions using the target LLM itself, based on the hypothesis that LLMs can directly sample safeguard-bypassing expressions. Demonstrated through experiments with ChatGPT (GPT-3.5 and GPT-4) and Gemini-Pro, this method achieved an attack success rate of over 80% within an average of 5 iterations and remained effective despite model updates. The jailbreak prompts generated were naturally-worded and concise, suggesting they are less detectable. The results indicate that creating effective jailbreak prompts is simpler than previously considered, and black-box jailbreak attacks pose a more serious security threat.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Instant Answering in E-Commerce Buyer-Seller Messaging</b></summary>
  <p><b>编号</b>：[172]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09785</p>
  <p><b>作者</b>：Besnik Fetahu,  Tejas Mehta,  Qun Song,  Nikhita Vedula,  Oleg Rokhlenko,  Shervin Malmasi</p>
  <p><b>备注</b>：Accepted at ECIR 2024</p>
  <p><b>关键词</b>：commonly contacting sellers, contacting sellers directly, commonly contacting, directly with extended, frequently seek detailed</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>E-commerce customers frequently seek detailed product information for purchase decisions, commonly contacting sellers directly with extended queries. This manual response requirement imposes additional costs and disrupts buyer's shopping experience with response time fluctuations ranging from hours to days. We seek to automate buyer inquiries to sellers in a leading e-commerce store using a domain-specific federated Question Answering (QA) system. The main challenge is adapting current QA systems, designed for single questions, to address detailed customer queries. We address this with a low-latency, sequence-to-sequence approach, MESSAGE-TO-QUESTION ( M2Q ). It reformulates buyer messages into succinct questions by identifying and extracting the most salient information from a message. Evaluation against baselines shows that M2Q yields relative increases of 757% in question understanding, and 1,746% in answering rate from the federated QA system. Live deployment shows that automatic answering saves sellers from manually responding to millions of messages per year, and also accelerates customer purchase decisions by eliminating the need for buyers to wait for a reply</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Leveraging Biases in Large Language Models: "bias-kNN'' for Effective  Few-Shot Learning</b></summary>
  <p><b>编号</b>：[173]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09783</p>
  <p><b>作者</b>：Yong Zhang,  Hanzhang Li,  Zhitao Li,  Ning Cheng,  Ming Li,  Jing Xiao,  Jianzong Wang</p>
  <p><b>备注</b>：Accepted by the 49th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2024)</p>
  <p><b>关键词</b>：Large Language Models, shown significant promise, Large Language, including zero-shot, shown significant</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) have shown significant promise in various applications, including zero-shot and few-shot learning. However, their performance can be hampered by inherent biases. Instead of traditionally sought methods that aim to minimize or correct these biases, this study introduces a novel methodology named ``bias-kNN''. This approach capitalizes on the biased outputs, harnessing them as primary features for kNN and supplementing with gold labels. Our comprehensive evaluations, spanning diverse domain text classification datasets and different GPT-2 model sizes, indicate the adaptability and efficacy of the ``bias-kNN'' method. Remarkably, this approach not only outperforms conventional in-context learning in few-shot scenarios but also demonstrates robustness across a spectrum of samples, templates and verbalizers. This study, therefore, presents a unique perspective on harnessing biases, transforming them into assets for enhanced model performance.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Controllable Decontextualization of Yes/No Question and Answers into  Factual Statements</b></summary>
  <p><b>编号</b>：[174]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09775</p>
  <p><b>作者</b>：Lingbo Mo,  Besnik Fetahu,  Oleg Rokhlenko,  Shervin Malmasi</p>
  <p><b>备注</b>：Accepted at ECIR 2024</p>
  <p><b>关键词</b>：linguistic question categories, main linguistic question, polar questions, main linguistic, polar</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Yes/No or polar questions represent one of the main linguistic question categories. They consist of a main interrogative clause, for which the answer is binary (assertion or negation). Polar questions and answers (PQA) represent a valuable knowledge resource present in many community and other curated QA sources, such as forums or e-commerce applications. Using answers to polar questions alone in other contexts is not trivial. Answers are contextualized, and presume that the interrogative question clause and any shared knowledge between the asker and answerer are provided.
We address the problem of controllable rewriting of answers to polar questions into decontextualized and succinct factual statements. We propose a Transformer sequence to sequence model that utilizes soft-constraints to ensure controllable rewriting, such that the output statement is semantically equivalent to its PQA input. Evaluation on three separate PQA datasets as measured through automated and human evaluation metrics show that our proposed approach achieves the best performance when compared to existing baselines.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：On the Audio Hallucinations in Large Audio-Video Language Models</b></summary>
  <p><b>编号</b>：[175]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09774</p>
  <p><b>作者</b>：Taichi Nishimura,  Shota Nakada,  Masayoshi Kondo</p>
  <p><b>备注</b>：6 pages</p>
  <p><b>关键词</b>：Large audio-video language, audio-video language models, audio-video language, Large audio-video, audio</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large audio-video language models can generate descriptions for both video and audio. However, they sometimes ignore audio content, producing audio descriptions solely reliant on visual information. This paper refers to this as audio hallucinations and analyzes them in large audio-video language models. We gather 1,000 sentences by inquiring about audio information and annotate them whether they contain hallucinations. If a sentence is hallucinated, we also categorize the type of hallucination. The results reveal that 332 sentences are hallucinated with distinct trends observed in nouns and verbs for each hallucination type. Based on this, we tackle a task of audio hallucination classification using pre-trained audio-text models in the zero-shot and fine-tuning settings. Our experimental results reveal that the zero-shot models achieve higher performance (52.2% in F1) than the random (40.3%) and the fine-tuning models achieve 87.9%, outperforming the zero-shot models.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：A Comparative Study on Annotation Quality of Crowdsourcing and LLM via  Label Aggregation</b></summary>
  <p><b>编号</b>：[183]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09760</p>
  <p><b>作者</b>：Jiyi Li</p>
  <p><b>备注</b>：Accepted in ICASSP 2024</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, attracting interest recently, data annotation task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Whether Large Language Models (LLMs) can outperform crowdsourcing on the data annotation task is attracting interest recently. Some works verified this issue with the average performance of individual crowd workers and LLM workers on some specific NLP tasks by collecting new datasets. However, on the one hand, existing datasets for the studies of annotation quality in crowdsourcing are not yet utilized in such evaluations, which potentially provide reliable evaluations from a different viewpoint. On the other hand, the quality of these aggregated labels is crucial because, when utilizing crowdsourcing, the estimated labels aggregated from multiple crowd labels to the same instances are the eventually collected labels. Therefore, in this paper, we first investigate which existing crowdsourcing datasets can be used for a comparative study and create a benchmark. We then compare the quality between individual crowd labels and LLM labels and make the evaluations on the aggregated labels. In addition, we propose a Crowd-LLM hybrid label aggregation method and verify the performance. We find that adding LLM labels from good LLMs to existing crowdsourcing datasets can enhance the quality of the aggregated labels of the datasets, which is also higher than the quality of LLM labels themselves.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Resolving Regular Polysemy in Named Entities</b></summary>
  <p><b>编号</b>：[185]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09758</p>
  <p><b>作者</b>：Shu-Kai Hsieh,  Yu-Hsiang Tseng,  Hsin-Yu Chou,  Ching-Wen Yang,  Yu-Yun Chang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：predefined sense inventory, disambiguation primarily addresses, common words based, primarily addresses, sense disambiguation primarily</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Word sense disambiguation primarily addresses the lexical ambiguity of common words based on a predefined sense inventory. Conversely, proper names are usually considered to denote an ad-hoc real-world referent. Once the reference is decided, the ambiguity is purportedly resolved. However, proper names also exhibit ambiguities through appellativization, i.e., they act like common words and may denote different aspects of their referents. We proposed to address the ambiguities of proper names through the light of regular polysemy, which we formalized as dot objects. This paper introduces a combined word sense disambiguation (WSD) model for disambiguating common words against Chinese Wordnet (CWN) and proper names as dot objects. The model leverages the flexibility of a gloss-based model architecture, which takes advantage of the glosses and example sentences of CWN. We show that the model achieves competitive results on both common and proper nouns, even on a relatively sparse sense dataset. Aside from being a performant WSD tool, the model further facilitates the future development of the lexical resource.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Large Language Model Lateral Spear Phishing: A Comparative Study in  Large-Scale Organizational Settings</b></summary>
  <p><b>编号</b>：[200]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09727</p>
  <p><b>作者</b>：Mazal Bethany,  Athanasios Galiopoulos,  Emet Bethany,  Mohammad Bahrami Karkevandi,  Nishant Vishwamitra,  Peyman Najafirad</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：automated spear phishing, generate highly targeted, spear phishing attacks, generate highly, automated spear</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The critical threat of phishing emails has been further exacerbated by the potential of LLMs to generate highly targeted, personalized, and automated spear phishing attacks. Two critical problems concerning LLM-facilitated phishing require further investigation: 1) Existing studies on lateral phishing lack specific examination of LLM integration for large-scale attacks targeting the entire organization, and 2) Current anti-phishing infrastructure, despite its extensive development, lacks the capability to prevent LLM-generated attacks, potentially impacting both employees and IT security incident management. However, the execution of such investigative studies necessitates a real-world environment, one that functions during regular business operations and mirrors the complexity of a large organizational infrastructure. This setting must also offer the flexibility required to facilitate a diverse array of experimental conditions, particularly the incorporation of phishing emails crafted by LLMs. This study is a pioneering exploration into the use of Large Language Models (LLMs) for the creation of targeted lateral phishing emails, targeting a large tier 1 university's operation and workforce of approximately 9,000 individuals over an 11-month period. It also evaluates the capability of email filtering infrastructure to detect such LLM-generated phishing attempts, providing insights into their effectiveness and identifying potential areas for improvement. Based on our findings, we propose machine learning-based detection techniques for such emails to detect LLM-generated phishing emails that were missed by the existing infrastructure, with an F1-score of 98.96.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Predicting Viral Rumors and Vulnerable Users for Infodemic Surveillance</b></summary>
  <p><b>编号</b>：[202]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09724</p>
  <p><b>作者</b>：Xuan Zhang,  Wei Gao</p>
  <p><b>备注</b>：Accepted by IP&M</p>
  <p><b>关键词</b>：identifying vulnerable users, spreading such misinformation, user vulnerability, monitoring the spread, spread of rampant</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the age of the infodemic, it is crucial to have tools for effectively monitoring the spread of rampant rumors that can quickly go viral, as well as identifying vulnerable users who may be more susceptible to spreading such misinformation. This proactive approach allows for timely preventive measures to be taken, mitigating the negative impact of false information on society. We propose a novel approach to predict viral rumors and vulnerable users using a unified graph neural network model. We pre-train network-based user embeddings and leverage a cross-attention mechanism between users and posts, together with a community-enhanced vulnerability propagation (CVP) method to improve user and propagation graph representations. Furthermore, we employ two multi-task training strategies to mitigate negative transfer effects among tasks in different settings, enhancing the overall performance of our approach. We also construct two datasets with ground-truth annotations on information virality and user vulnerability in rumor and non-rumor events, which are automatically derived from existing rumor detection datasets. Extensive evaluation results of our joint learning model confirm its superiority over strong baselines in all three tasks: rumor detection, virality prediction, and user vulnerability scoring. For instance, compared to the best baselines based on the Weibo dataset, our model makes 3.8\% and 3.0\% improvements on Accuracy and MacF1 for rumor detection, and reduces mean squared error (MSE) by 23.9\% and 16.5\% for virality prediction and user vulnerability scoring, respectively. Our findings suggest that our approach effectively captures the correlation between rumor virality and user vulnerability, leveraging this information to improve prediction performance and provide a valuable tool for infodemic surveillance.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Curriculum Recommendations Using Transformer Base Model with InfoNCE  Loss And Language Switching Method</b></summary>
  <p><b>编号</b>：[214]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09699</p>
  <p><b>作者</b>：Xiaonan Xu,  Bin Yuan,  Yongyao Mo,  Tianbo Song,  Shulin Li</p>
  <p><b>备注</b>：4pages, 2 figures, ICAICA2023</p>
  <p><b>关键词</b>：fostering learning equality, dedicated to fostering, ever-evolving realms, Curriculum Recommendations paradigm, language translation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The Curriculum Recommendations paradigm is dedicated to fostering learning equality within the ever-evolving realms of educational technology and curriculum development. In acknowledging the inherent obstacles posed by existing methodologies, such as content conflicts and disruptions from language translation, this paradigm aims to confront and overcome these challenges. Notably, it addresses content conflicts and disruptions introduced by language translation, hindrances that can impede the creation of an all-encompassing and personalized learning experience. The paradigm's objective is to cultivate an educational environment that not only embraces diversity but also customizes learning experiences to suit the distinct needs of each learner. To overcome these challenges, our approach builds upon notable contributions in curriculum development and personalized learning, introducing three key innovations. These include the integration of Transformer Base Model to enhance computational efficiency, the implementation of InfoNCE Loss for accurate content-topic matching, and the adoption of a language switching strategy to alleviate translation-related ambiguities. Together, these innovations aim to collectively tackle inherent challenges and contribute to forging a more equitable and effective learning journey for a diverse range of learners. Competitive cross-validation scores underscore the efficacy of sentence-transformers/LaBSE, achieving 0.66314, showcasing our methodology's effectiveness in diverse linguistic nuances for content alignment prediction. Index Terms-Curriculum Recommendation, Transformer model with InfoNCE Loss, Language Switching.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：Characterizing Online Eating Disorder Communities with Large Language  Models</b></summary>
  <p><b>编号</b>：[233]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09647</p>
  <p><b>作者</b>：Minh Duc Chu,  Aryan Karnati,  Zihao He,  Kristina Lerman</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：dangerous mental health, mental health condition, idealized body images, social media, eating disorders</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rise in eating disorders, a dangerous mental health condition with high mortality and morbidity, has been linked to the proliferation of idealized body images on social media. However, the link between social media and eating disorders is far more complex. We argue that social media platforms create a feedback loop that amplifies the growth of content and communities that promote eating disorders like anorexia and bulimia. Specifically, social media platforms make it easy for vulnerable individuals to find and connect to like-minded others, while group dynamic processes encourage them to stay engaged within communities that promote and glorify harmful behaviors linked to eating disorders. We characterize this dynamic empirically through a combination of network and language analysis. We describe a novel framework that leverages large language models to analyze the discourse within online communities and probe their attitudes on topics related to eating disorders to identify potentially harmful content. Our work emphasizes the need for better social media moderation to disrupt harmful feedback loops and protect vulnerable individuals.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on  Climate Change</b></summary>
  <p><b>编号</b>：[234]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09646</p>
  <p><b>作者</b>：David Thulke,  Yingbo Gao,  Petrus Pelser,  Rein Brune,  Rricha Jalota,  Floris Fok,  Michael Ramos,  Ian van Wyk,  Abdallah Nasir,  Hayden Goldstein,  Taylor Tragemann,  Katie Nguyen,  Ariana Fowler,  Andrew Stanco,  Jon Gabriel,  Jordan Taylor,  Dean Moro,  Evgenii Tsymbalov,  Juliette de Waal,  Evgeny Matusov,  Mudar Yaghi,  Mohammad Shihadah,  Hermann Ney,  Christian Dugast,  Jonathan Dotan,  Daniel Erasmus</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：paper introduces ClimateGPT, introduces ClimateGPT, paper introduces, model, domain-specific</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper introduces ClimateGPT, a model family of domain-specific large language models that synthesize interdisciplinary research on climate change. We trained two 7B models from scratch on a science-oriented dataset of 300B tokens. For the first model, the 4.2B domain-specific tokens were included during pre-training and the second was adapted to the climate domain after pre-training. Additionally, ClimateGPT-7B, 13B and 70B are continuously pre-trained from Llama~2 on a domain-specific dataset of 4.2B tokens. Each model is instruction fine-tuned on a high-quality and human-generated domain-specific dataset that has been created in close cooperation with climate scientists. To reduce the number of hallucinations, we optimize the model for retrieval augmentation and propose a hierarchical retrieval strategy. To increase the accessibility of our model to non-English speakers, we propose to make use of cascaded machine translation and show that this approach can perform comparably to natively multilingual models while being easier to scale to a large number of languages. Further, to address the intrinsic interdisciplinary aspect of climate change we consider different research perspectives. Therefore, the model can produce in-depth answers focusing on different perspectives in addition to an overall answer. We propose a suite of automatic climate-specific benchmarks to evaluate LLMs. On these benchmarks, ClimateGPT-7B performs on par with the ten times larger Llama-2-70B Chat model while not degrading results on general domain benchmarks. Our human evaluation confirms the trends we saw in our benchmarks. All models were trained and evaluated using renewable energy and are released publicly.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Impact of Large Language Model Assistance on Patients Reading Clinical  Notes: A Mixed-Methods Study</b></summary>
  <p><b>编号</b>：[237]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09637</p>
  <p><b>作者</b>：Niklas Mannhardt,  Elizabeth Bondi-Kelly,  Barbara Lam,  Chloe O'Connell,  Mercy Asiedu,  Hussein Mozannar,  Monica Agrawal,  Alejandro Buendia,  Tatiana Urman,  Irbaz B. Riaz,  Catherine E. Ricciardi,  Marzyeh Ghassemi,  David Sontag</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：derive numerous benefits, clinical notes, Patients derive numerous, notes, including an increased</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Patients derive numerous benefits from reading their clinical notes, including an increased sense of control over their health and improved understanding of their care plan. However, complex medical concepts and jargon within clinical notes hinder patient comprehension and may lead to anxiety. We developed a patient-facing tool to make clinical notes more readable, leveraging large language models (LLMs) to simplify, extract information from, and add context to notes. We prompt engineered GPT-4 to perform these augmentation tasks on real clinical notes donated by breast cancer survivors and synthetic notes generated by a clinician, a total of 12 notes with 3868 words. In June 2023, 200 female-identifying US-based participants were randomly assigned three clinical notes with varying levels of augmentations using our tool. Participants answered questions about each note, evaluating their understanding of follow-up actions and self-reported confidence. We found that augmentations were associated with a significant increase in action understanding score (0.63 $\pm$ 0.04 for select augmentations, compared to 0.54 $\pm$ 0.02 for the control) with p=0.002. In-depth interviews of self-identifying breast cancer patients (N=7) were also conducted via video conferencing. Augmentations, especially definitions, elicited positive responses among the seven participants, with some concerns about relying on LLMs. Augmentations were evaluated for errors by clinicians, and we found misleading errors occur, with errors more common in real donated notes than synthetic notes, illustrating the importance of carefully written clinical notes. Augmentations improve some but not all readability metrics. This work demonstrates the potential of LLMs to improve patients' experience with clinical notes at a lower burden to clinicians. However, having a human in the loop is important to correct potential model errors.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Learning Shortcuts: On the Misleading Promise of NLU in Language Models</b></summary>
  <p><b>编号</b>：[244]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09615</p>
  <p><b>作者</b>：Geetanjali Bihani,  Julia Taylor Rayz</p>
  <p><b>备注</b>：Accepted at HICSS-SDPS 2024</p>
  <p><b>关键词</b>：significant performance gains, enabled significant performance, natural language processing, large language models, advent of large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The advent of large language models (LLMs) has enabled significant performance gains in the field of natural language processing. However, recent studies have found that LLMs often resort to shortcuts when performing tasks, creating an illusion of enhanced performance while lacking generalizability in their decision rules. This phenomenon introduces challenges in accurately assessing natural language understanding in LLMs. Our paper provides a concise survey of relevant research in this area and puts forth a perspective on the implications of shortcut learning in the evaluation of language models, specifically for NLU tasks. This paper urges more research efforts to be put towards deepening our comprehension of shortcut learning, contributing to the development of more robust language models, and raising the standards of NLU evaluation in real-world scenarios.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：Aligning Large Language Models with Counterfactual DPO</b></summary>
  <p><b>编号</b>：[260]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09566</p>
  <p><b>作者</b>：Bradley Butcher</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, demonstrated remarkable capabilities, Advancements in large, range of applications, large language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Advancements in large language models (LLMs) have demonstrated remarkable capabilities across a diverse range of applications. These models excel in generating text completions that are contextually coherent and cover an extensive array of subjects. However, the vast datasets required for their training make aligning response styles during the pretraining and instruction tuning phases challenging. Consequently, an additional alignment phase is typically employed, wherein the model is further trained with human preference data to better align its outputs with human expectations. While this process doesn't introduce new capabilities per se, it does accentuate generation styles innate to the model. This paper explores the utilization of counterfactual prompting within the framework of Direct Preference Optimization (DPO) to align the model's style without relying on human intervention. We demonstrate that this method effectively instils desirable behaviour, mitigates undesirable ones, and encourages the model to disregard inappropriate instructions. Our findings suggest that counterfactual prompting with DPO presents a low-resource way to fine-tune LLMs to meet the demands for responsible and ethically aligned AI systems.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Improving Classification Performance With Human Feedback: Label a few,  we label the rest</b></summary>
  <p><b>编号</b>：[262]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09555</p>
  <p><b>作者</b>：Natan Vidra,  Thomas Clifford,  Katherine Jijo,  Eden Chung,  Liang Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：obtaining substantial amounts, train supervised machine, supervised machine learning, machine learning models, learning models poses</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the realm of artificial intelligence, where a vast majority of data is unstructured, obtaining substantial amounts of labeled data to train supervised machine learning models poses a significant challenge. To address this, we delve into few-shot and active learning, where are goal is to improve AI models with human feedback on a few labeled examples. This paper focuses on understanding how a continuous feedback loop can refine models, thereby enhancing their accuracy, recall, and precision through incremental human input. By employing Large Language Models (LLMs) such as GPT-3.5, BERT, and SetFit, we aim to analyze the efficacy of using a limited number of labeled examples to substantially improve model accuracy. We benchmark this approach on the Financial Phrasebank, Banking, Craigslist, Trec, Amazon Reviews datasets to prove that with just a few labeled examples, we are able to surpass the accuracy of zero shot large language models to provide enhanced text classification performance. We demonstrate that rather than needing to manually label millions of rows of data, we just need to label a few and the model can effectively predict the rest.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：BERTologyNavigator: Advanced Question Answering with BERT-based  Semantics</b></summary>
  <p><b>编号</b>：[263]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09553</p>
  <p><b>作者</b>：Shreya Rajpal (1,2),  Ricardo Usbeck (1) ((1) Universität Hamburg, Hamburg, Germany,(2) Vellore Institute of Technology, Vellore, Tamil Nadu, India)</p>
  <p><b>备注</b>：Accepted in Scholarly QALD Challenge @ ISWC 2023</p>
  <p><b>关键词</b>：natural language processing, DBLP Knowledge Graph, language processing, language models, natural language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The development and integration of knowledge graphs and language models has significance in artificial intelligence and natural language processing. In this study, we introduce the BERTologyNavigator -- a two-phased system that combines relation extraction techniques and BERT embeddings to navigate the relationships within the DBLP Knowledge Graph (KG). Our approach focuses on extracting one-hop relations and labelled candidate pairs in the first phases. This is followed by employing BERT's CLS embeddings and additional heuristics for relation selection in the second phase. Our system reaches an F1 score of 0.2175 on the DBLP QuAD Final test dataset for Scholarly QALD and 0.98 F1 score on the subset of the DBLP QuAD test dataset during the QA phase.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：LoMA: Lossless Compressed Memory Attention</b></summary>
  <p><b>编号</b>：[284]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09486</p>
  <p><b>作者</b>：Yumeng Wang,  Zhenyang Xiao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, handle long texts, capabilities of Large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The ability to handle long texts is one of the most important capabilities of Large Language Models (LLMs), but as the text length increases, the consumption of resources also increases dramatically. At present, reducing resource consumption by compressing the KV cache is a common approach. Although there are many existing compression methods, they share a common drawback: the compression is not lossless. That is, information is inevitably lost during the compression process. If the compression rate is high, the probability of losing important information increases dramatically. We propose a new method, Lossless Compressed Memory Attention (LoMA), which allows for lossless compression of information into special memory token KV pairs according to a set compression ratio. Our experiments have achieved remarkable results, demonstrating that LoMA can be efficiently trained and has very effective performance.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Voila-A: Aligning Vision-Language Models with User's Gaze Attention</b></summary>
  <p><b>编号</b>：[297]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09454</p>
  <p><b>作者</b>：Kun Yan,  Lei Ji,  Zeyu Wang,  Yuntao Wang,  Nan Duan,  Shuai Ma</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：recent years, artificial intelligence, integration of vision, vision and language, language understanding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, the integration of vision and language understanding has led to significant advancements in artificial intelligence, particularly through Vision-Language Models (VLMs). However, existing VLMs face challenges in handling real-world applications with complex scenes and multiple objects, as well as aligning their focus with the diverse attention patterns of human users. In this paper, we introduce gaze information, feasibly collected by AR or VR devices, as a proxy for human attention to guide VLMs and propose a novel approach, Voila-A, for gaze alignment to enhance the interpretability and effectiveness of these models in real-world applications. First, we collect hundreds of minutes of gaze data to demonstrate that we can mimic human gaze modalities using localized narratives. We then design an automatic data annotation pipeline utilizing GPT-4 to generate the VOILA-COCO dataset. Additionally, we innovate the Voila Perceiver modules to integrate gaze information into VLMs while preserving their pretrained knowledge. We evaluate Voila-A using a hold-out validation set and a newly collected VOILA-GAZE Testset, which features real-life scenarios captured with a gaze-tracking device. Our experimental results demonstrate that Voila-A significantly outperforms several baseline models. By aligning model attention with human gaze patterns, Voila-A paves the way for more intuitive, user-centric VLMs and fosters engaging human-AI interaction across a wide range of applications.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Explainable Multimodal Sentiment Analysis on Bengali Memes</b></summary>
  <p><b>编号</b>：[303]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09446</p>
  <p><b>作者</b>：Kazi Toufique Elahi,  Tasnuva Binte Rahman,  Shakil Shahriar,  Samir Sarker,  Sajib Kumar Saha Joy,  Faisal Muhammad Shah</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：attracting online communities, digital era, attracting online, cultural barriers, distinctive and effective</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Memes have become a distinctive and effective form of communication in the digital era, attracting online communities and cutting across cultural barriers. Even though memes are frequently linked with humor, they have an amazing capacity to convey a wide range of emotions, including happiness, sarcasm, frustration, and more. Understanding and interpreting the sentiment underlying memes has become crucial in the age of information. Previous research has explored text-based, image-based, and multimodal approaches, leading to the development of models like CAPSAN and PromptHate for detecting various meme categories. However, the study of low-resource languages like Bengali memes remains scarce, with limited availability of publicly accessible datasets. A recent contribution includes the introduction of the MemoSen dataset. However, the achieved accuracy is notably low, and the dataset suffers from imbalanced distribution. In this study, we employed a multimodal approach using ResNet50 and BanglishBERT and achieved a satisfactory result of 0.71 weighted F1-score, performed comparison with unimodal approaches, and interpreted behaviors of the models using explainable artificial intelligence (XAI) techniques.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：RoleCraft-GLM: Advancing Personalized Role-Playing in Large Language  Models</b></summary>
  <p><b>编号</b>：[310]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09432</p>
  <p><b>作者</b>：Meiling Tao,  Xuechen Liang,  Tianyu Shi,  Lei Yu,  Yiting Xie</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, study presents RoleCraft-GLM, Language Models, innovative framework aimed, Large Language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This study presents RoleCraft-GLM, an innovative framework aimed at enhancing personalized role-playing with Large Language Models (LLMs). RoleCraft-GLM addresses the key issue of lacking personalized interactions in conversational AI, and offers a solution with detailed and emotionally nuanced character portrayals. We contribute a unique conversational dataset that shifts from conventional celebrity-centric characters to diverse, non-celebrity personas, thus enhancing the realism and complexity of language modeling interactions. Additionally, our approach includes meticulous character development, ensuring dialogues are both realistic and emotionally resonant. The effectiveness of RoleCraft-GLM is validated through various case studies, highlighting its versatility and skill in different scenarios. Our framework excels in generating dialogues that accurately reflect characters' personality traits and emotions, thereby boosting user engagement. In conclusion, RoleCraft-GLM marks a significant leap in personalized AI interactions, and paves the way for more authentic and immersive AI-assisted role-playing experiences by enabling more nuanced and emotionally rich dialogues</p>
  </details>
</details>
<h1>计算机视觉</h1>
<details>
  <summary>1. <b>标题：ParaHome: Parameterizing Everyday Home Activities Towards 3D Generative  Modeling of Human-Object Interactions</b></summary>
  <p><b>编号</b>：[1]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10232</p>
  <p><b>作者</b>：Jeonghwan Kim,  Jisoo Kim,  Jeonghyeon Na,  Hanbyul Joo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：provide rich data, enable machines, machines to learn, physical world, crucial to provide</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To enable machines to learn how humans interact with the physical world in our daily activities, it is crucial to provide rich data that encompasses the 3D motion of humans as well as the motion of objects in a learnable 3D representation. Ideally, this data should be collected in a natural setup, capturing the authentic dynamic 3D signals during human-object interactions. To address this challenge, we introduce the ParaHome system, designed to capture and parameterize dynamic 3D movements of humans and objects within a common home environment. Our system consists of a multi-view setup with 70 synchronized RGB cameras, as well as wearable motion capture devices equipped with an IMU-based body suit and hand motion capture gloves. By leveraging the ParaHome system, we collect a novel large-scale dataset of human-object interaction. Notably, our dataset offers key advancement over existing datasets in three main aspects: (1) capturing 3D body and dexterous hand manipulation motion alongside 3D object movement within a contextual home environment during natural activities; (2) encompassing human interaction with multiple objects in various episodic scenarios with corresponding descriptions in texts; (3) including articulated objects with multiple parts expressed with parameterized articulations. Building upon our dataset, we introduce new research tasks aimed at building a generative model for learning and synthesizing human-object interactions in a real-world room setting.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：OMG-Seg: Is One Model Good Enough For All Segmentation?</b></summary>
  <p><b>编号</b>：[3]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10229</p>
  <p><b>作者</b>：Xiangtai Li,  Haobo Yuan,  Wei Li,  Henghui Ding,  Size Wu,  Wenwei Zhang,  Yining Li,  Kai Chen,  Chen Change Loy</p>
  <p><b>备注</b>：Project Page: this https URL</p>
  <p><b>关键词</b>：partially unified models, segmentation tasks, traditionally tackled, partially unified, distinct segmentation tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we address various segmentation tasks, each traditionally tackled by distinct or partially unified models. We propose OMG-Seg, One Model that is Good enough to efficiently and effectively handle all the segmentation tasks, including image semantic, instance, and panoptic segmentation, as well as their video counterparts, open vocabulary settings, prompt-driven, interactive segmentation like SAM, and video object segmentation. To our knowledge, this is the first model to handle all these tasks in one model and achieve satisfactory performance. We show that OMG-Seg, a transformer-based encoder-decoder architecture with task-specific queries and outputs, can support over ten distinct segmentation tasks and yet significantly reduce computational and parameter overhead across various tasks and datasets. We rigorously evaluate the inter-task influences and correlations during co-training. Code and models are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：RAP-SAM: Towards Real-Time All-Purpose Segment Anything</b></summary>
  <p><b>编号</b>：[4]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10228</p>
  <p><b>作者</b>：Shilin Xu,  Haobo Yuan,  Qingyu Shi,  Lu Qi,  Jingbo Wang,  Yibo Yang,  Yining Li,  Kai Chen,  Yunhai Tong,  Bernard Ghanem,  Xiangtai Li,  Ming-Hsuan Yang</p>
  <p><b>备注</b>：Project Page: this https URL</p>
  <p><b>关键词</b>：vision foundation models, achieve remarkable progress, Advanced by transformer, transformer architecture, vision foundation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Advanced by transformer architecture, vision foundation models (VFMs) achieve remarkable progress in performance and generalization ability. Segment Anything Model (SAM) is one remarkable model that can achieve generalized segmentation. However, most VFMs cannot run in realtime, which makes it difficult to transfer them into several products. On the other hand, current real-time segmentation mainly has one purpose, such as semantic segmentation on the driving scene. We argue that diverse outputs are needed for real applications. Thus, this work explores a new real-time segmentation setting, named all-purpose segmentation in real-time, to transfer VFMs in real-time deployment. It contains three different tasks, including interactive segmentation, panoptic segmentation, and video segmentation. We aim to use one model to achieve the above tasks in real-time. We first benchmark several strong baselines. Then, we present Real-Time All Purpose SAM (RAP-SAM). It contains an efficient encoder and an efficient decoupled decoder to perform prompt-driven decoding. Moreover, we further explore different training strategies and tuning methods to boost co-training performance further. Our code and model are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask  Inpainting</b></summary>
  <p><b>编号</b>：[5]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10227</p>
  <p><b>作者</b>：Wouter Van Gansbeke,  Bert De Brabandere</p>
  <p><b>备注</b>：Code: this https URL</p>
  <p><b>关键词</b>：complex loss functions, object detection modules, specialized object detection, ad-hoc post-processing steps, instance segmentation networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Panoptic and instance segmentation networks are often trained with specialized object detection modules, complex loss functions, and ad-hoc post-processing steps to handle the permutation-invariance of the instance masks. This work builds upon Stable Diffusion and proposes a latent diffusion approach for panoptic segmentation, resulting in a simple architecture which omits these complexities. Our training process consists of two steps: (1) training a shallow autoencoder to project the segmentation masks to latent space; (2) training a diffusion model to allow image-conditioned sampling in latent space. The use of a generative model unlocks the exploration of mask completion or inpainting, which has applications in interactive segmentation. The experimental validation yields promising results for both panoptic segmentation and mask inpainting. While not setting a new state-of-the-art, our model's simplicity, generality, and mask completion capability are desirable properties.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Towards Language-Driven Video Inpainting via Multimodal Large Language  Models</b></summary>
  <p><b>编号</b>：[6]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10226</p>
  <p><b>作者</b>：Jianzong Wu,  Xiangtai Li,  Chenyang Si,  Shangchen Zhou,  Jingkang Yang,  Jiangning Zhang,  Yining Li,  Kai Chen,  Yunhai Tong,  Ziwei Liu,  Chen Change Loy</p>
  <p><b>备注</b>：Project Page: this https URL</p>
  <p><b>关键词</b>：natural language instructions, language-driven video inpainting, Multimodal Large Language, video inpainting, inpainting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce a new task -- language-driven video inpainting, which uses natural language instructions to guide the inpainting process. This approach overcomes the limitations of traditional video inpainting methods that depend on manually labeled binary masks, a process often tedious and labor-intensive. We present the Remove Objects from Videos by Instructions (ROVI) dataset, containing 5,650 videos and 9,091 inpainting results, to support training and evaluation for this task. We also propose a novel diffusion-based language-driven video inpainting framework, the first end-to-end baseline for this task, integrating Multimodal Large Language Models to understand and execute complex language-based inpainting requests effectively. Our comprehensive results showcase the dataset's versatility and the model's effectiveness in various language-instructed inpainting scenarios. We will make datasets, code, and models publicly available.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：The Manga Whisperer: Automatically Generating Transcriptions for Comics</b></summary>
  <p><b>编号</b>：[8]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10224</p>
  <p><b>作者</b>：Ragav Sachdeva,  Andrew Zisserman</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：true worldwide sensation, Japanese comics, past few decades, commonly referred, worldwide sensation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the past few decades, Japanese comics, commonly referred to as Manga, have transcended both cultural and linguistic boundaries to become a true worldwide sensation. Yet, the inherent reliance on visual cues and illustration within manga renders it largely inaccessible to individuals with visual impairments. In this work, we seek to address this substantial barrier, with the aim of ensuring that manga can be appreciated and actively engaged by everyone. Specifically, we tackle the problem of diarisation i.e. generating a transcription of who said what and when, in a fully automatic way.
To this end, we make the following contributions: (1) we present a unified model, Magi, that is able to (a) detect panels, text boxes and character boxes, (b) cluster characters by identity (without knowing the number of clusters apriori), and (c) associate dialogues to their speakers; (2) we propose a novel approach that is able to sort the detected text boxes in their reading order and generate a dialogue transcript; (3) we annotate an evaluation benchmark for this task using publicly available [English] manga pages. The code, evaluation datasets and the pre-trained model can be found at: this https URL.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Supervised Fine-tuning in turn Improves Visual Foundation Models</b></summary>
  <p><b>编号</b>：[9]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10222</p>
  <p><b>作者</b>：Xiaohu Jiang,  Yixiao Ge,  Yuying Ge,  Chun Yuan,  Ying Shan</p>
  <p><b>备注</b>：14 pages, 3 figures, Project page: this https URL</p>
  <p><b>关键词</b>：vision foundation models, vision foundation, Image-text training, recent years, foundation models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image-text training like CLIP has dominated the pretraining of vision foundation models in recent years. Subsequent efforts have been made to introduce region-level visual learning into CLIP's pretraining but face scalability challenges due to the lack of large-scale region-level datasets. Drawing inspiration from supervised fine-tuning (SFT) in natural language processing such as instruction tuning, we explore the potential of fine-grained SFT in enhancing the generation of vision foundation models after their pretraining. Thus a two-stage method ViSFT (Vision SFT) is proposed to unleash the fine-grained knowledge of vision foundation models. In ViSFT, the vision foundation model is enhanced by performing visual joint learning on some in-domain tasks and then tested on out-of-domain benchmarks. With updating using ViSFT on 8 V100 GPUs in less than 2 days, a vision transformer with over 4.4B parameters shows improvements across various out-of-domain benchmarks including vision and vision-linguistic scenarios.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：AutoFT: Robust Fine-Tuning by Optimizing Hyperparameters on OOD Data</b></summary>
  <p><b>编号</b>：[10]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10220</p>
  <p><b>作者</b>：Caroline Choi,  Yoonho Lee,  Annie Chen,  Allan Zhou,  Aditi Raghunathan,  Chelsea Finn</p>
  <p><b>备注</b>：16 pages</p>
  <p><b>关键词</b>：encode rich representations, models encode rich, Foundation models encode, fine-tuning, encode rich</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Foundation models encode rich representations that can be adapted to a desired task by fine-tuning on task-specific data. However, fine-tuning a model on one particular data distribution often compromises the model's original performance on other distributions. Current methods for robust fine-tuning utilize hand-crafted regularization techniques to constrain the fine-tuning process towards the base foundation model. Yet, it is hard to precisely specify what characteristics of the foundation model to retain during fine-tuning, as this depends on how the pre-training, fine-tuning, and evaluation data distributions relate to each other. We propose AutoFT, a data-driven approach for guiding foundation model fine-tuning. AutoFT optimizes fine-tuning hyperparameters to maximize performance on a small out-of-distribution (OOD) validation set. To guide fine-tuning in a granular way, AutoFT searches a highly expressive hyperparameter space that includes weight coefficients for many different losses, in addition to learning rate and weight decay values. We evaluate AutoFT on nine natural distribution shifts which include domain shifts and subpopulation shifts. Our experiments show that AutoFT significantly improves generalization to new OOD data, outperforming existing robust fine-tuning methods. Notably, AutoFT achieves new state-of-the-art performance on the WILDS-iWildCam and WILDS-FMoW benchmarks, outperforming the previous best methods by $6.0\%$ and $1.5\%$, respectively.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Edit One for All: Interactive Batch Image Editing</b></summary>
  <p><b>编号</b>：[11]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10219</p>
  <p><b>作者</b>：Thao Nguyen,  Utkarsh Ojha,  Yuheng Li,  Haotian Liu,  Yong Jae Lee</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：recent years, advanced remarkably, image, editing, edit</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, image editing has advanced remarkably. With increased human control, it is now possible to edit an image in a plethora of ways; from specifying in text what we want to change, to straight up dragging the contents of the image in an interactive point-based manner. However, most of the focus has remained on editing single images at a time. Whether and how we can simultaneously edit large batches of images has remained understudied. With the goal of minimizing human supervision in the editing process, this paper presents a novel method for interactive batch image editing using StyleGAN as the medium. Given an edit specified by users in an example image (e.g., make the face frontal), our method can automatically transfer that edit to other test images, so that regardless of their initial state (pose), they all arrive at the same final state (e.g., all facing front). Extensive experiments demonstrate that edits performed using our method have similar visual quality to existing single-image-editing methods, while having more visual consistency and saving significant time and human effort.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Explaining the Implicit Neural Canvas: Connecting Pixels to Neurons by  Tracing their Contributions</b></summary>
  <p><b>编号</b>：[12]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10217</p>
  <p><b>作者</b>：Namitha Padmanabhan,  Matthew Gwilliam,  Pulkit Kumar,  Shishira R Maiya,  Max Ehrlich,  Abhinav Shrivastava</p>
  <p><b>备注</b>：Project site: this https URL</p>
  <p><b>关键词</b>：Implicit Neural Canvas, tremendous practical utility, downstream tasks including, Implicit Neural, Implicit Neural Representations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The many variations of Implicit Neural Representations (INRs), where a neural network is trained as a continuous representation of a signal, have tremendous practical utility for downstream tasks including novel view synthesis, video compression, and image superresolution. Unfortunately, the inner workings of these networks are seriously under-studied. Our work, eXplaining the Implicit Neural Canvas (XINC), is a unified framework for explaining properties of INRs by examining the strength of each neuron's contribution to each output pixel. We call the aggregate of these contribution maps the Implicit Neural Canvas and we use this concept to demonstrate that the INRs which we study learn to ''see'' the frames they represent in surprising ways. For example, INRs tend to have highly distributed representations. While lacking high-level object semantics, they have a significant bias for color and edges, and are almost entirely space-agnostic. We arrive at our conclusions by examining how objects are represented across time in video INRs, using clustering to visualize similar neurons across layers and architectures, and show that this is dominated by motion. These insights demonstrate the general usefulness of our analysis framework. Our project page is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：GPAvatar: Generalizable and Precise Head Avatar from Image(s)</b></summary>
  <p><b>编号</b>：[14]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10215</p>
  <p><b>作者</b>：Xuangeng Chu,  Yu Li,  Ailing Zeng,  Tianyu Yang,  Lijian Lin,  Yunfei Liu,  Tatsuya Harada</p>
  <p><b>备注</b>：ICLR 2024, code is available at this https URL</p>
  <p><b>关键词</b>：computer vision community, garnered substantial attention, online meetings, crucial for applications, virtual reality</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Head avatar reconstruction, crucial for applications in virtual reality, online meetings, gaming, and film industries, has garnered substantial attention within the computer vision community. The fundamental objective of this field is to faithfully recreate the head avatar and precisely control expressions and postures. Existing methods, categorized into 2D-based warping, mesh-based, and neural rendering approaches, present challenges in maintaining multi-view consistency, incorporating non-facial information, and generalizing to new identities. In this paper, we propose a framework named GPAvatar that reconstructs 3D head avatars from one or several images in a single forward pass. The key idea of this work is to introduce a dynamic point-based expression field driven by a point cloud to precisely and effectively capture expressions. Furthermore, we use a Multi Tri-planes Attention (MTA) fusion module in the tri-planes canonical field to leverage information from multiple input images. The proposed method achieves faithful identity reconstruction, precise expression control, and multi-view consistency, demonstrating promising results for free-viewpoint rendering and novel view synthesis.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Improving automatic detection of driver fatigue and distraction using  machine learning</b></summary>
  <p><b>编号</b>：[16]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10213</p>
  <p><b>作者</b>：Dongjiang Wu</p>
  <p><b>备注</b>：Master's thesis, 55 pages</p>
  <p><b>关键词</b>：distracted driving behaviors, recent years, advances in information, information technology, technology have played</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Changes and advances in information technology have played an important role in the development of intelligent vehicle systems in recent years. Driver fatigue and distracted driving are important factors in traffic accidents. Thus, onboard monitoring of driving behavior has become a crucial component of advanced driver assistance systems for intelligent vehicles. In this article, we present techniques for simultaneously detecting fatigue and distracted driving behaviors using vision-based and machine learning-based approaches. In driving fatigue detection, we use facial alignment networks to identify facial feature points in the images, and calculate the distance of the facial feature points to detect the opening and closing of the eyes and mouth. Furthermore, we use a convolutional neural network (CNN) based on the MobileNet architecture to identify various distracted driving behaviors. Experiments are performed on a PC based setup with a webcam and results are demonstrated using public datasets as well as custom datasets created for training and testing. Compared to previous approaches, we build our own datasets and provide better results in terms of accuracy and computation time.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：MM-Interleaved: Interleaved Image-Text Generative Modeling via  Multi-modal Feature Synchronizer</b></summary>
  <p><b>编号</b>：[19]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10208</p>
  <p><b>作者</b>：Changyao Tian,  Xizhou Zhu,  Yuwen Xiong,  Weiyun Wang,  Zhe Chen,  Wenhai Wang,  Yuntao Chen,  Lewei Lu,  Tong Lu,  Jie Zhou,  Hongsheng Li,  Yu Qiao,  Jifeng Dai</p>
  <p><b>备注</b>：20 pages, 9 figures, 17 tables</p>
  <p><b>关键词</b>：interleaved image-text data, Developing generative models, research and practical, interleaved image-text, Developing generative</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Developing generative models for interleaved image-text data has both research and practical value. It requires models to understand the interleaved sequences and subsequently generate images and text. However, existing attempts are limited by the issue that the fixed number of visual tokens cannot efficiently capture image details, which is particularly problematic in the multi-image scenarios. To address this, this paper presents MM-Interleaved, an end-to-end generative model for interleaved image-text data. It introduces a multi-scale and multi-image feature synchronizer module, allowing direct access to fine-grained image features in the previous context during the generation process. MM-Interleaved is end-to-end pre-trained on both paired and interleaved image-text corpora. It is further enhanced through a supervised fine-tuning phase, wherein the model improves its ability to follow complex multi-modal instructions. Experiments demonstrate the versatility of MM-Interleaved in recognizing visual details following multi-modal instructions and generating consistent images following both textual and visual conditions. Code and models are available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Divide and not forget: Ensemble of selectively trained experts in  Continual Learning</b></summary>
  <p><b>编号</b>：[24]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10191</p>
  <p><b>作者</b>：Grzegorz Rypeść,  Sebastian Cygert,  Valeriya Khan,  Tomasz Trzciński,  Bartosz Zieliński,  Bartłomiej Twardowski</p>
  <p><b>备注</b>：Accepted to ICLR2024 (main track), code is available at: this https URL</p>
  <p><b>关键词</b>：widen their applicability, models widen, SEED, Class-incremental learning, expert</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Class-incremental learning is becoming more popular as it helps models widen their applicability while not forgetting what they already know. A trend in this area is to use a mixture-of-expert technique, where different models work together to solve the task. However, the experts are usually trained all at once using whole task data, which makes them all prone to forgetting and increasing computational burden. To address this limitation, we introduce a novel approach named SEED. SEED selects only one, the most optimal expert for a considered task, and uses data from this task to fine-tune only this expert. For this purpose, each expert represents each class with a Gaussian distribution, and the optimal expert is selected based on the similarity of those distributions. Consequently, SEED increases diversity and heterogeneity within the experts while maintaining the high stability of this ensemble method. The extensive experiments demonstrate that SEED achieves state-of-the-art performance in exemplar-free settings across various scenarios, showing the potential of expert diversification through data in continual learning.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Neural Echos: Depthwise Convolutional Filters Replicate Biological  Receptive Fields</b></summary>
  <p><b>编号</b>：[30]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10178</p>
  <p><b>作者</b>：Zahra Babaiee,  Peyman M. Kiasari,  Daniela Rus,  Radu Grosu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：present evidence suggesting, biological receptive fields, receptive fields observed, mammalian retina, effectively replicating</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this study, we present evidence suggesting that depthwise convolutional kernels are effectively replicating the structural intricacies of the biological receptive fields observed in the mammalian retina. We provide analytics of trained kernels from various state-of-the-art models substantiating this evidence. Inspired by this intriguing discovery, we propose an initialization scheme that draws inspiration from the biological receptive fields. Experimental analysis of the ImageNet dataset with multiple CNN architectures featuring depthwise convolutions reveals a marked enhancement in the accuracy of the learned model when initialized with biologically derived weights. This underlies the potential for biologically inspired computational models to further our understanding of vision processing systems and to improve the efficacy of convolutional networks.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Comprehensive OOD Detection Improvements</b></summary>
  <p><b>编号</b>：[31]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10176</p>
  <p><b>作者</b>：Anish Lakkapragada,  Amol Khanna,  Edward Raff,  Nathan Inkawhich</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：expected input distribution, model expected input, impactful decisions, recognizing when inference, machine learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As machine learning becomes increasingly prevalent in impactful decisions, recognizing when inference data is outside the model's expected input distribution is paramount for giving context to predictions. Out-of-distribution (OOD) detection methods have been created for this task. Such methods can be split into representation-based or logit-based methods from whether they respectively utilize the model's embeddings or predictions for OOD detection. In contrast to most papers which solely focus on one such group, we address both. We employ dimensionality reduction on feature embeddings in representation-based methods for both time speedups and improved performance. Additionally, we propose DICE-COL, a modification of the popular logit-based method Directed Sparsification (DICE) that resolves an unnoticed flaw. We demonstrate the effectiveness of our methods on the OpenOODv1.5 benchmark framework, where they significantly improve performance and set state-of-the-art results.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：SHINOBI: Shape and Illumination using Neural Object Decomposition via  BRDF Optimization In-the-wild</b></summary>
  <p><b>编号</b>：[33]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10171</p>
  <p><b>作者</b>：Andreas Engelhardt,  Amit Raj,  Mark Boss,  Yunzhi Zhang,  Abhishek Kar,  Yuanzhen Li,  Deqing Sun,  Ricardo Martin Brualla,  Jonathan T. Barron,  Hendrik P. A. Lensch,  Varun Jampani</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：present SHINOBI, object images captured, varying lighting, captured with varying, images captured</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present SHINOBI, an end-to-end framework for the reconstruction of shape, material, and illumination from object images captured with varying lighting, pose, and background. Inverse rendering of an object based on unconstrained image collections is a long-standing challenge in computer vision and graphics and requires a joint optimization over shape, radiance, and pose. We show that an implicit shape representation based on a multi-resolution hash encoding enables faster and robust shape reconstruction with joint camera alignment optimization that outperforms prior work. Further, to enable the editing of illumination and object reflectance (i.e. material) we jointly optimize BRDF and illumination together with the object's shape. Our method is class-agnostic and works on in-the-wild image collections of objects to produce relightable 3D assets for several use cases such as AR/VR, movies, games, etc. Project page: this https URL Video: this https URL</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：VMamba: Visual State Space Model</b></summary>
  <p><b>编号</b>：[34]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10166</p>
  <p><b>作者</b>：Yue Liu,  Yunjie Tian,  Yuzhong Zhao,  Hongtian Yu,  Lingxi Xie,  Yaowei Wang,  Qixiang Ye,  Yunfan Liu</p>
  <p><b>备注</b>：13 pages, 6 figures, 4 tables</p>
  <p><b>关键词</b>：Convolutional Neural Networks, Neural Networks, Vision Transformers, Convolutional Neural, popular foundation models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) stand as the two most popular foundation models for visual representation learning. While CNNs exhibit remarkable scalability with linear complexity w.r.t. image resolution, ViTs surpass them in fitting capabilities despite contending with quadratic complexity. A closer inspection reveals that ViTs achieve superior visual modeling performance through the incorporation of global receptive fields and dynamic weights. This observation motivates us to propose a novel architecture that inherits these components while enhancing computational efficiency. To this end, we draw inspiration from the recently introduced state space model and propose the Visual State Space Model (VMamba), which achieves linear complexity without sacrificing global receptive fields. To address the encountered direction-sensitive issue, we introduce the Cross-Scan Module (CSM) to traverse the spatial domain and convert any non-causal visual image into order patch sequences. Extensive experimental results substantiate that VMamba not only demonstrates promising capabilities across various visual perception tasks, but also exhibits more pronounced advantages over established benchmarks as the image resolution increases. Source code has been available at this https URL.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Importance-Aware Image Segmentation-based Semantic Communication for  Autonomous Driving</b></summary>
  <p><b>编号</b>：[38]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10153</p>
  <p><b>作者</b>：Jie Lv,  Haonan Tong,  Qiang Pan,  Zhilong Zhang,  Xinxin He,  Tao Luo,  Changchuan Yin</p>
  <p><b>备注</b>：10 pages, 8 figures</p>
  <p><b>关键词</b>：segmentation-based semantic communication, image segmentation-based semantic, article studies, studies the problem, Swin Transformer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This article studies the problem of image segmentation-based semantic communication in autonomous driving. In real traffic scenes, detecting the key objects (e.g., vehicles, pedestrians and obstacles) is more crucial than that of other objects to guarantee driving safety. Therefore, we propose a vehicular image segmentation-oriented semantic communication system, termed VIS-SemCom, where image segmentation features of important objects are transmitted to reduce transmission redundancy. First, to accurately extract image semantics, we develop a semantic codec based on Swin Transformer architecture, which expands the perceptual field thus improving the segmentation accuracy. Next, we propose a multi-scale semantic extraction scheme via assigning the number of Swin Transformer blocks for diverse resolution features, thus highlighting the important objects' accuracy. Furthermore, the importance-aware loss is invoked to emphasize the important objects, and an online hard sample mining (OHEM) strategy is proposed to handle small sample issues in the dataset. Experimental results demonstrate that the proposed VIS-SemCom can achieve a coding gain of nearly 6 dB with a 60% mean intersection over union (mIoU), reduce the transmitted data amount by up to 70% with a 60% mIoU, and improve the segmentation intersection over union (IoU) of important objects by 4%, compared to traditional transmission scheme.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Motion-Zero: Zero-Shot Moving Object Control Framework for  Diffusion-Based Video Generation</b></summary>
  <p><b>编号</b>：[39]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10150</p>
  <p><b>作者</b>：Changgu Chen,  Junwei Shu,  Lianggangxu Chen,  Gaoqi He,  Changbo Wang,  Yang Li</p>
  <p><b>备注</b>：9 pages, 4 figures, IJCAI paper</p>
  <p><b>关键词</b>：Recent large-scale pre-trained, detailed text descriptions, powerful generative ability, large-scale pre-trained diffusion, pre-trained diffusion models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent large-scale pre-trained diffusion models have demonstrated a powerful generative ability to produce high-quality videos from detailed text descriptions. However, exerting control over the motion of objects in videos generated by any video diffusion model is a challenging problem. In this paper, we propose a novel zero-shot moving object trajectory control framework, Motion-Zero, to enable a bounding-box-trajectories-controlled text-to-video diffusion this http URL this end, an initial noise prior module is designed to provide a position-based prior to improve the stability of the appearance of the moving object and the accuracy of position. In addition, based on the attention map of the U-net, spatial constraints are directly applied to the denoising process of diffusion models, which further ensures the positional and spatial consistency of moving objects during the inference. Furthermore, temporal consistency is guaranteed with a proposed shift temporal attention mechanism. Our method can be flexibly applied to various state-of-the-art video diffusion models without any training process. Extensive experiments demonstrate our proposed method can control the motion trajectories of objects and generate high-quality videos.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Explicitly Disentangled Representations in Object-Centric Learning</b></summary>
  <p><b>编号</b>：[41]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10148</p>
  <p><b>作者</b>：Riccardo Majellaro,  Jonathan Collu,  Aske Plaat,  Thomas M. Moerland</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Extracting structured representations, raw visual data, Extracting structured, raw visual, important and long-standing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Extracting structured representations from raw visual data is an important and long-standing challenge in machine learning. Recently, techniques for unsupervised learning of object-centric representations have raised growing interest. In this context, enhancing the robustness of the latent features can improve the efficiency and effectiveness of the training of downstream tasks. A promising step in this direction is to disentangle the factors that cause variation in the data. Previously, Invariant Slot Attention disentangled position, scale, and orientation from the remaining features. Extending this approach, we focus on separating the shape and texture components. In particular, we propose a novel architecture that biases object-centric models toward disentangling shape and texture components into two non-overlapping subsets of the latent space dimensions. These subsets are known a priori, hence before the training process. Experiments on a range of object-centric benchmarks reveal that our approach achieves the desired disentanglement while also numerically improving baseline performance in most cases. In addition, we show that our method can generate novel textures for a specific object or transfer textures between objects with distinct shapes.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Model Compression Techniques in Biometrics Applications: A Survey</b></summary>
  <p><b>编号</b>：[43]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10139</p>
  <p><b>作者</b>：Eduarda Caldeira,  Pedro C. Neto,  Marco Huber,  Naser Damer,  Ana F. Sequeira</p>
  <p><b>备注</b>：Under review at IEEE Journal</p>
  <p><b>关键词</b>：task automatization capacity, extensively empowered humanity, empowered humanity task, humanity task automatization, deep learning algorithms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The development of deep learning algorithms has extensively empowered humanity's task automatization capacity. However, the huge improvement in the performance of these models is highly correlated with their increasing level of complexity, limiting their usefulness in human-oriented applications, which are usually deployed in resource-constrained devices. This led to the development of compression techniques that drastically reduce the computational and memory costs of deep learning models without significant performance degradation. This paper aims to systematize the current literature on this topic by presenting a comprehensive survey of model compression techniques in biometrics applications, namely quantization, knowledge distillation and pruning. We conduct a critical analysis of the comparative value of these techniques, focusing on their advantages and disadvantages and presenting suggestions for future work directions that can potentially improve the current methods. Additionally, we discuss and analyze the link between model bias and model compression, highlighting the need to direct compression research toward model fairness in future works.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Exposing Lip-syncing Deepfakes from Mouth Inconsistencies</b></summary>
  <p><b>编号</b>：[51]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10113</p>
  <p><b>作者</b>：Soumyya Kanti Datta,  Shan Jia,  Siwei Lyu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：person lip movements, digitally manipulated video, digitally manipulated, movements are created, created convincingly</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A lip-syncing deepfake is a digitally manipulated video in which a person's lip movements are created convincingly using AI models to match altered or entirely new audio. Lip-syncing deepfakes are a dangerous type of deepfakes as the artifacts are limited to the lip region and more difficult to discern. In this paper, we describe a novel approach, LIP-syncing detection based on mouth INConsistency (LIPINC), for lip-syncing deepfake detection by identifying temporal inconsistencies in the mouth region. These inconsistencies are seen in the adjacent frames and throughout the video. Our model can successfully capture these irregularities and outperforms the state-of-the-art methods on several benchmark deepfake datasets.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：VIPTR: A Vision Permutable Extractor for Fast and Efficient Scene Text  Recognition</b></summary>
  <p><b>编号</b>：[53]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10110</p>
  <p><b>作者</b>：Xianfu Cheng,  Weixiao Zhou,  Xiang Li,  Xiaoming Chen,  Jian Yang,  Tongliang Li,  Zhoujun Li</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2205.00159 by other authors</p>
  <p><b>关键词</b>：involves recognizing text, Scene Text Recognition, Text Recognition, Scene Text, challenging task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Scene Text Recognition (STR) is a challenging task that involves recognizing text within images of natural scenes. Although current state-of-the-art models for STR exhibit high performance, they typically suffer from low inference efficiency due to their reliance on hybrid architectures comprised of visual encoders and sequence decoders. In this work, we propose the VIsion Permutable extractor for fast and efficient scene Text Recognition (VIPTR), which achieves an impressive balance between high performance and rapid inference speeds in the domain of STR. Specifically, VIPTR leverages a visual-semantic extractor with a pyramid structure, characterized by multiple self-attention layers, while eschewing the traditional sequence decoder. This design choice results in a lightweight and efficient model capable of handling inputs of varying sizes. Extensive experimental results on various standard datasets for both Chinese and English scene text recognition validate the superiority of VIPTR. Notably, the VIPTR-T (Tiny) variant delivers highly competitive accuracy on par with other lightweight models and achieves SOTA inference speeds. Meanwhile, the VIPTR-L (Large) variant attains greater recognition accuracy, while maintaining a low parameter count and favorable inference speed. Our proposed method provides a compelling solution for the STR challenge, which blends high accuracy with efficiency and greatly benefits real-world applications requiring fast and reliable text recognition. The code is publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Cross-Modality Perturbation Synergy Attack for Person Re-identification</b></summary>
  <p><b>编号</b>：[57]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10090</p>
  <p><b>作者</b>：Yunpeng Gong,  others</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：single-modal person re-identification, significant research focusing, based on RGB, addressing security concerns, RGB images</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, there has been significant research focusing on addressing security concerns in single-modal person re-identification (ReID) systems that are based on RGB images. However, the safety of cross-modality scenarios, which are more commonly encountered in practical applications involving images captured by infrared cameras, has not received adequate attention. The main challenge in cross-modality ReID lies in effectively dealing with visual differences between different modalities. For instance, infrared images are typically grayscale, unlike visible images that contain color information. Existing attack methods have primarily focused on the characteristics of the visible image modality, overlooking the features of other modalities and the variations in data distribution among different modalities. This oversight can potentially undermine the effectiveness of these methods in image retrieval across diverse modalities. This study represents the first exploration into the security of cross-modality ReID models and proposes a universal perturbation attack specifically designed for cross-modality ReID. This attack optimizes perturbations by leveraging gradients from diverse modality data, thereby disrupting the discriminator and reinforcing the differences between modalities. We conducted experiments on two widely used cross-modality datasets, namely RegDB and SYSU, which not only demonstrated the effectiveness of our method but also provided insights for future enhancements in the robustness of cross-modality ReID systems.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：A locally statistical active contour model for SAR image segmentation  can be solved by denoising algorithms</b></summary>
  <p><b>编号</b>：[61]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10083</p>
  <p><b>作者</b>：Guangming Liu,  Quanying Sun,  Jing Liang,  Qi Liu</p>
  <p><b>备注</b>：18 pages, 15 figures. arXiv admin note: substantial text overlap with arXiv:2312.11849, arXiv:2312.08376, arXiv:2312.09365</p>
  <p><b>关键词</b>：statistical variational active, hybrides geodesic active, locally statistical variational, variational active contour, geodesic active contour</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we propose a novel locally statistical variational active contour model based on I-divergence-TV denoising model, which hybrides geodesic active contour (GAC) model with active contours without edges (ACWE) model, and can be used to segment images corrupted by multiplicative gamma noise. By adding a diffusion term into the level set evolution (LSE) equation of the proposed model, we construct a reaction-diffusion (RD) equation, which can gradually regularize the level set function (LSF) to be piecewise constant in each segment domain and gain the stable solution. We further transform the proposed model into classic ROF model by adding a proximity term. Inspired by a fast denoising algorithm proposed by Jia-Zhao recently, we propose two fast fixed point algorithms to solve SAR image segmentation question. Experimental results for real SAR images show that the proposed image segmentation model can efficiently stop the contours at weak or blurred edges, and can automatically detect the exterior and interior boundaries of images with multiplicative gamma noise. The proposed FPRD1/FPRD2 models are about 1/2 (or less than) of the time required for the SBRD model based on the Split Bregman technique.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：DiffusionGPT: LLM-Driven Text-to-Image Generation System</b></summary>
  <p><b>编号</b>：[66]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10061</p>
  <p><b>作者</b>：Jie Qin,  Jie Wu,  Weifeng Chen,  Yuxi Ren,  Huixia Li,  Hefeng Wu,  Xuefeng Xiao,  Rui Wang,  Shilei Wen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：high-quality models shared, open-source platforms, proliferation of high-quality, shared on open-source, Large Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diffusion models have opened up new avenues for the field of image generation, resulting in the proliferation of high-quality models shared on open-source platforms. However, a major challenge persists in current text-to-image systems are often unable to handle diverse inputs, or are limited to single model results. Current unified attempts often fall into two orthogonal aspects: i) parse Diverse Prompts in input stage; ii) activate expert model to output. To combine the best of both worlds, we propose DiffusionGPT, which leverages Large Language Models (LLM) to offer a unified generation system capable of seamlessly accommodating various types of prompts and integrating domain-expert models. DiffusionGPT constructs domain-specific Trees for various generative models based on prior knowledge. When provided with an input, the LLM parses the prompt and employs the Trees-of-Thought to guide the selection of an appropriate model, thereby relaxing input constraints and ensuring exceptional performance across diverse domains. Moreover, we introduce Advantage Databases, where the Tree-of-Thought is enriched with human feedback, aligning the model selection process with human preferences. Through extensive experiments and comparisons, we demonstrate the effectiveness of DiffusionGPT, showcasing its potential for pushing the boundaries of image synthesis in diverse domains.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：ContextMix: A context-aware data augmentation method for industrial  visual inspection systems</b></summary>
  <p><b>编号</b>：[68]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10050</p>
  <p><b>作者</b>：Hyungmin Kim,  Donghun Kim,  Pyunghwan Ahn,  Sungho Suh,  Hansang Cho,  Junmo Kim</p>
  <p><b>备注</b>：Accepted to EAAI</p>
  <p><b>关键词</b>：achieved remarkable performance, deep neural networks, deep neural, achieved remarkable, strategy to mitigate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While deep neural networks have achieved remarkable performance, data augmentation has emerged as a crucial strategy to mitigate overfitting and enhance network performance. These techniques hold particular significance in industrial manufacturing contexts. Recently, image mixing-based methods have been introduced, exhibiting improved performance on public benchmark datasets. However, their application to industrial tasks remains challenging. The manufacturing environment generates massive amounts of unlabeled data on a daily basis, with only a few instances of abnormal data occurrences. This leads to severe data imbalance. Thus, creating well-balanced datasets is not straightforward due to the high costs associated with labeling. Nonetheless, this is a crucial step for enhancing productivity. For this reason, we introduce ContextMix, a method tailored for industrial applications and benchmark datasets. ContextMix generates novel data by resizing entire images and integrating them into other images within the batch. This approach enables our method to learn discriminative features based on varying sizes from resized images and train informative secondary features for object recognition using occluded images. With the minimal additional computation cost of image resizing, ContextMix enhances performance compared to existing augmentation techniques. We evaluate its effectiveness across classification, detection, and segmentation tasks using various network architectures on public benchmark datasets. Our proposed method demonstrates improved results across a range of robustness tasks. Its efficacy in real industrial environments is particularly noteworthy, as demonstrated using the passive component dataset.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Deep spatial context: when attention-based models meet spatial  regression</b></summary>
  <p><b>编号</b>：[70]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10044</p>
  <p><b>作者</b>：Paulina Tomaszewska,  Elżbieta Sienkiewicz,  Mai P. Hoang,  Przemysław Biecek</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Deep spatial context, attention-based vision models, spatial context, Deep spatial, Spatial Context Measures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose 'Deep spatial context' (DSCon) method, which serves for investigation of the attention-based vision models using the concept of spatial context. It was inspired by histopathologists, however, the method can be applied to various domains. The DSCon allows for a quantitative measure of the spatial context's role using three Spatial Context Measures: $SCM_{features}$, $SCM_{targets}$, $SCM_{residuals}$ to distinguish whether the spatial context is observable within the features of neighboring regions, their target values (attention scores) or residuals, respectively. It is achieved by integrating spatial regression into the pipeline. The DSCon helps to verify research questions. The experiments reveal that spatial relationships are much bigger in the case of the classification of tumor lesions than normal tissues. Moreover, it turns out that the larger the size of the neighborhood taken into account within spatial regression, the less valuable contextual information is. Furthermore, it is observed that the spatial context measure is the largest when considered within the feature space as opposed to the targets and residuals.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：CMFN: Cross-Modal Fusion Network for Irregular Scene Text Recognition</b></summary>
  <p><b>编号</b>：[72]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10041</p>
  <p><b>作者</b>：Jinzhi Zheng,  Ruyi Ji,  Libo Zhang,  Yanjun Wu,  Chen Zhao</p>
  <p><b>备注</b>：Accepted to ICONIP 2023</p>
  <p><b>关键词</b>：task involving vision, important research topic, visual recognition branch, semantic recognition branch, recognition branch</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Scene text recognition, as a cross-modal task involving vision and text, is an important research topic in computer vision. Most existing methods use language models to extract semantic information for optimizing visual recognition. However, the guidance of visual cues is ignored in the process of semantic mining, which limits the performance of the algorithm in recognizing irregular scene text. To tackle this issue, we propose a novel cross-modal fusion network (CMFN) for irregular scene text recognition, which incorporates visual cues into the semantic mining process. Specifically, CMFN consists of a position self-enhanced encoder, a visual recognition branch and an iterative semantic recognition branch. The position self-enhanced encoder provides character sequence position encoding for both the visual recognition branch and the iterative semantic recognition branch. The visual recognition branch carries out visual recognition based on the visual features extracted by CNN and the position encoding information provided by the position self-enhanced encoder. The iterative semantic recognition branch, which consists of a language recognition module and a cross-modal fusion gate, simulates the way that human recognizes scene text and integrates cross-modal visual cues for text recognition. The experiments demonstrate that the proposed CMFN algorithm achieves comparable performance to state-of-the-art algorithms, indicating its effectiveness.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：GPT4Ego: Unleashing the Potential of Pre-trained Models for Zero-Shot  Egocentric Action Recognition</b></summary>
  <p><b>编号</b>：[74]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10039</p>
  <p><b>作者</b>：Guangzhao Dai,  Xiangbo Shu,  Wenhao Wu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Vision-Language Models, shown impressive performance, visual recognition tasks, Egocentric Action Recognition, shown impressive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision-Language Models (VLMs), pre-trained on large-scale datasets, have shown impressive performance in various visual recognition tasks. This advancement paves the way for notable performance in Zero-Shot Egocentric Action Recognition (ZS-EAR). Typically, VLMs handle ZS-EAR as a global video-text matching task, which often leads to suboptimal alignment of vision and linguistic knowledge. We propose a refined approach for ZS-EAR using VLMs, emphasizing fine-grained concept-description alignment that capitalizes on the rich semantic and contextual details in egocentric videos. In this paper, we introduce GPT4Ego, a straightforward yet remarkably potent VLM framework for ZS-EAR, designed to enhance the fine-grained alignment of concept and description between vision and language. Extensive experiments demonstrate GPT4Ego significantly outperforms existing VLMs on three large-scale egocentric video benchmarks, i.e., EPIC-KITCHENS-100 (33.2%, +9.4%), EGTEA (39.6%, +5.5%), and CharadesEgo (31.5%, +2.6%).</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Depth Over RGB: Automatic Evaluation of Open Surgery Skills Using Depth  Camera</b></summary>
  <p><b>编号</b>：[75]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10037</p>
  <p><b>作者</b>：Ido Zuckerman,  Nicole Werner,  Jonathan Kouchly,  Emma Huston,  Shannon DiMarco,  Paul DiMusto,  Shlomi Laufer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：open surgery skills, depth cameras, RGB cameras, cameras, open surgery</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Purpose: In this paper, we present a novel approach to the automatic evaluation of open surgery skills using depth cameras. This work is intended to show that depth cameras achieve similar results to RGB cameras, which is the common method in the automatic evaluation of open surgery skills. Moreover, depth cameras offer advantages such as robustness to lighting variations, camera positioning, simplified data compression, and enhanced privacy, making them a promising alternative to RGB cameras.
Methods: Experts and novice surgeons completed two simulators of open suturing. We focused on hand and tool detection, and action segmentation in suturing procedures. YOLOv8 was used for tool detection in RGB and depth videos. Furthermore, UVAST and MSTCN++ were used for action segmentation. Our study includes the collection and annotation of a dataset recorded with Azure Kinect.
Results: We demonstrated that using depth cameras in object detection and action segmentation achieves comparable results to RGB cameras. Furthermore, we analyzed 3D hand path length, revealing significant differences between experts and novice surgeons, emphasizing the potential of depth cameras in capturing surgical skills. We also investigated the influence of camera angles on measurement accuracy, highlighting the advantages of 3D cameras in providing a more accurate representation of hand movements.
Conclusion: Our research contributes to advancing the field of surgical skill assessment by leveraging depth cameras for more reliable and privacy evaluations. The findings suggest that depth cameras can be valuable in assessing surgical skills and provide a foundation for future research in this area.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Text Region Multiple Information Perception Network for Scene Text  Detection</b></summary>
  <p><b>编号</b>：[82]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10017</p>
  <p><b>作者</b>：Jinzhi Zheng,  Libo Zhang,  Yanjun Wu,  Chen Zhao</p>
  <p><b>备注</b>：Accepted to ICASSP 2024</p>
  <p><b>关键词</b>：attracted wide attention, handle arbitrary shape, arbitrary shape scene, scene text detection, Segmentation-based scene text</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Segmentation-based scene text detection algorithms can handle arbitrary shape scene texts and have strong robustness and adaptability, so it has attracted wide attention. Existing segmentation-based scene text detection algorithms usually only segment the pixels in the center region of the text, while ignoring other information of the text region, such as edge information, distance information, etc., thus limiting the detection accuracy of the algorithm for scene text. This paper proposes a plug-and-play module called the Region Multiple Information Perception Module (RMIPM) to enhance the detection performance of segmentation-based algorithms. Specifically, we design an improved module that can perceive various types of information about scene text regions, such as text foreground classification maps, distance maps, direction maps, etc. Experiments on MSRA-TD500 and TotalText datasets show that our method achieves comparable performance with current state-of-the-art algorithms.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：CPCL: Cross-Modal Prototypical Contrastive Learning for Weakly  Supervised Text-based Person Re-Identification</b></summary>
  <p><b>编号</b>：[86]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10011</p>
  <p><b>作者</b>：Yanwei Zheng,  Xinpeng Zhao,  Chuanlin Lan,  Xiaowei Zhang,  Bowen Huang,  Jibin Yang,  Dongxiao Yu</p>
  <p><b>备注</b>：9 pages, 6 figures</p>
  <p><b>关键词</b>：text-based person re-identification, seeks to retrieve, challenging and practical, supervised text-based person, Weakly supervised text-based</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Weakly supervised text-based person re-identification (TPRe-ID) seeks to retrieve images of a target person using textual descriptions, without relying on identity annotations and is more challenging and practical. The primary challenge is the intra-class differences, encompassing intra-modal feature variations and cross-modal semantic gaps. Prior works have focused on instance-level samples and ignored prototypical features of each person which are intrinsic and invariant. Toward this, we propose a Cross-Modal Prototypical Contrastive Learning (CPCL) method. In practice, the CPCL introduces the CLIP model to weakly supervised TPRe-ID for the first time, mapping visual and textual instances into a shared latent space. Subsequently, the proposed Prototypical Multi-modal Memory (PMM) module captures associations between heterogeneous modalities of image-text pairs belonging to the same person through the Hybrid Cross-modal Matching (HCM) module in a many-to-many mapping fashion. Moreover, the Outlier Pseudo Label Mining (OPLM) module further distinguishes valuable outlier samples from each modality, enhancing the creation of more reliable clusters by mining implicit relationships between image-text pairs. Experimental results demonstrate that our proposed CPCL attains state-of-the-art performance on all three public datasets, with a significant improvement of 11.58%, 8.77% and 5.25% in Rank@1 accuracy on CUHK-PEDES, ICFG-PEDES and RSTPReid datasets, respectively. The code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and  Visual Question Generation</b></summary>
  <p><b>编号</b>：[89]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10005</p>
  <p><b>作者</b>：Kohei Uehara,  Nabarun Goswami,  Hanqin Wang,  Toshiaki Baba,  Kohtaro Tanaka,  Tomohiro Hashimoto,  Kai Wang,  Rei Ito,  Takagi Naoya,  Ryo Umagami,  Yingyi Wen,  Tanachai Anakewat,  Tatsuya Harada</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Multi-Modal Models, visual content requires, Large Language Model, increasing demand, demand for intelligent</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The increasing demand for intelligent systems capable of interpreting and reasoning about visual content requires the development of Large Multi-Modal Models (LMMs) that are not only accurate but also have explicit reasoning capabilities. This paper presents a novel approach to imbue an LMM with the ability to conduct explicit reasoning based on visual content and textual instructions. We introduce a system that can ask a question to acquire necessary knowledge, thereby enhancing the robustness and explicability of the reasoning process. Our method comprises the development of a novel dataset generated by a Large Language Model (LLM), designed to promote chain-of-thought reasoning combined with a question-asking mechanism. We designed an LMM, which has high capabilities on region awareness to address the intricate requirements of image-text alignment. The model undergoes a three-stage training phase, starting with large-scale image-text alignment using a large-scale datasets, followed by instruction tuning, and fine-tuning with a focus on chain-of-thought reasoning. The results demonstrate a stride toward a more robust, accurate, and interpretable LMM, capable of reasoning explicitly and seeking information proactively when confronted with ambiguous visual input.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：BPDO:Boundary Points Dynamic Optimization for Arbitrary Shape Scene Text  Detection</b></summary>
  <p><b>编号</b>：[91]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09997</p>
  <p><b>作者</b>：Jinzhi Zheng,  Libo Zhang,  Yanjun Wu,  Chen Zhao</p>
  <p><b>备注</b>：Accepted to ICASSP 2024</p>
  <p><b>关键词</b>：Arbitrary shape scene, scene understanding tasks, shape scene text, scene text detection, Arbitrary shape</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Arbitrary shape scene text detection is of great importance in scene understanding tasks. Due to the complexity and diversity of text in natural scenes, existing scene text algorithms have limited accuracy for detecting arbitrary shape text. In this paper, we propose a novel arbitrary shape scene text detector through boundary points dynamic optimization(BPDO). The proposed model is designed with a text aware module (TAM) and a boundary point dynamic optimization module (DOM). Specifically, the model designs a text aware module based on segmentation to obtain boundary points describing the central region of the text by extracting a priori information about the text region. Then, based on the idea of deformable attention, it proposes a dynamic optimization model for boundary points, which gradually optimizes the exact position of the boundary points based on the information of the adjacent region of each boundary point. Experiments on CTW-1500, Total-Text, and MSRA-TD500 datasets show that the model proposed in this paper achieves a performance that is better than or comparable to the state-of-the-art algorithm, proving the effectiveness of the model.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Developing an AI-based Integrated System for Bee Health Evaluation</b></summary>
  <p><b>编号</b>：[93]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09988</p>
  <p><b>作者</b>：Andrew Liang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：world food supply, past decade due, Honey bees pollinate, food supply, including pesticides</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Honey bees pollinate about one-third of the world's food supply, but bee colonies have alarmingly declined by nearly 40% over the past decade due to several factors, including pesticides and pests. Traditional methods for monitoring beehives, such as human inspection, are subjective, disruptive, and time-consuming. To overcome these limitations, artificial intelligence has been used to assess beehive health. However, previous studies have lacked an end-to-end solution and primarily relied on data from a single source, either bee images or sounds. This study introduces a comprehensive system consisting of bee object detection and health evaluation. Additionally, it utilized a combination of visual and audio signals to analyze bee behaviors. An Attention-based Multimodal Neural Network (AMNN) was developed to adaptively focus on key features from each type of signal for accurate bee health assessment. The AMNN achieved an overall accuracy of 92.61%, surpassing eight existing single-signal Convolutional Neural Networks and Recurrent Neural Networks. It outperformed the best image-based model by 32.51% and the top sound-based model by 13.98% while maintaining efficient processing times. Furthermore, it improved prediction robustness, attaining an F1-score higher than 90% across all four evaluated health conditions. The study also shows that audio signals are more reliable than images for assessing bee health. By seamlessly integrating AMNN with image and sound data in a comprehensive bee health monitoring system, this approach provides a more efficient and non-invasive solution for the early detection of bee diseases and the preservation of bee colonies.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：WorldDreamer: Towards General World Models for Video Generation via  Predicting Masked Tokens</b></summary>
  <p><b>编号</b>：[96]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09985</p>
  <p><b>作者</b>：Xiaofeng Wang,  Zheng Zhu,  Guan Huang,  Boyuan Wang,  Xinze Chen,  Jiwen Lu</p>
  <p><b>备注</b>：project page: this https URL</p>
  <p><b>关键词</b>：World models play, World, play a crucial, crucial role, role in understanding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>World models play a crucial role in understanding and predicting the dynamics of the world, which is essential for video generation. However, existing world models are confined to specific scenarios such as gaming or driving, limiting their ability to capture the complexity of general world dynamic environments. Therefore, we introduce WorldDreamer, a pioneering world model to foster a comprehensive comprehension of general world physics and motions, which significantly enhances the capabilities of video generation. Drawing inspiration from the success of large language models, WorldDreamer frames world modeling as an unsupervised visual sequence modeling challenge. This is achieved by mapping visual inputs to discrete tokens and predicting the masked ones. During this process, we incorporate multi-modal prompts to facilitate interaction within the world model. Our experiments show that WorldDreamer excels in generating videos across different scenarios, including natural scenes and driving environments. WorldDreamer showcases versatility in executing tasks such as text-to-video conversion, image-tovideo synthesis, and video editing. These results underscore WorldDreamer's effectiveness in capturing dynamic elements within diverse general world environments.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects</b></summary>
  <p><b>编号</b>：[105]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09962</p>
  <p><b>作者</b>：Zhao Wang,  Aoxue Li,  Enze Xie,  Lingting Zhu,  Yong Guo,  Qi Dou,  Zhenguo Li</p>
  <p><b>备注</b>：10 pages, 7 figures, 5 tables</p>
  <p><b>关键词</b>：text prompts, high-quality videos guided, generate high-quality videos, subjects, multiple subjects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Customized text-to-video generation aims to generate high-quality videos guided by text prompts and subject references. Current approaches designed for single subjects suffer from tackling multiple subjects, which is a more challenging and practical scenario. In this work, we aim to promote multi-subject guided text-to-video customization. We propose CustomVideo, a novel framework that can generate identity-preserving videos with the guidance of multiple subjects. To be specific, firstly, we encourage the co-occurrence of multiple subjects via composing them in a single image. Further, upon a basic text-to-video diffusion model, we design a simple yet effective attention control strategy to disentangle different subjects in the latent space of diffusion model. Moreover, to help the model focus on the specific object area, we segment the object from given reference images and provide a corresponding object mask for attention learning. Also, we collect a multi-subject text-to-video generation dataset as a comprehensive benchmark, with 69 individual subjects and 57 meaningful pairs. Extensive qualitative, quantitative, and user study results demonstrate the superiority of our method, compared with the previous state-of-the-art approaches.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Multi-task Learning for Joint Re-identification, Team Affiliation, and  Role Classification for Sports Visual Tracking</b></summary>
  <p><b>编号</b>：[113]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09942</p>
  <p><b>作者</b>：Amir M. Mansourian,  Vladimir Somers,  Christophe De Vleeschouwer,  Shohreh Kasaei</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：analyzing soccer videos, soccer videos, essential for analyzing, analyzing soccer, tracking</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Effective tracking and re-identification of players is essential for analyzing soccer videos. But, it is a challenging task due to the non-linear motion of players, the similarity in appearance of players from the same team, and frequent occlusions. Therefore, the ability to extract meaningful embeddings to represent players is crucial in developing an effective tracking and re-identification system. In this paper, a multi-purpose part-based person representation method, called PRTreID, is proposed that performs three tasks of role classification, team affiliation, and re-identification, simultaneously. In contrast to available literature, a single network is trained with multi-task supervision to solve all three tasks, jointly. The proposed joint method is computationally efficient due to the shared backbone. Also, the multi-task learning leads to richer and more discriminative representations, as demonstrated by both quantitative and qualitative results. To demonstrate the effectiveness of PRTreID, it is integrated with a state-of-the-art tracking method, using a part-based post-processing module to handle long-term tracking. The proposed tracking method outperforms all existing tracking methods on the challenging SoccerNet tracking dataset.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：ICGNet: A Unified Approach for Instance-Centric Grasping</b></summary>
  <p><b>编号</b>：[115]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09939</p>
  <p><b>作者</b>：René Zurbrügg,  Yifan Liu,  Francis Engelmann,  Suryansh Kumar,  Marco Hutter,  Vaishakh Patil,  Fisher Yu</p>
  <p><b>备注</b>：7 pages, 5 figures</p>
  <p><b>关键词</b>：robotic tasks including, tasks including assembly, robotic tasks, household robotics, tasks including</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Accurate grasping is the key to several robotic tasks including assembly and household robotics. Executing a successful grasp in a cluttered environment requires multiple levels of scene understanding: First, the robot needs to analyze the geometric properties of individual objects to find feasible grasps. These grasps need to be compliant with the local object geometry. Second, for each proposed grasp, the robot needs to reason about the interactions with other objects in the scene. Finally, the robot must compute a collision-free grasp trajectory while taking into account the geometry of the target object. Most grasp detection algorithms directly predict grasp poses in a monolithic fashion, which does not capture the composability of the environment. In this paper, we introduce an end-to-end architecture for object-centric grasping. The method uses pointcloud data from a single arbitrary viewing direction as an input and generates an instance-centric representation for each partially observed object in the scene. This representation is further used for object reconstruction and grasp detection in cluttered table-top scenes. We show the effectiveness of the proposed method by extensively evaluating it against state-of-the-art methods on synthetic datasets, indicating superior performance for grasping and reconstruction. Additionally, we demonstrate real-world applicability by decluttering scenes with varying numbers of objects.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：MAMBA: Multi-level Aggregation via Memory Bank for Video Object  Detection</b></summary>
  <p><b>编号</b>：[118]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09923</p>
  <p><b>作者</b>：Guanxiong Sun,  Yang Hua,  Guosheng Hu,  Neil Robertson</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：video object detection, object detection methods, detection methods maintain, attention mechanisms, object detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>State-of-the-art video object detection methods maintain a memory structure, either a sliding window or a memory queue, to enhance the current frame using attention mechanisms. However, we argue that these memory structures are not efficient or sufficient because of two implied operations: (1) concatenating all features in memory for enhancement, leading to a heavy computational cost; (2) frame-wise memory updating, preventing the memory from capturing more temporal information. In this paper, we propose a multi-level aggregation architecture via memory bank called MAMBA. Specifically, our memory bank employs two novel operations to eliminate the disadvantages of existing methods: (1) light-weight key-set construction which can significantly reduce the computational cost; (2) fine-grained feature-wise updating strategy which enables our method to utilize knowledge from the whole video. To better enhance features from complementary levels, i.e., feature maps and proposals, we further propose a generalized enhancement operation (GEO) to aggregate multi-level features in a unified manner. We conduct extensive evaluations on the challenging ImageNetVID dataset. Compared with existing state-of-the-art methods, our method achieves superior performance in terms of both speed and accuracy. More remarkably, MAMBA achieves mAP of 83.7/84.6% at 12.6/9.1 FPS with ResNet-101. Code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：BlenDA: Domain Adaptive Object Detection through diffusion-based  blending</b></summary>
  <p><b>编号</b>：[119]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09921</p>
  <p><b>作者</b>：Tzuhsuan Huang,  Chen-Che Huang,  Chung-Hao Ku,  Jun-Cheng Chen</p>
  <p><b>备注</b>：ICASSP(2024):2024 IEEE International Conference on Acoustics, Speech and Signal Processing</p>
  <p><b>关键词</b>：domain adaptive object, Unsupervised domain adaptation, labeled data, unlabeled data, adaptive object detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Unsupervised domain adaptation (UDA) aims to transfer a model learned using labeled data from the source domain to unlabeled data in the target domain. To address the large domain gap issue between the source and target domains, we propose a novel regularization method for domain adaptive object detection, BlenDA, by generating the pseudo samples of the intermediate domains and their corresponding soft domain labels for adaptation training. The intermediate samples are generated by dynamically blending the source images with their corresponding translated images using an off-the-shelf pre-trained text-to-image diffusion model which takes the text label of the target domain as input and has demonstrated superior image-to-image translation quality. Based on experimental results from two adaptation benchmarks, our proposed approach can significantly enhance the performance of the state-of-the-art domain adaptive object detector, Adversarial Query Transformer (AQT). Particularly, in the Cityscapes to Foggy Cityscapes adaptation, we achieve an impressive 53.4% mAP on the Foggy Cityscapes dataset, surpassing the previous state-of-the-art by 1.5%. It is worth noting that our proposed method is also applicable to various paradigms of domain adaptive object detection. The code is available at:this https URL</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：XAI-Enhanced Semantic Segmentation Models for Visual Quality Inspection</b></summary>
  <p><b>编号</b>：[125]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09900</p>
  <p><b>作者</b>：Tobias Clement,  Truong Thanh Hung Nguyen,  Mohamed Abdelaal,  Hung Cao</p>
  <p><b>备注</b>：IEEE ICCE 2024</p>
  <p><b>关键词</b>：rapid defect detection, employ computer vision, Visual quality inspection, crucial in sectors, manufacturing and logistics</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Visual quality inspection systems, crucial in sectors like manufacturing and logistics, employ computer vision and machine learning for precise, rapid defect detection. However, their unexplained nature can hinder trust, error identification, and system improvement. This paper presents a framework to bolster visual quality inspection by using CAM-based explanations to refine semantic segmentation models. Our approach consists of 1) Model Training, 2) XAI-based Model Explanation, 3) XAI Evaluation, and 4) Annotation Augmentation for Model Enhancement, informed by explanations and expert insights. Evaluations show XAI-enhanced models surpass original DeepLabv3-ResNet101 models, especially in intricate object segmentation.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Skeleton-Guided Instance Separation for Fine-Grained Segmentation in  Microscopy</b></summary>
  <p><b>编号</b>：[128]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09895</p>
  <p><b>作者</b>：Jun Wang,  Chengfeng Zhou,  Zhaoyan Ming,  Lina Wei,  Xudong Jiang,  Dahong Qian</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：segmenting cluster regions, arbitrary orientations, segmenting cluster, multiple objects, objects of varying</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>One of the fundamental challenges in microscopy (MS) image analysis is instance segmentation (IS), particularly when segmenting cluster regions where multiple objects of varying sizes and shapes may be connected or even overlapped in arbitrary orientations. Existing IS methods usually fail in handling such scenarios, as they rely on coarse instance representations such as keypoints and horizontal bounding boxes (h-bboxes). In this paper, we propose a novel one-stage framework named A2B-IS to address this challenge and enhance the accuracy of IS in MS images. Our approach represents each instance with a pixel-level mask map and a rotated bounding box (r-bbox). Unlike two-stage methods that use box proposals for segmentations, our method decouples mask and box predictions, enabling simultaneous processing to streamline the model pipeline. Additionally, we introduce a Gaussian skeleton map to aid the IS task in two key ways: (1) It guides anchor placement, reducing computational costs while improving the model's capacity to learn RoI-aware features by filtering out noise from background regions. (2) It ensures accurate isolation of densely packed instances by rectifying erroneous box predictions near instance boundaries. To further enhance the performance, we integrate two modules into the framework: (1) An Atrous Attention Block (A2B) designed to extract high-resolution feature maps with fine-grained multiscale information, and (2) A Semi-Supervised Learning (SSL) strategy that leverages both labeled and unlabeled images for model training. Our method has been thoroughly validated on two large-scale MS datasets, demonstrating its superiority over most state-of-the-art approaches.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Question-Answer Cross Language Image Matching for Weakly Supervised  Semantic Segmentation</b></summary>
  <p><b>编号</b>：[132]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09883</p>
  <p><b>作者</b>：Songhe Deng,  Wei Zhuo,  Jinheng Xie,  Linlin Shen</p>
  <p><b>备注</b>：ACM MM 2023</p>
  <p><b>关键词</b>：supervised semantic segmentation, weakly supervised semantic, Class Activation Map, background regions, semantic segmentation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Class Activation Map (CAM) has emerged as a popular tool for weakly supervised semantic segmentation (WSSS), allowing the localization of object regions in an image using only image-level labels. However, existing CAM methods suffer from under-activation of target object regions and false-activation of background regions due to the fact that a lack of detailed supervision can hinder the model's ability to understand the image as a whole. In this paper, we propose a novel Question-Answer Cross-Language-Image Matching framework for WSSS (QA-CLIMS), leveraging the vision-language foundation model to maximize the text-based understanding of images and guide the generation of activation maps. First, a series of carefully designed questions are posed to the VQA (Visual Question Answering) model with Question-Answer Prompt Engineering (QAPE) to generate a corpus of both foreground target objects and backgrounds that are adaptive to query images. We then employ contrastive learning in a Region Image Text Contrastive (RITC) network to compare the obtained foreground and background regions with the generated corpus. Our approach exploits the rich textual information from the open vocabulary as additional supervision, enabling the model to generate high-quality CAMs with a more complete object region and reduce false-activation of background regions. We conduct extensive analysis to validate the proposed method and show that our approach performs state-of-the-art on both PASCAL VOC 2012 and MS COCO datasets. Code is available at: this https URL</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Boosting Few-Shot Segmentation via Instance-Aware Data Augmentation and  Local Consensus Guided Cross Attention</b></summary>
  <p><b>编号</b>：[138]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09866</p>
  <p><b>作者</b>：Li Guo,  Haoming Liu,  Yuxuan Xia,  Chengyu Zhang,  Xiaochen Lu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Few-shot segmentation aims, aims to train, fast adapt, Few-shot segmentation, standard few-shot segmentation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Few-shot segmentation aims to train a segmentation model that can fast adapt to a novel task for which only a few annotated images are provided. Most recent models have adopted a prototype-based paradigm for few-shot inference. These approaches may have limited generalization capacity beyond the standard 1- or 5-shot settings. In this paper, we closely examine and reevaluate the fine-tuning based learning scheme that fine-tunes the classification layer of a deep segmentation network pre-trained on diverse base classes. To improve the generalizability of the classification layer optimized with sparsely annotated samples, we introduce an instance-aware data augmentation (IDA) strategy that augments the support images based on the relative sizes of the target objects. The proposed IDA effectively increases the support set's diversity and promotes the distribution consistency between support and query images. On the other hand, the large visual difference between query and support images may hinder knowledge transfer and cripple the segmentation performance. To cope with this challenge, we introduce the local consensus guided cross attention (LCCA) to align the query feature with support features based on their dense correlation, further improving the model's generalizability to the query image. The significant performance improvements on the standard few-shot segmentation benchmarks PASCAL-$5^i$ and COCO-$20^i$ verify the efficacy of our proposed method.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Improving fine-grained understanding in image-text pre-training</b></summary>
  <p><b>编号</b>：[139]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09865</p>
  <p><b>作者</b>：Ioana Bica,  Anastasija Ilić,  Matthias Bauer,  Goker Erdogan,  Matko Bošnjak,  Christos Kaplanis,  Alexey A. Gritsenko,  Matthias Minderer,  Charles Blundell,  Razvan Pascanu,  Jovana Mitrović</p>
  <p><b>备注</b>：26 pages</p>
  <p><b>关键词</b>：Fine-grained Contrastive Alignment, Contrastive Alignment, image-text pairs, introduce SPARse Fine-grained, image patches</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce SPARse Fine-grained Contrastive Alignment (SPARC), a simple method for pretraining more fine-grained multimodal representations from image-text pairs. Given that multiple image patches often correspond to single words, we propose to learn a grouping of image patches for every token in the caption. To achieve this, we use a sparse similarity metric between image patches and language tokens and compute for each token a language-grouped vision embedding as the weighted average of patches. The token and language-grouped vision embeddings are then contrasted through a fine-grained sequence-wise loss that only depends on individual samples and does not require other batch samples as negatives. This enables more detailed information to be learned in a computationally inexpensive manner. SPARC combines this fine-grained loss with a contrastive loss between global image and text embeddings to learn representations that simultaneously encode global and local information. We thoroughly evaluate our proposed method and show improved performance over competing approaches both on image-level tasks relying on coarse-grained information, e.g. classification, as well as region-level tasks relying on fine-grained information, e.g. retrieval, object detection, and segmentation. Moreover, SPARC improves model faithfulness and captioning in foundational vision-language models.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Temporal Insight Enhancement: Mitigating Temporal Hallucination in  Multimodal Large Language Models</b></summary>
  <p><b>编号</b>：[141]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09861</p>
  <p><b>作者</b>：Li Sun,  Liuan Wang,  Jun Sun,  Takayuki Okatani</p>
  <p><b>备注</b>：7 pages, 7 figures</p>
  <p><b>关键词</b>：Multimodal Large Language, Large Language Models, Multimodal Large, Large Language, advancements in Multimodal</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced the comprehension of multimedia content, bringing together diverse modalities such as text, images, and videos. However, a critical challenge faced by these models, especially when processing video inputs, is the occurrence of hallucinations - erroneous perceptions or interpretations, particularly at the event level. This study introduces an innovative method to address event-level hallucinations in MLLMs, focusing on specific temporal understanding in video content. Our approach leverages a novel framework that extracts and utilizes event-specific information from both the event query and the provided video to refine MLLMs' response. We propose a unique mechanism that decomposes on-demand event queries into iconic actions. Subsequently, we employ models like CLIP and BLIP2 to predict specific timestamps for event occurrences. Our evaluation, conducted using the Charades-STA dataset, demonstrates a significant reduction in temporal hallucinations and an improvement in the quality of event-related responses. This research not only provides a new perspective in addressing a critical limitation of MLLMs but also contributes a quantitatively measurable method for evaluating MLLMs in the context of temporal-related questions.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Enhancing the Fairness and Performance of Edge Cameras with Explainable  AI</b></summary>
  <p><b>编号</b>：[148]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09852</p>
  <p><b>作者</b>：Truong Thanh Hung Nguyen,  Vo Thanh Khang Nguyen,  Quoc Hung Cao,  Van Binh Truong,  Quoc Khanh Nguyen,  Hung Cao</p>
  <p><b>备注</b>：IEEE ICCE 2024</p>
  <p><b>关键词</b>：Artificial Intelligence, Edge camera systems, challenging to interpret, interpret and debug, human detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rising use of Artificial Intelligence (AI) in human detection on Edge camera systems has led to accurate but complex models, challenging to interpret and debug. Our research presents a diagnostic method using Explainable AI (XAI) for model debugging, with expert-driven problem identification and solution creation. Validated on the Bytetrack model in a real-world office Edge network, we found the training dataset as the main bias source and suggested model augmentation as a solution. Our approach helps identify model biases, essential for achieving fair and trustworthy models.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Exploring Latent Cross-Channel Embedding for Accurate 3D Human Pose  Reconstruction in a Diffusion Framework</b></summary>
  <p><b>编号</b>：[152]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09836</p>
  <p><b>作者</b>：Junkun Jiang,  Jie Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：pose estimation poses, human pose estimation, inherent depth ambiguities, estimation poses significant, human pose</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Monocular 3D human pose estimation poses significant challenges due to the inherent depth ambiguities that arise during the reprojection process from 2D to 3D. Conventional approaches that rely on estimating an over-fit projection matrix struggle to effectively address these challenges and often result in noisy outputs. Recent advancements in diffusion models have shown promise in incorporating structural priors to address reprojection ambiguities. However, there is still ample room for improvement as these methods often overlook the exploration of correlation between the 2D and 3D joint-level features. In this study, we propose a novel cross-channel embedding framework that aims to fully explore the correlation between joint-level features of 3D coordinates and their 2D projections. In addition, we introduce a context guidance mechanism to facilitate the propagation of joint graph attention across latent channels during the iterative diffusion process. To evaluate the effectiveness of our proposed method, we conduct experiments on two benchmark datasets, namely Human3.6M and MPI-INF-3DHP. Our results demonstrate a significant improvement in terms of reconstruction accuracy compared to state-of-the-art methods. The code for our method will be made available online for further reference.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Enhanced Automated Quality Assessment Network for Interactive Building  Segmentation in High-Resolution Remote Sensing Imagery</b></summary>
  <p><b>编号</b>：[155]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09828</p>
  <p><b>作者</b>：Zhili Zhang,  Xiangyun Hu,  Jiabo Xu</p>
  <p><b>备注</b>：The manuscript is submitted to IEEE International Geoscience and Remote Sensing Symposium(IGARSS2024)</p>
  <p><b>关键词</b>：high-resolution remote sensing, enhanced automated quality, remote sensing imagery, quality assessment network, automated quality assessment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this research, we introduce the enhanced automated quality assessment network (IBS-AQSNet), an innovative solution for assessing the quality of interactive building segmentation within high-resolution remote sensing imagery. This is a new challenge in segmentation quality assessment, and our proposed IBS-AQSNet allievate this by identifying missed and mistaken segment areas. First of all, to acquire robust image features, our method combines a robust, pre-trained backbone with a lightweight counterpart for comprehensive feature extraction from imagery and segmentation results. These features are then fused through a simple combination of concatenation, convolution layers, and residual connections. Additionally, ISR-AQSNet incorporates a multi-scale differential quality assessment decoder, proficient in pinpointing areas where segmentation result is either missed or mistaken. Experiments on a newly-built EVLab-BGZ dataset, which includes over 39,198 buildings, demonstrate the superiority of the proposed method in automating segmentation quality assessment, thereby setting a new benchmark in the field.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Boosting Few-Shot Semantic Segmentation Via Segment Anything Model</b></summary>
  <p><b>编号</b>：[156]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09826</p>
  <p><b>作者</b>：Chen-Bin Feng,  Qi Lai,  Kangdao Liu,  Houcheng Su,  Chi-Man Vong</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：medical image analysis, semantic segmentation, image editing, few-shot semantic segmentation, medical image</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In semantic segmentation, accurate prediction masks are crucial for downstream tasks such as medical image analysis and image editing. Due to the lack of annotated data, few-shot semantic segmentation (FSS) performs poorly in predicting masks with precise contours. Recently, we have noticed that the large foundation model segment anything model (SAM) performs well in processing detailed features. Inspired by SAM, we propose FSS-SAM to boost FSS methods by addressing the issue of inaccurate contour. The FSS-SAM is training-free. It works as a post-processing tool for any FSS methods and can improve the accuracy of predicted masks. Specifically, we use predicted masks from FSS methods to generate prompts and then use SAM to predict new masks. To avoid predicting wrong masks with SAM, we propose a prediction result selection (PRS) algorithm. The algorithm can remarkably decrease wrong predictions. Experiment results on public datasets show that our method is superior to base FSS methods in both quantitative and qualitative aspects.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Enhancing Small Object Encoding in Deep Neural Networks: Introducing  Fast&Focused-Net with Volume-wise Dot Product Layer</b></summary>
  <p><b>编号</b>：[158]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09823</p>
  <p><b>作者</b>：Ali Tofik,  Roy Partha Pratim</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Convolutional Neural Networks, Volume-wise Dot Product, conventional Convolutional Neural, deep neural network, neural network architecture</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we introduce Fast&Focused-Net, a novel deep neural network architecture tailored for efficiently encoding small objects into fixed-length feature vectors. Contrary to conventional Convolutional Neural Networks (CNNs), Fast&Focused-Net employs a series of our newly proposed layer, the Volume-wise Dot Product (VDP) layer, designed to address several inherent limitations of CNNs. Specifically, CNNs often exhibit a smaller effective receptive field than their theoretical counterparts, limiting their vision span. Additionally, the initial layers in CNNs produce low-dimensional feature vectors, presenting a bottleneck for subsequent learning. Lastly, the computational overhead of CNNs, particularly in capturing diverse image regions by parameter sharing, is significantly high. The VDP layer, at the heart of Fast&Focused-Net, aims to remedy these issues by efficiently covering the entire image patch information with reduced computational demand. Experimental results demonstrate the prowess of Fast&Focused-Net in a variety of applications. For small object classification tasks, our network outperformed state-of-the-art methods on datasets such as CIFAR-10, CIFAR-100, STL-10, SVHN-Cropped, and Fashion-MNIST. In the context of larger image classification, when combined with a transformer encoder (ViT), Fast&Focused-Net produced competitive results for OpenImages V6, ImageNet-1K, and Places365 datasets. Moreover, the same combination showcased unparalleled performance in text recognition tasks across SVT, IC15, SVTP, and HOST datasets. This paper presents the architecture, the underlying motivation, and extensive empirical evidence suggesting that Fast&Focused-Net is a promising direction for efficient and focused deep learning.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Wavelet-Guided Acceleration of Text Inversion in Diffusion-Based Image  Editing</b></summary>
  <p><b>编号</b>：[167]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09794</p>
  <p><b>作者</b>：Gwanhyeong Koo,  Sunjae Yoon,  Chang D. Yoo</p>
  <p><b>备注</b>：The International Conference on Acoustics, Speech, & Signal Processing (ICASSP) 2024</p>
  <p><b>关键词</b>：optimizing null embeddings, DDIM sampling process, enables fine-grained editing, Null-text Inversion, NTI</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the field of image editing, Null-text Inversion (NTI) enables fine-grained editing while preserving the structure of the original image by optimizing null embeddings during the DDIM sampling process. However, the NTI process is time-consuming, taking more than two minutes per image. To address this, we introduce an innovative method that maintains the principles of the NTI while accelerating the image editing process. We propose the WaveOpt-Estimator, which determines the text optimization endpoint based on frequency characteristics. Utilizing wavelet transform analysis to identify the image's frequency characteristics, we can limit text optimization to specific timesteps during the DDIM sampling process. By adopting the Negative-Prompt Inversion (NPI) concept, a target prompt representing the original image serves as the initial text value for optimization. This approach maintains performance comparable to NTI while reducing the average editing time by over 80% compared to the NTI method. Our method presents a promising approach for efficient, high-quality image editing based on diffusion models.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：Adaptive Self-training Framework for Fine-grained Scene Graph Generation</b></summary>
  <p><b>编号</b>：[171]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09786</p>
  <p><b>作者</b>：Kibum Kim,  Kanghoon Yoon,  Yeonjun In,  Jinyoung Moon,  Donghyun Kim,  Chanyoung Park</p>
  <p><b>备注</b>：9 pages; ICLR 2024</p>
  <p><b>关键词</b>：missing annotation problems, Scene graph generation, SGG models, SGG, benchmark datasets</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Scene graph generation (SGG) models have suffered from inherent problems regarding the benchmark datasets such as the long-tailed predicate distribution and missing annotation problems. In this work, we aim to alleviate the long-tailed problem of SGG by utilizing unannotated triplets. To this end, we introduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels for unannotated triplets based on which the SGG models are trained. While there has been significant progress in self-training for image recognition, designing a self-training framework for the SGG task is more challenging due to its inherent nature such as the semantic ambiguity and the long-tailed distribution of predicate classes. Hence, we propose a novel pseudo-labeling technique for SGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is a model-agnostic framework that can be applied to any existing SGG models. Furthermore, we devise a graph structure learner (GSL) that is beneficial when adopting our proposed self-training framework to the state-of-the-art message-passing neural network (MPNN)-based SGG models. Our extensive experiments verify the effectiveness of ST-SGG on various SGG models, particularly in enhancing the performance on fine-grained predicate classes.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：On the Audio Hallucinations in Large Audio-Video Language Models</b></summary>
  <p><b>编号</b>：[175]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09774</p>
  <p><b>作者</b>：Taichi Nishimura,  Shota Nakada,  Masayoshi Kondo</p>
  <p><b>备注</b>：6 pages</p>
  <p><b>关键词</b>：Large audio-video language, audio-video language models, audio-video language, Large audio-video, audio</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large audio-video language models can generate descriptions for both video and audio. However, they sometimes ignore audio content, producing audio descriptions solely reliant on visual information. This paper refers to this as audio hallucinations and analyzes them in large audio-video language models. We gather 1,000 sentences by inquiring about audio information and annotate them whether they contain hallucinations. If a sentence is hallucinated, we also categorize the type of hallucination. The results reveal that 332 sentences are hallucinated with distinct trends observed in nouns and verbs for each hallucination type. Based on this, we tackle a task of audio hallucination classification using pre-trained audio-text models in the zero-shot and fine-tuning settings. Our experimental results reveal that the zero-shot models achieve higher performance (52.2% in F1) than the random (40.3%) and the fine-tuning models achieve 87.9%, outperforming the zero-shot models.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：SEINE: Structure Encoding and Interaction Network for Nuclei Instance  Segmentation</b></summary>
  <p><b>编号</b>：[176]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09773</p>
  <p><b>作者</b>：Ye Zhang,  Linghan Cai,  Ziyue Wang,  Yongbing Zhang</p>
  <p><b>备注</b>：10 pages, 12 figures, 6 tables, submitted to TMI</p>
  <p><b>关键词</b>：Nuclei instance segmentation, Nuclei, structure, segmentation in histopathological, histopathological images</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Nuclei instance segmentation in histopathological images is of great importance for biological analysis and cancer diagnosis but remains challenging for two reasons. (1) Similar visual presentation of intranuclear and extranuclear regions of chromophobe nuclei often causes under-segmentation, and (2) current methods lack the exploration of nuclei structure, resulting in fragmented instance predictions. To address these problems, this paper proposes a structure encoding and interaction network, termed SEINE, which develops the structure modeling scheme of nuclei and exploits the structure similarity between nuclei to improve the integrality of each segmented instance. Concretely, SEINE introduces a contour-based structure encoding (SE) that considers the correlation between nuclei structure and semantics, realizing a reasonable representation of the nuclei structure. Based on the encoding, we propose a structure-guided attention (SGA) that takes the clear nuclei as prototypes to enhance the structure learning for the fuzzy nuclei. To strengthen the structural learning ability, a semantic feature fusion (SFF) is presented to boost the semantic consistency of semantic and structure branches. Furthermore, a position enhancement (PE) method is applied to suppress incorrect nuclei boundary predictions. Extensive experiments demonstrate the superiority of our approaches, and SEINE achieves state-of-the-art (SOTA) performance on four datasets. The code is available at \href{this https URL}{this https URL}.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：CLIP Model for Images to Textual Prompts Based on Top-k Neighbors</b></summary>
  <p><b>编号</b>：[182]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09763</p>
  <p><b>作者</b>：Xin Zhang,  Xin Zhang,  YeMing Cai,  Tianzhi Jia</p>
  <p><b>备注</b>：CLIP model, KNN, image-to-prompts</p>
  <p><b>关键词</b>：gained significant attention, recent years, subfield of multimodal, gained significant, significant attention</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Text-to-image synthesis, a subfield of multimodal generation, has gained significant attention in recent years. We propose a cost-effective approach for image-to-prompt generation that leverages generative models to generate textual prompts without the need for large amounts of annotated data. We divide our method into two stages: online stage and offline stage. We use a combination of the CLIP model and K-nearest neighbors (KNN) algorithm. The proposed system consists of two main parts: an offline task and an online task. Our method owns the highest metric 0.612 among these models, which is 0.013, 0.055, 0.011 higher than Clip, Clip + KNN(top 10) respectively.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：SlideAVSR: A Dataset of Paper Explanation Videos for Audio-Visual Speech  Recognition</b></summary>
  <p><b>编号</b>：[184]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09759</p>
  <p><b>作者</b>：Hao Wang,  Shuhei Kurita,  Shuichiro Shimizu,  Daisuke Kawahara</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Audio-visual speech recognition, automatic speech recognition, speech recognition, complement to audio, multimodal extension</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Audio-visual speech recognition (AVSR) is a multimodal extension of automatic speech recognition (ASR), using video as a complement to audio. In AVSR, considerable efforts have been directed at datasets for facial features such as lip-readings, while they often fall short in evaluating the image comprehension capabilities in broader contexts. In this paper, we construct SlideAVSR, an AVSR dataset using scientific paper explanation videos. SlideAVSR provides a new benchmark where models transcribe speech utterances with texts on the slides on the presentation recordings. As technical terminologies that are frequent in paper explanations are notoriously challenging to transcribe without reference texts, our SlideAVSR dataset spotlights a new aspect of AVSR problems. As a simple yet effective baseline, we propose DocWhisper, an AVSR model that can refer to textual information from slides, and confirm its effectiveness on SlideAVSR.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：Image Translation as Diffusion Visual Programmers</b></summary>
  <p><b>编号</b>：[194]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09742</p>
  <p><b>作者</b>：Cheng Han,  James C. Liang,  Qifan Wang,  Majid Rabbani,  Sohail Dianat,  Raghuveer Rao,  Ying Nian Wu,  Dongfang Liu</p>
  <p><b>备注</b>：25 pages, 20 figures</p>
  <p><b>关键词</b>：Diffusion Visual Programmer, Visual Programmer, neuro-symbolic image translation, image translation, image translation processes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce the novel Diffusion Visual Programmer (DVP), a neuro-symbolic image translation framework. Our proposed DVP seamlessly embeds a condition-flexible diffusion model within the GPT architecture, orchestrating a coherent sequence of visual programs (i.e., computer vision models) for various pro-symbolic steps, which span RoI identification, style transfer, and position manipulation, facilitating transparent and controllable image translation processes. Extensive experiments demonstrate DVP's remarkable performance, surpassing concurrent arts. This success can be attributed to several key features of DVP: First, DVP achieves condition-flexible translation via instance normalization, enabling the model to eliminate sensitivity caused by the manual guidance and optimally focus on textual descriptions for high-quality content generation. Second, the framework enhances in-context reasoning by deciphering intricate high-dimensional concepts in feature spaces into more accessible low-dimensional symbols (e.g., [Prompt], [RoI object]), allowing for localized, context-free editing while maintaining overall coherence. Last but not least, DVP improves systemic controllability and explainability by offering explicit symbolic representations at each programming stage, empowering users to intuitively interpret and modify results. Our research marks a substantial step towards harmonizing artificial image translation processes with cognitive intelligence, promising broader applications.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：Measuring the Discrepancy between 3D Geometric Models using Directional  Distance Fields</b></summary>
  <p><b>编号</b>：[196]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09736</p>
  <p><b>作者</b>：Siyu Ren,  Junhui Hou,  Xiaodong Chen,  Hongkai Xiong,  Wenping Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：triangle meshes, board applications, clouds or triangle, pivotal issue, issue with board</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Qualifying the discrepancy between 3D geometric models, which could be represented with either point clouds or triangle meshes, is a pivotal issue with board applications. Existing methods mainly focus on directly establishing the correspondence between two models and then aggregating point-wise distance between corresponding points, resulting in them being either inefficient or ineffective. In this paper, we propose DirDist, an efficient, effective, robust, and differentiable distance metric for 3D geometry data. Specifically, we construct DirDist based on the proposed implicit representation of 3D models, namely directional distance field (DDF), which defines the directional distances of 3D points to a model to capture its local surface geometry. We then transfer the discrepancy between two 3D geometric models as the discrepancy between their DDFs defined on an identical domain, naturally establishing model correspondence. To demonstrate the advantage of our DirDist, we explore various distance metric-driven 3D geometric modeling tasks, including template surface fitting, rigid registration, non-rigid registration, scene flow estimation and human pose optimization. Extensive experiments show that our DirDist achieves significantly higher accuracy under all tasks. As a generic distance metric, DirDist has the potential to advance the field of 3D geometric modeling. The source code is available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Instance Brownian Bridge as Texts for Open-vocabulary Video Instance  Segmentation</b></summary>
  <p><b>编号</b>：[198]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09732</p>
  <p><b>作者</b>：Zesen Cheng,  Kehan Li,  Hao Li,  Peng Jin,  Chang Liu,  Xiawu Zheng,  Rongrong Ji,  Jie Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Video Instance Segmentation, Temporally locating objects, Instance Segmentation, class texts, arbitrary class texts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Temporally locating objects with arbitrary class texts is the primary pursuit of open-vocabulary Video Instance Segmentation (VIS). Because of the insufficient vocabulary of video data, previous methods leverage image-text pretraining model for recognizing object instances by separately aligning each frame and class texts, ignoring the correlation between frames. As a result, the separation breaks the instance movement context of videos, causing inferior alignment between video and text. To tackle this issue, we propose to link frame-level instance representations as a Brownian Bridge to model instance dynamics and align bridge-level instance representation to class texts for more precisely open-vocabulary VIS (BriVIS). Specifically, we build our system upon a frozen video segmentor to generate frame-level instance queries, and design Temporal Instance Resampler (TIR) to generate queries with temporal context from frame queries. To mold instance queries to follow Brownian bridge and accomplish alignment with class texts, we design Bridge-Text Alignment (BTA) to learn discriminative bridge-level representations of instances via contrastive objectives. Setting MinVIS as the basic video segmentor, BriVIS surpasses the Open-vocabulary SOTA (OV2Seg) by a clear margin. For example, on the challenging large-vocabulary VIS dataset (BURST), BriVIS achieves 7.43 mAP and exhibits 49.49% improvement compared to OV2Seg (4.97 mAP).</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：fast graph-based denoising for point cloud color information</b></summary>
  <p><b>编号</b>：[203]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09721</p>
  <p><b>作者</b>：Ryosuke Watanabe,  Keisuke Nonaka,  Eduardo Pavez,  Tatsuya Kobayashi,  Antonio Ortega</p>
  <p><b>备注</b>：Published in the proceeding of 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2024)</p>
  <p><b>关键词</b>：point cloud, Point, denoising, cloud, real-time point cloud</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Point clouds are utilized in various 3D applications such as cross-reality (XR) and realistic 3D displays. In some applications, e.g., for live streaming using a 3D point cloud, real-time point cloud denoising methods are required to enhance the visual quality. However, conventional high-precision denoising methods cannot be executed in real time for large-scale point clouds owing to the complexity of graph constructions with K nearest neighbors and noise level estimation. This paper proposes a fast graph-based denoising (FGBD) for a large-scale point cloud. First, high-speed graph construction is achieved by scanning a point cloud in various directions and searching adjacent neighborhoods on the scanning lines. Second, we propose a fast noise level estimation method using eigenvalues of the covariance matrix on a graph. Finally, we also propose a new low-cost filter selection method to enhance denoising accuracy to compensate for the degradation caused by the acceleration algorithms. In our experiments, we succeeded in reducing the processing time dramatically while maintaining accuracy relative to conventional denoising methods. Denoising was performed at 30fps, with frames containing approximately 1 million points.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting</b></summary>
  <p><b>编号</b>：[204]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09720</p>
  <p><b>作者</b>：Mengtian Li,  Shengxiang Yao,  Zhifeng Xie,  Keyu Chen,  Yu-Gang Jiang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Gaussian Splatting, Gaussian Splatting model, method called GaussianBody, called GaussianBody, reconstruction method called</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we propose a novel clothed human reconstruction method called GaussianBody, based on 3D Gaussian Splatting. Compared with the costly neural radiance based models, 3D Gaussian Splatting has recently demonstrated great performance in terms of training time and rendering quality. However, applying the static 3D Gaussian Splatting model to the dynamic human reconstruction problem is non-trivial due to complicated non-rigid deformations and rich cloth details. To address these challenges, our method considers explicit pose-guided deformation to associate dynamic Gaussians across the canonical space and the observation space, introducing a physically-based prior with regularized transformations helps mitigate ambiguity between the two spaces. During the training process, we further propose a pose refinement strategy to update the pose regression for compensating the inaccurate initial estimation and a split-with-scale mechanism to enhance the density of regressed point clouds. The experiments validate that our method can achieve state-of-the-art photorealistic novel-view rendering results with high-quality details for dynamic clothed human bodies, along with explicit geometry reconstruction.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：HCVP: Leveraging Hierarchical Contrastive Visual Prompt for Domain  Generalization</b></summary>
  <p><b>编号</b>：[205]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09716</p>
  <p><b>作者</b>：Guanglin Zhou,  Zhongyi Han,  Shiming Chen,  Biwei Huang,  Liming Zhu,  Tongliang Liu,  Lina Yao,  Kun Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：create machine learning, invariant features, endeavors to create, machine learning models, create machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Domain Generalization (DG) endeavors to create machine learning models that excel in unseen scenarios by learning invariant features. In DG, the prevalent practice of constraining models to a fixed structure or uniform parameterization to encapsulate invariant features can inadvertently blend specific aspects. Such an approach struggles with nuanced differentiation of inter-domain variations and may exhibit bias towards certain domains, hindering the precise learning of domain-invariant features. Recognizing this, we introduce a novel method designed to supplement the model with domain-level and task-specific characteristics. This approach aims to guide the model in more effectively separating invariant features from specific characteristics, thereby boosting the generalization. Building on the emerging trend of visual prompts in the DG paradigm, our work introduces the novel \textbf{H}ierarchical \textbf{C}ontrastive \textbf{V}isual \textbf{P}rompt (HCVP) methodology. This represents a significant advancement in the field, setting itself apart with a unique generative approach to prompts, alongside an explicit model structure and specialized loss functions. Differing from traditional visual prompts that are often shared across entire datasets, HCVP utilizes a hierarchical prompt generation network enhanced by prompt contrastive learning. These generative prompts are instance-dependent, catering to the unique characteristics inherent to different domains and tasks. Additionally, we devise a prompt modulation network that serves as a bridge, effectively incorporating the generated visual prompts into the vision transformer backbone. Experiments conducted on five DG datasets demonstrate the effectiveness of HCVP, outperforming both established DG algorithms and adaptation protocols.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction  Tuning with Large Language Model</b></summary>
  <p><b>编号</b>：[207]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09712</p>
  <p><b>作者</b>：Yang Zhan,  Zhitong Xiong,  Yuan Yuan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：obtaining impressive general, Large language models, general multi-modal capabilities, multi-modal large language, impressive general multi-modal</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have recently been extended to the vision-language realm, obtaining impressive general multi-modal capabilities. However, the exploration of multi-modal large language models (MLLMs) for remote sensing (RS) data is still in its infancy, and the performance is not satisfactory. In this work, we introduce SkyEyeGPT, a unified multi-modal large language model specifically designed for RS vision-language understanding. To this end, we meticulously curate an RS multi-modal instruction tuning dataset, including single-task and multi-task conversation instructions. After manual verification, we obtain a high-quality RS instruction-following dataset with 968k samples. Our research demonstrates that with a simple yet effective design, SkyEyeGPT works surprisingly well on considerably different tasks without the need for extra encoding modules. Specifically, after projecting RS visual features to the language domain via an alignment layer, they are fed jointly with task-specific instructions into an LLM-based RS decoder to predict answers for RS open-ended tasks. In addition, we design a two-stage tuning method to enhance instruction-following and multi-turn dialogue ability at different granularities. Experiments on 8 datasets for RS vision-language tasks demonstrate SkyEyeGPT's superiority in image-level and region-level tasks, such as captioning and visual grounding. In particular, SkyEyeGPT exhibits encouraging results compared to GPT-4V in some qualitative tests. The online demo, code, and dataset will be released in this https URL.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：P2Seg: Pointly-supervised Segmentation via Mutual Distillation</b></summary>
  <p><b>编号</b>：[209]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09709</p>
  <p><b>作者</b>：Zipeng Wang,  Xuehui Yu,  Xumeng Han,  Wenwen Yu,  Zhixun Huang,  Jianbin Jiao,  Zhenjun Han</p>
  <p><b>备注</b>：14 pages, 12 figures, published to ICLR2024</p>
  <p><b>关键词</b>：Point-level Supervised Instance, Supervised Instance Segmentation, PSIS methods, supervised semantic segmentation, Existing PSIS methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Point-level Supervised Instance Segmentation (PSIS) aims to enhance the applicability and scalability of instance segmentation by utilizing low-cost yet instance-informative annotations. Existing PSIS methods usually rely on positional information to distinguish objects, but predicting precise boundaries remains challenging due to the lack of contour annotations. Nevertheless, weakly supervised semantic segmentation methods are proficient in utilizing intra-class feature consistency to capture the boundary contours of the same semantic regions. In this paper, we design a Mutual Distillation Module (MDM) to leverage the complementary strengths of both instance position and semantic information and achieve accurate instance-level object perception. The MDM consists of Semantic to Instance (S2I) and Instance to Semantic (I2S). S2I is guided by the precise boundaries of semantic regions to learn the association between annotated points and instance contours. I2S leverages discriminative relationships between instances to facilitate the differentiation of various objects within the semantic map. Extensive experiments substantiate the efficacy of MDM in fostering the synergy between instance and semantic information, consequently improving the quality of instance-level object representations. Our method achieves 55.7 mAP$_{50}$ and 17.6 mAP on the PASCAL VOC and MS COCO datasets, significantly outperforming recent PSIS methods and several box-supervised instance segmentation competitors.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：Eye Motion Matters for 3D Face Reconstruction</b></summary>
  <p><b>编号</b>：[223]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09677</p>
  <p><b>作者</b>：Xuan Wang,  Mengyuan Liu</p>
  <p><b>备注</b>：6 pages, 5 figures</p>
  <p><b>关键词</b>：shown remarkable progress, Recent advances, advances in single-image, face reconstruction, Local Dynamic Loss</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advances in single-image 3D face reconstruction have shown remarkable progress in various applications. Nevertheless, prevailing techniques tend to prioritize the global facial contour and expression, often neglecting the nuanced dynamics of the eye region. In response, we introduce an Eye Landmark Adjustment Module, complemented by a Local Dynamic Loss, designed to capture the dynamic features of the eyes area. Our module allows for flexible adjustment of landmarks, resulting in accurate recreation of various eye states. In this paper, we present a comprehensive evaluation of our approach, conducting extensive experiments on two datasets. The results underscore the superior performance of our approach, highlighting its significant contributions in addressing this particular challenge.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：Artwork Protection Against Neural Style Transfer Using Locally Adaptive  Adversarial Color Attack</b></summary>
  <p><b>编号</b>：[225]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09673</p>
  <p><b>作者</b>：Zhongliang Guo,  Kaixuan Wang,  Weiye Li,  Yifei Qian,  Ognjen Arandjelović,  Lei Fang</p>
  <p><b>备注</b>：9 pages, 5 figures</p>
  <p><b>关键词</b>：Neural style transfer, widely adopted, adopted in computer, computer vision, vision to generate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural style transfer (NST) is widely adopted in computer vision to generate new images with arbitrary styles. This process leverages neural networks to merge aesthetic elements of a style image with the structural aspects of a content image into a harmoniously integrated visual result. However, unauthorized NST can exploit artwork. Such misuse raises socio-technical concerns regarding artists' rights and motivates the development of technical approaches for the proactive protection of original creations. Adversarial attack is a concept primarily explored in machine learning security. Our work introduces this technique to protect artists' intellectual property. In this paper Locally Adaptive Adversarial Color Attack (LAACA), a method for altering images in a manner imperceptible to the human eyes but disruptive to NST. Specifically, we design perturbations targeting image areas rich in high-frequency content, generated by disrupting intermediate features. Our experiments and user study confirm that by attacking NST using the proposed method results in visually worse neural style transfer, thus making it an effective solution for visual artwork protection.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：Towards Identifiable Unsupervised Domain Translation: A Diversified  Distribution Matching Approach</b></summary>
  <p><b>编号</b>：[226]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09671</p>
  <p><b>作者</b>：Sagar Shrestha,  Xiao Fu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：high-level semantic meaning, Unsupervised domain translation, aims to find, semantic meaning, convert samples</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Unsupervised domain translation (UDT) aims to find functions that convert samples from one domain (e.g., sketches) to another domain (e.g., photos) without changing the high-level semantic meaning (also referred to as ``content''). The translation functions are often sought by probability distribution matching of the transformed source domain and target domain. CycleGAN stands as arguably the most representative approach among this line of work. However, it was noticed in the literature that CycleGAN and variants could fail to identify the desired translation functions and produce content-misaligned translations. This limitation arises due to the presence of multiple translation functions -- referred to as ``measure-preserving automorphism" (MPA) -- in the solution space of the learning criteria. Despite awareness of such identifiability issues, solutions have remained elusive. This study delves into the core identifiability inquiry and introduces an MPA elimination theory. Our analysis shows that MPA is unlikely to exist, if multiple pairs of diverse cross-domain conditional distributions are matched by the learning function. Our theory leads to a UDT learner using distribution matching over auxiliary variable-induced subsets of the domains -- other than over the entire data domains as in the classical approaches. The proposed framework is the first to rigorously establish translation identifiability under reasonable UDT settings, to our best knowledge. Experiments corroborate with our theoretical claims.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：Land Cover Image Classification</b></summary>
  <p><b>编号</b>：[246]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09607</p>
  <p><b>作者</b>：Antonio Rangel,  Juan Terven,  Diana M. Cordova-Esparza,  E.A. Chavez-Urbiola</p>
  <p><b>备注</b>：7 pages, 4 figures, 1 table, published in conference</p>
  <p><b>关键词</b>：Land Cover, urban planning, disaster management, increasingly significant, significant in understanding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Land Cover (LC) image classification has become increasingly significant in understanding environmental changes, urban planning, and disaster management. However, traditional LC methods are often labor-intensive and prone to human error. This paper explores state-of-the-art deep learning models for enhanced accuracy and efficiency in LC analysis. We compare convolutional neural networks (CNN) against transformer-based methods, showcasing their applications and advantages in LC studies. We used EuroSAT, a patch-based LC classification data set based on Sentinel-2 satellite images and achieved state-of-the-art results using current transformer models.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：Robustness Evaluation of Machine Learning Models for Robot Arm Action  Recognition in Noisy Environments</b></summary>
  <p><b>编号</b>：[247]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09606</p>
  <p><b>作者</b>：Elaheh Motamedi,  Kian Behzad,  Rojin Zandi,  Hojjat Salehinejad,  Milad Siami</p>
  <p><b>备注</b>：Accepted at ICASSP</p>
  <p><b>关键词</b>：spatially proximate arm, noisy environments poses, proximate arm movements, noisy environments, identifying distinct</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the realm of robot action recognition, identifying distinct but spatially proximate arm movements using vision systems in noisy environments poses a significant challenge. This paper studies robot arm action recognition in noisy environments using machine learning techniques. Specifically, a vision system is used to track the robot's movements followed by a deep learning model to extract the arm's key points. Through a comparative analysis of machine learning methods, the effectiveness and robustness of this model are assessed in noisy environments. A case study was conducted using the Tic-Tac-Toe game in a 3-by-3 grid environment, where the focus is to accurately identify the actions of the arms in selecting specific locations within this constrained environment. Experimental results show that our approach can achieve precise key point detection and action classification despite the addition of noise and uncertainties to the dataset.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：MedBlindTuner: Towards Privacy-preserving Fine-tuning on Biomedical  Images with Transformers and Fully Homomorphic Encryption</b></summary>
  <p><b>编号</b>：[249]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09604</p>
  <p><b>作者</b>：Prajwal Panzade,  Daniel Takabi,  Zhipeng Cai</p>
  <p><b>备注</b>：Accepted for the presentation at W3PHIAI, The 38th Annual AAAI Conference on Artificial Intelligence 2024</p>
  <p><b>关键词</b>：significantly revolutionized medical, Advancements in machine, machine learning, prompting hospitals, external ML services</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Advancements in machine learning (ML) have significantly revolutionized medical image analysis, prompting hospitals to rely on external ML services. However, the exchange of sensitive patient data, such as chest X-rays, poses inherent privacy risks when shared with third parties. Addressing this concern, we propose MedBlindTuner, a privacy-preserving framework leveraging fully homomorphic encryption (FHE) and a data-efficient image transformer (DEiT). MedBlindTuner enables the training of ML models exclusively on FHE-encrypted medical images. Our experimental evaluation demonstrates that MedBlindTuner achieves comparable accuracy to models trained on non-encrypted images, offering a secure solution for outsourcing ML computations while preserving patient data privacy. To the best of our knowledge, this is the first work that uses data-efficient image transformers and fully homomorphic encryption in this domain.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：Rethinking FID: Towards a Better Evaluation Metric for Image Generation</b></summary>
  <p><b>编号</b>：[250]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09603</p>
  <p><b>作者</b>：Sadeep Jayasumana,  Srikumar Ramalingam,  Andreas Veit,  Daniel Glasner,  Ayan Chakrabarti,  Sanjiv Kumar</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：machine learning problems, generation methods hinges, image generation methods, Frechet Inception Distance, learning problems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As with many machine learning problems, the progress of image generation methods hinges on good evaluation metrics. One of the most popular is the Frechet Inception Distance (FID). FID estimates the distance between a distribution of Inception-v3 features of real images, and those of images generated by the algorithm. We highlight important drawbacks of FID: Inception's poor representation of the rich and varied content generated by modern text-to-image models, incorrect normality assumptions, and poor sample complexity. We call for a reevaluation of FID's use as the primary quality metric for generated images. We empirically demonstrate that FID contradicts human raters, it does not reflect gradual improvement of iterative text-to-image models, it does not capture distortion levels, and that it produces inconsistent results when varying the sample size. We also propose an alternative new metric, CMMD, based on richer CLIP embeddings and the maximum mean discrepancy distance with the Gaussian RBF kernel. It is an unbiased estimator that does not make any assumptions on the probability distribution of the embeddings and is sample efficient. Through extensive experiments and analysis, we demonstrate that FID-based evaluations of text-to-image models may be unreliable, and that CMMD offers a more robust and reliable assessment of image quality.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：Efficient generative adversarial networks using linear  additive-attention Transformers</b></summary>
  <p><b>编号</b>：[252]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09596</p>
  <p><b>作者</b>：Emilio Morales-Juarez,  Gibran Fuentes-Pineda</p>
  <p><b>备注</b>：12 pages, 6 figures</p>
  <p><b>关键词</b>：computationally expensive architectures, Generative Adversarial Networks, Diffusion Models, image generation, recent years</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Although the capacity of deep generative models for image generation, such as Diffusion Models (DMs) and Generative Adversarial Networks (GANs), has dramatically improved in recent years, much of their success can be attributed to computationally expensive architectures. This has limited their adoption and use to research laboratories and companies with large resources, while significantly raising the carbon footprint for training, fine-tuning, and inference. In this work, we present LadaGAN, an efficient generative adversarial network that is built upon a novel Transformer block named Ladaformer. The main component of this block is a linear additive-attention mechanism that computes a single attention vector per head instead of the quadratic dot-product attention. We employ Ladaformer in both the generator and discriminator, which reduces the computational complexity and overcomes the training instabilities often associated with Transformer GANs. LadaGAN consistently outperforms existing convolutional and Transformer GANs on benchmark datasets at different resolutions while being significantly more efficient. Moreover, LadaGAN shows competitive performance compared to state-of-the-art multi-step generative models (e.g. DMs) using orders of magnitude less computational resources.</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：On-Off Pattern Encoding and Path-Count Encoding as Deep Neural Network  Representations</b></summary>
  <p><b>编号</b>：[267]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09518</p>
  <p><b>作者</b>：Euna Jung,  Jaekeol Choi,  EungGu Yun,  Wonjong Rhee</p>
  <p><b>备注</b>：8 pages, 4 figures</p>
  <p><b>关键词</b>：Deep Neural Networks, Deep Neural, Neural Networks, Understanding the encoded, challenging objective</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Understanding the encoded representation of Deep Neural Networks (DNNs) has been a fundamental yet challenging objective. In this work, we focus on two possible directions for analyzing representations of DNNs by studying simple image classification tasks. Specifically, we consider \textit{On-Off pattern} and \textit{PathCount} for investigating how information is stored in deep representations. On-off pattern of a neuron is decided as `on' or `off' depending on whether the neuron's activation after ReLU is non-zero or zero. PathCount is the number of paths that transmit non-zero energy from the input to a neuron. We investigate how neurons in the network encodes information by replacing each layer's activation with On-Off pattern or PathCount and evaluating its effect on classification performance. We also examine correlation between representation and PathCount. Finally, we show a possible way to improve an existing DNN interpretation method, Class Activation Map (CAM), by directly utilizing On-Off or PathCount.</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：Enhancing Surveillance Camera FOV Quality via Semantic Line Detection  and Classification with Deep Hough Transform</b></summary>
  <p><b>编号</b>：[270]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09515</p>
  <p><b>作者</b>：Andrew C. Freeman,  Wenjing Shi,  Bin Hwang</p>
  <p><b>备注</b>：Appeared in the WACV 2024 Workshop on Image/Video/Audio Quality in Computer Vision and Generative AI</p>
  <p><b>关键词</b>：significantly influenced, FOV, video and image, image quality, recorded videos</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The quality of recorded videos and images is significantly influenced by the camera's field of view (FOV). In critical applications like surveillance systems and self-driving cars, an inadequate FOV can give rise to severe safety and security concerns, including car accidents and thefts due to the failure to detect individuals and objects. The conventional methods for establishing the correct FOV heavily rely on human judgment and lack automated mechanisms to assess video and image quality based on FOV. In this paper, we introduce an innovative approach that harnesses semantic line detection and classification alongside deep Hough transform to identify semantic lines, thus ensuring a suitable FOV by understanding 3D view through parallel lines. Our approach yields an effective F1 score of 0.729 on the public EgoCart dataset, coupled with a notably high median score in the line placement metric. We illustrate that our method offers a straightforward means of assessing the quality of the camera's field of view, achieving a classification accuracy of 83.8\%. This metric can serve as a proxy for evaluating the potential performance of video and image quality applications.</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：Learning to Generalize over Subpartitions for Heterogeneity-aware Domain  Adaptive Nuclei Segmentation</b></summary>
  <p><b>编号</b>：[277]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09496</p>
  <p><b>作者</b>：Jianan Fan,  Dongnan Liu,  Hang Chang,  Weidong Cai</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：major obstacles hindering, deep learning models, stain data distribution, data distribution shifts, potential applications</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Annotation scarcity and cross-modality/stain data distribution shifts are two major obstacles hindering the application of deep learning models for nuclei analysis, which holds a broad spectrum of potential applications in digital pathology. Recently, unsupervised domain adaptation (UDA) methods have been proposed to mitigate the distributional gap between different imaging modalities for unsupervised nuclei segmentation in histopathology images. However, existing UDA methods are built upon the assumption that data distributions within each domain should be uniform. Based on the over-simplified supposition, they propose to align the histopathology target domain with the source domain integrally, neglecting severe intra-domain discrepancy over subpartitions incurred by mixed cancer types and sampling organs. In this paper, for the first time, we propose to explicitly consider the heterogeneity within the histopathology domain and introduce open compound domain adaptation (OCDA) to resolve the crux. In specific, a two-stage disentanglement framework is proposed to acquire domain-invariant feature representations at both image and instance levels. The holistic design addresses the limitations of existing OCDA approaches which struggle to capture instance-wise variations. Two regularization strategies are specifically devised herein to leverage the rich subpartition-specific characteristics in histopathology images and facilitate subdomain decomposition. Moreover, we propose a dual-branch nucleus shape and structure preserving module to prevent nucleus over-generation and deformation in the synthesized images. Experimental results on both cross-modality and cross-stain scenarios over a broad range of diverse datasets demonstrate the superiority of our method compared with state-of-the-art UDA and OCDA methods.</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：IPR-NeRF: Ownership Verification meets Neural Radiance Field</b></summary>
  <p><b>编号</b>：[278]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09495</p>
  <p><b>作者</b>：Win Kent Ong,  Kam Woh Ng,  Chee Seng Chan,  Yi Zhe Song,  Tao Xiang</p>
  <p><b>备注</b>：21 pages</p>
  <p><b>关键词</b>：Neural Radiance Field, Radiance Field, produced impressive demonstrations, gained significant attention, computer vision community</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural Radiance Field (NeRF) models have gained significant attention in the computer vision community in the recent past with state-of-the-art visual quality and produced impressive demonstrations. Since then, technopreneurs have sought to leverage NeRF models into a profitable business. Therefore, NeRF models make it worth the risk of plagiarizers illegally copying, re-distributing, or misusing those models. This paper proposes a comprehensive intellectual property (IP) protection framework for the NeRF model in both black-box and white-box settings, namely IPR-NeRF. In the black-box setting, a diffusion-based solution is introduced to embed and extract the watermark via a two-stage optimization process. In the white-box setting, a designated digital signature is embedded into the weights of the NeRF model by adopting the sign loss objective. Our extensive experiments demonstrate that not only does our approach maintain the fidelity (\ie, the rendering quality) of IPR-NeRF models, but it is also robust against both ambiguity and removal attacks compared to prior arts.</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：Triamese-ViT: A 3D-Aware Method for Robust Brain Age Estimation from  MRIs</b></summary>
  <p><b>编号</b>：[287]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09475</p>
  <p><b>作者</b>：Zhaonian Zhang,  Richard Jiang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：improved diagnostic precision, significantly improved diagnostic, Magnetic Resonance Imaging, diagnostic precision, integration of machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The integration of machine learning in medicine has significantly improved diagnostic precision, particularly in the interpretation of complex structures like the human brain. Diagnosing challenging conditions such as Alzheimer's disease has prompted the development of brain age estimation techniques. These methods often leverage three-dimensional Magnetic Resonance Imaging (MRI) scans, with recent studies emphasizing the efficacy of 3D convolutional neural networks (CNNs) like 3D ResNet. However, the untapped potential of Vision Transformers (ViTs), known for their accuracy and interpretability, persists in this domain due to limitations in their 3D versions. This paper introduces Triamese-ViT, an innovative adaptation of the ViT model for brain age estimation. Our model uniquely combines ViTs from three different orientations to capture 3D information, significantly enhancing accuracy and interpretability. Tested on a dataset of 1351 MRI scans, Triamese-ViT achieves a Mean Absolute Error (MAE) of 3.84, a 0.9 Spearman correlation coefficient with chronological age, and a -0.29 Spearman correlation coefficient between the brain age gap (BAG) and chronological age, significantly better than previous methods for brian age estimation. A key innovation of Triamese-ViT is its capacity to generate a comprehensive 3D-like attention map, synthesized from 2D attention maps of each orientation-specific ViT. This feature is particularly beneficial for in-depth brain age analysis and disease diagnosis, offering deeper insights into brain health and the mechanisms of age-related neural changes.</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：Plug-in for visualizing 3D tool tracking from videos of Minimally  Invasive Surgeries</b></summary>
  <p><b>编号</b>：[290]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09472</p>
  <p><b>作者</b>：Shubhangi Nema,  Abhishek Mathur,  Leena Vachhani</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：minimally invasive surgery, paper tackles instrument, invasive surgery, crucial for computer-assisted, computer-assisted interventions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper tackles instrument tracking and 3D visualization challenges in minimally invasive surgery (MIS), crucial for computer-assisted interventions. Conventional and robot-assisted MIS encounter issues with limited 2D camera projections and minimal hardware integration. The objective is to track and visualize the entire surgical instrument, including shaft and metallic clasper, enabling safe navigation within the surgical environment. The proposed method involves 2D tracking based on segmentation maps, facilitating creation of labeled dataset without extensive ground-truth knowledge. Geometric changes in 2D intervals express motion, and kinematics based algorithms process results into 3D tracking information. Synthesized and experimental results in 2D and 3D motion estimates demonstrate negligible errors, validating the method for labeling and motion tracking of instruments in MIS videos. The conclusion underscores the proposed 2D segmentation technique's simplicity and computational efficiency, emphasizing its potential as direct plug-in for 3D visualization in instrument tracking and MIS practices.</p>
  </details>
</details>
<details>
  <summary>83. <b>标题：Offline Handwriting Signature Verification: A Transfer Learning and  Feature Selection Approach</b></summary>
  <p><b>编号</b>：[291]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09467</p>
  <p><b>作者</b>：Fatih Ozyurt,  Jafar Majidpour,  Tarik A. Rashid,  Canan Koc</p>
  <p><b>备注</b>：11 pages</p>
  <p><b>关键词</b>：Handwritten signature verification, signature verification poses, Handwritten signature, poses a formidable, formidable challenge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Handwritten signature verification poses a formidable challenge in biometrics and document authenticity. The objective is to ascertain the authenticity of a provided handwritten signature, distinguishing between genuine and forged ones. This issue has many applications in sectors such as finance, legal documentation, and security. Currently, the field of computer vision and machine learning has made significant progress in the domain of handwritten signature verification. The outcomes, however, may be enhanced depending on the acquired findings, the structure of the datasets, and the used models. Four stages make up our suggested strategy. First, we collected a large dataset of 12600 images from 420 distinct individuals, and each individual has 30 signatures of a certain kind (All authors signatures are genuine). In the subsequent stage, the best features from each image were extracted using a deep learning model named MobileNetV2. During the feature selection step, three selectors neighborhood component analysis (NCA), Chi2, and mutual info (MI) were used to pull out 200, 300, 400, and 500 features, giving a total of 12 feature vectors. Finally, 12 results have been obtained by applying machine learning techniques such as SVM with kernels (rbf, poly, and linear), KNN, DT, Linear Discriminant Analysis, and Naive Bayes. Without employing feature selection techniques, our suggested offline signature verification achieved a classification accuracy of 91.3%, whereas using the NCA feature selection approach with just 300 features it achieved a classification accuracy of 97.7%. High classification accuracy was achieved using the designed and suggested model, which also has the benefit of being a self-organized framework. Consequently, using the optimum minimally chosen features, the proposed method could identify the best model performance and result validation prediction vectors.</p>
  </details>
</details>
<details>
  <summary>84. <b>标题：Voila-A: Aligning Vision-Language Models with User's Gaze Attention</b></summary>
  <p><b>编号</b>：[297]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09454</p>
  <p><b>作者</b>：Kun Yan,  Lei Ji,  Zeyu Wang,  Yuntao Wang,  Nan Duan,  Shuai Ma</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：recent years, artificial intelligence, integration of vision, vision and language, language understanding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, the integration of vision and language understanding has led to significant advancements in artificial intelligence, particularly through Vision-Language Models (VLMs). However, existing VLMs face challenges in handling real-world applications with complex scenes and multiple objects, as well as aligning their focus with the diverse attention patterns of human users. In this paper, we introduce gaze information, feasibly collected by AR or VR devices, as a proxy for human attention to guide VLMs and propose a novel approach, Voila-A, for gaze alignment to enhance the interpretability and effectiveness of these models in real-world applications. First, we collect hundreds of minutes of gaze data to demonstrate that we can mimic human gaze modalities using localized narratives. We then design an automatic data annotation pipeline utilizing GPT-4 to generate the VOILA-COCO dataset. Additionally, we innovate the Voila Perceiver modules to integrate gaze information into VLMs while preserving their pretrained knowledge. We evaluate Voila-A using a hold-out validation set and a newly collected VOILA-GAZE Testset, which features real-life scenarios captured with a gaze-tracking device. Our experimental results demonstrate that Voila-A significantly outperforms several baseline models. By aligning model attention with human gaze patterns, Voila-A paves the way for more intuitive, user-centric VLMs and fosters engaging human-AI interaction across a wide range of applications.</p>
  </details>
</details>
<details>
  <summary>85. <b>标题：Joining Forces for Pathology Diagnostics with AI Assistance: The EMPAIA  Initiative</b></summary>
  <p><b>编号</b>：[299]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09450</p>
  <p><b>作者</b>：Norman Zerbe,  Lars Ole Schwen,  Christian Geißler,  Katja Wiesemann,  Tom Bisson,  Peter Boor,  Rita Carvalho,  Michael Franz,  Christoph Jansen,  Tim-Rasmus Kiehl,  Björn Lindequist,  Nora Charlotte Pohlan,  Sarah Schmell,  Klaus Strohmenger,  Falk Zakrzewski,  Markus Plass,  Michael Takla,  Tobias Küster,  André Homeyer,  Peter Hufnagl</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：artificial intelligence, past decade, advanced substantially, EMPAIA, pathology</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Over the past decade, artificial intelligence (AI) methods in pathology have advanced substantially. However, integration into routine clinical practice has been slow due to numerous challenges, including technical and regulatory hurdles in translating research results into clinical diagnostic products and the lack of standardized interfaces. The open and vendor-neutral EMPAIA initiative addresses these challenges. Here, we provide an overview of EMPAIA's achievements and lessons learned. EMPAIA integrates various stakeholders of the pathology AI ecosystem, i.e., pathologists, computer scientists, and industry. In close collaboration, we developed technical interoperability standards, recommendations for AI testing and product development, and explainability methods. We implemented the modular and open-source EMPAIA platform and successfully integrated 11 AI-based image analysis apps from 6 different vendors, demonstrating how different apps can use a single standardized interface. We prioritized requirements and evaluated the use of AI in real clinical settings with 14 different pathology laboratories in Europe and Asia. In addition to technical developments, we created a forum for all stakeholders to share information and experiences on digital pathology and AI. Commercial, clinical, and academic stakeholders can now adopt EMPAIA's common open-source interfaces, providing a unique opportunity for large-scale standardization and streamlining of processes. Further efforts are needed to effectively and broadly establish AI assistance in routine laboratory use. To this end, a sustainable infrastructure, the non-profit association EMPAIA International, has been established to continue standardization and support broad implementation and advocacy for an AI-assisted digital pathology future.</p>
  </details>
</details>
<details>
  <summary>86. <b>标题：Explainable Multimodal Sentiment Analysis on Bengali Memes</b></summary>
  <p><b>编号</b>：[303]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09446</p>
  <p><b>作者</b>：Kazi Toufique Elahi,  Tasnuva Binte Rahman,  Shakil Shahriar,  Samir Sarker,  Sajib Kumar Saha Joy,  Faisal Muhammad Shah</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：attracting online communities, digital era, attracting online, cultural barriers, distinctive and effective</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Memes have become a distinctive and effective form of communication in the digital era, attracting online communities and cutting across cultural barriers. Even though memes are frequently linked with humor, they have an amazing capacity to convey a wide range of emotions, including happiness, sarcasm, frustration, and more. Understanding and interpreting the sentiment underlying memes has become crucial in the age of information. Previous research has explored text-based, image-based, and multimodal approaches, leading to the development of models like CAPSAN and PromptHate for detecting various meme categories. However, the study of low-resource languages like Bengali memes remains scarce, with limited availability of publicly accessible datasets. A recent contribution includes the introduction of the MemoSen dataset. However, the achieved accuracy is notably low, and the dataset suffers from imbalanced distribution. In this study, we employed a multimodal approach using ResNet50 and BanglishBERT and achieved a satisfactory result of 0.71 weighted F1-score, performed comparison with unimodal approaches, and interpreted behaviors of the models using explainable artificial intelligence (XAI) techniques.</p>
  </details>
</details>
<details>
  <summary>87. <b>标题：CRD: Collaborative Representation Distance for Practical Anomaly  Detection</b></summary>
  <p><b>编号</b>：[306]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09443</p>
  <p><b>作者</b>：Chao Han,  Yudong Yan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：defect detection plays, Visual defect detection, intelligent industry, detection plays, plays an important</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Visual defect detection plays an important role in intelligent industry. Patch based methods consider visual images as a collection of image patches according to positions, which have stronger discriminative ability for small defects in products, e.g. scratches on pills. However, the nearest neighbor search for the query image and the stored patches will occupy $O(n)$ complexity in terms of time and space requirements, posing strict challenges for deployment in edge environments. In this paper, we propose an alternative approach to the distance calculation of image patches via collaborative representation models. Starting from the nearest neighbor distance with $L_0$ constraint, we relax the constraint to $L_2$ constraint and solve the distance quickly in close-formed without actually accessing the original stored collection of image patches. Furthermore, we point out that the main computational burden of this close-formed solution can be pre-computed by high-performance server before deployment. Consequently, the distance calculation on edge devices only requires a simple matrix multiplication, which is extremely lightweight and GPU-friendly. Performance on real industrial scenarios demonstrates that compared to the existing state-of-the-art methods, this distance achieves several hundred times improvement in computational efficiency with slight performance drop, while greatly reducing memory overhead.</p>
  </details>
</details>
<details>
  <summary>88. <b>标题：Object Attribute Matters in Visual Question Answering</b></summary>
  <p><b>编号</b>：[307]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09442</p>
  <p><b>作者</b>：Peize Li,  Qingyi Si,  Peng Fu,  Zheng Lin,  Yan Wang</p>
  <p><b>备注</b>：AAAI 2024</p>
  <p><b>关键词</b>：Visual question answering, question answering, task that requires, requires the joint, joint comprehension</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Visual question answering is a multimodal task that requires the joint comprehension of visual and textual information. However, integrating visual and textual semantics solely through attention layers is insufficient to comprehensively understand and align information from both modalities. Intuitively, object attributes can naturally serve as a bridge to unify them, which has been overlooked in previous research. In this paper, we propose a novel VQA approach from the perspective of utilizing object attribute, aiming to achieve better object-level visual-language alignment and multimodal scene understanding. Specifically, we design an attribute fusion module and a contrastive knowledge distillation module. The attribute fusion module constructs a multimodal graph neural network to fuse attributes and visual features through message passing. The enhanced object-level visual features contribute to solving fine-grained problem like counting-question. The better object-level visual-language alignment aids in understanding multimodal scenes, thereby improving the model's robustness. Furthermore, to augment scene understanding and the out-of-distribution performance, the contrastive knowledge distillation module introduces a series of implicit knowledge. We distill knowledge into attributes through contrastive loss, which further strengthens the representation learning of attribute features and facilitates visual-linguistic alignment. Intensive experiments on six datasets, COCO-QA, VQAv2, VQA-CPv2, VQA-CPv1, VQAvs and TDIUC, show the superiority of the proposed method.</p>
  </details>
</details>
<details>
  <summary>89. <b>标题：Few-shot learning for COVID-19 Chest X-Ray Classification with  Imbalanced Data: An Inter vs. Intra Domain Study</b></summary>
  <p><b>编号</b>：[317]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10129</p>
  <p><b>作者</b>：Alejandro Galán-Cuenca,  Antonio Javier Gallego,  Marcelo Saval-Calvo,  Antonio Pertusa</p>
  <p><b>备注</b>：Submited to Pattern Analysis and Applications</p>
  <p><b>关键词</b>：Medical image datasets, medical research, treatment planning, computer-aided diagnosis, essential for training</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Medical image datasets are essential for training models used in computer-aided diagnosis, treatment planning, and medical research. However, some challenges are associated with these datasets, including variability in data distribution, data scarcity, and transfer learning issues when using models pre-trained from generic images. This work studies the effect of these challenges at the intra- and inter-domain level in few-shot learning scenarios with severe data imbalance. For this, we propose a methodology based on Siamese neural networks in which a series of techniques are integrated to mitigate the effects of data scarcity and distribution imbalance. Specifically, different initialization and data augmentation methods are analyzed, and four adaptations to Siamese networks of solutions to deal with imbalanced data are introduced, including data balancing and weighted loss, both separately and combined, and with a different balance of pairing ratios. Moreover, we also assess the inference process considering four classifiers, namely Histogram, $k$NN, SVM, and Random Forest. Evaluation is performed on three chest X-ray datasets with annotated cases of both positive and negative COVID-19 diagnoses. The accuracy of each technique proposed for the Siamese architecture is analyzed separately and their results are compared to those obtained using equivalent methods on a state-of-the-art CNN. We conclude that the introduced techniques offer promising improvements over the baseline in almost all cases, and that the selection of the technique may vary depending on the amount of data available and the level of imbalance.</p>
  </details>
</details>
<details>
  <summary>90. <b>标题：Sub2Full: split spectrum to boost OCT despeckling without clean data</b></summary>
  <p><b>编号</b>：[318]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10128</p>
  <p><b>作者</b>：Lingyun Wang,  Jose A Sahel,  Shaohua Pi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Optical coherence tomography, visible light OCT, Optical coherence, coherence tomography, suffers from speckle</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Optical coherence tomography (OCT) suffers from speckle noise, causing the deterioration of image quality, especially in high-resolution modalities like visible light OCT (vis-OCT). The potential of conventional supervised deep learning denoising methods is limited by the difficulty of obtaining clean data. Here, we proposed an innovative self-supervised strategy called Sub2Full (S2F) for OCT despeckling without clean data. This approach works by acquiring two repeated B-scans, splitting the spectrum of the first repeat as a low-resolution input, and utilizing the full spectrum of the second repeat as the high-resolution target. The proposed method was validated on vis-OCT retinal images visualizing sublaminar structures in outer retina and demonstrated superior performance over conventional Noise2Noise and Noise2Void schemes. The code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>91. <b>标题：Ventricular Segmentation: A Brief Comparison of U-Net Derivatives</b></summary>
  <p><b>编号</b>：[326]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09980</p>
  <p><b>作者</b>：Ketan Suhaas Saichandran</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Medical imaging refers, treat medical disorders, Magnetic Resonance Imaging, medical disorders related, medical disorders</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Medical imaging refers to the technologies and methods utilized to view the human body and its inside, in order to diagnose, monitor, or even treat medical disorders. This paper aims to explore the application of deep learning techniques in the semantic segmentation of Cardiac short-axis MRI (Magnetic Resonance Imaging) images, aiming to enhance the diagnosis, monitoring, and treatment of medical disorders related to the heart. The focus centers on implementing various architectures that are derivatives of U-Net, to effectively isolate specific parts of the heart for comprehensive anatomical and functional analysis. Through a combination of images, graphs, and quantitative metrics, the efficacy of the models and their predictions are showcased. Additionally, this paper addresses encountered challenges and outline strategies for future improvements. This abstract provides a concise overview of the efforts in utilizing deep learning for cardiac image segmentation, emphasizing both the accomplishments and areas for further refinement.</p>
  </details>
</details>
<details>
  <summary>92. <b>标题：Slicer Networks</b></summary>
  <p><b>编号</b>：[332]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09833</p>
  <p><b>作者</b>：Hang Zhang,  Xiang Chen,  Rongguang Wang,  Renjiu Hu,  Dongdong Liu,  Gaolei Li</p>
  <p><b>备注</b>：8 figures and 3 tables</p>
  <p><b>关键词</b>：consistent internal intensities, scans often reveal, intensities or textures, Slicer Network, reveal objects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In medical imaging, scans often reveal objects with varied contrasts but consistent internal intensities or textures. This characteristic enables the use of low-frequency approximations for tasks such as segmentation and deformation field estimation. Yet, integrating this concept into neural network architectures for medical image analysis remains underexplored. In this paper, we propose the Slicer Network, a novel architecture designed to leverage these traits. Comprising an encoder utilizing models like vision transformers for feature extraction and a slicer employing a learnable bilateral grid, the Slicer Network strategically refines and upsamples feature maps via a splatting-blurring-slicing process. This introduces an edge-preserving low-frequency approximation for the network outcome, effectively enlarging the effective receptive field. The enhancement not only reduces computational complexity but also boosts overall performance. Experiments across different medical imaging applications, including unsupervised and keypoints-based image registration and lesion segmentation, have verified the Slicer Network's improved accuracy and efficiency.</p>
  </details>
</details>
<details>
  <summary>93. <b>标题：Multilingual Visual Speech Recognition with a Single Model by Learning  with Discrete Visual Speech Units</b></summary>
  <p><b>编号</b>：[333]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09802</p>
  <p><b>作者</b>：Minsu Kim,  Jeong Hun Yeo,  Jeongsoo Choi,  Se Jin Park,  Yong Man Ro</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：visual speech units, Visual Speech, Multilingual Visual Speech, visual speech model, self-supervised visual speech</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper explores sentence-level Multilingual Visual Speech Recognition with a single model for the first time. As the massive multilingual modeling of visual data requires huge computational costs, we propose a novel strategy, processing with visual speech units. Motivated by the recent success of the audio speech unit, the proposed visual speech unit is obtained by discretizing the visual speech features extracted from the self-supervised visual speech model. To correctly capture multilingual visual speech, we first train the self-supervised visual speech model on 5,512 hours of multilingual audio-visual data. Through analysis, we verify that the visual speech units mainly contain viseme information while suppressing non-linguistic information. By using the visual speech units as the inputs of our system, we pre-train the model to predict corresponding text outputs on massive multilingual data constructed by merging several VSR databases. As both the inputs and outputs are discrete, we can greatly improve the training efficiency compared to the standard VSR training. Specifically, the input data size is reduced to 0.016% of the original video inputs. In order to complement the insufficient visual information in speech recognition, we apply curriculum learning where the inputs of the system begin with audio-visual speech units and gradually change to visual speech units. After pre-training, the model is finetuned on continuous features. We set new state-of-the-art multilingual VSR performances by achieving comparable performances to the previous language-specific VSR models, with a single trained model.</p>
  </details>
</details>
<details>
  <summary>94. <b>标题：BreastRegNet: A Deep Learning Framework for Registration of Breast  Faxitron and Histopathology Images</b></summary>
  <p><b>编号</b>：[334]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09791</p>
  <p><b>作者</b>：Negar Golestani,  Aihui Wang,  Gregory R Bean,  Mirabela Rusu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：standard treatment protocol, cancer entails administering, entails administering neoadjuvant, administering neoadjuvant therapy, breast cancer entails</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A standard treatment protocol for breast cancer entails administering neoadjuvant therapy followed by surgical removal of the tumor and surrounding tissue. Pathologists typically rely on cabinet X-ray radiographs, known as Faxitron, to examine the excised breast tissue and diagnose the extent of residual disease. However, accurately determining the location, size, and focality of residual cancer can be challenging, and incorrect assessments can lead to clinical consequences. The utilization of automated methods can improve the histopathology process, allowing pathologists to choose regions for sampling more effectively and precisely. Despite the recognized necessity, there are currently no such methods available. Training such automated detection models require accurate ground truth labels on ex-vivo radiology images, which can be acquired through registering Faxitron and histopathology images and mapping the extent of cancer from histopathology to x-ray images. This study introduces a deep learning-based image registration approach trained on mono-modal synthetic image pairs. The models were trained using data from 50 women who received neoadjuvant chemotherapy and underwent surgery. The results demonstrate that our method is faster and yields significantly lower average landmark error ($2.1\pm1.96$ mm) over the state-of-the-art iterative ($4.43\pm4.1$ mm) and deep learning ($4.02\pm3.15$ mm) approaches. Improved performance of our approach in integrating radiology and pathology information facilitates generating large datasets, which allows training models for more accurate breast cancer detection.</p>
  </details>
</details>
<details>
  <summary>95. <b>标题：Uncertainty Modeling in Ultrasound Image Segmentation for Precise Fetal  Biometric Measurements</b></summary>
  <p><b>编号</b>：[343]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09639</p>
  <p><b>作者</b>：Shuge Lei</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Medical image segmentation, ultrasound image segmentation, medical imaging, Medical image, crucial aspect</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Medical image segmentation, particularly in the context of ultrasound data, is a crucial aspect of computer vision and medical imaging. This paper delves into the complexities of uncertainty in the segmentation process, focusing on fetal head and femur ultrasound images. The proposed methodology involves extracting target contours and exploring techniques for precise parameter measurement. Uncertainty modeling methods are employed to enhance the training and testing processes of the segmentation network. The study reveals that the average absolute error in fetal head circumference measurement is 8.0833mm, with a relative error of 4.7347%. Similarly, the average absolute error in fetal femur measurement is 2.6163mm, with a relative error of 6.3336%. Uncertainty modeling experiments employing Test-Time Augmentation (TTA) demonstrate effective interpretability of data uncertainty on both datasets. This suggests that incorporating data uncertainty based on the TTA method can support clinical practitioners in making informed decisions and obtaining more reliable measurement results in practical clinical applications. The paper contributes to the advancement of ultrasound image segmentation, addressing critical challenges and improving the reliability of biometric measurements.</p>
  </details>
</details>
<details>
  <summary>96. <b>标题：Automatic 3D Multi-modal Ultrasound Segmentation of Human Placenta using  Fusion Strategies and Deep Learning</b></summary>
  <p><b>编号</b>：[344]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09638</p>
  <p><b>作者</b>：Sonit Singh,  Gordon Stevenson,  Brendan Mein,  Alec Welsh,  Arcot Sowmya</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：medical imaging modality, power Doppler, power Doppler scans, power Doppler ultrasound, B-mode and power</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Purpose: Ultrasound is the most commonly used medical imaging modality for diagnosis and screening in clinical practice. Due to its safety profile, noninvasive nature and portability, ultrasound is the primary imaging modality for fetal assessment in pregnancy. Current ultrasound processing methods are either manual or semi-automatic and are therefore laborious, time-consuming and prone to errors, and automation would go a long way in addressing these challenges. Automated identification of placental changes at earlier gestation could facilitate potential therapies for conditions such as fetal growth restriction and pre-eclampsia that are currently detected only at late gestational age, potentially preventing perinatal morbidity and mortality.
Methods: We propose an automatic three-dimensional multi-modal (B-mode and power Doppler) ultrasound segmentation of the human placenta using deep learning combined with different fusion strategies.We collected data containing Bmode and power Doppler ultrasound scans for 400 studies.
Results: We evaluated different fusion strategies and state-of-the-art image segmentation networks for placenta segmentation based on standard overlap- and boundary-based metrics. We found that multimodal information in the form of B-mode and power Doppler scans outperform any single modality. Furthermore, we found that B-mode and power Doppler input scans fused at the data level provide the best results with a mean Dice Similarity Coefficient (DSC) of 0.849.
Conclusion: We conclude that the multi-modal approach of combining B-mode and power Doppler scans is effective in segmenting the placenta from 3D ultrasound scans in a fully automated manner and is robust to quality variation of the datasets.</p>
  </details>
</details>
<details>
  <summary>97. <b>标题：CT Liver Segmentation via PVT-based Encoding and Refined Decoding</b></summary>
  <p><b>编号</b>：[345]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09630</p>
  <p><b>作者</b>：Debesh Jha,  Nikhil Kumar Tomar,  Koushik Biswas,  Gorkem Durak,  Alpay Medetalibeyoglu,  Matthew Antalek,  Yury Velichko,  Daniela Ladner,  Amir Borhani,  Ulas Bagci</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：treatment planning, scans is essential, essential for computer-aided, computer-aided diagnosis, diagnosis and treatment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Accurate liver segmentation from CT scans is essential for computer-aided diagnosis and treatment planning. Recently, Vision Transformers achieved a competitive performance in computer vision tasks compared to convolutional neural networks due to their exceptional ability to learn global representations. However, they often struggle with scalability, memory constraints, and computational inefficiency, particularly in handling high-resolution medical images. To overcome scalability and efficiency issues, we propose a novel deep learning approach, \textit{\textbf{PVTFormer}}, that is built upon a pretrained pyramid vision transformer (PVT v2) combined with advanced residual upsampling and decoder block. By integrating a refined feature channel approach with hierarchical decoding strategy, PVTFormer generates high quality segmentation masks by enhancing semantic features. Rigorous evaluation of the proposed method on Liver Tumor Segmentation Benchmark (LiTS) 2017 demonstrates that our proposed architecture not only achieves a high dice coefficient of 86.78\%, mIoU of 78.46\%, but also obtains a low HD of 3.50. The results underscore PVTFormer's efficacy in setting a new benchmark for state-of-the-art liver segmentation methods. The source code of the proposed PVTFormer is available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>98. <b>标题：SymTC: A Symbiotic Transformer-CNN Net for Instance Segmentation of  Lumbar Spine MRI</b></summary>
  <p><b>编号</b>：[346]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09627</p>
  <p><b>作者</b>：Jiasong Chen,  Linchen Qian,  Linhai Ma,  Timur Urakov,  Weiyong Gu,  Liang Liang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：low back pain, persistent low back, Convolutional Neural Network, image segmentation, Intervertebral disc disease</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Intervertebral disc disease, a prevalent ailment, frequently leads to intermittent or persistent low back pain, and diagnosing and assessing of this disease rely on accurate measurement of vertebral bone and intervertebral disc geometries from lumbar MR images. Deep neural network (DNN) models may assist clinicians with more efficient image segmentation of individual instances (disks and vertebrae) of the lumbar spine in an automated way, which is termed as instance image segmentation. In this work, we proposed SymTC, an innovative lumbar spine MR image segmentation model that combines the strengths of Transformer and Convolutional Neural Network (CNN). Specifically, we designed a parallel dual-path architecture to merge CNN layers and Transformer layers, and we integrated a novel position embedding into the self-attention module of Transformer, enhancing the utilization of positional information for more accurate segmentation. To further improves model performance, we introduced a new data augmentation technique to create synthetic yet realistic MR image dataset, named SSMSpine, which is made publicly available. We evaluated our SymTC and the other 15 existing image segmentation models on our private in-house dataset and the public SSMSpine dataset, using two metrics, Dice Similarity Coefficient and 95% Hausdorff Distance. The results show that our SymTC has the best performance for segmenting vertebral bones and intervertebral discs in lumbar spine MR images. The SymTC code and SSMSpine dataset are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>99. <b>标题：MITS-GAN: Safeguarding Medical Imaging from Tampering with Generative  Adversarial Networks</b></summary>
  <p><b>编号</b>：[347]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09624</p>
  <p><b>作者</b>：Giovanni Pasqualino,  Luca Guarnera,  Alessandro Ortis,  Sebastiano Battiato</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Generative Adversarial Networks, Adversarial Networks, Generative Adversarial, opened new possibilities, generation but raised</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The progress in generative models, particularly Generative Adversarial Networks (GANs), opened new possibilities for image generation but raised concerns about potential malicious uses, especially in sensitive areas like medical imaging. This study introduces MITS-GAN, a novel approach to prevent tampering in medical images, with a specific focus on CT scans. The approach disrupts the output of the attacker's CT-GAN architecture by introducing imperceptible but yet precise perturbations. Specifically, the proposed approach involves the introduction of appropriate Gaussian noise to the input as a protective measure against various attacks. Our method aims to enhance tamper resistance, comparing favorably to existing techniques. Experimental results on a CT scan dataset demonstrate MITS-GAN's superior performance, emphasizing its ability to generate tamper-resistant images with negligible artifacts. As image tampering in medical domains poses life-threatening risks, our proactive approach contributes to the responsible and ethical use of generative models. This work provides a foundation for future research in countering cyber threats in medical imaging. Models and codes are publicly available at the following link \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>100. <b>标题：Brain Tumor Radiogenomic Classification</b></summary>
  <p><b>编号</b>：[355]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09471</p>
  <p><b>作者</b>：Amr Mohamed,  Mahmoud Rabea,  Aya Sameh,  Ehab Kamal</p>
  <p><b>备注</b>：6 Pages with 4 Tables, 4 Figures and 4 Images</p>
  <p><b>关键词</b>：predict MGMT biomarker, MGMT biomarker status, Multi parameter mpMRI, tumor radiogenomic classification, radiogenomic classification challenge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The RSNA-MICCAI brain tumor radiogenomic classification challenge aimed to predict MGMT biomarker status in glioblastoma through binary classification on Multi parameter mpMRI scans: T1w, T1wCE, T2w and FLAIR. The dataset is splitted into three main cohorts: training set, validation set which were used during training, and the testing were only used during final evaluation. Images were either in a DICOM format or in Png format. different architectures were used to investigate the problem including the 3D version of Vision Transformer (ViT3D), ResNet50, Xception and EfficientNet-B3. AUC was used as the main evaluation metric and the results showed an advantage for both the ViT3D and the Xception models achieving 0.6015 and 0.61745 respectively on the testing set. compared to other results, our results proved to be valid given the complexity of the task. further improvements can be made through exploring different strategies, different architectures and more diverse datasets.</p>
  </details>
</details>
<details>
  <summary>101. <b>标题：Self Supervised Vision for Climate Downscaling</b></summary>
  <p><b>编号</b>：[356]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09466</p>
  <p><b>作者</b>：Karandeep Singh,  Chaeyoon Jeong,  Naufal Shidqi,  Sungwon Park,  Arjun Nellikkattil,  Elke Zeller,  Meeyoung Cha</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：facing today, Earth climate system, critical challenges, planet is facing, Earth System Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Climate change is one of the most critical challenges that our planet is facing today. Rising global temperatures are already bringing noticeable changes to Earth's weather and climate patterns with an increased frequency of unpredictable and extreme weather events. Future projections for climate change research are based on Earth System Models (ESMs), the computer models that simulate the Earth's climate system. ESMs provide a framework to integrate various physical systems, but their output is bound by the enormous computational resources required for running and archiving higher-resolution simulations. For a given resource budget, the ESMs are generally run on a coarser grid, followed by a computationally lighter $downscaling$ process to obtain a finer-resolution output. In this work, we present a deep-learning model for downscaling ESM simulation data that does not require high-resolution ground truth data for model optimization. This is realized by leveraging salient data distribution patterns and the hidden dependencies between weather variables for an $\textit{individual}$ data point at $\textit{runtime}$. Extensive evaluation with $2$x, $3$x, and $4$x scaling factors demonstrates that the proposed model consistently obtains superior performance over that of various baselines. The improved downscaling performance and no dependence on high-resolution ground truth data make the proposed method a valuable tool for climate research and mark it as a promising direction for future research.</p>
  </details>
</details>
<details>
  <summary>102. <b>标题：Multispectral Stereo-Image Fusion for 3D Hyperspectral Scene  Reconstruction</b></summary>
  <p><b>编号</b>：[362]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09428</p>
  <p><b>作者</b>：Eric L. Wisotzky,  Jost Triller,  Anna Hilsmann,  Peter Eisert</p>
  <p><b>备注</b>：VISAPP 2024 - 19th International Conference on Computer Vision Theory and Applications</p>
  <p><b>关键词</b>：optical material properties, human eye, optical material, material properties, Spectral imaging enables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Spectral imaging enables the analysis of optical material properties that are invisible to the human eye. Different spectral capturing setups, e.g., based on filter-wheel, push-broom, line-scanning, or mosaic cameras, have been introduced in the last years to support a wide range of applications in agriculture, medicine, and industrial surveillance. However, these systems often suffer from different disadvantages, such as lack of real-time capability, limited spectral coverage or low spatial resolution. To address these drawbacks, we present a novel approach combining two calibrated multispectral real-time capable snapshot cameras, covering different spectral ranges, into a stereo-system. Therefore, a hyperspectral data-cube can be continuously captured. The combined use of different multispectral snapshot cameras enables both 3D reconstruction and spectral analysis. Both captured images are demosaicked avoiding spatial resolution loss. We fuse the spectral data from one camera into the other to receive a spatially and spectrally high resolution video stream. Experiments demonstrate the feasibility of this approach and the system is investigated with regard to its applicability for surgical assistance monitoring.</p>
  </details>
</details>
<details>
  <summary>103. <b>标题：Precipitation Prediction Using an Ensemble of Lightweight Learners</b></summary>
  <p><b>编号</b>：[363]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09424</p>
  <p><b>作者</b>：Xinzhe Li,  Sun Rui,  Yiming Niu,  Yao Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Precipitation prediction plays, agriculture and industry, high precipitation events, prediction plays, plays a crucial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Precipitation prediction plays a crucial role in modern agriculture and industry. However, it poses significant challenges due to the diverse patterns and dynamics in time and space, as well as the scarcity of high precipitation events.
To address this challenge, we propose an ensemble learning framework that leverages multiple learners to capture the diverse patterns of precipitation distribution. Specifically, the framework consists of a precipitation predictor with multiple lightweight heads (learners) and a controller that combines the outputs from these heads. The learners and the controller are separately optimized with a proposed 3-stage training scheme.
By utilizing provided satellite images, the proposed approach can effectively model the intricate rainfall patterns, especially for high precipitation events. It achieved 1st place on the core test as well as the nowcasting leaderboards of the Weather4Cast 2023 competition. For detailed implementation, please refer to our GitHub repository at: this https URL.</p>
  </details>
</details>
<h1>机器学习</h1>
<details>
  <summary>1. <b>标题：A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask  Inpainting</b></summary>
  <p><b>编号</b>：[5]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10227</p>
  <p><b>作者</b>：Wouter Van Gansbeke,  Bert De Brabandere</p>
  <p><b>备注</b>：Code: this https URL</p>
  <p><b>关键词</b>：complex loss functions, object detection modules, specialized object detection, ad-hoc post-processing steps, instance segmentation networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Panoptic and instance segmentation networks are often trained with specialized object detection modules, complex loss functions, and ad-hoc post-processing steps to handle the permutation-invariance of the instance masks. This work builds upon Stable Diffusion and proposes a latent diffusion approach for panoptic segmentation, resulting in a simple architecture which omits these complexities. Our training process consists of two steps: (1) training a shallow autoencoder to project the segmentation masks to latent space; (2) training a diffusion model to allow image-conditioned sampling in latent space. The use of a generative model unlocks the exploration of mask completion or inpainting, which has applications in interactive segmentation. The experimental validation yields promising results for both panoptic segmentation and mask inpainting. While not setting a new state-of-the-art, our model's simplicity, generality, and mask completion capability are desirable properties.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：ChatQA: Building GPT-4 Level Conversational QA Models</b></summary>
  <p><b>编号</b>：[7]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10225</p>
  <p><b>作者</b>：Zihan Liu,  Wei Ping,  Rajarshi Roy,  Peng Xu,  Mohammad Shoeybi,  Bryan Catanzaro</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：conversational question answering, level accuracies, introduce ChatQA, question answering, conversational question</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：AutoFT: Robust Fine-Tuning by Optimizing Hyperparameters on OOD Data</b></summary>
  <p><b>编号</b>：[10]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10220</p>
  <p><b>作者</b>：Caroline Choi,  Yoonho Lee,  Annie Chen,  Allan Zhou,  Aditi Raghunathan,  Chelsea Finn</p>
  <p><b>备注</b>：16 pages</p>
  <p><b>关键词</b>：encode rich representations, models encode rich, Foundation models encode, fine-tuning, encode rich</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Foundation models encode rich representations that can be adapted to a desired task by fine-tuning on task-specific data. However, fine-tuning a model on one particular data distribution often compromises the model's original performance on other distributions. Current methods for robust fine-tuning utilize hand-crafted regularization techniques to constrain the fine-tuning process towards the base foundation model. Yet, it is hard to precisely specify what characteristics of the foundation model to retain during fine-tuning, as this depends on how the pre-training, fine-tuning, and evaluation data distributions relate to each other. We propose AutoFT, a data-driven approach for guiding foundation model fine-tuning. AutoFT optimizes fine-tuning hyperparameters to maximize performance on a small out-of-distribution (OOD) validation set. To guide fine-tuning in a granular way, AutoFT searches a highly expressive hyperparameter space that includes weight coefficients for many different losses, in addition to learning rate and weight decay values. We evaluate AutoFT on nine natural distribution shifts which include domain shifts and subpopulation shifts. Our experiments show that AutoFT significantly improves generalization to new OOD data, outperforming existing robust fine-tuning methods. Notably, AutoFT achieves new state-of-the-art performance on the WILDS-iWildCam and WILDS-FMoW benchmarks, outperforming the previous best methods by $6.0\%$ and $1.5\%$, respectively.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt  Tensor Products</b></summary>
  <p><b>编号</b>：[13]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10216</p>
  <p><b>作者</b>：Shengjie Luo,  Tianlang Chen,  Aditi S. Krishnapriyan</p>
  <p><b>备注</b>：36 pages; ICLR 2024 (Spotlight Presentation); Code: this https URL</p>
  <p><b>关键词</b>：Gaunt Tensor Product, Developing equivariant neural, tensor products, equivariant neural networks, group plays</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Developing equivariant neural networks for the E(3) group plays an important role in modeling 3D data across real-world applications. Enforcing this equivariance primarily involves the tensor products of irreducible representations (irreps). However, the computational complexity of such operations increases significantly as higher-order tensors are used. In this work, we propose a systematic approach to substantially accelerate the computation of the tensor products of irreps. We mathematically connect the commonly used Clebsch-Gordan coefficients to the Gaunt coefficients, which are integrals of products of three spherical harmonics. Through Gaunt coefficients, the tensor product of irreps becomes equivalent to the multiplication between spherical functions represented by spherical harmonics. This perspective further allows us to change the basis for the equivariant operations from spherical harmonics to a 2D Fourier basis. Consequently, the multiplication between spherical functions represented by a 2D Fourier basis can be efficiently computed via the convolution theorem and Fast Fourier Transforms. This transformation reduces the complexity of full tensor products of irreps from $\mathcal{O}(L^6)$ to $\mathcal{O}(L^3)$, where $L$ is the max degree of irreps. Leveraging this approach, we introduce the Gaunt Tensor Product, which serves as a new method to construct efficient equivariant operations across different model architectures. Our experiments on the Open Catalyst Project and 3BPA datasets demonstrate both the increased efficiency and improved performance of our approach.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Improving automatic detection of driver fatigue and distraction using  machine learning</b></summary>
  <p><b>编号</b>：[16]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10213</p>
  <p><b>作者</b>：Dongjiang Wu</p>
  <p><b>备注</b>：Master's thesis, 55 pages</p>
  <p><b>关键词</b>：distracted driving behaviors, recent years, advances in information, information technology, technology have played</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Changes and advances in information technology have played an important role in the development of intelligent vehicle systems in recent years. Driver fatigue and distracted driving are important factors in traffic accidents. Thus, onboard monitoring of driving behavior has become a crucial component of advanced driver assistance systems for intelligent vehicles. In this article, we present techniques for simultaneously detecting fatigue and distracted driving behaviors using vision-based and machine learning-based approaches. In driving fatigue detection, we use facial alignment networks to identify facial feature points in the images, and calculate the distance of the facial feature points to detect the opening and closing of the eyes and mouth. Furthermore, we use a convolutional neural network (CNN) based on the MobileNet architecture to identify various distracted driving behaviors. Experiments are performed on a PC based setup with a webcam and results are demonstrated using public datasets as well as custom datasets created for training and testing. Compared to previous approaches, we build our own datasets and provide better results in terms of accuracy and computation time.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Mastery Guided Non-parametric Clustering to Scale-up Strategy Prediction</b></summary>
  <p><b>编号</b>：[17]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10210</p>
  <p><b>作者</b>：Anup Shakya,  Vasile Rus,  Deepak Venugopal</p>
  <p><b>备注</b>：Proceedings of 37th AAAI Conference on Artificial Intelligence Artificial Intelligence for Education. arXiv admin note: substantial text overlap with arXiv:2308.03892</p>
  <p><b>关键词</b>：Adaptive Instructional Systems, Instructional Systems, Adaptive Instructional, problem-solving helps Adaptive, types of learners</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Predicting the strategy (sequence of concepts) that a student is likely to use in problem-solving helps Adaptive Instructional Systems (AISs) better adapt themselves to different types of learners based on their learning abilities. This can lead to a more dynamic, engaging, and personalized experience for students. To scale up training a prediction model (such as LSTMs) over large-scale education datasets, we develop a non-parametric approach to cluster symmetric instances in the data. Specifically, we learn a representation based on Node2Vec that encodes symmetries over mastery or skill level since, to solve a problem, it is natural that a student's strategy is likely to involve concepts in which they have gained mastery. Using this representation, we use DP-Means to group symmetric instances through a coarse-to-fine refinement of the clusters. We apply our model to learn strategies for Math learning from large-scale datasets from MATHia, a leading AIS for middle-school math learning. Our results illustrate that our approach can consistently achieve high accuracy using a small sample that is representative of the full dataset. Further, we show that this approach helps us learn strategies with high accuracy for students at different skill levels, i.e., leveraging symmetries improves fairness in the prediction model.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Eclectic Rule Extraction for Explainability of Deep Neural Network based  Intrusion Detection Systems</b></summary>
  <p><b>编号</b>：[20]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10207</p>
  <p><b>作者</b>：Jesse Ables,  Nathaniel Childers,  William Anderson,  Sudip Mittal,  Shahram Rahimi,  Ioana Banicescu,  Maria Seale</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Explainable Artificial Intelligence, Intrusion Detection Systems, paper addresses trust, addresses trust issues, trust issues created</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper addresses trust issues created from the ubiquity of black box algorithms and surrogate explainers in Explainable Intrusion Detection Systems (X-IDS). While Explainable Artificial Intelligence (XAI) aims to enhance transparency, black box surrogate explainers, such as Local Interpretable Model-Agnostic Explanation (LIME) and SHapley Additive exPlanation (SHAP), are difficult to trust. The black box nature of these surrogate explainers makes the process behind explanation generation opaque and difficult to understand. To avoid this problem, one can use transparent white box algorithms such as Rule Extraction (RE). There are three types of RE algorithms: pedagogical, decompositional, and eclectic. Pedagogical methods offer fast but untrustworthy white-box explanations, while decompositional RE provides trustworthy explanations with poor scalability. This work explores eclectic rule extraction, which strikes a balance between scalability and trustworthiness. By combining techniques from pedagogical and decompositional approaches, eclectic rule extraction leverages the advantages of both, while mitigating some of their drawbacks. The proposed Hybrid X-IDS architecture features eclectic RE as a white box surrogate explainer for black box Deep Neural Networks (DNN). The presented eclectic RE algorithm extracts human-readable rules from hidden layers, facilitating explainable and trustworthy rulesets. Evaluations on UNSW-NB15 and CIC-IDS-2017 datasets demonstrate the algorithm's ability to generate rulesets with 99.9% accuracy, mimicking DNN outputs. The contributions of this work include the hybrid X-IDS architecture, the eclectic rule extraction algorithm applicable to intrusion detection datasets, and a thorough analysis of performance and explainability, demonstrating the trade-offs involved in rule extraction speed and accuracy.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Divide and not forget: Ensemble of selectively trained experts in  Continual Learning</b></summary>
  <p><b>编号</b>：[24]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10191</p>
  <p><b>作者</b>：Grzegorz Rypeść,  Sebastian Cygert,  Valeriya Khan,  Tomasz Trzciński,  Bartosz Zieliński,  Bartłomiej Twardowski</p>
  <p><b>备注</b>：Accepted to ICLR2024 (main track), code is available at: this https URL</p>
  <p><b>关键词</b>：widen their applicability, models widen, SEED, Class-incremental learning, expert</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Class-incremental learning is becoming more popular as it helps models widen their applicability while not forgetting what they already know. A trend in this area is to use a mixture-of-expert technique, where different models work together to solve the task. However, the experts are usually trained all at once using whole task data, which makes them all prone to forgetting and increasing computational burden. To address this limitation, we introduce a novel approach named SEED. SEED selects only one, the most optimal expert for a considered task, and uses data from this task to fine-tune only this expert. For this purpose, each expert represents each class with a Gaussian distribution, and the optimal expert is selected based on the similarity of those distributions. Consequently, SEED increases diversity and heterogeneity within the experts while maintaining the high stability of this ensemble method. The extensive experiments demonstrate that SEED achieves state-of-the-art performance in exemplar-free settings across various scenarios, showing the potential of expert diversification through data in continual learning.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through  Text Reconstruction</b></summary>
  <p><b>编号</b>：[25]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10189</p>
  <p><b>作者</b>：Qingyun Wang,  Zixuan Zhang,  Hongxiang Li,  Xuan Liu,  Jiawei Han,  Heng Ji,  Huimin Zhao</p>
  <p><b>备注</b>：16 pages. Accepted by Findings of the Association for Computational Linguistics: EACL 2024. Code and resources are available at this https URL</p>
  <p><b>关键词</b>：entity extraction, entity, extraction, chemical domain faces, faces two unique</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Fine-grained few-shot entity extraction in the chemical domain faces two unique challenges. First, compared with entity extraction tasks in the general domain, sentences from chemical papers usually contain more entities. Moreover, entity extraction models usually have difficulty extracting entities of long-tailed types. In this paper, we propose Chem-FINESE, a novel sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to address these two challenges. Our Chem-FINESE has two components: a seq2seq entity extractor to extract named entities from the input sentence and a seq2seq self-validation module to reconstruct the original input sentence from extracted entities. Inspired by the fact that a good entity extraction system needs to extract entities faithfully, our new self-validation module leverages entity extraction results to reconstruct the original input sentence. Besides, we design a new contrastive loss to reduce excessive copying during the extraction process. Finally, we release ChemNER+, a new fine-grained chemical entity extraction dataset that is annotated by domain experts with the ChemNER schema. Experiments in few-shot settings with both ChemNER+ and CHEMET datasets show that our newly proposed framework has contributed up to 8.26% and 6.84% absolute F1-score gains respectively.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Transfer Learning in Human Activity Recognition: A Survey</b></summary>
  <p><b>编号</b>：[28]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10185</p>
  <p><b>作者</b>：Sourish Gunesh Dhekane,  Thomas Ploetz</p>
  <p><b>备注</b>：40 pages, 5 figures, 7 tables</p>
  <p><b>关键词</b>：human activity recognition, Sensor-based human activity, active research area, assisted living, activity recognition</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sensor-based human activity recognition (HAR) has been an active research area, owing to its applications in smart environments, assisted living, fitness, healthcare, etc. Recently, deep learning based end-to-end training has resulted in state-of-the-art performance in domains such as computer vision and natural language, where large amounts of annotated data are available. However, large quantities of annotated data are not available for sensor-based HAR. Moreover, the real-world settings on which the HAR is performed differ in terms of sensor modalities, classification tasks, and target users. To address this problem, transfer learning has been employed extensively. In this survey, we focus on these transfer learning methods in the application domains of smart home and wearables-based HAR. In particular, we provide a problem-solution perspective by categorizing and presenting the works in terms of their contributions and the challenges they address. We also present an updated view of the state-of-the-art for both application domains. Based on our analysis of 205 papers, we highlight the gaps in the literature and provide a roadmap for addressing them. This survey provides a reference to the HAR community, by summarizing the existing works and providing a promising research agenda.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Comprehensive OOD Detection Improvements</b></summary>
  <p><b>编号</b>：[31]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10176</p>
  <p><b>作者</b>：Anish Lakkapragada,  Amol Khanna,  Edward Raff,  Nathan Inkawhich</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：expected input distribution, model expected input, impactful decisions, recognizing when inference, machine learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As machine learning becomes increasingly prevalent in impactful decisions, recognizing when inference data is outside the model's expected input distribution is paramount for giving context to predictions. Out-of-distribution (OOD) detection methods have been created for this task. Such methods can be split into representation-based or logit-based methods from whether they respectively utilize the model's embeddings or predictions for OOD detection. In contrast to most papers which solely focus on one such group, we address both. We employ dimensionality reduction on feature embeddings in representation-based methods for both time speedups and improved performance. Additionally, we propose DICE-COL, a modification of the popular logit-based method Directed Sparsification (DICE) that resolves an unnoticed flaw. We demonstrate the effectiveness of our methods on the OpenOODv1.5 benchmark framework, where they significantly improve performance and set state-of-the-art results.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：DISTINQT: A Distributed Privacy Aware Learning Framework for QoS  Prediction for Future Mobile and Wireless Networks</b></summary>
  <p><b>编号</b>：[35]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10158</p>
  <p><b>作者</b>：Nikolaos Koursioumpas,  Lina Magoula,  Ioannis Stavrakakis,  Nancy Alonistioti,  M. A. Gutierrez-Estevez,  Ramin Khalili</p>
  <p><b>备注</b>：11 Pages Double Column, 9 Figures, Submitted for possible publication in the IEEE Transactions on Vehicular Technology (IEEE TVT)</p>
  <p><b>关键词</b>：Quality of Service, level of Quality, operate smoothly, QoS prediction, centralized Artificial Intelligence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Beyond 5G and 6G networks are expected to support new and challenging use cases and applications that depend on a certain level of Quality of Service (QoS) to operate smoothly. Predicting the QoS in a timely manner is of high importance, especially for safety-critical applications as in the case of vehicular communications. Although until recent years the QoS prediction has been carried out by centralized Artificial Intelligence (AI) solutions, a number of privacy, computational, and operational concerns have emerged. Alternative solutions have been surfaced (e.g. Split Learning, Federated Learning), distributing AI tasks of reduced complexity across nodes, while preserving the privacy of the data. However, new challenges rise when it comes to scalable distributed learning approaches, taking into account the heterogeneous nature of future wireless networks. The current work proposes DISTINQT, a privacy-aware distributed learning framework for QoS prediction. Our framework supports multiple heterogeneous nodes, in terms of data types and model architectures, by sharing computations across them. This, enables the incorporation of diverse knowledge into a sole learning process that will enhance the robustness and generalization capabilities of the final QoS prediction model. DISTINQT also contributes to data privacy preservation by encoding any raw input data into a non-linear latent representation before any transmission. Evaluation results showcase that our framework achieves a statistically identical performance compared to its centralized version and an average performance improvement of up to 65% against six state-of-the-art centralized baseline solutions in the Tele-Operated Driving use case.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：A novel hybrid time-varying graph neural network for traffic flow  forecasting</b></summary>
  <p><b>编号</b>：[37]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10155</p>
  <p><b>作者</b>：Ben Ao Dai,  Bao-Lin Ye</p>
  <p><b>备注</b>：12 pages 1figures</p>
  <p><b>关键词</b>：urban road networks, traffic flow prediction, describe spatial correlation, accurate traffic flow, traffic flow</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Real-time and accurate traffic flow prediction is the foundation for ensuring the efficient operation of intelligent transportation this http URL existing traffic flow prediction methods based on graph neural networks (GNNs), pre-defined graphs were usually used to describe the spatial correlations of different traffic nodes in urban road networks. However, the ability of pre-defined graphs used to describe spatial correlation was limited by prior knowledge and graph generation methods. Although time-varying graphs based on data-driven learning can partially overcome the drawbacks of pre-defined graphs, the learning ability of existing adaptive graphs was limited. For example, time-varying graphs cannot adequately capture the inherent spatial correlations in traffic flow this http URL order to solve these problems, we have proposed a hybrid time-varying graph neural network (HTVGNN) for traffic flow prediction.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Multi-Agent Reinforcement Learning for Maritime Operational Technology  Cyber Security</b></summary>
  <p><b>编号</b>：[40]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10149</p>
  <p><b>作者</b>：Alec Wilson,  Ryan Menzies,  Neela Morarji,  David Foster,  Marco Casassa Mont,  Esin Turkbeyler,  Lisa Gralewski</p>
  <p><b>备注</b>：13 pages, 7 figures, Proceedings of the Conference on Applied Machine Learning in Information Security 2023 (CAMLIS)</p>
  <p><b>关键词</b>：Multi-Agent Reinforcement Learning, explore Multi-Agent Reinforcement, Reinforcement Learning, IPMS Operational Technology, Integrated Platform Management</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper demonstrates the potential for autonomous cyber defence to be applied on industrial control systems and provides a baseline environment to further explore Multi-Agent Reinforcement Learning's (MARL) application to this problem domain. It introduces a simulation environment, IPMSRL, of a generic Integrated Platform Management System (IPMS) and explores the use of MARL for autonomous cyber defence decision-making on generic maritime based IPMS Operational Technology (OT). OT cyber defensive actions are less mature than they are for Enterprise IT. This is due to the relatively brittle nature of OT infrastructure originating from the use of legacy systems, design-time engineering assumptions, and lack of full-scale modern security controls. There are many obstacles to be tackled across the cyber landscape due to continually increasing cyber-attack sophistication and the limitations of traditional IT-centric cyber defence solutions. Traditional IT controls are rarely deployed on OT infrastructure, and where they are, some threats aren't fully addressed. In our experiments, a shared critic implementation of Multi Agent Proximal Policy Optimisation (MAPPO) outperformed Independent Proximal Policy Optimisation (IPPO). MAPPO reached an optimal policy (episode outcome mean of 1) after 800K timesteps, whereas IPPO was only able to reach an episode outcome mean of 0.966 after one million timesteps. Hyperparameter tuning greatly improved training performance. Across one million timesteps the tuned hyperparameters reached an optimal policy whereas the default hyperparameters only managed to win sporadically, with most simulations resulting in a draw. We tested a real-world constraint, attack detection alert success, and found that when alert success probability is reduced to 0.75 or 0.9, the MARL defenders were still able to win in over 97.5% or 99.5% of episodes, respectively.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Explicitly Disentangled Representations in Object-Centric Learning</b></summary>
  <p><b>编号</b>：[41]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10148</p>
  <p><b>作者</b>：Riccardo Majellaro,  Jonathan Collu,  Aske Plaat,  Thomas M. Moerland</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Extracting structured representations, raw visual data, Extracting structured, raw visual, important and long-standing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Extracting structured representations from raw visual data is an important and long-standing challenge in machine learning. Recently, techniques for unsupervised learning of object-centric representations have raised growing interest. In this context, enhancing the robustness of the latent features can improve the efficiency and effectiveness of the training of downstream tasks. A promising step in this direction is to disentangle the factors that cause variation in the data. Previously, Invariant Slot Attention disentangled position, scale, and orientation from the remaining features. Extending this approach, we focus on separating the shape and texture components. In particular, we propose a novel architecture that biases object-centric models toward disentangling shape and texture components into two non-overlapping subsets of the latent space dimensions. These subsets are known a priori, hence before the training process. Experiments on a range of object-centric benchmarks reveal that our approach achieves the desired disentanglement while also numerically improving baseline performance in most cases. In addition, we show that our method can generate novel textures for a specific object or transfer textures between objects with distinct shapes.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Spatial-Temporal Large Language Model for Traffic Prediction</b></summary>
  <p><b>编号</b>：[46]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10134</p>
  <p><b>作者</b>：Chenxi Liu,  Sun Yang,  Qianxiong Xu,  Zhishuai Li,  Cheng Long,  Ziyue Li,  Rui Zhao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：intelligent transportation systems, foresee future traffic, Large Language, transportation systems, endeavors to foresee</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Traffic prediction, a critical component for intelligent transportation systems, endeavors to foresee future traffic at specific locations using historical data. Although existing traffic prediction models often emphasize developing complex neural network structures, their accuracy has not seen improvements accordingly. Recently, Large Language Models (LLMs) have shown outstanding capabilities in time series analysis. Differing from existing models, LLMs progress mainly through parameter expansion and extensive pre-training while maintaining their fundamental structures. In this paper, we propose a Spatial-Temporal Large Language Model (ST-LLM) for traffic prediction. Specifically, ST-LLM redefines the timesteps at each location as tokens and incorporates a spatial-temporal embedding module to learn the spatial location and global temporal representations of tokens. Then these representations are fused to provide each token with unified spatial and temporal information. Furthermore, we propose a novel partially frozen attention strategy of the LLM, which is designed to capture spatial-temporal dependencies for traffic prediction. Comprehensive experiments on real traffic datasets offer evidence that ST-LLM outperforms state-of-the-art models. Notably, the ST-LLM also exhibits robust performance in both few-shot and zero-shot prediction scenarios.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Towards Principled Graph Transformers</b></summary>
  <p><b>编号</b>：[49]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10119</p>
  <p><b>作者</b>：Luis Müller,  Christopher Morris</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：learning architectures based, k-dimensional Weisfeiler-Leman, well-understood expressive power, Edge Transformer, Graph learning architectures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Graph learning architectures based on the k-dimensional Weisfeiler-Leman (k-WL) hierarchy offer a theoretically well-understood expressive power. However, such architectures often fail to deliver solid predictive performance on real-world tasks, limiting their practical impact. In contrast, global attention-based models such as graph transformers demonstrate strong performance in practice, but comparing their expressive power with the k-WL hierarchy remains challenging, particularly since these architectures rely on positional or structural encodings for their expressivity and predictive performance. To address this, we show that the recently proposed Edge Transformer, a global attention model operating on node pairs instead of nodes, has at least 3-WL expressive power. Empirically, we demonstrate that the Edge Transformer surpasses other theoretically aligned architectures regarding predictive performance while not relying on positional or structural encodings.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Optimizing Medication Decisions for Patients with Atrial Fibrillation  through Path Development Network</b></summary>
  <p><b>编号</b>：[85]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10014</p>
  <p><b>作者</b>：Tian Xie</p>
  <p><b>备注</b>：Master's thesis</p>
  <p><b>关键词</b>：common cardiac arrhythmia, cardiac arrhythmia characterized, common cardiac, cardiac arrhythmia, arrhythmia characterized</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Atrial fibrillation (AF) is a common cardiac arrhythmia characterized by rapid and irregular contractions of the atria. It significantly elevates the risk of strokes due to slowed blood flow in the atria, especially in the left atrial appendage, which is prone to blood clot formation. Such clots can migrate into cerebral arteries, leading to ischemic stroke. To assess whether AF patients should be prescribed anticoagulants, doctors often use the CHA2DS2-VASc scoring system. However, anticoagulant use must be approached with caution as it can impact clotting functions. This study introduces a machine learning algorithm that predicts whether patients with AF should be recommended anticoagulant therapy using 12-lead ECG data. In this model, we use STOME to enhance time-series data and then process it through a Convolutional Neural Network (CNN). By incorporating a path development layer, the model achieves a specificity of 30.6% under the condition of an NPV of 1. In contrast, LSTM algorithms without path development yield a specificity of only 2.7% under the same NPV condition.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Developing an AI-based Integrated System for Bee Health Evaluation</b></summary>
  <p><b>编号</b>：[93]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09988</p>
  <p><b>作者</b>：Andrew Liang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：world food supply, past decade due, Honey bees pollinate, food supply, including pesticides</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Honey bees pollinate about one-third of the world's food supply, but bee colonies have alarmingly declined by nearly 40% over the past decade due to several factors, including pesticides and pests. Traditional methods for monitoring beehives, such as human inspection, are subjective, disruptive, and time-consuming. To overcome these limitations, artificial intelligence has been used to assess beehive health. However, previous studies have lacked an end-to-end solution and primarily relied on data from a single source, either bee images or sounds. This study introduces a comprehensive system consisting of bee object detection and health evaluation. Additionally, it utilized a combination of visual and audio signals to analyze bee behaviors. An Attention-based Multimodal Neural Network (AMNN) was developed to adaptively focus on key features from each type of signal for accurate bee health assessment. The AMNN achieved an overall accuracy of 92.61%, surpassing eight existing single-signal Convolutional Neural Networks and Recurrent Neural Networks. It outperformed the best image-based model by 32.51% and the top sound-based model by 13.98% while maintaining efficient processing times. Furthermore, it improved prediction robustness, attaining an F1-score higher than 90% across all four evaluated health conditions. The study also shows that audio signals are more reliable than images for assessing bee health. By seamlessly integrating AMNN with image and sound data in a comprehensive bee health monitoring system, this approach provides a more efficient and non-invasive solution for the early detection of bee diseases and the preservation of bee colonies.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：FLex&Chill: Improving Local Federated Learning Training with Logit  Chilling</b></summary>
  <p><b>编号</b>：[95]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09986</p>
  <p><b>作者</b>：Kichang Lee,  Songkuk Kim,  JeongGil Ko</p>
  <p><b>备注</b>：9 pages</p>
  <p><b>关键词</b>：Logit Chilling method, local clients, Federated learning, inherently hampered, non-iid distributed training</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Federated learning are inherently hampered by data heterogeneity: non-iid distributed training data over local clients. We propose a novel model training approach for federated learning, FLex&Chill, which exploits the Logit Chilling method. Through extensive evaluations, we demonstrate that, in the presence of non-iid data characteristics inherent in federated learning systems, this approach can expedite model convergence and improve inference accuracy. Quantitatively, from our experiments, we observe up to 6X improvement in the global federated learning model convergence time, and up to 3.37% improvement in inference accuracy.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Through the Dual-Prism: A Spectral Perspective on Graph Data  Augmentation for Graph Classification</b></summary>
  <p><b>编号</b>：[108]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09953</p>
  <p><b>作者</b>：Yutong Xia,  Runpeng Yu,  Yuxuan Liang,  Xavier Bresson,  Xinchao Wang,  Roger Zimmermann</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Graph Neural Networks, Neural Networks, data augmentation techniques, Graph Neural, preferred tool</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Graph Neural Networks (GNNs) have become the preferred tool to process graph data, with their efficacy being boosted through graph data augmentation techniques. Despite the evolution of augmentation methods, issues like graph property distortions and restricted structural changes persist. This leads to the question: Is it possible to develop more property-conserving and structure-sensitive augmentation methods? Through a spectral lens, we investigate the interplay between graph properties, their augmentation, and their spectral behavior, and found that keeping the low-frequency eigenvalues unchanged can preserve the critical properties at a large scale when generating augmented graphs. These observations inform our introduction of the Dual-Prism (DP) augmentation method, comprising DP-Noise and DP-Mask, which adeptly retains essential graph properties while diversifying augmented graphs. Extensive experiments validate the efficiency of our approach, providing a new and promising direction for graph data augmentation.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：SymbolNet: Neural Symbolic Regression with Adaptive Dynamic Pruning</b></summary>
  <p><b>编号</b>：[109]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09949</p>
  <p><b>作者</b>：Ho Fung Tsoi,  Vladimir Loncar,  Sridhara Dasu,  Philip Harris</p>
  <p><b>备注</b>：11 pages. Submitted to IEEE TNNLS, under review</p>
  <p><b>关键词</b>：faster equation searching, leverage gradient methods, neural network approach, high input dimension, genetic programming</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Contrary to the use of genetic programming, the neural network approach to symbolic regression can scale well with high input dimension and leverage gradient methods for faster equation searching. Common ways of constraining expression complexity have relied on multistage pruning methods with fine-tuning, but these often lead to significant performance loss. In this work, we propose SymbolNet, a neural network approach to symbolic regression in a novel framework that enables dynamic pruning of model weights, input features, and mathematical operators in a single training, where both training loss and expression complexity are optimized simultaneously. We introduce a sparsity regularization term per pruning type, which can adaptively adjust its own strength and lead to convergence to a target sparsity level. In contrast to most existing symbolic regression methods that cannot efficiently handle datasets with more than $O$(10) inputs, we demonstrate the effectiveness of our model on the LHC jet tagging task (16 inputs), MNIST (784 inputs), and SVHN (3072 inputs).</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：HGAttack: Transferable Heterogeneous Graph Adversarial Attack</b></summary>
  <p><b>编号</b>：[110]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09945</p>
  <p><b>作者</b>：He Zhao,  Zhiwei Zeng,  Yongwei Wang,  Deheng Ye,  Chunyan Miao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Graph Neural Networks, Neural Networks, Heterogeneous Graph Neural, Graph Neural, adversarial attack methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Heterogeneous Graph Neural Networks (HGNNs) are increasingly recognized for their performance in areas like the web and e-commerce, where resilience against adversarial attacks is crucial. However, existing adversarial attack methods, which are primarily designed for homogeneous graphs, fall short when applied to HGNNs due to their limited ability to address the structural and semantic complexity of HGNNs. This paper introduces HGAttack, the first dedicated gray box evasion attack method for heterogeneous graphs. We design a novel surrogate model to closely resemble the behaviors of the target HGNN and utilize gradient-based methods for perturbation generation. Specifically, the proposed surrogate model effectively leverages heterogeneous information by extracting meta-path induced subgraphs and applying GNNs to learn node embeddings with distinct semantics from each subgraph. This approach improves the transferability of generated attacks on the target HGNN and significantly reduces memory costs. For perturbation generation, we introduce a semantics-aware mechanism that leverages subgraph gradient information to autonomously identify vulnerable edges across a wide range of relations within a constrained perturbation budget. We validate HGAttack's efficacy with comprehensive experiments on three datasets, providing empirical analyses of its generated perturbations. Outperforming baseline methods, HGAttack demonstrated significant efficacy in diminishing the performance of target HGNN models, affirming the effectiveness of our approach in evaluating the robustness of HGNNs against adversarial attacks.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：WindSeer: Real-time volumetric wind prediction over complex terrain  aboard a small UAV</b></summary>
  <p><b>编号</b>：[111]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09944</p>
  <p><b>作者</b>：Florian Achermann,  Thomas Stastny,  Bogdan Danciu,  Andrey Kolobov,  Jen Jen Chung,  Roland Siegwart,  Nicholas Lawrance</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：including safe manned, applications including safe, unmanned aviation, including safe, safe manned</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Real-time high-resolution wind predictions are beneficial for various applications including safe manned and unmanned aviation. Current weather models require too much compute and lack the necessary predictive capabilities as they are valid only at the scale of multiple kilometers and hours - much lower spatial and temporal resolutions than these applications require. Our work, for the first time, demonstrates the ability to predict low-altitude wind in real-time on limited-compute devices, from only sparse measurement data. We train a neural network, WindSeer, using only synthetic data from computational fluid dynamics simulations and show that it can successfully predict real wind fields over terrain with known topography from just a few noisy and spatially clustered wind measurements. WindSeer can generate accurate predictions at different resolutions and domain sizes on previously unseen topography without retraining. We demonstrate that the model successfully predicts historical wind data collected by weather stations and wind measured onboard drones.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Infinite-Horizon Graph Filters: Leveraging Power Series to Enhance  Sparse Information Aggregation</b></summary>
  <p><b>编号</b>：[112]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09943</p>
  <p><b>作者</b>：Ruizhe Zhang,  Xinke Jiang,  Yuchen Fang,  Jiayuan Luo,  Yongxin Xu,  Yichen Zhu,  Xu Chu,  Junfeng Zhao,  Yasha Zhao</p>
  <p><b>备注</b>：v1</p>
  <p><b>关键词</b>：Graph Neural Networks, shown considerable effectiveness, Neural Networks, graph learning tasks, Graph Neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Graph Neural Networks (GNNs) have shown considerable effectiveness in a variety of graph learning tasks, particularly those based on the message-passing approach in recent years. However, their performance is often constrained by a limited receptive field, a challenge that becomes more acute in the presence of sparse graphs. In light of the power series, which possesses infinite expansion capabilities, we propose a novel \underline{G}raph \underline{P}ower \underline{F}ilter \underline{N}eural Network (GPFN) that enhances node classification by employing a power series graph filter to augment the receptive field. Concretely, our GPFN designs a new way to build a graph filter with an infinite receptive field based on the convergence power series, which can be analyzed in the spectral and spatial domains. Besides, we theoretically prove that our GPFN is a general framework that can integrate any power series and capture long-range dependencies. Finally, experimental results on three datasets demonstrate the superiority of our GPFN over state-of-the-art baselines.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Biases in Expected Goals Models Confound Finishing Ability</b></summary>
  <p><b>编号</b>：[114]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09940</p>
  <p><b>作者</b>：Jesse Davis,  Pieter Robberechts</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：evaluating finishing skill, popular tool, tool for evaluating, finishing skill, Expected Goals</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Expected Goals (xG) has emerged as a popular tool for evaluating finishing skill in soccer analytics. It involves comparing a player's cumulative xG with their actual goal output, where consistent overperformance indicates strong finishing ability. However, the assessment of finishing skill in soccer using xG remains contentious due to players' difficulty in consistently outperforming their cumulative xG. In this paper, we aim to address the limitations and nuances surrounding the evaluation of finishing skill using xG statistics. Specifically, we explore three hypotheses: (1) the deviation between actual and expected goals is an inadequate metric due to the high variance of shot outcomes and limited sample sizes, (2) the inclusion of all shots in cumulative xG calculation may be inappropriate, and (3) xG models contain biases arising from interdependencies in the data that affect skill measurement. We found that sustained overperformance of cumulative xG requires both high shot volumes and exceptional finishing, including all shot types can obscure the finishing ability of proficient strikers, and that there is a persistent bias that makes the actual and expected goals closer for excellent finishers than it really is. Overall, our analysis indicates that we need more nuanced quantitative approaches for investigating a player's finishing ability, which we achieved using a technique from AI fairness to learn an xG model that is calibrated for multiple subgroups of players. As a concrete use case, we show that (1) the standard biased xG model underestimates Messi's GAX by 17% and (2) Messi's GAX is 27% higher than the typical elite high-shot-volume attacker, indicating that Messi is even a more exceptional finisher than people commonly believed.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Probabilistic Truly Unordered Rule Sets</b></summary>
  <p><b>编号</b>：[121]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09918</p>
  <p><b>作者</b>：Lincen Yang,  Matthijs van Leeuwen</p>
  <p><b>备注</b>：Submitted to JMLR</p>
  <p><b>关键词</b>：Existing methods, recently been frequently, frequently revisited, rules, Rule Sets</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Rule set learning has recently been frequently revisited because of its interpretability. Existing methods have several shortcomings though. First, most existing methods impose orders among rules, either explicitly or implicitly, which makes the models less comprehensible. Second, due to the difficulty of handling conflicts caused by overlaps (i.e., instances covered by multiple rules), existing methods often do not consider probabilistic rules. Third, learning classification rules for multi-class target is understudied, as most existing methods focus on binary classification or multi-class classification via the ``one-versus-rest" approach.
To address these shortcomings, we propose TURS, for Truly Unordered Rule Sets. To resolve conflicts caused by overlapping rules, we propose a novel model that exploits the probabilistic properties of our rule sets, with the intuition of only allowing rules to overlap if they have similar probabilistic outputs. We next formalize the problem of learning a TURS model based on the MDL principle and develop a carefully designed heuristic algorithm. We benchmark against a wide range of rule-based methods and demonstrate that our method learns rule sets that have lower model complexity and highly competitive predictive performance. In addition, we empirically show that rules in our model are empirically ``independent" and hence truly unordered.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Enabling On-device Continual Learning with Binary Neural Networks</b></summary>
  <p><b>编号</b>：[122]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09916</p>
  <p><b>作者</b>：Lorenzo Vorabbi,  Davide Maltoni,  Guido Borghi,  Stefano Santi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：limited computational capabilities, Binary Neural Networks, computational capabilities, On-device learning remains, remains a formidable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>On-device learning remains a formidable challenge, especially when dealing with resource-constrained devices that have limited computational capabilities. This challenge is primarily rooted in two key issues: first, the memory available on embedded devices is typically insufficient to accommodate the memory-intensive back-propagation algorithm, which often relies on floating-point precision. Second, the development of learning algorithms on models with extreme quantization levels, such as Binary Neural Networks (BNNs), is critical due to the drastic reduction in bit representation. In this study, we propose a solution that combines recent advancements in the field of Continual Learning (CL) and Binary Neural Networks to enable on-device training while maintaining competitive performance. Specifically, our approach leverages binary latent replay (LR) activations and a novel quantization scheme that significantly reduces the number of bits required for gradient computation. The experimental validation demonstrates a significant accuracy improvement in combination with a noticeable reduction in memory requirement, confirming the suitability of our approach in expanding the practical applications of deep learning in real-world scenarios.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：A Survey on Hardware Accelerators for Large Language Models</b></summary>
  <p><b>编号</b>：[129]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09890</p>
  <p><b>作者</b>：Christoforos Kachris</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, generate human-like text, language processing tasks, natural language processing, Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) have emerged as powerful tools for natural language processing tasks, revolutionizing the field with their ability to understand and generate human-like text. As the demand for more sophisticated LLMs continues to grow, there is a pressing need to address the computational challenges associated with their scale and complexity. This paper presents a comprehensive survey on hardware accelerators designed to enhance the performance and energy efficiency of Large Language Models. By examining a diverse range of accelerators, including GPUs, FPGAs, and custom-designed architectures, we explore the landscape of hardware solutions tailored to meet the unique computational demands of LLMs. The survey encompasses an in-depth analysis of architecture, performance metrics, and energy efficiency considerations, providing valuable insights for researchers, engineers, and decision-makers aiming to optimize the deployment of LLMs in real-world applications.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Cooperative Edge Caching Based on Elastic Federated and Multi-Agent Deep  Reinforcement Learning in Next-Generation Network</b></summary>
  <p><b>编号</b>：[130]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09886</p>
  <p><b>作者</b>：Qiong Wu,  Wenhua Wang,  Pingyi Fan,  Qiang Fan,  Huiling Zhu,  Khaled B. Letaief</p>
  <p><b>备注</b>：This paper has been submitted to IEEE TNSM. The source code has been released at: this https URL</p>
  <p><b>关键词</b>：small-cell base stations, popular contents, empowering caching units, users' requested contents, contents</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Edge caching is a promising solution for next-generation networks by empowering caching units in small-cell base stations (SBSs), which allows user equipments (UEs) to fetch users' requested contents that have been pre-cached in SBSs. It is crucial for SBSs to predict accurate popular contents through learning while protecting users' personal information. Traditional federated learning (FL) can protect users' privacy but the data discrepancies among UEs can lead to a degradation in model quality. Therefore, it is necessary to train personalized local models for each UE to predict popular contents accurately. In addition, the cached contents can be shared among adjacent SBSs in next-generation networks, thus caching predicted popular contents in different SBSs may affect the cost to fetch contents. Hence, it is critical to determine where the popular contents are cached cooperatively. To address these issues, we propose a cooperative edge caching scheme based on elastic federated and multi-agent deep reinforcement learning (CEFMR) to optimize the cost in the network. We first propose an elastic FL algorithm to train the personalized model for each UE, where adversarial autoencoder (AAE) model is adopted for training to improve the prediction accuracy, then {a popular} content prediction algorithm is proposed to predict the popular contents for each SBS based on the trained AAE model. Finally, we propose a multi-agent deep reinforcement learning (MADRL) based algorithm to decide where the predicted popular contents are collaboratively cached among SBSs. Our experimental results demonstrate the superiority of our proposed scheme to existing baseline caching schemes.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：GA-SmaAt-GNet: Generative Adversarial Small Attention GNet for Extreme  Precipitation Nowcasting</b></summary>
  <p><b>编号</b>：[133]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09881</p>
  <p><b>作者</b>：Eloy Reulen,  Siamak Mehrkanoon</p>
  <p><b>备注</b>：16 pages, 11 figurs</p>
  <p><b>关键词</b>：gained considerable traction, data-driven modeling approaches, recent years, data-driven modeling, meteorological applications</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, data-driven modeling approaches have gained considerable traction in various meteorological applications, particularly in the realm of weather forecasting. However, these approaches often encounter challenges when dealing with extreme weather conditions. In light of this, we propose GA-SmaAt-GNet, a novel generative adversarial architecture that makes use of two methodologies aimed at enhancing the performance of deep learning models for extreme precipitation nowcasting. Firstly, it uses a novel SmaAt-GNet built upon the successful SmaAt-UNet architecture as generator. This network incorporates precipitation masks (binarized precipitation maps) as an additional data source, leveraging valuable information for improved predictions. Additionally, GA-SmaAt-GNet utilizes an attention-augmented discriminator inspired by the well-established Pix2Pix architecture. Furthermore, we assess the performance of GA-SmaAt-GNet using real-life precipitation dataset from the Netherlands. Our experimental results reveal a notable improvement in both overall performance and for extreme precipitation events. Furthermore, we conduct uncertainty analysis on the proposed GA-SmaAt-GNet model as well as on the precipitation dataset, providing additional insights into the predictive capabilities of the model. Finally, we offer further insights into the predictions of our proposed model using Grad-CAM. This visual explanation technique generates activation heatmaps, illustrating areas of the input that are more activated for various parts of the network.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Attention-Based Recurrent Neural Network For Automatic Behavior Laying  Hen Recognition</b></summary>
  <p><b>编号</b>：[134]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09880</p>
  <p><b>作者</b>：Fréjus A. A. Laleye,  Mikaël A. Mousse</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：modern poultry farming, laying hens, laying hen behavior, frequency domain features, laying hen call</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>One of the interests of modern poultry farming is the vocalization of laying hens which contain very useful information on health behavior. This information is used as health and well-being indicators that help breeders better monitor laying hens, which involves early detection of problems for rapid and more effective intervention. In this work, we focus on the sound analysis for the recognition of the types of calls of the laying hens in order to propose a robust system of characterization of their behavior for a better monitoring. To do this, we first collected and annotated laying hen call signals, then designed an optimal acoustic characterization based on the combination of time and frequency domain features. We then used these features to build the multi-label classification models based on recurrent neural network to assign a semantic class to the vocalization that characterize the laying hen behavior. The results show an overall performance with our model based on the combination of time and frequency domain features that obtained the highest F1-score (F1=92.75) with a gain of 17% on the models using the frequency domain features and of 8% on the compared approaches from the litterature.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Reconciling Spatial and Temporal Abstractions for Goal Representation</b></summary>
  <p><b>编号</b>：[137]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09870</p>
  <p><b>作者</b>：Mehdi Zadem,  Sergio Mover,  Sao Mai Nguyen</p>
  <p><b>备注</b>：Accepted for publication in ICLR 2024</p>
  <p><b>关键词</b>：Hierarchical Reinforcement Learning, Hierarchical Reinforcement, Reinforcement Learning, performance of Hierarchical, complex learning problem</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Goal representation affects the performance of Hierarchical Reinforcement Learning (HRL) algorithms by decomposing the complex learning problem into easier subtasks. Recent studies show that representations that preserve temporally abstract environment dynamics are successful in solving difficult problems and provide theoretical guarantees for optimality. These methods however cannot scale to tasks where environment dynamics increase in complexity i.e. the temporally abstract transition relations depend on larger number of variables. On the other hand, other efforts have tried to use spatial abstraction to mitigate the previous issues. Their limitations include scalability to high dimensional environments and dependency on prior knowledge.
In this paper, we propose a novel three-layer HRL algorithm that introduces, at different levels of the hierarchy, both a spatial and a temporal goal abstraction. We provide a theoretical study of the regret bounds of the learned policies. We evaluate the approach on complex continuous control tasks, demonstrating the effectiveness of spatial and temporal abstractions learned by this approach.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Improving fine-grained understanding in image-text pre-training</b></summary>
  <p><b>编号</b>：[139]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09865</p>
  <p><b>作者</b>：Ioana Bica,  Anastasija Ilić,  Matthias Bauer,  Goker Erdogan,  Matko Bošnjak,  Christos Kaplanis,  Alexey A. Gritsenko,  Matthias Minderer,  Charles Blundell,  Razvan Pascanu,  Jovana Mitrović</p>
  <p><b>备注</b>：26 pages</p>
  <p><b>关键词</b>：Fine-grained Contrastive Alignment, Contrastive Alignment, image-text pairs, introduce SPARse Fine-grained, image patches</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce SPARse Fine-grained Contrastive Alignment (SPARC), a simple method for pretraining more fine-grained multimodal representations from image-text pairs. Given that multiple image patches often correspond to single words, we propose to learn a grouping of image patches for every token in the caption. To achieve this, we use a sparse similarity metric between image patches and language tokens and compute for each token a language-grouped vision embedding as the weighted average of patches. The token and language-grouped vision embeddings are then contrasted through a fine-grained sequence-wise loss that only depends on individual samples and does not require other batch samples as negatives. This enables more detailed information to be learned in a computationally inexpensive manner. SPARC combines this fine-grained loss with a contrastive loss between global image and text embeddings to learn representations that simultaneously encode global and local information. We thoroughly evaluate our proposed method and show improved performance over competing approaches both on image-level tasks relying on coarse-grained information, e.g. classification, as well as region-level tasks relying on fine-grained information, e.g. retrieval, object detection, and segmentation. Moreover, SPARC improves model faithfulness and captioning in foundational vision-language models.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Evolutionary Multi-Objective Optimization of Large Language Model  Prompts for Balancing Sentiments</b></summary>
  <p><b>编号</b>：[140]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09862</p>
  <p><b>作者</b>：Jill Baumann,  Oliver Kramer</p>
  <p><b>备注</b>：Accepted in EvoApps at EvoStar 2024</p>
  <p><b>关键词</b>：attracted considerable attention, large language models, advent of large, large language, ChatGPT has attracted</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The advent of large language models (LLMs) such as ChatGPT has attracted considerable attention in various domains due to their remarkable performance and versatility. As the use of these models continues to grow, the importance of effective prompt engineering has come to the fore. Prompt optimization emerges as a crucial challenge, as it has a direct impact on model performance and the extraction of relevant information. Recently, evolutionary algorithms (EAs) have shown promise in addressing this issue, paving the way for novel optimization strategies. In this work, we propose a evolutionary multi-objective (EMO) approach specifically tailored for prompt optimization called EMO-Prompts, using sentiment analysis as a case study. We use sentiment analysis capabilities as our experimental targets. Our results demonstrate that EMO-Prompts effectively generates prompts capable of guiding the LLM to produce texts embodying two conflicting emotions simultaneously.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：PPNet: A Novel Neural Network Structure for End-to-End Near-Optimal Path  Planning</b></summary>
  <p><b>编号</b>：[159]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09819</p>
  <p><b>作者</b>：Qinglong Meng,  Chongkun Xia,  Xueqian Wang,  Songping Mai,  Bin Liang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：classical path planners, sampling-based path planners, path planning, path planners, path</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The classical path planners, such as sampling-based path planners, have the limitations of sensitivity to the initial solution and slow convergence to the optimal solution. However, finding a near-optimal solution in a short period is challenging in many applications such as the autonomous vehicle with limited power/fuel. To achieve an end-to-end near-optimal path planner, we first divide the path planning problem into two subproblems, which are path's space segmentation and waypoints generation in the given path's space. We further propose a two-level cascade neural network named Path Planning Network (PPNet) to solve the path planning problem by solving the abovementioned subproblems. Moreover, we propose a novel efficient data generation method for path planning named EDaGe-PP. The results show the total computation time is less than 1/33 and the success rate of PPNet trained by the dataset that is generated by EDaGe-PP is about $2 \times$ compared to other methods. We validate PPNet against state-of-the-art path planning methods. The results show PPNet can find a near-optimal solution in 15.3ms, which is much shorter than the state-of-the-art path planners.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Clickbait vs. Quality: How Engagement-Based Optimization Shapes the  Content Landscape in Online Platforms</b></summary>
  <p><b>编号</b>：[162]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09804</p>
  <p><b>作者</b>：Nicole Immorlica,  Meena Jagadeesan,  Brendan Lucier</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：content, content creators, Online content platforms, engagement-based optimization, content platforms commonly</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Online content platforms commonly use engagement-based optimization when making recommendations. This encourages content creators to invest in quality, but also rewards gaming tricks such as clickbait. To understand the total impact on the content landscape, we study a game between content creators competing on the basis of engagement metrics and analyze the equilibrium decisions about investment in quality and gaming. First, we show the content created at equilibrium exhibits a positive correlation between quality and gaming, and we empirically validate this finding on a Twitter dataset. Using the equilibrium structure of the content landscape, we then examine the downstream performance of engagement-based optimization along several axes. Perhaps counterintuitively, the average quality of content consumed by users can decrease at equilibrium as gaming tricks become more costly for content creators to employ. Moreover, engagement-based optimization can perform worse in terms of user utility than a baseline with random recommendations, and engagement-based optimization is also suboptimal in terms of realized engagement relative to quality-based optimization. Altogether, our results highlight the need to consider content creator incentives when evaluating a platform's choice of optimization metric.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：A Fast, Performant, Secure Distributed Training Framework For Large  Language Model</b></summary>
  <p><b>编号</b>：[165]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09796</p>
  <p><b>作者</b>：Wei Huang,  Yinggui Wang,  Anda Cheng,  Aihui Zhou,  Chaofan Yu,  Lei Wang</p>
  <p><b>备注</b>：Accept ICASSP2024</p>
  <p><b>关键词</b>：co-training the domain-specific, secure distributed LLM, domain-specific LLM, distributed LLM based, Trusted Execution Environment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The distributed (federated) LLM is an important method for co-training the domain-specific LLM using siloed data. However, maliciously stealing model parameters and data from the server or client side has become an urgent problem to be solved. In this paper, we propose a secure distributed LLM based on model slicing. In this case, we deploy the Trusted Execution Environment (TEE) on both the client and server side, and put the fine-tuned structure (LoRA or embedding of P-tuning v2) into the TEE. Then, secure communication is executed in the TEE and general environments through lightweight encryption. In order to further reduce the equipment cost as well as increase the model performance and accuracy, we propose a split fine-tuning scheme. In particular, we split the LLM by layers and place the latter layers in a server-side TEE (the client does not need a TEE). We then combine the proposed Sparsification Parameter Fine-tuning (SPF) with the LoRA part to improve the accuracy of the downstream task. Numerous experiments have shown that our method guarantees accuracy while maintaining security.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：PatchAD: Patch-based MLP-Mixer for Time Series Anomaly Detection</b></summary>
  <p><b>编号</b>：[168]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09793</p>
  <p><b>作者</b>：Zhijie Zhong,  Zhiwen Yu,  Yiyuan Yang,  Weizheng Wang,  Kaixiang Yang</p>
  <p><b>备注</b>：13 pages, 16 figures, IJCAI 2024 under review, paper id 3166</p>
  <p><b>关键词</b>：identify abnormal events, aiming to identify, crucial aspect, time series analysis, time series samples</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Anomaly detection stands as a crucial aspect of time series analysis, aiming to identify abnormal events in time series samples. The central challenge of this task lies in effectively learning the representations of normal and abnormal patterns in a label-lacking scenario. Previous research mostly relied on reconstruction-based approaches, restricting the representational abilities of the models. In addition, most of the current deep learning-based methods are not lightweight enough, which prompts us to design a more efficient framework for anomaly detection. In this study, we introduce PatchAD, a novel multi-scale patch-based MLP-Mixer architecture that leverages contrastive learning for representational extraction and anomaly detection. Specifically, PatchAD is composed of four distinct MLP Mixers, exclusively utilizing the MLP architecture for high efficiency and lightweight architecture. Additionally, we also innovatively crafted a dual project constraint module to mitigate potential model degradation. Comprehensive experiments demonstrate that PatchAD achieves state-of-the-art results across multiple real-world multivariate time series datasets. Our code is publicly available.\footnote{\url{this https URL}}</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Querying Easily Flip-flopped Samples for Deep Active Learning</b></summary>
  <p><b>编号</b>：[170]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09787</p>
  <p><b>作者</b>：Seong Jin Cho,  Gwangsu Kim,  Junghyun Lee,  Jinwoo Shin,  Chang D. Yoo</p>
  <p><b>备注</b>：34 pages, 17 figures, 5 tables. Accepted to the 12th International Conference on Learning Representations (ICLR 2024)</p>
  <p><b>关键词</b>：machine learning paradigm, paradigm that aims, aims to improve, strategically selecting, model predictive uncertainty</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Active learning is a machine learning paradigm that aims to improve the performance of a model by strategically selecting and querying unlabeled data. One effective selection strategy is to base it on the model's predictive uncertainty, which can be interpreted as a measure of how informative a sample is. The sample's distance to the decision boundary is a natural measure of predictive uncertainty, but it is often intractable to compute, especially for complex decision boundaries formed in multiclass classification tasks. To address this issue, this paper proposes the {\it least disagree metric} (LDM), defined as the smallest probability of disagreement of the predicted label, and an estimator for LDM proven to be asymptotically consistent under mild assumptions. The estimator is computationally efficient and can be easily implemented for deep learning models using parameter perturbation. The LDM-based active learning is performed by querying unlabeled data with the smallest LDM. Experimental results show that our LDM-based active learning algorithm obtains state-of-the-art overall performance on all considered datasets and deep architectures.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Towards Learning from Graphs with Heterophily: Progress and Future</b></summary>
  <p><b>编号</b>：[179]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09769</p>
  <p><b>作者</b>：Chenghua Gong,  Yao Cheng,  Xiang Li,  Caihua Shan,  Siqiang Luo,  Chuan Shi</p>
  <p><b>备注</b>：9 pages</p>
  <p><b>关键词</b>：models complex relations, real-world entities, structured data, complex relations, relations between real-world</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Graphs are structured data that models complex relations between real-world entities. Heterophilous graphs, where linked nodes are prone to be with different labels or dissimilar features, have recently attracted significant attention and found many applications. Meanwhile, increasing efforts have been made to advance learning from heterophilous graphs. Although there exist surveys on the relevant topic, they focus on heterophilous GNNs, which are only sub-topics of heterophilous graph learning. In this survey, we comprehensively overview existing works on learning from graphs with heterophily.First, we collect over 180 publications and introduce the development of this field. Then, we systematically categorize existing methods based on a hierarchical taxonomy including learning strategies, model architectures and practical applications. Finally, we discuss the primary challenges of existing studies and highlight promising avenues for future research.More publication details and corresponding open-source codes can be accessed and will be continuously updated at our repositories:this https URL.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Explaining Drift using Shapley Values</b></summary>
  <p><b>编号</b>：[187]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09756</p>
  <p><b>作者</b>：Narayanan U. Edakunni,  Utkarsh Tekriwal,  Anukriti Jain</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：predict the outcomes, Machine learning models, Machine learning, machine learning research, drift</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine learning models often deteriorate in their performance when they are used to predict the outcomes over data on which they were not trained. These scenarios can often arise in real world when the distribution of data changes gradually or abruptly due to major events like a pandemic. There have been many attempts in machine learning research to come up with techniques that are resilient to such Concept drifts. However, there is no principled framework to identify the drivers behind the drift in model performance. In this paper, we propose a novel framework - DBShap that uses Shapley values to identify the main contributors of the drift and quantify their respective contributions. The proposed framework not only quantifies the importance of individual features in driving the drift but also includes the change in the underlying relation between the input and output as a possible driver. The explanation provided by DBShap can be used to understand the root cause behind the drift and use it to make the model resilient to the drift.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：Universally Robust Graph Neural Networks by Preserving Neighbor  Similarity</b></summary>
  <p><b>编号</b>：[188]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09754</p>
  <p><b>作者</b>：Yulin Zhu,  Yuni Lai,  Xing Ai,  Kai Zhou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：graph neural networks, learning relational data, neural networks, graph neural, heterophilic graphs</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite the tremendous success of graph neural networks in learning relational data, it has been widely investigated that graph neural networks are vulnerable to structural attacks on homophilic graphs. Motivated by this, a surge of robust models is crafted to enhance the adversarial robustness of graph neural networks on homophilic graphs. However, the vulnerability based on heterophilic graphs remains a mystery to us. To bridge this gap, in this paper, we start to explore the vulnerability of graph neural networks on heterophilic graphs and theoretically prove that the update of the negative classification loss is negatively correlated with the pairwise similarities based on the powered aggregated neighbor features. This theoretical proof explains the empirical observations that the graph attacker tends to connect dissimilar node pairs based on the similarities of neighbor features instead of ego features both on homophilic and heterophilic graphs. In this way, we novelly introduce a novel robust model termed NSPGNN which incorporates a dual-kNN graphs pipeline to supervise the neighbor similarity-guided propagation. This propagation utilizes the low-pass filter to smooth the features of node pairs along the positive kNN graphs and the high-pass filter to discriminate the features of node pairs along the negative kNN graphs. Extensive experiments on both homophilic and heterophilic graphs validate the universal robustness of NSPGNN compared to the state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Applications of Machine Learning to Optimizing Polyolefin Manufacturing</b></summary>
  <p><b>编号</b>：[189]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09753</p>
  <p><b>作者</b>：Niket Sharma,  Y.A. Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：polyolefin manufacturing optimization, leveraging machine learning, focusing on leveraging, manufacturing optimization, leveraging machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This chapter is a preprint from our book by , focusing on leveraging machine learning (ML) in chemical and polyolefin manufacturing optimization. It's crafted for both novices and seasoned professionals keen on the latest ML applications in chemical processes. We trace the evolution of AI and ML in chemical industries, delineate core ML components, and provide resources for ML beginners. A detailed discussion on various ML methods is presented, covering regression, classification, and unsupervised learning techniques, with performance metrics and examples. Ensemble methods, deep learning networks, including MLP, DNNs, RNNs, CNNs, and transformers, are explored for their growing role in chemical applications. Practical workshops guide readers through predictive modeling using advanced ML algorithms. The chapter culminates with insights into science-guided ML, advocating for a hybrid approach that enhances model accuracy. The extensive bibliography offers resources for further research and practical implementation. This chapter aims to be a thorough primer on ML's practical application in chemical engineering, particularly for polyolefin production, and sets the stage for continued learning in subsequent chapters. Please cite the original work [169,170] when referencing.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Improving Speaker-independent Speech Emotion Recognition Using Dynamic  Joint Distribution Adaptation</b></summary>
  <p><b>编号</b>：[190]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09752</p>
  <p><b>作者</b>：Cheng Lu,  Yuan Zong,  Hailun Lian,  Yan Zhao,  Björn Schuller,  Wenming Zheng</p>
  <p><b>备注</b>：Accepted by ICASSP 2024</p>
  <p><b>关键词</b>：Joint Distribution Adaptation, Distribution Adaptation, speech emotion recognition, training and testing, testing samples</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In speaker-independent speech emotion recognition, the training and testing samples are collected from diverse speakers, leading to a multi-domain shift challenge across the feature distributions of data from different speakers. Consequently, when the trained model is confronted with data from new speakers, its performance tends to degrade. To address the issue, we propose a Dynamic Joint Distribution Adaptation (DJDA) method under the framework of multi-source domain adaptation. DJDA firstly utilizes joint distribution adaptation (JDA), involving marginal distribution adaptation (MDA) and conditional distribution adaptation (CDA), to more precisely measure the multi-domain distribution shifts caused by different speakers. This helps eliminate speaker bias in emotion features, allowing for learning discriminative and speaker-invariant speech emotion features from coarse-level to fine-level. Furthermore, we quantify the adaptation contributions of MDA and CDA within JDA by using a dynamic balance factor based on $\mathcal{A}$-Distance, promoting to effectively handle the unknown distributions encountered in data from new speakers. Experimental results demonstrate the superior performance of our DJDA as compared to other state-of-the-art (SOTA) methods.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Exploration and Anti-Exploration with Distributional Random Network  Distillation</b></summary>
  <p><b>编号</b>：[191]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09750</p>
  <p><b>作者</b>：Kai Yang,  Jian Tao,  Jiafei Lyu,  Xiu Li</p>
  <p><b>备注</b>：Submitted to ICML 2024</p>
  <p><b>关键词</b>：deep reinforcement learning, attain high returns, Random Network Distillation, remains a critical, deep reinforcement</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Exploration remains a critical issue in deep reinforcement learning for an agent to attain high returns in unknown environments. Although the prevailing exploration Random Network Distillation (RND) algorithm has been demonstrated to be effective in numerous environments, it often needs more discriminative power in bonus allocation. This paper highlights the ``bonus inconsistency'' issue within RND, pinpointing its primary limitation. To address this issue, we introduce the Distributional RND (DRND), a derivative of the RND. DRND enhances the exploration process by distilling a distribution of random networks and implicitly incorporating pseudo counts to improve the precision of bonus allocation. This refinement encourages agents to engage in more extensive exploration. Our method effectively mitigates the inconsistency issue without introducing significant computational overhead. Both theoretical analysis and experimental results demonstrate the superiority of our approach over the original RND algorithm. Our method excels in challenging online exploration scenarios and effectively serves as an anti-exploration mechanism in D4RL offline tasks.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Bootstrapping OTS-Funcimg Pre-training Model (Botfip) -- A Comprehensive  Symbolic Regression Framework</b></summary>
  <p><b>编号</b>：[192]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09748</p>
  <p><b>作者</b>：Tianhao Chen,  Pengbo Xu,  Haibiao Zheng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：problem-solving approaches tend, deep multimodal information, multimodal information mining, Operation Tree Sequence, multimodal framework akin</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the field of scientific computing, many problem-solving approaches tend to focus only on the process and final outcome, even in AI for science, there is a lack of deep multimodal information mining behind the data, missing a multimodal framework akin to that in the image-text domain. In this paper, we take Symbolic Regression(SR) as our focal point and, drawing inspiration from the BLIP model in the image-text domain, propose a scientific computing multimodal framework based on Function Images (Funcimg) and Operation Tree Sequence (OTS), named Bootstrapping OTS-Funcimg Pre-training Model (Botfip). In SR experiments, we validate the advantages of Botfip in low-complexity SR problems, showcasing its potential. As a MED framework, Botfip holds promise for future applications in a broader range of scientific computing problems.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Offline Imitation Learning by Controlling the Effective Planning Horizon</b></summary>
  <p><b>编号</b>：[199]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09728</p>
  <p><b>作者</b>：Hee-Jun Ahn,  Seong-Woong Shim,  Byung-Jun Lee</p>
  <p><b>备注</b>：Preprint</p>
  <p><b>关键词</b>：supplementary offline dataset, effective planning horizon, expert policy, offline dataset, expert trajectories</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In offline imitation learning (IL), we generally assume only a handful of expert trajectories and a supplementary offline dataset from suboptimal behaviors to learn the expert policy. While it is now common to minimize the divergence between state-action visitation distributions so that the agent also considers the future consequences of an action, a sampling error in an offline dataset may lead to erroneous estimates of state-action visitations in the offline case. In this paper, we investigate the effect of controlling the effective planning horizon (i.e., reducing the discount factor) as opposed to imposing an explicit regularizer, as previously studied. Unfortunately, it turns out that the existing algorithms suffer from magnified approximation errors when the effective planning horizon is shortened, which results in a significant degradation in performance. We analyze the main cause of the problem and provide the right remedies to correct the algorithm. We show that the corrected algorithm improves on popular imitation learning benchmarks by controlling the effective planning horizon rather than an explicit regularization.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：EfficientRec an unlimited user-item scale recommendation system based on  clustering and users interaction embedding profile</b></summary>
  <p><b>编号</b>：[217]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09693</p>
  <p><b>作者</b>：Vu Hong Quan,  Le Hoang Ngan,  Le Minh Duc,  Nguyen Tran Ngoc Linh,  Hoang Quynh-Le</p>
  <p><b>备注</b>：Published in 14th Asian Conference on Intelligent Information and Database Systems (ACIIDS), 2022</p>
  <p><b>关键词</b>：technology companies nowadays, companies nowadays, systems are highly, highly interested, interested in technology</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recommendation systems are highly interested in technology companies nowadays. The businesses are constantly growing users and products, causing the number of users and items to continuously increase over time, to very large numbers. Traditional recommendation algorithms with complexity dependent on the number of users and items make them difficult to adapt to the industrial environment. In this paper, we introduce a new method applying graph neural networks with a contrastive learning framework in extracting user preferences. We incorporate a soft clustering architecture that significantly reduces the computational cost of the inference process. Experiments show that the model is able to learn user preferences with low computational cost in both training and prediction phases. At the same time, the model gives a very good accuracy. We call this architecture EfficientRec with the implication of model compactness and the ability to scale to unlimited users and products.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Imitation Learning Inputting Image Feature to Each Layer of Neural  Network</b></summary>
  <p><b>编号</b>：[218]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09691</p>
  <p><b>作者</b>：Koki Yamane,  Sho Sakaino,  Toshiaki Tsuji</p>
  <p><b>备注</b>：IEEE The 18th International Workshop on Advanced Motion Control (AMC2024)</p>
  <p><b>关键词</b>：replicate human behavior, Imitation learning enables, learning enables robots, robots to learn, learn and replicate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Imitation learning enables robots to learn and replicate human behavior from training data. Recent advances in machine learning enable end-to-end learning approaches that directly process high-dimensional observation data, such as images. However, these approaches face a critical challenge when processing data from multiple modalities, inadvertently ignoring data with a lower correlation to the desired output, especially when using short sampling periods. This paper presents a useful method to address this challenge, which amplifies the influence of data with a relatively low correlation to the output by inputting the data into each neural network layer. The proposed approach effectively incorporates diverse data sources into the learning process. Through experiments using a simple pick-and-place operation with raw images and joint information as input, significant improvements in success rates are demonstrated even when dealing with data from short sampling periods.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Comparative Study on the Performance of Categorical Variable Encoders in  Classification and Regression Tasks</b></summary>
  <p><b>编号</b>：[219]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09682</p>
  <p><b>作者</b>：Wenbin Zhu,  Runwen Qiu,  Ying Fu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Categorical variables, classification and regression, encoded into numerical, ATI models, regression tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Categorical variables often appear in datasets for classification and regression tasks, and they need to be encoded into numerical values before training. Since many encoders have been developed and can significantly impact performance, choosing the appropriate encoder for a task becomes a time-consuming yet important practical issue. This study broadly classifies machine learning models into three categories: 1) ATI models that implicitly perform affine transformations on inputs, such as multi-layer perceptron neural network; 2) Tree-based models that are based on decision trees, such as random forest; and 3) the rest, such as kNN. Theoretically, we prove that the one-hot encoder is the best choice for ATI models in the sense that it can mimic any other encoders by learning suitable weights from the data. We also explain why the target encoder and its variants are the most suitable encoders for tree-based models. This study conducted comprehensive computational experiments to evaluate 14 encoders, including one-hot and target encoders, along with eight common machine-learning models on 28 datasets. The computational results agree with our theoretical analysis. The findings in this study shed light on how to select the suitable encoder for data scientists in fields such as fraud detection, disease diagnosis, etc.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Harnessing Density Ratios for Online Reinforcement Learning</b></summary>
  <p><b>编号</b>：[220]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09681</p>
  <p><b>作者</b>：Philip Amortila,  Dylan J. Foster,  Nan Jiang,  Ayush Sekhari,  Tengyang Xie</p>
  <p><b>备注</b>：ICLR 2024</p>
  <p><b>关键词</b>：online reinforcement learning, reinforcement learning, evolved in parallel, analysis techniques, online</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The theories of offline and online reinforcement learning, despite having evolved in parallel, have begun to show signs of the possibility for a unification, with algorithms and analysis techniques for one setting often having natural counterparts in the other. However, the notion of density ratio modeling, an emerging paradigm in offline RL, has been largely absent from online RL, perhaps for good reason: the very existence and boundedness of density ratios relies on access to an exploratory dataset with good coverage, but the core challenge in online RL is to collect such a dataset without having one to start. In this work we show -- perhaps surprisingly -- that density ratio-based algorithms have online counterparts. Assuming only the existence of an exploratory distribution with good coverage, a structural condition known as coverability (Xie et al., 2023), we give a new algorithm (GLOW) that uses density ratio realizability and value function realizability to perform sample-efficient online exploration. GLOW addresses unbounded density ratios via careful use of truncation, and combines this with optimism to guide exploration. GLOW is computationally inefficient; we complement it with a more efficient counterpart, HyGLOW, for the Hybrid RL setting (Song et al., 2022) wherein online RL is augmented with additional offline data. HyGLOW is derived as a special case of a more general meta-algorithm that provides a provable black-box reduction from hybrid RL to offline RL, which may be of independent interest.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Artwork Protection Against Neural Style Transfer Using Locally Adaptive  Adversarial Color Attack</b></summary>
  <p><b>编号</b>：[225]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09673</p>
  <p><b>作者</b>：Zhongliang Guo,  Kaixuan Wang,  Weiye Li,  Yifei Qian,  Ognjen Arandjelović,  Lei Fang</p>
  <p><b>备注</b>：9 pages, 5 figures</p>
  <p><b>关键词</b>：Neural style transfer, widely adopted, adopted in computer, computer vision, vision to generate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural style transfer (NST) is widely adopted in computer vision to generate new images with arbitrary styles. This process leverages neural networks to merge aesthetic elements of a style image with the structural aspects of a content image into a harmoniously integrated visual result. However, unauthorized NST can exploit artwork. Such misuse raises socio-technical concerns regarding artists' rights and motivates the development of technical approaches for the proactive protection of original creations. Adversarial attack is a concept primarily explored in machine learning security. Our work introduces this technique to protect artists' intellectual property. In this paper Locally Adaptive Adversarial Color Attack (LAACA), a method for altering images in a manner imperceptible to the human eyes but disruptive to NST. Specifically, we design perturbations targeting image areas rich in high-frequency content, generated by disrupting intermediate features. Our experiments and user study confirm that by attacking NST using the proposed method results in visually worse neural style transfer, thus making it an effective solution for visual artwork protection.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Towards Identifiable Unsupervised Domain Translation: A Diversified  Distribution Matching Approach</b></summary>
  <p><b>编号</b>：[226]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09671</p>
  <p><b>作者</b>：Sagar Shrestha,  Xiao Fu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：high-level semantic meaning, Unsupervised domain translation, aims to find, semantic meaning, convert samples</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Unsupervised domain translation (UDT) aims to find functions that convert samples from one domain (e.g., sketches) to another domain (e.g., photos) without changing the high-level semantic meaning (also referred to as ``content''). The translation functions are often sought by probability distribution matching of the transformed source domain and target domain. CycleGAN stands as arguably the most representative approach among this line of work. However, it was noticed in the literature that CycleGAN and variants could fail to identify the desired translation functions and produce content-misaligned translations. This limitation arises due to the presence of multiple translation functions -- referred to as ``measure-preserving automorphism" (MPA) -- in the solution space of the learning criteria. Despite awareness of such identifiability issues, solutions have remained elusive. This study delves into the core identifiability inquiry and introduces an MPA elimination theory. Our analysis shows that MPA is unlikely to exist, if multiple pairs of diverse cross-domain conditional distributions are matched by the learning function. Our theory leads to a UDT learner using distribution matching over auxiliary variable-induced subsets of the domains -- other than over the entire data domains as in the classical approaches. The proposed framework is the first to rigorously establish translation identifiability under reasonable UDT settings, to our best knowledge. Experiments corroborate with our theoretical claims.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Mobility Accelerates Learning: Convergence Analysis on Hierarchical  Federated Learning in Vehicular Networks</b></summary>
  <p><b>编号</b>：[230]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09656</p>
  <p><b>作者</b>：Tan Chen,  Jintao Yan,  Yuxuan Sun,  Sheng Zhou,  Deniz Gündüz,  Zhisheng Niu</p>
  <p><b>备注</b>：Submitted to IEEE for possible publication</p>
  <p><b>关键词</b>：Hierarchical federated learning, cloud edge server, enables distributed training, Hierarchical federated, federated learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Hierarchical federated learning (HFL) enables distributed training of models across multiple devices with the help of several edge servers and a cloud edge server in a privacy-preserving manner. In this paper, we consider HFL with highly mobile devices, mainly targeting at vehicular networks. Through convergence analysis, we show that mobility influences the convergence speed by both fusing the edge data and shuffling the edge models. While mobility is usually considered as a challenge from the perspective of communication, we prove that it increases the convergence speed of HFL with edge-level heterogeneous data, since more diverse data can be incorporated. Furthermore, we demonstrate that a higher speed leads to faster convergence, since it accelerates the fusion of data. Simulation results show that mobility increases the model accuracy of HFL by up to 15.1% when training a convolutional neural network on the CIFAR-10 dataset.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：Convex and Bilevel Optimization for Neuro-Symbolic Inference and  Learning</b></summary>
  <p><b>编号</b>：[232]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09651</p>
  <p><b>作者</b>：Charles Dickens,  Changyu Gao,  Connor Pryor,  Stephen Wright,  Lise Getoor</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：bilevel optimization techniques, general gradient-based framework, symbolic parameter learning, challenge for neuro-symbolic, systems by leveraging</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We address a key challenge for neuro-symbolic (NeSy) systems by leveraging convex and bilevel optimization techniques to develop a general gradient-based framework for end-to-end neural and symbolic parameter learning. The applicability of our framework is demonstrated with NeuPSL, a state-of-the-art NeSy architecture. To achieve this, we propose a smooth primal and dual formulation of NeuPSL inference and show learning gradients are functions of the optimal dual variables. Additionally, we develop a dual block coordinate descent algorithm for the new formulation that naturally exploits warm-starts. This leads to over 100x learning runtime improvements over the current best NeuPSL inference method. Finally, we provide extensive empirical evaluations across $8$ datasets covering a range of tasks and demonstrate our learning framework achieves up to a 16% point prediction performance improvement over alternative learning methods.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on  Climate Change</b></summary>
  <p><b>编号</b>：[234]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09646</p>
  <p><b>作者</b>：David Thulke,  Yingbo Gao,  Petrus Pelser,  Rein Brune,  Rricha Jalota,  Floris Fok,  Michael Ramos,  Ian van Wyk,  Abdallah Nasir,  Hayden Goldstein,  Taylor Tragemann,  Katie Nguyen,  Ariana Fowler,  Andrew Stanco,  Jon Gabriel,  Jordan Taylor,  Dean Moro,  Evgenii Tsymbalov,  Juliette de Waal,  Evgeny Matusov,  Mudar Yaghi,  Mohammad Shihadah,  Hermann Ney,  Christian Dugast,  Jonathan Dotan,  Daniel Erasmus</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：paper introduces ClimateGPT, introduces ClimateGPT, paper introduces, model, domain-specific</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper introduces ClimateGPT, a model family of domain-specific large language models that synthesize interdisciplinary research on climate change. We trained two 7B models from scratch on a science-oriented dataset of 300B tokens. For the first model, the 4.2B domain-specific tokens were included during pre-training and the second was adapted to the climate domain after pre-training. Additionally, ClimateGPT-7B, 13B and 70B are continuously pre-trained from Llama~2 on a domain-specific dataset of 4.2B tokens. Each model is instruction fine-tuned on a high-quality and human-generated domain-specific dataset that has been created in close cooperation with climate scientists. To reduce the number of hallucinations, we optimize the model for retrieval augmentation and propose a hierarchical retrieval strategy. To increase the accessibility of our model to non-English speakers, we propose to make use of cascaded machine translation and show that this approach can perform comparably to natively multilingual models while being easier to scale to a large number of languages. Further, to address the intrinsic interdisciplinary aspect of climate change we consider different research perspectives. Therefore, the model can produce in-depth answers focusing on different perspectives in addition to an overall answer. We propose a suite of automatic climate-specific benchmarks to evaluate LLMs. On these benchmarks, ClimateGPT-7B performs on par with the ten times larger Llama-2-70B Chat model while not degrading results on general domain benchmarks. Our human evaluation confirms the trends we saw in our benchmarks. All models were trained and evaluated using renewable energy and are released publicly.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Functional Linear Non-Gaussian Acyclic Model for Causal Discovery</b></summary>
  <p><b>编号</b>：[235]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09641</p>
  <p><b>作者</b>：Tian-Le Yang,  Kuang-Yao Lee,  Kun Zhang,  Joe Suzuki</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Non-Gaussian Acyclic Model, Linear Non-Gaussian Acyclic, respective connection strengths, Acyclic Model, Functional Linear Non-Gaussian</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In causal discovery, non-Gaussianity has been used to characterize the complete configuration of a Linear Non-Gaussian Acyclic Model (LiNGAM), encompassing both the causal ordering of variables and their respective connection strengths. However, LiNGAM can only deal with the finite-dimensional case. To expand this concept, we extend the notion of variables to encompass vectors and even functions, leading to the Functional Linear Non-Gaussian Acyclic Model (Func-LiNGAM). Our motivation stems from the desire to identify causal relationships in brain-effective connectivity tasks involving, for example, fMRI and EEG datasets. We demonstrate why the original LiNGAM fails to handle these inherently infinite-dimensional datasets and explain the availability of functional data analysis from both empirical and theoretical perspectives. {We establish theoretical guarantees of the identifiability of the causal relationship among non-Gaussian random vectors and even random functions in infinite-dimensional Hilbert spaces.} To address the issue of sparsity in discrete time points within intrinsic infinite-dimensional functional data, we propose optimizing the coordinates of the vectors using functional principal component analysis. Experimental results on synthetic data verify the ability of the proposed framework to identify causal relationships among multivariate functions using the observed samples. For real data, we focus on analyzing the brain connectivity patterns derived from fMRI data.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Physics-Informed Calibration of Aeromagnetic Compensation in Magnetic  Navigation Systems using Liquid Time-Constant Networks</b></summary>
  <p><b>编号</b>：[238]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09631</p>
  <p><b>作者</b>：Favour Nerrise (1 and 2),  Andrew Sosa Sosanya (2),  Patrick Neary (2) ((1) Department of Electrical Engineering, Stanford University, CA, USA, (2) SandboxAQ, Palo Alto, CA, USA)</p>
  <p><b>备注</b>：Accepted at the NeurIPS 2023 Machine Learning and the Physical Sciences workshop, 7 pages, 4 figures, see code here: this https URL</p>
  <p><b>关键词</b>：Global Positioning System, Global Positioning, Positioning System, aircraft navigation systems, rising alternative</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Magnetic navigation (MagNav) is a rising alternative to the Global Positioning System (GPS) and has proven useful for aircraft navigation. Traditional aircraft navigation systems, while effective, face limitations in precision and reliability in certain environments and against attacks. Airborne MagNav leverages the Earth's magnetic field to provide accurate positional information. However, external magnetic fields induced by aircraft electronics and Earth's large-scale magnetic fields disrupt the weaker signal of interest. We introduce a physics-informed approach using Tolles-Lawson coefficients for compensation and Liquid Time-Constant Networks (LTCs) to remove complex, noisy signals derived from the aircraft's magnetic sources. Using real flight data with magnetometer measurements and aircraft measurements, we observe up to a 64% reduction in aeromagnetic compensation error (RMSE nT), outperforming conventional models. This significant improvement underscores the potential of a physics-informed, machine learning approach for extracting clean, reliable, and accurate magnetic signals for MagNav positional estimation.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Multiple Locally Linear Kernel Machines</b></summary>
  <p><b>编号</b>：[239]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09629</p>
  <p><b>作者</b>：David Picard</p>
  <p><b>备注</b>：This paper was written in 2014 and was originally submitted but rejected at ICML'15</p>
  <p><b>关键词</b>：Multiple Kernel Learning, paper we propose, locally linear, locally linear kernels, non-linear classifier based</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper we propose a new non-linear classifier based on a combination of locally linear classifiers. A well known optimization formulation is given as we cast the problem in a $\ell_1$ Multiple Kernel Learning (MKL) problem using many locally linear kernels. Since the number of such kernels is huge, we provide a scalable generic MKL training algorithm handling streaming kernels. With respect to the inference time, the resulting classifier fits the gap between high accuracy but slow non-linear classifiers (such as classical MKL) and fast but low accuracy linear classifiers.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：SMOOTHIE: A Theory of Hyper-parameter Optimization for Software  Analytics</b></summary>
  <p><b>编号</b>：[241]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09622</p>
  <p><b>作者</b>：Rahul Yedida,  Tim Menzies</p>
  <p><b>备注</b>：v1</p>
  <p><b>关键词</b>：learner control parameters, software analytics, Hyper-parameter optimization, SMOOTHIE, control parameters</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Hyper-parameter optimization is the black art of tuning a learner's control parameters. In software analytics, a repeated result is that such tuning can result in dramatic performance improvements. Despite this, hyper-parameter optimization is often applied rarely or poorly in software analytics--perhaps due to the CPU cost of exploring all those parameter options can be prohibitive.
We theorize that learners generalize better when the loss landscape is ``smooth''. This theory is useful since the influence on ``smoothness'' of different hyper-parameter choices can be tested very quickly (e.g. for a deep learner, after just one epoch).
To test this theory, this paper implements and tests SMOOTHIE, a novel hyper-parameter optimizer that guides its optimizations via considerations of ``smothness''. The experiments of this paper test SMOOTHIE on numerous SE tasks including (a) GitHub issue lifetime prediction; (b) detecting false alarms in static code warnings; (c) defect prediction, and (d) a set of standard ML datasets. In all these experiments, SMOOTHIE out-performed state-of-the-art optimizers. Better yet, SMOOTHIE ran 300% faster than the prior state-of-the art. We hence conclude that this theory (that hyper-parameter optimization is best viewed as a ``smoothing'' function for the decision landscape), is both theoretically interesting and practically very useful.
To support open science and other researchers working in this area, all our scripts and datasets are available on-line at this https URL.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：Land Cover Image Classification</b></summary>
  <p><b>编号</b>：[246]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09607</p>
  <p><b>作者</b>：Antonio Rangel,  Juan Terven,  Diana M. Cordova-Esparza,  E.A. Chavez-Urbiola</p>
  <p><b>备注</b>：7 pages, 4 figures, 1 table, published in conference</p>
  <p><b>关键词</b>：Land Cover, urban planning, disaster management, increasingly significant, significant in understanding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Land Cover (LC) image classification has become increasingly significant in understanding environmental changes, urban planning, and disaster management. However, traditional LC methods are often labor-intensive and prone to human error. This paper explores state-of-the-art deep learning models for enhanced accuracy and efficiency in LC analysis. We compare convolutional neural networks (CNN) against transformer-based methods, showcasing their applications and advantages in LC studies. We used EuroSAT, a patch-based LC classification data set based on Sentinel-2 satellite images and achieved state-of-the-art results using current transformer models.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Robustness Evaluation of Machine Learning Models for Robot Arm Action  Recognition in Noisy Environments</b></summary>
  <p><b>编号</b>：[247]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09606</p>
  <p><b>作者</b>：Elaheh Motamedi,  Kian Behzad,  Rojin Zandi,  Hojjat Salehinejad,  Milad Siami</p>
  <p><b>备注</b>：Accepted at ICASSP</p>
  <p><b>关键词</b>：spatially proximate arm, noisy environments poses, proximate arm movements, noisy environments, identifying distinct</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the realm of robot action recognition, identifying distinct but spatially proximate arm movements using vision systems in noisy environments poses a significant challenge. This paper studies robot arm action recognition in noisy environments using machine learning techniques. Specifically, a vision system is used to track the robot's movements followed by a deep learning model to extract the arm's key points. Through a comparative analysis of machine learning methods, the effectiveness and robustness of this model are assessed in noisy environments. A case study was conducted using the Tic-Tac-Toe game in a 3-by-3 grid environment, where the focus is to accurately identify the actions of the arms in selecting specific locations within this constrained environment. Experimental results show that our approach can achieve precise key point detection and action classification despite the addition of noise and uncertainties to the dataset.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：MedBlindTuner: Towards Privacy-preserving Fine-tuning on Biomedical  Images with Transformers and Fully Homomorphic Encryption</b></summary>
  <p><b>编号</b>：[249]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09604</p>
  <p><b>作者</b>：Prajwal Panzade,  Daniel Takabi,  Zhipeng Cai</p>
  <p><b>备注</b>：Accepted for the presentation at W3PHIAI, The 38th Annual AAAI Conference on Artificial Intelligence 2024</p>
  <p><b>关键词</b>：significantly revolutionized medical, Advancements in machine, machine learning, prompting hospitals, external ML services</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Advancements in machine learning (ML) have significantly revolutionized medical image analysis, prompting hospitals to rely on external ML services. However, the exchange of sensitive patient data, such as chest X-rays, poses inherent privacy risks when shared with third parties. Addressing this concern, we propose MedBlindTuner, a privacy-preserving framework leveraging fully homomorphic encryption (FHE) and a data-efficient image transformer (DEiT). MedBlindTuner enables the training of ML models exclusively on FHE-encrypted medical images. Our experimental evaluation demonstrates that MedBlindTuner achieves comparable accuracy to models trained on non-encrypted images, offering a secure solution for outsourcing ML computations while preserving patient data privacy. To the best of our knowledge, this is the first work that uses data-efficient image transformers and fully homomorphic encryption in this domain.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：Efficient generative adversarial networks using linear  additive-attention Transformers</b></summary>
  <p><b>编号</b>：[252]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09596</p>
  <p><b>作者</b>：Emilio Morales-Juarez,  Gibran Fuentes-Pineda</p>
  <p><b>备注</b>：12 pages, 6 figures</p>
  <p><b>关键词</b>：computationally expensive architectures, Generative Adversarial Networks, Diffusion Models, image generation, recent years</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Although the capacity of deep generative models for image generation, such as Diffusion Models (DMs) and Generative Adversarial Networks (GANs), has dramatically improved in recent years, much of their success can be attributed to computationally expensive architectures. This has limited their adoption and use to research laboratories and companies with large resources, while significantly raising the carbon footprint for training, fine-tuning, and inference. In this work, we present LadaGAN, an efficient generative adversarial network that is built upon a novel Transformer block named Ladaformer. The main component of this block is a linear additive-attention mechanism that computes a single attention vector per head instead of the quadratic dot-product attention. We employ Ladaformer in both the generator and discriminator, which reduces the computational complexity and overcomes the training instabilities often associated with Transformer GANs. LadaGAN consistently outperforms existing convolutional and Transformer GANs on benchmark datasets at different resolutions while being significantly more efficient. Moreover, LadaGAN shows competitive performance compared to state-of-the-art multi-step generative models (e.g. DMs) using orders of magnitude less computational resources.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：Bilevel Optimization under Unbounded Smoothness: A New Algorithm and  Convergence Analysis</b></summary>
  <p><b>编号</b>：[254]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09587</p>
  <p><b>作者</b>：Jie Hao,  Xiaochuan Gong,  Mingrui Liu</p>
  <p><b>备注</b>：Accepted by ICLR 2024, Spotlight</p>
  <p><b>关键词</b>：bilevel optimization algorithms, Bilevel optimization, Current bilevel optimization, lower-level variable, important formulation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Bilevel optimization is an important formulation for many machine learning problems. Current bilevel optimization algorithms assume that the gradient of the upper-level function is Lipschitz. However, recent studies reveal that certain neural networks such as recurrent neural networks (RNNs) and long-short-term memory networks (LSTMs) exhibit potential unbounded smoothness, rendering conventional bilevel optimization algorithms unsuitable. In this paper, we design a new bilevel optimization algorithm, namely BO-REP, to address this challenge. This algorithm updates the upper-level variable using normalized momentum and incorporates two novel techniques for updating the lower-level variable: \textit{initialization refinement} and \textit{periodic updates}. Specifically, once the upper-level variable is initialized, a subroutine is invoked to obtain a refined estimate of the corresponding optimal lower-level variable, and the lower-level variable is updated only after every specific period instead of each iteration. When the upper-level problem is nonconvex and unbounded smooth, and the lower-level problem is strongly convex, we prove that our algorithm requires $\widetilde{\mathcal{O}}(1/\epsilon^4)$ iterations to find an $\epsilon$-stationary point in the stochastic setting, where each iteration involves calling a stochastic gradient or Hessian-vector product oracle. Notably, this result matches the state-of-the-art complexity results under the bounded smoothness setting and without mean-squared smoothness of the stochastic gradient, up to logarithmic factors. Our proof relies on novel technical lemmas for the periodically updated lower-level variable, which are of independent interest. Our experiments on hyper-representation learning, hyperparameter optimization, and data hyper-cleaning for text classification tasks demonstrate the effectiveness of our proposed algorithm.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：eipy: An Open-Source Python Package for Multi-modal Data Integration  using Heterogeneous Ensembles</b></summary>
  <p><b>编号</b>：[256]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09582</p>
  <p><b>作者</b>：Jamie J. R. Bennett,  Yan Chak Li,  Gaurav Pandey</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：open-source Python package, open-source Python, multi-modal heterogeneous ensembles, developing effective, ensembles for classification</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we introduce eipy--an open-source Python package for developing effective, multi-modal heterogeneous ensembles for classification. eipy simultaneously provides both a rigorous, and user-friendly framework for comparing and selecting the best-performing multi-modal data integration and predictive modeling methods by systematically evaluating their performance using nested cross-validation. The package is designed to leverage scikit-learn-like estimators as components to build multi-modal predictive models. An up-to-date user guide, including API reference and tutorials, for eipy is maintained at this https URL . The main repository for this project can be found on GitHub at this https URL .</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：Towards Scalable and Robust Model Versioning</b></summary>
  <p><b>编号</b>：[258]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09574</p>
  <p><b>作者</b>：Wenxin Ding,  Arjun Nitin Bhagoji,  Ben Y. Zhao,  Haitao Zheng</p>
  <p><b>备注</b>：Accepted in IEEE SaTML 2024</p>
  <p><b>关键词</b>：malicious incursions aimed, learning models continues, deep learning models, model, expand across industries</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As the deployment of deep learning models continues to expand across industries, the threat of malicious incursions aimed at gaining access to these deployed models is on the rise. Should an attacker gain access to a deployed model, whether through server breaches, insider attacks, or model inversion techniques, they can then construct white-box adversarial attacks to manipulate the model's classification outcomes, thereby posing significant risks to organizations that rely on these models for critical tasks. Model owners need mechanisms to protect themselves against such losses without the necessity of acquiring fresh training data - a process that typically demands substantial investments in time and capital.
In this paper, we explore the feasibility of generating multiple versions of a model that possess different attack properties, without acquiring new training data or changing model architecture. The model owner can deploy one version at a time and replace a leaked version immediately with a new version. The newly deployed model version can resist adversarial attacks generated leveraging white-box access to one or all previously leaked versions. We show theoretically that this can be accomplished by incorporating parameterized hidden distributions into the model training data, forcing the model to learn task-irrelevant features uniquely defined by the chosen data. Additionally, optimal choices of hidden distributions can produce a sequence of model versions capable of resisting compound transferability attacks over time. Leveraging our analytical insights, we design and implement a practical model versioning method for DNN classifiers, which leads to significant robustness improvements over existing methods. We believe our work presents a promising direction for safeguarding DNN services beyond their initial deployment.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：Sharing Knowledge in Multi-Task Deep Reinforcement Learning</b></summary>
  <p><b>编号</b>：[261]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09561</p>
  <p><b>作者</b>：Carlo D'Eramo,  Davide Tateo,  Andrea Bonarini,  Marcello Restelli,  Jan Peters</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：deep neural networks, Reinforcement Learning, Reinforcement Learning algorithms, Multi-Task Reinforcement Learning, Reinforcement Learning benchmarks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the benefit of sharing representations among tasks to enable the effective use of deep neural networks in Multi-Task Reinforcement Learning. We leverage the assumption that learning from different tasks, sharing common properties, is helpful to generalize the knowledge of them resulting in a more effective feature extraction compared to learning a single task. Intuitively, the resulting set of features offers performance benefits when used by Reinforcement Learning algorithms. We prove this by providing theoretical guarantees that highlight the conditions for which is convenient to share representations among tasks, extending the well-known finite-time bounds of Approximate Value-Iteration to the multi-task setting. In addition, we complement our analysis by proposing multi-task extensions of three Reinforcement Learning algorithms that we empirically evaluate on widely used Reinforcement Learning benchmarks showing significant improvements over the single-task counterparts in terms of sample efficiency and performance.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：Improving Classification Performance With Human Feedback: Label a few,  we label the rest</b></summary>
  <p><b>编号</b>：[262]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09555</p>
  <p><b>作者</b>：Natan Vidra,  Thomas Clifford,  Katherine Jijo,  Eden Chung,  Liang Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：obtaining substantial amounts, train supervised machine, supervised machine learning, machine learning models, learning models poses</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the realm of artificial intelligence, where a vast majority of data is unstructured, obtaining substantial amounts of labeled data to train supervised machine learning models poses a significant challenge. To address this, we delve into few-shot and active learning, where are goal is to improve AI models with human feedback on a few labeled examples. This paper focuses on understanding how a continuous feedback loop can refine models, thereby enhancing their accuracy, recall, and precision through incremental human input. By employing Large Language Models (LLMs) such as GPT-3.5, BERT, and SetFit, we aim to analyze the efficacy of using a limited number of labeled examples to substantially improve model accuracy. We benchmark this approach on the Financial Phrasebank, Banking, Craigslist, Trec, Amazon Reviews datasets to prove that with just a few labeled examples, we are able to surpass the accuracy of zero shot large language models to provide enhanced text classification performance. We demonstrate that rather than needing to manually label millions of rows of data, we just need to label a few and the model can effectively predict the rest.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：Dimensional Neuroimaging Endophenotypes: Neurobiological Representations  of Disease Heterogeneity Through Machine Learning</b></summary>
  <p><b>编号</b>：[268]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09517</p>
  <p><b>作者</b>：Junhao Wen,  Mathilde Antoniades,  Zhijian Yang,  Gyujoon Hwang,  Ioanna Skampardoni,  Rongguang Wang,  Christos Davatzikos</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：obtain individualized neuroimaging, individualized neuroimaging signatures, obtain individualized, response to treatment, neuropsychiatric and neurodegenerative</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine learning has been increasingly used to obtain individualized neuroimaging signatures for disease diagnosis, prognosis, and response to treatment in neuropsychiatric and neurodegenerative disorders. Therefore, it has contributed to a better understanding of disease heterogeneity by identifying disease subtypes that present significant differences in various brain phenotypic measures. In this review, we first present a systematic literature overview of studies using machine learning and multimodal MRI to unravel disease heterogeneity in various neuropsychiatric and neurodegenerative disorders, including Alzheimer disease, schizophrenia, major depressive disorder, autism spectrum disorder, multiple sclerosis, as well as their potential in transdiagnostic settings. Subsequently, we summarize relevant machine learning methodologies and discuss an emerging paradigm which we call dimensional neuroimaging endophenotype (DNE). DNE dissects the neurobiological heterogeneity of neuropsychiatric and neurodegenerative disorders into a low dimensional yet informative, quantitative brain phenotypic representation, serving as a robust intermediate phenotype (i.e., endophenotype) largely reflecting underlying genetics and etiology. Finally, we discuss the potential clinical implications of the current findings and envision future research avenues.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：Accelerating Data Generation for Neural Operators via Krylov Subspace  Recycling</b></summary>
  <p><b>编号</b>：[269]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09516</p>
  <p><b>作者</b>：Hong Wang,  Zhongkai Hao,  Jie Wang,  Zijie Geng,  Zhen Wang,  Bin Li,  Feng Wu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：attracted great attention, great attention due, partial differential equations, solving partial differential, partial differential</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Learning neural operators for solving partial differential equations (PDEs) has attracted great attention due to its high inference efficiency. However, training such operators requires generating a substantial amount of labeled data, i.e., PDE problems together with their solutions. The data generation process is exceptionally time-consuming, as it involves solving numerous systems of linear equations to obtain numerical solutions to the PDEs. Many existing methods solve these systems independently without considering their inherent similarities, resulting in extremely redundant computations. To tackle this problem, we propose a novel method, namely Sorting Krylov Recycling (SKR), to boost the efficiency of solving these systems, thus significantly accelerating data generation for neural operators training. To the best of our knowledge, SKR is the first attempt to address the time-consuming nature of data generation for learning neural operators. The working horse of SKR is Krylov subspace recycling, a powerful technique for solving a series of interrelated systems by leveraging their inherent similarities. Specifically, SKR employs a sorting algorithm to arrange these systems in a sequence, where adjacent systems exhibit high similarities. Then it equips a solver with Krylov subspace recycling to solve the systems sequentially instead of independently, thus effectively enhancing the solving efficiency. Both theoretical analysis and extensive experiments demonstrate that SKR can significantly accelerate neural operator data generation, achieving a remarkable speedup of up to 13.9 times.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：Enhancing Surveillance Camera FOV Quality via Semantic Line Detection  and Classification with Deep Hough Transform</b></summary>
  <p><b>编号</b>：[270]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09515</p>
  <p><b>作者</b>：Andrew C. Freeman,  Wenjing Shi,  Bin Hwang</p>
  <p><b>备注</b>：Appeared in the WACV 2024 Workshop on Image/Video/Audio Quality in Computer Vision and Generative AI</p>
  <p><b>关键词</b>：significantly influenced, FOV, video and image, image quality, recorded videos</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The quality of recorded videos and images is significantly influenced by the camera's field of view (FOV). In critical applications like surveillance systems and self-driving cars, an inadequate FOV can give rise to severe safety and security concerns, including car accidents and thefts due to the failure to detect individuals and objects. The conventional methods for establishing the correct FOV heavily rely on human judgment and lack automated mechanisms to assess video and image quality based on FOV. In this paper, we introduce an innovative approach that harnesses semantic line detection and classification alongside deep Hough transform to identify semantic lines, thus ensuring a suitable FOV by understanding 3D view through parallel lines. Our approach yields an effective F1 score of 0.729 on the public EgoCart dataset, coupled with a notably high median score in the line placement metric. We illustrate that our method offers a straightforward means of assessing the quality of the camera's field of view, achieving a classification accuracy of 83.8\%. This metric can serve as a proxy for evaluating the potential performance of video and image quality applications.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：Community Detection in the Multi-View Stochastic Block Model</b></summary>
  <p><b>编号</b>：[272]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09510</p>
  <p><b>作者</b>：Yexin Zhang,  Zhongtian Ma,  Qiaosheng Zhang,  Zhen Wang,  Xuelong Li</p>
  <p><b>备注</b>：Submitted to IEEE for possible publication</p>
  <p><b>关键词</b>：potentially correlated graphs, multiple potentially correlated, information-theoretical perspective, correlated graphs, generate correlated graphs</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper considers the problem of community detection on multiple potentially correlated graphs from an information-theoretical perspective. We first put forth a random graph model, called the multi-view stochastic block model (MVSBM), designed to generate correlated graphs on the same set of nodes (with cardinality $n$). The $n$ nodes are partitioned into two disjoint communities of equal size. The presence or absence of edges in the graphs for each pair of nodes depends on whether the two nodes belong to the same community or not. The objective for the learner is to recover the hidden communities with observed graphs. Our technical contributions are two-fold: (i) We establish an information-theoretic upper bound (Theorem~1) showing that exact recovery of community is achievable when the model parameters of MVSBM exceed a certain threshold. (ii) Conversely, we derive an information-theoretic lower bound (Theorem~2) showing that when the model parameters of MVSBM fall below the aforementioned threshold, then for any estimator, the expected number of misclassified nodes will always be greater than one. Our results for the MVSBM recover several prior results for community detection in the standard SBM as well as in multiple independent SBMs as special cases.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：Exploration of Activation Fault Reliability in Quantized Systolic  Array-Based DNN Accelerators</b></summary>
  <p><b>编号</b>：[273]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09509</p>
  <p><b>作者</b>：Mahdi Taheri,  Natalia Cherezova,  Mohammad Saeed Ansari,  Maksim Jenihhin,  Ali Mahani,  Masoud Daneshtalab,  Jaan Raik</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Deep Neural Networks, Deep Neural, DNN accelerator implementation, specialized DNN accelerators, accelerator reliability stand</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The stringent requirements for the Deep Neural Networks (DNNs) accelerator's reliability stand along with the need for reducing the computational burden on the hardware platforms, i.e. reducing the energy consumption and execution time as well as increasing the efficiency of DNN accelerators. Moreover, the growing demand for specialized DNN accelerators with tailored requirements, particularly for safety-critical applications, necessitates a comprehensive design space exploration to enable the development of efficient and robust accelerators that meet those requirements. Therefore, the trade-off between hardware performance, i.e. area and delay, and the reliability of the DNN accelerator implementation becomes critical and requires tools for analysis. This paper presents a comprehensive methodology for exploring and enabling a holistic assessment of the trilateral impact of quantization on model accuracy, activation fault reliability, and hardware efficiency. A fully automated framework is introduced that is capable of applying various quantization-aware techniques, fault injection, and hardware implementation, thus enabling the measurement of hardware parameters. Moreover, this paper proposes a novel lightweight protection technique integrated within the framework to ensure the dependable deployment of the final systolic-array-based FPGA implementation. The experiments on established benchmarks demonstrate the analysis flow and the profound implications of quantization on reliability, hardware performance, and network accuracy, particularly concerning the transient faults in the network's activations.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：Deep Ensemble Shape Calibration: Multi-Field Post-hoc Calibration in  Online Advertising</b></summary>
  <p><b>编号</b>：[274]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09507</p>
  <p><b>作者</b>：Shuai Yang,  Hao Yang,  Zhuang Zou,  Linhe Xu,  Shuo Yuan,  Yifan Zeng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：CTR and CVR, CVR is critical, calibration, shape calibration, seller and platform</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the e-commerce advertising scenario, estimating the true probabilities (known as a calibrated estimate) on CTR and CVR is critical and can directly affect the benefits of the buyer, seller and platform. Previous research has introduced numerous solutions for addressing the calibration problem. These methods typically involve the training of calibrators using a validation set and subsequently applying these calibrators to correct the original estimated values during online inference. However, what sets e-commerce advertising scenarios is the challenge of multi-field calibration. Multi-field calibration can be subdivided into two distinct sub-problems: value calibration and shape calibration. Value calibration is defined as no over- or under-estimation for each value under concerned fields. Shape calibration is defined as no over- or under-estimation for each subset of the pCTR within the specified range under condition of concerned fields. In order to achieve shape calibration and value calibration, it is necessary to have a strong data utilization ability.Because the quantity of pCTR specified range for single field-value sample is relative small, which makes the calibrator more difficult to train. However the existing methods cannot simultaneously fulfill both value calibration and shape calibration. To solve these problems, we propose a new method named Deep Ensemble Shape Calibration (DESC). We introduce innovative basis calibration functions, which enhance both function expression capabilities and data utilization by combining these basis calibration functions. A significant advancement lies in the development of an allocator capable of allocating the most suitable shape calibrators to different estimation error distributions within diverse fields and values.</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：Functional Autoencoder for Smoothing and Representation Learning</b></summary>
  <p><b>编号</b>：[275]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09499</p>
  <p><b>作者</b>：Sidi Wu,  Cédric Beaulac,  Jiguo Cao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：discretely observed data, functional data, summarizing the information, data, common pipeline</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A common pipeline in functional data analysis is to first convert the discretely observed data to smooth functions, and then represent the functions by a finite-dimensional vector of coefficients summarizing the information. Existing methods for data smoothing and dimensional reduction mainly focus on learning the linear mappings from the data space to the representation space, however, learning only the linear representations may not be sufficient. In this study, we propose to learn the nonlinear representations of functional data using neural network autoencoders designed to process data in the form it is usually collected without the need of preprocessing. We design the encoder to employ a projection layer computing the weighted inner product of the functional data and functional weights over the observed timestamp, and the decoder to apply a recovery layer that maps the finite-dimensional vector extracted from the functional data back to functional space using a set of predetermined basis functions. The developed architecture can accommodate both regularly and irregularly spaced data. Our experiments demonstrate that the proposed method outperforms functional principal component analysis in terms of prediction and classification, and maintains superior smoothing ability and better computational efficiency in comparison to the conventional autoencoders under both linear and nonlinear settings.</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：Technical Report: On the Convergence of Gossip Learning in the Presence  of Node Inaccessibility</b></summary>
  <p><b>编号</b>：[276]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09498</p>
  <p><b>作者</b>：Tian Liu,  Yue Cui,  Xueyang Hu,  Yecheng Xu,  Bo Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：unmanned aerial vehicles, resource-constrained wireless networks, Gossip learning, federated learning, aerial vehicles</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Gossip learning (GL), as a decentralized alternative to federated learning (FL), is more suitable for resource-constrained wireless networks, such as FANETs that are formed by unmanned aerial vehicles (UAVs). GL can significantly enhance the efficiency and extend the battery life of UAV networks. Despite the advantages, the performance of GL is strongly affected by data distribution, communication speed, and network connectivity. However, how these factors influence the GL convergence is still unclear. Existing work studied the convergence of GL based on a virtual quantity for the sake of convenience, which fail to reflect the real state of the network when some nodes are inaccessible. In this paper, we formulate and investigate the impact of inaccessible nodes to GL under a dynamic network topology. We first decompose the weight divergence by whether the node is accessible or not. Then, we investigate the GL convergence under the dynamic of node accessibility and theoretically provide how the number of inaccessible nodes, data non-i.i.d.-ness, and duration of inaccessibility affect the convergence. Extensive experiments are carried out in practical settings to comprehensively verify the correctness of our theoretical findings.</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：VeriBug: An Attention-based Framework for Bug-Localization in Hardware  Designs</b></summary>
  <p><b>编号</b>：[279]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09494</p>
  <p><b>作者</b>：Giuseppe Stracquadanio,  Sourav Medya,  Stefano Quer,  Debjit Pal</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：specialized applications, exponential growth, size and complexity, targeting different specialized, recent years</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, there has been an exponential growth in the size and complexity of System-on-Chip designs targeting different specialized applications. The cost of an undetected bug in these systems is much higher than in traditional processor systems as it may imply the loss of property or life. The problem is further exacerbated by the ever-shrinking time-to-market and ever-increasing demand to churn out billions of devices. Despite decades of research in simulation and formal methods for debugging and verification, it is still one of the most time-consuming and resource intensive processes in contemporary hardware design cycle. In this work, we propose VeriBug, which leverages recent advances in deep learning to accelerate debugging at the Register-Transfer Level and generates explanations of likely root causes. First, VeriBug uses control-data flow graph of a hardware design and learns to execute design statements by analyzing the context of operands and their assignments. Then, it assigns an importance score to each operand in a design statement and uses that score for generating explanations for failures. Finally, VeriBug produces a heatmap highlighting potential buggy source code portions. Our experiments show that VeriBug can achieve an average bug localization coverage of 82.5% on open-source designs and different types of injected bugs.</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：Uncertainty-Aware Calibration of a Hot-Wire Anemometer With Gaussian  Process Regression</b></summary>
  <p><b>编号</b>：[280]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09492</p>
  <p><b>作者</b>：Rubén Antonio García-Ruiz,  José Luis Blanco-Claraco,  Javier López-Martínez,  Ángel Jesús Callejón-Ferre</p>
  <p><b>备注</b>：10 pages, 6 figures, Published in "IEEE Sensors Journal"</p>
  <p><b>关键词</b>：Gaussian Process Regression, Expensive ultrasonic anemometers, Process Regression, Gaussian Process, Expensive ultrasonic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Expensive ultrasonic anemometers are usually required to measure wind speed accurately. The aim of this work is to overcome the loss of accuracy of a low cost hot-wire anemometer caused by the changes of air temperature, by means of a probabilistic calibration using Gaussian Process Regression. Gaussian Process Regression is a non-parametric, Bayesian, and supervised learning method designed to make predictions of an unknown target variable as a function of one or more known input variables. Our approach is validated against real datasets, obtaining a good performance in inferring the actual wind speed values. By performing, before its real use in the field, a calibration of the hot-wire anemometer taking into account air temperature, permits that the wind speed can be estimated for the typical range of ambient temperatures, including a grounded uncertainty estimation for each speed measure.</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：PUPAE: Intuitive and Actionable Explanations for Time Series Anomalies</b></summary>
  <p><b>编号</b>：[282]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09489</p>
  <p><b>作者</b>：Audrey Der,  Chin-Chia Michael Yeh,  Yan Zheng,  Junpeng Wang,  Zhongfang Zhuang,  Liang Wang,  Wei Zhang,  Eamonn J. Keogh</p>
  <p><b>备注</b>：9 Page Manuscript, 1 Page Supplementary (Supplement not published in conference proceedings.)</p>
  <p><b>关键词</b>：recent years, significant progress, series anomaly detection, anomaly, anomaly detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years there has been significant progress in time series anomaly detection. However, after detecting an (perhaps tentative) anomaly, can we explain it? Such explanations would be useful to triage anomalies. For example, in an oil refinery, should we respond to an anomaly by dispatching a hydraulic engineer, or an intern to replace the battery on a sensor? There have been some parallel efforts to explain anomalies, however many proposed techniques produce explanations that are indirect, and often seem more complex than the anomaly they seek to explain. Our review of the literature/checklists/user-manuals used by frontline practitioners in various domains reveals an interesting near-universal commonality. Most practitioners discuss, explain and report anomalies in the following format: The anomaly would be like normal data A, if not for the corruption B. The reader will appreciate that is a type of counterfactual explanation. In this work we introduce a domain agnostic counterfactual explanation technique to produce explanations for time series anomalies. As we will show, our method can produce both visual and text-based explanations that are objectively correct, intuitive and in many circumstances, directly actionable.</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：LoMA: Lossless Compressed Memory Attention</b></summary>
  <p><b>编号</b>：[284]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09486</p>
  <p><b>作者</b>：Yumeng Wang,  Zhenyang Xiao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, handle long texts, capabilities of Large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The ability to handle long texts is one of the most important capabilities of Large Language Models (LLMs), but as the text length increases, the consumption of resources also increases dramatically. At present, reducing resource consumption by compressing the KV cache is a common approach. Although there are many existing compression methods, they share a common drawback: the compression is not lossless. That is, information is inevitably lost during the compression process. If the compression rate is high, the probability of losing important information increases dramatically. We propose a new method, Lossless Compressed Memory Attention (LoMA), which allows for lossless compression of information into special memory token KV pairs according to a set compression ratio. Our experiments have achieved remarkable results, demonstrating that LoMA can be efficiently trained and has very effective performance.</p>
  </details>
</details>
<details>
  <summary>83. <b>标题：Uncertainty-Aware Hardware Trojan Detection Using Multimodal Deep  Learning</b></summary>
  <p><b>编号</b>：[285]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09479</p>
  <p><b>作者</b>：Rahul Vishwakarma,  Amin Rezaei</p>
  <p><b>备注</b>：2024 Design, Automation and Test in Europe Conference | The European Event for Electronic System Design & Test (accepted)</p>
  <p><b>关键词</b>：zero-trust fabless era, hardware Trojans, hardware Trojan detection, fabless era, stages of chip</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The risk of hardware Trojans being inserted at various stages of chip production has increased in a zero-trust fabless era. To counter this, various machine learning solutions have been developed for the detection of hardware Trojans. While most of the focus has been on either a statistical or deep learning approach, the limited number of Trojan-infected benchmarks affects the detection accuracy and restricts the possibility of detecting zero-day Trojans. To close the gap, we first employ generative adversarial networks to amplify our data in two alternative representation modalities, a graph and a tabular, ensuring that the dataset is distributed in a representative manner. Further, we propose a multimodal deep learning approach to detect hardware Trojans and evaluate the results from both early fusion and late fusion strategies. We also estimate the uncertainty quantification metrics of each prediction for risk-aware decision-making. The outcomes not only confirms the efficacy of our proposed hardware Trojan detection method but also opens a new door for future studies employing multimodality and uncertainty quantification to address other hardware security challenges.</p>
  </details>
</details>
<details>
  <summary>84. <b>标题：Triamese-ViT: A 3D-Aware Method for Robust Brain Age Estimation from  MRIs</b></summary>
  <p><b>编号</b>：[287]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09475</p>
  <p><b>作者</b>：Zhaonian Zhang,  Richard Jiang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：improved diagnostic precision, significantly improved diagnostic, Magnetic Resonance Imaging, diagnostic precision, integration of machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The integration of machine learning in medicine has significantly improved diagnostic precision, particularly in the interpretation of complex structures like the human brain. Diagnosing challenging conditions such as Alzheimer's disease has prompted the development of brain age estimation techniques. These methods often leverage three-dimensional Magnetic Resonance Imaging (MRI) scans, with recent studies emphasizing the efficacy of 3D convolutional neural networks (CNNs) like 3D ResNet. However, the untapped potential of Vision Transformers (ViTs), known for their accuracy and interpretability, persists in this domain due to limitations in their 3D versions. This paper introduces Triamese-ViT, an innovative adaptation of the ViT model for brain age estimation. Our model uniquely combines ViTs from three different orientations to capture 3D information, significantly enhancing accuracy and interpretability. Tested on a dataset of 1351 MRI scans, Triamese-ViT achieves a Mean Absolute Error (MAE) of 3.84, a 0.9 Spearman correlation coefficient with chronological age, and a -0.29 Spearman correlation coefficient between the brain age gap (BAG) and chronological age, significantly better than previous methods for brian age estimation. A key innovation of Triamese-ViT is its capacity to generate a comprehensive 3D-like attention map, synthesized from 2D attention maps of each orientation-specific ViT. This feature is particularly beneficial for in-depth brain age analysis and disease diagnosis, offering deeper insights into brain health and the mechanisms of age-related neural changes.</p>
  </details>
</details>
<details>
  <summary>85. <b>标题：Parametric Constraints for Bayesian Knowledge Tracing from First  Principles</b></summary>
  <p><b>编号</b>：[295]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09456</p>
  <p><b>作者</b>：Denis Shchepakin,  Sreecharan Sankaranarayanan,  Dawn Zimmaro</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Bayesian Knowledge Tracing, BKT parameter space, Hidden Markov Model, learner state, Knowledge Tracing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Bayesian Knowledge Tracing (BKT) is a probabilistic model of a learner's state of mastery corresponding to a knowledge component. It considers the learner's state of mastery as a "hidden" or latent binary variable and updates this state based on the observed correctness of the learner's response using parameters that represent transition probabilities between states. BKT is often represented as a Hidden Markov Model and the Expectation-Maximization (EM) algorithm is used to infer these parameters. However, this algorithm can suffer from several issues including producing multiple viable sets of parameters, settling into a local minima, producing degenerate parameter values, and a high computational cost during fitting. This paper takes a "from first principles" approach to deriving constraints that can be imposed on the BKT parameter space. Starting from the basic mathematical truths of probability and building up to the behaviors expected of the BKT parameters in real systems, this paper presents a mathematical derivation that results in succinct constraints that can be imposed on the BKT parameter space. Since these constraints are necessary conditions, they can be applied prior to fitting in order to reduce computational cost and the likelihood of issues that can emerge from the EM procedure. In order to see that promise through, the paper further introduces a novel algorithm for estimating BKT parameters subject to the newly defined constraints. While the issue of degenerate parameter values has been reported previously, this paper is the first, to our best knowledge, to derive the constrains from first principles while also presenting an algorithm that respects those constraints.</p>
  </details>
</details>
<details>
  <summary>86. <b>标题：Dynamic Routing for Integrated Satellite-Terrestrial Networks: A  Constrained Multi-Agent Reinforcement Learning Approach</b></summary>
  <p><b>编号</b>：[296]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09455</p>
  <p><b>作者</b>：Yifeng Lyu,  Han Hu,  Rongfei Fan,  Zhi Liu,  Jianping An,  Shiwen Mao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：integrated satellite-terrestrial network, experienced significant growth, limited terrestrial infrastructure, offering seamless communication, seamless communication services</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The integrated satellite-terrestrial network (ISTN) system has experienced significant growth, offering seamless communication services in remote areas with limited terrestrial infrastructure. However, designing a routing scheme for ISTN is exceedingly difficult, primarily due to the heightened complexity resulting from the inclusion of additional ground stations, along with the requirement to satisfy various constraints related to satellite service quality. To address these challenges, we study packet routing with ground stations and satellites working jointly to transmit packets, while prioritizing fast communication and meeting energy efficiency and packet loss requirements. Specifically, we formulate the problem of packet routing with constraints as a max-min problem using the Lagrange method. Then we propose a novel constrained Multi-Agent reinforcement learning (MARL) dynamic routing algorithm named CMADR, which efficiently balances objective improvement and constraint satisfaction during the updating of policy and Lagrange multipliers. Finally, we conduct extensive experiments and an ablation study using the OneWeb and Telesat mega-constellations. Results demonstrate that CMADR reduces the packet delay by a minimum of 21% and 15%, while meeting stringent energy consumption and packet loss rate constraints, outperforming several baseline algorithms.</p>
  </details>
</details>
<details>
  <summary>87. <b>标题：Voila-A: Aligning Vision-Language Models with User's Gaze Attention</b></summary>
  <p><b>编号</b>：[297]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09454</p>
  <p><b>作者</b>：Kun Yan,  Lei Ji,  Zeyu Wang,  Yuntao Wang,  Nan Duan,  Shuai Ma</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：recent years, artificial intelligence, integration of vision, vision and language, language understanding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, the integration of vision and language understanding has led to significant advancements in artificial intelligence, particularly through Vision-Language Models (VLMs). However, existing VLMs face challenges in handling real-world applications with complex scenes and multiple objects, as well as aligning their focus with the diverse attention patterns of human users. In this paper, we introduce gaze information, feasibly collected by AR or VR devices, as a proxy for human attention to guide VLMs and propose a novel approach, Voila-A, for gaze alignment to enhance the interpretability and effectiveness of these models in real-world applications. First, we collect hundreds of minutes of gaze data to demonstrate that we can mimic human gaze modalities using localized narratives. We then design an automatic data annotation pipeline utilizing GPT-4 to generate the VOILA-COCO dataset. Additionally, we innovate the Voila Perceiver modules to integrate gaze information into VLMs while preserving their pretrained knowledge. We evaluate Voila-A using a hold-out validation set and a newly collected VOILA-GAZE Testset, which features real-life scenarios captured with a gaze-tracking device. Our experimental results demonstrate that Voila-A significantly outperforms several baseline models. By aligning model attention with human gaze patterns, Voila-A paves the way for more intuitive, user-centric VLMs and fosters engaging human-AI interaction across a wide range of applications.</p>
  </details>
</details>
<details>
  <summary>88. <b>标题：Incorporating Riemannian Geometric Features for Learning Coefficient of  Pressure Distributions on Airplane Wings</b></summary>
  <p><b>编号</b>：[298]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09452</p>
  <p><b>作者</b>：Liwei Hu,  Wenyong Wang,  Yu Xiang,  Stefan Sommer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：angle of attack, significantly impacted, aerodynamic coefficients, geometric features, predicted aerodynamic coefficients</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The aerodynamic coefficients of aircrafts are significantly impacted by its geometry, especially when the angle of attack (AoA) is large. In the field of aerodynamics, traditional polynomial-based parameterization uses as few parameters as possible to describe the geometry of an airfoil. However, because the 3D geometry of a wing is more complicated than the 2D airfoil, polynomial-based parameterizations have difficulty in accurately representing the entire shape of a wing in 3D space. Existing deep learning-based methods can extract massive latent neural representations for the shape of 2D airfoils or 2D slices of wings. Recent studies highlight that directly taking geometric features as inputs to the neural networks can improve the accuracy of predicted aerodynamic coefficients. Motivated by geometry theory, we propose to incorporate Riemannian geometric features for learning Coefficient of Pressure (CP) distributions on wing surfaces. Our method calculates geometric features (Riemannian metric, connection, and curvature) and further inputs the geometric features, coordinates and flight conditions into a deep learning model to predict the CP distribution. Experimental results show that our method, compared to state-of-the-art Deep Attention Network (DAN), reduces the predicted mean square error (MSE) of CP by an average of 8.41% for the DLR-F11 aircraft test set.</p>
  </details>
</details>
<details>
  <summary>89. <b>标题：Explainable Multimodal Sentiment Analysis on Bengali Memes</b></summary>
  <p><b>编号</b>：[303]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09446</p>
  <p><b>作者</b>：Kazi Toufique Elahi,  Tasnuva Binte Rahman,  Shakil Shahriar,  Samir Sarker,  Sajib Kumar Saha Joy,  Faisal Muhammad Shah</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：attracting online communities, digital era, attracting online, cultural barriers, distinctive and effective</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Memes have become a distinctive and effective form of communication in the digital era, attracting online communities and cutting across cultural barriers. Even though memes are frequently linked with humor, they have an amazing capacity to convey a wide range of emotions, including happiness, sarcasm, frustration, and more. Understanding and interpreting the sentiment underlying memes has become crucial in the age of information. Previous research has explored text-based, image-based, and multimodal approaches, leading to the development of models like CAPSAN and PromptHate for detecting various meme categories. However, the study of low-resource languages like Bengali memes remains scarce, with limited availability of publicly accessible datasets. A recent contribution includes the introduction of the MemoSen dataset. However, the achieved accuracy is notably low, and the dataset suffers from imbalanced distribution. In this study, we employed a multimodal approach using ResNet50 and BanglishBERT and achieved a satisfactory result of 0.71 weighted F1-score, performed comparison with unimodal approaches, and interpreted behaviors of the models using explainable artificial intelligence (XAI) techniques.</p>
  </details>
</details>
<details>
  <summary>90. <b>标题：CRD: Collaborative Representation Distance for Practical Anomaly  Detection</b></summary>
  <p><b>编号</b>：[306]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09443</p>
  <p><b>作者</b>：Chao Han,  Yudong Yan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：defect detection plays, Visual defect detection, intelligent industry, detection plays, plays an important</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Visual defect detection plays an important role in intelligent industry. Patch based methods consider visual images as a collection of image patches according to positions, which have stronger discriminative ability for small defects in products, e.g. scratches on pills. However, the nearest neighbor search for the query image and the stored patches will occupy $O(n)$ complexity in terms of time and space requirements, posing strict challenges for deployment in edge environments. In this paper, we propose an alternative approach to the distance calculation of image patches via collaborative representation models. Starting from the nearest neighbor distance with $L_0$ constraint, we relax the constraint to $L_2$ constraint and solve the distance quickly in close-formed without actually accessing the original stored collection of image patches. Furthermore, we point out that the main computational burden of this close-formed solution can be pre-computed by high-performance server before deployment. Consequently, the distance calculation on edge devices only requires a simple matrix multiplication, which is extremely lightweight and GPU-friendly. Performance on real industrial scenarios demonstrates that compared to the existing state-of-the-art methods, this distance achieves several hundred times improvement in computational efficiency with slight performance drop, while greatly reducing memory overhead.</p>
  </details>
</details>
<details>
  <summary>91. <b>标题：Voxceleb-ESP: preliminary experiments detecting Spanish celebrities from  their voices</b></summary>
  <p><b>编号</b>：[308]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09441</p>
  <p><b>作者</b>：Beltrán Labrador,  Manuel Otero-Gonzalez,  Alicia Lozano-Diez,  Daniel Ramos,  Doroteo T. Toledano,  Joaquin Gonzalez-Rodriguez</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：YouTube videos facilitating, paper presents VoxCeleb-ESP, paper presents, collection of pointers, pointers and timestamps</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents VoxCeleb-ESP, a collection of pointers and timestamps to YouTube videos facilitating the creation of a novel speaker recognition dataset. VoxCeleb-ESP captures real-world scenarios, incorporating diverse speaking styles, noises, and channel distortions. It includes 160 Spanish celebrities spanning various categories, ensuring a representative distribution across age groups and geographic regions in Spain. We provide two speaker trial lists for speaker identification tasks, each of them with same-video or different-video target trials respectively, accompanied by a cross-lingual evaluation of ResNet pretrained models. Preliminary speaker identification results suggest that the complexity of the detection task in VoxCeleb-ESP is equivalent to that of the original and much larger VoxCeleb in English. VoxCeleb-ESP contributes to the expansion of speaker recognition benchmarks with a comprehensive and diverse dataset for the Spanish language.</p>
  </details>
</details>
<details>
  <summary>92. <b>标题：RoleCraft-GLM: Advancing Personalized Role-Playing in Large Language  Models</b></summary>
  <p><b>编号</b>：[310]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09432</p>
  <p><b>作者</b>：Meiling Tao,  Xuechen Liang,  Tianyu Shi,  Lei Yu,  Yiting Xie</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, study presents RoleCraft-GLM, Language Models, innovative framework aimed, Large Language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This study presents RoleCraft-GLM, an innovative framework aimed at enhancing personalized role-playing with Large Language Models (LLMs). RoleCraft-GLM addresses the key issue of lacking personalized interactions in conversational AI, and offers a solution with detailed and emotionally nuanced character portrayals. We contribute a unique conversational dataset that shifts from conventional celebrity-centric characters to diverse, non-celebrity personas, thus enhancing the realism and complexity of language modeling interactions. Additionally, our approach includes meticulous character development, ensuring dialogues are both realistic and emotionally resonant. The effectiveness of RoleCraft-GLM is validated through various case studies, highlighting its versatility and skill in different scenarios. Our framework excels in generating dialogues that accurately reflect characters' personality traits and emotions, thereby boosting user engagement. In conclusion, RoleCraft-GLM marks a significant leap in personalized AI interactions, and paves the way for more authentic and immersive AI-assisted role-playing experiences by enabling more nuanced and emotionally rich dialogues</p>
  </details>
</details>
<details>
  <summary>93. <b>标题：Transduce: learning transduction grammars for string transformation</b></summary>
  <p><b>编号</b>：[311]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09426</p>
  <p><b>作者</b>：Francis Frydman,  Philippe Mangion</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：string transformation programs, utilizes various techniques, synthesis of string, programs from input-output, input-output examples utilizes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The synthesis of string transformation programs from input-output examples utilizes various techniques, all based on an inductive bias that comprises a restricted set of basic operators to be combined. A new algorithm, Transduce, is proposed, which is founded on the construction of abstract transduction grammars and their generalization. We experimentally demonstrate that Transduce can learn positional transformations efficiently from one or two positive examples without inductive bias, achieving a success rate higher than the current state of the art.</p>
  </details>
</details>
<details>
  <summary>94. <b>标题：Improving PTM Site Prediction by Coupling of Multi-Granularity Structure  and Multi-Scale Sequence Representation</b></summary>
  <p><b>编号</b>：[312]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10211</p>
  <p><b>作者</b>：Zhengyi Li,  Menglu Li,  Lida Zhu,  Wen Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Protein post-translational modification, post-translational modification, task in bioinformatics, PTM, fundamental task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Protein post-translational modification (PTM) site prediction is a fundamental task in bioinformatics. Several computational methods have been developed to predict PTM sites. However, existing methods ignore the structure information and merely utilize protein sequences. Furthermore, designing a more fine-grained structure representation learning method is urgently needed as PTM is a biological event that occurs at the atom granularity. In this paper, we propose a PTM site prediction method by Coupling of Multi-Granularity structure and Multi-Scale sequence representation, PTM-CMGMS for brevity. Specifically, multigranularity structure-aware representation learning is designed to learn neighborhood structure representations at the amino acid, atom, and whole protein granularity from AlphaFold predicted structures, followed by utilizing contrastive learning to optimize the structure representations.Additionally, multi-scale sequence representation learning is used to extract context sequence information, and motif generated by aligning all context sequences of PTM sites assists the prediction. Extensive experiments on three datasets show that PTM-CMGMS outperforms the state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>95. <b>标题：A Kaczmarz-inspired approach to accelerate the optimization of neural  network wavefunctions</b></summary>
  <p><b>编号</b>：[314]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10190</p>
  <p><b>作者</b>：Gil Goldshlager,  Nilin Abrahamsen,  Lin Lin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：variational Monte Carlo, Monte Carlo method, Neural network wavefunctions, network wavefunctions optimized, produce highly accurate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural network wavefunctions optimized using the variational Monte Carlo method have been shown to produce highly accurate results for the electronic structure of atoms and small molecules, but the high cost of optimizing such wavefunctions prevents their application to larger systems. We propose the Subsampled Projected-Increment Natural Gradient Descent (SPRING) optimizer to reduce this bottleneck. SPRING combines ideas from the recently introduced minimum-step stochastic reconfiguration optimizer (MinSR) and the classical randomized Kaczmarz method for solving linear least-squares problems. We demonstrate that SPRING outperforms both MinSR and the popular Kronecker-Factored Approximate Curvature method (KFAC) across a number of small atoms and molecules, given that the learning rates of all methods are optimally tuned. For example, on the oxygen atom, SPRING attains chemical accuracy after forty thousand training iterations, whereas both MinSR and KFAC fail to do so even after one hundred thousand iterations.</p>
  </details>
</details>
<details>
  <summary>96. <b>标题：Exploiting Hierarchical Interactions for Protein Surface Learning</b></summary>
  <p><b>编号</b>：[316]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10144</p>
  <p><b>作者</b>：Yiqun Lin,  Liang Pan,  Yi Li,  Ziwei Liu,  Xiaomeng Li</p>
  <p><b>备注</b>：Accepted to J-BHI</p>
  <p><b>关键词</b>：structural bioinformatics, important yet challenging, challenging problems, problems in structural, geometric features</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Predicting interactions between proteins is one of the most important yet challenging problems in structural bioinformatics. Intrinsically, potential function sites in protein surfaces are determined by both geometric and chemical features. However, existing works only consider handcrafted or individually learned chemical features from the atom type and extract geometric features independently. Here, we identify two key properties of effective protein surface learning: 1) relationship among atoms: atoms are linked with each other by covalent bonds to form biomolecules instead of appearing alone, leading to the significance of modeling the relationship among atoms in chemical feature learning. 2) hierarchical feature interaction: the neighboring residue effect validates the significance of hierarchical feature interaction among atoms and between surface points and atoms (or residues). In this paper, we present a principled framework based on deep learning techniques, namely Hierarchical Chemical and Geometric Feature Interaction Network (HCGNet), for protein surface analysis by bridging chemical and geometric features with hierarchical interactions. Extensive experiments demonstrate that our method outperforms the prior state-of-the-art method by 2.3% in site prediction task and 3.2% in interaction matching task, respectively. Our code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>97. <b>标题：Comparison analysis between standard polysomnographic data and  in-ear-EEG signals: A preliminary study</b></summary>
  <p><b>编号</b>：[320]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10107</p>
  <p><b>作者</b>：Gianpaolo Palo,  Luigi Fiorillo,  Giuliana Monachino,  Michal Bechny,  Mark Melnykowycz,  Athina Tzovara,  Valentina Agostini,  Francesca Dalia Faraci</p>
  <p><b>备注</b>：29 pages, 12 figures, 1 table</p>
  <p><b>关键词</b>：Study Objectives, PSG, evaluating sleep disorders, Polysomnography, Objectives</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Study Objectives: Polysomnography (PSG) currently serves as the benchmark for evaluating sleep disorders. Its discomfort, impracticality for home-use, and introduction of bias in sleep quality assessment necessitate the exploration of less invasive, cost-effective, and portable alternatives. One promising contender is the in-ear-EEG sensor, which offers advantages in terms of comfort, fixed electrode positions, resistance to electromagnetic interference, and user-friendliness. This study aims to establish a methodology to assess the similarity between the in-ear-EEG signal and standard PSG.
Methods: We assess the agreement between the PSG and in-ear-EEG derived hypnograms. We extract features in the time- and frequency- domain from PSG and in-ear-EEG 30-second epochs. We only consider the epochs where the PSG-scorers and the in-ear-EEG-scorers were in agreement. We introduce a methodology to quantify the similarity between PSG derivations and the single-channel in-ear-EEG. The approach relies on a comparison of distributions of selected features -- extracted for each sleep stage and subject on both PSG and the in-ear-EEG signals -- via a Jensen-Shannon Divergence Feature-based Similarity Index (JSD-FSI).
Results: We found a high intra-scorer variability, mainly due to the uncertainty the scorers had in evaluating the in-ear-EEG signals. We show that the similarity between PSG and in-ear-EEG signals is high (JSD-FSI: 0.61 +/- 0.06 in awake, 0.60 +/- 0.07 in NREM and 0.51 +/- 0.08 in REM), and in line with the similarity values computed independently on standard PSG-channel-combinations.
Conclusions: In-ear-EEG is a valuable solution for home-based sleep monitoring, however further studies with a larger and more heterogeneous dataset are needed.</p>
  </details>
</details>
<details>
  <summary>98. <b>标题：Learning shallow quantum circuits</b></summary>
  <p><b>编号</b>：[321]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10095</p>
  <p><b>作者</b>：Hsin-Yuan Huang,  Yunchao Liu,  Michael Broughton,  Isaac Kim,  Anurag Anshu,  Zeph Landau,  Jarrod R. McClean</p>
  <p><b>备注</b>：10 pages, 14 figures (7 inline; 7 floating) + 76-page appendix</p>
  <p><b>关键词</b>：shallow quantum circuits, shallow quantum, quantum circuits, quantum circuits remains, quantum</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite fundamental interests in learning quantum circuits, the existence of a computationally efficient algorithm for learning shallow quantum circuits remains an open question. Because shallow quantum circuits can generate distributions that are classically hard to sample from, existing learning algorithms do not apply. In this work, we present a polynomial-time classical algorithm for learning the description of any unknown $n$-qubit shallow quantum circuit $U$ (with arbitrary unknown architecture) within a small diamond distance using single-qubit measurement data on the output states of $U$. We also provide a polynomial-time classical algorithm for learning the description of any unknown $n$-qubit state $\lvert \psi \rangle = U \lvert 0^n \rangle$ prepared by a shallow quantum circuit $U$ (on a 2D lattice) within a small trace distance using single-qubit measurements on copies of $\lvert \psi \rangle$. Our approach uses a quantum circuit representation based on local inversions and a technique to combine these inversions. This circuit representation yields an optimization landscape that can be efficiently navigated and enables efficient learning of quantum circuits that are classically hard to simulate.</p>
  </details>
</details>
<details>
  <summary>99. <b>标题：Ventricular Segmentation: A Brief Comparison of U-Net Derivatives</b></summary>
  <p><b>编号</b>：[326]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09980</p>
  <p><b>作者</b>：Ketan Suhaas Saichandran</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Medical imaging refers, treat medical disorders, Magnetic Resonance Imaging, medical disorders related, medical disorders</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Medical imaging refers to the technologies and methods utilized to view the human body and its inside, in order to diagnose, monitor, or even treat medical disorders. This paper aims to explore the application of deep learning techniques in the semantic segmentation of Cardiac short-axis MRI (Magnetic Resonance Imaging) images, aiming to enhance the diagnosis, monitoring, and treatment of medical disorders related to the heart. The focus centers on implementing various architectures that are derivatives of U-Net, to effectively isolate specific parts of the heart for comprehensive anatomical and functional analysis. Through a combination of images, graphs, and quantitative metrics, the efficacy of the models and their predictions are showcased. Additionally, this paper addresses encountered challenges and outline strategies for future improvements. This abstract provides a concise overview of the efforts in utilizing deep learning for cardiac image segmentation, emphasizing both the accomplishments and areas for further refinement.</p>
  </details>
</details>
<details>
  <summary>100. <b>标题：False Discovery Rate Control for Gaussian Graphical Models via  Neighborhood Screening</b></summary>
  <p><b>编号</b>：[327]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09979</p>
  <p><b>作者</b>：Taulant Koka,  Jasin Machkour,  Michael Muma</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Gaussian graphical models, graphical models emerge, range of fields, wide range, models emerge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Gaussian graphical models emerge in a wide range of fields. They model the statistical relationships between variables as a graph, where an edge between two variables indicates conditional dependence. Unfortunately, well-established estimators, such as the graphical lasso or neighborhood selection, are known to be susceptible to a high prevalence of false edge detections. False detections may encourage inaccurate or even incorrect scientific interpretations, with major implications in applications, such as biomedicine or healthcare. In this paper, we introduce a nodewise variable selection approach to graph learning and provably control the false discovery rate of the selected edge set at a self-estimated level. A novel fusion method of the individual neighborhoods outputs an undirected graph estimate. The proposed method is parameter-free and does not require tuning by the user. Benchmarks against competing false discovery rate controlling methods in numerical experiments considering different graph topologies show a significant gain in performance.</p>
  </details>
</details>
<details>
  <summary>101. <b>标题：Qadence: a differentiable interface for digital-analog programs</b></summary>
  <p><b>编号</b>：[329]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09915</p>
  <p><b>作者</b>：Dominik Seitz,  Niklas Heim,  João P. Moutinho,  Roland Guichard,  Vytautas Abramavicius,  Aleksander Wennersteen,  Gert-Jan Both,  Anton Quelle,  Caroline de Groot,  Gergana V. Velikova,  Vincent E. Elfving,  Mario Dagrada</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：computation combining digital, combining digital single-qubit, digital single-qubit gates, global analog operations, analog operations acting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Digital-analog quantum computing (DAQC) is an alternative paradigm for universal quantum computation combining digital single-qubit gates with global analog operations acting on a register of interacting qubits. Currently, no available open-source software is tailored to express, differentiate, and execute programs within the DAQC paradigm. In this work, we address this shortfall by presenting Qadence, a high-level programming interface for building complex digital-analog quantum programs developed at Pasqal. Thanks to its flexible interface, native differentiability, and focus on real-device execution, Qadence aims at advancing research on variational quantum algorithms built for native DAQC platforms such as Rydberg atom arrays.</p>
  </details>
</details>
<details>
  <summary>102. <b>标题：Interplay between depth and width for interpolation in neural ODEs</b></summary>
  <p><b>编号</b>：[330]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09902</p>
  <p><b>作者</b>：Antonio Álvarez-López,  Arselane Hadj Slimane,  Enrique Zuazua Iriondo</p>
  <p><b>备注</b>：16 pages, 10 figures, double column</p>
  <p><b>关键词</b>：Neural ordinary differential, ordinary differential equations, architecture remains elusive, optimal architecture remains, neural ODEs</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural ordinary differential equations (neural ODEs) have emerged as a natural tool for supervised learning from a control perspective, yet a complete understanding of their optimal architecture remains elusive. In this work, we examine the interplay between their width $p$ and number of layer transitions $L$ (effectively the depth $L+1$). Specifically, we assess the model expressivity in terms of its capacity to interpolate either a finite dataset $D$ comprising $N$ pairs of points or two probability measures in $\mathbb{R}^d$ within a Wasserstein error margin $\varepsilon>0$. Our findings reveal a balancing trade-off between $p$ and $L$, with $L$ scaling as $O(1+N/p)$ for dataset interpolation, and $L=O\left(1+(p\varepsilon^d)^{-1}\right)$ for measure interpolation.
In the autonomous case, where $L=0$, a separate study is required, which we undertake focusing on dataset interpolation. We address the relaxed problem of $\varepsilon$-approximate controllability and establish an error decay of $\varepsilon\sim O(\log(p)p^{-1/d})$. This decay rate is a consequence of applying a universal approximation theorem to a custom-built Lipschitz vector field that interpolates $D$. In the high-dimensional setting, we further demonstrate that $p=O(N)$ neurons are likely sufficient to achieve exact control.</p>
  </details>
</details>
<details>
  <summary>103. <b>标题：FREED++: Improving RL Agents for Fragment-Based Molecule Generation by  Thorough Reproduction</b></summary>
  <p><b>编号</b>：[331]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09840</p>
  <p><b>作者</b>：Alexander Telepov,  Artem Tsypin,  Kuzma Khrabrov,  Sergey Yakukhnov,  Pavel Strashnov,  Petr Zhilyaev,  Egor Rumiantsev,  Daniel Ezhov,  Manvel Avetisian,  Olga Popova,  Artur Kadurin</p>
  <p><b>备注</b>：37 pages, 10 figures, to be published in TMLR journal (this https URL)</p>
  <p><b>关键词</b>：desired biological functionality, therapeutic drugs aims, biological functionality, rational design, therapeutic drugs</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A rational design of new therapeutic drugs aims to find a molecular structure with desired biological functionality, e.g., an ability to activate or suppress a specific protein via binding to it. Molecular docking is a common technique for evaluating protein-molecule interactions. Recently, Reinforcement Learning (RL) has emerged as a promising approach to generating molecules with the docking score (DS) as a reward. In this work, we reproduce, scrutinize and improve the recent RL model for molecule generation called FREED (arXiv:2110.01219). Extensive evaluation of the proposed method reveals several limitations and challenges despite the outstanding results reported for three target proteins. Our contributions include fixing numerous implementation bugs and simplifying the model while increasing its quality, significantly extending experiments, and conducting an accurate comparison with current state-of-the-art methods for protein-conditioned molecule generation. We show that the resulting fixed model is capable of producing molecules with superior docking scores compared to alternative approaches.</p>
  </details>
</details>
<details>
  <summary>104. <b>标题：BreastRegNet: A Deep Learning Framework for Registration of Breast  Faxitron and Histopathology Images</b></summary>
  <p><b>编号</b>：[334]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09791</p>
  <p><b>作者</b>：Negar Golestani,  Aihui Wang,  Gregory R Bean,  Mirabela Rusu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：standard treatment protocol, cancer entails administering, entails administering neoadjuvant, administering neoadjuvant therapy, breast cancer entails</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A standard treatment protocol for breast cancer entails administering neoadjuvant therapy followed by surgical removal of the tumor and surrounding tissue. Pathologists typically rely on cabinet X-ray radiographs, known as Faxitron, to examine the excised breast tissue and diagnose the extent of residual disease. However, accurately determining the location, size, and focality of residual cancer can be challenging, and incorrect assessments can lead to clinical consequences. The utilization of automated methods can improve the histopathology process, allowing pathologists to choose regions for sampling more effectively and precisely. Despite the recognized necessity, there are currently no such methods available. Training such automated detection models require accurate ground truth labels on ex-vivo radiology images, which can be acquired through registering Faxitron and histopathology images and mapping the extent of cancer from histopathology to x-ray images. This study introduces a deep learning-based image registration approach trained on mono-modal synthetic image pairs. The models were trained using data from 50 women who received neoadjuvant chemotherapy and underwent surgery. The results demonstrate that our method is faster and yields significantly lower average landmark error ($2.1\pm1.96$ mm) over the state-of-the-art iterative ($4.43\pm4.1$ mm) and deep learning ($4.02\pm3.15$ mm) approaches. Improved performance of our approach in integrating radiology and pathology information facilitates generating large datasets, which allows training models for more accurate breast cancer detection.</p>
  </details>
</details>
<details>
  <summary>105. <b>标题：Accelerating Distributed Stochastic Optimization via Self-Repellent  Random Walks</b></summary>
  <p><b>编号</b>：[339]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09665</p>
  <p><b>作者</b>：Jie Hu,  Vishwaraj Doshi,  Do Young Eun</p>
  <p><b>备注</b>：Accepted for oral presentation at the Twelfth International Conference on Learning Representations (ICLR 2024)</p>
  <p><b>关键词</b>：Markov chain, base Markov chain, nonlinear Markov chain, study a family, gradients are sampled</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study a family of distributed stochastic optimization algorithms where gradients are sampled by a token traversing a network of agents in random-walk fashion. Typically, these random-walks are chosen to be Markov chains that asymptotically sample from a desired target distribution, and play a critical role in the convergence of the optimization iterates. In this paper, we take a novel approach by replacing the standard linear Markovian token by one which follows a nonlinear Markov chain - namely the Self-Repellent Radom Walk (SRRW). Defined for any given 'base' Markov chain, the SRRW, parameterized by a positive scalar {\alpha}, is less likely to transition to states that were highly visited in the past, thus the name. In the context of MCMC sampling on a graph, a recent breakthrough in Doshi et al. (2023) shows that the SRRW achieves O(1/{\alpha}) decrease in the asymptotic variance for sampling. We propose the use of a 'generalized' version of the SRRW to drive token algorithms for distributed stochastic optimization in the form of stochastic approximation, termed SA-SRRW. We prove that the optimization iterate errors of the resulting SA-SRRW converge to zero almost surely and prove a central limit theorem, deriving the explicit form of the resulting asymptotic covariance matrix corresponding to iterate errors. This asymptotic covariance is always smaller than that of an algorithm driven by the base Markov chain and decreases at rate O(1/{\alpha}^2) - the performance benefit of using SRRW thereby amplified in the stochastic optimization context. Empirical results support our theoretical findings.</p>
  </details>
</details>
<details>
  <summary>106. <b>标题：Automatic 3D Multi-modal Ultrasound Segmentation of Human Placenta using  Fusion Strategies and Deep Learning</b></summary>
  <p><b>编号</b>：[344]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09638</p>
  <p><b>作者</b>：Sonit Singh,  Gordon Stevenson,  Brendan Mein,  Alec Welsh,  Arcot Sowmya</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：medical imaging modality, power Doppler, power Doppler scans, power Doppler ultrasound, B-mode and power</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Purpose: Ultrasound is the most commonly used medical imaging modality for diagnosis and screening in clinical practice. Due to its safety profile, noninvasive nature and portability, ultrasound is the primary imaging modality for fetal assessment in pregnancy. Current ultrasound processing methods are either manual or semi-automatic and are therefore laborious, time-consuming and prone to errors, and automation would go a long way in addressing these challenges. Automated identification of placental changes at earlier gestation could facilitate potential therapies for conditions such as fetal growth restriction and pre-eclampsia that are currently detected only at late gestational age, potentially preventing perinatal morbidity and mortality.
Methods: We propose an automatic three-dimensional multi-modal (B-mode and power Doppler) ultrasound segmentation of the human placenta using deep learning combined with different fusion strategies.We collected data containing Bmode and power Doppler ultrasound scans for 400 studies.
Results: We evaluated different fusion strategies and state-of-the-art image segmentation networks for placenta segmentation based on standard overlap- and boundary-based metrics. We found that multimodal information in the form of B-mode and power Doppler scans outperform any single modality. Furthermore, we found that B-mode and power Doppler input scans fused at the data level provide the best results with a mean Dice Similarity Coefficient (DSC) of 0.849.
Conclusion: We conclude that the multi-modal approach of combining B-mode and power Doppler scans is effective in segmenting the placenta from 3D ultrasound scans in a fully automated manner and is robust to quality variation of the datasets.</p>
  </details>
</details>
<details>
  <summary>107. <b>标题：SymTC: A Symbiotic Transformer-CNN Net for Instance Segmentation of  Lumbar Spine MRI</b></summary>
  <p><b>编号</b>：[346]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09627</p>
  <p><b>作者</b>：Jiasong Chen,  Linchen Qian,  Linhai Ma,  Timur Urakov,  Weiyong Gu,  Liang Liang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：low back pain, persistent low back, Convolutional Neural Network, image segmentation, Intervertebral disc disease</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Intervertebral disc disease, a prevalent ailment, frequently leads to intermittent or persistent low back pain, and diagnosing and assessing of this disease rely on accurate measurement of vertebral bone and intervertebral disc geometries from lumbar MR images. Deep neural network (DNN) models may assist clinicians with more efficient image segmentation of individual instances (disks and vertebrae) of the lumbar spine in an automated way, which is termed as instance image segmentation. In this work, we proposed SymTC, an innovative lumbar spine MR image segmentation model that combines the strengths of Transformer and Convolutional Neural Network (CNN). Specifically, we designed a parallel dual-path architecture to merge CNN layers and Transformer layers, and we integrated a novel position embedding into the self-attention module of Transformer, enhancing the utilization of positional information for more accurate segmentation. To further improves model performance, we introduced a new data augmentation technique to create synthetic yet realistic MR image dataset, named SSMSpine, which is made publicly available. We evaluated our SymTC and the other 15 existing image segmentation models on our private in-house dataset and the public SSMSpine dataset, using two metrics, Dice Similarity Coefficient and 95% Hausdorff Distance. The results show that our SymTC has the best performance for segmenting vertebral bones and intervertebral discs in lumbar spine MR images. The SymTC code and SSMSpine dataset are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>108. <b>标题：MITS-GAN: Safeguarding Medical Imaging from Tampering with Generative  Adversarial Networks</b></summary>
  <p><b>编号</b>：[347]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09624</p>
  <p><b>作者</b>：Giovanni Pasqualino,  Luca Guarnera,  Alessandro Ortis,  Sebastiano Battiato</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Generative Adversarial Networks, Adversarial Networks, Generative Adversarial, opened new possibilities, generation but raised</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The progress in generative models, particularly Generative Adversarial Networks (GANs), opened new possibilities for image generation but raised concerns about potential malicious uses, especially in sensitive areas like medical imaging. This study introduces MITS-GAN, a novel approach to prevent tampering in medical images, with a specific focus on CT scans. The approach disrupts the output of the attacker's CT-GAN architecture by introducing imperceptible but yet precise perturbations. Specifically, the proposed approach involves the introduction of appropriate Gaussian noise to the input as a protective measure against various attacks. Our method aims to enhance tamper resistance, comparing favorably to existing techniques. Experimental results on a CT scan dataset demonstrate MITS-GAN's superior performance, emphasizing its ability to generate tamper-resistant images with negligible artifacts. As image tampering in medical domains poses life-threatening risks, our proactive approach contributes to the responsible and ethical use of generative models. This work provides a foundation for future research in countering cyber threats in medical imaging. Models and codes are publicly available at the following link \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>109. <b>标题：Fully-blind Neural Network Based Equalization for Severe Nonlinear  Distortions in 112 Gbit/s Passive Optical Networks</b></summary>
  <p><b>编号</b>：[349]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09579</p>
  <p><b>作者</b>：Vincent Lauinger,  Patrick Matalla,  Jonas Ney,  Norbert Wehn,  Sebastian Randel,  Laurent Schmalen</p>
  <p><b>备注</b>：Accepted and to be presented at the Optical Fiber Communication Conference (OFC) 2024</p>
  <p><b>关键词</b>：digital signal processing, low hardware complexity, passive optical networks, fully-blind digital signal, equalizer topologies based</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We demonstrate and evaluate a fully-blind digital signal processing (DSP) chain for 100G passive optical networks (PONs), and analyze different equalizer topologies based on neural networks with low hardware complexity.</p>
  </details>
</details>
<details>
  <summary>110. <b>标题：Deep learning enhanced mixed integer optimization: Learning to reduce  model dimensionality</b></summary>
  <p><b>编号</b>：[350]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09556</p>
  <p><b>作者</b>：Niki Triantafyllou,  Maria M. Papathanasiou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：computational complexity inherent, models by harnessing, deep learning, work introduces, address the computational</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This work introduces a framework to address the computational complexity inherent in Mixed-Integer Programming (MIP) models by harnessing the potential of deep learning. We compare the effectiveness of (a) feed-forward neural networks (ANN) and (b) convolutional neural networks (CNN) in approximating the active dimensions within MIP problems. We utilize multi-label classification to account for more than one active dimension. To enhance the framework's performance, we employ Bayesian optimization for hyperparameter tuning, aiming to maximize sample-level accuracy. The primary objective is to train the neural networks to predict all active dimensions accurately, thereby maximizing the occurrence of global optimum solutions. We apply this framework to a flow-based facility location allocation Mixed-Integer Linear Programming (MILP) formulation that describes long-term investment planning and medium-term tactical planning in a personalized medicine supply chain for cell therapy manufacturing and distribution.</p>
  </details>
</details>
<details>
  <summary>111. <b>标题：Identifying Three-Dimensional Radiative Patterns Associated with Early  Tropical Cyclone Intensification</b></summary>
  <p><b>编号</b>：[353]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09493</p>
  <p><b>作者</b>：Frederick Iat-Hin Tam,  Tom Beucler,  James H. Ruppert Jr</p>
  <p><b>备注</b>：12 pages, 4 figures (main text)</p>
  <p><b>关键词</b>：early tropical cyclone, existing diagnostic frameworks, diagnostic frameworks make, transient radiative heating, feedback impacts early</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cloud radiative feedback impacts early tropical cyclone (TC) intensification, but limitations in existing diagnostic frameworks make them unsuitable for studying asymmetric or transient radiative heating. We propose a linear Variational Encoder-Decoder (VED) to learn the hidden relationship between radiation and the surface intensification of realistic simulated TCs. Limiting VED model inputs enables using its uncertainty to identify periods when radiation has more importance for intensification. A close examination of the extracted 3D radiative structures suggests that longwave radiative forcing from inner core deep convection and shallow clouds both contribute to intensification, with the deep convection having the most impact overall. We find that deep convection downwind of the shallow clouds is critical to the intensification of Haiyan. Our work demonstrates that machine learning can discover thermodynamic-kinematic relationships without relying on axisymmetric or deterministic assumptions, paving the way towards the objective discovery of processes leading to TC intensification in realistic conditions.</p>
  </details>
</details>
<details>
  <summary>112. <b>标题：Brain Tumor Radiogenomic Classification</b></summary>
  <p><b>编号</b>：[355]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09471</p>
  <p><b>作者</b>：Amr Mohamed,  Mahmoud Rabea,  Aya Sameh,  Ehab Kamal</p>
  <p><b>备注</b>：6 Pages with 4 Tables, 4 Figures and 4 Images</p>
  <p><b>关键词</b>：predict MGMT biomarker, MGMT biomarker status, Multi parameter mpMRI, tumor radiogenomic classification, radiogenomic classification challenge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The RSNA-MICCAI brain tumor radiogenomic classification challenge aimed to predict MGMT biomarker status in glioblastoma through binary classification on Multi parameter mpMRI scans: T1w, T1wCE, T2w and FLAIR. The dataset is splitted into three main cohorts: training set, validation set which were used during training, and the testing were only used during final evaluation. Images were either in a DICOM format or in Png format. different architectures were used to investigate the problem including the 3D version of Vision Transformer (ViT3D), ResNet50, Xception and EfficientNet-B3. AUC was used as the main evaluation metric and the results showed an advantage for both the ViT3D and the Xception models achieving 0.6015 and 0.61745 respectively on the testing set. compared to other results, our results proved to be valid given the complexity of the task. further improvements can be made through exploring different strategies, different architectures and more diverse datasets.</p>
  </details>
</details>
<details>
  <summary>113. <b>标题：Self Supervised Vision for Climate Downscaling</b></summary>
  <p><b>编号</b>：[356]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09466</p>
  <p><b>作者</b>：Karandeep Singh,  Chaeyoon Jeong,  Naufal Shidqi,  Sungwon Park,  Arjun Nellikkattil,  Elke Zeller,  Meeyoung Cha</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：facing today, Earth climate system, critical challenges, planet is facing, Earth System Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Climate change is one of the most critical challenges that our planet is facing today. Rising global temperatures are already bringing noticeable changes to Earth's weather and climate patterns with an increased frequency of unpredictable and extreme weather events. Future projections for climate change research are based on Earth System Models (ESMs), the computer models that simulate the Earth's climate system. ESMs provide a framework to integrate various physical systems, but their output is bound by the enormous computational resources required for running and archiving higher-resolution simulations. For a given resource budget, the ESMs are generally run on a coarser grid, followed by a computationally lighter $downscaling$ process to obtain a finer-resolution output. In this work, we present a deep-learning model for downscaling ESM simulation data that does not require high-resolution ground truth data for model optimization. This is realized by leveraging salient data distribution patterns and the hidden dependencies between weather variables for an $\textit{individual}$ data point at $\textit{runtime}$. Extensive evaluation with $2$x, $3$x, and $4$x scaling factors demonstrates that the proposed model consistently obtains superior performance over that of various baselines. The improved downscaling performance and no dependence on high-resolution ground truth data make the proposed method a valuable tool for climate research and mark it as a promising direction for future research.</p>
  </details>
</details>
<details>
  <summary>114. <b>标题：Diffusion-Driven Generative Framework for Molecular Conformation  Prediction</b></summary>
  <p><b>编号</b>：[359]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09451</p>
  <p><b>作者</b>：Bobin Yang,  Zhenghan Chen</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2105.07246 by other authors</p>
  <p><b>关键词</b>：development of pharmaceuticals, inferring three-dimensional molecular, three-dimensional molecular configurations, critical significance, domains of computational</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The task of inferring three-dimensional molecular configurations from their two-dimensional graph representations is of critical significance in the domains of computational chemistry and the development of pharmaceuticals. It contributes fundamentally to our grasp of molecular mechanisms and interactions. The rapid evolution of machine learning, especially in the realm of deep generative networks, has catalyzed breakthroughs in the precision of such predictive modeling. Traditional methodologies typically employ a bifurcated strategy: initially estimating interatomic distances followed by sculpting the spatial molecular structure via solving a distance geometry problem. This sequential approach, however, occasionally fails to capture the intricacies of local atomic arrangements accurately, thus compromising the integrity of the resultant structural models. Addressing these deficiencies, this work introduces an avant-garde generative framework: \method{}, which is predicated on the diffusion principles found in classical non-equilibrium thermodynamics. \method{} envisages atoms as discrete entities and is adept at guiding the reversal of diffusion morphing a distribution of stochastic noise back into coherent molecular forms through a process akin to a Markov chain. This transformation begins with the initial representation of a molecular graph in an abstract latent space, progressing to the realization of the three-dimensional forms via an elaborate bilevel optimization scheme, tailored to respect the task's specific requirements.</p>
  </details>
</details>
<details>
  <summary>115. <b>标题：A Smoothing Algorithm for l1 Support Vector Machines</b></summary>
  <p><b>编号</b>：[361]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09431</p>
  <p><b>作者</b>：Ibrahim Emirahmetoglu,  Jeffrey Hajewski,  Suely Oliveira,  David E. Stewart</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:1808.07100</p>
  <p><b>关键词</b>：Support Vector Machine, soft-margin Support Vector, Vector Machine, Support Vector, soft-margin Support</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A smoothing algorithm is presented for solving the soft-margin Support Vector Machine (SVM) optimization problem with an $\ell^{1}$ penalty. This algorithm is designed to require a modest number of passes over the data, which is an important measure of its cost for very large datasets. The algorithm uses smoothing for the hinge-loss function, and an active set approach for the $\ell^{1}$ penalty. The smoothing parameter $\alpha$ is initially large, but typically halved when the smoothed problem is solved to sufficient accuracy. Convergence theory is presented that shows $\mathcal{O}(1+\log(1+\log_+(1/\alpha)))$ guarded Newton steps for each value of $\alpha$ except for asymptotic bands $\alpha=\Theta(1)$ and $\alpha=\Theta(1/N)$, with only one Newton step provided $\eta\alpha\gg1/N$, where $N$ is the number of data points and the stopping criterion that the predicted reduction is less than $\eta\alpha$. The experimental results show that our algorithm is capable of strong test accuracy without sacrificing training speed.</p>
  </details>
</details>
<details>
  <summary>116. <b>标题：Precipitation Prediction Using an Ensemble of Lightweight Learners</b></summary>
  <p><b>编号</b>：[363]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09424</p>
  <p><b>作者</b>：Xinzhe Li,  Sun Rui,  Yiming Niu,  Yao Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Precipitation prediction plays, agriculture and industry, high precipitation events, prediction plays, plays a crucial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Precipitation prediction plays a crucial role in modern agriculture and industry. However, it poses significant challenges due to the diverse patterns and dynamics in time and space, as well as the scarcity of high precipitation events.
To address this challenge, we propose an ensemble learning framework that leverages multiple learners to capture the diverse patterns of precipitation distribution. Specifically, the framework consists of a precipitation predictor with multiple lightweight heads (learners) and a controller that combines the outputs from these heads. The learners and the controller are separately optimized with a proposed 3-stage training scheme.
By utilizing provided satellite images, the proposed approach can effectively model the intricate rainfall patterns, especially for high precipitation events. It achieved 1st place on the core test as well as the nowcasting leaderboards of the Weather4Cast 2023 competition. For detailed implementation, please refer to our GitHub repository at: this https URL.</p>
  </details>
</details>
<h1>人工智能</h1>
<details>
  <summary>1. <b>标题：ChatQA: Building GPT-4 Level Conversational QA Models</b></summary>
  <p><b>编号</b>：[7]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10225</p>
  <p><b>作者</b>：Zihan Liu,  Wei Ping,  Rajarshi Roy,  Peng Xu,  Mohammad Shoeybi,  Bryan Catanzaro</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：conversational question answering, level accuracies, introduce ChatQA, question answering, conversational question</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Supervised Fine-tuning in turn Improves Visual Foundation Models</b></summary>
  <p><b>编号</b>：[9]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10222</p>
  <p><b>作者</b>：Xiaohu Jiang,  Yixiao Ge,  Yuying Ge,  Chun Yuan,  Ying Shan</p>
  <p><b>备注</b>：14 pages, 3 figures, Project page: this https URL</p>
  <p><b>关键词</b>：vision foundation models, vision foundation, Image-text training, recent years, foundation models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image-text training like CLIP has dominated the pretraining of vision foundation models in recent years. Subsequent efforts have been made to introduce region-level visual learning into CLIP's pretraining but face scalability challenges due to the lack of large-scale region-level datasets. Drawing inspiration from supervised fine-tuning (SFT) in natural language processing such as instruction tuning, we explore the potential of fine-grained SFT in enhancing the generation of vision foundation models after their pretraining. Thus a two-stage method ViSFT (Vision SFT) is proposed to unleash the fine-grained knowledge of vision foundation models. In ViSFT, the vision foundation model is enhanced by performing visual joint learning on some in-domain tasks and then tested on out-of-domain benchmarks. With updating using ViSFT on 8 V100 GPUs in less than 2 days, a vision transformer with over 4.4B parameters shows improvements across various out-of-domain benchmarks including vision and vision-linguistic scenarios.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Mastery Guided Non-parametric Clustering to Scale-up Strategy Prediction</b></summary>
  <p><b>编号</b>：[17]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10210</p>
  <p><b>作者</b>：Anup Shakya,  Vasile Rus,  Deepak Venugopal</p>
  <p><b>备注</b>：Proceedings of 37th AAAI Conference on Artificial Intelligence Artificial Intelligence for Education. arXiv admin note: substantial text overlap with arXiv:2308.03892</p>
  <p><b>关键词</b>：Adaptive Instructional Systems, Instructional Systems, Adaptive Instructional, problem-solving helps Adaptive, types of learners</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Predicting the strategy (sequence of concepts) that a student is likely to use in problem-solving helps Adaptive Instructional Systems (AISs) better adapt themselves to different types of learners based on their learning abilities. This can lead to a more dynamic, engaging, and personalized experience for students. To scale up training a prediction model (such as LSTMs) over large-scale education datasets, we develop a non-parametric approach to cluster symmetric instances in the data. Specifically, we learn a representation based on Node2Vec that encodes symmetries over mastery or skill level since, to solve a problem, it is natural that a student's strategy is likely to involve concepts in which they have gained mastery. Using this representation, we use DP-Means to group symmetric instances through a coarse-to-fine refinement of the clusters. We apply our model to learn strategies for Math learning from large-scale datasets from MATHia, a leading AIS for middle-school math learning. Our results illustrate that our approach can consistently achieve high accuracy using a small sample that is representative of the full dataset. Further, we show that this approach helps us learn strategies with high accuracy for students at different skill levels, i.e., leveraging symmetries improves fairness in the prediction model.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Eclectic Rule Extraction for Explainability of Deep Neural Network based  Intrusion Detection Systems</b></summary>
  <p><b>编号</b>：[20]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10207</p>
  <p><b>作者</b>：Jesse Ables,  Nathaniel Childers,  William Anderson,  Sudip Mittal,  Shahram Rahimi,  Ioana Banicescu,  Maria Seale</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Explainable Artificial Intelligence, Intrusion Detection Systems, paper addresses trust, addresses trust issues, trust issues created</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper addresses trust issues created from the ubiquity of black box algorithms and surrogate explainers in Explainable Intrusion Detection Systems (X-IDS). While Explainable Artificial Intelligence (XAI) aims to enhance transparency, black box surrogate explainers, such as Local Interpretable Model-Agnostic Explanation (LIME) and SHapley Additive exPlanation (SHAP), are difficult to trust. The black box nature of these surrogate explainers makes the process behind explanation generation opaque and difficult to understand. To avoid this problem, one can use transparent white box algorithms such as Rule Extraction (RE). There are three types of RE algorithms: pedagogical, decompositional, and eclectic. Pedagogical methods offer fast but untrustworthy white-box explanations, while decompositional RE provides trustworthy explanations with poor scalability. This work explores eclectic rule extraction, which strikes a balance between scalability and trustworthiness. By combining techniques from pedagogical and decompositional approaches, eclectic rule extraction leverages the advantages of both, while mitigating some of their drawbacks. The proposed Hybrid X-IDS architecture features eclectic RE as a white box surrogate explainer for black box Deep Neural Networks (DNN). The presented eclectic RE algorithm extracts human-readable rules from hidden layers, facilitating explainable and trustworthy rulesets. Evaluations on UNSW-NB15 and CIC-IDS-2017 datasets demonstrate the algorithm's ability to generate rulesets with 99.9% accuracy, mimicking DNN outputs. The contributions of this work include the hybrid X-IDS architecture, the eclectic rule extraction algorithm applicable to intrusion detection datasets, and a thorough analysis of performance and explainability, demonstrating the trade-offs involved in rule extraction speed and accuracy.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through  Text Reconstruction</b></summary>
  <p><b>编号</b>：[25]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10189</p>
  <p><b>作者</b>：Qingyun Wang,  Zixuan Zhang,  Hongxiang Li,  Xuan Liu,  Jiawei Han,  Heng Ji,  Huimin Zhao</p>
  <p><b>备注</b>：16 pages. Accepted by Findings of the Association for Computational Linguistics: EACL 2024. Code and resources are available at this https URL</p>
  <p><b>关键词</b>：entity extraction, entity, extraction, chemical domain faces, faces two unique</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Fine-grained few-shot entity extraction in the chemical domain faces two unique challenges. First, compared with entity extraction tasks in the general domain, sentences from chemical papers usually contain more entities. Moreover, entity extraction models usually have difficulty extracting entities of long-tailed types. In this paper, we propose Chem-FINESE, a novel sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to address these two challenges. Our Chem-FINESE has two components: a seq2seq entity extractor to extract named entities from the input sentence and a seq2seq self-validation module to reconstruct the original input sentence from extracted entities. Inspired by the fact that a good entity extraction system needs to extract entities faithfully, our new self-validation module leverages entity extraction results to reconstruct the original input sentence. Besides, we design a new contrastive loss to reduce excessive copying during the extraction process. Finally, we release ChemNER+, a new fine-grained chemical entity extraction dataset that is annotated by domain experts with the ChemNER schema. Experiments in few-shot settings with both ChemNER+ and CHEMET datasets show that our newly proposed framework has contributed up to 8.26% and 6.84% absolute F1-score gains respectively.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Neural Echos: Depthwise Convolutional Filters Replicate Biological  Receptive Fields</b></summary>
  <p><b>编号</b>：[30]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10178</p>
  <p><b>作者</b>：Zahra Babaiee,  Peyman M. Kiasari,  Daniela Rus,  Radu Grosu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：present evidence suggesting, biological receptive fields, receptive fields observed, mammalian retina, effectively replicating</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this study, we present evidence suggesting that depthwise convolutional kernels are effectively replicating the structural intricacies of the biological receptive fields observed in the mammalian retina. We provide analytics of trained kernels from various state-of-the-art models substantiating this evidence. Inspired by this intriguing discovery, we propose an initialization scheme that draws inspiration from the biological receptive fields. Experimental analysis of the ImageNet dataset with multiple CNN architectures featuring depthwise convolutions reveals a marked enhancement in the accuracy of the learned model when initialized with biologically derived weights. This underlies the potential for biologically inspired computational models to further our understanding of vision processing systems and to improve the efficacy of convolutional networks.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：DISTINQT: A Distributed Privacy Aware Learning Framework for QoS  Prediction for Future Mobile and Wireless Networks</b></summary>
  <p><b>编号</b>：[35]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10158</p>
  <p><b>作者</b>：Nikolaos Koursioumpas,  Lina Magoula,  Ioannis Stavrakakis,  Nancy Alonistioti,  M. A. Gutierrez-Estevez,  Ramin Khalili</p>
  <p><b>备注</b>：11 Pages Double Column, 9 Figures, Submitted for possible publication in the IEEE Transactions on Vehicular Technology (IEEE TVT)</p>
  <p><b>关键词</b>：Quality of Service, level of Quality, operate smoothly, QoS prediction, centralized Artificial Intelligence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Beyond 5G and 6G networks are expected to support new and challenging use cases and applications that depend on a certain level of Quality of Service (QoS) to operate smoothly. Predicting the QoS in a timely manner is of high importance, especially for safety-critical applications as in the case of vehicular communications. Although until recent years the QoS prediction has been carried out by centralized Artificial Intelligence (AI) solutions, a number of privacy, computational, and operational concerns have emerged. Alternative solutions have been surfaced (e.g. Split Learning, Federated Learning), distributing AI tasks of reduced complexity across nodes, while preserving the privacy of the data. However, new challenges rise when it comes to scalable distributed learning approaches, taking into account the heterogeneous nature of future wireless networks. The current work proposes DISTINQT, a privacy-aware distributed learning framework for QoS prediction. Our framework supports multiple heterogeneous nodes, in terms of data types and model architectures, by sharing computations across them. This, enables the incorporation of diverse knowledge into a sole learning process that will enhance the robustness and generalization capabilities of the final QoS prediction model. DISTINQT also contributes to data privacy preservation by encoding any raw input data into a non-linear latent representation before any transmission. Evaluation results showcase that our framework achieves a statistically identical performance compared to its centralized version and an average performance improvement of up to 65% against six state-of-the-art centralized baseline solutions in the Tele-Operated Driving use case.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Explicitly Disentangled Representations in Object-Centric Learning</b></summary>
  <p><b>编号</b>：[41]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10148</p>
  <p><b>作者</b>：Riccardo Majellaro,  Jonathan Collu,  Aske Plaat,  Thomas M. Moerland</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Extracting structured representations, raw visual data, Extracting structured, raw visual, important and long-standing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Extracting structured representations from raw visual data is an important and long-standing challenge in machine learning. Recently, techniques for unsupervised learning of object-centric representations have raised growing interest. In this context, enhancing the robustness of the latent features can improve the efficiency and effectiveness of the training of downstream tasks. A promising step in this direction is to disentangle the factors that cause variation in the data. Previously, Invariant Slot Attention disentangled position, scale, and orientation from the remaining features. Extending this approach, we focus on separating the shape and texture components. In particular, we propose a novel architecture that biases object-centric models toward disentangling shape and texture components into two non-overlapping subsets of the latent space dimensions. These subsets are known a priori, hence before the training process. Experiments on a range of object-centric benchmarks reveal that our approach achieves the desired disentanglement while also numerically improving baseline performance in most cases. In addition, we show that our method can generate novel textures for a specific object or transfer textures between objects with distinct shapes.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Model Compression Techniques in Biometrics Applications: A Survey</b></summary>
  <p><b>编号</b>：[43]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10139</p>
  <p><b>作者</b>：Eduarda Caldeira,  Pedro C. Neto,  Marco Huber,  Naser Damer,  Ana F. Sequeira</p>
  <p><b>备注</b>：Under review at IEEE Journal</p>
  <p><b>关键词</b>：task automatization capacity, extensively empowered humanity, empowered humanity task, humanity task automatization, deep learning algorithms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The development of deep learning algorithms has extensively empowered humanity's task automatization capacity. However, the huge improvement in the performance of these models is highly correlated with their increasing level of complexity, limiting their usefulness in human-oriented applications, which are usually deployed in resource-constrained devices. This led to the development of compression techniques that drastically reduce the computational and memory costs of deep learning models without significant performance degradation. This paper aims to systematize the current literature on this topic by presenting a comprehensive survey of model compression techniques in biometrics applications, namely quantization, knowledge distillation and pruning. We conduct a critical analysis of the comparative value of these techniques, focusing on their advantages and disadvantages and presenting suggestions for future work directions that can potentially improve the current methods. Additionally, we discuss and analyze the link between model bias and model compression, highlighting the need to direct compression research toward model fairness in future works.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Towards Principled Graph Transformers</b></summary>
  <p><b>编号</b>：[49]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10119</p>
  <p><b>作者</b>：Luis Müller,  Christopher Morris</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：learning architectures based, k-dimensional Weisfeiler-Leman, well-understood expressive power, Edge Transformer, Graph learning architectures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Graph learning architectures based on the k-dimensional Weisfeiler-Leman (k-WL) hierarchy offer a theoretically well-understood expressive power. However, such architectures often fail to deliver solid predictive performance on real-world tasks, limiting their practical impact. In contrast, global attention-based models such as graph transformers demonstrate strong performance in practice, but comparing their expressive power with the k-WL hierarchy remains challenging, particularly since these architectures rely on positional or structural encodings for their expressivity and predictive performance. To address this, we show that the recently proposed Edge Transformer, a global attention model operating on node pairs instead of nodes, has at least 3-WL expressive power. Empirically, we demonstrate that the Edge Transformer surpasses other theoretically aligned architectures regarding predictive performance while not relying on positional or structural encodings.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Counterfactual Reasoning with Probabilistic Graphical Models for  Analyzing Socioecological Systems</b></summary>
  <p><b>编号</b>：[55]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10101</p>
  <p><b>作者</b>：Rafael Cabañas,  Ana D. Maldonado,  María Morales,  Pedro A. Aguilera,  Antonio Salmerón</p>
  <p><b>备注</b>：34 pages</p>
  <p><b>关键词</b>：emerging directions, hypothetical scenarios, directions in data, scenarios, data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Causal and counterfactual reasoning are emerging directions in data science that allow us to reason about hypothetical scenarios. This is particularly useful in domains where experimental data are usually not available. In the context of environmental and ecological sciences, causality enables us, for example, to predict how an ecosystem would respond to hypothetical interventions. A structural causal model is a class of probabilistic graphical models for causality, which, due to its intuitive nature, can be easily understood by experts in multiple fields. However, certain queries, called unidentifiable, cannot be calculated in an exact and precise manner. This paper proposes applying a novel and recent technique for bounding unidentifiable queries within the domain of socioecological systems. Our findings indicate that traditional statistical analysis, including probabilistic graphical models, can identify the influence between variables. However, such methods do not offer insights into the nature of the relationship, specifically whether it involves necessity or sufficiency. This is where counterfactual reasoning becomes valuable.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：DiffusionGPT: LLM-Driven Text-to-Image Generation System</b></summary>
  <p><b>编号</b>：[66]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10061</p>
  <p><b>作者</b>：Jie Qin,  Jie Wu,  Weifeng Chen,  Yuxi Ren,  Huixia Li,  Hefeng Wu,  Xuefeng Xiao,  Rui Wang,  Shilei Wen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：high-quality models shared, open-source platforms, proliferation of high-quality, shared on open-source, Large Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diffusion models have opened up new avenues for the field of image generation, resulting in the proliferation of high-quality models shared on open-source platforms. However, a major challenge persists in current text-to-image systems are often unable to handle diverse inputs, or are limited to single model results. Current unified attempts often fall into two orthogonal aspects: i) parse Diverse Prompts in input stage; ii) activate expert model to output. To combine the best of both worlds, we propose DiffusionGPT, which leverages Large Language Models (LLM) to offer a unified generation system capable of seamlessly accommodating various types of prompts and integrating domain-expert models. DiffusionGPT constructs domain-specific Trees for various generative models based on prior knowledge. When provided with an input, the LLM parses the prompt and employs the Trees-of-Thought to guide the selection of an appropriate model, thereby relaxing input constraints and ensuring exceptional performance across diverse domains. Moreover, we introduce Advantage Databases, where the Tree-of-Thought is enriched with human feedback, aligning the model selection process with human preferences. Through extensive experiments and comparisons, we demonstrate the effectiveness of DiffusionGPT, showcasing its potential for pushing the boundaries of image synthesis in diverse domains.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Large Language Models for Scientific Information Extraction: An  Empirical Study for Virology</b></summary>
  <p><b>编号</b>：[73]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10040</p>
  <p><b>作者</b>：Mahsa Shamsabadi,  Jennifer D'Souza,  Sören Auer</p>
  <p><b>备注</b>：8 pages, 6 figures, Accepted as Findings of the ACL: EACL 2024</p>
  <p><b>关键词</b>：Amazon product descriptions, structured Amazon product, semantic content representation, discourse-based scholarly communication, tools like Wikipedia</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we champion the use of structured and semantic content representation of discourse-based scholarly communication, inspired by tools like Wikipedia infoboxes or structured Amazon product descriptions. These representations provide users with a concise overview, aiding scientists in navigating the dense academic landscape. Our novel automated approach leverages the robust text generation capabilities of LLMs to produce structured scholarly contribution summaries, offering both a practical solution and insights into LLMs' emergent abilities.
For LLMs, the prime focus is on improving their general intelligence as conversational agents. We argue that these models can also be applied effectively in information extraction (IE), specifically in complex IE tasks within terse domains like Science. This paradigm shift replaces the traditional modular, pipelined machine learning approach with a simpler objective expressed through instructions. Our results show that finetuned FLAN-T5 with 1000x fewer parameters than the state-of-the-art GPT-davinci is competitive for the task.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：LOCALINTEL: Generating Organizational Threat Intelligence from Global  and Local Cyber Knowledge</b></summary>
  <p><b>编号</b>：[76]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10036</p>
  <p><b>作者</b>：Shaswata Mitra,  Subash Neupane,  Trisha Chakraborty,  Sudip Mittal,  Aritran Piplai,  Manas Gaur,  Shahram Rahimi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Security Operations Center, Operations Center, Security Operations, local knowledge database, local knowledge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Security Operations Center (SoC) analysts gather threat reports from openly accessible global threat databases and customize them manually to suit a particular organization's needs. These analysts also depend on internal repositories, which act as private local knowledge database for an organization. Credible cyber intelligence, critical operational details, and relevant organizational information are all stored in these local knowledge databases. Analysts undertake a labor intensive task utilizing these global and local knowledge databases to manually create organization's unique threat response and mitigation strategies. Recently, Large Language Models (LLMs) have shown the capability to efficiently process large diverse knowledge sources. We leverage this ability to process global and local knowledge databases to automate the generation of organization-specific threat intelligence.
In this work, we present LOCALINTEL, a novel automated knowledge contextualization system that, upon prompting, retrieves threat reports from the global threat repositories and uses its local knowledge database to contextualize them for a specific organization. LOCALINTEL comprises of three key phases: global threat intelligence retrieval, local knowledge retrieval, and contextualized completion generation. The former retrieves intelligence from global threat repositories, while the second retrieves pertinent knowledge from the local knowledge database. Finally, the fusion of these knowledge sources is orchestrated through a generator to produce a contextualized completion.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Evolutionary Computation in the Era of Large Language Model: Survey and  Roadmap</b></summary>
  <p><b>编号</b>：[77]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10034</p>
  <p><b>作者</b>：Xingyu Wu,  Sheng-hao Wu,  Jibin Wu,  Liang Feng,  Kay Chen Tan</p>
  <p><b>备注</b>：evolutionary algorithm (EA), large language model (LLM), optimization problem, prompt optimization, architecture search, code generation</p>
  <p><b>关键词</b>：Large Language Models, revolutionized natural language, Language Models, artificial general intelligence, natural language processing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs), built upon Transformer-based architectures with massive pretraining on diverse data, have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence. The interplay between LLMs and Evolutionary Algorithms (EAs), despite differing in objectives and methodologies, reveals intriguing parallels, especially in their shared optimization nature, black-box characteristics, and proficiency in handling complex problems. Meanwhile, EA can not only provide an optimization framework for LLM's further enhancement under black-box settings but also empower LLM with flexible global search and iterative mechanism in applications. On the other hand, LLM's abundant domain knowledge enables EA to perform smarter searches, while its text processing capability assist in deploying EA across various tasks. Based on their complementary advantages, this paper presents a comprehensive review and forward-looking roadmap, categorizing their mutual inspiration into LLM-enhanced evolutionary optimization and EA-enhanced LLM. Some integrated synergy methods are further introduced to exemplify the amalgamation of LLMs and EAs in various application scenarios, including neural architecture search, code generation, software engineering, and text generation. As the first comprehensive review specifically focused on the EA research in the era of LLMs, this paper provides a foundational stepping stone for understanding and harnessing the collaborative potential of LLMs and EAs. By presenting a comprehensive review, categorization, and critical analysis, we contribute to the ongoing discourse on the cross-disciplinary study of these two powerful paradigms. The identified challenges and future directions offer guidance to unlock the full potential of this innovative collaboration.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Self-Rewarding Language Models</b></summary>
  <p><b>编号</b>：[80]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10020</p>
  <p><b>作者</b>：Weizhe Yuan,  Richard Yuanzhe Pang,  Kyunghyun Cho,  Sainbayar Sukhbaatar,  Jing Xu,  Jason Weston</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：achieve superhuman agents, require superhuman feedback, future models require, models require superhuman, adequate training signal</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While only a preliminary study, this work opens the door to the possibility of models that can continually improve in both axes.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：R-Judge: Benchmarking Safety Risk Awareness for LLM Agents</b></summary>
  <p><b>编号</b>：[81]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10019</p>
  <p><b>作者</b>：Tongxin Yuan,  Zhiwei He,  Lingzhong Dong,  Yiming Wang,  Ruijie Zhao,  Tian Xia,  Lizhen Xu,  Binglin Zhou,  Fangqi Li,  Zhuosheng Zhang,  Rui Wang,  Gongshen Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：exhibited great potential, autonomously completing tasks, Large language models, Large language, safety</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have exhibited great potential in autonomously completing tasks across real-world applications. Despite this, these LLM agents introduce unexpected safety risks when operating in interactive environments. Instead of centering on LLM-generated content safety in most prior studies, this work addresses the imperative need for benchmarking the behavioral safety of LLM agents within diverse environments. We introduce R-Judge, a benchmark crafted to evaluate the proficiency of LLMs in judging safety risks given agent interaction records. R-Judge comprises 162 agent interaction records, encompassing 27 key risk scenarios among 7 application categories and 10 risk types. It incorporates human consensus on safety with annotated safety risk labels and high-quality risk descriptions. Utilizing R-Judge, we conduct a comprehensive evaluation of 8 prominent LLMs commonly employed as the backbone for agents. The best-performing model, GPT-4, achieves 72.29% in contrast to the human score of 89.38%, showing considerable room for enhancing the risk awareness of LLMs. Notably, leveraging risk descriptions as environment feedback significantly improves model performance, revealing the importance of salient safety risk feedback. Furthermore, we design an effective chain of safety analysis technique to help the judgment of safety risks and conduct an in-depth case study to facilitate future research. R-Judge is publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Gender Bias in Machine Translation and The Era of Large Language Models</b></summary>
  <p><b>编号</b>：[83]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10016</p>
  <p><b>作者</b>：Eva Vanmassenhove</p>
  <p><b>备注</b>：24 pages</p>
  <p><b>关键词</b>：Neural Machine Translation, Machine Translation systems, Machine Translation, Generative Pretrained Transformer, Machine Translation approaches</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This chapter examines the role of Machine Translation in perpetuating gender bias, highlighting the challenges posed by cross-linguistic settings and statistical dependencies. A comprehensive overview of relevant existing work related to gender bias in both conventional Neural Machine Translation approaches and Generative Pretrained Transformer models employed as Machine Translation systems is provided. Through an experiment using ChatGPT (based on GPT-3.5) in an English-Italian translation context, we further assess ChatGPT's current capacity to address gender bias. The findings emphasize the ongoing need for advancements in mitigating bias in Machine Translation systems and underscore the importance of fostering fairness and inclusivity in language technologies.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：A-KIT: Adaptive Kalman-Informed Transformer</b></summary>
  <p><b>编号</b>：[94]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09987</p>
  <p><b>作者</b>：Nadav Cohen,  Itzik Klein</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：extended Kalman filter, extended Kalman, widely adopted method, process noise, process noise covariance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The extended Kalman filter (EKF) is a widely adopted method for sensor fusion in navigation applications. A crucial aspect of the EKF is the online determination of the process noise covariance matrix reflecting the model uncertainty. While common EKF implementation assumes a constant process noise, in real-world scenarios, the process noise varies, leading to inaccuracies in the estimated state and potentially causing the filter to diverge. To cope with such situations, model-based adaptive EKF methods were proposed and demonstrated performance improvements, highlighting the need for a robust adaptive approach. In this paper, we derive and introduce A-KIT, an adaptive Kalman-informed transformer to learn the varying process noise covariance online. The A-KIT framework is applicable to any type of sensor fusion. Here, we present our approach to nonlinear sensor fusion based on an inertial navigation system and Doppler velocity log. By employing real recorded data from an autonomous underwater vehicle, we show that A-KIT outperforms the conventional EKF by more than 49.5% and model-based adaptive EKF by an average of 35.4% in terms of position accuracy.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：FLex&Chill: Improving Local Federated Learning Training with Logit  Chilling</b></summary>
  <p><b>编号</b>：[95]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09986</p>
  <p><b>作者</b>：Kichang Lee,  Songkuk Kim,  JeongGil Ko</p>
  <p><b>备注</b>：9 pages</p>
  <p><b>关键词</b>：Logit Chilling method, local clients, Federated learning, inherently hampered, non-iid distributed training</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Federated learning are inherently hampered by data heterogeneity: non-iid distributed training data over local clients. We propose a novel model training approach for federated learning, FLex&Chill, which exploits the Logit Chilling method. Through extensive evaluations, we demonstrate that, in the presence of non-iid data characteristics inherent in federated learning systems, this approach can expedite model convergence and improve inference accuracy. Quantitatively, from our experiments, we observe up to 6X improvement in the global federated learning model convergence time, and up to 3.37% improvement in inference accuracy.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Multiobjective Optimization Analysis for Finding Infrastructure-as-Code  Deployment Configurations</b></summary>
  <p><b>编号</b>：[98]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09983</p>
  <p><b>作者</b>：Eneko Osaba,  Josu Diaz-de-Arcaya,  Juncal Alonso,  Jesus L. Lobo,  Gorka Benguria,  Iñaki Etxaniz</p>
  <p><b>备注</b>：9 pages, 1 figure, 4 tables. Paper presented in the 11th International Conference on Computer and Communications Management (ICCCM 2023)</p>
  <p><b>关键词</b>：operations research communities, research communities, hot topic, artificial intelligence, intelligence and operations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multiobjective optimization is a hot topic in the artificial intelligence and operations research communities. The design and development of multiobjective methods is a frequent task for researchers and practitioners. As a result of this vibrant activity, a myriad of techniques have been proposed in the literature to date, demonstrating a significant effectiveness for dealing with situations coming from a wide range of real-world areas. This paper is focused on a multiobjective problem related to optimizing Infrastructure-as-Code deployment configurations. The system implemented for solving this problem has been coined as IaC Optimizer Platform (IOP). Despite the fact that a prototypical version of the IOP has been introduced in the literature before, a deeper analysis focused on the resolution of the problem is needed, in order to determine which is the most appropriate multiobjective method for embedding in the IOP. The main motivation behind the analysis conducted in this work is to enhance the IOP performance as much as possible. This is a crucial aspect of this system, deeming that it will be deployed in a real environment, as it is being developed as part of a H2020 European project. Going deeper, we resort in this paper to nine different evolutionary computation-based multiobjective algorithms. For assessing the quality of the considered solvers, 12 different problem instances have been generated based on real-world settings. Results obtained by each method after 10 independent runs have been compared using Friedman's non-parametric tests. Findings reached from the tests carried out lad to the creation of a multi-algorithm system, capable of applying different techniques according to the user's needs.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Towards Generative Abstract Reasoning: Completing Raven's Progressive  Matrix via Rule Abstraction and Selection</b></summary>
  <p><b>编号</b>：[103]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09966</p>
  <p><b>作者</b>：Fan Shi,  Bin Li,  Xiangyang Xue</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：long-term research topic, Raven Progressive Matrix, Progressive Matrix, rules, long-term research</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Endowing machines with abstract reasoning ability has been a long-term research topic in artificial intelligence. Raven's Progressive Matrix (RPM) is widely used to probe abstract visual reasoning in machine intelligence, where models need to understand the underlying rules and select the missing bottom-right images out of candidate sets to complete image matrices. The participators can display powerful reasoning ability by inferring the underlying attribute-changing rules and imagining the missing images at arbitrary positions. However, existing solvers can hardly manifest such an ability in realistic RPM problems. In this paper, we propose a conditional generative model to solve answer generation problems through Rule AbstractIon and SElection (RAISE) in the latent space. RAISE encodes image attributes as latent concepts and decomposes underlying rules into atomic rules by means of concepts, which are abstracted as global learnable parameters. When generating the answer, RAISE selects proper atomic rules out of the global knowledge set for each concept and composes them into the integrated rule of an RPM. In most configurations, RAISE outperforms the compared generative solvers in tasks of generating bottom-right and arbitrary-position answers. We test RAISE in the odd-one-out task and two held-out configurations to demonstrate how learning decoupled latent concepts and atomic rules helps find the image breaking the underlying rules and handle RPMs with unseen combinations of rules and attributes.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：When Neural Code Completion Models Size up the Situation: Attaining  Cheaper and Faster Completion through Dynamic Model Inference</b></summary>
  <p><b>编号</b>：[104]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09964</p>
  <p><b>作者</b>：Zhensu Sun,  Xiaoning Du,  Fu Song,  Shangwen Wang,  Li Li</p>
  <p><b>备注</b>：Accepted to ICSE24</p>
  <p><b>关键词</b>：Leveraging recent advancements, generate highly accurate, modern neural code, accurate code suggestions, highly accurate code</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Leveraging recent advancements in large language models, modern neural code completion models have demonstrated the capability to generate highly accurate code suggestions. However, their massive size poses challenges in terms of computational costs and environmental impact, hindering their widespread adoption in practical scenarios. Dynamic inference emerges as a promising solution, as it allocates minimal computation during inference while maintaining the model's performance. In this research, we explore dynamic inference within the context of code completion. Initially, we conducted an empirical investigation on GPT-2, focusing on the inference capabilities of intermediate layers for code completion. We found that 54.4% of tokens can be accurately generated using just the first layer, signifying significant computational savings potential. Moreover, despite using all layers, the model still fails to predict 14.5% of tokens correctly, and the subsequent completions continued from them are rarely considered helpful, with only a 4.2% Acceptance Rate. These findings motivate our exploration of dynamic inference in code completion and inspire us to enhance it with a decision-making mechanism that stops the generation of incorrect code. We thus propose a novel dynamic inference method specifically tailored for code completion models. This method aims not only to produce correct predictions with largely reduced computation but also to prevent incorrect predictions proactively. Our extensive evaluation shows that it can averagely skip 1.7 layers out of 16 layers in the models, leading to an 11.2% speedup with only a marginal 1.1% reduction in ROUGE-L.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：WindSeer: Real-time volumetric wind prediction over complex terrain  aboard a small UAV</b></summary>
  <p><b>编号</b>：[111]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09944</p>
  <p><b>作者</b>：Florian Achermann,  Thomas Stastny,  Bogdan Danciu,  Andrey Kolobov,  Jen Jen Chung,  Roland Siegwart,  Nicholas Lawrance</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：including safe manned, applications including safe, unmanned aviation, including safe, safe manned</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Real-time high-resolution wind predictions are beneficial for various applications including safe manned and unmanned aviation. Current weather models require too much compute and lack the necessary predictive capabilities as they are valid only at the scale of multiple kilometers and hours - much lower spatial and temporal resolutions than these applications require. Our work, for the first time, demonstrates the ability to predict low-altitude wind in real-time on limited-compute devices, from only sparse measurement data. We train a neural network, WindSeer, using only synthetic data from computational fluid dynamics simulations and show that it can successfully predict real wind fields over terrain with known topography from just a few noisy and spatially clustered wind measurements. WindSeer can generate accurate predictions at different resolutions and domain sizes on previously unseen topography without retraining. We demonstrate that the model successfully predicts historical wind data collected by weather stations and wind measured onboard drones.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Multi-task Learning for Joint Re-identification, Team Affiliation, and  Role Classification for Sports Visual Tracking</b></summary>
  <p><b>编号</b>：[113]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09942</p>
  <p><b>作者</b>：Amir M. Mansourian,  Vladimir Somers,  Christophe De Vleeschouwer,  Shohreh Kasaei</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：analyzing soccer videos, soccer videos, essential for analyzing, analyzing soccer, tracking</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Effective tracking and re-identification of players is essential for analyzing soccer videos. But, it is a challenging task due to the non-linear motion of players, the similarity in appearance of players from the same team, and frequent occlusions. Therefore, the ability to extract meaningful embeddings to represent players is crucial in developing an effective tracking and re-identification system. In this paper, a multi-purpose part-based person representation method, called PRTreID, is proposed that performs three tasks of role classification, team affiliation, and re-identification, simultaneously. In contrast to available literature, a single network is trained with multi-task supervision to solve all three tasks, jointly. The proposed joint method is computationally efficient due to the shared backbone. Also, the multi-task learning leads to richer and more discriminative representations, as demonstrated by both quantitative and qualitative results. To demonstrate the effectiveness of PRTreID, it is integrated with a state-of-the-art tracking method, using a part-based post-processing module to handle long-term tracking. The proposed tracking method outperforms all existing tracking methods on the challenging SoccerNet tracking dataset.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：XAI-Enhanced Semantic Segmentation Models for Visual Quality Inspection</b></summary>
  <p><b>编号</b>：[125]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09900</p>
  <p><b>作者</b>：Tobias Clement,  Truong Thanh Hung Nguyen,  Mohamed Abdelaal,  Hung Cao</p>
  <p><b>备注</b>：IEEE ICCE 2024</p>
  <p><b>关键词</b>：rapid defect detection, employ computer vision, Visual quality inspection, crucial in sectors, manufacturing and logistics</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Visual quality inspection systems, crucial in sectors like manufacturing and logistics, employ computer vision and machine learning for precise, rapid defect detection. However, their unexplained nature can hinder trust, error identification, and system improvement. This paper presents a framework to bolster visual quality inspection by using CAM-based explanations to refine semantic segmentation models. Our approach consists of 1) Model Training, 2) XAI-based Model Explanation, 3) XAI Evaluation, and 4) Annotation Augmentation for Model Enhancement, informed by explanations and expert insights. Evaluations show XAI-enhanced models surpass original DeepLabv3-ResNet101 models, especially in intricate object segmentation.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Cooperative Edge Caching Based on Elastic Federated and Multi-Agent Deep  Reinforcement Learning in Next-Generation Network</b></summary>
  <p><b>编号</b>：[130]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09886</p>
  <p><b>作者</b>：Qiong Wu,  Wenhua Wang,  Pingyi Fan,  Qiang Fan,  Huiling Zhu,  Khaled B. Letaief</p>
  <p><b>备注</b>：This paper has been submitted to IEEE TNSM. The source code has been released at: this https URL</p>
  <p><b>关键词</b>：small-cell base stations, popular contents, empowering caching units, users' requested contents, contents</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Edge caching is a promising solution for next-generation networks by empowering caching units in small-cell base stations (SBSs), which allows user equipments (UEs) to fetch users' requested contents that have been pre-cached in SBSs. It is crucial for SBSs to predict accurate popular contents through learning while protecting users' personal information. Traditional federated learning (FL) can protect users' privacy but the data discrepancies among UEs can lead to a degradation in model quality. Therefore, it is necessary to train personalized local models for each UE to predict popular contents accurately. In addition, the cached contents can be shared among adjacent SBSs in next-generation networks, thus caching predicted popular contents in different SBSs may affect the cost to fetch contents. Hence, it is critical to determine where the popular contents are cached cooperatively. To address these issues, we propose a cooperative edge caching scheme based on elastic federated and multi-agent deep reinforcement learning (CEFMR) to optimize the cost in the network. We first propose an elastic FL algorithm to train the personalized model for each UE, where adversarial autoencoder (AAE) model is adopted for training to improve the prediction accuracy, then {a popular} content prediction algorithm is proposed to predict the popular contents for each SBS based on the trained AAE model. Finally, we propose a multi-agent deep reinforcement learning (MADRL) based algorithm to decide where the predicted popular contents are collaboratively cached among SBSs. Our experimental results demonstrate the superiority of our proposed scheme to existing baseline caching schemes.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Attention-Based Recurrent Neural Network For Automatic Behavior Laying  Hen Recognition</b></summary>
  <p><b>编号</b>：[134]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09880</p>
  <p><b>作者</b>：Fréjus A. A. Laleye,  Mikaël A. Mousse</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：modern poultry farming, laying hens, laying hen behavior, frequency domain features, laying hen call</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>One of the interests of modern poultry farming is the vocalization of laying hens which contain very useful information on health behavior. This information is used as health and well-being indicators that help breeders better monitor laying hens, which involves early detection of problems for rapid and more effective intervention. In this work, we focus on the sound analysis for the recognition of the types of calls of the laying hens in order to propose a robust system of characterization of their behavior for a better monitoring. To do this, we first collected and annotated laying hen call signals, then designed an optimal acoustic characterization based on the combination of time and frequency domain features. We then used these features to build the multi-label classification models based on recurrent neural network to assign a semantic class to the vocalization that characterize the laying hen behavior. The results show an overall performance with our model based on the combination of time and frequency domain features that obtained the highest F1-score (F1=92.75) with a gain of 17% on the models using the frequency domain features and of 8% on the compared approaches from the litterature.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Reconciling Spatial and Temporal Abstractions for Goal Representation</b></summary>
  <p><b>编号</b>：[137]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09870</p>
  <p><b>作者</b>：Mehdi Zadem,  Sergio Mover,  Sao Mai Nguyen</p>
  <p><b>备注</b>：Accepted for publication in ICLR 2024</p>
  <p><b>关键词</b>：Hierarchical Reinforcement Learning, Hierarchical Reinforcement, Reinforcement Learning, performance of Hierarchical, complex learning problem</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Goal representation affects the performance of Hierarchical Reinforcement Learning (HRL) algorithms by decomposing the complex learning problem into easier subtasks. Recent studies show that representations that preserve temporally abstract environment dynamics are successful in solving difficult problems and provide theoretical guarantees for optimality. These methods however cannot scale to tasks where environment dynamics increase in complexity i.e. the temporally abstract transition relations depend on larger number of variables. On the other hand, other efforts have tried to use spatial abstraction to mitigate the previous issues. Their limitations include scalability to high dimensional environments and dependency on prior knowledge.
In this paper, we propose a novel three-layer HRL algorithm that introduces, at different levels of the hierarchy, both a spatial and a temporal goal abstraction. We provide a theoretical study of the regret bounds of the learned policies. We evaluate the approach on complex continuous control tasks, demonstrating the effectiveness of spatial and temporal abstractions learned by this approach.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Improving fine-grained understanding in image-text pre-training</b></summary>
  <p><b>编号</b>：[139]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09865</p>
  <p><b>作者</b>：Ioana Bica,  Anastasija Ilić,  Matthias Bauer,  Goker Erdogan,  Matko Bošnjak,  Christos Kaplanis,  Alexey A. Gritsenko,  Matthias Minderer,  Charles Blundell,  Razvan Pascanu,  Jovana Mitrović</p>
  <p><b>备注</b>：26 pages</p>
  <p><b>关键词</b>：Fine-grained Contrastive Alignment, Contrastive Alignment, image-text pairs, introduce SPARse Fine-grained, image patches</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce SPARse Fine-grained Contrastive Alignment (SPARC), a simple method for pretraining more fine-grained multimodal representations from image-text pairs. Given that multiple image patches often correspond to single words, we propose to learn a grouping of image patches for every token in the caption. To achieve this, we use a sparse similarity metric between image patches and language tokens and compute for each token a language-grouped vision embedding as the weighted average of patches. The token and language-grouped vision embeddings are then contrasted through a fine-grained sequence-wise loss that only depends on individual samples and does not require other batch samples as negatives. This enables more detailed information to be learned in a computationally inexpensive manner. SPARC combines this fine-grained loss with a contrastive loss between global image and text embeddings to learn representations that simultaneously encode global and local information. We thoroughly evaluate our proposed method and show improved performance over competing approaches both on image-level tasks relying on coarse-grained information, e.g. classification, as well as region-level tasks relying on fine-grained information, e.g. retrieval, object detection, and segmentation. Moreover, SPARC improves model faithfulness and captioning in foundational vision-language models.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Evolutionary Multi-Objective Optimization of Large Language Model  Prompts for Balancing Sentiments</b></summary>
  <p><b>编号</b>：[140]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09862</p>
  <p><b>作者</b>：Jill Baumann,  Oliver Kramer</p>
  <p><b>备注</b>：Accepted in EvoApps at EvoStar 2024</p>
  <p><b>关键词</b>：attracted considerable attention, large language models, advent of large, large language, ChatGPT has attracted</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The advent of large language models (LLMs) such as ChatGPT has attracted considerable attention in various domains due to their remarkable performance and versatility. As the use of these models continues to grow, the importance of effective prompt engineering has come to the fore. Prompt optimization emerges as a crucial challenge, as it has a direct impact on model performance and the extraction of relevant information. Recently, evolutionary algorithms (EAs) have shown promise in addressing this issue, paving the way for novel optimization strategies. In this work, we propose a evolutionary multi-objective (EMO) approach specifically tailored for prompt optimization called EMO-Prompts, using sentiment analysis as a case study. We use sentiment analysis capabilities as our experimental targets. Our results demonstrate that EMO-Prompts effectively generates prompts capable of guiding the LLM to produce texts embodying two conflicting emotions simultaneously.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Temporal Insight Enhancement: Mitigating Temporal Hallucination in  Multimodal Large Language Models</b></summary>
  <p><b>编号</b>：[141]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09861</p>
  <p><b>作者</b>：Li Sun,  Liuan Wang,  Jun Sun,  Takayuki Okatani</p>
  <p><b>备注</b>：7 pages, 7 figures</p>
  <p><b>关键词</b>：Multimodal Large Language, Large Language Models, Multimodal Large, Large Language, advancements in Multimodal</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced the comprehension of multimedia content, bringing together diverse modalities such as text, images, and videos. However, a critical challenge faced by these models, especially when processing video inputs, is the occurrence of hallucinations - erroneous perceptions or interpretations, particularly at the event level. This study introduces an innovative method to address event-level hallucinations in MLLMs, focusing on specific temporal understanding in video content. Our approach leverages a novel framework that extracts and utilizes event-specific information from both the event query and the provided video to refine MLLMs' response. We propose a unique mechanism that decomposes on-demand event queries into iconic actions. Subsequently, we employ models like CLIP and BLIP2 to predict specific timestamps for event occurrences. Our evaluation, conducted using the Charades-STA dataset, demonstrates a significant reduction in temporal hallucinations and an improvement in the quality of event-related responses. This research not only provides a new perspective in addressing a critical limitation of MLLMs but also contributes a quantitatively measurable method for evaluating MLLMs in the context of temporal-related questions.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Enhancing the Fairness and Performance of Edge Cameras with Explainable  AI</b></summary>
  <p><b>编号</b>：[148]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09852</p>
  <p><b>作者</b>：Truong Thanh Hung Nguyen,  Vo Thanh Khang Nguyen,  Quoc Hung Cao,  Van Binh Truong,  Quoc Khanh Nguyen,  Hung Cao</p>
  <p><b>备注</b>：IEEE ICCE 2024</p>
  <p><b>关键词</b>：Artificial Intelligence, Edge camera systems, challenging to interpret, interpret and debug, human detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rising use of Artificial Intelligence (AI) in human detection on Edge camera systems has led to accurate but complex models, challenging to interpret and debug. Our research presents a diagnostic method using Explainable AI (XAI) for model debugging, with expert-driven problem identification and solution creation. Validated on the Bytetrack model in a real-world office Edge network, we found the training dataset as the main bias source and suggested model augmentation as a solution. Our approach helps identify model biases, essential for achieving fair and trustworthy models.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Behavioral Simulation: Exploring A Possible Next Paradigm for Science</b></summary>
  <p><b>编号</b>：[149]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09851</p>
  <p><b>作者</b>：Cheng Wang,  Chuwen Wang,  Yu Zhao,  Shirong Zeng,  Wang Zhang,  Ronghui Ning</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：weather forecasting, fluid mechanics, biological populations, Simulation technologies, widely utilized</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Simulation technologies have been widely utilized in many scientific research fields such as weather forecasting, fluid mechanics and biological populations. It is the best tool to handle problems in complex systems, where closed-form expressions are unavailable and the target distribution in the representation space is too complex to be fully represented by a deep learning (DL) model. We believe that the development of simulation technologies is consistent with scientific paradigms. This paper induces the evolution of scientific paradigms from the perspective of data, algorithms, and computational power. Building upon this perspective, we divide simulation technologies into three stages aligning with the emergence of new paradigms, and find that advanced simulation technologies are typical instances of paradigms integration. Moreover, we propose the concept of behavioral simulation (BS), specifically sophisticated behavioral simulation (SBS), representing a higher degree of paradigms integration based on foundation models to simulate complex social systems involving sophisticated human strategies and behaviors. BS and further SBS are designed to tackle challenges concerning the complex human system that surpasses the capacity of traditional agent-based modeling simulation (ABMS), which can be regarded as a possible next paradigm for science. Through this work, we look forward to more powerful BS and SBS applications in scientific research branches within social science.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：PPNet: A Novel Neural Network Structure for End-to-End Near-Optimal Path  Planning</b></summary>
  <p><b>编号</b>：[159]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09819</p>
  <p><b>作者</b>：Qinglong Meng,  Chongkun Xia,  Xueqian Wang,  Songping Mai,  Bin Liang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：classical path planners, sampling-based path planners, path planning, path planners, path</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The classical path planners, such as sampling-based path planners, have the limitations of sensitivity to the initial solution and slow convergence to the optimal solution. However, finding a near-optimal solution in a short period is challenging in many applications such as the autonomous vehicle with limited power/fuel. To achieve an end-to-end near-optimal path planner, we first divide the path planning problem into two subproblems, which are path's space segmentation and waypoints generation in the given path's space. We further propose a two-level cascade neural network named Path Planning Network (PPNet) to solve the path planning problem by solving the abovementioned subproblems. Moreover, we propose a novel efficient data generation method for path planning named EDaGe-PP. The results show the total computation time is less than 1/33 and the success rate of PPNet trained by the dataset that is generated by EDaGe-PP is about $2 \times$ compared to other methods. We validate PPNet against state-of-the-art path planning methods. The results show PPNet can find a near-optimal solution in 15.3ms, which is much shorter than the state-of-the-art path planners.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks</b></summary>
  <p><b>编号</b>：[164]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09798</p>
  <p><b>作者</b>：Kazuhiro Takemoto</p>
  <p><b>备注</b>：11 pages, 3 figures, 2 tables</p>
  <p><b>关键词</b>：Large Language Models, Large Language, produce ethically harmful, Language Models, ethically harmful prompts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) like ChatGPT face `jailbreak' challenges, where safeguards are bypassed to produce ethically harmful prompts. This study introduces a simple black-box method to effectively generate jailbreak prompts, overcoming the limitations of high complexity and computational costs associated with existing methods. The proposed technique iteratively rewrites harmful prompts into non-harmful expressions using the target LLM itself, based on the hypothesis that LLMs can directly sample safeguard-bypassing expressions. Demonstrated through experiments with ChatGPT (GPT-3.5 and GPT-4) and Gemini-Pro, this method achieved an attack success rate of over 80% within an average of 5 iterations and remained effective despite model updates. The jailbreak prompts generated were naturally-worded and concise, suggesting they are less detectable. The results indicate that creating effective jailbreak prompts is simpler than previously considered, and black-box jailbreak attacks pose a more serious security threat.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：A Comparative Analysis on Metaheuristic Algorithms Based Vision  Transformer Model for Early Detection of Alzheimer's Disease</b></summary>
  <p><b>编号</b>：[166]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09795</p>
  <p><b>作者</b>：Anuvab Sen,  Udayon Sen,  Subhabrata Roy</p>
  <p><b>备注</b>：2023 IEEE 15th International Conference on Computational Intelligence and Communication Networks (CICN). arXiv admin note: text overlap with arXiv:2309.16796</p>
  <p><b>关键词</b>：life threatening neuro-degenerative, threatening neuro-degenerative disorders, life threatening, quality of life, threatening neuro-degenerative</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A number of life threatening neuro-degenerative disorders had degraded the quality of life for the older generation in particular. Dementia is one such symptom which may lead to a severe condition called Alzheimer's disease if not detected at an early stage. It has been reported that the progression of such disease from a normal stage is due to the change in several parameters inside the human brain. In this paper, an innovative metaheuristic algorithms based ViT model has been proposed for the identification of dementia at different stage. A sizeable number of test data have been utilized for the validation of the proposed scheme. It has also been demonstrated that our model exhibits superior performance in terms of accuracy, precision, recall as well as F1-score.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：A Semantic Approach for Big Data Exploration in Industry 4.0</b></summary>
  <p><b>编号</b>：[169]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09789</p>
  <p><b>作者</b>：Idoia Berges,  Víctor Julio Ramírez-Durán,  Arantza Illarramendi</p>
  <p><b>备注</b>：Published version of paper: Idoia Berges, V\'ictor Julio Ram\'irez-Dur\'an, Arantza Illarramendi: A Semantic Approach for Big Data Exploration in Industry 4.0. Big Data Res. 25: 100222 (2021). DOI: 10.1016/j.bdr.2021.100222</p>
  <p><b>关键词</b>：Internet of Things, fourth industrial revolution, cloud computing technologies, Information Technology experts, trends in automation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The growing trends in automation, Internet of Things, big data and cloud computing technologies have led to the fourth industrial revolution (Industry 4.0), where it is possible to visualize and identify patterns and insights, which results in a better understanding of the data and can improve the manufacturing process. However, many times, the task of data exploration results difficult for manufacturing experts because they might be interested in analyzing also data that does not appear in pre-designed visualizations and therefore they must be assisted by Information Technology experts. In this paper, we present a proposal materialized in a semantic-based visual query system developed for a real Industry 4.0 scenario that allows domain experts to explore and visualize data in a friendly way. The main novelty of the system is the combined use that it makes of captured data that are semantically annotated first, and a 2D customized digital representation of a machine that is also linked with semantic descriptions. Those descriptions are expressed using terms of an ontology, where, among others, the sensors that are used to capture indicators about the performance of a machine that belongs to a Industry 4.0 scenario have been modeled. Moreover, this semantic description allows to: formulate queries at a higher level of abstraction, provide customized graphical visualizations of the results based on the format and nature of the data, and download enriched data enabling further types of analysis.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：Querying Easily Flip-flopped Samples for Deep Active Learning</b></summary>
  <p><b>编号</b>：[170]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09787</p>
  <p><b>作者</b>：Seong Jin Cho,  Gwangsu Kim,  Junghyun Lee,  Jinwoo Shin,  Chang D. Yoo</p>
  <p><b>备注</b>：34 pages, 17 figures, 5 tables. Accepted to the 12th International Conference on Learning Representations (ICLR 2024)</p>
  <p><b>关键词</b>：machine learning paradigm, paradigm that aims, aims to improve, strategically selecting, model predictive uncertainty</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Active learning is a machine learning paradigm that aims to improve the performance of a model by strategically selecting and querying unlabeled data. One effective selection strategy is to base it on the model's predictive uncertainty, which can be interpreted as a measure of how informative a sample is. The sample's distance to the decision boundary is a natural measure of predictive uncertainty, but it is often intractable to compute, especially for complex decision boundaries formed in multiclass classification tasks. To address this issue, this paper proposes the {\it least disagree metric} (LDM), defined as the smallest probability of disagreement of the predicted label, and an estimator for LDM proven to be asymptotically consistent under mild assumptions. The estimator is computationally efficient and can be easily implemented for deep learning models using parameter perturbation. The LDM-based active learning is performed by querying unlabeled data with the smallest LDM. Experimental results show that our LDM-based active learning algorithm obtains state-of-the-art overall performance on all considered datasets and deep architectures.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Adaptive Self-training Framework for Fine-grained Scene Graph Generation</b></summary>
  <p><b>编号</b>：[171]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09786</p>
  <p><b>作者</b>：Kibum Kim,  Kanghoon Yoon,  Yeonjun In,  Jinyoung Moon,  Donghyun Kim,  Chanyoung Park</p>
  <p><b>备注</b>：9 pages; ICLR 2024</p>
  <p><b>关键词</b>：missing annotation problems, Scene graph generation, SGG models, SGG, benchmark datasets</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Scene graph generation (SGG) models have suffered from inherent problems regarding the benchmark datasets such as the long-tailed predicate distribution and missing annotation problems. In this work, we aim to alleviate the long-tailed problem of SGG by utilizing unannotated triplets. To this end, we introduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels for unannotated triplets based on which the SGG models are trained. While there has been significant progress in self-training for image recognition, designing a self-training framework for the SGG task is more challenging due to its inherent nature such as the semantic ambiguity and the long-tailed distribution of predicate classes. Hence, we propose a novel pseudo-labeling technique for SGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is a model-agnostic framework that can be applied to any existing SGG models. Furthermore, we devise a graph structure learner (GSL) that is beneficial when adopting our proposed self-training framework to the state-of-the-art message-passing neural network (MPNN)-based SGG models. Our extensive experiments verify the effectiveness of ST-SGG on various SGG models, particularly in enhancing the performance on fine-grained predicate classes.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：SEINE: Structure Encoding and Interaction Network for Nuclei Instance  Segmentation</b></summary>
  <p><b>编号</b>：[176]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09773</p>
  <p><b>作者</b>：Ye Zhang,  Linghan Cai,  Ziyue Wang,  Yongbing Zhang</p>
  <p><b>备注</b>：10 pages, 12 figures, 6 tables, submitted to TMI</p>
  <p><b>关键词</b>：Nuclei instance segmentation, Nuclei, structure, segmentation in histopathological, histopathological images</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Nuclei instance segmentation in histopathological images is of great importance for biological analysis and cancer diagnosis but remains challenging for two reasons. (1) Similar visual presentation of intranuclear and extranuclear regions of chromophobe nuclei often causes under-segmentation, and (2) current methods lack the exploration of nuclei structure, resulting in fragmented instance predictions. To address these problems, this paper proposes a structure encoding and interaction network, termed SEINE, which develops the structure modeling scheme of nuclei and exploits the structure similarity between nuclei to improve the integrality of each segmented instance. Concretely, SEINE introduces a contour-based structure encoding (SE) that considers the correlation between nuclei structure and semantics, realizing a reasonable representation of the nuclei structure. Based on the encoding, we propose a structure-guided attention (SGA) that takes the clear nuclei as prototypes to enhance the structure learning for the fuzzy nuclei. To strengthen the structural learning ability, a semantic feature fusion (SFF) is presented to boost the semantic consistency of semantic and structure branches. Furthermore, a position enhancement (PE) method is applied to suppress incorrect nuclei boundary predictions. Extensive experiments demonstrate the superiority of our approaches, and SEINE achieves state-of-the-art (SOTA) performance on four datasets. The code is available at \href{this https URL}{this https URL}.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Towards Learning from Graphs with Heterophily: Progress and Future</b></summary>
  <p><b>编号</b>：[179]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09769</p>
  <p><b>作者</b>：Chenghua Gong,  Yao Cheng,  Xiang Li,  Caihua Shan,  Siqiang Luo,  Chuan Shi</p>
  <p><b>备注</b>：9 pages</p>
  <p><b>关键词</b>：models complex relations, real-world entities, structured data, complex relations, relations between real-world</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Graphs are structured data that models complex relations between real-world entities. Heterophilous graphs, where linked nodes are prone to be with different labels or dissimilar features, have recently attracted significant attention and found many applications. Meanwhile, increasing efforts have been made to advance learning from heterophilous graphs. Although there exist surveys on the relevant topic, they focus on heterophilous GNNs, which are only sub-topics of heterophilous graph learning. In this survey, we comprehensively overview existing works on learning from graphs with heterophily.First, we collect over 180 publications and introduce the development of this field. Then, we systematically categorize existing methods based on a hierarchical taxonomy including learning strategies, model architectures and practical applications. Finally, we discuss the primary challenges of existing studies and highlight promising avenues for future research.More publication details and corresponding open-source codes can be accessed and will be continuously updated at our repositories:this https URL.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：CLIP Model for Images to Textual Prompts Based on Top-k Neighbors</b></summary>
  <p><b>编号</b>：[182]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09763</p>
  <p><b>作者</b>：Xin Zhang,  Xin Zhang,  YeMing Cai,  Tianzhi Jia</p>
  <p><b>备注</b>：CLIP model, KNN, image-to-prompts</p>
  <p><b>关键词</b>：gained significant attention, recent years, subfield of multimodal, gained significant, significant attention</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Text-to-image synthesis, a subfield of multimodal generation, has gained significant attention in recent years. We propose a cost-effective approach for image-to-prompt generation that leverages generative models to generate textual prompts without the need for large amounts of annotated data. We divide our method into two stages: online stage and offline stage. We use a combination of the CLIP model and K-nearest neighbors (KNN) algorithm. The proposed system consists of two main parts: an offline task and an online task. Our method owns the highest metric 0.612 among these models, which is 0.013, 0.055, 0.011 higher than Clip, Clip + KNN(top 10) respectively.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Cooperative Tri-Point Model-Based Ground-to-Air Coverage Extension in  Beyond 5G Networks</b></summary>
  <p><b>编号</b>：[186]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09757</p>
  <p><b>作者</b>：Ziwei Cai,  Min Sheng,  Junju Liu,  Chenxi Zhao,  Jiandong Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：potentially low-cost solution, coverage, existing terrestrial infrastructures, low-cost solution, infrastructures to provide</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The utilization of existing terrestrial infrastructures to provide coverage for aerial users is a potentially low-cost solution. However, the already deployed terrestrial base stations (TBSs) result in weak ground-to-air (G2A) coverage due to the down-tilted antennas. Furthermore, achieving optimal coverage across the entire airspace through antenna adjustment is challenging due to the complex signal coverage requirements in three-dimensional space, especially in the vertical direction. In this paper, we propose a cooperative tri-point (CoTP) model-based method that utilizes cooperative beams to enhance the G2A coverage extension. To utilize existing TBSs for establishing effective cooperation, we prove that the cooperation among three TBSs can ensure G2A coverage with a minimum coverage overlap, and design the CoTP model to analyze the G2A coverage extension. Using the model, a cooperative coverage structure based on Delaunay triangulation is designed to divide triangular prism-shaped subspaces and corresponding TBS cooperation sets. To enable TBSs in the cooperation set to cover different height subspaces while maintaining ground coverage, we design a cooperative beam generation algorithm to maximize the coverage in the triangular prism-shaped airspace. The simulation results and field trials demonstrate that the proposed method can efficiently enhance the G2A coverage extension while guaranteeing ground coverage.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Explaining Drift using Shapley Values</b></summary>
  <p><b>编号</b>：[187]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09756</p>
  <p><b>作者</b>：Narayanan U. Edakunni,  Utkarsh Tekriwal,  Anukriti Jain</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：predict the outcomes, Machine learning models, Machine learning, machine learning research, drift</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine learning models often deteriorate in their performance when they are used to predict the outcomes over data on which they were not trained. These scenarios can often arise in real world when the distribution of data changes gradually or abruptly due to major events like a pandemic. There have been many attempts in machine learning research to come up with techniques that are resilient to such Concept drifts. However, there is no principled framework to identify the drivers behind the drift in model performance. In this paper, we propose a novel framework - DBShap that uses Shapley values to identify the main contributors of the drift and quantify their respective contributions. The proposed framework not only quantifies the importance of individual features in driving the drift but also includes the change in the underlying relation between the input and output as a possible driver. The explanation provided by DBShap can be used to understand the root cause behind the drift and use it to make the model resilient to the drift.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Bootstrapping OTS-Funcimg Pre-training Model (Botfip) -- A Comprehensive  Symbolic Regression Framework</b></summary>
  <p><b>编号</b>：[192]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09748</p>
  <p><b>作者</b>：Tianhao Chen,  Pengbo Xu,  Haibiao Zheng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：problem-solving approaches tend, deep multimodal information, multimodal information mining, Operation Tree Sequence, multimodal framework akin</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the field of scientific computing, many problem-solving approaches tend to focus only on the process and final outcome, even in AI for science, there is a lack of deep multimodal information mining behind the data, missing a multimodal framework akin to that in the image-text domain. In this paper, we take Symbolic Regression(SR) as our focal point and, drawing inspiration from the BLIP model in the image-text domain, propose a scientific computing multimodal framework based on Function Images (Funcimg) and Operation Tree Sequence (OTS), named Bootstrapping OTS-Funcimg Pre-training Model (Botfip). In SR experiments, we validate the advantages of Botfip in low-complexity SR problems, showcasing its potential. As a MED framework, Botfip holds promise for future applications in a broader range of scientific computing problems.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：HCVP: Leveraging Hierarchical Contrastive Visual Prompt for Domain  Generalization</b></summary>
  <p><b>编号</b>：[205]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09716</p>
  <p><b>作者</b>：Guanglin Zhou,  Zhongyi Han,  Shiming Chen,  Biwei Huang,  Liming Zhu,  Tongliang Liu,  Lina Yao,  Kun Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：create machine learning, invariant features, endeavors to create, machine learning models, create machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Domain Generalization (DG) endeavors to create machine learning models that excel in unseen scenarios by learning invariant features. In DG, the prevalent practice of constraining models to a fixed structure or uniform parameterization to encapsulate invariant features can inadvertently blend specific aspects. Such an approach struggles with nuanced differentiation of inter-domain variations and may exhibit bias towards certain domains, hindering the precise learning of domain-invariant features. Recognizing this, we introduce a novel method designed to supplement the model with domain-level and task-specific characteristics. This approach aims to guide the model in more effectively separating invariant features from specific characteristics, thereby boosting the generalization. Building on the emerging trend of visual prompts in the DG paradigm, our work introduces the novel \textbf{H}ierarchical \textbf{C}ontrastive \textbf{V}isual \textbf{P}rompt (HCVP) methodology. This represents a significant advancement in the field, setting itself apart with a unique generative approach to prompts, alongside an explicit model structure and specialized loss functions. Differing from traditional visual prompts that are often shared across entire datasets, HCVP utilizes a hierarchical prompt generation network enhanced by prompt contrastive learning. These generative prompts are instance-dependent, catering to the unique characteristics inherent to different domains and tasks. Additionally, we devise a prompt modulation network that serves as a bridge, effectively incorporating the generated visual prompts into the vision transformer backbone. Experiments conducted on five DG datasets demonstrate the effectiveness of HCVP, outperforming both established DG algorithms and adaptation protocols.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Curriculum Recommendations Using Transformer Base Model with InfoNCE  Loss And Language Switching Method</b></summary>
  <p><b>编号</b>：[214]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09699</p>
  <p><b>作者</b>：Xiaonan Xu,  Bin Yuan,  Yongyao Mo,  Tianbo Song,  Shulin Li</p>
  <p><b>备注</b>：4pages, 2 figures, ICAICA2023</p>
  <p><b>关键词</b>：fostering learning equality, dedicated to fostering, ever-evolving realms, Curriculum Recommendations paradigm, language translation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The Curriculum Recommendations paradigm is dedicated to fostering learning equality within the ever-evolving realms of educational technology and curriculum development. In acknowledging the inherent obstacles posed by existing methodologies, such as content conflicts and disruptions from language translation, this paradigm aims to confront and overcome these challenges. Notably, it addresses content conflicts and disruptions introduced by language translation, hindrances that can impede the creation of an all-encompassing and personalized learning experience. The paradigm's objective is to cultivate an educational environment that not only embraces diversity but also customizes learning experiences to suit the distinct needs of each learner. To overcome these challenges, our approach builds upon notable contributions in curriculum development and personalized learning, introducing three key innovations. These include the integration of Transformer Base Model to enhance computational efficiency, the implementation of InfoNCE Loss for accurate content-topic matching, and the adoption of a language switching strategy to alleviate translation-related ambiguities. Together, these innovations aim to collectively tackle inherent challenges and contribute to forging a more equitable and effective learning journey for a diverse range of learners. Competitive cross-validation scores underscore the efficacy of sentence-transformers/LaBSE, achieving 0.66314, showcasing our methodology's effectiveness in diverse linguistic nuances for content alignment prediction. Index Terms-Curriculum Recommendation, Transformer model with InfoNCE Loss, Language Switching.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Should ChatGPT Write Your Breakup Text? Exploring the Role of AI in  Relationship Dissolution</b></summary>
  <p><b>编号</b>：[215]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09695</p>
  <p><b>作者</b>：Yue Fu,  Yixin Chen,  Zelia Gomes Da Costa Lai,  Alexis Hiniker</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：happiness and wellbeing, breakup process, breakup, process, relationship</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Relationships are essential to our happiness and wellbeing. The dissolution of a relationship, the final stage of relationship's lifecycle and one of the most stressful events in an individual's life, can have profound and long-lasting impacts on people. With the breakup process increasingly facilitated by computer-mediated communication (CMC), and the likely future influence of AI-mediated communication (AIMC) tools, we conducted a semi-structured interview study with 21 participants. We aim to understand: 1) the current role of technology in the breakup process, 2) the needs and support individuals have during the process, and 3) how AI might address these needs. Our research shows that people have distinct needs at various stages of ending a relationship. Presently, technology is used for information gathering and community support, acting as a catalyst for breakups, enabling ghosting and blocking, and facilitating communication. Participants anticipate that AI could aid in sense-making of their relationship leading up to the breakup, act as a mediator, assist in crafting appropriate wording, tones, and language during breakup conversations, and support companionship, reflection, recovery, and growth after a breakup. Our findings also demonstrate an overlap between the breakup process and the Transtheoretical Model (TTM) of behavior change. Through the lens of TTM, we explore the potential support and affordances AI could offer in breakups, including its benefits and the necessary precautions regarding AI's role in this sensitive process.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Imitation Learning Inputting Image Feature to Each Layer of Neural  Network</b></summary>
  <p><b>编号</b>：[218]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09691</p>
  <p><b>作者</b>：Koki Yamane,  Sho Sakaino,  Toshiaki Tsuji</p>
  <p><b>备注</b>：IEEE The 18th International Workshop on Advanced Motion Control (AMC2024)</p>
  <p><b>关键词</b>：replicate human behavior, Imitation learning enables, learning enables robots, robots to learn, learn and replicate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Imitation learning enables robots to learn and replicate human behavior from training data. Recent advances in machine learning enable end-to-end learning approaches that directly process high-dimensional observation data, such as images. However, these approaches face a critical challenge when processing data from multiple modalities, inadvertently ignoring data with a lower correlation to the desired output, especially when using short sampling periods. This paper presents a useful method to address this challenge, which amplifies the influence of data with a relatively low correlation to the output by inputting the data into each neural network layer. The proposed approach effectively incorporates diverse data sources into the learning process. Through experiments using a simple pick-and-place operation with raw images and joint information as input, significant improvements in success rates are demonstrated even when dealing with data from short sampling periods.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Tiny Multi-Agent DRL for Twins Migration in UAV Metaverses: A  Multi-Leader Multi-Follower Stackelberg Game Approach</b></summary>
  <p><b>编号</b>：[221]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09680</p>
  <p><b>作者</b>：Jiawen Kang,  Yue Zhong,  Minrui Xu,  Jiangtian Nie,  Jinbo Wen,  Hongyang Du,  Dongdong Ye,  Xumin Huang,  Dusit Niyato,  Shengli Xie</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Unmanned Aerial Vehicles, Aerial Vehicles, Unmanned Aerial, transforming drone interaction, emerging paradigm named</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The synergy between Unmanned Aerial Vehicles (UAVs) and metaverses is giving rise to an emerging paradigm named UAV metaverses, which create a unified ecosystem that blends physical and virtual spaces, transforming drone interaction and virtual exploration. UAV Twins (UTs), as the digital twins of UAVs that revolutionize UAV applications by making them more immersive, realistic, and informative, are deployed and updated on ground base stations, e.g., RoadSide Units (RSUs), to offer metaverse services for UAV Metaverse Users (UMUs). Due to the dynamic mobility of UAVs and limited communication coverages of RSUs, it is essential to perform real-time UT migration to ensure seamless immersive experiences for UMUs. However, selecting appropriate RSUs and optimizing the required bandwidth is challenging for achieving reliable and efficient UT migration. To address the challenges, we propose a tiny machine learning-based Stackelberg game framework based on pruning techniques for efficient UT migration in UAV metaverses. Specifically, we formulate a multi-leader multi-follower Stackelberg model considering a new immersion metric of UMUs in the utilities of UAVs. Then, we design a Tiny Multi-Agent Deep Reinforcement Learning (Tiny MADRL) algorithm to obtain the tiny networks representing the optimal game solution. Specifically, the actor-critic network leverages the pruning techniques to reduce the number of network parameters and achieve model size and computation reduction, allowing for efficient implementation of Tiny MADRL. Numerical results demonstrate that our proposed schemes have better performance than traditional schemes.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Towards Identifiable Unsupervised Domain Translation: A Diversified  Distribution Matching Approach</b></summary>
  <p><b>编号</b>：[226]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09671</p>
  <p><b>作者</b>：Sagar Shrestha,  Xiao Fu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：high-level semantic meaning, Unsupervised domain translation, aims to find, semantic meaning, convert samples</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Unsupervised domain translation (UDT) aims to find functions that convert samples from one domain (e.g., sketches) to another domain (e.g., photos) without changing the high-level semantic meaning (also referred to as ``content''). The translation functions are often sought by probability distribution matching of the transformed source domain and target domain. CycleGAN stands as arguably the most representative approach among this line of work. However, it was noticed in the literature that CycleGAN and variants could fail to identify the desired translation functions and produce content-misaligned translations. This limitation arises due to the presence of multiple translation functions -- referred to as ``measure-preserving automorphism" (MPA) -- in the solution space of the learning criteria. Despite awareness of such identifiability issues, solutions have remained elusive. This study delves into the core identifiability inquiry and introduces an MPA elimination theory. Our analysis shows that MPA is unlikely to exist, if multiple pairs of diverse cross-domain conditional distributions are matched by the learning function. Our theory leads to a UDT learner using distribution matching over auxiliary variable-induced subsets of the domains -- other than over the entire data domains as in the classical approaches. The proposed framework is the first to rigorously establish translation identifiability under reasonable UDT settings, to our best knowledge. Experiments corroborate with our theoretical claims.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Traffic Smoothing Controllers for Autonomous Vehicles Using Deep  Reinforcement Learning and Real-World Trajectory Data</b></summary>
  <p><b>编号</b>：[228]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09666</p>
  <p><b>作者</b>：Nathan Lichtlé,  Kathy Jang,  Adit Shah,  Eugene Vinitsky,  Jonathan W. Lee,  Alexandre M. Bayen</p>
  <p><b>备注</b>：Accepted to be published as part of the 26th IEEE International Conference on Intelligent Transportation Systems (ITSC) 2023, Bilbao, Spain, September 24-28, 2023</p>
  <p><b>关键词</b>：Designing traffic-smoothing cruise, improving traffic flow, mixed autonomy traffic, enhancing fuel efficiency, traffic-smoothing cruise controllers</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Designing traffic-smoothing cruise controllers that can be deployed onto autonomous vehicles is a key step towards improving traffic flow, reducing congestion, and enhancing fuel efficiency in mixed autonomy traffic. We bypass the common issue of having to carefully fine-tune a large traffic microsimulator by leveraging real-world trajectory data from the I-24 highway in Tennessee, replayed in a one-lane simulation. Using standard deep reinforcement learning methods, we train energy-reducing wave-smoothing policies. As an input to the agent, we observe the speed and distance of only the vehicle in front, which are local states readily available on most recent vehicles, as well as non-local observations about the downstream state of the traffic. We show that at a low 4% autonomous vehicle penetration rate, we achieve significant fuel savings of over 15% on trajectories exhibiting many stop-and-go waves. Finally, we analyze the smoothing effect of the controllers and demonstrate robustness to adding lane-changing into the simulation as well as the removal of downstream information.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Mobility Accelerates Learning: Convergence Analysis on Hierarchical  Federated Learning in Vehicular Networks</b></summary>
  <p><b>编号</b>：[230]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09656</p>
  <p><b>作者</b>：Tan Chen,  Jintao Yan,  Yuxuan Sun,  Sheng Zhou,  Deniz Gündüz,  Zhisheng Niu</p>
  <p><b>备注</b>：Submitted to IEEE for possible publication</p>
  <p><b>关键词</b>：Hierarchical federated learning, cloud edge server, enables distributed training, Hierarchical federated, federated learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Hierarchical federated learning (HFL) enables distributed training of models across multiple devices with the help of several edge servers and a cloud edge server in a privacy-preserving manner. In this paper, we consider HFL with highly mobile devices, mainly targeting at vehicular networks. Through convergence analysis, we show that mobility influences the convergence speed by both fusing the edge data and shuffling the edge models. While mobility is usually considered as a challenge from the perspective of communication, we prove that it increases the convergence speed of HFL with edge-level heterogeneous data, since more diverse data can be incorporated. Furthermore, we demonstrate that a higher speed leads to faster convergence, since it accelerates the fusion of data. Simulation results show that mobility increases the model accuracy of HFL by up to 15.1% when training a convolutional neural network on the CIFAR-10 dataset.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Convex and Bilevel Optimization for Neuro-Symbolic Inference and  Learning</b></summary>
  <p><b>编号</b>：[232]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09651</p>
  <p><b>作者</b>：Charles Dickens,  Changyu Gao,  Connor Pryor,  Stephen Wright,  Lise Getoor</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：bilevel optimization techniques, general gradient-based framework, symbolic parameter learning, challenge for neuro-symbolic, systems by leveraging</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We address a key challenge for neuro-symbolic (NeSy) systems by leveraging convex and bilevel optimization techniques to develop a general gradient-based framework for end-to-end neural and symbolic parameter learning. The applicability of our framework is demonstrated with NeuPSL, a state-of-the-art NeSy architecture. To achieve this, we propose a smooth primal and dual formulation of NeuPSL inference and show learning gradients are functions of the optimal dual variables. Additionally, we develop a dual block coordinate descent algorithm for the new formulation that naturally exploits warm-starts. This leads to over 100x learning runtime improvements over the current best NeuPSL inference method. Finally, we provide extensive empirical evaluations across $8$ datasets covering a range of tasks and demonstrate our learning framework achieves up to a 16% point prediction performance improvement over alternative learning methods.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on  Climate Change</b></summary>
  <p><b>编号</b>：[234]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09646</p>
  <p><b>作者</b>：David Thulke,  Yingbo Gao,  Petrus Pelser,  Rein Brune,  Rricha Jalota,  Floris Fok,  Michael Ramos,  Ian van Wyk,  Abdallah Nasir,  Hayden Goldstein,  Taylor Tragemann,  Katie Nguyen,  Ariana Fowler,  Andrew Stanco,  Jon Gabriel,  Jordan Taylor,  Dean Moro,  Evgenii Tsymbalov,  Juliette de Waal,  Evgeny Matusov,  Mudar Yaghi,  Mohammad Shihadah,  Hermann Ney,  Christian Dugast,  Jonathan Dotan,  Daniel Erasmus</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：paper introduces ClimateGPT, introduces ClimateGPT, paper introduces, model, domain-specific</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper introduces ClimateGPT, a model family of domain-specific large language models that synthesize interdisciplinary research on climate change. We trained two 7B models from scratch on a science-oriented dataset of 300B tokens. For the first model, the 4.2B domain-specific tokens were included during pre-training and the second was adapted to the climate domain after pre-training. Additionally, ClimateGPT-7B, 13B and 70B are continuously pre-trained from Llama~2 on a domain-specific dataset of 4.2B tokens. Each model is instruction fine-tuned on a high-quality and human-generated domain-specific dataset that has been created in close cooperation with climate scientists. To reduce the number of hallucinations, we optimize the model for retrieval augmentation and propose a hierarchical retrieval strategy. To increase the accessibility of our model to non-English speakers, we propose to make use of cascaded machine translation and show that this approach can perform comparably to natively multilingual models while being easier to scale to a large number of languages. Further, to address the intrinsic interdisciplinary aspect of climate change we consider different research perspectives. Therefore, the model can produce in-depth answers focusing on different perspectives in addition to an overall answer. We propose a suite of automatic climate-specific benchmarks to evaluate LLMs. On these benchmarks, ClimateGPT-7B performs on par with the ten times larger Llama-2-70B Chat model while not degrading results on general domain benchmarks. Our human evaluation confirms the trends we saw in our benchmarks. All models were trained and evaluated using renewable energy and are released publicly.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Blackout Mitigation via Physics-guided RL</b></summary>
  <p><b>编号</b>：[236]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09640</p>
  <p><b>作者</b>：Anmol Dwivedi,  Santiago Paternain,  Ali Tajer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：remedial control actions, sequential design, ultimate objective, objective of preventing, system anomalies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper considers the sequential design of remedial control actions in response to system anomalies for the ultimate objective of preventing blackouts. A physics-guided reinforcement learning (RL) framework is designed to identify effective sequences of real-time remedial look-ahead decisions accounting for the long-term impact on the system's stability. The paper considers a space of control actions that involve both discrete-valued transmission line-switching decisions (line reconnections and removals) and continuous-valued generator adjustments. To identify an effective blackout mitigation policy, a physics-guided approach is designed that uses power-flow sensitivity factors associated with the power transmission network to guide the RL exploration during agent training. Comprehensive empirical evaluations using the open-source Grid2Op platform demonstrate the notable advantages of incorporating physical signals into RL decisions, establishing the gains of the proposed physics-guided approach compared to its black box counterparts. One important observation is that strategically~\emph{removing} transmission lines, in conjunction with multiple real-time generator adjustments, often renders effective long-term decisions that are likely to prevent or delay blackouts.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Impact of Large Language Model Assistance on Patients Reading Clinical  Notes: A Mixed-Methods Study</b></summary>
  <p><b>编号</b>：[237]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09637</p>
  <p><b>作者</b>：Niklas Mannhardt,  Elizabeth Bondi-Kelly,  Barbara Lam,  Chloe O'Connell,  Mercy Asiedu,  Hussein Mozannar,  Monica Agrawal,  Alejandro Buendia,  Tatiana Urman,  Irbaz B. Riaz,  Catherine E. Ricciardi,  Marzyeh Ghassemi,  David Sontag</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：derive numerous benefits, clinical notes, Patients derive numerous, notes, including an increased</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Patients derive numerous benefits from reading their clinical notes, including an increased sense of control over their health and improved understanding of their care plan. However, complex medical concepts and jargon within clinical notes hinder patient comprehension and may lead to anxiety. We developed a patient-facing tool to make clinical notes more readable, leveraging large language models (LLMs) to simplify, extract information from, and add context to notes. We prompt engineered GPT-4 to perform these augmentation tasks on real clinical notes donated by breast cancer survivors and synthetic notes generated by a clinician, a total of 12 notes with 3868 words. In June 2023, 200 female-identifying US-based participants were randomly assigned three clinical notes with varying levels of augmentations using our tool. Participants answered questions about each note, evaluating their understanding of follow-up actions and self-reported confidence. We found that augmentations were associated with a significant increase in action understanding score (0.63 $\pm$ 0.04 for select augmentations, compared to 0.54 $\pm$ 0.02 for the control) with p=0.002. In-depth interviews of self-identifying breast cancer patients (N=7) were also conducted via video conferencing. Augmentations, especially definitions, elicited positive responses among the seven participants, with some concerns about relying on LLMs. Augmentations were evaluated for errors by clinicians, and we found misleading errors occur, with errors more common in real donated notes than synthetic notes, illustrating the importance of carefully written clinical notes. Augmentations improve some but not all readability metrics. This work demonstrates the potential of LLMs to improve patients' experience with clinical notes at a lower burden to clinicians. However, having a human in the loop is important to correct potential model errors.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Learning Shortcuts: On the Misleading Promise of NLU in Language Models</b></summary>
  <p><b>编号</b>：[244]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09615</p>
  <p><b>作者</b>：Geetanjali Bihani,  Julia Taylor Rayz</p>
  <p><b>备注</b>：Accepted at HICSS-SDPS 2024</p>
  <p><b>关键词</b>：significant performance gains, enabled significant performance, natural language processing, large language models, advent of large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The advent of large language models (LLMs) has enabled significant performance gains in the field of natural language processing. However, recent studies have found that LLMs often resort to shortcuts when performing tasks, creating an illusion of enhanced performance while lacking generalizability in their decision rules. This phenomenon introduces challenges in accurately assessing natural language understanding in LLMs. Our paper provides a concise survey of relevant research in this area and puts forth a perspective on the implications of shortcut learning in the evaluation of language models, specifically for NLU tasks. This paper urges more research efforts to be put towards deepening our comprehension of shortcut learning, contributing to the development of more robust language models, and raising the standards of NLU evaluation in real-world scenarios.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Handling Large-scale Cardinality in building recommendation systems</b></summary>
  <p><b>编号</b>：[259]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09572</p>
  <p><b>作者</b>：Dhruva Dixith Kurra,  Bo Ling,  Chun Zh,  Seyedshahin Ashrafzadeh</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：capturing user preferences, universally unique identifiers, requiring incorporating numerous, incorporating numerous features, user preferences</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Effective recommendation systems rely on capturing user preferences, often requiring incorporating numerous features such as universally unique identifiers (UUIDs) of entities. However, the exceptionally high cardinality of UUIDs poses a significant challenge in terms of model degradation and increased model size due to sparsity. This paper presents two innovative techniques to address the challenge of high cardinality in recommendation systems. Specifically, we propose a bag-of-words approach, combined with layer sharing, to substantially decrease the model size while improving performance. Our techniques were evaluated through offline and online experiments on Uber use cases, resulting in promising results demonstrating our approach's effectiveness in optimizing recommendation systems and enhancing their overall performance.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：Aligning Large Language Models with Counterfactual DPO</b></summary>
  <p><b>编号</b>：[260]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09566</p>
  <p><b>作者</b>：Bradley Butcher</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, demonstrated remarkable capabilities, Advancements in large, range of applications, large language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Advancements in large language models (LLMs) have demonstrated remarkable capabilities across a diverse range of applications. These models excel in generating text completions that are contextually coherent and cover an extensive array of subjects. However, the vast datasets required for their training make aligning response styles during the pretraining and instruction tuning phases challenging. Consequently, an additional alignment phase is typically employed, wherein the model is further trained with human preference data to better align its outputs with human expectations. While this process doesn't introduce new capabilities per se, it does accentuate generation styles innate to the model. This paper explores the utilization of counterfactual prompting within the framework of Direct Preference Optimization (DPO) to align the model's style without relying on human intervention. We demonstrate that this method effectively instils desirable behaviour, mitigates undesirable ones, and encourages the model to disregard inappropriate instructions. Our findings suggest that counterfactual prompting with DPO presents a low-resource way to fine-tune LLMs to meet the demands for responsible and ethically aligned AI systems.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：Improving Classification Performance With Human Feedback: Label a few,  we label the rest</b></summary>
  <p><b>编号</b>：[262]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09555</p>
  <p><b>作者</b>：Natan Vidra,  Thomas Clifford,  Katherine Jijo,  Eden Chung,  Liang Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：obtaining substantial amounts, train supervised machine, supervised machine learning, machine learning models, learning models poses</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the realm of artificial intelligence, where a vast majority of data is unstructured, obtaining substantial amounts of labeled data to train supervised machine learning models poses a significant challenge. To address this, we delve into few-shot and active learning, where are goal is to improve AI models with human feedback on a few labeled examples. This paper focuses on understanding how a continuous feedback loop can refine models, thereby enhancing their accuracy, recall, and precision through incremental human input. By employing Large Language Models (LLMs) such as GPT-3.5, BERT, and SetFit, we aim to analyze the efficacy of using a limited number of labeled examples to substantially improve model accuracy. We benchmark this approach on the Financial Phrasebank, Banking, Craigslist, Trec, Amazon Reviews datasets to prove that with just a few labeled examples, we are able to surpass the accuracy of zero shot large language models to provide enhanced text classification performance. We demonstrate that rather than needing to manually label millions of rows of data, we just need to label a few and the model can effectively predict the rest.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：BERTologyNavigator: Advanced Question Answering with BERT-based  Semantics</b></summary>
  <p><b>编号</b>：[263]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09553</p>
  <p><b>作者</b>：Shreya Rajpal (1,2),  Ricardo Usbeck (1) ((1) Universität Hamburg, Hamburg, Germany,(2) Vellore Institute of Technology, Vellore, Tamil Nadu, India)</p>
  <p><b>备注</b>：Accepted in Scholarly QALD Challenge @ ISWC 2023</p>
  <p><b>关键词</b>：natural language processing, DBLP Knowledge Graph, language processing, language models, natural language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The development and integration of knowledge graphs and language models has significance in artificial intelligence and natural language processing. In this study, we introduce the BERTologyNavigator -- a two-phased system that combines relation extraction techniques and BERT embeddings to navigate the relationships within the DBLP Knowledge Graph (KG). Our approach focuses on extracting one-hop relations and labelled candidate pairs in the first phases. This is followed by employing BERT's CLS embeddings and additional heuristics for relation selection in the second phase. Our system reaches an F1 score of 0.2175 on the DBLP QuAD Final test dataset for Scholarly QALD and 0.98 F1 score on the subset of the DBLP QuAD test dataset during the QA phase.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：Accelerating Data Generation for Neural Operators via Krylov Subspace  Recycling</b></summary>
  <p><b>编号</b>：[269]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09516</p>
  <p><b>作者</b>：Hong Wang,  Zhongkai Hao,  Jie Wang,  Zijie Geng,  Zhen Wang,  Bin Li,  Feng Wu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：attracted great attention, great attention due, partial differential equations, solving partial differential, partial differential</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Learning neural operators for solving partial differential equations (PDEs) has attracted great attention due to its high inference efficiency. However, training such operators requires generating a substantial amount of labeled data, i.e., PDE problems together with their solutions. The data generation process is exceptionally time-consuming, as it involves solving numerous systems of linear equations to obtain numerical solutions to the PDEs. Many existing methods solve these systems independently without considering their inherent similarities, resulting in extremely redundant computations. To tackle this problem, we propose a novel method, namely Sorting Krylov Recycling (SKR), to boost the efficiency of solving these systems, thus significantly accelerating data generation for neural operators training. To the best of our knowledge, SKR is the first attempt to address the time-consuming nature of data generation for learning neural operators. The working horse of SKR is Krylov subspace recycling, a powerful technique for solving a series of interrelated systems by leveraging their inherent similarities. Specifically, SKR employs a sorting algorithm to arrange these systems in a sequence, where adjacent systems exhibit high similarities. Then it equips a solver with Krylov subspace recycling to solve the systems sequentially instead of independently, thus effectively enhancing the solving efficiency. Both theoretical analysis and extensive experiments demonstrate that SKR can significantly accelerate neural operator data generation, achieving a remarkable speedup of up to 13.9 times.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：Technical Report: On the Convergence of Gossip Learning in the Presence  of Node Inaccessibility</b></summary>
  <p><b>编号</b>：[276]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09498</p>
  <p><b>作者</b>：Tian Liu,  Yue Cui,  Xueyang Hu,  Yecheng Xu,  Bo Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：unmanned aerial vehicles, resource-constrained wireless networks, Gossip learning, federated learning, aerial vehicles</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Gossip learning (GL), as a decentralized alternative to federated learning (FL), is more suitable for resource-constrained wireless networks, such as FANETs that are formed by unmanned aerial vehicles (UAVs). GL can significantly enhance the efficiency and extend the battery life of UAV networks. Despite the advantages, the performance of GL is strongly affected by data distribution, communication speed, and network connectivity. However, how these factors influence the GL convergence is still unclear. Existing work studied the convergence of GL based on a virtual quantity for the sake of convenience, which fail to reflect the real state of the network when some nodes are inaccessible. In this paper, we formulate and investigate the impact of inaccessible nodes to GL under a dynamic network topology. We first decompose the weight divergence by whether the node is accessible or not. Then, we investigate the GL convergence under the dynamic of node accessibility and theoretically provide how the number of inaccessible nodes, data non-i.i.d.-ness, and duration of inaccessibility affect the convergence. Extensive experiments are carried out in practical settings to comprehensively verify the correctness of our theoretical findings.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：Memory, Space, and Planning: Multiscale Predictive Representations</b></summary>
  <p><b>编号</b>：[281]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09491</p>
  <p><b>作者</b>：Ida Momennejad</p>
  <p><b>备注</b>：To be published as a chapter in an edited volume by Oxford University Press (Editors: Sara Aronowitz and Lynn Nadel)</p>
  <p><b>关键词</b>：inherently entangled, memory structures, prediction and planning, predictive memory structures, Memory</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Memory is inherently entangled with prediction and planning. Flexible behavior in biological and artificial agents depends on the interplay of learning from the past and predicting the future in ever-changing environments. This chapter reviews computational, behavioral, and neural evidence suggesting these processes rely on learning the relational structure of experiences, known as cognitive maps, and draws two key takeaways. First, that these memory structures are organized as multiscale, compact predictive representations in hippocampal and prefrontal cortex, or PFC, hierarchies. Second, we argue that such predictive memory structures are crucial to the complementary functions of the hippocampus and PFC, both for enabling a recall of detailed and coherent past episodes as well as generalizing experiences at varying scales for efficient prediction and planning. These insights advance our understanding of memory and planning mechanisms in the brain and hold significant implications for advancing artificial intelligence systems.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：PUPAE: Intuitive and Actionable Explanations for Time Series Anomalies</b></summary>
  <p><b>编号</b>：[282]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09489</p>
  <p><b>作者</b>：Audrey Der,  Chin-Chia Michael Yeh,  Yan Zheng,  Junpeng Wang,  Zhongfang Zhuang,  Liang Wang,  Wei Zhang,  Eamonn J. Keogh</p>
  <p><b>备注</b>：9 Page Manuscript, 1 Page Supplementary (Supplement not published in conference proceedings.)</p>
  <p><b>关键词</b>：recent years, significant progress, series anomaly detection, anomaly, anomaly detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years there has been significant progress in time series anomaly detection. However, after detecting an (perhaps tentative) anomaly, can we explain it? Such explanations would be useful to triage anomalies. For example, in an oil refinery, should we respond to an anomaly by dispatching a hydraulic engineer, or an intern to replace the battery on a sensor? There have been some parallel efforts to explain anomalies, however many proposed techniques produce explanations that are indirect, and often seem more complex than the anomaly they seek to explain. Our review of the literature/checklists/user-manuals used by frontline practitioners in various domains reveals an interesting near-universal commonality. Most practitioners discuss, explain and report anomalies in the following format: The anomaly would be like normal data A, if not for the corruption B. The reader will appreciate that is a type of counterfactual explanation. In this work we introduce a domain agnostic counterfactual explanation technique to produce explanations for time series anomalies. As we will show, our method can produce both visual and text-based explanations that are objectively correct, intuitive and in many circumstances, directly actionable.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：Uncertainty-Aware Hardware Trojan Detection Using Multimodal Deep  Learning</b></summary>
  <p><b>编号</b>：[285]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09479</p>
  <p><b>作者</b>：Rahul Vishwakarma,  Amin Rezaei</p>
  <p><b>备注</b>：2024 Design, Automation and Test in Europe Conference | The European Event for Electronic System Design & Test (accepted)</p>
  <p><b>关键词</b>：zero-trust fabless era, hardware Trojans, hardware Trojan detection, fabless era, stages of chip</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The risk of hardware Trojans being inserted at various stages of chip production has increased in a zero-trust fabless era. To counter this, various machine learning solutions have been developed for the detection of hardware Trojans. While most of the focus has been on either a statistical or deep learning approach, the limited number of Trojan-infected benchmarks affects the detection accuracy and restricts the possibility of detecting zero-day Trojans. To close the gap, we first employ generative adversarial networks to amplify our data in two alternative representation modalities, a graph and a tabular, ensuring that the dataset is distributed in a representative manner. Further, we propose a multimodal deep learning approach to detect hardware Trojans and evaluate the results from both early fusion and late fusion strategies. We also estimate the uncertainty quantification metrics of each prediction for risk-aware decision-making. The outcomes not only confirms the efficacy of our proposed hardware Trojan detection method but also opens a new door for future studies employing multimodality and uncertainty quantification to address other hardware security challenges.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：A Framework for Agricultural Food Supply Chain using Blockchain</b></summary>
  <p><b>编号</b>：[286]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09476</p>
  <p><b>作者</b>：Sudarssan N</p>
  <p><b>备注</b>：5 Pages, 5 figures, Under Review</p>
  <p><b>关键词</b>：food supply chain, supply chain system, supply chain, ensuring food safety, food supply</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The main aim of the paper is to create a trust and transparency in the food supply chain system, ensuring food safety for everyone with the help of Blockchain Technology. Food supply chain is the process of tracing a crop from the farmer or producer to the buyer. With the advent of blockchain, providing a safe and fraud-free environment for the provision of numerous agricultural necessities has become much easier. Because of the globalization of trade, the present supply chain market today includes various companies involving integration of data, complex transactions and distribution. Information tamper resistance, supply-demand relationships, and traceable oversight are all difficulties that arise as a result of this. Blockchain is a distributed ledger technology that can provide information that is resistant to tampering. This strategy can eliminate the need for a centralized trusted authority, intermediaries, and business histories, allowing for increased production and security while maintaining the highest levels of integrity, liability, and safety. In order to have an integrity and transparency in food supply chain in the agricultural sector, a framework is proposed here based on block chain and IoT.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：Business and ethical concerns in domestic Conversational Generative  AI-empowered multi-robot systems</b></summary>
  <p><b>编号</b>：[289]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09473</p>
  <p><b>作者</b>：Rebekah Rousi,  Hooman Samani,  Niko Mäkitalo,  Ville Vakkuri,  Simo Linkola,  Kai-Kristian Kemell,  Paulius Daubaris,  Ilenia Fronza,  Tommi Mikkonen,  Pekka Abrahamsson</p>
  <p><b>备注</b>：15 pages, 4 figures, International Conference on Software Business</p>
  <p><b>关键词</b>：logic and design, technology are intricately, intricately connected, connected through logic, Generative artificial intelligence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Business and technology are intricately connected through logic and design. They are equally sensitive to societal changes and may be devastated by scandal. Cooperative multi-robot systems (MRSs) are on the rise, allowing robots of different types and brands to work together in diverse contexts. Generative artificial intelligence has been a dominant topic in recent artificial intelligence (AI) discussions due to its capacity to mimic humans through the use of natural language and the production of media, including deep fakes. In this article, we focus specifically on the conversational aspects of generative AI, and hence use the term Conversational Generative artificial intelligence (CGI). Like MRSs, CGIs have enormous potential for revolutionizing processes across sectors and transforming the way humans conduct business. From a business perspective, cooperative MRSs alone, with potential conflicts of interest, privacy practices, and safety concerns, require ethical examination. MRSs empowered by CGIs demand multi-dimensional and sophisticated methods to uncover imminent ethical pitfalls. This study focuses on ethics in CGI-empowered MRSs while reporting the stages of developing the MORUL model.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：Offline Handwriting Signature Verification: A Transfer Learning and  Feature Selection Approach</b></summary>
  <p><b>编号</b>：[291]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09467</p>
  <p><b>作者</b>：Fatih Ozyurt,  Jafar Majidpour,  Tarik A. Rashid,  Canan Koc</p>
  <p><b>备注</b>：11 pages</p>
  <p><b>关键词</b>：Handwritten signature verification, signature verification poses, Handwritten signature, poses a formidable, formidable challenge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Handwritten signature verification poses a formidable challenge in biometrics and document authenticity. The objective is to ascertain the authenticity of a provided handwritten signature, distinguishing between genuine and forged ones. This issue has many applications in sectors such as finance, legal documentation, and security. Currently, the field of computer vision and machine learning has made significant progress in the domain of handwritten signature verification. The outcomes, however, may be enhanced depending on the acquired findings, the structure of the datasets, and the used models. Four stages make up our suggested strategy. First, we collected a large dataset of 12600 images from 420 distinct individuals, and each individual has 30 signatures of a certain kind (All authors signatures are genuine). In the subsequent stage, the best features from each image were extracted using a deep learning model named MobileNetV2. During the feature selection step, three selectors neighborhood component analysis (NCA), Chi2, and mutual info (MI) were used to pull out 200, 300, 400, and 500 features, giving a total of 12 feature vectors. Finally, 12 results have been obtained by applying machine learning techniques such as SVM with kernels (rbf, poly, and linear), KNN, DT, Linear Discriminant Analysis, and Naive Bayes. Without employing feature selection techniques, our suggested offline signature verification achieved a classification accuracy of 91.3%, whereas using the NCA feature selection approach with just 300 features it achieved a classification accuracy of 97.7%. High classification accuracy was achieved using the designed and suggested model, which also has the benefit of being a self-organized framework. Consequently, using the optimum minimally chosen features, the proposed method could identify the best model performance and result validation prediction vectors.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：What's my role? Modelling responsibility for AI-based safety-critical  systems</b></summary>
  <p><b>编号</b>：[293]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09459</p>
  <p><b>作者</b>：Philippa Ryan,  Zoe Porter,  Joanna Al-Qaddoumi,  John McDermid,  Ibrahim Habli</p>
  <p><b>备注</b>：22 pages, 7 figures, 2 tables</p>
  <p><b>关键词</b>：AI-Based Safety-Critical Systems, Safety-Critical Systems, real world, increasingly deployed, Systems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>AI-Based Safety-Critical Systems (AI-SCS) are being increasingly deployed in the real world. These can pose a risk of harm to people and the environment. Reducing that risk is an overarching priority during development and operation. As more AI-SCS become autonomous, a layer of risk management via human intervention has been removed. Following an accident it will be important to identify causal contributions and the different responsible actors behind those to learn from mistakes and prevent similar future events. Many authors have commented on the "responsibility gap" where it is difficult for developers and manufacturers to be held responsible for harmful behaviour of an AI-SCS. This is due to the complex development cycle for AI, uncertainty in AI performance, and dynamic operating environment. A human operator can become a "liability sink" absorbing blame for the consequences of AI-SCS outputs they weren't responsible for creating, and may not have understanding of.
This cross-disciplinary paper considers different senses of responsibility (role, moral, legal and causal), and how they apply in the context of AI-SCS safety. We use a core concept (Actor(A) is responsible for Occurrence(O)) to create role responsibility models, producing a practical method to capture responsibility relationships and provide clarity on the previously identified responsibility issues. Our paper demonstrates the approach with two examples: a retrospective analysis of the Tempe Arizona fatal collision involving an autonomous vehicle, and a safety focused predictive role-responsibility analysis for an AI-based diabetes co-morbidity predictor. In both examples our primary focus is on safety, aiming to reduce unfair or disproportionate blame being placed on operators or developers. We present a discussion and avenues for future research.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：Dynamic Routing for Integrated Satellite-Terrestrial Networks: A  Constrained Multi-Agent Reinforcement Learning Approach</b></summary>
  <p><b>编号</b>：[296]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09455</p>
  <p><b>作者</b>：Yifeng Lyu,  Han Hu,  Rongfei Fan,  Zhi Liu,  Jianping An,  Shiwen Mao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：integrated satellite-terrestrial network, experienced significant growth, limited terrestrial infrastructure, offering seamless communication, seamless communication services</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The integrated satellite-terrestrial network (ISTN) system has experienced significant growth, offering seamless communication services in remote areas with limited terrestrial infrastructure. However, designing a routing scheme for ISTN is exceedingly difficult, primarily due to the heightened complexity resulting from the inclusion of additional ground stations, along with the requirement to satisfy various constraints related to satellite service quality. To address these challenges, we study packet routing with ground stations and satellites working jointly to transmit packets, while prioritizing fast communication and meeting energy efficiency and packet loss requirements. Specifically, we formulate the problem of packet routing with constraints as a max-min problem using the Lagrange method. Then we propose a novel constrained Multi-Agent reinforcement learning (MARL) dynamic routing algorithm named CMADR, which efficiently balances objective improvement and constraint satisfaction during the updating of policy and Lagrange multipliers. Finally, we conduct extensive experiments and an ablation study using the OneWeb and Telesat mega-constellations. Results demonstrate that CMADR reduces the packet delay by a minimum of 21% and 15%, while meeting stringent energy consumption and packet loss rate constraints, outperforming several baseline algorithms.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：Voila-A: Aligning Vision-Language Models with User's Gaze Attention</b></summary>
  <p><b>编号</b>：[297]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09454</p>
  <p><b>作者</b>：Kun Yan,  Lei Ji,  Zeyu Wang,  Yuntao Wang,  Nan Duan,  Shuai Ma</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：recent years, artificial intelligence, integration of vision, vision and language, language understanding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, the integration of vision and language understanding has led to significant advancements in artificial intelligence, particularly through Vision-Language Models (VLMs). However, existing VLMs face challenges in handling real-world applications with complex scenes and multiple objects, as well as aligning their focus with the diverse attention patterns of human users. In this paper, we introduce gaze information, feasibly collected by AR or VR devices, as a proxy for human attention to guide VLMs and propose a novel approach, Voila-A, for gaze alignment to enhance the interpretability and effectiveness of these models in real-world applications. First, we collect hundreds of minutes of gaze data to demonstrate that we can mimic human gaze modalities using localized narratives. We then design an automatic data annotation pipeline utilizing GPT-4 to generate the VOILA-COCO dataset. Additionally, we innovate the Voila Perceiver modules to integrate gaze information into VLMs while preserving their pretrained knowledge. We evaluate Voila-A using a hold-out validation set and a newly collected VOILA-GAZE Testset, which features real-life scenarios captured with a gaze-tracking device. Our experimental results demonstrate that Voila-A significantly outperforms several baseline models. By aligning model attention with human gaze patterns, Voila-A paves the way for more intuitive, user-centric VLMs and fosters engaging human-AI interaction across a wide range of applications.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：Incorporating Riemannian Geometric Features for Learning Coefficient of  Pressure Distributions on Airplane Wings</b></summary>
  <p><b>编号</b>：[298]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09452</p>
  <p><b>作者</b>：Liwei Hu,  Wenyong Wang,  Yu Xiang,  Stefan Sommer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：angle of attack, significantly impacted, aerodynamic coefficients, geometric features, predicted aerodynamic coefficients</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The aerodynamic coefficients of aircrafts are significantly impacted by its geometry, especially when the angle of attack (AoA) is large. In the field of aerodynamics, traditional polynomial-based parameterization uses as few parameters as possible to describe the geometry of an airfoil. However, because the 3D geometry of a wing is more complicated than the 2D airfoil, polynomial-based parameterizations have difficulty in accurately representing the entire shape of a wing in 3D space. Existing deep learning-based methods can extract massive latent neural representations for the shape of 2D airfoils or 2D slices of wings. Recent studies highlight that directly taking geometric features as inputs to the neural networks can improve the accuracy of predicted aerodynamic coefficients. Motivated by geometry theory, we propose to incorporate Riemannian geometric features for learning Coefficient of Pressure (CP) distributions on wing surfaces. Our method calculates geometric features (Riemannian metric, connection, and curvature) and further inputs the geometric features, coordinates and flight conditions into a deep learning model to predict the CP distribution. Experimental results show that our method, compared to state-of-the-art Deep Attention Network (DAN), reduces the predicted mean square error (MSE) of CP by an average of 8.41% for the DLR-F11 aircraft test set.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：Joining Forces for Pathology Diagnostics with AI Assistance: The EMPAIA  Initiative</b></summary>
  <p><b>编号</b>：[299]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09450</p>
  <p><b>作者</b>：Norman Zerbe,  Lars Ole Schwen,  Christian Geißler,  Katja Wiesemann,  Tom Bisson,  Peter Boor,  Rita Carvalho,  Michael Franz,  Christoph Jansen,  Tim-Rasmus Kiehl,  Björn Lindequist,  Nora Charlotte Pohlan,  Sarah Schmell,  Klaus Strohmenger,  Falk Zakrzewski,  Markus Plass,  Michael Takla,  Tobias Küster,  André Homeyer,  Peter Hufnagl</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：artificial intelligence, past decade, advanced substantially, EMPAIA, pathology</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Over the past decade, artificial intelligence (AI) methods in pathology have advanced substantially. However, integration into routine clinical practice has been slow due to numerous challenges, including technical and regulatory hurdles in translating research results into clinical diagnostic products and the lack of standardized interfaces. The open and vendor-neutral EMPAIA initiative addresses these challenges. Here, we provide an overview of EMPAIA's achievements and lessons learned. EMPAIA integrates various stakeholders of the pathology AI ecosystem, i.e., pathologists, computer scientists, and industry. In close collaboration, we developed technical interoperability standards, recommendations for AI testing and product development, and explainability methods. We implemented the modular and open-source EMPAIA platform and successfully integrated 11 AI-based image analysis apps from 6 different vendors, demonstrating how different apps can use a single standardized interface. We prioritized requirements and evaluated the use of AI in real clinical settings with 14 different pathology laboratories in Europe and Asia. In addition to technical developments, we created a forum for all stakeholders to share information and experiences on digital pathology and AI. Commercial, clinical, and academic stakeholders can now adopt EMPAIA's common open-source interfaces, providing a unique opportunity for large-scale standardization and streamlining of processes. Further efforts are needed to effectively and broadly establish AI assistance in routine laboratory use. To this end, a sustainable infrastructure, the non-profit association EMPAIA International, has been established to continue standardization and support broad implementation and advocacy for an AI-assisted digital pathology future.</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：Tumbug: A pictorial, universal knowledge representation method</b></summary>
  <p><b>编号</b>：[301]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09448</p>
  <p><b>作者</b>：Mark A. Atkins</p>
  <p><b>备注</b>：346 pages, 334 figures</p>
  <p><b>关键词</b>：artificial general intelligence, suitable for CSR, CSR, Primitive Conceptual Categories, KRM called Tumbug</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Since the key to artificial general intelligence (AGI) is commonly believed to be commonsense reasoning (CSR) or, roughly equivalently, discovery of a knowledge representation method (KRM) that is particularly suitable for CSR, the author developed a custom KRM for CSR. This novel KRM called Tumbug was designed to be pictorial in nature because there exists increasing evidence that the human brain uses some pictorial type of KRM, and no well-known prior research in AGI has researched this KRM possibility. Tumbug is somewhat similar to Roger Schank's Conceptual Dependency (CD) theory, but Tumbug is pictorial and uses about 30 components based on fundamental concepts from the sciences and human life, in contrast to CD theory, which is textual and uses about 17 components (= 6 Primitive Conceptual Categories + 11 Primitive Acts) based mainly on human-oriented activities. All the Building Blocks of Tumbug were found to generalize to only five Basic Building Blocks that exactly correspond to the three components {O, A, V} of traditional Object-Attribute-Value representation plus two new components {C, S}, which are Change and System. Collectively this set of five components, called "SCOVA," seems to be a universal foundation for all knowledge representation.</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：Explainable Multimodal Sentiment Analysis on Bengali Memes</b></summary>
  <p><b>编号</b>：[303]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09446</p>
  <p><b>作者</b>：Kazi Toufique Elahi,  Tasnuva Binte Rahman,  Shakil Shahriar,  Samir Sarker,  Sajib Kumar Saha Joy,  Faisal Muhammad Shah</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：attracting online communities, digital era, attracting online, cultural barriers, distinctive and effective</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Memes have become a distinctive and effective form of communication in the digital era, attracting online communities and cutting across cultural barriers. Even though memes are frequently linked with humor, they have an amazing capacity to convey a wide range of emotions, including happiness, sarcasm, frustration, and more. Understanding and interpreting the sentiment underlying memes has become crucial in the age of information. Previous research has explored text-based, image-based, and multimodal approaches, leading to the development of models like CAPSAN and PromptHate for detecting various meme categories. However, the study of low-resource languages like Bengali memes remains scarce, with limited availability of publicly accessible datasets. A recent contribution includes the introduction of the MemoSen dataset. However, the achieved accuracy is notably low, and the dataset suffers from imbalanced distribution. In this study, we employed a multimodal approach using ResNet50 and BanglishBERT and achieved a satisfactory result of 0.71 weighted F1-score, performed comparison with unimodal approaches, and interpreted behaviors of the models using explainable artificial intelligence (XAI) techniques.</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：Online Handbook of Argumentation for AI: Volume 4</b></summary>
  <p><b>编号</b>：[305]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09444</p>
  <p><b>作者</b>：Lars Bengel,  Lydia Blümel,  Elfia Bezou-Vrakatseli,  Federico Castagna,  Giulia D'Agostino,  Isabelle Kuhlmann,  Jack Mumford,  Daphne Odekerken,  Fabrizio Russo,  Stefan Sarkadi,  Madeleine Waller,  Andreas Xydis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Online Handbook, fourth volume, revised versions, papers selected, volume contains revised</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This volume contains revised versions of the papers selected for the fourth volume of the Online Handbook of Argumentation for AI (OHAAI). Previously, formal theories of argument and argument interaction have been proposed and studied, and this has led to the more recent study of computational models of argument. Argumentation, as a field within artificial intelligence (AI), is highly relevant for researchers interested in symbolic representations of knowledge and defeasible reasoning. The purpose of this handbook is to provide an open access and curated anthology for the argumentation research community. OHAAI is designed to serve as a research hub to keep track of the latest and upcoming PhD-driven research on the theory and application of argumentation in all areas related to AI.</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：CRD: Collaborative Representation Distance for Practical Anomaly  Detection</b></summary>
  <p><b>编号</b>：[306]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09443</p>
  <p><b>作者</b>：Chao Han,  Yudong Yan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：defect detection plays, Visual defect detection, intelligent industry, detection plays, plays an important</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Visual defect detection plays an important role in intelligent industry. Patch based methods consider visual images as a collection of image patches according to positions, which have stronger discriminative ability for small defects in products, e.g. scratches on pills. However, the nearest neighbor search for the query image and the stored patches will occupy $O(n)$ complexity in terms of time and space requirements, posing strict challenges for deployment in edge environments. In this paper, we propose an alternative approach to the distance calculation of image patches via collaborative representation models. Starting from the nearest neighbor distance with $L_0$ constraint, we relax the constraint to $L_2$ constraint and solve the distance quickly in close-formed without actually accessing the original stored collection of image patches. Furthermore, we point out that the main computational burden of this close-formed solution can be pre-computed by high-performance server before deployment. Consequently, the distance calculation on edge devices only requires a simple matrix multiplication, which is extremely lightweight and GPU-friendly. Performance on real industrial scenarios demonstrates that compared to the existing state-of-the-art methods, this distance achieves several hundred times improvement in computational efficiency with slight performance drop, while greatly reducing memory overhead.</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：Object Attribute Matters in Visual Question Answering</b></summary>
  <p><b>编号</b>：[307]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09442</p>
  <p><b>作者</b>：Peize Li,  Qingyi Si,  Peng Fu,  Zheng Lin,  Yan Wang</p>
  <p><b>备注</b>：AAAI 2024</p>
  <p><b>关键词</b>：Visual question answering, question answering, task that requires, requires the joint, joint comprehension</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Visual question answering is a multimodal task that requires the joint comprehension of visual and textual information. However, integrating visual and textual semantics solely through attention layers is insufficient to comprehensively understand and align information from both modalities. Intuitively, object attributes can naturally serve as a bridge to unify them, which has been overlooked in previous research. In this paper, we propose a novel VQA approach from the perspective of utilizing object attribute, aiming to achieve better object-level visual-language alignment and multimodal scene understanding. Specifically, we design an attribute fusion module and a contrastive knowledge distillation module. The attribute fusion module constructs a multimodal graph neural network to fuse attributes and visual features through message passing. The enhanced object-level visual features contribute to solving fine-grained problem like counting-question. The better object-level visual-language alignment aids in understanding multimodal scenes, thereby improving the model's robustness. Furthermore, to augment scene understanding and the out-of-distribution performance, the contrastive knowledge distillation module introduces a series of implicit knowledge. We distill knowledge into attributes through contrastive loss, which further strengthens the representation learning of attribute features and facilitates visual-linguistic alignment. Intensive experiments on six datasets, COCO-QA, VQAv2, VQA-CPv2, VQA-CPv1, VQAvs and TDIUC, show the superiority of the proposed method.</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：RoleCraft-GLM: Advancing Personalized Role-Playing in Large Language  Models</b></summary>
  <p><b>编号</b>：[310]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09432</p>
  <p><b>作者</b>：Meiling Tao,  Xuechen Liang,  Tianyu Shi,  Lei Yu,  Yiting Xie</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, study presents RoleCraft-GLM, Language Models, innovative framework aimed, Large Language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This study presents RoleCraft-GLM, an innovative framework aimed at enhancing personalized role-playing with Large Language Models (LLMs). RoleCraft-GLM addresses the key issue of lacking personalized interactions in conversational AI, and offers a solution with detailed and emotionally nuanced character portrayals. We contribute a unique conversational dataset that shifts from conventional celebrity-centric characters to diverse, non-celebrity personas, thus enhancing the realism and complexity of language modeling interactions. Additionally, our approach includes meticulous character development, ensuring dialogues are both realistic and emotionally resonant. The effectiveness of RoleCraft-GLM is validated through various case studies, highlighting its versatility and skill in different scenarios. Our framework excels in generating dialogues that accurately reflect characters' personality traits and emotions, thereby boosting user engagement. In conclusion, RoleCraft-GLM marks a significant leap in personalized AI interactions, and paves the way for more authentic and immersive AI-assisted role-playing experiences by enabling more nuanced and emotionally rich dialogues</p>
  </details>
</details>
<details>
  <summary>83. <b>标题：Improving PTM Site Prediction by Coupling of Multi-Granularity Structure  and Multi-Scale Sequence Representation</b></summary>
  <p><b>编号</b>：[312]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10211</p>
  <p><b>作者</b>：Zhengyi Li,  Menglu Li,  Lida Zhu,  Wen Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Protein post-translational modification, post-translational modification, task in bioinformatics, PTM, fundamental task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Protein post-translational modification (PTM) site prediction is a fundamental task in bioinformatics. Several computational methods have been developed to predict PTM sites. However, existing methods ignore the structure information and merely utilize protein sequences. Furthermore, designing a more fine-grained structure representation learning method is urgently needed as PTM is a biological event that occurs at the atom granularity. In this paper, we propose a PTM site prediction method by Coupling of Multi-Granularity structure and Multi-Scale sequence representation, PTM-CMGMS for brevity. Specifically, multigranularity structure-aware representation learning is designed to learn neighborhood structure representations at the amino acid, atom, and whole protein granularity from AlphaFold predicted structures, followed by utilizing contrastive learning to optimize the structure representations.Additionally, multi-scale sequence representation learning is used to extract context sequence information, and motif generated by aligning all context sequences of PTM sites assists the prediction. Extensive experiments on three datasets show that PTM-CMGMS outperforms the state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>84. <b>标题：FreGrad: Lightweight and Fast Frequency-aware Diffusion Vocoder</b></summary>
  <p><b>编号</b>：[324]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.10032</p>
  <p><b>作者</b>：Tan Dat Nguyen,  Ji-Hoon Kim,  Youngjoon Jang,  Jaehun Kim,  Joon Son Chung</p>
  <p><b>备注</b>：Accepted to ICASSP 2024</p>
  <p><b>关键词</b>：fast diffusion-based vocoder, generate realistic audio, diffusion-based vocoder, generate realistic, lightweight and fast</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The goal of this paper is to generate realistic audio with a lightweight and fast diffusion-based vocoder, named FreGrad. Our framework consists of the following three key components: (1) We employ discrete wavelet transform that decomposes a complicated waveform into sub-band wavelets, which helps FreGrad to operate on a simple and concise feature space, (2) We design a frequency-aware dilated convolution that elevates frequency awareness, resulting in generating speech with accurate frequency information, and (3) We introduce a bag of tricks that boosts the generation quality of the proposed model. In our experiments, FreGrad achieves 3.7 times faster training time and 2.2 times faster inference speed compared to our baseline while reducing the model size by 0.6 times (only 1.78M parameters) without sacrificing the output quality. Audio samples are available at: this https URL.</p>
  </details>
</details>
<details>
  <summary>85. <b>标题：Slicer Networks</b></summary>
  <p><b>编号</b>：[332]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09833</p>
  <p><b>作者</b>：Hang Zhang,  Xiang Chen,  Rongguang Wang,  Renjiu Hu,  Dongdong Liu,  Gaolei Li</p>
  <p><b>备注</b>：8 figures and 3 tables</p>
  <p><b>关键词</b>：consistent internal intensities, scans often reveal, intensities or textures, Slicer Network, reveal objects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In medical imaging, scans often reveal objects with varied contrasts but consistent internal intensities or textures. This characteristic enables the use of low-frequency approximations for tasks such as segmentation and deformation field estimation. Yet, integrating this concept into neural network architectures for medical image analysis remains underexplored. In this paper, we propose the Slicer Network, a novel architecture designed to leverage these traits. Comprising an encoder utilizing models like vision transformers for feature extraction and a slicer employing a learnable bilateral grid, the Slicer Network strategically refines and upsamples feature maps via a splatting-blurring-slicing process. This introduces an edge-preserving low-frequency approximation for the network outcome, effectively enlarging the effective receptive field. The enhancement not only reduces computational complexity but also boosts overall performance. Experiments across different medical imaging applications, including unsupervised and keypoints-based image registration and lesion segmentation, have verified the Slicer Network's improved accuracy and efficiency.</p>
  </details>
</details>
<details>
  <summary>86. <b>标题：Parameter Selection for Analyzing Conversations with Autism Spectrum  Disorder</b></summary>
  <p><b>编号</b>：[336]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09717</p>
  <p><b>作者</b>：Tahiya Chowdhury,  Veronica Romero,  Amanda Stent</p>
  <p><b>备注</b>：5 pages, 4 tables, Proceedings of INTERSPEECH 2023</p>
  <p><b>关键词</b>：autism spectrum disorder, spectrum disorder, autism spectrum, ASD, challenging task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The diagnosis of autism spectrum disorder (ASD) is a complex, challenging task as it depends on the analysis of interactional behaviors by psychologists rather than the use of biochemical diagnostics. In this paper, we present a modeling approach to ASD diagnosis by analyzing acoustic/prosodic and linguistic features extracted from diagnostic conversations between a psychologist and children who either are typically developing (TD) or have ASD. We compare the contributions of different features across a range of conversation tasks. We focus on finding a minimal set of parameters that characterize conversational behaviors of children with ASD. Because ASD is diagnosed through conversational interaction, in addition to analyzing the behavior of the children, we also investigate whether the psychologist's conversational behaviors vary across diagnostic groups. Our results can facilitate fine-grained analysis of conversation data for children with ASD to support diagnosis and intervention.</p>
  </details>
</details>
<details>
  <summary>87. <b>标题：Deep learning enhanced mixed integer optimization: Learning to reduce  model dimensionality</b></summary>
  <p><b>编号</b>：[350]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09556</p>
  <p><b>作者</b>：Niki Triantafyllou,  Maria M. Papathanasiou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：computational complexity inherent, models by harnessing, deep learning, work introduces, address the computational</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This work introduces a framework to address the computational complexity inherent in Mixed-Integer Programming (MIP) models by harnessing the potential of deep learning. We compare the effectiveness of (a) feed-forward neural networks (ANN) and (b) convolutional neural networks (CNN) in approximating the active dimensions within MIP problems. We utilize multi-label classification to account for more than one active dimension. To enhance the framework's performance, we employ Bayesian optimization for hyperparameter tuning, aiming to maximize sample-level accuracy. The primary objective is to train the neural networks to predict all active dimensions accurately, thereby maximizing the occurrence of global optimum solutions. We apply this framework to a flow-based facility location allocation Mixed-Integer Linear Programming (MILP) formulation that describes long-term investment planning and medium-term tactical planning in a personalized medicine supply chain for cell therapy manufacturing and distribution.</p>
  </details>
</details>
<details>
  <summary>88. <b>标题：Self Supervised Vision for Climate Downscaling</b></summary>
  <p><b>编号</b>：[356]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09466</p>
  <p><b>作者</b>：Karandeep Singh,  Chaeyoon Jeong,  Naufal Shidqi,  Sungwon Park,  Arjun Nellikkattil,  Elke Zeller,  Meeyoung Cha</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：facing today, Earth climate system, critical challenges, planet is facing, Earth System Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Climate change is one of the most critical challenges that our planet is facing today. Rising global temperatures are already bringing noticeable changes to Earth's weather and climate patterns with an increased frequency of unpredictable and extreme weather events. Future projections for climate change research are based on Earth System Models (ESMs), the computer models that simulate the Earth's climate system. ESMs provide a framework to integrate various physical systems, but their output is bound by the enormous computational resources required for running and archiving higher-resolution simulations. For a given resource budget, the ESMs are generally run on a coarser grid, followed by a computationally lighter $downscaling$ process to obtain a finer-resolution output. In this work, we present a deep-learning model for downscaling ESM simulation data that does not require high-resolution ground truth data for model optimization. This is realized by leveraging salient data distribution patterns and the hidden dependencies between weather variables for an $\textit{individual}$ data point at $\textit{runtime}$. Extensive evaluation with $2$x, $3$x, and $4$x scaling factors demonstrates that the proposed model consistently obtains superior performance over that of various baselines. The improved downscaling performance and no dependence on high-resolution ground truth data make the proposed method a valuable tool for climate research and mark it as a promising direction for future research.</p>
  </details>
</details>
<details>
  <summary>89. <b>标题：Diffusion-Driven Generative Framework for Molecular Conformation  Prediction</b></summary>
  <p><b>编号</b>：[359]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09451</p>
  <p><b>作者</b>：Bobin Yang,  Zhenghan Chen</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2105.07246 by other authors</p>
  <p><b>关键词</b>：development of pharmaceuticals, inferring three-dimensional molecular, three-dimensional molecular configurations, critical significance, domains of computational</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The task of inferring three-dimensional molecular configurations from their two-dimensional graph representations is of critical significance in the domains of computational chemistry and the development of pharmaceuticals. It contributes fundamentally to our grasp of molecular mechanisms and interactions. The rapid evolution of machine learning, especially in the realm of deep generative networks, has catalyzed breakthroughs in the precision of such predictive modeling. Traditional methodologies typically employ a bifurcated strategy: initially estimating interatomic distances followed by sculpting the spatial molecular structure via solving a distance geometry problem. This sequential approach, however, occasionally fails to capture the intricacies of local atomic arrangements accurately, thus compromising the integrity of the resultant structural models. Addressing these deficiencies, this work introduces an avant-garde generative framework: \method{}, which is predicated on the diffusion principles found in classical non-equilibrium thermodynamics. \method{} envisages atoms as discrete entities and is adept at guiding the reversal of diffusion morphing a distribution of stochastic noise back into coherent molecular forms through a process akin to a Markov chain. This transformation begins with the initial representation of a molecular graph in an abstract latent space, progressing to the realization of the three-dimensional forms via an elaborate bilevel optimization scheme, tailored to respect the task's specific requirements.</p>
  </details>
</details>
<details>
  <summary>90. <b>标题：Reasoning with random sets: An agenda for the future</b></summary>
  <p><b>编号</b>：[360]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09435</p>
  <p><b>作者</b>：Fabio Cuzzolin</p>
  <p><b>备注</b>：94 pages, 17 figures</p>
  <p><b>关键词</b>：general random sets, include general random, alternative geometric representations, statistical learning theory, random sets</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we discuss a potential agenda for future work in the theory of random sets and belief functions, touching upon a number of focal issues: the development of a fully-fledged theory of statistical reasoning with random sets, including the generalisation of logistic regression and of the classical laws of probability; the further development of the geometric approach to uncertainty, to include general random sets, a wider range of uncertainty measures and alternative geometric representations; the application of this new theory to high-impact areas such as climate change, machine learning and statistical learning theory.</p>
  </details>
</details>
<details>
  <summary>91. <b>标题：Precipitation Prediction Using an Ensemble of Lightweight Learners</b></summary>
  <p><b>编号</b>：[363]</p>
  <p><b>链接</b>：https://arxiv.org/abs/2401.09424</p>
  <p><b>作者</b>：Xinzhe Li,  Sun Rui,  Yiming Niu,  Yao Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Precipitation prediction plays, agriculture and industry, high precipitation events, prediction plays, plays a crucial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Precipitation prediction plays a crucial role in modern agriculture and industry. However, it poses significant challenges due to the diverse patterns and dynamics in time and space, as well as the scarcity of high precipitation events.
To address this challenge, we propose an ensemble learning framework that leverages multiple learners to capture the diverse patterns of precipitation distribution. Specifically, the framework consists of a precipitation predictor with multiple lightweight heads (learners) and a controller that combines the outputs from these heads. The learners and the controller are separately optimized with a proposed 3-stage training scheme.
By utilizing provided satellite images, the proposed approach can effectively model the intricate rainfall patterns, especially for high precipitation events. It achieved 1st place on the core test as well as the nowcasting leaderboards of the Weather4Cast 2023 competition. For detailed implementation, please refer to our GitHub repository at: this https URL.</p>
  </details>
</details>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">徐耀彬</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://louishsu.xyz/2024/01/22/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">http://louishsu.xyz/2024/01/22/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://louishsu.xyz" target="_blank">LOUIS' BLOG</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2023/09/22/vLLM%EF%BC%9A%E5%88%A9%E7%94%A8%E5%88%86%E9%A1%B5%E7%BC%93%E5%AD%98%E5%92%8C%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E6%8F%90%E9%AB%98%E5%A4%A7%E6%A8%A1%E5%9E%8B2~4x%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6.html"><img class="next-cover" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww5.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">徐耀彬</div><div class="author-info__description">💭这个人很懒，什么都没有留下</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/isLouisHsu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/isLouisHsu" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:is.louishsu@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">记录和分享一些学习和开源内容，若有问题可通过邮箱is.louishsu@foxmail.com联系，欢迎交流！！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">统计</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">自然语言处理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">计算机视觉</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">机器学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">人工智能</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/01/22/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2024-01-22)"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv每日速递(2024-01-22)"/></a><div class="content"><a class="title" href="/2024/01/22/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2024-01-22)">Arxiv每日速递(2024-01-22)</a><time datetime="2024-01-22T00:40:44.003Z" title="发表于 2024-01-22 08:40:44">2024-01-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/09/22/vLLM%EF%BC%9A%E5%88%A9%E7%94%A8%E5%88%86%E9%A1%B5%E7%BC%93%E5%AD%98%E5%92%8C%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E6%8F%90%E9%AB%98%E5%A4%A7%E6%A8%A1%E5%9E%8B2~4x%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6.html" title="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww5.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度"/></a><div class="content"><a class="title" href="/2023/09/22/vLLM%EF%BC%9A%E5%88%A9%E7%94%A8%E5%88%86%E9%A1%B5%E7%BC%93%E5%AD%98%E5%92%8C%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E6%8F%90%E9%AB%98%E5%A4%A7%E6%A8%A1%E5%9E%8B2~4x%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6.html" title="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度">vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度</a><time datetime="2023-09-22T14:55:45.000Z" title="发表于 2023-09-22 22:55:45">2023-09-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/09/06/Prompt%EF%BC%9A%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%89%A7%E8%A1%8C%E6%8C%87%E5%8D%97.html" title="Prompt：大语言模型的执行指南"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww6.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Prompt：大语言模型的执行指南"/></a><div class="content"><a class="title" href="/2023/09/06/Prompt%EF%BC%9A%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%89%A7%E8%A1%8C%E6%8C%87%E5%8D%97.html" title="Prompt：大语言模型的执行指南">Prompt：大语言模型的执行指南</a><time datetime="2023-09-06T14:45:45.000Z" title="发表于 2023-09-06 22:45:45">2023-09-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/09/03/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9C%A81688%E7%94%B5%E5%95%86%E5%9C%BA%E6%99%AF%E7%9A%84%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5.html" title="【转载】大语言模型在1688电商场景的算法实践"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【转载】大语言模型在1688电商场景的算法实践"/></a><div class="content"><a class="title" href="/2023/09/03/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9C%A81688%E7%94%B5%E5%95%86%E5%9C%BA%E6%99%AF%E7%9A%84%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5.html" title="【转载】大语言模型在1688电商场景的算法实践">【转载】大语言模型在1688电商场景的算法实践</a><time datetime="2023-09-03T15:35:45.000Z" title="发表于 2023-09-03 23:35:45">2023-09-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/07/%E3%80%90%E6%A2%B3%E7%90%86%E3%80%91%E9%99%86%E5%A5%87%E6%9C%80%E6%96%B0%E6%BC%94%E8%AE%B2%E5%AE%9E%E5%BD%95%EF%BC%9A%E6%88%91%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%96%E7%95%8C%E8%A7%82%20.html" title="【梳理】陆奇最新演讲实录：我的大模型世界观"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【梳理】陆奇最新演讲实录：我的大模型世界观"/></a><div class="content"><a class="title" href="/2023/05/07/%E3%80%90%E6%A2%B3%E7%90%86%E3%80%91%E9%99%86%E5%A5%87%E6%9C%80%E6%96%B0%E6%BC%94%E8%AE%B2%E5%AE%9E%E5%BD%95%EF%BC%9A%E6%88%91%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%96%E7%95%8C%E8%A7%82%20.html" title="【梳理】陆奇最新演讲实录：我的大模型世界观">【梳理】陆奇最新演讲实录：我的大模型世界观</a><time datetime="2023-05-07T11:07:45.000Z" title="发表于 2023-05-07 19:07:45">2023-05-07</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By 徐耀彬</div><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender"title="主题采用butterfly"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr"title="本站使用JsDelivr为静态资源提供CDN加速"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub"title="本站项目由Gtihub托管"alt="img"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"alt="img"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></br></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'default',
      })
      false && mermaid.init()
    })
  }
}</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    let initData = {
      el: '#twikoo-wrap',
      envId: 'blog-',
      region: ''
    }

    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    twikoo.init(initData)
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'blog-',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/机器学习/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐱 机器学习 (3)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/自然语言处理/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 自然语言处理 (7)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/竞赛相关/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 竞赛相关 (3)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/阅读笔记/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 阅读笔记 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="http://louishsu.xyz/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt=""><img width="48" height="48" src="https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161037709574435991610377095138.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2021-05-19</span><a class="blog-slider__title" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt="">全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/cail2021.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2021-10-22</span><a class="blog-slider__title" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt="">中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt=""><img width="48" height="48" src="https://cdn.kesci.com/upload/image/r7j60un866.png?imageView2/2/w/2500/h/2500" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-11-17</span><a class="blog-slider__title" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt="">2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww5.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-09-22</span><a class="blog-slider__title" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt="">vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww6.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-09-06</span><a class="blog-slider__title" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt="">Prompt：大语言模型的执行指南</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/11/26/升级深度学习开发环境全攻略.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/default.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-11-26</span><a class="blog-slider__title" href="2022/11/26/升级深度学习开发环境全攻略.html" alt="">升级深度学习开发环境全攻略</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2022/11/26/升级深度学习开发环境全攻略.html" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>