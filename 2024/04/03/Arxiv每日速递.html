<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Arxiv每日速递(2024-04-03) | LOUIS' BLOG</title><meta name="author" content="徐耀彬"><meta name="copyright" content="徐耀彬"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以自然语言处理、计算机视觉、机器学习、人工智能等大方向进行划分。 统计 今日共更新748篇论文，其中：  159篇自然语言处理（cs.CL） 241篇计算机视觉（cs.CV） 199篇机器学习（cs.LG） 155篇人工智能（cs.AI）  自然语言处理    1. 标题：CausalChaos! Dataset for Comprehe">
<meta property="og:type" content="article">
<meta property="og:title" content="Arxiv每日速递(2024-04-03)">
<meta property="og:url" content="http://louishsu.xyz/2024/04/03/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">
<meta property="og:site_name" content="LOUIS&#39; BLOG">
<meta property="og:description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以自然语言处理、计算机视觉、机器学习、人工智能等大方向进行划分。 统计 今日共更新748篇论文，其中：  159篇自然语言处理（cs.CL） 241篇计算机视觉（cs.CV） 199篇机器学习（cs.LG） 155篇人工智能（cs.AI）  自然语言处理    1. 标题：CausalChaos! Dataset for Comprehe">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png">
<meta property="article:published_time" content="2024-04-03T00:36:54.645Z">
<meta property="article:modified_time" content="2024-04-03T00:38:34.101Z">
<meta property="article:author" content="徐耀彬">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://louishsu.xyz/2024/04/03/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2024-04-03 08:38:34'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/background.css"><script src="https://cdn.jsdelivr.net/npm/echarts@4.7.0/dist/echarts.min.js"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-brands fa-app-store"></i><span> 利器</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://code.visualstudio.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> VSCode：微软旗下的跨平台代码编辑软件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mobaxterm.mobatek.net/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> MobaXterm：超好用的全能远程终端</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/CopyTranslator/CopyTranslator"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> CopyTranslator：“复制即翻译”的外文辅助阅读翻译解决方案</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.zotero.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zotero：便于收集、组织、引用、共享的文献管理工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://zealdocs.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zeal：离线文档浏览器，其灵感来自 OS X平台上的 Dash，目前支持 Window 和 Liunx，基于 QT5</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ditto-cp.sourceforge.io/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Ditto：强大的Windows剪贴板增强工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://wise-system-monitor.en.softonic.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Wise System Monitor：监控从系统到本地网络的所有运行情况</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/indiff/qttabbar"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> QtTabBar：在Windows资源管理器中使用多标签功能扩展工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/sentialx/multrin"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Multrin：“窗口合并”辅助小工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/Wox-launcher/Wox"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Wox &amp; Everything：基于名称快速定位文件和文件夹的搜索工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/NickeManarin/ScreenToGif"><i class="fa-fw fa-regular fa-star"></i><span> ScreenToGif：快速录制屏幕指定区域并保存为动图文件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mathpix.com/"><i class="fa-fw fa-regular fa-star"></i><span> Mathpix Snipping：识别数学公式并转换成LaTeX</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="http://www.uderzo.it/main_products/space_sniffer/index.html"><i class="fa-fw fa-regular fa-star"></i><span> Space Sniffer：磁盘空间分析工具</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">LOUIS' BLOG</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-brands fa-app-store"></i><span> 利器</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://code.visualstudio.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> VSCode：微软旗下的跨平台代码编辑软件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mobaxterm.mobatek.net/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> MobaXterm：超好用的全能远程终端</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/CopyTranslator/CopyTranslator"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> CopyTranslator：“复制即翻译”的外文辅助阅读翻译解决方案</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.zotero.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zotero：便于收集、组织、引用、共享的文献管理工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://zealdocs.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zeal：离线文档浏览器，其灵感来自 OS X平台上的 Dash，目前支持 Window 和 Liunx，基于 QT5</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ditto-cp.sourceforge.io/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Ditto：强大的Windows剪贴板增强工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://wise-system-monitor.en.softonic.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Wise System Monitor：监控从系统到本地网络的所有运行情况</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/indiff/qttabbar"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> QtTabBar：在Windows资源管理器中使用多标签功能扩展工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/sentialx/multrin"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Multrin：“窗口合并”辅助小工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/Wox-launcher/Wox"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Wox &amp; Everything：基于名称快速定位文件和文件夹的搜索工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/NickeManarin/ScreenToGif"><i class="fa-fw fa-regular fa-star"></i><span> ScreenToGif：快速录制屏幕指定区域并保存为动图文件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mathpix.com/"><i class="fa-fw fa-regular fa-star"></i><span> Mathpix Snipping：识别数学公式并转换成LaTeX</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="http://www.uderzo.it/main_products/space_sniffer/index.html"><i class="fa-fw fa-regular fa-star"></i><span> Space Sniffer：磁盘空间分析工具</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Arxiv每日速递(2024-04-03)<a class="post-edit-link" href="https://github.com/isLouisHsu/blog/tree/master/source_posts/Arxiv每日速递.md" title="编辑" target="_blank"><i class="fas fa-pencil-square"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-04-03T00:36:54.645Z" title="发表于 2024-04-03 08:36:54">2024-04-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-04-03T00:38:34.101Z" title="更新于 2024-04-03 08:38:34">2024-04-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">阅读笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">126.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>758分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2024/04/03/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以自然语言处理、计算机视觉、机器学习、人工智能等大方向进行划分。</p>
<h1>统计</h1>
<p>今日共更新748篇论文，其中：</p>
<ul>
<li><a href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86">159篇自然语言处理（cs.CL）</a></li>
<li><a href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89">241篇计算机视觉（cs.CV）</a></li>
<li><a href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">199篇机器学习（cs.LG）</a></li>
<li><a href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD">155篇人工智能（cs.AI）</a></li>
</ul>
<h1>自然语言处理</h1>
<details>
  <summary>1. <b>标题：CausalChaos! Dataset for Comprehensive Causal Action Question Answering  Over Longer Causal Chains Grounded in Dynamic Visual Scenes</b></summary>
  <p><b>编号</b>：[2]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01299">https://arxiv.org/abs/2404.01299</a></p>
  <p><b>作者</b>：Ting En Lam,  Yuhan Chen,  Elston Tan,  Eric Peh,  Ruirui Chen,  Paritosh Parmar,  Basura Fernando</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：garnered increasing interest, video question answering, causal reasoning analysis, increasing interest, reasoning analysis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Causal video question answering (QA) has garnered increasing interest, yet existing datasets often lack depth in causal reasoning analysis. To address this gap, we capitalize on the unique properties of cartoons and construct CausalChaos!, a novel, challenging causal Why-QA dataset built upon the iconic "Tom and Jerry" cartoon series. With thoughtful questions and multi-level answers, our dataset contains much longer causal chains embedded in dynamic interactions and visuals, at the same time principles of animation allows animators to create well-defined, unambiguous causal relationships. These factors allow models to solve more challenging, yet well-defined causal relationships. We also introduce hard negative mining, including CausalConfusion version. While models perform well, there is much room for improvement, especially, on open-ended answers. We identify more advanced/explicit causal relationship modeling and joint modeling of vision and language as the immediate areas for future efforts to focus upon. Along with the other complementary datasets, our new challenging dataset will pave the way for these developments in the field. We will release our dataset, codes, and models to help future efforts in this domain.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Towards Safety and Helpfulness Balanced Responses via Controllable Large  Language Models</b></summary>
  <p><b>编号</b>：[6]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01295">https://arxiv.org/abs/2404.01295</a></p>
  <p><b>作者</b>：Yi-Lin Tuan,  Xilun Chen,  Eric Michael Smith,  Louis Martin,  Soumya Batra,  Asli Celikyilmaz,  William Yang Wang,  Daniel M. Bikel</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：easily accessible nowadays, impact user experience, significantly impact user, large language models, accessible nowadays</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As large language models (LLMs) become easily accessible nowadays, the trade-off between safety and helpfulness can significantly impact user experience. A model that prioritizes safety will cause users to feel less engaged and assisted while prioritizing helpfulness will potentially cause harm. Possible harms include teaching people how to build a bomb, exposing youth to inappropriate content, and hurting users' mental health. In this work, we propose to balance safety and helpfulness in diverse use cases by controlling both attributes in LLM. We explore training-free and fine-tuning methods that do not require extra human annotations and analyze the challenges of controlling safety and helpfulness in LLMs. Our experiments demonstrate that our method can rewind a learned model and unlock its controllability.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Evaluating Text-to-Visual Generation with Image-to-Text Generation</b></summary>
  <p><b>编号</b>：[9]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01291">https://arxiv.org/abs/2404.01291</a></p>
  <p><b>作者</b>：Zhiqiu Lin,  Deepak Pathak,  Baiqi Li,  Jiayao Li,  Xide Xia,  Graham Neubig,  Pengchuan Zhang,  Deva Ramanan</p>
  <p><b>备注</b>：We open-source our data, model, and code at: this https URL ; Project page: this https URL</p>
  <p><b>关键词</b>：comprehensive evaluation remains, evaluation remains challenging, comprehensive evaluation, significant progress, progress in generative</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite significant progress in generative AI, comprehensive evaluation remains challenging because of the lack of effective metrics and standardized benchmarks. For instance, the widely-used CLIPScore measures the alignment between a (generated) image and text prompt, but it fails to produce reliable scores for complex prompts involving compositions of objects, attributes, and relations. One reason is that text encoders of CLIP can notoriously act as a "bag of words", conflating prompts such as "the horse is eating the grass" with "the grass is eating the horse". To address this, we introduce the VQAScore, which uses a visual-question-answering (VQA) model to produce an alignment score by computing the probability of a "Yes" answer to a simple "Does this figure show '{text}'?" question. Though simpler than prior art, VQAScore computed with off-the-shelf models produces state-of-the-art results across many (8) image-text alignment benchmarks. We also compute VQAScore with an in-house model that follows best practices in the literature. For example, we use a bidirectional image-question encoder that allows image embeddings to depend on the question being asked (and vice versa). Our in-house model, CLIP-FlanT5, outperforms even the strongest baselines that make use of the proprietary GPT-4V. Interestingly, although we train with only images, VQAScore can also align text with video and 3D models. VQAScore allows researchers to benchmark text-to-visual generation using complex texts that capture the compositional structure of real-world prompts. We introduce GenAI-Bench, a more challenging benchmark with 1,600 compositional text prompts that require parsing scenes, objects, attributes, relationships, and high-order reasoning like comparison and logic. GenAI-Bench also offers over 15,000 human ratings for leading image and video generation models such as Stable Diffusion, DALL-E 3, and Gen2.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Large Language Models are Capable of Offering Cognitive Reappraisal, if  Guided</b></summary>
  <p><b>编号</b>：[10]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01288">https://arxiv.org/abs/2404.01288</a></p>
  <p><b>作者</b>：Hongli Zhan,  Allen Zheng,  Yoon Kyung Lee,  Jina Suh,  Junyi Jessy Li,  Desmond C. Ong</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large language models, produce empathic responses, people in distress, offered new opportunities, produce empathic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have offered new opportunities for emotional support, and recent work has shown that they can produce empathic responses to people in distress. However, long-term mental well-being requires emotional self-regulation, where a one-time empathic response falls short. This work takes a first step by engaging with cognitive reappraisals, a strategy from psychology practitioners that uses language to targetedly change negative appraisals that an individual makes of the situation; such appraisals is known to sit at the root of human emotional experience. We hypothesize that psychologically grounded principles could enable such advanced psychology capabilities in LLMs, and design RESORT which consists of a series of reappraisal constitutions across multiple dimensions that can be used as LLM instructions. We conduct a first-of-its-kind expert evaluation (by clinical psychologists with M.S. or Ph.D. degrees) of an LLM's zero-shot ability to generate cognitive reappraisal responses to medium-length social media messages asking for support. This fine-grained evaluation showed that even LLMs at the 7B scale guided by RESORT are capable of generating empathic responses that can help users reappraise their situations.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：TWIN-GPT: Digital Twins for Clinical Trials via Large Language Model</b></summary>
  <p><b>编号</b>：[16]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01273">https://arxiv.org/abs/2404.01273</a></p>
  <p><b>作者</b>：Yue Wang,  Yingzhou Lu,  Yinlong Xu,  Zihan Ma,  Hongxia Xu,  Bang Du,  Honghao Gao,  Jian Wu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：simulate real-world scenarios, clinical trial outcome, enhance patient safety, broader scientific knowledge, virtual clinical trials</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, there has been a burgeoning interest in virtual clinical trials, which simulate real-world scenarios and hold the potential to significantly enhance patient safety, expedite development, reduce costs, and contribute to the broader scientific knowledge in healthcare. Existing research often focuses on leveraging electronic health records (EHRs) to support clinical trial outcome prediction. Yet, trained with limited clinical trial outcome data, existing approaches frequently struggle to perform accurate predictions. Some research has attempted to generate EHRs to augment model development but has fallen short in personalizing the generation for individual patient profiles. Recently, the emergence of large language models has illuminated new possibilities, as their embedded comprehensive clinical knowledge has proven beneficial in addressing medical issues. In this paper, we propose a large language model-based digital twin creation approach, called TWIN-GPT. TWIN-GPT can establish cross-dataset associations of medical information given limited data, generating unique personalized digital twins for different patients, thereby preserving individual patient characteristics. Comprehensive experiments show that using digital twins created by TWIN-GPT can boost clinical trial outcome prediction, exceeding various previous prediction approaches. Besides, we also demonstrate that TWIN-GPT can generate high-fidelity trial data that closely approximate specific patients, aiding in more accurate result predictions in data-scarce situations. Moreover, our study provides practical evidence for the application of digital twins in healthcare, highlighting its potential significance.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Mapping the Increasing Use of LLMs in Scientific Papers</b></summary>
  <p><b>编号</b>：[19]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01268">https://arxiv.org/abs/2404.01268</a></p>
  <p><b>作者</b>：Weixin Liang,  Yaohui Zhang,  Zhengxuan Wu,  Haley Lepp,  Wenlong Ji,  Xuandong Zhao,  Hancheng Cao,  Sheng Liu,  Siyu He,  Zhi Huang,  Diyi Yang,  Christopher Potts,  Christopher D Manning,  James Y. Zou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Scientific publishing lays, fostering collaboration, encouraging reproducibility, knowledge is accessible, publishing lays</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time. Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices. However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs. To address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time. Our statistical estimation operates on the corpus level and is more robust than inference on individual instances. Our findings reveal a steady increase in LLM usage, with the largest and fastest growth observed in Computer Science papers (up to 17.5%). In comparison, Mathematics papers and the Nature portfolio showed the least LLM modification (up to 6.3%). Moreover, at an aggregate level, our analysis reveals that higher levels of LLM-modification are associated with papers whose first authors post preprints more frequently, papers in more crowded research areas, and papers of shorter lengths. Our findings suggests that LLMs are being broadly used in scientific writings.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic  Representations</b></summary>
  <p><b>编号</b>：[20]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01266">https://arxiv.org/abs/2404.01266</a></p>
  <p><b>作者</b>：Deqing Fu,  Ghazal Khalighinejad,  Ollie Liu,  Bhuwan Dhingra,  Dani Yogatama,  Robin Jia,  Willie Neiswanger</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：exhibit impressive capabilities, models exhibit impressive, exhibit impressive, impressive capabilities, Current foundation models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Current foundation models exhibit impressive capabilities when prompted either with text only or with both image and text inputs. But do their capabilities change depending on the input modality? In this work, we propose $\textbf{IsoBench}$, a benchmark dataset containing problems from four major areas: math, science, algorithms, and games. Each example is presented with multiple $\textbf{isomorphic representations}$ of inputs, such as visual, textual, and mathematical presentations. IsoBench provides fine-grained feedback to diagnose performance gaps caused by the form of the representation. Across various foundation models, we observe that on the same problem, models have a consistent preference towards textual representations. Most prominently, when evaluated on all IsoBench problems, Claude-3 Opus performs 28.7 points worse when provided with images instead of text; similarly, GPT-4 Turbo is 18.7 points worse and Gemini Pro is 14.9 points worse. Finally, we present two prompting techniques, $\textit{IsoCombination}$ and $\textit{IsoScratchPad}$, which improve model performance by considering combinations of, and translations between, different input representations.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Artificial Intelligence and the Spatial Documentation of Languages</b></summary>
  <p><b>编号</b>：[21]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01263">https://arxiv.org/abs/2404.01263</a></p>
  <p><b>作者</b>：Hakam Ghanim</p>
  <p><b>备注</b>：29 pages, 1 figure, 16 maps</p>
  <p><b>关键词</b>：made interdisciplinary research, research more accessible, advancement in technology, technology has made, GPT Data Analyst</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The advancement in technology has made interdisciplinary research more accessible. Particularly the breakthrough in Artificial Intelligence AI has given huge advantages to researchers working in interdisciplinary and multidisciplinary fields. This study investigates the ability of AI models, particularly GPT4 and GPT Data Analyst in creating language maps for language documentation. The study Integrates documentary linguistics linguistic geography and AI by showcasing how AI models facilitate the spatial documentation of languages through the creation of language maps with minimal cartographic expertise. The study is conducted using a CSV file and a GeoJSON file both obtained from HDX and from the researchers fieldwork. The study data is then applied in realtime conversations with the AI models in order to generate the language distribution maps. The study highlights the two AI models capabilities in generating highquality static and interactive web maps and streamlining the mapmaking process, despite facing challenges like inconsistencies and difficulties in adding legends. The findings suggest a promising future for AI in generating language maps and enhancing the work of documentary linguists as they collect their data in the field pointing towards the need for further development to fully harness AI potential in this field.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：FABLES: Evaluating faithfulness and content selection in book-length  summarization</b></summary>
  <p><b>编号</b>：[22]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01261">https://arxiv.org/abs/2404.01261</a></p>
  <p><b>作者</b>：Yekyung Kim,  Yapei Chang,  Marzena Karpinska,  Aparna Garimella,  Varun Manjunatha,  Kyle Lo,  Tanya Goyal,  Mohit Iyyer</p>
  <p><b>备注</b>：preprint - 39 pages</p>
  <p><b>关键词</b>：large language models, summarize book-length documents, long-context large language, technically summarize book-length, language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While long-context large language models (LLMs) can technically summarize book-length documents (>100K tokens), the length and complexity of the documents have so far prohibited evaluations of input-dependent aspects like faithfulness. In this paper, we conduct the first large-scale human evaluation of faithfulness and content selection on LLM-generated summaries of fictional books. Our study mitigates the issue of data contamination by focusing on summaries of books published in 2023 or 2024, and we hire annotators who have fully read each book prior to the annotation task to minimize cost and cognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims made in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which allows us to rank LLM summarizers based on faithfulness: Claude-3-Opus significantly outperforms all closed-source LLMs, while the open-source Mixtral is on par with GPT-3.5-Turbo. An analysis of the annotations reveals that most unfaithful claims relate to events and character states, and they generally require indirect reasoning over the narrative to invalidate. While LLM-based auto-raters have proven reliable for factuality and coherence in other settings, we implement several LLM raters of faithfulness and find that none correlates strongly with human annotations, especially with regard to detecting unfaithful claims. Our experiments suggest that detecting unfaithful claims is an important future direction not only for summarization evaluation but also as a testbed for long-context understanding. Finally, we move beyond faithfulness by exploring content selection errors in book-length summarization: we develop a typology of omission errors related to crucial narrative elements and also identify a systematic over-emphasis on events occurring towards the end of the book.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：UniArk: Improving Generalisation and Consistency for Factual Knowledge  Extraction through Debiasing</b></summary>
  <p><b>编号</b>：[26]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01253">https://arxiv.org/abs/2404.01253</a></p>
  <p><b>作者</b>：Yijun Yang,  Jie He,  Pinzhen Chen,  Víctor Gutiérrez-Basulto,  Jeff Z. Pan</p>
  <p><b>备注</b>：NAACL 2024</p>
  <p><b>关键词</b>：extracting factual knowledge, recent papers, papers have investigated, investigated the potential, existence of severe</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Several recent papers have investigated the potential of language models as knowledge bases as well as the existence of severe biases when extracting factual knowledge. In this work, we focus on the factual probing performance over unseen prompts from tuning, and using a probabilistic view we show the inherent misalignment between pre-training and downstream tuning objectives in language models for probing knowledge. We hypothesize that simultaneously debiasing these objectives can be the key to generalisation over unseen prompts. We propose an adapter-based framework, UniArk, for generalised and consistent factual knowledge extraction through simple methods without introducing extra parameters. Extensive experiments show that UniArk can significantly improve the model's out-of-domain generalisation as well as consistency under various prompts. Additionally, we construct ParaTrex, a large-scale and diverse dataset for measuring the inconsistency and out-of-domain generation of models. Further, ParaTrex offers a reference method for constructing paraphrased datasets using large language models.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：An image speaks a thousand words, but can everyone listen? On  translating images for cultural relevance</b></summary>
  <p><b>编号</b>：[30]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01247">https://arxiv.org/abs/2404.01247</a></p>
  <p><b>作者</b>：Simran Khanuja,  Sathyanarayanan Ramamoorthy,  Yueqi Song,  Graham Neubig</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：translators increasingly focus, human translators increasingly, multimedia content, rise of multimedia, translators increasingly</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Given the rise of multimedia content, human translators increasingly focus on culturally adapting not only words but also other modalities such as images to convey the same meaning. While several applications stand to benefit from this, machine translation systems remain confined to dealing with language in speech and text. In this work, we take a first step towards translating images to make them culturally relevant. First, we build three pipelines comprising state-of-the-art generative models to do the task. Next, we build a two-part evaluation dataset: i) concept: comprising 600 images that are cross-culturally coherent, focusing on a single concept per image, and ii) application: comprising 100 images curated from real-world applications. We conduct a multi-faceted human evaluation of translated images to assess for cultural relevance and meaning preservation. We find that as of today, image-editing models fail at this task, but can be improved by leveraging LLMs and retrievers in the loop. Best pipelines can only translate 5% of images for some countries in the easier concept dataset and no translation is successful for some countries in the application dataset, highlighting the challenging nature of the task. Our code and data is released here: this https URL.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Effectively Prompting Small-sized Language Models for Cross-lingual  Tasks via Winning Tickets</b></summary>
  <p><b>编号</b>：[32]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01242">https://arxiv.org/abs/2404.01242</a></p>
  <p><b>作者</b>：Mingqi Li,  Feng Luo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：methods yield limited, yield limited performance, prompt methods yield, Current soft prompt, soft prompt methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Current soft prompt methods yield limited performance when applied to small-sized models (fewer than a billion parameters). Deep prompt-tuning, which entails prepending parameters in each layer for enhanced efficacy, presents a solution for prompting small-sized models, albeit requiring carefully designed implementation. In this paper, we introduce the Lottery Ticket Prompt-learning (LTP) framework that integrates winning tickets with soft prompts. The LTP offers a simpler implementation and requires only a one-time execution. We demonstrate LTP on cross-lingual tasks, where prior works rely on external tools like human-designed multilingual templates and bilingual dictionaries, which may not be feasible in a low-resource regime. Specifically, we select a subset of parameters that have been changed the most during the fine-tuning with the Masked Language Modeling objective. Then, we prepend soft prompts to the original pre-trained language model and only update the selected parameters together with prompt-related parameters when adapting to the downstream tasks. We verify the effectiveness of our LTP framework on cross-lingual tasks, specifically targeting low-resource languages. Our approach outperforms the baselines by only updating 20\% of the original parameters.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：AURORA: Navigating UI Tarpits via Automated Neural Screen Understanding</b></summary>
  <p><b>编号</b>：[34]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01240">https://arxiv.org/abs/2404.01240</a></p>
  <p><b>作者</b>：Safwat Ali Khan,  Wenyu Wang,  Yiran Ren,  Bin Zhu,  Jiangfan Shi,  Alyssa McGowan,  Wing Lam,  Kevin Moran</p>
  <p><b>备注</b>：Published at 17th IEEE International Conference on Software Testing, Verification and Validation (ICST) 2024, 12 pages</p>
  <p><b>关键词</b>：Automated Input Generation, Input Generation tools, software platform, software engineering, Input Generation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Nearly a decade of research in software engineering has focused on automating mobile app testing to help engineers in overcoming the unique challenges associated with the software platform. Much of this work has come in the form of Automated Input Generation tools (AIG tools) that dynamically explore app screens. However, such tools have repeatedly been demonstrated to achieve lower-than-expected code coverage - particularly on sophisticated proprietary apps. Prior work has illustrated that a primary cause of these coverage deficiencies is related to so-called tarpits, or complex screens that are difficult to navigate.
In this paper, we take a critical step toward enabling AIG tools to effectively navigate tarpits during app exploration through a new form of automated semantic screen understanding. We introduce AURORA, a technique that learns from the visual and textual patterns that exist in mobile app UIs to automatically detect common screen designs and navigate them accordingly. The key idea of AURORA is that there are a finite number of mobile app screen designs, albeit with subtle variations, such that the general patterns of different categories of UI designs can be learned. As such, AURORA employs a multi-modal, neural screen classifier that is able to recognize the most common types of UI screen designs. After recognizing a given screen, it then applies a set of flexible and generalizable heuristics to properly navigate the screen. We evaluated AURORA both on a set of 12 apps with known tarpits from prior work, and on a new set of five of the most popular apps from the Google Play store. Our results indicate that AURORA is able to effectively navigate tarpit screens, outperforming prior approaches that avoid tarpits by 19.6% in terms of method coverage. The improvements can be attributed to AURORA's UI design classification and heuristic navigation techniques.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：GFLean: An Autoformalisation Framework for Lean via GF</b></summary>
  <p><b>编号</b>：[36]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01234">https://arxiv.org/abs/2404.01234</a></p>
  <p><b>作者</b>：Shashank Pathak</p>
  <p><b>备注</b>：19 Pages, 3 Figures</p>
  <p><b>关键词</b>：Lean theorem prover, Lean theorem, theorem prover, called Grammatical Framework, tool called Grammatical</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present an autoformalisation framework for the Lean theorem prover, called GFLean. GFLean uses a high-level grammar writing tool called Grammatical Framework (GF) for parsing and linearisation. GFLean is implemented in Haskell. We explain the functionalities of GFLean, its inner working and discuss its limitations. We also discuss how we can use neural network based translation programs and rule based translation programs together complimenting each other to build robust autoformalisation frameworks.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Open-Vocabulary Federated Learning with Multimodal Prototyping</b></summary>
  <p><b>编号</b>：[37]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01232">https://arxiv.org/abs/2404.01232</a></p>
  <p><b>作者</b>：Huimin Zeng,  Zhenrui Yue,  Dong Wang</p>
  <p><b>备注</b>：Accepted at NAACL 204</p>
  <p><b>关键词</b>：training label space, test label space, label space, Existing federated learning, training label</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing federated learning (FL) studies usually assume the training label space and test label space are identical. However, in real-world applications, this assumption is too ideal to be true. A new user could come up with queries that involve data from unseen classes, and such open-vocabulary queries would directly defect such FL systems. Therefore, in this work, we explicitly focus on the under-explored open-vocabulary challenge in FL. That is, for a new user, the global server shall understand her/his query that involves arbitrary unknown classes. To address this problem, we leverage the pre-trained vision-language models (VLMs). In particular, we present a novel adaptation framework tailored for VLMs in the context of FL, named as Federated Multimodal Prototyping (Fed-MP). Fed-MP adaptively aggregates the local model weights based on light-weight client residuals, and makes predictions based on a novel multimodal prototyping mechanism. Fed-MP exploits the knowledge learned from the seen classes, and robustifies the adapted VLM to unseen categories. Our empirical evaluation on various datasets validates the effectiveness of Fed-MP.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language  Models</b></summary>
  <p><b>编号</b>：[39]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01230">https://arxiv.org/abs/2404.01230</a></p>
  <p><b>作者</b>：Yadong Zhang,  Shaoguang Mao,  Tao Ge,  Xun Wang,  Adrian de Wynter,  Yan Xia,  Wenshan Wu,  Ting Song,  Man Lan,  Furu Wei</p>
  <p><b>备注</b>：9 pages, 5 figures</p>
  <p><b>关键词</b>：Large Language Models, predicting adversary actions, Language Models, Large Language, opportunities for Large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a comprehensive survey of the current status and opportunities for Large Language Models (LLMs) in strategic reasoning, a sophisticated form of reasoning that necessitates understanding and predicting adversary actions in multi-agent settings while adjusting strategies accordingly. Strategic reasoning is distinguished by its focus on the dynamic and uncertain nature of interactions among multi-agents, where comprehending the environment and anticipating the behavior of others is crucial. We explore the scopes, applications, methodologies, and evaluation metrics related to strategic reasoning with LLMs, highlighting the burgeoning development in this area and the interdisciplinary approaches enhancing their decision-making performance. It aims to systematize and clarify the scattered literature on this subject, providing a systematic review that underscores the importance of strategic reasoning as a critical cognitive capability and offers insights into future research directions and potential improvements.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Stable Code Technical Report</b></summary>
  <p><b>编号</b>：[42]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01226">https://arxiv.org/abs/2404.01226</a></p>
  <p><b>作者</b>：Nikhil Pinnaparaju,  Reshinth Adithyan,  Duy Phung,  Jonathan Tow,  James Baicoianu,  Ashish Datta,  Maksym Zhuravinskyi,  Dakota Mahan,  Marco Bellagente,  Carlos Riquelme,  Nathan Cooper</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：base code language, Stable Code Instruct, general-purpose base code, language models series, Stable Code</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce Stable Code, the first in our new-generation of code language models series, which serves as a general-purpose base code language model targeting code completion, reasoning, math, and other software engineering-based tasks. Additionally, we introduce an instruction variant named Stable Code Instruct that allows conversing with the model in a natural chat interface for performing question-answering and instruction-based tasks. In this technical report, we detail the data and training procedure leading to both models. Their weights are available via Hugging Face for anyone to download and use at this https URL and this https URL. This report contains thorough evaluations of the models, including multilingual programming benchmarks, and the MT benchmark focusing on multi-turn dialogues. At the time of its release, Stable Code is the state-of-the-art open model under 3B parameters and even performs comparably to larger models of sizes 7 billion and 15 billion parameters on the popular Multi-PL benchmark. Stable Code Instruct also exhibits state-of-the-art performance on the MT-Bench coding tasks and on Multi-PL completion compared to other instruction tuned models. Given its appealing small size, we also provide throughput measurements on a number of edge devices. In addition, we open source several quantized checkpoints and provide their performance metrics compared to the original model.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：AILS-NTUA at SemEval-2024 Task 6: Efficient model tuning for  hallucination detection and analysis</b></summary>
  <p><b>编号</b>：[51]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01210">https://arxiv.org/abs/2404.01210</a></p>
  <p><b>作者</b>：Natalia Griogoriadou,  Maria Lymperaiou,  Giorgos Filandrianos,  Giorgos Stamou</p>
  <p><b>备注</b>：SemEval-2024</p>
  <p><b>关键词</b>：Observable Overgeneration Mistakes, Related Observable Overgeneration, Related Observable, Overgeneration Mistakes, Observable Overgeneration</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we present our team's submissions for SemEval-2024 Task-6 - SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes. The participants were asked to perform binary classification to identify cases of fluent overgeneration hallucinations. Our experimentation included fine-tuning a pre-trained model on hallucination detection and a Natural Language Inference (NLI) model. The most successful strategy involved creating an ensemble of these models, resulting in accuracy rates of 77.8% and 79.9% on model-agnostic and model-aware datasets respectively, outperforming the organizers' baseline and achieving notable results when contrasted with the top-performing results in the competition, which reported accuracies of 84.7% and 81.3% correspondingly.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：The Fine Line: Navigating Large Language Model Pretraining with  Down-streaming Capability Analysis</b></summary>
  <p><b>编号</b>：[55]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01204">https://arxiv.org/abs/2404.01204</a></p>
  <p><b>作者</b>：Chen Yang,  Junzhuo Li,  Xinyao Niu,  Xinrun Du,  Songyang Gao,  Haoran Zhang,  Zhaoliang Chen,  Xingwei Qu,  Ruibin Yuan,  Yizhi Li,  Jiaheng Liu,  Stephen W. Huang,  Shawn Yue,  Wenhu Chen,  Jie Fu,  Ge Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：reflect final model, Uncovering early-stage metrics, Uncovering early-stage, reflect final, final model performance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Uncovering early-stage metrics that reflect final model performance is one core principle for large-scale pretraining. The existing scaling law demonstrates the power-law correlation between pretraining loss and training flops, which serves as an important indicator of the current training state for large language models. However, this principle only focuses on the model's compression properties on the training data, resulting in an inconsistency with the ability improvements on the downstream tasks. Some follow-up works attempted to extend the scaling-law to more complex metrics (such as hyperparameters), but still lacked a comprehensive analysis of the dynamic differences among various capabilities during pretraining. To address the aforementioned limitations, this paper undertakes a comprehensive comparison of model capabilities at various pretraining intermediate checkpoints. Through this analysis, we confirm that specific downstream metrics exhibit similar training dynamics across models of different sizes, up to 67 billion parameters. In addition to our core findings, we've reproduced Amber and OpenLLaMA, releasing their intermediate checkpoints. This initiative offers valuable resources to the research community and facilitates the verification and exploration of LLM pretraining by open-source researchers. Besides, we provide empirical summaries, including performance comparisons of different models and capabilities, and tuition of key metrics for different training phases. Based on these findings, we provide a more user-friendly strategy for evaluating the optimization state, offering guidance for establishing a stable pretraining process.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Estimating Lexical Complexity from Document-Level Distributions</b></summary>
  <p><b>编号</b>：[59]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01196">https://arxiv.org/abs/2404.01196</a></p>
  <p><b>作者</b>：Sondre Wold,  Petter Mæhlum,  Oddbjørn Hove</p>
  <p><b>备注</b>：LREC-COLING 2024</p>
  <p><b>关键词</b>：entire documents, developed for entire, Existing methods, assessment tools, typically developed</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing methods for complexity estimation are typically developed for entire documents. This limitation in scope makes them inapplicable for shorter pieces of text, such as health assessment tools. These typically consist of lists of independent sentences, all of which are too short for existing methods to apply. The choice of wording in these assessment tools is crucial, as both the cognitive capacity and the linguistic competency of the intended patient groups could vary substantially. As a first step towards creating better tools for supporting health practitioners, we develop a two-step approach for estimating lexical complexity that does not rely on any pre-annotated data. We implement our approach for the Norwegian language and verify its effectiveness using statistical testing and a qualitative evaluation of samples from real assessment tools. We also investigate the relationship between our complexity measure and certain features typically associated with complexity in the literature, such as word length, frequency, and the number of syllables.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Generating Faithful and Complete Hospital-Course Summaries from the  Electronic Health Record</b></summary>
  <p><b>编号</b>：[61]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01189">https://arxiv.org/abs/2404.01189</a></p>
  <p><b>作者</b>：Griffin Adams</p>
  <p><b>备注</b>：PhD thesis</p>
  <p><b>关键词</b>：Electronic Health Records, Health Records, Electronic Health, streamlining administrative tasks, adoption of Electronic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rapid adoption of Electronic Health Records (EHRs) has been instrumental in streamlining administrative tasks, increasing transparency, and enabling continuity of care across providers. An unintended consequence of the increased documentation burden, however, has been reduced face-time with patients and, concomitantly, a dramatic rise in clinician burnout. In this thesis, we pinpoint a particularly time-intensive, yet critical, documentation task: generating a summary of a patient's hospital admissions, and propose and evaluate automated solutions. In Chapter 2, we construct a dataset based on 109,000 hospitalizations (2M source notes) and perform exploratory analyses to motivate future work on modeling and evaluation [NAACL 2021]. In Chapter 3, we address faithfulness from a modeling perspective by revising noisy references [EMNLP 2022] and, to reduce the reliance on references, directly calibrating model outputs to metrics [ACL 2023]. These works relied heavily on automatic metrics as human annotations were limited. To fill this gap, in Chapter 4, we conduct a fine-grained expert annotation of system errors in order to meta-evaluate existing metrics and better understand task-specific issues of domain adaptation and source-summary alignments. To learn a metric less correlated to extractiveness (copy-and-paste), we derive noisy faithfulness labels from an ensemble of existing metrics and train a faithfulness classifier on these pseudo labels [MLHC 2023]. Finally, in Chapter 5, we demonstrate that fine-tuned LLMs (Mistral and Zephyr) are highly prone to entity hallucinations and cover fewer salient entities. We improve both coverage and faithfulness by performing sentence-level entity planning based on a set of pre-computed salient entities from the source text, which extends our work on entity-guided news summarization [ACL, 2023], [EMNLP, 2023].</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：A Neuro-Symbolic Approach to Monitoring Salt Content in Food</b></summary>
  <p><b>编号</b>：[64]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01182">https://arxiv.org/abs/2404.01182</a></p>
  <p><b>作者</b>：Anuja Tayal,  Barbara Di Eugenio,  Devika Salunke,  Andrew D. Boyd,  Carolyn A Dickens,  Eulalia P Abril,  Olga Garcia-Bedoya,  Paula G Allen-Meares</p>
  <p><b>备注</b>：Accepted in CL4Health workshop in LREC-COLING'24</p>
  <p><b>关键词</b>：enables heart failure, heart failure patients, reduce salt intake, propose a dialogue, enables heart</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a dialogue system that enables heart failure patients to inquire about salt content in foods and help them monitor and reduce salt intake. Addressing the lack of specific datasets for food-based salt content inquiries, we develop a template-based conversational dataset. The dataset is structured to ask clarification questions to identify food items and their salt content. Our findings indicate that while fine-tuning transformer-based models on the dataset yields limited performance, the integration of Neuro-Symbolic Rules significantly enhances the system's performance. Our experiments show that by integrating neuro-symbolic rules, our system achieves an improvement in joint goal accuracy of over 20% across different data sizes compared to naively fine-tuning transformer-based models.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：LITE: Modeling Environmental Ecosystems with Multimodal Large Language  Models</b></summary>
  <p><b>编号</b>：[72]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01165">https://arxiv.org/abs/2404.01165</a></p>
  <p><b>作者</b>：Haoran Li,  Junqi Liu,  Zexian Wang,  Shiyuan Luo,  Xiaowei Jia,  Huaxiu Yao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：plays a pivotal, pivotal role, sustainable management, environmental ecosystems plays, environmental</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The modeling of environmental ecosystems plays a pivotal role in the sustainable management of our planet. Accurate prediction of key environmental variables over space and time can aid in informed policy and decision-making, thus improving people's livelihood. Recently, deep learning-based methods have shown promise in modeling the spatial-temporal relationships for predicting environmental variables. However, these approaches often fall short in handling incomplete features and distribution shifts, which are commonly observed in environmental data due to the substantial cost of data collection and malfunctions in measuring instruments. To address these issues, we propose LITE -- a multimodal large language model for environmental ecosystems modeling. Specifically, LITE unifies different environmental variables by transforming them into natural language descriptions and line graph images. Then, LITE utilizes unified encoders to capture spatial-temporal dynamics and correlations in different modalities. During this step, the incomplete features are imputed by a sparse Mixture-of-Experts framework, and the distribution shift is handled by incorporating multi-granularity information from past observations. Finally, guided by domain instructions, a language model is employed to fuse the multimodal representations for the prediction. Our experiments demonstrate that LITE significantly enhances performance in environmental spatial-temporal prediction across different domains compared to the best baseline, with a 41.25% reduction in prediction error. This justifies its effectiveness. Our data and code are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Dialogue with Robots: Proposals for Broadening Participation and  Research in the SLIVAR Community</b></summary>
  <p><b>编号</b>：[77]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01158">https://arxiv.org/abs/2404.01158</a></p>
  <p><b>作者</b>：Casey Kennington,  Malihe Alikhani,  Heather Pon-Barry,  Katherine Atwell,  Yonatan Bisk,  Daniel Fried,  Felix Gervits,  Zhao Han,  Mert Inan,  Michael Johnston,  Raj Korpan,  Diane Litman,  Matthew Marge,  Cynthia Matuszek,  Ross Mead,  Shiwali Mohan,  Raymond Mooney,  Natalie Parde,  Jivko Sinapov,  Angela Stewart,  Matthew Stone,  Stefanie Tellex,  Tom Williams</p>
  <p><b>备注</b>：NSF Report on the "Dialogue with Robots" Workshop held in Pittsburg, PA, April 2023</p>
  <p><b>关键词</b>：natural human language, ability to interact, natural human, machines including robots, human language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The ability to interact with machines using natural human language is becoming not just commonplace, but expected. The next step is not just text interfaces, but speech interfaces and not just with computers, but with all machines including robots. In this paper, we chronicle the recent history of this growing field of spoken dialogue with robots and offer the community three proposals, the first focused on education, the second on benchmarks, and the third on the modeling of language when it comes to spoken interaction with robots. The three proposals should act as white papers for any researcher to take and build upon.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Green AI: Exploring Carbon Footprints, Mitigation Strategies, and Trade  Offs in Large Language Model Training</b></summary>
  <p><b>编号</b>：[78]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01157">https://arxiv.org/abs/2404.01157</a></p>
  <p><b>作者</b>：Vivian Liu,  Yiqiao Yin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Natural Language Processing, altering model architecture, Processing have long, field of Natural, Natural Language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Prominent works in the field of Natural Language Processing have long attempted to create new innovative models by improving upon previous model training approaches, altering model architecture, and developing more in-depth datasets to better their performance. However, with the quickly advancing field of NLP comes increased greenhouse gas emissions, posing concerns over the environmental damage caused by training LLMs. Gaining a comprehensive understanding of the various costs, particularly those pertaining to environmental aspects, that are associated with artificial intelligence serves as the foundational basis for ensuring safe AI models. Currently, investigations into the CO2 emissions of AI models remain an emerging area of research, and as such, in this paper, we evaluate the CO2 emissions of well-known large language models, which have an especially high carbon footprint due to their significant amount of model parameters. We argue for the training of LLMs in a way that is responsible and sustainable by suggesting measures for reducing carbon emissions. Furthermore, we discuss how the choice of hardware affects CO2 emissions by contrasting the CO2 emissions during model training for two widely used GPUs. Based on our results, we present the benefits and drawbacks of our proposed solutions and make the argument for the possibility of training more environmentally safe AI models without sacrificing their robustness and performance.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Do LLMs Find Human Answers To Fact-Driven Questions Perplexing? A Case  Study on Reddit</b></summary>
  <p><b>编号</b>：[85]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01147">https://arxiv.org/abs/2404.01147</a></p>
  <p><b>作者</b>：Parker Seegmiller,  Joseph Gatto,  Omar Sharif,  Madhusudan Basak,  Sarah Masud Preum</p>
  <p><b>备注</b>：4 pages, 2 figures</p>
  <p><b>关键词</b>：Large language models, correctly answering questions, Large language, online discourse, proficient in correctly</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have been shown to be proficient in correctly answering questions in the context of online discourse. However, the study of using LLMs to model human-like answers to fact-driven social media questions is still under-explored. In this work, we investigate how LLMs model the wide variety of human answers to fact-driven questions posed on several topic-specific Reddit communities, or subreddits. We collect and release a dataset of 409 fact-driven questions and 7,534 diverse, human-rated answers from 15 r/Ask{Topic} communities across 3 categories: profession, social identity, and geographic location. We find that LLMs are considerably better at modeling highly-rated human answers to such questions, as opposed to poorly-rated human answers. We present several directions for future research based on our initial findings.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：KoCoNovel: Annotated Dataset of Character Coreference in Korean Novels</b></summary>
  <p><b>编号</b>：[89]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01140">https://arxiv.org/abs/2404.01140</a></p>
  <p><b>作者</b>：Kyuhee Kim,  Surin Lee,  Sangah Lee</p>
  <p><b>备注</b>：12 pages</p>
  <p><b>关键词</b>：detailed annotation guidelines, Korean literary texts, literary texts, complete with detailed, annotation guidelines</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present KoCoNovel, an novel character coreference dataset derived from Korean literary texts, complete with detailed annotation guidelines. Comprising 178K tokens from 50 modern and contemporary Korean novels, KoCoNovel stands as the second-largest public coreference resolution corpus in Korean, after the NIKL corpus, and the first to be based on literary texts. To broaden its utility, we provide four distinct versions of KoCoNovel, offering options for the perspectives of the omniscient author and readers, and for handling multiple entities as either separate or overlapping. This approach integrates existing discourse surrounding coreference resolution in literary texts, providing a comprehensive dataset for exploration. One of KoCoNovel's distinctive features is that 24% of all character mentions are single common nouns, lacking possessive markers or articles. This feature is particularly influenced by the nuances of Korean address term culture, which favors the use of terms denoting social relationships and kinship over personal names. In experiments with a BERT-based coreference model, we observed notable performance enhancements with KoCoNovel in comparison to the NIKL corpus. Such findings underscore KoCoNovel's potential to significantly enhance coreference resolution models through the integration of Korean cultural and linguistic dynamics.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Structured Information Matters: Incorporating Abstract Meaning  Representation into LLMs for Improved Open-Domain Dialogue Evaluation</b></summary>
  <p><b>编号</b>：[95]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01129">https://arxiv.org/abs/2404.01129</a></p>
  <p><b>作者</b>：Bohao Yang,  Kun Zhao,  Chen Tang,  Liang Zhan,  Chenghua Lin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：attracted increasing attention, open-domain dialogue evaluation, increasing attention, Automatic open-domain dialogue, open-domain dialogue</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automatic open-domain dialogue evaluation has attracted increasing attention. Trainable evaluation metrics are commonly trained with true positive and randomly selected negative responses, resulting in a tendency for them to assign a higher score to the responses that share higher content similarity with a given context. However, adversarial negative responses possess high content similarity with the contexts whilst being semantically different. Therefore, existing evaluation metrics are not robust enough to evaluate such responses, resulting in low correlations with human judgments. While recent studies have shown some efficacy in utilizing Large Language Models (LLMs) for open-domain dialogue evaluation, they still encounter challenges in effectively handling adversarial negative examples. In this paper, we propose a simple yet effective framework for open-domain dialogue evaluation, which combines domain-specific language models (SLMs) with LLMs. The SLMs can explicitly incorporate Abstract Meaning Representation (AMR) graph information of the dialogue through a gating mechanism for enhanced semantic representation learning. The evaluation result of SLMs and AMR graph information are plugged into the prompt of LLM, for the enhanced in-context learning performance. Experimental results on open-domain dialogue evaluation tasks demonstrate the superiority of our method compared to a wide range of state-of-the-art baselines, especially in discriminating adversarial negative responses. Our code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：SentiCSE: A Sentiment-aware Contrastive Sentence Embedding Framework  with Sentiment-guided Textual Similarity</b></summary>
  <p><b>编号</b>：[109]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01104">https://arxiv.org/abs/2404.01104</a></p>
  <p><b>作者</b>：Jaemin Kim,  Yohan Na,  Kangmin Kim,  Sang Rak Lee,  Dong-Kyu Chae</p>
  <p><b>备注</b>：14 pages, 8 figures</p>
  <p><b>关键词</b>：pre-trained language models, demonstrate impressive results, sentiment analysis tasks, sentiment-aware pre-trained language, language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, sentiment-aware pre-trained language models (PLMs) demonstrate impressive results in downstream sentiment analysis tasks. However, they neglect to evaluate the quality of their constructed sentiment representations; they just focus on improving the fine-tuning performance, which overshadows the representation quality. We argue that without guaranteeing the representation quality, their downstream performance can be highly dependent on the supervision of the fine-tuning data rather than representation quality. This problem would make them difficult to foray into other sentiment-related domains, especially where labeled data is scarce. We first propose Sentiment-guided Textual Similarity (SgTS), a novel metric for evaluating the quality of sentiment representations, which is designed based on the degree of equivalence in sentiment polarity between two sentences. We then propose SentiCSE, a novel Sentiment-aware Contrastive Sentence Embedding framework for constructing sentiment representations via combined word-level and sentence-level objectives, whose quality is guaranteed by SgTS. Qualitative and quantitative comparison with the previous sentiment-aware PLMs shows the superiority of our work. Our code is available at: this https URL</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：What's in Your "Safe" Data?: Identifying Benign Data that Breaks Safety</b></summary>
  <p><b>编号</b>：[113]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01099">https://arxiv.org/abs/2404.01099</a></p>
  <p><b>作者</b>：Luxi He,  Mengzhou Xia,  Peter Henderson</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Current Large Language, Large Language Models, Large Language, Current Large, Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Current Large Language Models (LLMs), even those tuned for safety and alignment, are susceptible to jailbreaking. Some have found that just further fine-tuning an aligned model with benign data (i.e., data without harmful content) surprisingly leads to substantial degradation in safety. We delve into the data-centric aspects of why benign fine-tuning inadvertently contributes to jailbreaking. First, we represent fine-tuning data through two lenses: representation and gradient spaces. Furthermore, we propose a bi-directional anchoring method that prioritizes data points that are close to harmful examples and distant from benign ones. By doing so, our approach effectively identifies subsets of benign data that are more likely to degrade the model's safety after fine-tuning. Training on just 100 of these seemingly benign datapoints can lead to the fine-tuned model affirmatively responding to > 70% of tested harmful requests, compared to < 20% after fine-tuning on randomly selected data. We further find that selected data are often in the form of lists and bullet points, or math questions.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：AILS-NTUA at SemEval-2024 Task 9: Cracking Brain Teasers: Transformer  Models for Lateral Thinking Puzzles</b></summary>
  <p><b>编号</b>：[118]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01084">https://arxiv.org/abs/2404.01084</a></p>
  <p><b>作者</b>：Ioannis Panagiotopoulos,  Giorgos Filandrianos,  Maria Lymperaiou,  Giorgos Stamou</p>
  <p><b>备注</b>：SemEval-2024</p>
  <p><b>关键词</b>：Defying Common Sense, Task Defying Common, Common Sense, Task Defying, Defying Common</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we outline our submission for the SemEval-2024 Task 9 competition: 'BRAINTEASER: A Novel Task Defying Common Sense'. We engage in both sub-tasks: Sub-task A-Sentence Puzzle and Sub-task B-Word Puzzle. We evaluate a plethora of pre-trained transformer-based language models of different sizes through fine-tuning. Subsequently, we undertake an analysis of their scores and responses to aid future researchers in understanding and utilizing these models effectively. Our top-performing approaches secured competitive positions on the competition leaderboard across both sub-tasks. In the evaluation phase, our best submission attained an average accuracy score of 81.7% in the Sentence Puzzle, and 85.4% in the Word Puzzle, significantly outperforming the best neural baseline (ChatGPT) by more than 20% and 30% respectively.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Efficient Prompting Methods for Large Language Models: A Survey</b></summary>
  <p><b>编号</b>：[123]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01077">https://arxiv.org/abs/2404.01077</a></p>
  <p><b>作者</b>：Kaiyan Chang,  Songcheng Xu,  Chenglong Wang,  Yingfeng Luo,  Tong Xiao,  Jingbo Zhu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：language processing tasks, adapting large language, specific natural language, natural language processing, large language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Prompting has become a mainstream paradigm for adapting large language models (LLMs) to specific natural language processing tasks. While this approach opens the door to in-context learning of LLMs, it brings the additional computational burden of model inference and human effort of manual-designed prompts, particularly when using lengthy and complex prompts to guide and control the behavior of LLMs. As a result, the LLM field has seen a remarkable surge in efficient prompting methods. In this paper, we present a comprehensive overview of these methods. At a high level, efficient prompting methods can broadly be categorized into two approaches: prompting with efficient computation and prompting with efficient design. The former involves various ways of compressing prompts, and the latter employs techniques for automatic prompt optimization. We present the basic concepts of prompting, review the advances for efficient prompting, and highlight future research directions.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Advancing AI with Integrity: Ethical Challenges and Solutions in Neural  Machine Translation</b></summary>
  <p><b>编号</b>：[126]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01070">https://arxiv.org/abs/2404.01070</a></p>
  <p><b>作者</b>：Richard Kimera,  Yun-Seon Kim,  Heeyoul Choi</p>
  <p><b>备注</b>：11 pages</p>
  <p><b>关键词</b>：Neural Machine Translation, Artificial Intelligence, Intelligence in Neural, Neural Machine, challenges of Artificial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper addresses the ethical challenges of Artificial Intelligence in Neural Machine Translation (NMT) systems, emphasizing the imperative for developers to ensure fairness and cultural sensitivity. We investigate the ethical competence of AI models in NMT, examining the Ethical considerations at each stage of NMT development, including data handling, privacy, data ownership, and consent. We identify and address ethical issues through empirical studies. These include employing Transformer models for Luganda-English translations and enhancing efficiency with sentence mini-batching. And complementary studies that refine data labeling techniques and fine-tune BERT and Longformer models for analyzing Luganda and English social media content. Our second approach is a literature review from databases such as Google Scholar and platforms like GitHub. Additionally, the paper probes the distribution of responsibility between AI systems and humans, underscoring the essential role of human oversight in upholding NMT ethical standards. Incorporating a biblical perspective, we discuss the societal impact of NMT and the broader ethical responsibilities of developers, positing them as stewards accountable for the societal repercussions of their creations.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Exploring the Mystery of Influential Data for Mathematical Reasoning</b></summary>
  <p><b>编号</b>：[127]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01067">https://arxiv.org/abs/2404.01067</a></p>
  <p><b>作者</b>：Xinzhe Ni,  Yeyun Gong,  Zhibin Gou,  Yelong Shen,  Yujiu Yang,  Nan Duan,  Weizhu Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：mathematical reasoning, computation efficiency, data, mathematical reasoning tasks, influential data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Selecting influential data for fine-tuning on downstream tasks is a key factor for both performance and computation efficiency. Recent works have shown that training with only limited data can show a superior performance on general tasks. However, the feasibility on mathematical reasoning tasks has not been validated. To go further, there exist two open questions for mathematical reasoning: how to select influential data and what is an influential data composition. For the former one, we propose a Quality-aware Diverse Selection (QaDS) strategy adaptable for mathematical reasoning. A comparison with other selection strategies validates the superiority of QaDS. For the latter one, we first enlarge our setting and explore the influential data composition. We conduct a series of experiments and highlight: scaling up reasoning data, and training with general data selected by QaDS is helpful. Then, we define our optimal mixture as OpenMathMix, an influential data mixture with open-source data selected by QaDS. With OpenMathMix, we achieve a state-of-the-art 48.8% accuracy on MATH with 7B base model. Additionally, we showcase the use of QaDS in creating efficient fine-tuning mixtures with various selection ratios, and analyze the quality of a wide range of open-source datasets, which can perform as a reference for future works on mathematical reasoning tasks.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language  Model Alignment</b></summary>
  <p><b>编号</b>：[136]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01054">https://arxiv.org/abs/2404.01054</a></p>
  <p><b>作者</b>：Yuu Jinnai,  Tetsuro Morimura,  Kaito Ariu,  Kenshi Abe</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：aligning Large Language, Large Language Models, aligning Large, Large Language, time of decoding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) to human preferences at the time of decoding. BoN sampling is susceptible to a problem known as reward hacking. Because the reward model is an imperfect proxy for the true objective, over-optimizing its value can compromise its performance on the true objective. A common solution to prevent reward hacking in preference learning techniques is to optimize a reward using proximity regularization (e.g., KL regularization), which ensures that the language model remains close to the reference model. In this research, we propose Regularized Best-of-N (RBoN), a variant of BoN that aims to mitigate reward hacking by incorporating a proximity term in response selection, similar to preference learning techniques. We evaluate two variants of RBoN on the AlpacaFarm dataset and find that they outperform BoN, especially when the proxy reward model has a low correlation with the true objective.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：ARAGOG: Advanced RAG Output Grading</b></summary>
  <p><b>编号</b>：[142]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01037">https://arxiv.org/abs/2404.01037</a></p>
  <p><b>作者</b>：Matouš Eibich,  Shivay Nagpal,  Alexander Fred-Ojala</p>
  <p><b>备注</b>：14 pages, 8 figures, associated Github repo: this https URL</p>
  <p><b>关键词</b>：Large Language Model, Language Model, Large Language, integrating external knowledge, Retrieval-Augmented Generation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Retrieval-Augmented Generation (RAG) is essential for integrating external knowledge into Large Language Model (LLM) outputs. While the literature on RAG is growing, it primarily focuses on systematic reviews and comparisons of new state-of-the-art (SoTA) techniques against their predecessors, with a gap in extensive experimental comparisons. This study begins to address this gap by assessing various RAG methods' impacts on retrieval precision and answer similarity. We found that Hypothetical Document Embedding (HyDE) and LLM reranking significantly enhance retrieval precision. However, Maximal Marginal Relevance (MMR) and Cohere rerank did not exhibit notable advantages over a baseline Naive RAG system, and Multi-query approaches underperformed. Sentence Window Retrieval emerged as the most effective for retrieval precision, despite its variable performance on answer similarity. The study confirms the potential of the Document Summary Index as a competent retrieval approach. All resources related to this research are publicly accessible for further investigation through our GitHub repository ARAGOG (this https URL). We welcome the community to further this exploratory study in RAG systems.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Verifying Claims About Metaphors with Large-Scale Automatic Metaphor  Identification</b></summary>
  <p><b>编号</b>：[145]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01029">https://arxiv.org/abs/2404.01029</a></p>
  <p><b>作者</b>：Kotaro Aono,  Ryohei Sasano,  Koichi Takeda</p>
  <p><b>备注</b>：9 pages, 0 figures, accepted in NAACL2024</p>
  <p><b>关键词</b>：situations where words, linguistic claims, Common Crawl, claims, claims about situations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>There are several linguistic claims about situations where words are more likely to be used as metaphors. However, few studies have sought to verify such claims with large corpora. This study entails a large-scale, corpus-based analysis of certain existing claims about verb metaphors, by applying metaphor detection to sentences extracted from Common Crawl and using the statistics obtained from the results. The verification results indicate that the direct objects of verbs used as metaphors tend to have lower degrees of concreteness, imageability, and familiarity, and that metaphors are more likely to be used in emotional and subjective sentences.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Source-Aware Training Enables Knowledge Attribution in Language Models</b></summary>
  <p><b>编号</b>：[149]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01019">https://arxiv.org/abs/2404.01019</a></p>
  <p><b>作者</b>：Muhammad Khalifa,  David Wadden,  Emma Strubell,  Honglak Lee,  Lu Wang,  Iz Beltagy,  Hao Peng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large language models, Large language, learn a vast, intrinsic source citation, vast amount</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) learn a vast amount of knowledge during pretraining, but they are often oblivious to the source(s) of such knowledge. We investigate the problem of intrinsic source citation, where LLMs are required to cite the pretraining source supporting a generated response. Intrinsic source citation can enhance LLM transparency, interpretability, and verifiability. To give LLMs such ability, we explore source-aware training -- a post pretraining recipe that involves (i) training the LLM to associate unique source document identifiers with the knowledge in each document, followed by (ii) an instruction-tuning to teach the LLM to cite a supporting pretraining source when prompted. Source-aware training can easily be applied to pretrained LLMs off the shelf, and diverges minimally from existing pretraining/fine-tuning frameworks. Through experiments on carefully curated data, we demonstrate that our training recipe can enable faithful attribution to the pretraining data without a substantial impact on the model's quality compared to standard pretraining. Our results also highlight the importance of data augmentation in achieving attribution.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：PairEval: Open-domain Dialogue Evaluation with Pairwise Comparison</b></summary>
  <p><b>编号</b>：[150]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01015">https://arxiv.org/abs/2404.01015</a></p>
  <p><b>作者</b>：ChaeHun Park,  Minseok Choi,  Dohyun Lee,  Jaegul Choo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Building a reliable, automated evaluation metric, reliable and automated, challenging problem, automated evaluation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Building a reliable and automated evaluation metric is a necessary but challenging problem for open-domain dialogue systems. Recent studies proposed evaluation metrics that assess generated responses by considering their relevance to previous dialogue histories. Although effective, these metrics evaluate individual responses directly rather than considering their relative quality compared to other responses. To handle this, we propose PairEval, a novel dialogue evaluation metric for assessing responses by comparing their quality against responses in different conversations. PairEval is built on top of open-sourced and moderate-size language models, and we make them specialized in pairwise comparison between dialogue responses. Extensive experiments on multiple benchmarks demonstrate that our metric exhibits a higher correlation with human judgments than baseline metrics. We also find that the proposed comparative metric is more robust in detecting common failures from open-domain dialogue systems, including repetition and speaker insensitivity.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Query Performance Prediction using Relevance Judgments Generated by  Large Language Models</b></summary>
  <p><b>编号</b>：[153]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01012">https://arxiv.org/abs/2404.01012</a></p>
  <p><b>作者</b>：Chuan Meng,  Negar Arabzadeh,  Arian Askari,  Mohammad Aliannejadi,  Maarten de Rijke</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：generated relevance judgments, relevance judgments, QPP, human relevance judgments, single scalar</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Query performance prediction (QPP) aims to estimate the retrieval quality of a search system for a query without human relevance judgments. Previous QPP methods typically return a single scalar value and do not require the predicted values to approximate a specific information retrieval (IR) evaluation measure, leading to certain drawbacks: (i) a single scalar is insufficient to accurately represent different IR evaluation measures, especially when metrics do not highly correlate, and (ii) a single scalar limits the interpretability of QPP methods because solely using a scalar is insufficient to explain QPP results. To address these issues, we propose a QPP framework using automatically generated relevance judgments (QPP-GenRE), which decomposes QPP into independent subtasks of judging the relevance of each item in a ranked list to a given query. This allows us to predict any IR evaluation measure using the generated relevance judgments as pseudo-labels; Also, this allows us to interpret predicted IR evaluation measures, and identify, track and rectify errors in generated relevance judgments to improve QPP quality. We judge relevance by leveraging a leading open-source large language model (LLM), LLaMA, to ensure scientific reproducibility. In doing so, we address two main challenges: (i) excessive computational costs of judging the entire corpus for predicting a recall-based metric, and (ii) poor performance in prompting LLaMA in a zero-/few-shot manner. We devise an approximation strategy to predict a recall-oriented IR measure and propose to fine-tune LLaMA using human-labeled relevance judgments. Experiments on the TREC 2019-2022 deep learning tracks show that QPP-GenRE achieves state-of-the-art QPP accuracy for both lexical and neural rankers in both precision- and recall-oriented metrics.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Constructing and Expanding Low-Resource and Underrepresented Parallel  Datasets for Indonesian Local Languages</b></summary>
  <p><b>编号</b>：[154]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01009">https://arxiv.org/abs/2404.01009</a></p>
  <p><b>作者</b>：Joanito Agili Lopo,  Radius Tanone</p>
  <p><b>备注</b>：Submitted for consideration at the EAMT, 2024. Results pending</p>
  <p><b>关键词</b>：Natural Language Processing, local languages play, play an integral, integral role, Indonesian local languages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In Indonesia, local languages play an integral role in the culture. However, the available Indonesian language resources still fall into the category of limited data in the Natural Language Processing (NLP) field. This is become problematic when build NLP model for these languages. To address this gap, we introduce Bhinneka Korpus, a multilingual parallel corpus featuring five Indonesian local languages. Our goal is to enhance access and utilization of these resources, extending their reach within the country. We explained in a detail the dataset collection process and associated challenges. Additionally, we experimented with translation task using the IBM Model 1 due to data constraints. The result showed that the performance of each language already shows good indications for further development. Challenges such as lexical variation, smoothing effects, and cross-linguistic variability are discussed. We intend to evaluate the corpus using advanced NLP techniques for low-resource languages, paving the way for multilingual translation models.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：What Causes the Failure of Explicit to Implicit Discourse Relation  Recognition?</b></summary>
  <p><b>编号</b>：[157]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00999">https://arxiv.org/abs/2404.00999</a></p>
  <p><b>作者</b>：Wei Liu,  Stephen Wan,  Michael Strube</p>
  <p><b>备注</b>：Accepted by NAACL2024 (Long Paper)</p>
  <p><b>关键词</b>：real implicit scenarios, discourse processing community, processing community, perform poorly, unanswered question</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider an unanswered question in the discourse processing community: why do relation classifiers trained on explicit examples (with connectives removed) perform poorly in real implicit scenarios? Prior work claimed this is due to linguistic dissimilarity between explicit and implicit examples but provided no empirical evidence. In this study, we show that one cause for such failure is a label shift after connectives are eliminated. Specifically, we find that the discourse relations expressed by some explicit instances will change when connectives disappear. Unlike previous work manually analyzing a few examples, we present empirical evidence at the corpus level to prove the existence of such shift. Then, we analyze why label shift occurs by considering factors such as the syntactic role played by connectives, ambiguity of connectives, and more. Finally, we investigate two strategies to mitigate the label shift: filtering out noisy data and joint learning with connectives. Experiments on PDTB 2.0, PDTB 3.0, and the GUM dataset demonstrate that classifiers trained with our strategies outperform strong baselines.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report  Generation</b></summary>
  <p><b>编号</b>：[158]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00998">https://arxiv.org/abs/2404.00998</a></p>
  <p><b>作者</b>：Zilong Wang,  Xufang Luo,  Xinyang Jiang,  Dongsheng Li,  Lili Qiu</p>
  <p><b>备注</b>：11 pages, 6 figures</p>
  <p><b>关键词</b>：task clinical requirements, Evaluating generated radiology, existing metrics fail, generated radiology reports, Evaluating generated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Evaluating generated radiology reports is crucial for the development of radiology AI, but existing metrics fail to reflect the task's clinical requirements. This study proposes a novel evaluation framework using large language models (LLMs) to compare radiology reports for assessment. We compare the performance of various LLMs and demonstrate that, when using GPT-4, our proposed metric achieves evaluation consistency close to that of radiologists. Furthermore, to reduce costs and improve accessibility, making this method practical, we construct a dataset using LLM evaluation results and perform knowledge distillation to train a smaller model. The distilled model achieves evaluation capabilities comparable to GPT-4. Our framework and distilled model offer an accessible and efficient evaluation method for radiology report generation, facilitating the development of more clinically relevant models. The model will be further open-sourced and accessible.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Exploring the Nexus of Large Language Models and Legal Systems: A Short  Survey</b></summary>
  <p><b>编号</b>：[162]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00990">https://arxiv.org/abs/2404.00990</a></p>
  <p><b>作者</b>：Weicong Qin,  Zhongxiang Sun</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Artificial Intelligence, profound transformation occurring, natural language processing, Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the advancement of Artificial Intelligence (AI) and Large Language Models (LLMs), there is a profound transformation occurring in the realm of natural language processing tasks within the legal domain. The capabilities of LLMs are increasingly demonstrating unique roles in the legal sector, bringing both distinctive benefits and various challenges. This survey delves into the synergy between LLMs and the legal system, such as their applications in tasks like legal text comprehension, case retrieval, and analysis. Furthermore, this survey highlights key challenges faced by LLMs in the legal domain, including bias, interpretability, and ethical considerations, as well as how researchers are addressing these issues. The survey showcases the latest advancements in fine-tuned legal LLMs tailored for various legal systems, along with legal datasets available for fine-tuning LLMs in various languages. Additionally, it proposes directions for future research and development.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Prior Constraints-based Reward Model Training for Aligning Large  Language Models</b></summary>
  <p><b>编号</b>：[170]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00978">https://arxiv.org/abs/2404.00978</a></p>
  <p><b>作者</b>：Hang Zhou,  Chenglong Wang,  Yimin Hu,  Tong Xiao,  Chunliang Zhang,  Jingbo Zhu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：reinforcement learning due, Prior Constraints-based Reward, Constraints-based Reward Model, model.This paper proposes, large language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reinforcement learning with human feedback for aligning large language models (LLMs) trains a reward model typically using ranking loss with comparison pairs.However, the training procedure suffers from an inherent problem: the uncontrolled scaling of reward scores during reinforcement learning due to the lack of constraints while training the reward model.This paper proposes a Prior Constraints-based Reward Model (namely PCRM) training method to mitigate this problem. PCRM incorporates prior constraints, specifically, length ratio and cosine similarity between outputs of each comparison pair, during reward model training to regulate optimization magnitude and control score margins. We comprehensively evaluate PCRM by examining its rank correlation with human preferences and its effectiveness in aligning LLMs via RL. Experimental results demonstrate that PCRM significantly improves alignment performance by effectively constraining reward score scaling. As another bonus, our method is easily integrated into arbitrary rank-based alignment methods, such as direct preference optimization, and can yield consistent improvement.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：AISPACE at SemEval-2024 task 8: A Class-balanced Soft-voting System for  Detecting Multi-generator Machine-generated Text</b></summary>
  <p><b>编号</b>：[183]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00950">https://arxiv.org/abs/2404.00950</a></p>
  <p><b>作者</b>：Renhua Gu,  Xiangfeng Meng</p>
  <p><b>备注</b>：1st place at SemEval-2024 Task 8, Subtask B, to appear in SemEval-2024 proceedings</p>
  <p><b>关键词</b>：human-written and machine-generated, Large Language Model, specific Large Language, detect human-written, machine-generated text</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>SemEval-2024 Task 8 provides a challenge to detect human-written and machine-generated text. There are 3 subtasks for different detection scenarios. This paper proposes a system that mainly deals with Subtask B. It aims to detect if given full text is written by human or is generated by a specific Large Language Model (LLM), which is actually a multi-class text classification task. Our team AISPACE conducted a systematic study of fine-tuning transformer-based models, including encoderonly, decoder-only and encoder-decoder models. We compared their performance on this task and identified that encoder-only models performed exceptionally well. We also applied a weighted Cross Entropy loss function to address the issue of data imbalance of different class samples. Additionally, we employed softvoting strategy over multi-models ensemble to enhance the reliability of our predictions. Our system ranked top 1 in Subtask B, which sets a state-of-the-art benchmark for this new challenge.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Evalverse: Unified and Accessible Library for Large Language Model  Evaluation</b></summary>
  <p><b>编号</b>：[187]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00943">https://arxiv.org/abs/2404.00943</a></p>
  <p><b>作者</b>：Jihoo Kim,  Wonho Song,  Dahyun Kim,  Yunsu Kim,  Yungi Kim,  Chanjun Park</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, paper introduces Evalverse, unifying disparate evaluation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper introduces Evalverse, a novel library that streamlines the evaluation of Large Language Models (LLMs) by unifying disparate evaluation tools into a single, user-friendly framework. Evalverse enables individuals with limited knowledge of artificial intelligence to easily request LLM evaluations and receive detailed reports, facilitated by an integration with communication platforms like Slack. Thus, Evalverse serves as a powerful tool for the comprehensive assessment of LLMs, offering both researchers and practitioners a centralized and easily accessible evaluation framework. Finally, we also provide a demo video for Evalverse, showcasing its capabilities and implementation in a two-minute format.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Evaluating the Factuality of Large Language Models using Large-Scale  Knowledge Graphs</b></summary>
  <p><b>编号</b>：[188]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00942">https://arxiv.org/abs/2404.00942</a></p>
  <p><b>作者</b>：Xiaoze Liu,  Feijie Wu,  Tianyang Xu,  Zhuo Chen,  Yichi Zhang,  Xiaoqian Wang,  Jing Gao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：enhancing machine learning, Large Language Models, Large Language, Language Models, enhancing machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The advent of Large Language Models (LLMs) has significantly transformed the AI landscape, enhancing machine learning and AI capabilities. Factuality issue is a critical concern for LLMs, as they may generate factually incorrect responses. In this paper, we propose GraphEval to evaluate an LLM's performance using a substantially large test dataset. Specifically, the test dataset is retrieved from a large knowledge graph with more than 10 million facts without expensive human efforts. Unlike conventional methods that evaluate LLMs based on generated responses, GraphEval streamlines the evaluation process by creating a judge model to estimate the correctness of the answers given by the LLM. Our experiments demonstrate that the judge model's factuality assessment aligns closely with the correctness of the LLM's generated outputs, while also substantially reducing evaluation costs. Besides, our findings offer valuable insights into LLM performance across different metrics and highlight the potential for future improvements in ensuring the factual integrity of LLM outputs. The code is publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：How Can Large Language Models Enable Better Socially Assistive  Human-Robot Interaction: A Brief Survey</b></summary>
  <p><b>编号</b>：[189]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00938">https://arxiv.org/abs/2404.00938</a></p>
  <p><b>作者</b>：Zhonghao Shi,  Ellen Landrum,  Amy O' Connell,  Mina Kian,  Leticia Pinto-Alva,  Kaleen Shrestha,  Xiaoyuan Zhu,  Maja J Matarić</p>
  <p><b>备注</b>：2 pages, to be submitted to 2024 AAAI Spring Symposium</p>
  <p><b>关键词</b>：autism spectrum disorder, shown great success, providing personalized cognitive-affective, personalized cognitive-affective support, Socially assistive robots</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Socially assistive robots (SARs) have shown great success in providing personalized cognitive-affective support for user populations with special needs such as older adults, children with autism spectrum disorder (ASD), and individuals with mental health challenges. The large body of work on SAR demonstrates its potential to provide at-home support that complements clinic-based interventions delivered by mental health professionals, making these interventions more effective and accessible. However, there are still several major technical challenges that hinder SAR-mediated interactions and interventions from reaching human-level social intelligence and efficacy. With the recent advances in large language models (LLMs), there is an increased potential for novel applications within the field of SAR that can significantly expand the current capabilities of SARs. However, incorporating LLMs introduces new risks and ethical concerns that have not yet been encountered, and must be carefully be addressed to safely deploy these more advanced systems. In this work, we aim to conduct a brief survey on the use of LLMs in SAR technologies, and discuss the potentials and risks of applying LLMs to the following three major technical challenges of SAR: 1) natural language dialog; 2) multimodal understanding; 3) LLMs as robot policies.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：ChatGLM-RLHF: Practices of Aligning Large Language Models with Human  Feedback</b></summary>
  <p><b>编号</b>：[191]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00934">https://arxiv.org/abs/2404.00934</a></p>
  <p><b>作者</b>：Zhenyu Hou,  Yiin Niu,  Zhengxiao Du,  Xiaohan Zhang,  Xiao Liu,  Aohan Zeng,  Qinkai Zheng,  Minlie Huang,  Hongning Wang,  Jie Tang,  Yuxiao Dong</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, service powered, family of large, large language, human preferences</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>ChatGLM is a free-to-use AI service powered by the ChatGLM family of large language models (LLMs). In this paper, we present the ChatGLM-RLHF pipeline -- a reinforcement learning from human feedback (RLHF) system -- designed to enhance ChatGLM's alignment with human preferences. ChatGLM-RLHF encompasses three major components: the collection of human preference data, the training of the reward model, and the optimization of policies. Throughout the process of integrating ChatGLM-RLHF into production, we encountered and addressed several unprecedented challenges. We introduce the strategies to mitigate reward variance for stabilized large-scale training, implement model parallelism with fused gradient-descent, and design regularization constraints to avoid catastrophic forgetting in LLMs. Experiments show that ChatGLM-RLHF brings significant improvements in alignment tasks compared to the supervised fine-tuned (SFT) version of ChatGLM. For instance, it achieves on average 15\% more wins against ChatGLM-SFT in Chinese alignment tasks. The work presents our practices of aligning LLMs with human preferences, offering insights into the challenges and solutions in RLHF implementations.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：PSYDIAL: Personality-based Synthetic Dialogue Generation using Large  Language Models</b></summary>
  <p><b>编号</b>：[193]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00930">https://arxiv.org/abs/2404.00930</a></p>
  <p><b>作者</b>：Ji-Eun Han,  Jun-Seok Koh,  Hyeon-Tae Seo,  Du-Seong Chang,  Kyung-Ah Sohn</p>
  <p><b>备注</b>：LREC-COLING 2024 Main</p>
  <p><b>关键词</b>：synthetic dialogue data, dialogue data generation, data generation pipeline, specifically designed, data generation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a novel end-to-end personality-based synthetic dialogue data generation pipeline, specifically designed to elicit responses from large language models via prompting. We design the prompts to generate more human-like dialogues considering real-world scenarios when users engage with chatbots. We introduce PSYDIAL, the first Korean dialogue dataset focused on personality-based dialogues, curated using our proposed pipeline. Notably, we focus on the Extraversion dimension of the Big Five personality model in our research. Experimental results indicate that while pre-trained models and those fine-tuned with a chit-chat dataset struggle to generate responses reflecting personality, models trained with PSYDIAL show significant improvements. The versatility of our pipeline extends beyond dialogue tasks, offering potential for other non-dialogue related applications. This research opens doors for more nuanced, personality-driven conversational AI in Korean and potentially other languages. Our code is publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：A Survey on Multilingual Large Language Models: Corpora, Alignment, and  Bias</b></summary>
  <p><b>编号</b>：[194]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00929">https://arxiv.org/abs/2404.00929</a></p>
  <p><b>作者</b>：Yuemei Xu,  Ling Hu,  Jiayi Zhao,  Zihan Qiu,  Yuqi Ye,  Hanwen Gu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, Multilingual Large Language, achieve knowledge transfer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Based on the foundation of Large Language Models (LLMs), Multilingual Large Language Models (MLLMs) have been developed to address the challenges of multilingual natural language processing tasks, hoping to achieve knowledge transfer from high-resource to low-resource languages. However, significant limitations and challenges still exist, such as language imbalance, multilingual alignment, and inherent bias. In this paper, we aim to provide a comprehensive analysis of MLLMs, delving deeply into discussions surrounding these critical issues. First of all, we start by presenting an overview of MLLMs, covering their evolution, key techniques, and multilingual capacities. Secondly, we explore widely utilized multilingual corpora for MLLMs' training and multilingual datasets oriented for downstream tasks that are crucial for enhancing the cross-lingual capability of MLLMs. Thirdly, we survey the existing studies on multilingual representations and investigate whether the current MLLMs can learn a universal language representation. Fourthly, we discuss bias on MLLMs including its category and evaluation metrics, and summarize the existing debiasing techniques. Finally, we discuss existing challenges and point out promising research directions. By demonstrating these aspects, this paper aims to facilitate a deeper understanding of MLLMs and their potentiality in various domains.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：LLMs are Good Sign Language Translators</b></summary>
  <p><b>编号</b>：[196]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00925">https://arxiv.org/abs/2404.00925</a></p>
  <p><b>作者</b>：Jia Gong,  Lin Geng Foo,  Yixuan He,  Hossein Rahmani,  Jun Liu</p>
  <p><b>备注</b>：Accepted to CVPR 2024</p>
  <p><b>关键词</b>：Sign Language Translation, translate sign videos, sign videos, challenging task, Sign</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sign Language Translation (SLT) is a challenging task that aims to translate sign videos into spoken language. Inspired by the strong translation capabilities of large language models (LLMs) that are trained on extensive multilingual text corpora, we aim to harness off-the-shelf LLMs to handle SLT. In this paper, we regularize the sign videos to embody linguistic characteristics of spoken language, and propose a novel SignLLM framework to transform sign videos into a language-like representation for improved readability by off-the-shelf LLMs. SignLLM comprises two key modules: (1) The Vector-Quantized Visual Sign module converts sign videos into a sequence of discrete character-level sign tokens, and (2) the Codebook Reconstruction and Alignment module converts these character-level tokens into word-level sign representations using an optimal transport formulation. A sign-text alignment loss further bridges the gap between sign and text tokens, enhancing semantic compatibility. We achieve state-of-the-art gloss-free results on two widely-used SLT benchmarks.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Token-Efficient Leverage Learning in Large Language Models</b></summary>
  <p><b>编号</b>：[204]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00914">https://arxiv.org/abs/2404.00914</a></p>
  <p><b>作者</b>：Yuanhao Zeng,  Min Wang,  Yihang Wang,  Yingxia Shao</p>
  <p><b>备注</b>：15 pages, 16 figures</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Leverage Learning, Large Language, high-resource scenarios</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) have excelled in various tasks but perform better in high-resource scenarios, which presents challenges in low-resource scenarios. Data scarcity and the inherent difficulty of adapting LLMs to specific tasks compound the challenge. To address the twin hurdles, we introduce \textbf{Leverage Learning}. We present a streamlined implement of this methodology called Token-Efficient Leverage Learning (TELL). TELL showcases the potential of Leverage Learning, demonstrating effectiveness across various LLMs and low-resource tasks, ranging from $10^4$ to $10^6$ tokens. It reduces task data requirements by up to nearly an order of magnitude compared to conventional Supervised Fine-Tuning (SFT) while delivering competitive performance. With the same amount of task data, TELL leads in improving task performance compared to SFT. We discuss the mechanism of Leverage Learning, suggesting it aligns with quantization hypothesis and explore its promising potential through empirical testing.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：LLaMA-Excitor: General Instruction Tuning via Indirect Feature  Interaction</b></summary>
  <p><b>编号</b>：[205]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00913">https://arxiv.org/abs/2404.00913</a></p>
  <p><b>作者</b>：Bo Zou,  Chao Yang,  Yu Qiao,  Chengbin Quan,  Youjian Zhao</p>
  <p><b>备注</b>：This paper is accepted by CVPR 2024</p>
  <p><b>关键词</b>：introduce extra modules, introduce extra, sequences to inject, inject new skills, compromise the innate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing methods to fine-tune LLMs, like Adapter, Prefix-tuning, and LoRA, which introduce extra modules or additional input sequences to inject new skills or knowledge, may compromise the innate abilities of LLMs. In this paper, we propose LLaMA-Excitor, a lightweight method that stimulates the LLMs' potential to better follow instructions by gradually paying more attention to worthwhile information. Specifically, the LLaMA-Excitor does not directly change the intermediate hidden state during the self-attention calculation of the transformer structure. We designed the Excitor block as a bypass module for the similarity score computation in LLMs' self-attention to reconstruct keys and change the importance of values by learnable prompts. LLaMA-Excitor ensures a self-adaptive allocation of additional attention to input instructions, thus effectively preserving LLMs' pre-trained knowledge when fine-tuning LLMs on low-quality instruction-following datasets. Furthermore, we unify the modeling of multi-modal tuning and language-only tuning, extending LLaMA-Excitor to a powerful visual instruction follower without the need for complex multi-modal alignment. Our proposed approach is evaluated in language-only and multi-modal tuning experimental scenarios. Notably, LLaMA-Excitor is the only method that maintains basic capabilities while achieving a significant improvement (+6%) on the MMLU benchmark. In the visual instruction tuning, we achieve a new state-of-the-art image captioning performance of 157.5 CIDEr on MSCOCO, and a comparable performance (88.39%) on ScienceQA to cutting-edge models with more parameters and extensive vision-language pertaining.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：TM-TREK at SemEval-2024 Task 8: Towards LLM-Based Automatic Boundary  Detection for Human-Machine Mixed Text</b></summary>
  <p><b>编号</b>：[212]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00899">https://arxiv.org/abs/2404.00899</a></p>
  <p><b>作者</b>：Xiaoyan Qu,  Xiangfeng Meng</p>
  <p><b>备注</b>：1st place at SemEval-2024 Task 8, Subtask C, to appear in SemEval-2024 proceedings</p>
  <p><b>关键词</b>：large language models, academic dishonesty, mixed texts, Mixed Text Detection, increasing prevalence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the increasing prevalence of text generated by large language models (LLMs), there is a growing concern about distinguishing between LLM-generated and human-written texts in order to prevent the misuse of LLMs, such as the dissemination of misleading information and academic dishonesty. Previous research has primarily focused on classifying text as either entirely human-written or LLM-generated, neglecting the detection of mixed texts that contain both types of content. This paper explores LLMs' ability to identify boundaries in human-written and machine-generated mixed texts. We approach this task by transforming it into a token classification problem and regard the label turning point as the boundary. Notably, our ensemble model of LLMs achieved first place in the 'Human-Machine Mixed Text Detection' sub-task of the SemEval'24 Competition Task 8. Additionally, we investigate factors that influence the capability of LLMs in detecting boundaries within mixed texts, including the incorporation of extra layers on top of LLMs, combination of segmentation loss, and the impact of pretraining. Our findings aim to provide valuable insights for future research in this area.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large  Language Models</b></summary>
  <p><b>编号</b>：[221]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00884">https://arxiv.org/abs/2404.00884</a></p>
  <p><b>作者</b>：Wei He,  Shichun Liu,  Jun Zhao,  Yiwen Ding,  Yi Lu,  Zhiheng Xi,  Tao Gui,  Qi Zhang,  Xuanjing Huang</p>
  <p><b>备注</b>：Accepted to NAACL 2024 Findings</p>
  <p><b>关键词</b>：Large language models, shown promising abilities, Large language, language models, in-context learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have shown promising abilities of in-context learning (ICL), adapting swiftly to new tasks with only few-shot demonstrations. However, current few-shot methods heavily depend on high-quality, query-specific demos, which are often lacking. When faced with out-of-demonstration (OOD) queries, methods that rely on hand-crafted demos or external retrievers might fail. To bridge the gap between limited demos and OOD queries, we propose Self-Demos, a novel prompting method that elicits the inherent generalizability in LLMs by query-aware demo generation. The generated demos strategically interpolate between existing demos and the given query, transforming the query from OOD to ID. To evaluate the effectiveness of our approach, we manually constructed OOD-Toolset, a dataset in the tool-using scenario with over 300 real-world APIs and 1000 instances, each consisting of three tool-use cases as demos and an OOD query. Thorough experiments on our dataset and two public math benchmarks have shown that our method can outperform state-of-the-art baselines in the OOD setting. Moreover, we conduct a range of analyses to validate Self-Demos's generalization and provide more insights.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Bailong: Bilingual Transfer Learning based on QLoRA and Zip-tie  Embedding</b></summary>
  <p><b>编号</b>：[232]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00862">https://arxiv.org/abs/2404.00862</a></p>
  <p><b>作者</b>：Lung-Chuan Chen,  Zong-Ru Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：NLP applications, Traditional Chinese, demonstrated exceptional performance, demonstrated exceptional, Traditional Chinese data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have demonstrated exceptional performance in various NLP applications. However, the majority of existing open-source LLMs are pre-trained primarily on English data and little part of other languages. This deficiency in multilingual training data results in suboptimal performance when applied to languages with fewer available resources. Furthermore, enhancing the performance of LLMs on low-resource languages by full-parameter fine-tuning with additional data requires substantial computational resources, posing computational barriers for research organizations and individual researchers. Consequently, several techniques such as parameter-efficient tuning and advanced embedding initialization have been proposed to address these challenges. In this work, we combine them to facilitate cross-lingual transfer on English-dominated open-source LLM. To effectively enhance the model's proficiency in Traditional Chinese, we conduct secondary pre-training on Llama 2 7B with Traditional Chinese data by leveraging QLoRA and our proposed zip-tie embedding initialization. The resulting model called Bailong, which stands for Bilingual trAnsfer learnIng based on qLOra and zip-tie embeddiNG. We present Bailong-instruct 7B, a fine-tuned version of Bailong 7B optimized for multi-turn dialogue scenarios. Recognizing the inadequacy of benchmark datasets in Traditional Chinese, we further introduce Bailong-bench to assess the alignment of models with human preferences and the capability to follow instructions in both Traditional Chinese and English tasks. In our evaluation, Bailong-instruct 7B exhibits competitive performance on Bailong-bench and other benchmark datasets when compared to other open-source models of similar or even larger parameter sizes. Bailong-instruct 7B and Bailong-bench are publicly available with the aim of empowering the community to build upon our efforts.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Do language models plan ahead for future tokens?</b></summary>
  <p><b>编号</b>：[234]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00859">https://arxiv.org/abs/2404.00859</a></p>
  <p><b>作者</b>：Wilson Wu,  John X. Morris,  Lionel Levine</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：transformers prepare information, future forward passes, inference, tau, present inference task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Do transformers "think ahead" during inference at a given position? It is known transformers prepare information in the hidden states of the forward pass at $t$ that is then used in future forward passes $t+\tau$. We posit two explanations for this phenomenon: pre-caching, in which off-diagonal gradient terms present in training result in the model computing features at $t$ irrelevant to the present inference task but useful for the future, and breadcrumbs, in which features most relevant to time step $t$ are already the same as those that would most benefit inference at time $t+\tau$. We test these hypotheses by training language models without propagating gradients to past timesteps, a scheme we formalize as myopic training. In a synthetic data setting, we find clear evidence for pre-caching. In the autoregressive language modeling setting, our experiments are more suggestive of the breadcrumbs hypothesis.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Returning to the Start: Generating Narratives with Related Endpoints</b></summary>
  <p><b>编号</b>：[251]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00829">https://arxiv.org/abs/2404.00829</a></p>
  <p><b>作者</b>：Anneliese Brei,  Chao Zhao,  Snigdha Chaturvedi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：closes the loop, writers often bookend, bookend their writing, writing with ending, relate back</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Human writers often bookend their writing with ending sentences that relate back to the beginning sentences in order to compose a satisfying narrative that "closes the loop." Motivated by this observation, we propose RENarGen, a controllable story-generation paradigm that generates narratives by ensuring the first and last sentences are related and then infilling the middle sentences. Our contributions include an initial exploration of how various methods of bookending from Narratology affect language modeling for stories. Automatic and human evaluations indicate RENarGen produces better stories with more narrative closure than current autoregressive models.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：PID Control-Based Self-Healing to Improve the Robustness of Large  Language Models</b></summary>
  <p><b>编号</b>：[252]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00828">https://arxiv.org/abs/2404.00828</a></p>
  <p><b>作者</b>：Zhuotong Chen,  Zihu Wang,  Yifan Yang,  Qianxiao Li,  Zheng Zhang</p>
  <p><b>备注</b>：Transactions on Machine Learning Research</p>
  <p><b>关键词</b>：language processing applications, numerous natural language, natural language processing, language models, deep neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite the effectiveness of deep neural networks in numerous natural language processing applications, recent findings have exposed the vulnerability of these language models when minor perturbations are introduced. While appearing semantically indistinguishable to humans, these perturbations can significantly reduce the performance of well-trained language models, raising concerns about the reliability of deploying them in safe-critical situations. In this work, we construct a computationally efficient self-healing process to correct undesired model behavior during online inference when perturbations are applied to input data. This is formulated as a trajectory optimization problem in which the internal states of the neural network layers are automatically corrected using a PID (Proportional-Integral-Derivative) control mechanism. The P controller targets immediate state adjustments, while the I and D controllers consider past states and future dynamical trends, respectively. We leverage the geometrical properties of the training data to design effective linear PID controllers. This approach reduces the computational cost to that of using just the P controller, instead of the full PID control. Further, we introduce an analytical method for approximating the optimal control solutions, enhancing the real-time inference capabilities of this controlled system. Moreover, we conduct a theoretical error analysis of the analytic solution in a simplified setting. The proposed PID control-based self-healing is a low cost framework that improves the robustness of pre-trained large language models, whether standard or robustly trained, against a wide range of perturbations. A detailed implementation can be found in:this https URL.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：Extracting Social Determinants of Health from Pediatric Patient Notes  Using Large Language Models: Novel Corpus and Methods</b></summary>
  <p><b>编号</b>：[253]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00826">https://arxiv.org/abs/2404.00826</a></p>
  <p><b>作者</b>：Yujuan Fu,  Giridhar Kaushik Ramachandran,  Nicholas J Dobbins,  Namu Park,  Michael Leu,  Abby R. Rosenberg,  Kevin Lybarger,  Fei Xia,  Ozlem Uzuner,  Meliha Yetisgen</p>
  <p><b>备注</b>：12 pages, 2 figures and 3 tables. Accepted by LERC-COLING 2024</p>
  <p><b>关键词</b>：shaping health outcomes, Electronic Health Record, Large Language Models, play a critical, long-term implications</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Social determinants of health (SDoH) play a critical role in shaping health outcomes, particularly in pediatric populations where interventions can have long-term implications. SDoH are frequently studied in the Electronic Health Record (EHR), which provides a rich repository for diverse patient data. In this work, we present a novel annotated corpus, the Pediatric Social History Annotation Corpus (PedSHAC), and evaluate the automatic extraction of detailed SDoH representations using fine-tuned and in-context learning methods with Large Language Models (LLMs). PedSHAC comprises annotated social history sections from 1,260 clinical notes obtained from pediatric patients within the University of Washington (UW) hospital system. Employing an event-based annotation scheme, PedSHAC captures ten distinct health determinants to encompass living and economic stability, prior trauma, education access, substance use history, and mental health with an overall annotator agreement of 81.9 F1. Our proposed fine-tuning LLM-based extractors achieve high performance at 78.4 F1 for event arguments. In-context learning approaches with GPT-4 demonstrate promise for reliable SDoH extraction with limited annotated examples, with extraction performance at 82.3 F1 for event triggers.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Rehearsal-Free Modular and Compositional Continual Learning for Language  Models</b></summary>
  <p><b>编号</b>：[268]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00790">https://arxiv.org/abs/2404.00790</a></p>
  <p><b>作者</b>：Mingyang Wang,  Heike Adel,  Lukas Lange,  Jannik Strötgen,  Hinrich Schütze</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Continual learning aims, Continual learning, aims at incrementally, incrementally acquiring, Compositional Continual Learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Continual learning aims at incrementally acquiring new knowledge while not forgetting existing knowledge. To overcome catastrophic forgetting, methods are either rehearsal-based, i.e., store data examples from previous tasks for data replay, or isolate parameters dedicated to each task. However, rehearsal-based methods raise privacy and memory issues, and parameter-isolation continual learning does not consider interaction between tasks, thus hindering knowledge transfer. In this work, we propose MoCL, a rehearsal-free Modular and Compositional Continual Learning framework which continually adds new modules to language models and composes them with existing modules. Experiments on various benchmarks show that MoCL outperforms state of the art and effectively facilitates knowledge transfer.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：From Robustness to Improved Generalization and Calibration in  Pre-trained Language Models</b></summary>
  <p><b>编号</b>：[282]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00758">https://arxiv.org/abs/2404.00758</a></p>
  <p><b>作者</b>：Josip Jukić,  Jan Šnajder</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：pre-trained language models, effectiveness and reliability, Jacobian and Hessian, uncertainty quantification, quantification in pre-trained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Enhancing generalization and uncertainty quantification in pre-trained language models (PLMs) is crucial for their effectiveness and reliability. Building on machine learning research that established the importance of robustness for improving generalization, we investigate the role of representation smoothness, achieved via Jacobian and Hessian regularization, in enhancing PLM performance. Although such regularization methods have proven effective in computer vision, their application in natural language processing (NLP), where PLM inputs are derived from a discrete domain, poses unique challenges. We introduce a novel two-phase regularization approach, JacHess, which minimizes the norms of the Jacobian and Hessian matrices within PLM intermediate representations relative to their inputs. Our evaluation using the GLUE benchmark demonstrates that JacHess significantly improves in-domain generalization and calibration in PLMs, outperforming unregularized fine-tuning and other similar regularization methods.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：On the True Distribution Approximation of Minimum Bayes-Risk Decoding</b></summary>
  <p><b>编号</b>：[284]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00752">https://arxiv.org/abs/2404.00752</a></p>
  <p><b>作者</b>：Atsumoto Ohashi,  Ukyo Honda,  Tetsuro Morimura,  Yuu Jinnai</p>
  <p><b>备注</b>：NAACL 2024 (main conference)</p>
  <p><b>关键词</b>：recently gained renewed, gained renewed attention, Minimum Bayes-risk, MBR decoding, recently gained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Minimum Bayes-risk (MBR) decoding has recently gained renewed attention in text generation. MBR decoding considers texts sampled from a model as pseudo-references and selects the text with the highest similarity to the others. Therefore, sampling is one of the key elements of MBR decoding, and previous studies reported that the performance varies by sampling methods. From a theoretical standpoint, this performance variation is likely tied to how closely the samples approximate the true distribution of references. However, this approximation has not been the subject of in-depth study. In this study, we propose using anomaly detection to measure the degree of approximation. We first closely examine the performance variation and then show that previous hypotheses about samples do not correlate well with the variation, but our introduced anomaly scores do. The results are the first to empirically support the link between the performance and the core assumption of MBR decoding.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：Can Language Models Recognize Convincing Arguments?</b></summary>
  <p><b>编号</b>：[285]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00750">https://arxiv.org/abs/2404.00750</a></p>
  <p><b>作者</b>：Paula Rescala,  Manoel Horta Ribeiro,  Tiancheng Hu,  Robert West</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, creating personalized, misinformation and propaganda</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The remarkable and ever-increasing capabilities of Large Language Models (LLMs) have raised concerns about their potential misuse for creating personalized, convincing misinformation and propaganda. To gain insights into LLMs' persuasive capabilities without directly engaging in experimentation with humans, we propose studying their performance on the related task of detecting convincing arguments. We extend a dataset by Durmus & Cardie (2018) with debates, votes, and user traits and propose tasks measuring LLMs' ability to (1) distinguish between strong and weak arguments, (2) predict stances based on beliefs and demographic characteristics, and (3) determine the appeal of an argument to an individual based on their traits. We show that LLMs perform on par with humans in these tasks and that combining predictions from different LLMs yields significant performance gains, even surpassing human performance. The data and code released with this paper contribute to the crucial ongoing effort of continuously evaluating and monitoring the rapidly evolving capabilities and potential impact of LLMs.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：Benchmark Transparency: Measuring the Impact of Data on Evaluation</b></summary>
  <p><b>编号</b>：[286]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00748">https://arxiv.org/abs/2404.00748</a></p>
  <p><b>作者</b>：Venelin Kovatchev,  Matthew Lease</p>
  <p><b>备注</b>：Accepted at NAACL 2024</p>
  <p><b>关键词</b>：NLP models, data distribution, paper we present, present an exploratory, exploratory research</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper we present an exploratory research on quantifying the impact that data distribution has on the performance and evaluation of NLP models. We propose an automated framework that measures the data point distribution across 6 different dimensions: ambiguity, difficulty, discriminability, length, noise, and perplexity.
We use disproportional stratified sampling to measure how much the data distribution affects absolute (Acc/F1) and relative (Rank) model performance. We experiment on 2 different datasets (SQUAD and MNLI) and test a total of 135 different models (125 on SQUAD and 10 on MNLI). We demonstrate that without explicit control of the data distribution, standard evaluation frameworks are inconsistent and unreliable. We find that the impact of the data is statistically significant and is often larger than the impact of changing the metric.
In a second set of experiments, we demonstrate that the impact of data on evaluation is not just observable, but also predictable. We propose to use benchmark transparency as a method for comparing datasets and quantifying the similarity between them. We find that the ``dataset similarity vector'' can be used to predict how well a model generalizes out of distribution.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：Opera Graeca Adnotata: Building a 34M+ Token Multilayer Corpus for  Ancient Greek</b></summary>
  <p><b>编号</b>：[290]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00739">https://arxiv.org/abs/2404.00739</a></p>
  <p><b>作者</b>：Giuseppe G. A. Celano</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Opera Graeca Adnotata, Graeca Adnotata, Opera Graeca, largest open-access multilayer, open-access multilayer corpus</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this article, the beta version 0.1.0 of Opera Graeca Adnotata (OGA), the largest open-access multilayer corpus for Ancient Greek (AG) is presented. OGA consists of 1,687 literary works and 34M+ tokens coming from the PerseusDL and OpenGreekAndLatin GitHub repositories, which host AG texts ranging from about 800 BCE to about 250 CE. The texts have been enriched with seven annotation layers: (i) tokenization layer; (ii) sentence segmentation layer; (iii) lemmatization layer; (iv) morphological layer; (v) dependency layer; (vi) dependency function layer; (vii) Canonical Text Services (CTS) citation layer. The creation of each layer is described by highlighting the main technical and annotation-related issues encountered. Tokenization, sentence segmentation, and CTS citation are performed by rule-based algorithms, while morphosyntactic annotation is the output of the COMBO parser trained on the data of the Ancient Greek Dependency Treebank. For the sake of scalability and reusability, the corpus is released in the standoff formats PAULA XML and its offspring LAULA XML.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：A Controlled Reevaluation of Coreference Resolution Models</b></summary>
  <p><b>编号</b>：[295]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00727">https://arxiv.org/abs/2404.00727</a></p>
  <p><b>作者</b>：Ian Porada,  Xiyuan Zou,  Jackie Chi Kit Cheung</p>
  <p><b>备注</b>：LREC-COLING 2024</p>
  <p><b>关键词</b>：models involve finetuning, pretrained language model, language model, coreference resolution, involve finetuning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>All state-of-the-art coreference resolution (CR) models involve finetuning a pretrained language model. Whether the superior performance of one CR model over another is due to the choice of language model or other factors, such as the task-specific architecture, is difficult or impossible to determine due to lack of a standardized experimental setup. To resolve this ambiguity, we systematically evaluate five CR models and control for certain design decisions including the pretrained language model used by each. When controlling for language model size, encoder-based CR models outperform more recent decoder-based models in terms of both accuracy and inference speed. Surprisingly, among encoder-based CR models, more recent models are not always more accurate, and the oldest CR model that we test generalizes the best to out-of-domain textual genres. We conclude that controlling for the choice of language model reduces most, but not all, of the increase in F1 score reported in the past five years.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：The Larger the Better? Improved LLM Code-Generation via Budget  Reallocation</b></summary>
  <p><b>编号</b>：[296]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00725">https://arxiv.org/abs/2404.00725</a></p>
  <p><b>作者</b>：Michael Hassid,  Tal Remez,  Jonas Gehring,  Roy Schwartz,  Yossi Adi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, common belief, belief that large, large language, models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>It is a common belief that large language models (LLMs) are better than smaller-sized ones. However, larger models also require significantly more time and compute during inference. This begs the question: what happens when both models operate under the same budget? (e.g., compute, run-time). To address this question, we analyze code generation LLMs of various sizes and make comparisons such as running a 70B model once vs. generating five outputs from a 13B model and selecting one. Our findings reveal that, in a standard unit-test setup, the repeated use of smaller models can yield consistent improvements, with gains of up to 15% across five tasks. On the other hand, in scenarios where unit-tests are unavailable, a ranking-based selection of candidates from the smaller model falls short of the performance of a single output from larger ones. Our results highlight the potential of using smaller models instead of larger ones, and the importance of studying approaches for ranking LLM outputs.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：How Much are LLMs Contaminated? A Comprehensive Survey and the  LLMSanitize Library</b></summary>
  <p><b>编号</b>：[306]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00699">https://arxiv.org/abs/2404.00699</a></p>
  <p><b>作者</b>：Mathieu Ravaut,  Bosheng Ding,  Fangkai Jiao,  Hailin Chen,  Xingxuan Li,  Ruochen Zhao,  Chengwei Qin,  Caiming Xiong,  Shafiq Joty</p>
  <p><b>备注</b>：10 pages, 1 figure, 3 tables</p>
  <p><b>关键词</b>：Large Language Models, Large Language, rise of Large, Language Models, opportunities are emerging</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the rise of Large Language Models (LLMs) in recent years, new opportunities are emerging, but also new challenges, and contamination is quickly becoming critical. Business applications and fundraising in AI have reached a scale at which a few percentage points gained on popular question-answering benchmarks could translate into dozens of millions of dollars, placing high pressure on model integrity. At the same time, it is becoming harder and harder to keep track of the data that LLMs have seen; if not impossible with closed-source models like GPT-4 and Claude-3 not divulging any information on the training set. As a result, contamination becomes a critical issue: LLMs' performance may not be reliable anymore, as the high performance may be at least partly due to their previous exposure to the data. This limitation jeopardizes the entire progress in the field of NLP, yet, there remains a lack of methods on how to efficiently address contamination, or a clear consensus on prevention, mitigation and classification of contamination. In this paper, we survey all recent work on contamination with LLMs, and help the community track contamination levels of LLMs by releasing an open-source Python library named LLMSanitize implementing major contamination detection algorithms, which link is: this https URL.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：CoUDA: Coherence Evaluation via Unified Data Augmentation</b></summary>
  <p><b>编号</b>：[314]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00681">https://arxiv.org/abs/2404.00681</a></p>
  <p><b>作者</b>：Dawei Zhu,  Wenhao Wu,  Yifan Song,  Fangwei Zhu,  Ziqiang Cao,  Sujian Li</p>
  <p><b>备注</b>：NAACL 2024</p>
  <p><b>关键词</b>：large language models, Coherence evaluation aims, remains challenging, era of large, large language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Coherence evaluation aims to assess the organization and structure of a discourse, which remains challenging even in the era of large language models. Due to the scarcity of annotated data, data augmentation is commonly used for training coherence evaluation models. However, previous augmentations for this task primarily rely on heuristic rules, lacking designing criteria as guidance. In this paper, we take inspiration from linguistic theory of discourse structure, and propose a data augmentation framework named CoUDA. CoUDA breaks down discourse coherence into global and local aspects, and designs augmentation strategies for both aspects, respectively. Especially for local coherence, we propose a novel generative strategy for constructing augmentation samples, which involves post-pretraining a generative model and applying two controlling mechanisms to control the difficulty of generated samples. During inference, CoUDA also jointly evaluates both global and local aspects to comprehensively assess the overall coherence of a discourse. Extensive experiments in coherence evaluation show that, with only 233M parameters, CoUDA achieves state-of-the-art performance in both pointwise scoring and pairwise ranking tasks, even surpassing recent GPT-3.5 and GPT-4 based metrics.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：A General and Efficient Training for Transformer via Token Expansion</b></summary>
  <p><b>编号</b>：[322]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00672">https://arxiv.org/abs/2404.00672</a></p>
  <p><b>作者</b>：Wenxuan Huang,  Yunhang Shen,  Jiao Xie,  Baochang Zhang,  Gaoqi He,  Ke Li,  Xing Sun,  Shaohui Lin</p>
  <p><b>备注</b>：Accepted to CVPR 2024. Code is available at this https URL</p>
  <p><b>关键词</b>：large training cost, extremely large training, Vision Transformers, training, requires an extremely</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The remarkable performance of Vision Transformers (ViTs) typically requires an extremely large training cost. Existing methods have attempted to accelerate the training of ViTs, yet typically disregard method universality with accuracy dropping. Meanwhile, they break the training consistency of the original transformers, including the consistency of hyper-parameters, architecture, and strategy, which prevents them from being widely applied to different Transformer networks. In this paper, we propose a novel token growth scheme Token Expansion (termed ToE) to achieve consistent training acceleration for ViTs. We introduce an "initialization-expansion-merging" pipeline to maintain the integrity of the intermediate feature distribution of original transformers, preventing the loss of crucial learnable information in the training process. ToE can not only be seamlessly integrated into the training and fine-tuning process of transformers (e.g., DeiT and LV-ViT), but also effective for efficient training frameworks (e.g., EfficientTrain), without twisting the original training hyper-parameters, architecture, and introducing additional training strategies. Extensive experiments demonstrate that ToE achieves about 1.3x faster for the training of ViTs in a lossless manner, or even with performance gains over the full-token training baselines. Code is available at this https URL .</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：Observations on Building RAG Systems for Technical Documents</b></summary>
  <p><b>编号</b>：[329]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00657">https://arxiv.org/abs/2404.00657</a></p>
  <p><b>作者</b>：Sumit Soman,  Sujoy Roychowdhury</p>
  <p><b>备注</b>：Published as a Tiny Paper at ICLR 2024</p>
  <p><b>关键词</b>：Retrieval augmented generation, capture domain information, technical documents creates, documents creates challenges, Retrieval augmented</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Retrieval augmented generation (RAG) for technical documents creates challenges as embeddings do not often capture domain information. We review prior art for important factors affecting RAG and perform experiments to highlight best practices and potential challenges to build RAG systems for technical documents.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：WavLLM: Towards Robust and Adaptive Speech Large Language Model</b></summary>
  <p><b>编号</b>：[330]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00656">https://arxiv.org/abs/2404.00656</a></p>
  <p><b>作者</b>：Shujie Hu,  Long Zhou,  Shujie Liu,  Sanyuan Chen,  Hongkun Hao,  Jing Pan,  Xunying Liu,  Jinyu Li,  Sunit Sivasankaran,  Linquan Liu,  Furu Wei</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：natural language processing, progressively broadening, perception and generation, recent advancements, revolutionized the field</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. We validate the proposed model on universal speech benchmarks including tasks such as ASR, ST, SV, ER, and also apply it to specialized datasets like Gaokao English listening comprehension set for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks on the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach. Furthermore, our model successfully completes Gaokao tasks without specialized training. The codes, models, audio, and Gaokao evaluation set can be accessed at \url{this http URL}.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：Against The Achilles' Heel: A Survey on Red Teaming for Generative  Models</b></summary>
  <p><b>编号</b>：[345]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00629">https://arxiv.org/abs/2404.00629</a></p>
  <p><b>作者</b>：Lizhi Lin,  Honglin Mu,  Zenan Zhai,  Minghan Wang,  Yuxia Wang,  Renxi Wang,  Junjie Gao,  Yixuan Zhang,  Wanxiang Che,  Timothy Baldwin,  Xudong Han,  Haonan Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：rapidly gaining popularity, raising concerns, vulnerabilities are exposed, rapidly gaining, gaining popularity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generative models are rapidly gaining popularity and being integrated into everyday applications, raising concerns over their safety issues as various vulnerabilities are exposed. Faced with the problem, the field of red teaming is experiencing fast-paced growth, which highlights the need for a comprehensive organization covering the entire pipeline and addressing emerging topics for the community. Our extensive survey, which examines over 120 papers, introduces a taxonomy of fine-grained attack strategies grounded in the inherent capabilities of language models. Additionally, we have developed the searcher framework that unifies various automatic red teaming approaches. Moreover, our survey covers novel areas including multimodal attacks and defenses, risks around multilingual models, overkill of harmless queries, and safety of downstream applications. We hope this survey can provide a systematic perspective on the field and unlock new areas of research.</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：Reporting Eye-Tracking Data Quality: Towards a New Standard</b></summary>
  <p><b>编号</b>：[352]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00620">https://arxiv.org/abs/2404.00620</a></p>
  <p><b>作者</b>：Deborah N. Jakobi,  Daniel G. Krakowczyk,  Lena A. Jäger</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：data considered irrelevant, original analyses, primary purpose, considered irrelevant, data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Eye-tracking datasets are often shared in the format used by their creators for their original analyses, usually resulting in the exclusion of data considered irrelevant to the primary purpose. In order to increase re-usability of existing eye-tracking datasets for more diverse and initially not considered use cases, this work advocates a new approach of sharing eye-tracking data. Instead of publishing filtered and pre-processed datasets, the eye-tracking data at all pre-processing stages should be published together with data quality reports. In order to transparently report data quality and enable cross-dataset comparisons, we develop data quality reporting standards and metrics that can be automatically applied to a dataset, and integrate them into the open-source Python package pymovements (this https URL).</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：Learning to Plan for Language Modeling from Unlabeled Data</b></summary>
  <p><b>编号</b>：[354]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00614">https://arxiv.org/abs/2404.00614</a></p>
  <p><b>作者</b>：Nathan Cornille,  Marie-Francine Moens,  Florian Mai</p>
  <p><b>备注</b>：under review</p>
  <p><b>关键词</b>：unlabeled corpus, labeled data, training to predict, learn to perform, perform many tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>By training to predict the next token in an unlabeled corpus, large language models learn to perform many tasks without any labeled data. However, their next-token-prediction objective arguably limits their performance in scenarios that require planning, such as writing a coherent article. In this paper, we train a module for planning the future writing process via a self-supervised learning objective. By conditioning on generated latent plans, our model extends the successful language model formula to more abstract planning in an unsupervised way. Empirically, we demonstrate that our method improves language modeling performance in general, particularly with respect to the text structure. Because our framework uses a planner module that is unsupervised and external to the language model, new planner modules can be trained at large scale and easily be shared with the community.</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation</b></summary>
  <p><b>编号</b>：[358]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00610">https://arxiv.org/abs/2404.00610</a></p>
  <p><b>作者</b>：Chi-Min Chan,  Chunpu Xu,  Ruibin Yuan,  Hongyin Luo,  Wei Xue,  Yike Guo,  Jie Fu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Large Language, exhibit remarkable capabilities, exhibit remarkable, prone to generating</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) exhibit remarkable capabilities but are prone to generating inaccurate or hallucinatory responses. This limitation stems from their reliance on vast pretraining datasets, making them susceptible to errors in unseen scenarios. To tackle these challenges, Retrieval-Augmented Generation (RAG) addresses this by incorporating external, relevant documents into the response generation process, thus leveraging non-parametric knowledge alongside LLMs' in-context learning abilities. However, existing RAG implementations primarily focus on initial input for context retrieval, overlooking the nuances of ambiguous or complex queries that necessitate further clarification or decomposition for accurate responses. To this end, we propose learning to Refine Query for Retrieval Augmented Generation (RQ-RAG) in this paper, endeavoring to enhance the model by equipping it with capabilities for explicit rewriting, decomposition, and disambiguation. Our experimental results indicate that our method, when applied to a 7B Llama2 model, surpasses the previous state-of-the-art (SOTA) by an average of 1.9\% across three single-hop QA datasets, and also demonstrates enhanced performance in handling complex, multi-hop QA datasets. Our code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：Extensive Self-Contrast Enables Feedback-Free Language Model Alignment</b></summary>
  <p><b>编号</b>：[359]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00604">https://arxiv.org/abs/2404.00604</a></p>
  <p><b>作者</b>：Xiao Liu,  Xixuan Song,  Yuxiao Dong,  Jie Tang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Reinforcement learning, recent large language, central technique, technique for recent, RLHF</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reinforcement learning from human feedback (RLHF) has been a central technique for recent large language model (LLM) alignment. However, its heavy dependence on costly human or LLM-as-Judge preference feedback could stymie its wider applications. In this work, we introduce Self-Contrast, a feedback-free large language model alignment method via exploiting extensive self-generated negatives. With only supervised fine-tuning (SFT) targets, Self-Contrast leverages the LLM itself to generate massive diverse candidates, and harnesses a pre-trained embedding model to filter multiple negatives according to text similarity. Theoretically, we illustrate that in this setting, merely scaling negative responses can still effectively approximate situations with more balanced positive and negative preference annotations. Our experiments with direct preference optimization (DPO) on three datasets show that, Self-Contrast could consistently outperform SFT and standard DPO training by large margins. And as the number of self-generated negatives increases, the performance of Self-Contrast continues to grow. Code and data are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：AI Act and Large Language Models (LLMs): When critical issues and  privacy impact require human and ethical oversight</b></summary>
  <p><b>编号</b>：[362]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00600">https://arxiv.org/abs/2404.00600</a></p>
  <p><b>作者</b>：Nicola Fabiano</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, personal data protection, artificial intelligence systems, Language Models, Large Language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The imposing evolution of artificial intelligence systems and, specifically, of Large Language Models (LLM) makes it necessary to carry out assessments of their level of risk and the impact they may have in the area of privacy, personal data protection and at an ethical level, especially on the weakest and most vulnerable. This contribution addresses human oversight, ethical oversight, and privacy impact assessment.</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：EvoCodeBench: An Evolving Code Generation Benchmark Aligned with  Real-World Code Repositories</b></summary>
  <p><b>编号</b>：[363]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00599">https://arxiv.org/abs/2404.00599</a></p>
  <p><b>作者</b>：Jia Li,  Ge Li,  Xuanming Zhang,  Yihong Dong,  Zhi Jin</p>
  <p><b>备注</b>：Data: this https URL</p>
  <p><b>关键词</b>：Large Language Models, evaluate Large Language, Language Models, Large Language, evaluate Large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>How to evaluate Large Language Models (LLMs) in code generation is an open question. Existing benchmarks demonstrate poor alignment with real-world code repositories and are insufficient to evaluate the coding abilities of LLMs. This paper proposes a new benchmark - EvoCodeBench to address the preceding problems, which has three primary advances. (1) EvoCodeBench aligns with real-world repositories in multiple dimensions, e.g., code distributions and dependency distributions. (2) EvoCodeBench offers comprehensive annotations (e.g., requirements, reference code, and reference dependencies), and robust evaluation metrics (e.g., Pass@k and Recall@k). (3) EvoCodeBench is an evolving benchmark to avoid data leakage. We build an automatic pipeline to update EvoCodeBench from the latest repositories. We release the first version - EvoCodeBench-2403, containing 275 samples from 25 real-world repositories. Based on EvoCodeBench, we propose repository-level code generation and evaluate 10 popular LLMs (e.g., gpt-4, gpt-3.5, DeepSeek Coder, StarCoder 2, CodeLLaMa, Gemma, and Qwen 1.5). Our experiments reveal the coding abilities of these LLMs in real-world repositories. For example, the highest Pass@1 of gpt-4 only is 20.73% in our experiments. We also analyze failed cases and summarize the shortcomings of existing LLMs in EvoCodeBench. We release EvoCodeBench, all prompts, and LLMs' completions for further community analysis.</p>
  </details>
</details>
<details>
  <summary>83. <b>标题：ECtHR-PCR: A Dataset for Precedent Understanding and Prior Case  Retrieval in the European Court of Human Rights</b></summary>
  <p><b>编号</b>：[366]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00596">https://arxiv.org/abs/2404.00596</a></p>
  <p><b>作者</b>：T.Y.S.S Santosh,  Rashid Gustav Haddad,  Matthias Grabmair</p>
  <p><b>备注</b>：Accepted to LREC-COLING 2024</p>
  <p><b>关键词</b>：legal practitioners rely, common law jurisdictions, stare decisis, common law, practitioners rely</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In common law jurisdictions, legal practitioners rely on precedents to construct arguments, in line with the doctrine of \emph{stare decisis}. As the number of cases grow over the years, prior case retrieval (PCR) has garnered significant attention. Besides lacking real-world scale, existing PCR datasets do not simulate a realistic setting, because their queries use complete case documents while only masking references to prior cases. The query is thereby exposed to legal reasoning not yet available when constructing an argument for an undecided case as well as spurious patterns left behind by citation masks, potentially short-circuiting a comprehensive understanding of case facts and legal principles. To address these limitations, we introduce a PCR dataset based on judgements from the European Court of Human Rights (ECtHR), which explicitly separate facts from arguments and exhibit precedential practices, aiding us to develop this PCR dataset to foster systems' comprehensive understanding. We benchmark different lexical and dense retrieval approaches with various negative sampling strategies, adapting them to deal with long text sequences using hierarchical variants. We found that difficulty-based negative sampling strategies were not effective for the PCR task, highlighting the need for investigation into domain-specific difficulty criteria. Furthermore, we observe performance of the dense models degrade with time and calls for further research into temporal adaptation of retrieval models. Additionally, we assess the influence of different views , Halsbury's and Goodhart's, in practice in ECtHR jurisdiction using PCR task.</p>
  </details>
</details>
<details>
  <summary>84. <b>标题：Query-driven Relevant Paragraph Extraction from Legal Judgments</b></summary>
  <p><b>编号</b>：[367]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00595">https://arxiv.org/abs/2404.00595</a></p>
  <p><b>作者</b>：T.Y.S.S Santosh,  Elvin Quero Hernandez,  Matthias Grabmair</p>
  <p><b>备注</b>：Accepted to LREC-COLING 2024</p>
  <p><b>关键词</b>：navigating lengthy legal, lengthy legal judgements, professionals often grapple, grapple with navigating, navigating lengthy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Legal professionals often grapple with navigating lengthy legal judgements to pinpoint information that directly address their queries. This paper focus on this task of extracting relevant paragraphs from legal judgements based on the query. We construct a specialized dataset for this task from the European Court of Human Rights (ECtHR) using the case law guides. We assess the performance of current retrieval models in a zero-shot way and also establish fine-tuning benchmarks using various models. The results highlight the significant gap between fine-tuned and zero-shot performance, emphasizing the challenge of handling distribution shift in the legal domain. We notice that the legal pre-training handles distribution shift on the corpus side but still struggles on query side distribution shift, with unseen legal queries. We also explore various Parameter Efficient Fine-Tuning (PEFT) methods to evaluate their practicality within the context of information retrieval, shedding light on the effectiveness of different PEFT methods across diverse configurations with pre-training and model architectures influencing the choice of PEFT method.</p>
  </details>
</details>
<details>
  <summary>85. <b>标题：LexAbSumm: Aspect-based Summarization of Legal Decisions</b></summary>
  <p><b>编号</b>：[368]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00594">https://arxiv.org/abs/2404.00594</a></p>
  <p><b>作者</b>：T.Y.S.S Santosh,  Mahmoud Aly,  Matthias Grabmair</p>
  <p><b>备注</b>：Accepted to LREC-COLING 2024</p>
  <p><b>关键词</b>：professionals frequently encounter, frequently encounter long, hold critical insights, encounter long legal, Legal professionals frequently</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Legal professionals frequently encounter long legal judgments that hold critical insights for their work. While recent advances have led to automated summarization solutions for legal documents, they typically provide generic summaries, which may not meet the diverse information needs of users. To address this gap, we introduce LexAbSumm, a novel dataset designed for aspect-based summarization of legal case decisions, sourced from the European Court of Human Rights jurisdiction. We evaluate several abstractive summarization models tailored for longer documents on LexAbSumm, revealing a challenge in conditioning these models to produce aspect-specific summaries. We release LexAbSum to facilitate research in aspect-based summarization for legal domain.</p>
  </details>
</details>
<details>
  <summary>86. <b>标题：CuSINeS: Curriculum-driven Structure Induced Negative Sampling for  Statutory Article Retrieval</b></summary>
  <p><b>编号</b>：[371]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00590">https://arxiv.org/abs/2404.00590</a></p>
  <p><b>作者</b>：T.Y.S.S Santosh,  Kristina Kaiser,  Matthias Grabmair</p>
  <p><b>备注</b>：Accepted to LREC-COLING 2024</p>
  <p><b>关键词</b>：Statutory Article Retrieval, Article Retrieval, Statutory Article, performance of Statutory, negative sampling approach</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we introduce CuSINeS, a negative sampling approach to enhance the performance of Statutory Article Retrieval (SAR). CuSINeS offers three key contributions. Firstly, it employs a curriculum-based negative sampling strategy guiding the model to focus on easier negatives initially and progressively tackle more difficult ones. Secondly, it leverages the hierarchical and sequential information derived from the structural organization of statutes to evaluate the difficulty of samples. Lastly, it introduces a dynamic semantic difficulty assessment using the being-trained model itself, surpassing conventional static methods like BM25, adapting the negatives to the model's evolving competence. Experimental results on a real-world expert-annotated SAR dataset validate the effectiveness of CuSINeS across four different baselines, demonstrating its versatility.</p>
  </details>
</details>
<details>
  <summary>87. <b>标题：Harnessing the Power of Large Language Model for Uncertainty Aware Graph  Processing</b></summary>
  <p><b>编号</b>：[372]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00589">https://arxiv.org/abs/2404.00589</a></p>
  <p><b>作者</b>：Zhenyu Qian,  Yiming Qian,  Yuting Song,  Fei Gao,  Hai Jin,  Chen Yu,  Xia Xie</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Handling graph data, graph data, large graph data, handling large graph, graph</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Handling graph data is one of the most difficult tasks. Traditional techniques, such as those based on geometry and matrix factorization, rely on assumptions about the data relations that become inadequate when handling large and complex graph data. On the other hand, deep learning approaches demonstrate promising results in handling large graph data, but they often fall short of providing interpretable explanations. To equip the graph processing with both high accuracy and explainability, we introduce a novel approach that harnesses the power of a large language model (LLM), enhanced by an uncertainty-aware module to provide a confidence score on the generated answer. We experiment with our approach on two graph processing tasks: few-shot knowledge graph completion and graph classification. Our results demonstrate that through parameter efficient fine-tuning, the LLM surpasses state-of-the-art algorithms by a substantial margin across ten diverse benchmark datasets. Moreover, to address the challenge of explainability, we propose an uncertainty estimation based on perturbation, along with a calibration scheme to quantify the confidence scores of the generated answers. Our confidence measure achieves an AUC of 0.8 or higher on seven out of the ten datasets in predicting the correctness of the answer generated by LLM.</p>
  </details>
</details>
<details>
  <summary>88. <b>标题：Explainable Multi-hop Question Generation: An End-to-End Approach  without Intermediate Question Labeling</b></summary>
  <p><b>编号</b>：[381]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00571">https://arxiv.org/abs/2404.00571</a></p>
  <p><b>作者</b>：Seonjeong Hwang,  Yunsu Kim,  Gary Geunbae Lee</p>
  <p><b>备注</b>：LREC-Coling 2024</p>
  <p><b>关键词</b>：interactive artificial intelligence, handle complex questions, questions, complex questions, artificial intelligence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In response to the increasing use of interactive artificial intelligence, the demand for the capacity to handle complex questions has increased. Multi-hop question generation aims to generate complex questions that requires multi-step reasoning over several documents. Previous studies have predominantly utilized end-to-end models, wherein questions are decoded based on the representation of context documents. However, these approaches lack the ability to explain the reasoning process behind the generated multi-hop questions. Additionally, the question rewriting approach, which incrementally increases the question complexity, also has limitations due to the requirement of labeling data for intermediate-stage questions. In this paper, we introduce an end-to-end question rewriting model that increases question complexity through sequential rewriting. The proposed model has the advantage of training with only the final multi-hop questions, without intermediate questions. Experimental results demonstrate the effectiveness of our model in generating complex questions, particularly 3- and 4-hop questions, which are appropriately paired with input answers. We also prove that our model logically and incrementally increases the complexity of questions, and the generated multi-hop questions are also beneficial for training question answering models.</p>
  </details>
</details>
<details>
  <summary>89. <b>标题：ParaICL: Towards Robust Parallel In-Context Learning</b></summary>
  <p><b>编号</b>：[382]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00570">https://arxiv.org/abs/2404.00570</a></p>
  <p><b>作者</b>：Xingxuan Li,  Xuan-Phi Nguyen,  Shafiq Joty,  Lidong Bing</p>
  <p><b>备注</b>：Work in progress</p>
  <p><b>关键词</b>：Large language models, natural language processing, Large language, language models, language processing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have become the norm in natural language processing (NLP), excelling in few-shot in-context learning (ICL) with their remarkable abilities. Nonetheless, the success of ICL largely hinges on the choice of few-shot demonstration examples, making the selection process increasingly crucial. Existing methods have delved into optimizing the quantity and semantic similarity of these examples to improve ICL performances. However, our preliminary experiments indicate that the effectiveness of ICL is limited by the length of the input context. Moreover, varying combinations of few-shot demonstration examples can significantly boost accuracy across different test samples. To address this, we propose a novel method named parallel in-context learning (ParaICL) that effectively utilizes all demonstration examples without exceeding the manageable input context length. ParaICL employs parallel batching to distribute demonstration examples into different batches according to the semantic similarities of the questions in the demonstrations to the test question. It then computes normalized batch semantic scores for each batch. A weighted average semantic objective, constrained by adaptive plausibility, is applied to select the most appropriate tokens. Through extensive experiments, we validate the effectiveness of ParaICL and conduct ablation studies to underscore its design rationale. We further demonstrate that ParaICL can seamlessly integrate with existing methods.</p>
  </details>
</details>
<details>
  <summary>90. <b>标题：CM-TTS: Enhancing Real Time Text-to-Speech Synthesis Efficiency through  Weighted Samplers and Consistency Models</b></summary>
  <p><b>编号</b>：[383]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00569">https://arxiv.org/abs/2404.00569</a></p>
  <p><b>作者</b>：Xiang Li,  Fan Bu,  Ambuj Mehrish,  Yingting Li,  Jiale Han,  Bo Cheng,  Soujanya Poria</p>
  <p><b>备注</b>：Accepted by Findings of NAACL 2024. Code is available at this https URL</p>
  <p><b>关键词</b>：find broad applications, systems find broad, voice assistants, audiobook creation, Diffusion Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural Text-to-Speech (TTS) systems find broad applications in voice assistants, e-learning, and audiobook creation. The pursuit of modern models, like Diffusion Models (DMs), holds promise for achieving high-fidelity, real-time speech synthesis. Yet, the efficiency of multi-step sampling in Diffusion Models presents challenges. Efforts have been made to integrate GANs with DMs, speeding up inference by approximating denoising distributions, but this introduces issues with model convergence due to adversarial training. To overcome this, we introduce CM-TTS, a novel architecture grounded in consistency models (CMs). Drawing inspiration from continuous-time diffusion models, CM-TTS achieves top-quality speech synthesis in fewer steps without adversarial training or pre-trained model dependencies. We further design weighted samplers to incorporate different sampling positions into model training with dynamic probabilities, ensuring unbiased learning throughout the entire training process. We present a real-time mel-spectrogram generation consistency model, validated through comprehensive evaluations. Experimental results underscore CM-TTS's superiority over existing single-step speech synthesis systems, representing a significant advancement in the field.</p>
  </details>
</details>
<details>
  <summary>91. <b>标题：CodeBenchGen: Creating Scalable Execution-based Code Generation  Benchmarks</b></summary>
  <p><b>编号</b>：[384]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00566">https://arxiv.org/abs/2404.00566</a></p>
  <p><b>作者</b>：Yiqing Xie,  Alex Xie,  Divyanshu Sheth,  Pengfei Liu,  Daniel Fried,  Carolyn Rose</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：scalable execution-based benchmarks, create scalable execution-based, requires light guidance, diverse scenarios, systems across diverse</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To facilitate evaluation of code generation systems across diverse scenarios, we present CodeBenchGen, a framework to create scalable execution-based benchmarks that only requires light guidance from humans. Specifically, we leverage a large language model (LLM) to convert an arbitrary piece of code into an evaluation example, including test cases for execution-based evaluation. We illustrate the usefulness of our framework by creating a dataset, Exec-CSN, which includes 1,931 examples involving 293 libraries revised from code in 367 GitHub repositories taken from the CodeSearchNet dataset. To demonstrate the complexity and solvability of examples in Exec-CSN, we present a human study demonstrating that 81.3% of the examples can be solved by humans and 61% are rated as ``requires effort to solve''. We conduct code generation experiments on open-source and proprietary models and analyze the performance of both humans and models. We will release the code of both the framework and the dataset upon acceptance.</p>
  </details>
</details>
<details>
  <summary>92. <b>标题：Leveraging Corpus Metadata to Detect Template-based Translation: An  Exploratory Case Study of the Egyptian Arabic Wikipedia Edition</b></summary>
  <p><b>编号</b>：[385]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00565">https://arxiv.org/abs/2404.00565</a></p>
  <p><b>作者</b>：Saied Alshahrani,  Hesham Haroon,  Ali Elfilali,  Mariama Njie,  Jeanna Matthews</p>
  <p><b>备注</b>：This paper has been accepted at LREC-COLING 2024: The 6th Workshop on Open-Source Arabic Corpora and Processing Tools (OSACT6)</p>
  <p><b>关键词</b>：Natural Language Processing, Egyptian Arabic Wikipedia, Arabic Wikipedia, Arabic Wikipedia editions, Moroccan Arabic Wikipedia</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Wikipedia articles (content pages) are commonly used corpora in Natural Language Processing (NLP) research, especially in low-resource languages other than English. Yet, a few research studies have studied the three Arabic Wikipedia editions, Arabic Wikipedia (AR), Egyptian Arabic Wikipedia (ARZ), and Moroccan Arabic Wikipedia (ARY), and documented issues in the Egyptian Arabic Wikipedia edition regarding the massive automatic creation of its articles using template-based translation from English to Arabic without human involvement, overwhelming the Egyptian Arabic Wikipedia with articles that do not only have low-quality content but also with articles that do not represent the Egyptian people, their culture, and their dialect. In this paper, we aim to mitigate the problem of template translation that occurred in the Egyptian Arabic Wikipedia by identifying these template-translated articles and their characteristics through exploratory analysis and building automatic detection systems. We first explore the content of the three Arabic Wikipedia editions in terms of density, quality, and human contributions and utilize the resulting insights to build multivariate machine learning classifiers leveraging articles' metadata to detect the template-translated articles automatically. We then publicly deploy and host the best-performing classifier, XGBoost, as an online application called EGYPTIAN WIKIPEDIA SCANNER and release the extracted, filtered, and labeled datasets to the research community to benefit from our datasets and the online, web-based detection system.</p>
  </details>
</details>
<details>
  <summary>93. <b>标题：DivTOD: Unleashing the Power of LLMs for Diversifying Task-Oriented  Dialogue Representations</b></summary>
  <p><b>编号</b>：[390]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00557">https://arxiv.org/abs/2404.00557</a></p>
  <p><b>作者</b>：Weihao Zeng,  Dayuan Fu,  Keqing He,  Yejie Wang,  Yukai Xu,  Weiran Xu</p>
  <p><b>备注</b>：NAACL 2024 (Findings)</p>
  <p><b>关键词</b>：achieved impressive results, Language models pre-trained, general text, achieved impressive, impressive results</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Language models pre-trained on general text have achieved impressive results in diverse fields. Yet, the distinct linguistic characteristics of task-oriented dialogues (TOD) compared to general text limit the practical utility of existing language models. Current task-oriented dialogue pre-training methods overlook the one-to-many property of conversations, where multiple responses can be appropriate given the same conversation context. In this paper, we propose a novel dialogue pre-training model called DivTOD, which collaborates with LLMs to learn diverse task-oriented dialogue representations. DivTOD guides LLMs in transferring diverse knowledge to smaller models while removing domain knowledge that contradicts task-oriented dialogues. Experiments show that our model outperforms strong TOD baselines on various downstream dialogue tasks and learns the intrinsic diversity of task-oriented dialogues.</p>
  </details>
</details>
<details>
  <summary>94. <b>标题：Comparing Bad Apples to Good Oranges: Aligning Large Language Models via  Joint Preference Optimization</b></summary>
  <p><b>编号</b>：[400]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00530">https://arxiv.org/abs/2404.00530</a></p>
  <p><b>作者</b>：Hritik Bansal,  Ashima Suvarna,  Gantavya Bhatt,  Nanyun Peng,  Kai-Wei Chang,  Aditya Grover</p>
  <p><b>备注</b>：25 pages, 14 figures, 5 tables</p>
  <p><b>关键词</b>：large language models, aligning large language, comparing multiple generations, multiple generations conditioned, acquiring human preferences</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A common technique for aligning large language models (LLMs) relies on acquiring human preferences by comparing multiple generations conditioned on a fixed context. This only leverages the pairwise comparisons when the generations are placed in an identical context. However, such conditional rankings often fail to capture the complex and multidimensional aspects of human preferences. In this work, we revisit the traditional paradigm of preference acquisition and propose a new axis that is based on eliciting preferences jointly over the instruction-response pairs. While prior preference optimizations are designed for conditional ranking protocols (e.g., DPO), our proposed preference acquisition protocol introduces DOVE, a new preference optimization objective that upweights the joint probability of the chosen instruction-response pair over the rejected instruction-response pair. Interestingly, we find that the LLM trained with joint instruction-response preference data using DOVE outperforms the LLM trained with DPO by 5.2% and 3.3% win-rate for the summarization and open-ended dialogue datasets, respectively. Our findings reveal that joint preferences over instruction and response pairs can significantly enhance the alignment of LLMs by tapping into a broader spectrum of human preference elicitation. The data and code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>95. <b>标题：MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in  Conversations with Multimodal Language Models</b></summary>
  <p><b>编号</b>：[412]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00511">https://arxiv.org/abs/2404.00511</a></p>
  <p><b>作者</b>：Zebang Cheng,  Fuqiang Niu,  Yuxiang Lin,  Zhi-Qi Cheng,  Bowen Zhang,  Xiaojiang Peng</p>
  <p><b>备注</b>：Ranked 3rd in SemEval '24 Task 3 with F1 of 0.3435, close to 1st & 2nd by 0.0339 & 0.0025</p>
  <p><b>关键词</b>：Multimodal Emotion Recognition, multimodal emotion, analysis in conversations, paper presents, presents our winning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents our winning submission to Subtask 2 of SemEval 2024 Task 3 on multimodal emotion cause analysis in conversations. We propose a novel Multimodal Emotion Recognition and Multimodal Emotion Cause Extraction (MER-MCE) framework that integrates text, audio, and visual modalities using specialized emotion encoders. Our approach sets itself apart from top-performing teams by leveraging modality-specific features for enhanced emotion understanding and causality inference. Experimental evaluation demonstrates the advantages of our multimodal approach, with our submission achieving a competitive weighted F1 score of 0.3435, ranking third with a margin of only 0.0339 behind the 1st team and 0.0025 behind the 2nd team. Project: this https URL</p>
  </details>
</details>
<details>
  <summary>96. <b>标题：The Shape of Word Embeddings: Recognizing Language Phylogenies through  Topological Data Analysis</b></summary>
  <p><b>编号</b>：[420]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00500">https://arxiv.org/abs/2404.00500</a></p>
  <p><b>作者</b>：Ondřej Draganov,  Steven Skiena</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Word embeddings represent, represent language vocabularies, dimensional points, embeddings represent language, Word embeddings</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Word embeddings represent language vocabularies as clouds of $d$-dimensional points. We investigate how information is conveyed by the general shape of these clouds, outside of representing the semantic meaning of each token. Specifically, we use the notion of persistent homology from topological data analysis (TDA) to measure the distances between language pairs from the shape of their unlabeled embeddings. We use these distance matrices to construct language phylogenetic trees over 81 Indo-European languages. Careful evaluation shows that our reconstructed trees exhibit strong similarities to the reference tree.</p>
  </details>
</details>
<details>
  <summary>97. <b>标题：Configurable Safety Tuning of Language Models with Synthetic Preference  Data</b></summary>
  <p><b>编号</b>：[422]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00495">https://arxiv.org/abs/2404.00495</a></p>
  <p><b>作者</b>：Victor Gallego</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Direct Preference Optimization, restrict user control, hard-coding predefined behaviors, Preference Optimization, language model fine-tuning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>State-of-the-art language model fine-tuning techniques, such as Direct Preference Optimization (DPO), restrict user control by hard-coding predefined behaviors into the model. To address this, we propose a novel method, Configurable Safety Tuning (CST), that augments DPO using synthetic preference data to facilitate flexible safety configuration of LLMs at inference time. CST overcomes the constraints of vanilla DPO by introducing a system prompt specifying safety configurations, enabling LLM deployers to disable/enable safety preferences based on their need, just changing the system prompt. Our experimental evaluations indicate that CST successfully manages different safety configurations and retains the original functionality of LLMs, showing it is a robust method for configurable deployment. Data and models available at this https URL</p>
  </details>
</details>
<details>
  <summary>98. <b>标题：Multi-hop Question Answering under Temporal Knowledge Editing</b></summary>
  <p><b>编号</b>：[424]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00492">https://arxiv.org/abs/2404.00492</a></p>
  <p><b>作者</b>：Keyuan Cheng,  Gang Lin,  Haoyang Fei,  Yuxuan zhai,  Lu Yu,  Muhammad Asif Ali,  Lijie Hu,  Di Wang</p>
  <p><b>备注</b>：23 pages</p>
  <p><b>关键词</b>：Multi-hop question answering, garnered significant attention, large language models, question answering, garnered significant</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multi-hop question answering (MQA) under knowledge editing (KE) has garnered significant attention in the era of large language models. However, existing models for MQA under KE exhibit poor performance when dealing with questions containing explicit temporal contexts. To address this limitation, we propose a novel framework, namely TEMPoral knowLEdge augmented Multi-hop Question Answering (TEMPLE-MQA). Unlike previous methods, TEMPLE-MQA first constructs a time-aware graph (TAG) to store edit knowledge in a structured manner. Then, through our proposed inference path, structural retrieval, and joint reasoning stages, TEMPLE-MQA effectively discerns temporal contexts within the question query. Experiments on benchmark datasets demonstrate that TEMPLE-MQA significantly outperforms baseline models. Additionally, we contribute a new dataset, namely TKEMQA, which serves as the inaugural benchmark tailored specifically for MQA with temporal scopes.</p>
  </details>
</details>
<details>
  <summary>99. <b>标题：PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt  Compression</b></summary>
  <p><b>编号</b>：[426]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00489">https://arxiv.org/abs/2404.00489</a></p>
  <p><b>作者</b>：Muhammad Asif Ali,  Zhengping Li,  Shu Yang,  Keyuan Cheng,  Yang Cao,  Tianhao Huang,  Lijie Hu,  Lu Yu,  Di Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：language processing tasks, natural language processing, shown exceptional abilities, Large language models, Large language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have shown exceptional abilities for multiple different natural language processing tasks. While prompting is a crucial tool for LLM inference, we observe that there is a significant cost associated with exceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead to sub-standard results in terms of readability and interpretability of the compressed prompt, with a detrimental impact on prompt utility. To address this, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an effective strategy for prompt compression over task-agnostic and task-aware prompts. PROMPT-SAW uses the prompt's textual information to build a graph, later extracts key information elements in the graph to come up with the compressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the existing GSM8k benchmark for task-agnostic prompts in order to provide a comprehensive evaluation platform. Experimental evaluation using benchmark datasets shows that prompts compressed by PROMPT-SAW are not only better in terms of readability, but they also outperform the best-performing baseline models by up to 14.3 and 13.7 respectively for task-aware and task-agnostic settings while compressing the original prompt text by 33.0 and 56.7.</p>
  </details>
</details>
<details>
  <summary>100. <b>标题：Noise-Aware Training of Layout-Aware Language Models</b></summary>
  <p><b>编号</b>：[427]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00488">https://arxiv.org/abs/2404.00488</a></p>
  <p><b>作者</b>：Ritesh Sarkhel,  Xiaoqi Ren,  Lauro Beltrao Costa,  Guolong Su,  Vincent Perot,  Yanan Xie,  Emmanouil Koukoumidis,  Arnab Nandi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：visually rich document, disseminate information, target document type, utilizes visual features, visually rich</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A visually rich document (VRD) utilizes visual features along with linguistic cues to disseminate information. Training a custom extractor that identifies named entities from a document requires a large number of instances of the target document type annotated at textual and visual modalities. This is an expensive bottleneck in enterprise scenarios, where we want to train custom extractors for thousands of different document types in a scalable way. Pre-training an extractor model on unlabeled instances of the target document type, followed by a fine-tuning step on human-labeled instances does not work in these scenarios, as it surpasses the maximum allowable training time allocated for the extractor. We address this scenario by proposing a Noise-Aware Training method or NAT in this paper. Instead of acquiring expensive human-labeled documents, NAT utilizes weakly labeled documents to train an extractor in a scalable way. To avoid degradation in the model's quality due to noisy, weakly labeled samples, NAT estimates the confidence of each training sample and incorporates it as uncertainty measure during training. We train multiple state-of-the-art extractor models using NAT. Experiments on a number of publicly available and in-house datasets show that NAT-trained models are not only robust in performance -- it outperforms a transfer-learning baseline by up to 6% in terms of macro-F1 score, but it is also more label-efficient -- it reduces the amount of human-effort required to obtain comparable performance by up to 73%.</p>
  </details>
</details>
<details>
  <summary>101. <b>标题：Dialectical Alignment: Resolving the Tension of 3H and Security Threats  of LLMs</b></summary>
  <p><b>编号</b>：[429]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00486">https://arxiv.org/abs/2404.00486</a></p>
  <p><b>作者</b>：Shu Yang,  Jiayuan Su,  Han Jiang,  Mengdi Li,  Keyuan Cheng,  Muhammad Asif Ali,  Lijie Hu,  Di Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, ensuring they embody, rise of large, large language, embody the principles</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the rise of large language models (LLMs), ensuring they embody the principles of being helpful, honest, and harmless (3H), known as Human Alignment, becomes crucial. While existing alignment methods like RLHF, DPO, etc., effectively fine-tune LLMs to match preferences in the preference dataset, they often lead LLMs to highly receptive human input and external evidence, even when this information is poisoned. This leads to a tendency for LLMs to be Adaptive Chameleons when external evidence conflicts with their parametric memory. This exacerbates the risk of LLM being attacked by external poisoned data, which poses a significant security risk to LLM system applications such as Retrieval-augmented generation (RAG). To address the challenge, we propose a novel framework: Dialectical Alignment (DA), which (1) utilizes AI feedback to identify optimal strategies for LLMs to navigate inter-context conflicts and context-memory conflicts with different external evidence in context window (i.e., different ratios of poisoned factual contexts); (2) constructs the SFT dataset as well as the preference dataset based on the AI feedback and strategies above; (3) uses the above datasets for LLM alignment to defense poisoned context attack while preserving the effectiveness of in-context knowledge editing. Our experiments show that the dialectical alignment model improves poisoned data attack defense by 20 and does not require any additional prompt engineering or prior declaration of ``you may be attacked`` to the LLMs' context window.</p>
  </details>
</details>
<details>
  <summary>102. <b>标题：Edinburgh Clinical NLP at SemEval-2024 Task 2: Fine-tune your model  unless you have access to GPT-4</b></summary>
  <p><b>编号</b>：[431]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00484">https://arxiv.org/abs/2404.00484</a></p>
  <p><b>作者</b>：Aryo Pradipta Gema,  Giwon Hong,  Pasquale Minervini,  Luke Daines,  Beatrice Alex</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Clinical Trial Reports, Natural Language Inference, task assesses Natural, Language Inference systems, assesses Natural Language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The NLI4CT task assesses Natural Language Inference systems in predicting whether hypotheses entail or contradict evidence from Clinical Trial Reports. In this study, we evaluate various Large Language Models (LLMs) with multiple strategies, including Chain-of-Thought, In-Context Learning, and Parameter-Efficient Fine-Tuning (PEFT). We propose a PEFT method to improve the consistency of LLMs by merging adapters that were fine-tuned separately using triplet and language modelling objectives. We found that merging the two PEFT adapters improves the F1 score (+0.0346) and consistency (+0.152) of the LLMs. However, our novel methods did not produce more accurate results than GPT-4 in terms of faithfulness and consistency. Averaging the three metrics, GPT-4 ranks joint-first in the competition with 0.8328. Finally, our contamination analysis with GPT-4 indicates that there was no test data leakage.</p>
  </details>
</details>
<details>
  <summary>103. <b>标题：Cross-lingual Named Entity Corpus for Slavic Languages</b></summary>
  <p><b>编号</b>：[432]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00482">https://arxiv.org/abs/2404.00482</a></p>
  <p><b>作者</b>：Jakub Piskorski,  Michał Marcińczuk,  Roman Yangarber</p>
  <p><b>备注</b>：Published in LREC-COLING 2024 - The 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation</p>
  <p><b>关键词</b>：Slavic Natural Language, Natural Language Processing, Slavic languages, Slavic Natural, corpus manually annotated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a corpus manually annotated with named entities for six Slavic languages - Bulgarian, Czech, Polish, Slovenian, Russian, and Ukrainian. This work is the result of a series of shared tasks, conducted in 2017-2023 as a part of the Workshops on Slavic Natural Language Processing. The corpus consists of 5 017 documents on seven topics. The documents are annotated with five classes of named entities. Each entity is described by a category, a lemma, and a unique cross-lingual identifier. We provide two train-tune dataset splits - single topic out and cross topics. For each split, we set benchmarks using a transformer-based neural network architecture with the pre-trained multilingual models - XLM-RoBERTa-large for named entity mention recognition and categorization, and mT5-large for named entity lemmatization and linking.</p>
  </details>
</details>
<details>
  <summary>104. <b>标题：Linguistic Calibration of Language Models</b></summary>
  <p><b>编号</b>：[435]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00474">https://arxiv.org/abs/2404.00474</a></p>
  <p><b>作者</b>：Neil Band,  Xuechen Li,  Tengyu Ma,  Tatsunori Hashimoto</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：suboptimal downstream decisions, confidently hallucinate, Language models, long-form generations, generations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate. This issue can be mitigated by having the LM verbally convey the probability that its claims are correct, but existing models cannot produce text with calibrated confidence statements. Through the lens of decision-making, we formalize linguistic calibration for long-form generations: an LM is linguistically calibrated if its generations enable its users to make calibrated probabilistic predictions. This definition enables a training framework where a supervised finetuning step bootstraps an LM to emit long-form generations with confidence statements such as "I estimate a 30% chance of..." or "I am certain that...", followed by a reinforcement learning step which rewards generations that enable a user to provide calibrated answers to related questions. We linguistically calibrate Llama 2 7B and find in automated and human evaluations of long-form generations that it is significantly more calibrated than strong finetuned factuality baselines with comparable accuracy. These findings generalize under distribution shift on question-answering and under a significant task shift to person biography generation. Our results demonstrate that long-form generations may be calibrated end-to-end by constructing an objective in the space of the predictions that users make in downstream decision-making.</p>
  </details>
</details>
<details>
  <summary>105. <b>标题：Addressing Both Statistical and Causal Gender Fairness in NLP Models</b></summary>
  <p><b>编号</b>：[441]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00463">https://arxiv.org/abs/2404.00463</a></p>
  <p><b>作者</b>：Hannah Chen,  Yangfeng Ji,  David Evans</p>
  <p><b>备注</b>：NAACL 2024 (Findings)</p>
  <p><b>关键词</b>：stipulates equivalent outcomes, fairness stipulates equivalent, protected group, protected characteristics, causal fairness prescribes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Statistical fairness stipulates equivalent outcomes for every protected group, whereas causal fairness prescribes that a model makes the same prediction for an individual regardless of their protected characteristics. Counterfactual data augmentation (CDA) is effective for reducing bias in NLP models, yet models trained with CDA are often evaluated only on metrics that are closely tied to the causal fairness notion; similarly, sampling-based methods designed to promote statistical fairness are rarely evaluated for causal fairness. In this work, we evaluate both statistical and causal debiasing methods for gender bias in NLP models, and find that while such methods are effective at reducing bias as measured by the targeted metric, they do not necessarily improve results on other bias metrics. We demonstrate that combinations of statistical and causal debiasing techniques are able to reduce bias measured through both types of metrics.</p>
  </details>
</details>
<details>
  <summary>106. <b>标题：Shortcuts Arising from Contrast: Effective and Covert Clean-Label  Attacks in Prompt-Based Learning</b></summary>
  <p><b>编号</b>：[443]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00461">https://arxiv.org/abs/2404.00461</a></p>
  <p><b>作者</b>：Xiaopeng Xie,  Ming Yan,  Xiwen Zhou,  Chenlong Zhao,  Suli Wang,  Yong Zhang,  Joey Tianyi Zhou</p>
  <p><b>备注</b>：10 pages, 6 figures, conference</p>
  <p><b>关键词</b>：pretrained language models, demonstrated remarkable efficacy, Prompt-based learning paradigm, learning paradigm, language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Prompt-based learning paradigm has demonstrated remarkable efficacy in enhancing the adaptability of pretrained language models (PLMs), particularly in few-shot scenarios. However, this learning paradigm has been shown to be vulnerable to backdoor attacks. The current clean-label attack, employing a specific prompt as a trigger, can achieve success without the need for external triggers and ensure correct labeling of poisoned samples, which is more stealthy compared to the poisoned-label attack, but on the other hand, it faces significant issues with false activations and poses greater challenges, necessitating a higher rate of poisoning. Using conventional negative data augmentation methods, we discovered that it is challenging to trade off between effectiveness and stealthiness in a clean-label setting. In addressing this issue, we are inspired by the notion that a backdoor acts as a shortcut and posit that this shortcut stems from the contrast between the trigger and the data utilized for poisoning. In this study, we propose a method named Contrastive Shortcut Injection (CSI), by leveraging activation values, integrates trigger design and data selection strategies to craft stronger shortcut features. With extensive experiments on full-shot and few-shot text classification tasks, we empirically validate CSI's high effectiveness and high stealthiness at low poisoning rates. Notably, we found that the two approaches play leading roles in full-shot and few-shot settings, respectively.</p>
  </details>
</details>
<details>
  <summary>107. <b>标题：NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning</b></summary>
  <p><b>编号</b>：[444]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00459">https://arxiv.org/abs/2404.00459</a></p>
  <p><b>作者</b>：Eli Schwartz,  Leshem Choshen,  Joseph Shtok,  Sivan Doveh,  Leonid Karlinsky,  Assaf Arbelle</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：handling numerical data, performing arithmetic operations, Language models struggle, struggle with handling, handling numerical</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation. When a digit is read or generated by a causal language model it does not know its place value (e.g. thousands vs. hundreds) until the entire number is processed. To address this issue, we propose a simple adjustment to how numbers are represented by including the count of digits before each number. For instance, instead of "42", we suggest using "{2:42}" as the new format. This approach, which we term NumeroLogic, offers an added advantage in number generation by serving as a Chain of Thought (CoT). By requiring the model to consider the number of digits first, it enhances the reasoning process before generating the actual number. We use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic formatting. We further demonstrate NumeroLogic applicability to general natural language modeling, improving language understanding performance in the MMLU benchmark.</p>
  </details>
</details>
<details>
  <summary>108. <b>标题：Beyond One-Size-Fits-All: Multi-Domain, Multi-Task Framework for  Embedding Model Selection</b></summary>
  <p><b>编号</b>：[445]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00458">https://arxiv.org/abs/2404.00458</a></p>
  <p><b>作者</b>：Vivek Khetan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：natural language processing, position paper proposes, effective embedding models, open-source encoder models, language processing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This position paper proposes a systematic approach towards developing a framework to help select the most effective embedding models for natural language processing (NLP) tasks, addressing the challenge posed by the proliferation of both proprietary and open-source encoder models.</p>
  </details>
</details>
<details>
  <summary>109. <b>标题：MetaIE: Distilling a Meta Model from LLM for All Kinds of Information  Extraction Tasks</b></summary>
  <p><b>编号</b>：[446]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00457">https://arxiv.org/abs/2404.00457</a></p>
  <p><b>作者</b>：Letian Peng,  Zilong Wang,  Feng Yao,  Zihan Wang,  Jingbo Shang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：defeat small LMs, small LMs tuned, natural language processing, important information, fundamental area</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Information extraction (IE) is a fundamental area in natural language processing where prompting large language models (LLMs), even with in-context examples, cannot defeat small LMs tuned on very small IE datasets. We observe that IE tasks, such as named entity recognition and relation extraction, all focus on extracting important information, which can be formalized as a label-to-span matching. In this paper, we propose a novel framework MetaIE to build a small LM as meta-model by learning to extract "important information", i.e., the meta-understanding of IE, so that this meta-model can be adapted to all kind of IE tasks effectively and efficiently. Specifically, MetaIE obtains the small LM via a symbolic distillation from an LLM following the label-to-span scheme. We construct the distillation dataset via sampling sentences from language model pre-training datasets (e.g., OpenWebText in our implementation) and prompting an LLM to identify the typed spans of "important information". We evaluate the meta-model under the few-shot adaptation setting. Extensive results on 13 datasets from 6 IE tasks confirm that MetaIE can offer a better starting point for few-shot tuning on IE datasets and outperform other meta-models from (1) vanilla language model pre-training, (2) multi-IE-task pre-training with human annotations, and (3) single-IE-task symbolic distillation from LLM. Moreover, we provide comprehensive analyses of MetaIE, such as the size of the distillation dataset, the meta-model architecture, and the size of the meta-model.</p>
  </details>
</details>
<details>
  <summary>110. <b>标题：Planning and Editing What You Retrieve for Enhanced Tool Learning</b></summary>
  <p><b>编号</b>：[449]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00450">https://arxiv.org/abs/2404.00450</a></p>
  <p><b>作者</b>：Tenghao Huang,  Dongwon Jung,  Muhao Chen</p>
  <p><b>备注</b>：This paper is accepted at NAACL-Findings 2024</p>
  <p><b>关键词</b>：Large Language Models, Large Language, Language Models, integrating external tools, code generators</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advancements in integrating external tools with Large Language Models (LLMs) have opened new frontiers, with applications in mathematical reasoning, code generators, and smart assistants. However, existing methods, relying on simple one-time retrieval strategies, fall short on effectively and accurately shortlisting relevant tools. This paper introduces a novel \modelname (\modelmeaning) approach, encompassing ``Plan-and-Retrieve (P\&R)'' and ``Edit-and-Ground (E\&G)'' paradigms. The P\&R paradigm consists of a neural retrieval module for shortlisting relevant tools and an LLM-based query planner that decomposes complex queries into actionable tasks, enhancing the effectiveness of tool utilization. The E\&G paradigm utilizes LLMs to enrich tool descriptions based on user scenarios, bridging the gap between user queries and tool functionalities. Experiment results demonstrate that these paradigms significantly improve the recall and NDCG in tool retrieval tasks, significantly surpassing current state-of-the-art models.</p>
  </details>
</details>
<details>
  <summary>111. <b>标题：DOCMASTER: A Unified Platform for Annotation, Training, & Inference in  Document Question-Answering</b></summary>
  <p><b>编号</b>：[455]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00439">https://arxiv.org/abs/2404.00439</a></p>
  <p><b>作者</b>：Alex Nguyen,  Zilong Wang,  Jingbo Shang,  Dheeraj Mekala</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：natural language processing, PDF documents, specific hurdles, business applications, natural language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The application of natural language processing models to PDF documents is pivotal for various business applications yet the challenge of training models for this purpose persists in businesses due to specific hurdles. These include the complexity of working with PDF formats that necessitate parsing text and layout information for curating training data and the lack of privacy-preserving annotation tools. This paper introduces DOCMASTER, a unified platform designed for annotating PDF documents, model training, and inference, tailored to document question-answering. The annotation interface enables users to input questions and highlight text spans within the PDF file as answers, saving layout information and text spans accordingly. Furthermore, DOCMASTER supports both state-of-the-art layout-aware and text models for comprehensive training purposes. Importantly, as annotations, training, and inference occur on-device, it also safeguards privacy. The platform has been instrumental in driving several research prototypes concerning document analysis such as the AI assistant utilized by University of California San Diego's (UCSD) International Services and Engagement Office (ISEO) for processing a substantial volume of PDF documents.</p>
  </details>
</details>
<details>
  <summary>112. <b>标题：Automatic explanation of the classification of Spanish legal judgments  in jurisdiction-dependent law categories with tree estimators</b></summary>
  <p><b>编号</b>：[457]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00437">https://arxiv.org/abs/2404.00437</a></p>
  <p><b>作者</b>：Jaime González-González,  Francisco de Arriba-Pérez,  Silvia García-Méndez,  Andrea Busto-Castiñeira,  Francisco J. González-Castaño</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：address knowledge extraction, detect their aspects, literature to address, extraction from judgments, judgments and detect</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automatic legal text classification systems have been proposed in the literature to address knowledge extraction from judgments and detect their aspects. However, most of these systems are black boxes even when their models are interpretable. This may raise concerns about their trustworthiness. Accordingly, this work contributes with a system combining Natural Language Processing (NLP) with Machine Learning (ML) to classify legal texts in an explainable manner. We analyze the features involved in the decision and the threshold bifurcation values of the decision paths of tree structures and present this information to the users in natural language. This is the first work on automatic analysis of legal texts combining NLP and ML along with Explainable Artificial Intelligence techniques to automatically make the models' decisions understandable to end users. Furthermore, legal experts have validated our solution, and this knowledge has also been incorporated into the explanation process as "expert-in-the-loop" dictionaries. Experimental results on an annotated data set in law categories by jurisdiction demonstrate that our system yields competitive classification performance, with accuracy values well above 90%, and that its automatic explanations are easily understandable even to non-expert users.</p>
  </details>
</details>
<details>
  <summary>113. <b>标题：Do Vision-Language Models Understand Compound Nouns?</b></summary>
  <p><b>编号</b>：[465]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00419">https://arxiv.org/abs/2404.00419</a></p>
  <p><b>作者</b>：Sonal Kumar,  Sreyan Ghosh,  S Sakshi,  Utkarsh Tyagi,  Dinesh Manocha</p>
  <p><b>备注</b>：Accepted to NAACL 2024 Main Conference</p>
  <p><b>关键词</b>：Open-vocabulary vision-language models, Open-vocabulary vision-language, trained using contrastive, contrastive loss, promising new paradigm</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Open-vocabulary vision-language models (VLMs) like CLIP, trained using contrastive loss, have emerged as a promising new paradigm for text-to-image retrieval. However, do VLMs understand compound nouns (CNs) (e.g., lab coat) as well as they understand nouns (e.g., lab)? We curate Compun, a novel benchmark with 400 unique and commonly used CNs, to evaluate the effectiveness of VLMs in interpreting CNs. The Compun benchmark challenges a VLM for text-to-image retrieval where, given a text prompt with a CN, the task is to select the correct image that shows the CN among a pair of distractor images that show the constituent nouns that make up the CN. Next, we perform an in-depth analysis to highlight CLIPs' limited understanding of certain types of CNs. Finally, we present an alternative framework that moves beyond hand-written templates for text prompts widely used by CLIP-like models. We employ a Large Language Model to generate multiple diverse captions that include the CN as an object in the scene described by the caption. Our proposed method improves CN understanding of CLIP by 8.25% on Compun. Code and benchmark are available at: this https URL</p>
  </details>
</details>
<details>
  <summary>114. <b>标题：CoDa: Constrained Generation based Data Augmentation for Low-Resource  NLP</b></summary>
  <p><b>编号</b>：[469]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00415">https://arxiv.org/abs/2404.00415</a></p>
  <p><b>作者</b>：Chandra Kiran Reddy Evuru,  Sreyan Ghosh,  Sonal Kumar,  Ramaneswaran S,  Utkarsh Tyagi,  Dinesh Manocha</p>
  <p><b>备注</b>：Accepted to NAACL 2024 Findings</p>
  <p><b>关键词</b>：Large Language Models, instruction-following Large Language, training-free data augmentation, Constrained Generation based, Large Language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present CoDa (Constrained Generation based Data Augmentation), a controllable, effective, and training-free data augmentation technique for low-resource (data-scarce) NLP. Our approach is based on prompting off-the-shelf instruction-following Large Language Models (LLMs) for generating text that satisfies a set of constraints. Precisely, we extract a set of simple constraints from every instance in the low-resource dataset and verbalize them to prompt an LLM to generate novel and diverse training instances. Our findings reveal that synthetic data that follows simple constraints in the downstream dataset act as highly effective augmentations, and CoDa can achieve this without intricate decoding-time constrained generation techniques or fine-tuning with complex algorithms that eventually make the model biased toward the small number of training instances. Additionally, CoDa is the first framework that provides users explicit control over the augmentation generation process, thereby also allowing easy adaptation to several domains. We demonstrate the effectiveness of CoDa across 11 datasets spanning 3 tasks and 3 low-resource settings. CoDa outperforms all our baselines, qualitatively and quantitatively, with improvements of 0.12%-7.19%. Code is available here: this https URL</p>
  </details>
</details>
<details>
  <summary>115. <b>标题：TACO -- Twitter Arguments from COnversations</b></summary>
  <p><b>编号</b>：[474]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00406">https://arxiv.org/abs/2404.00406</a></p>
  <p><b>作者</b>：Marc Feger,  Stefan Dietze</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：user-generated content, global hub, hub for engaging, research corpus, recognized the significance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Twitter has emerged as a global hub for engaging in online conversations and as a research corpus for various disciplines that have recognized the significance of its user-generated content. Argument mining is an important analytical task for processing and understanding online discourse. Specifically, it aims to identify the structural elements of arguments, denoted as information and inference. These elements, however, are not static and may require context within the conversation they are in, yet there is a lack of data and annotation frameworks addressing this dynamic aspect on Twitter. We contribute TACO, the first dataset of Twitter Arguments utilizing 1,814 tweets covering 200 entire conversations spanning six heterogeneous topics annotated with an agreement of 0.718 Krippendorff's alpha among six experts. Second, we provide our annotation framework, incorporating definitions from the Cambridge Dictionary, to define and identify argument components on Twitter. Our transformer-based classifier achieves an 85.06\% macro F1 baseline score in detecting arguments. Moreover, our data reveals that Twitter users tend to engage in discussions involving informed inferences and information. TACO serves multiple purposes, such as training tweet classifiers to manage tweets based on inference and information elements, while also providing valuable insights into the conversational reply patterns of tweets.</p>
  </details>
</details>
<details>
  <summary>116. <b>标题：UniMEEC: Towards Unified Multimodal Emotion Recognition and Emotion  Cause</b></summary>
  <p><b>编号</b>：[477]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00403">https://arxiv.org/abs/2404.00403</a></p>
  <p><b>作者</b>：Guimin Hu,  Zhihong Zhu,  Daniel Hershcovich,  Hasti Seifi,  Jiayuan Xie</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：garnered significant attention, recently garnered significant, emotion-cause pair extraction, Multimodal emotion recognition, pair extraction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multimodal emotion recognition in conversation (MERC) and multimodal emotion-cause pair extraction (MECPE) has recently garnered significant attention. Emotions are the expression of affect or feelings; responses to specific events, thoughts, or situations are known as emotion causes. Both are like two sides of a coin, collectively describing human behaviors and intents. However, most existing works treat MERC and MECPE as separate tasks, which may result in potential challenges in integrating emotion and cause in real-world applications. In this paper, we propose a Unified Multimodal Emotion recognition and Emotion-Cause analysis framework (UniMEEC) to explore the causality and complementarity between emotion and emotion cause. Concretely, UniMEEC reformulates the MERC and MECPE tasks as two mask prediction problems, enhancing the interaction between emotion and cause. Meanwhile, UniMEEC shares the prompt learning among modalities for probing modality-specific knowledge from the Pre-trained model. Furthermore, we propose a task-specific hierarchical context aggregation to control the information flow to the task. Experiment results on four public benchmark datasets verify the model performance on MERC and MECPE tasks and achieve consistent improvements compared with state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>117. <b>标题：How Robust are the Tabular QA Models for Scientific Tables? A Study  using Customized Dataset</b></summary>
  <p><b>编号</b>：[478]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00401">https://arxiv.org/abs/2404.00401</a></p>
  <p><b>作者</b>：Akash Ghosh,  B Venkata Sahith,  Niloy Ganguly,  Pawan Goyal,  Mayank Singh</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：textual data deals, scientific, tabular, complex numerical reasoning, data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Question-answering (QA) on hybrid scientific tabular and textual data deals with scientific information, and relies on complex numerical reasoning. In recent years, while tabular QA has seen rapid progress, understanding their robustness on scientific information is lacking due to absence of any benchmark dataset. To investigate the robustness of the existing state-of-the-art QA models on scientific hybrid tabular data, we propose a new dataset, "SciTabQA", consisting of 822 question-answer pairs from scientific tables and their descriptions. With the help of this dataset, we assess the state-of-the-art Tabular QA models based on their ability (i) to use heterogeneous information requiring both structured data (table) and unstructured data (text) and (ii) to perform complex scientific reasoning tasks. In essence, we check the capability of the models to interpret scientific tables and text. Our experiments show that "SciTabQA" is an innovative dataset to study question-answering over scientific heterogeneous data. We benchmark three state-of-the-art Tabular QA models, and find that the best F1 score is only 0.462.</p>
  </details>
</details>
<details>
  <summary>118. <b>标题：Aurora-M: The First Open Source Multilingual Language Model Red-teamed  according to the U.S. Executive Order</b></summary>
  <p><b>编号</b>：[479]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00399">https://arxiv.org/abs/2404.00399</a></p>
  <p><b>作者</b>：Taishi Nakamura,  Mayank Mishra,  Simone Tedeschi,  Yekun Chai,  Jason T Stillerman,  Felix Friedrich,  Prateek Yadav,  Tanmay Laud,  Vu Minh Chien,  Terry Yue Zhuo,  Diganta Misra,  Ben Bogin,  Xuan-Son Vu,  Marzena Karpinska,  Arnav Varma Dantuluri,  Wojciech Kusa,  Tommaso Furlanello,  Rio Yokota,  Niklas Muennighoff,  Suhas Pai,  Tosin Adewumi,  Veronika Laippala,  Xiaozhe Yao,  Adalberto Junior,  Alpay Ariyak,  Aleksandr Drozd,  Jordan Clive,  Kshitij Gupta,  Liangyu Chen,  Qi Sun,  Ken Tsui,  Noah Persaud,  Nour Fahmy,  Tianlong Chen,  Mohit Bansal,  Nicolo Monti,  Tai Dang,  Ziyang Luo,  Tien-Tung Bui,  Roberto Navigli,  Virendra Mehta,  Matthew Blumberg,  Victor May,  Huu Nguyen,  Sampo Pyysalo</p>
  <p><b>备注</b>：Preprint</p>
  <p><b>关键词</b>：high computational cost, training limits accessibility, limits accessibility, high computational, computational cost</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pretrained language models underpin several AI applications, but their high computational cost for training limits accessibility. Initiatives such as BLOOM and StarCoder aim to democratize access to pretrained models for collaborative community development. However, such existing models face challenges: limited multilingual capabilities, continual pretraining causing catastrophic forgetting, whereas pretraining from scratch is computationally expensive, and compliance with AI safety and development laws. This paper presents Aurora-M, a 15B parameter multilingual open-source model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually pretrained from StarCoderPlus on 435 billion additional tokens, Aurora-M surpasses 2 trillion tokens in total training token count. It is the first open-source multilingual model fine-tuned on human-reviewed safety instructions, thus aligning its development not only with conventional red-teaming considerations, but also with the specific concerns articulated in the Biden-Harris Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. Aurora-M is rigorously evaluated across various tasks and languages, demonstrating robustness against catastrophic forgetting and outperforming alternatives in multilingual settings, particularly in safety evaluations. To promote responsible open-source LLM development, Aurora-M and its variants are released at this https URL .</p>
  </details>
</details>
<details>
  <summary>119. <b>标题：An Analysis of BPE Vocabulary Trimming in Neural Machine Translation</b></summary>
  <p><b>编号</b>：[480]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00397">https://arxiv.org/abs/2404.00397</a></p>
  <p><b>作者</b>：Marco Cognetta,  Tatsuya Hiraoka,  Naoaki Okazaki,  Rico Sennrich,  Yuval Pinter</p>
  <p><b>备注</b>：15 pages</p>
  <p><b>关键词</b>：Byte-Pair Encoding subword, explore threshold vocabulary, Encoding subword tokenization, Byte-Pair Encoding, threshold vocabulary trimming</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We explore threshold vocabulary trimming in Byte-Pair Encoding subword tokenization, a postprocessing step that replaces rare subwords with their component subwords. The technique is available in popular tokenization libraries but has not been subjected to rigorous scientific scrutiny. While the removal of rare subwords is suggested as best practice in machine translation implementations, both as a means to reduce model size and for improving model performance through robustness, our experiments indicate that, across a large space of hyperparameter settings, vocabulary trimming fails to improve performance, and is even prone to incurring heavy degradation.</p>
  </details>
</details>
<details>
  <summary>120. <b>标题：Jetsons at FinNLP 2024: Towards Understanding the ESG Impact of a News  Article using Transformer-based Models</b></summary>
  <p><b>编号</b>：[486]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00386">https://arxiv.org/abs/2404.00386</a></p>
  <p><b>作者</b>：Parag Pravin Dakle,  Alolika Gon,  Sihan Zha,  Liang Wang,  SaiKrishna Rallabandi,  Preethi Raghavan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Impact Duration Inference, Multi-Lingual ESG Impact, ESG Impact Duration, Jetsons team, Duration Inference</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we describe the different approaches explored by the Jetsons team for the Multi-Lingual ESG Impact Duration Inference (ML-ESG-3) shared task. The shared task focuses on predicting the duration and type of the ESG impact of a news article. The shared task dataset consists of 2,059 news titles and articles in English, French, Korean, and Japanese languages. For the impact duration classification task, we fine-tuned XLM-RoBERTa with a custom fine-tuning strategy and using self-training and DeBERTa-v3 using only English translations. These models individually ranked first on the leaderboard for Korean and Japanese and in an ensemble for the English language, respectively. For the impact type classification task, our XLM-RoBERTa model fine-tuned using a custom fine-tuning strategy ranked first for the English language.</p>
  </details>
</details>
<details>
  <summary>121. <b>标题：Small Language Models Learn Enhanced Reasoning Skills from Medical  Textbooks</b></summary>
  <p><b>编号</b>：[491]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00376">https://arxiv.org/abs/2404.00376</a></p>
  <p><b>作者</b>：Hyunjae Kim,  Hyeon Hwang,  Jiwoo Lee,  Sihyeon Park,  Dain Kim,  Taewhoo Lee,  Chanwoong Yoon,  Jiwoong Sohn,  Donghee Choi,  Jaewoo Kang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：closed-source nature poses, nature poses significant, poses significant privacy, shown promising results, commercial large language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While recent advancements in commercial large language models (LM) have shown promising results in medical tasks, their closed-source nature poses significant privacy and security concerns, hindering their widespread use in the medical field. Despite efforts to create open-source models, their limited parameters often result in insufficient multi-step reasoning capabilities required for solving complex medical problems. To address this, we introduce Meerkat-7B, a novel medical AI system with 7 billion parameters. Meerkat-7B was trained using our new synthetic dataset consisting of high-quality chain-of-thought reasoning paths sourced from 18 medical textbooks, along with diverse instruction-following datasets. Our system achieved remarkable accuracy across seven medical benchmarks, surpassing GPT-3.5 by 13.1%, as well as outperforming the previous best 7B models such as MediTron-7B and BioMistral-7B by 13.4% and 9.8%, respectively. Notably, it surpassed the passing threshold of the United States Medical Licensing Examination (USMLE) for the first time for a 7B-parameter model. Additionally, our system offered more detailed free-form responses to clinical queries compared to existing 7B and 13B models, approaching the performance level of GPT-3.5. This significantly narrows the performance gap with large LMs, showcasing its effectiveness in addressing complex medical challenges.</p>
  </details>
</details>
<details>
  <summary>122. <b>标题：Controllable and Diverse Data Augmentation with Large Language Model for  Low-Resource Open-Domain Dialogue Generation</b></summary>
  <p><b>编号</b>：[501]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00361">https://arxiv.org/abs/2404.00361</a></p>
  <p><b>作者</b>：Zhenhua Liu,  Tong Zhu,  Jianxiang Xiang,  Wenliang Chen</p>
  <p><b>备注</b>：13 pages, 5 figures</p>
  <p><b>关键词</b>：mitigate model training, model training instability, crucial to mitigate, training instability, instability and over-fitting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Data augmentation (DA) is crucial to mitigate model training instability and over-fitting problems in low-resource open-domain dialogue generation. However, traditional DA methods often neglect semantic data diversity, restricting the overall quality. Recently, large language models (LLM) have been used for DA to generate diversified dialogues. However, they have limited controllability and tend to generate dialogues with a distribution shift compared to the seed dialogues. To maximize the augmentation diversity and address the controllability problem, we propose \textbf{S}ummary-based \textbf{D}ialogue \textbf{A}ugmentation with LLM (SDA). Our approach enhances the controllability of LLM by using dialogue summaries as a planning tool. Based on summaries, SDA can generate high-quality and diverse dialogue data even with a small seed dataset. To evaluate the efficacy of data augmentation methods for open-domain dialogue, we designed a clustering-based metric to characterize the semantic diversity of the augmented dialogue data. The experimental results show that SDA can augment high-quality and semantically diverse dialogues given a small seed dataset and an LLM, and the augmented data can boost the performance of open-domain dialogue models.</p>
  </details>
</details>
<details>
  <summary>123. <b>标题：Can LLMs Master Math? Investigating Large Language Models on Math Stack  Exchange</b></summary>
  <p><b>编号</b>：[513]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00344">https://arxiv.org/abs/2404.00344</a></p>
  <p><b>作者</b>：Ankit Satpute,  Noah Giessing,  Andre Greiner-Petter,  Moritz Schubotz,  Olaf Teschke,  Akiko Aizawa,  Bela Gipp</p>
  <p><b>备注</b>：Accepted for publication at the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) July 14--18, 2024, Washington D.C.,USA</p>
  <p><b>关键词</b>：Large Language Models, natural language tasks, Language Models, Large Language, language tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) have demonstrated exceptional capabilities in various natural language tasks, often achieving performances that surpass those of humans. Despite these advancements, the domain of mathematics presents a distinctive challenge, primarily due to its specialized structure and the precision it demands. In this study, we adopted a two-step approach for investigating the proficiency of LLMs in answering mathematical questions. First, we employ the most effective LLMs, as identified by their performance on math question-answer benchmarks, to generate answers to 78 questions from the Math Stack Exchange (MSE). Second, a case analysis is conducted on the LLM that showed the highest performance, focusing on the quality and accuracy of its answers through manual evaluation. We found that GPT-4 performs best (nDCG of 0.48 and P@10 of 0.37) amongst existing LLMs fine-tuned for answering mathematics questions and outperforms the current best approach on ArqMATH3 Task1, considering P@10. Our Case analysis indicates that while the GPT-4 can generate relevant responses in certain instances, it does not consistently answer all questions accurately. This paper explores the current limitations of LLMs in navigating complex mathematical problem-solving. Through case analysis, we shed light on the gaps in LLM capabilities within mathematics, thereby setting the stage for future research and advancements in AI-driven mathematical reasoning. We make our code and findings publicly available for research: \url{this https URL}</p>
  </details>
</details>
<details>
  <summary>124. <b>标题：A Comprehensive Study on NLP Data Augmentation for Hate Speech  Detection: Legacy Methods, BERT, and LLMs</b></summary>
  <p><b>编号</b>：[532]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00303">https://arxiv.org/abs/2404.00303</a></p>
  <p><b>作者</b>：Md Saroar Jahan,  Mourad Oussalah,  Djamila Romaissa Beddia,  Jhuma kabir Mim,  Nabil Arhab</p>
  <p><b>备注</b>：31 page, Table 18, Figure 14</p>
  <p><b>关键词</b>：social media vocabulary, address challenges posed, large-scale neural networks, neural networks requiring, networks requiring extensive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The surge of interest in data augmentation within the realm of NLP has been driven by the need to address challenges posed by hate speech domains, the dynamic nature of social media vocabulary, and the demands for large-scale neural networks requiring extensive training data. However, the prevalent use of lexical substitution in data augmentation has raised concerns, as it may inadvertently alter the intended meaning, thereby impacting the efficacy of supervised machine learning models. In pursuit of suitable data augmentation methods, this study explores both established legacy approaches and contemporary practices such as Large Language Models (LLM), including GPT in Hate Speech detection. Additionally, we propose an optimized utilization of BERT-based encoder models with contextual cosine similarity filtration, exposing significant limitations in prior synonym substitution methods. Our comparative analysis encompasses five popular augmentation techniques: WordNet and Fast-Text synonym replacement, Back-translation, BERT-mask contextual augmentation, and LLM. Our analysis across five benchmarked datasets revealed that while traditional methods like back-translation show low label alteration rates (0.3-1.5%), and BERT-based contextual synonym replacement offers sentence diversity but at the cost of higher label alteration rates (over 6%). Our proposed BERT-based contextual cosine similarity filtration markedly reduced label alteration to just 0.05%, demonstrating its efficacy in 0.7% higher F1 performance. However, augmenting data with GPT-3 not only avoided overfitting with up to sevenfold data increase but also improved embedding space coverage by 15% and classification F1 score by 1.4% over traditional methods, and by 0.8% over our method.</p>
  </details>
</details>
<details>
  <summary>125. <b>标题：TRABSA: Interpretable Sentiment Analysis of Tweets using Attention-based  BiLSTM and Twitter-RoBERTa</b></summary>
  <p><b>编号</b>：[536]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00297">https://arxiv.org/abs/2404.00297</a></p>
  <p><b>作者</b>：Md Abrar Jahin,  Md Sakib Hossain Shovon,  M. F. Mridha</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：understanding public opinion, consumer behavior, crucial for understanding, understanding public, public opinion</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sentiment analysis is crucial for understanding public opinion and consumer behavior. Existing models face challenges with linguistic diversity, generalizability, and explainability. We propose TRABSA, a hybrid framework integrating transformer-based architectures, attention mechanisms, and BiLSTM networks to address this. Leveraging RoBERTa-trained on 124M tweets, we bridge gaps in sentiment analysis benchmarks, ensuring state-of-the-art accuracy. Augmenting datasets with tweets from 32 countries and US states, we compare six word-embedding techniques and three lexicon-based labeling techniques, selecting the best for optimal sentiment analysis. TRABSA outperforms traditional ML and deep learning models with 94% accuracy and significant precision, recall, and F1-score gains. Evaluation across diverse datasets demonstrates consistent superiority and generalizability. SHAP and LIME analyses enhance interpretability, improving confidence in predictions. Our study facilitates pandemic resource management, aiding resource planning, policy formation, and vaccination tactics.</p>
  </details>
</details>
<details>
  <summary>126. <b>标题：A Likelihood Ratio Test of Genetic Relationship among Languages</b></summary>
  <p><b>编号</b>：[541]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00284">https://arxiv.org/abs/2404.00284</a></p>
  <p><b>作者</b>：V.S.D.S.Mahesh Akavarapu,  Arnab Bhattacharya</p>
  <p><b>备注</b>：Accepted at NAACL-2024 (Main Conference)</p>
  <p><b>关键词</b>：common ancestral language, common ancestral, Lexical resemblances, languages, language families</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Lexical resemblances among a group of languages indicate that the languages could be genetically related, i.e., they could have descended from a common ancestral language. However, such resemblances can arise by chance and, hence, need not always imply an underlying genetic relationship. Many tests of significance based on permutation of wordlists and word similarity measures appeared in the past to determine the statistical significance of such relationships. We demonstrate that although existing tests may work well for bilateral comparisons, i.e., on pairs of languages, they are either infeasible by design or are prone to yield false positives when applied to groups of languages or language families. To this end, inspired by molecular phylogenetics, we propose a likelihood ratio test to determine if given languages are related based on the proportion of invariant character sites in the aligned wordlists applied during tree inference. Further, we evaluate some language families and show that the proposed test solves the problem of false positives. Finally, we demonstrate that the test supports the existence of macro language families such as Nostratic and Macro-Mayan.</p>
  </details>
</details>
<details>
  <summary>127. <b>标题：Survey on Large Language Model-Enhanced Reinforcement Learning: Concept,  Taxonomy, and Methods</b></summary>
  <p><b>编号</b>：[542]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00282">https://arxiv.org/abs/2404.00282</a></p>
  <p><b>作者</b>：Yuji Cao,  Huan Zhao,  Yuheng Cheng,  Ting Shu,  Guolong Liu,  Gaoqi Liang,  Junhua Zhao,  Yun Li</p>
  <p><b>备注</b>：16 pages (including bibliography), 6 figures</p>
  <p><b>关键词</b>：augment reinforcement learning, large language models, high-level general capabilities, extensive pre-trained knowledge, reinforcement learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With extensive pre-trained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects such as multi-task learning, sample efficiency, and task planning. In this survey, we provide a comprehensive review of the existing literature in $\textit{LLM-enhanced RL}$ and summarize its characteristics compared to conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs' functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. Additionally, for each role, we summarize the methodologies, analyze the specific RL challenges that are mitigated, and provide insights into future directions. Lastly, potential applications, prospective opportunities and challenges of the $\textit{LLM-enhanced RL}$ are discussed.</p>
  </details>
</details>
<details>
  <summary>128. <b>标题：Secret Keepers: The Impact of LLMs on Linguistic Markers of Personal  Traits</b></summary>
  <p><b>编号</b>：[550]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00267">https://arxiv.org/abs/2404.00267</a></p>
  <p><b>作者</b>：Zhivar Sourati,  Meltem Ozcan,  Colin McDaniel,  Alireza Ziabari,  Nuan Wen,  Ala Tak,  Fred Morstatter,  Morteza Dehghani</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, individuals' language usage, patterns reveal information, linguistic patterns reveal, linguistic patterns</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Prior research has established associations between individuals' language usage and their personal traits; our linguistic patterns reveal information about our personalities, emotional states, and beliefs. However, with the increasing adoption of Large Language Models (LLMs) as writing assistants in everyday writing, a critical question emerges: are authors' linguistic patterns still predictive of their personal traits when LLMs are involved in the writing process? We investigate the impact of LLMs on the linguistic markers of demographic and psychological traits, specifically examining three LLMs - GPT3.5, Llama 2, and Gemini - across six different traits: gender, age, political affiliation, personality, empathy, and morality. Our findings indicate that although the use of LLMs slightly reduces the predictive power of linguistic patterns over authors' personal traits, the significant changes are infrequent, and the use of LLMs does not fully diminish the predictive power of authors' linguistic patterns over their personal traits. We also note that some theoretically established lexical-based linguistic markers lose their reliability as predictors when LLMs are used in the writing process. Our findings have important implications for the study of linguistic markers of personal traits in the age of LLMs.</p>
  </details>
</details>
<details>
  <summary>129. <b>标题：DiLM: Distilling Dataset into Language Model for Text-level Dataset  Distillation</b></summary>
  <p><b>编号</b>：[552]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00264">https://arxiv.org/abs/2404.00264</a></p>
  <p><b>作者</b>：Aru Maekawa,  Satoshi Kosugi,  Kotaro Funakoshi,  Manabu Okumura</p>
  <p><b>备注</b>：Accepted by Findings of NAACL 2024</p>
  <p><b>关键词</b>：neural networks trained, networks trained, Dataset distillation aims, aims to compress, creating a small</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Dataset distillation aims to compress a training dataset by creating a small number of informative synthetic samples such that neural networks trained on them perform as well as those trained on the original training dataset. Current text dataset distillation methods create each synthetic sample as a sequence of word embeddings instead of a text to apply gradient-based optimization; however, such embedding-level distilled datasets cannot be used for training other models whose word embedding weights are different from the model used for distillation. To address this issue, we propose a novel text dataset distillation approach, called Distilling dataset into Language Model (DiLM), which trains a language model to generate informative synthetic training samples as text data, instead of directly optimizing synthetic samples. We evaluated DiLM on various text classification datasets and showed that distilled synthetic datasets from DiLM outperform those from current coreset selection methods. DiLM achieved remarkable generalization performance in training different types of models and in-context learning of large language models. Our code will be available at this https URL.</p>
  </details>
</details>
<details>
  <summary>130. <b>标题：Your Co-Workers Matter: Evaluating Collaborative Capabilities of  Language Models in Blocks World</b></summary>
  <p><b>编号</b>：[562]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00246">https://arxiv.org/abs/2404.00246</a></p>
  <p><b>作者</b>：Guande Wu,  Chen Zhao,  Claudio Silva,  He He</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：automating digital tasks, great potential, potential for automating, automating digital, digital tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Language agents that interact with the world on their own have great potential for automating digital tasks. While large language model (LLM) agents have made progress in understanding and executing tasks such as textual games and webpage control, many real-world tasks also require collaboration with humans or other LLMs in equal roles, which involves intent understanding, task coordination, and communication. To test LLM's ability to collaborate, we design a blocks-world environment, where two agents, each having unique goals and skills, build a target structure together. To complete the goals, they can act in the world and communicate in natural language. Under this environment, we design increasingly challenging settings to evaluate different collaboration perspectives, from independent to more complex, dependent tasks. We further adopt chain-of-thought prompts that include intermediate reasoning steps to model the partner's state and identify and correct execution errors. Both human-machine and machine-machine experiments show that LLM agents have strong grounding capacities, and our approach significantly improves the evaluation metric.</p>
  </details>
</details>
<details>
  <summary>131. <b>标题：DeFT: Flash Tree-attention with IO-Awareness for Efficient  Tree-search-based LLM Inference</b></summary>
  <p><b>编号</b>：[565]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00242">https://arxiv.org/abs/2404.00242</a></p>
  <p><b>作者</b>：Jinwei Yao,  Kaiqi Chen,  Kexun Zhang,  Jiaxuan You,  Binhang Yuan,  Zeke Wang,  Tao Lin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, transformer-based Large Language, Language Models, Large Language, transformer-based Large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Decoding using tree search can greatly enhance the inference quality for transformer-based Large Language Models (LLMs). Depending on the guidance signal, it searches for the best path from root to leaf in the tree by forming LLM outputs to improve controllability, reasoning ability, alignment, et cetera. However, current tree decoding strategies and their inference systems do not suit each other well due to redundancy in computation, memory footprints, and memory access, resulting in inefficient inference. To address this issue, we propose DeFT, an IO-aware tree attention algorithm that maintains memory-efficient attention calculation with low memory footprints in two stages: (1) QKV Preparation: we propose a KV-Guided Tree Split strategy to group QKV wisely for high utilization of GPUs and reduction of memory reads/writes for the KV cache between GPU global memory and on-chip shared memory as much as possible; (2) Attention Calculation: we calculate partial attention of each QKV groups in a fused kernel then apply a Tree-topology-aware Global Reduction strategy to get final attention. Thanks to a reduction in KV cache IO by 3.6-4.5$\times$, along with an additional reduction in IO for $\mathbf{Q} \mathbf{K}^\top$ and Softmax equivalent to 25% of the total KV cache IO, DeFT can achieve a speedup of 1.7-2.4$\times$ in end-to-end latency across two practical reasoning tasks over the SOTA attention algorithms.</p>
  </details>
</details>
<details>
  <summary>132. <b>标题：Enhancing Content-based Recommendation via Large Language Model</b></summary>
  <p><b>编号</b>：[567]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00236">https://arxiv.org/abs/2404.00236</a></p>
  <p><b>作者</b>：Wentao Xu,  Qianqian Xie,  Shuo Yang,  Jiangxia Cao,  Shuchao Pang</p>
  <p><b>备注</b>：Work in progress</p>
  <p><b>关键词</b>：including implicit click, reviews interactions, implicit click, express different behaviors, including implicit</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In real-world applications, users express different behaviors when they interact with different items, including implicit click/like interactions, and explicit comments/reviews interactions. Nevertheless, almost all recommender works are focused on how to describe user preferences by the implicit click/like interactions, to find the synergy of people. For the content-based explicit comments/reviews interactions, some works attempt to utilize them to mine the semantic knowledge to enhance recommender models. However, they still neglect the following two points: (1) The content semantic is a universal world knowledge; how do we extract the multi-aspect semantic information to empower different domains? (2) The user/item ID feature is a fundamental element for recommender models; how do we align the ID and content semantic feature space? In this paper, we propose a `plugin' semantic knowledge transferring method \textbf{LoID}, which includes two major components: (1) LoRA-based large language model pretraining to extract multi-aspect semantic information; (2) ID-based contrastive objective to align their feature spaces. We conduct extensive experiments with SOTA baselines on real-world datasets, the detailed results demonstrating significant improvements of our method LoID.</p>
  </details>
</details>
<details>
  <summary>133. <b>标题：A Survey of using Large Language Models for Generating Infrastructure as  Code</b></summary>
  <p><b>编号</b>：[574]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00227">https://arxiv.org/abs/2404.00227</a></p>
  <p><b>作者</b>：Kalahasti Ganesh Srivatsa,  Sabyasachi Mukhopadhyay,  Ganesh Katrapati,  Manish Shrivastava</p>
  <p><b>备注</b>：Accepted in ICON2023</p>
  <p><b>关键词</b>：gained significant prominence, revolutionary approach, IaC, Large Language Models, Code</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Infrastructure as Code (IaC) is a revolutionary approach which has gained significant prominence in the Industry. IaC manages and provisions IT infrastructure using machine-readable code by enabling automation, consistency across the environments, reproducibility, version control, error reduction and enhancement in scalability. However, IaC orchestration is often a painstaking effort which requires specialised skills as well as a lot of manual effort. Automation of IaC is a necessity in the present conditions of the Industry and in this survey, we study the feasibility of applying Large Language Models (LLM) to address this problem. LLMs are large neural network-based models which have demonstrated significant language processing abilities and shown to be capable of following a range of instructions within a broad scope. Recently, they have also been adapted for code understanding and generation tasks successfully, which makes them a promising choice for the automatic generation of IaC configurations. In this survey, we delve into the details of IaC, usage of IaC in different platforms, their challenges, LLMs in terms of code-generation aspects and the importance of LLMs in IaC along with our own experiments. Finally, we conclude by presenting the challenges in this area and highlighting the scope for future research.</p>
  </details>
</details>
<details>
  <summary>134. <b>标题：Design as Desired: Utilizing Visual Question Answering for Multimodal  Pre-training</b></summary>
  <p><b>编号</b>：[575]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00226">https://arxiv.org/abs/2404.00226</a></p>
  <p><b>作者</b>：Tongkun Su,  Jun Li,  Xi Zhang,  Haibo Jin,  Hao Chen,  Qiong Wang,  Faqin Lv,  Baoliang Zhao,  Yin Hu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Visual Question Answering, medical visual representations, representations from paired, paired medical reports, learns medical visual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multimodal pre-training demonstrates its potential in the medical domain, which learns medical visual representations from paired medical reports. However, many pre-training tasks require extra annotations from clinicians, and most of them fail to explicitly guide the model to learn the desired features of different pathologies. To the best of our knowledge, we are the first to utilize Visual Question Answering (VQA) for multimodal pre-training to guide the framework focusing on targeted pathological features. In this work, we leverage descriptions in medical reports to design multi-granular question-answer pairs associated with different diseases, which assist the framework in pre-training without requiring extra annotations from experts. We also propose a novel pre-training framework with a quasi-textual feature transformer, a module designed to transform visual features into a quasi-textual space closer to the textual domain via a contrastive learning strategy. This narrows the vision-language gap and facilitates modality alignment. Our framework is applied to four downstream tasks: report generation, classification, segmentation, and detection across five datasets. Extensive experiments demonstrate the superiority of our framework compared to other state-of-the-art methods. Our code will be released upon acceptance.</p>
  </details>
</details>
<details>
  <summary>135. <b>标题：Classification and Clustering of Sentence-Level Embeddings of Scientific  Articles Generated by Contrastive Learning</b></summary>
  <p><b>编号</b>：[577]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00224">https://arxiv.org/abs/2404.00224</a></p>
  <p><b>作者</b>：Gustavo Bartz Guedes,  Ana Estela Antunes da Silva</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：long text documents, text documents organized, organized into sections, documents organized, describing aspects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Scientific articles are long text documents organized into sections, each describing aspects of the research. Analyzing scientific production has become progressively challenging due to the increase in the number of available articles. Within this scenario, our approach consisted of fine-tuning transformer language models to generate sentence-level embeddings from scientific articles, considering the following labels: background, objective, methods, results, and conclusion. We trained our models on three datasets with contrastive learning. Two datasets are from the article's abstracts in the computer science and medical domains. Also, we introduce PMC-Sents-FULL, a novel dataset of sentences extracted from the full texts of medical articles. We compare the fine-tuned and baseline models in clustering and classification tasks to evaluate our approach. On average, clustering agreement measures values were five times higher. For the classification measures, in the best-case scenario, we had an average improvement in F1-micro of 30.73\%. Results show that fine-tuning sentence transformers with contrastive learning and using the generated embeddings in downstream tasks is a feasible approach to sentence classification in scientific articles. Our experiment codes are available on GitHub.</p>
  </details>
</details>
<details>
  <summary>136. <b>标题：Rationale-based Opinion Summarization</b></summary>
  <p><b>编号</b>：[578]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00217">https://arxiv.org/abs/2404.00217</a></p>
  <p><b>作者</b>：Haoyuan Li,  Snigdha Chaturvedi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：generate concise summaries, aims to generate, generate concise, large group, Opinion summarization aims</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Opinion summarization aims to generate concise summaries that present popular opinions of a large group of reviews. However, these summaries can be too generic and lack supporting details. To address these issues, we propose a new paradigm for summarizing reviews, rationale-based opinion summarization. Rationale-based opinion summaries output the representative opinions as well as one or more corresponding rationales. To extract good rationales, we define four desirable properties: relatedness, specificity, popularity, and diversity and present a Gibbs-sampling-based method to extract rationales. Overall, we propose RATION, an unsupervised extractive system that has two components: an Opinion Extractor (to extract representative opinions) and Rationales Extractor (to extract corresponding rationales). We conduct automatic and human evaluations to show that rationales extracted by RATION have the proposed properties and its summaries are more useful than conventional summaries. The implementation of our work is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>137. <b>标题：Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge  Editing Benchmark</b></summary>
  <p><b>编号</b>：[579]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00216">https://arxiv.org/abs/2404.00216</a></p>
  <p><b>作者</b>：Baolong Bi,  Shenghua Liu,  Yiwei Wang,  Lingrui Mei,  Xueqi Cheng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, human-like fashion, decoding methods, rapid development, development of large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rapid development of large language models (LLMs) enables them to convey factual knowledge in a more human-like fashion. Extensive efforts have been made to reduce factual hallucinations by modifying LLMs with factuality decoding. However, they also pose risks of hindering knowledge updates, as they make models overly confident in known facts. In this work, we first revisite the current factuality decoding methods and verified their effectiveness in enhancing factual accuracy. Subsequently, we conduct further evaluation of several strong factuality decoding methods on the knowledge editing benchmark. All these decoding methods significantly diminish the performance of llama2 models compared to their original decoding, with the largest decrease being a staggering 81.3\%. This further indicates that the current existing decoding methods still cannot perfectly address the factual hallucinations, as they overlook the importance of preserving the flexibility for knowledge editing. Therefore, our work suggests that research into factual alignment should simultaneously focus on the effectiveness of knowledge editing.</p>
  </details>
</details>
<details>
  <summary>138. <b>标题：Injecting New Knowledge into Large Language Models via Supervised  Fine-Tuning</b></summary>
  <p><b>编号</b>：[580]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00213">https://arxiv.org/abs/2404.00213</a></p>
  <p><b>作者</b>：Nick Mecklenburg,  Yiyou Lin,  Xiaoxiao Li,  Daniel Holstein,  Leonardo Nunes,  Sara Malvar,  Bruno Silva,  Ranveer Chandra,  Vijay Aski,  Pavan Kumar Reddy Yannam,  Tolga Aktas</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Large Language, generating human-like text, Language Models, shown remarkable performance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, Large Language Models (LLMs) have shown remarkable performance in generating human-like text, proving to be a valuable asset across various applications. However, adapting these models to incorporate new, out-of-domain knowledge remains a challenge, particularly for facts and events that occur after the model's knowledge cutoff date. This paper investigates the effectiveness of Supervised Fine-Tuning (SFT) as a method for knowledge injection in LLMs, specifically focusing on the domain of recent sporting events. We compare different dataset generation strategies -- token-based and fact-based scaling -- to create training data that helps the model learn new information. Our experiments on GPT-4 demonstrate that while token-based scaling can lead to improvements in Q&A accuracy, it may not provide uniform coverage of new knowledge. Fact-based scaling, on the other hand, offers a more systematic approach to ensure even coverage across all facts. We present a novel dataset generation process that leads to more effective knowledge ingestion through SFT, and our results show considerable performance improvements in Q&A tasks related to out-of-domain knowledge. This study contributes to the understanding of domain adaptation for LLMs and highlights the potential of SFT in enhancing the factuality of LLM responses in specific knowledge domains.</p>
  </details>
</details>
<details>
  <summary>139. <b>标题：Multi-Conditional Ranking with Large Language Models</b></summary>
  <p><b>编号</b>：[582]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00211">https://arxiv.org/abs/2404.00211</a></p>
  <p><b>作者</b>：Pouya Pezeshkpour,  Estevam Hruschka</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Utilizing large language, large language models, Utilizing large, large language, recommendation and retrieval</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Utilizing large language models (LLMs) to rank a set of items has become a common approach in recommendation and retrieval systems. Typically, these systems focus on ordering a substantial number of documents in a monotonic order based on a given query. However, real-world scenarios often present a different challenge: ranking a comparatively smaller set of items, but according to a variety of diverse and occasionally conflicting conditions. In this paper, we define and explore the task of multi-conditional ranking by introducing MCRank, a benchmark tailored for assessing multi-conditional ranking across various item types and conditions. Our analysis of LLMs using MCRank indicates a significant decrease in performance as the number and complexity of items and conditions grow. To overcome this limitation, we propose a novel decomposed reasoning method, consisting of EXtracting and Sorting the conditions, and then Iterativly Ranking the items (EXSIR). Our extensive experiments show that this decomposed reasoning method enhances LLMs' performance significantly, achieving up to a 12% improvement over existing LLMs. We also provide a detailed analysis of LLMs performance across various condition categories, and examine the effectiveness of decomposition step. Furthermore, we compare our method with existing approaches such as Chain-of-Thought and an encoder-type ranking model, demonstrating the superiority of our approach and complexity of MCR task. We released our dataset and code.</p>
  </details>
</details>
<details>
  <summary>140. <b>标题：EventGround: Narrative Reasoning by Grounding to Eventuality-centric  Knowledge Graphs</b></summary>
  <p><b>编号</b>：[584]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00209">https://arxiv.org/abs/2404.00209</a></p>
  <p><b>作者</b>：Cheng Jiayang,  Lin Qiu,  Chunkit Chan,  Xin Liu,  Yangqiu Song,  Zheng Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：story contexts, requires a wealth, wealth of background, knowledge, background world knowledge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Narrative reasoning relies on the understanding of eventualities in story contexts, which requires a wealth of background world knowledge. To help machines leverage such knowledge, existing solutions can be categorized into two groups. Some focus on implicitly modeling eventuality knowledge by pretraining language models (LMs) with eventuality-aware objectives. However, this approach breaks down knowledge structures and lacks interpretability. Others explicitly collect world knowledge of eventualities into structured eventuality-centric knowledge graphs (KGs). However, existing research on leveraging these knowledge sources for free-texts is limited. In this work, we propose an initial comprehensive framework called EventGround, which aims to tackle the problem of grounding free-texts to eventuality-centric KGs for contextualized narrative reasoning. We identify two critical problems in this direction: the event representation and sparsity problems. We provide simple yet effective parsing and partial information extraction methods to tackle these problems. Experimental results demonstrate that our approach consistently outperforms baseline models when combined with graph neural network (GNN) or large language model (LLM) based graph reasoning models. Our framework, incorporating grounded knowledge, achieves state-of-the-art performance while providing interpretable evidence.</p>
  </details>
</details>
<details>
  <summary>141. <b>标题：Causal Inference for Human-Language Model Collaboration</b></summary>
  <p><b>编号</b>：[586]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00207">https://arxiv.org/abs/2404.00207</a></p>
  <p><b>作者</b>：Bohan Zhang,  Yixin Wang,  Paramveer S. Dhillon</p>
  <p><b>备注</b>：9 pages (Accepted for publication at NAACL 2024 (Main Conference))</p>
  <p><b>关键词</b>：typically involve LMs, involve LMs proposing, interactions typically involve, proposing text segments, LMs proposing text</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we examine the collaborative dynamics between humans and language models (LMs), where the interactions typically involve LMs proposing text segments and humans editing or responding to these proposals. Productive engagement with LMs in such scenarios necessitates that humans discern effective text-based interaction strategies, such as editing and response styles, from historical human-LM interactions. This objective is inherently causal, driven by the counterfactual `what-if' question: how would the outcome of collaboration change if humans employed a different text editing/refinement strategy? A key challenge in answering this causal inference question is formulating an appropriate causal estimand: the conventional average treatment effect (ATE) estimand is inapplicable to text-based treatments due to their high dimensionality. To address this concern, we introduce a new causal estimand -- Incremental Stylistic Effect (ISE) -- which characterizes the average impact of infinitesimally shifting a text towards a specific style, such as increasing formality. We establish the conditions for the non-parametric identification of ISE. Building on this, we develop CausalCollab, an algorithm designed to estimate the ISE of various interaction strategies in dynamic human-LM collaborations. Our empirical investigations across three distinct human-LM collaboration scenarios reveal that CausalCollab effectively reduces confounding and significantly improves counterfactual estimation over a set of competitive baselines.</p>
  </details>
</details>
<details>
  <summary>142. <b>标题：Conceptual and Unbiased Reasoning in Language Models</b></summary>
  <p><b>编号</b>：[588]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00205">https://arxiv.org/abs/2404.00205</a></p>
  <p><b>作者</b>：Ben Zhou,  Hongming Zhang,  Sihao Chen,  Dian Yu,  Hongwei Wang,  Baolin Peng,  Dan Roth,  Dong Yu</p>
  <p><b>备注</b>：Preprint under review</p>
  <p><b>关键词</b>：perform conceptual reasoning, Conceptual reasoning, human cognition, ability to reason, generalization in human</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Conceptual reasoning, the ability to reason in abstract and high-level perspectives, is key to generalization in human cognition. However, limited study has been done on large language models' capability to perform conceptual reasoning. In this work, we bridge this gap and propose a novel conceptualization framework that forces models to perform conceptual reasoning on abstract questions and generate solutions in a verifiable symbolic space. Using this framework as an analytical tool, we show that existing large language models fall short on conceptual reasoning, dropping 9% to 28% on various benchmarks compared to direct inference methods. We then discuss how models can improve since high-level abstract reasoning is key to unbiased and generalizable decision-making. We propose two techniques to add trustworthy induction signals by generating familiar questions with similar underlying reasoning paths and asking models to perform self-refinement. Experiments show that our proposed techniques improve models' conceptual reasoning performance by 8% to 11%, achieving a more robust reasoning system that relies less on inductive biases.</p>
  </details>
</details>
<details>
  <summary>143. <b>标题：GPTA: Generative Prompt Tuning Assistant for Synergistic Downstream  Neural Network Enhancement with LLMs</b></summary>
  <p><b>编号</b>：[597]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00189">https://arxiv.org/abs/2404.00189</a></p>
  <p><b>作者</b>：Xiao Liu,  Jiawei Zhang</p>
  <p><b>备注</b>：Work in Progress</p>
  <p><b>关键词</b>：Large Language Model, Large Language, Language Model assistance, study introduces GPTA, downstream task model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This study introduces GPTA, a Large Language Model assistance training framework, that enhances the training of downstream task models via prefix prompt. By minimizing data exposure to LLM, the framework addresses the security and legal challenges of applying LLM in downstream task model training. GPTA utilizes a new synergistic training approach, optimizing the downstream models with parameter gradients and LLMs with the novel ``dialogue gradient''. The framework not only demonstrates significant improvements in model performance across six NLP benchmark datasets, but also reduces overfitting in low-resource scenarios effectively. The detailed analyses further validate that our pioneer framework provides a cost-efficient and adaptive method for downstream task model training with LLM support.</p>
  </details>
</details>
<details>
  <summary>144. <b>标题：DataAgent: Evaluating Large Language Models' Ability to Answer  Zero-Shot, Natural Language Queries</b></summary>
  <p><b>编号</b>：[598]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00188">https://arxiv.org/abs/2404.00188</a></p>
  <p><b>作者</b>：Manit Mishra,  Abderrahman Braham,  Charles Marsom,  Bryan Chung,  Gavin Griffin,  Dakshesh Sidnerlikar,  Chatanya Sarin,  Arjun Rajaram</p>
  <p><b>备注</b>：5 pages, Submitted to International Conference on AI in Cybersecurity</p>
  <p><b>关键词</b>：extracting meaningful information, Conventional processes, time-consuming and laborious, processes for analyzing, extracting meaningful</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Conventional processes for analyzing datasets and extracting meaningful information are often time-consuming and laborious. Previous work has identified manual, repetitive coding and data collection as major obstacles that hinder data scientists from undertaking more nuanced labor and high-level projects. To combat this, we evaluated OpenAI's GPT-3.5 as a "Language Data Scientist" (LDS) that can extrapolate key findings, including correlations and basic information, from a given dataset. The model was tested on a diverse set of benchmark datasets to evaluate its performance across multiple standards, including data science code-generation based tasks involving libraries such as NumPy, Pandas, Scikit-Learn, and TensorFlow, and was broadly successful in correctly answering a given data science query related to the benchmark dataset. The LDS used various novel prompt engineering techniques to effectively answer a given question, including Chain-of-Thought reinforcement and SayCan prompt engineering. Our findings demonstrate great potential for leveraging Large Language Models for low-level, zero-shot data analysis.</p>
  </details>
</details>
<details>
  <summary>145. <b>标题：Word Ladders: A Mobile Application for Semantic Data Collection</b></summary>
  <p><b>编号</b>：[601]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00184">https://arxiv.org/abs/2404.00184</a></p>
  <p><b>作者</b>：Marianna Marcella Bolognesi,  Claudia Collacciani,  Andrea Ferrari,  Francesca Genovese,  Tommaso Lamarra,  Adele Loia,  Giulia Rambelli,  Andrea Amelio Ravelli,  Caterina Villani</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：collecting linguistic data, free mobile application, Android and iOS, Abstraction project, Word Ladders</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Word Ladders is a free mobile application for Android and iOS, developed for collecting linguistic data, specifically lists of words related to each other through semantic relations of categorical inclusion, within the Abstraction project (ERC-2021-STG-101039777). We hereby provide an overview of Word Ladders, explaining its game logic, motivation and expected results and applications to nlp tasks as well as to the investigation of cognitive scientific open questions</p>
  </details>
</details>
<details>
  <summary>146. <b>标题：The LSCD Benchmark: a Testbed for Diachronic Word Meaning Tasks</b></summary>
  <p><b>编号</b>：[603]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00176">https://arxiv.org/abs/2404.00176</a></p>
  <p><b>作者</b>：Dominik Schlechtweg,  Shafqat Mumtaz Virk,  Nikolay Arefyev</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Semantic Change Detection, Lexical Semantic Change, Change Detection, Semantic Change, Lexical Semantic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Lexical Semantic Change Detection (LSCD) is a complex, lemma-level task, which is usually operationalized based on two subsequently applied usage-level tasks: First, Word-in-Context (WiC) labels are derived for pairs of usages. Then, these labels are represented in a graph on which Word Sense Induction (WSI) is applied to derive sense clusters. Finally, LSCD labels are derived by comparing sense clusters over time. This modularity is reflected in most LSCD datasets and models. It also leads to a large heterogeneity in modeling options and task definitions, which is exacerbated by a variety of dataset versions, preprocessing options and evaluation metrics. This heterogeneity makes it difficult to evaluate models under comparable conditions, to choose optimal model combinations or to reproduce results. Hence, we provide a benchmark repository standardizing LSCD evaluation. Through transparent implementation results become easily reproducible and by standardization different components can be freely combined. The repository reflects the task's modularity by allowing model evaluation for WiC, WSI and LSCD. This allows for careful evaluation of increasingly complex model components providing new ways of model optimization.</p>
  </details>
</details>
<details>
  <summary>147. <b>标题：Individual Text Corpora Predict Openness, Interests, Knowledge and Level  of Education</b></summary>
  <p><b>编号</b>：[609]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00165">https://arxiv.org/abs/2404.00165</a></p>
  <p><b>作者</b>：Markus J. Hofmann,  Markus T. Jansen,  Christoph Wigbels,  Benny Briesemeister,  Arthur M. Jacobs</p>
  <p><b>备注</b>：Proceedings of the 8th workshop on Cognitive Aspects of the Lexicon (CogALex-VIII), LREC/Coling 2024</p>
  <p><b>关键词</b>：google search history, individual google search, individual google, personality dimension, individual text corpora</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Here we examine whether the personality dimension of openness to experience can be predicted from the individual google search history. By web scraping, individual text corpora (ICs) were generated from 214 participants with a mean number of 5 million word tokens. We trained word2vec models and used the similarities of each IC to label words, which were derived from a lexical approach of personality. These IC-label-word similarities were utilized as predictive features in neural models. For training and validation, we relied on 179 participants and held out a test sample of 35 participants. A grid search with varying number of predictive features, hidden units and boost factor was performed. As model selection criterion, we used R2 in the validation samples penalized by the absolute R2 difference between training and validation. The selected neural model explained 35% of the openness variance in the test sample, while an ensemble model with the same architecture often provided slightly more stable predictions for intellectual interests, knowledge in humanities and level of education. Finally, a learning curve analysis suggested that around 500 training participants are required for generalizable predictions. We discuss ICs as a complement or replacement of survey-based psychodiagnostics.</p>
  </details>
</details>
<details>
  <summary>148. <b>标题：On-the-fly Definition Augmentation of LLMs for Biomedical NER</b></summary>
  <p><b>编号</b>：[615]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00152">https://arxiv.org/abs/2404.00152</a></p>
  <p><b>作者</b>：Monica Munnangi,  Sergey Feldman,  Byron C Wallace,  Silvio Amir,  Tom Hope,  Aakanksha Naik</p>
  <p><b>备注</b>：To appear at NAACL 2024</p>
  <p><b>关键词</b>：biomedical NER tasks, biomedical NER, NER tasks, general capabilities, difficult due</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite their general capabilities, LLMs still struggle on biomedical NER tasks, which are difficult due to the presence of specialized terminology and lack of training data. In this work we set out to improve LLM performance on biomedical NER in limited data settings via a new knowledge augmentation approach which incorporates definitions of relevant concepts on-the-fly. During this process, to provide a test bed for knowledge augmentation, we perform a comprehensive exploration of prompting strategies. Our experiments show that definition augmentation is useful for both open source and closed LLMs. For example, it leads to a relative improvement of 15\% (on average) in GPT-4 performance (F1) across all (six) of our test datasets. We conduct extensive ablations and analyses to demonstrate that our performance improvements stem from adding relevant definitional knowledge. We find that careful prompting strategies also improve LLM performance, allowing them to outperform fine-tuned language models in few-shot settings. To facilitate future research in this direction, we release our code at this https URL.</p>
  </details>
</details>
<details>
  <summary>149. <b>标题：Classifying Conspiratorial Narratives At Scale: False Alarms and  Erroneous Connections</b></summary>
  <p><b>编号</b>：[620]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00141">https://arxiv.org/abs/2404.00141</a></p>
  <p><b>作者</b>：Ahmad Diab,  Rr. Nefriana,  Yu-Ru Lin</p>
  <p><b>备注</b>：12 pages, 6 tables, 1 figure, conference ICWSM_24</p>
  <p><b>关键词</b>：frequently involve conspiracy, discussions frequently involve, involve conspiracy theories, conspiracy theories, frequently involve</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Online discussions frequently involve conspiracy theories, which can contribute to the proliferation of belief in them. However, not all discussions surrounding conspiracy theories promote them, as some are intended to debunk them. Existing research has relied on simple proxies or focused on a constrained set of signals to identify conspiracy theories, which limits our understanding of conspiratorial discussions across different topics and online communities. This work establishes a general scheme for classifying discussions related to conspiracy theories based on authors' perspectives on the conspiracy belief, which can be expressed explicitly through narrative elements, such as the agent, action, or objective, or implicitly through references to known theories, such as chemtrails or the New World Order. We leverage human-labeled ground truth to train a BERT-based model for classifying online CTs, which we then compared to the Generative Pre-trained Transformer machine (GPT) for detecting online conspiratorial content. Despite GPT's known strengths in its expressiveness and contextual understanding, our study revealed significant flaws in its logical reasoning, while also demonstrating comparable strengths from our classifiers. We present the first large-scale classification study using posts from the most active conspiracy-related Reddit forums and find that only one-third of the posts are classified as positive. This research sheds light on the potential applications of large language models in tasks demanding nuanced contextual comprehension.</p>
  </details>
</details>
<details>
  <summary>150. <b>标题：Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in  Sorani Kurdish</b></summary>
  <p><b>编号</b>：[628]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00124">https://arxiv.org/abs/2404.00124</a></p>
  <p><b>作者</b>：Sana Isam,  Hossein Hassani</p>
  <p><b>备注</b>：30 pages, 25 figures, 6 tables</p>
  <p><b>关键词</b>：Classifying Sorani Kurdish, Classifying Sorani, Sorani Kurdish subdialects, Sorani Kurdish, data collection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Classifying Sorani Kurdish subdialects poses a challenge due to the need for publicly available datasets or reliable resources like social media or websites for data collection. We conducted field visits to various cities and villages to address this issue, connecting with native speakers from different age groups, genders, academic backgrounds, and professions. We recorded their voices while engaging in conversations covering diverse topics such as lifestyle, background history, hobbies, interests, vacations, and life lessons. The target area of the research was the Kurdistan Region of Iraq. As a result, we accumulated 29 hours, 16 minutes, and 40 seconds of audio recordings from 107 interviews, constituting an unbalanced dataset encompassing six subdialects. Subsequently, we adapted three deep learning models: ANN, CNN, and RNN-LSTM. We explored various configurations, including different track durations, dataset splitting, and imbalanced dataset handling techniques such as oversampling and undersampling. Two hundred and twenty-five(225) experiments were conducted, and the outcomes were evaluated. The results indicated that the RNN-LSTM outperforms the other methods by achieving an accuracy of 96%. CNN achieved an accuracy of 93%, and ANN 75%. All three models demonstrated improved performance when applied to balanced datasets, primarily when we followed the oversampling approach. Future studies can explore additional future research directions to include other Kurdish dialects.</p>
  </details>
</details>
<details>
  <summary>151. <b>标题：A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping  Attacks</b></summary>
  <p><b>编号</b>：[641]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00076">https://arxiv.org/abs/2404.00076</a></p>
  <p><b>作者</b>：Orson Mengara</p>
  <p><b>备注</b>：Accept by "IEEE Access" let's take a look at our global approach to the DNN(s) model(s) deployment chain in production: Danger NLP-Speech(Trigger universal approach)</p>
  <p><b>关键词</b>：Audio-based machine learning, machine learning systems, learning systems frequently, Audio-based machine, machine learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Audio-based machine learning systems frequently use public or third-party data, which might be inaccurate. This exposes deep neural network (DNN) models trained on such data to potential data poisoning attacks. In this type of assault, attackers can train the DNN model using poisoned data, potentially degrading its performance. Another type of data poisoning attack that is extremely relevant to our investigation is label flipping, in which the attacker manipulates the labels for a subset of data. It has been demonstrated that these assaults may drastically reduce system performance, even for attackers with minimal abilities. In this study, we propose a backdoor attack named 'DirtyFlipping', which uses dirty label techniques, "label-on-label", to input triggers (clapping) in the selected data patterns associated with the target class, thereby enabling a stealthy backdoor.</p>
  </details>
</details>
<details>
  <summary>152. <b>标题：Deja vu: Contrastive Historical Modeling with Prefix-tuning for Temporal  Knowledge Graph Reasoning</b></summary>
  <p><b>编号</b>：[653]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00051">https://arxiv.org/abs/2404.00051</a></p>
  <p><b>作者</b>：Miao Peng,  Ben Liu,  Wenjie Xu,  Zihao Jiang,  Jiahui Zhu,  Min Peng</p>
  <p><b>备注</b>：Accepted to NAACL 2024 Findings</p>
  <p><b>关键词</b>：Knowledge Graph Reasoning, gaining increasing attention, inferring missing facts, Temporal Knowledge Graph, Graph Reasoning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Temporal Knowledge Graph Reasoning (TKGR) is the task of inferring missing facts for incomplete TKGs in complex scenarios (e.g., transductive and inductive settings), which has been gaining increasing attention. Recently, to mitigate dependence on structured connections in TKGs, text-based methods have been developed to utilize rich linguistic information from entity descriptions. However, suffering from the enormous parameters and inflexibility of pre-trained language models, existing text-based methods struggle to balance the textual knowledge and temporal information with computationally expensive purpose-built training strategies. To tap the potential of text-based models for TKGR in various complex scenarios, we propose ChapTER, a Contrastive historical modeling framework with prefix-tuning for TEmporal Reasoning. ChapTER feeds history-contextualized text into the pseudo-Siamese encoders to strike a textual-temporal balance via contrastive estimation between queries and candidates. By introducing virtual time prefix tokens, it applies a prefix-based tuning method to facilitate the frozen PLM capable for TKGR tasks under different settings. We evaluate ChapTER on four transductive and three few-shot inductive TKGR benchmarks, and experimental results demonstrate that ChapTER achieves superior performance compared to competitive baselines with only 0.17% tuned parameters. We conduct thorough analysis to verify the effectiveness, flexibility and efficiency of ChapTER.</p>
  </details>
</details>
<details>
  <summary>153. <b>标题：LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership  and Reasoning</b></summary>
  <p><b>编号</b>：[668]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00027">https://arxiv.org/abs/2404.00027</a></p>
  <p><b>作者</b>：Azmine Toushik Wasi,  Rafia Islam,  Raima Islam</p>
  <p><b>备注</b>：4 Pages, 2 Figures; The Third Workshop on Intelligent and Interactive Writing Assistants, a hybrid event co-located with The ACM CHI Conference on Human Factors in Computing Systems (CHI 2024) Openreview: this https URL</p>
  <p><b>关键词</b>：investment of thoughts, leading to attachment, Large Language Models, confines our investment, credit Large Language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems.</p>
  </details>
</details>
<details>
  <summary>154. <b>标题：Ink and Individuality: Crafting a Personalised Narrative in the Age of  LLMs</b></summary>
  <p><b>编号</b>：[669]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00026">https://arxiv.org/abs/2404.00026</a></p>
  <p><b>作者</b>：Azmine Toushik Wasi,  Raima Islam,  Rafia Islam</p>
  <p><b>备注</b>：4 Pages, 2 Figures; The Third Workshop on Intelligent and Interactive Writing Assistants, a hybrid event co-located with The ACM CHI Conference on Human Factors in Computing Systems (CHI 2024) Openreview: this https URL</p>
  <p><b>关键词</b>：effectively engage readers, conveying authenticity, comprise the distinctive, distinctive characteristics, characteristics that make</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.</p>
  </details>
</details>
<details>
  <summary>155. <b>标题：Psittacines of Innovation? Assessing the True Novelty of AI Creations</b></summary>
  <p><b>编号</b>：[676]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00017">https://arxiv.org/abs/2404.00017</a></p>
  <p><b>作者</b>：Anirban Mukherjee</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Artificial Intelligence, regurgitating patterns learned, examine whether Artificial, learned during training, regurgitating patterns</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We examine whether Artificial Intelligence (AI) systems generate truly novel ideas rather than merely regurgitating patterns learned during training. Utilizing a novel experimental design, we task an AI with generating project titles for hypothetical crowdfunding campaigns. We compare within AI-generated project titles, measuring repetition and complexity. We compare between the AI-generated titles and actual observed field data using an extension of maximum mean discrepancy--a metric derived from the application of kernel mean embeddings of statistical distributions to high-dimensional machine learning (large language) embedding vectors--yielding a structured analysis of AI output novelty. Results suggest that (1) the AI generates unique content even under increasing task complexity, and at the limits of its computational capabilities, (2) the generated content has face validity, being consistent with both inputs to other generative AI and in qualitative comparison to field data, and (3) exhibits divergence from field data, mitigating concerns relating to intellectual property rights. We discuss implications for copyright and trademark law.</p>
  </details>
</details>
<details>
  <summary>156. <b>标题：A novel interface for adversarial trivia question-writing</b></summary>
  <p><b>编号</b>：[679]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00011">https://arxiv.org/abs/2404.00011</a></p>
  <p><b>作者</b>：Jason Liu</p>
  <p><b>备注</b>：17 pages, 1 figure, 1 table</p>
  <p><b>关键词</b>：developing question-answering AIs, Quiz Bowl, natural language, critical component, component when developing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A critical component when developing question-answering AIs is an adversarial dataset that challenges models to adapt to the complex syntax and reasoning underlying our natural language. Present techniques for procedurally generating adversarial texts are not robust enough for training on complex tasks such as answering multi-sentence trivia questions. We instead turn to human-generated data by introducing an interface for collecting adversarial human-written trivia questions. Our interface is aimed towards question writers and players of Quiz Bowl, a buzzer-based trivia competition where paragraph-long questions consist of a sequence of clues of decreasing difficulty. To incentivize usage, a suite of machine learning-based tools in our interface assist humans in writing questions that are more challenging to answer for Quiz Bowl players and computers alike. Not only does our interface gather training data for the groundbreaking Quiz Bowl AI project QANTA, but it is also a proof-of-concept of future adversarial data collection for question-answering systems. The results of performance-testing our interface with ten originally-composed questions indicate that, despite some flaws, our interface's novel question-writing features as well as its real-time exposure of useful responses from our machine models could facilitate and enhance the collection of adversarial questions.</p>
  </details>
</details>
<details>
  <summary>157. <b>标题：A Statistical Framework of Watermarks for Large Language Models: Pivot,  Detection Efficiency and Optimal Rules</b></summary>
  <p><b>编号</b>：[686]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01245">https://arxiv.org/abs/2404.01245</a></p>
  <p><b>作者</b>：Xiang Li,  Feng Ruan,  Huiyuan Wang,  Qi Long,  Weijie J. Su</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, unnoticeable statistical signals, introduced in November, detection rules, language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of incorrectly classifying LLM-generated text as human-written). Our framework further reduces the problem of determining the optimal detection rule to solving a minimax optimization program. We apply this framework to two representative watermarks -- one of which has been internally implemented at OpenAI -- and obtain several findings that can be instrumental in guiding the practice of implementing watermarks. In particular, we derive optimal detection rules for these watermarks under our framework. These theoretically derived detection rules are demonstrated to be competitive and sometimes enjoy a higher power than existing detection approaches through numerical experiments.</p>
  </details>
</details>
<details>
  <summary>158. <b>标题：Scaling Properties of Speech Language Models</b></summary>
  <p><b>编号</b>：[710]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00685">https://arxiv.org/abs/2404.00685</a></p>
  <p><b>作者</b>：Santiago Cuervo,  Ricard Marxer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Language Models, Large Language Models, aim to learn, raw audio, textual resources</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost semantic understanding and the effects of coarser speech tokenization.</p>
  </details>
</details>
<details>
  <summary>159. <b>标题：Stress index strategy enhanced with financial news sentiment analysis  for the equity markets</b></summary>
  <p><b>编号</b>：[748]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00012">https://arxiv.org/abs/2404.00012</a></p>
  <p><b>作者</b>：Baptiste Lefort,  Eric Benhamou,  Jean-Jacques Ohana,  David Saltiel,  Beatrice Guez,  Thomas Jacquot</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：interpreting Bloomberg daily, Bloomberg daily market, daily market summaries, risk-on risk-off strategy, interpreting Bloomberg</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper introduces a new risk-on risk-off strategy for the stock market, which combines a financial stress indicator with a sentiment analysis done by ChatGPT reading and interpreting Bloomberg daily market summaries. Forecasts of market stress derived from volatility and credit spreads are enhanced when combined with the financial news sentiment derived from GPT-4. As a result, the strategy shows improved performance, evidenced by higher Sharpe ratio and reduced maximum drawdowns. The improved performance is consistent across the NASDAQ, the S&P 500 and the six major equity markets, indicating that the method generalises across equities markets.</p>
  </details>
</details>
<h1>计算机视觉</h1>
<details>
  <summary>1. <b>标题：NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation  Learning for Neural Radiance Fields</b></summary>
  <p><b>编号</b>：[1]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01300">https://arxiv.org/abs/2404.01300</a></p>
  <p><b>作者</b>：Muhammad Zubair Irshad,  Sergey Zakahrov,  Vitor Guizilini,  Adrien Gaidon,  Zsolt Kira,  Rares Ambrus</p>
  <p><b>备注</b>：29 pages, 13 figures. Project Page: this https URL</p>
  <p><b>关键词</b>：Neural fields excel, Neural fields, visual world, excel in computer, ability to understand</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural fields excel in computer vision and robotics due to their ability to understand the 3D visual world such as inferring semantics, geometry, and dynamics. Given the capabilities of neural fields in densely representing a 3D scene from 2D images, we ask the question: Can we scale their self-supervised pretraining, specifically using masked autoencoders, to generate effective 3D representations from posed RGB images. Owing to the astounding success of extending transformers to novel data modalities, we employ standard 3D Vision Transformers to suit the unique formulation of NeRFs. We leverage NeRF's volumetric grid as a dense input to the transformer, contrasting it with other 3D representations such as pointclouds where the information density can be uneven, and the representation is irregular. Due to the difficulty of applying masked autoencoders to an implicit representation, such as NeRF, we opt for extracting an explicit representation that canonicalizes scenes across domains by employing the camera trajectory for sampling. Our goal is made possible by masking random patches from NeRF's radiance and density grid and employing a standard 3D Swin Transformer to reconstruct the masked patches. In doing so, the model can learn the semantic and spatial structure of complete scenes. We pretrain this representation at scale on our proposed curated posed-RGB data, totaling over 1.6 million images. Once pretrained, the encoder is used for effective 3D transfer learning. Our novel self-supervised pretraining for NeRFs, NeRF-MAE, scales remarkably well and improves performance on various challenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining, NeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF scene understanding baselines on Front3D and ScanNet datasets with an absolute performance improvement of over 20% AP50 and 8% AP25 for 3D object detection.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：CausalChaos! Dataset for Comprehensive Causal Action Question Answering  Over Longer Causal Chains Grounded in Dynamic Visual Scenes</b></summary>
  <p><b>编号</b>：[2]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01299">https://arxiv.org/abs/2404.01299</a></p>
  <p><b>作者</b>：Ting En Lam,  Yuhan Chen,  Elston Tan,  Eric Peh,  Ruirui Chen,  Paritosh Parmar,  Basura Fernando</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：garnered increasing interest, video question answering, causal reasoning analysis, increasing interest, reasoning analysis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Causal video question answering (QA) has garnered increasing interest, yet existing datasets often lack depth in causal reasoning analysis. To address this gap, we capitalize on the unique properties of cartoons and construct CausalChaos!, a novel, challenging causal Why-QA dataset built upon the iconic "Tom and Jerry" cartoon series. With thoughtful questions and multi-level answers, our dataset contains much longer causal chains embedded in dynamic interactions and visuals, at the same time principles of animation allows animators to create well-defined, unambiguous causal relationships. These factors allow models to solve more challenging, yet well-defined causal relationships. We also introduce hard negative mining, including CausalConfusion version. While models perform well, there is much room for improvement, especially, on open-ended answers. We identify more advanced/explicit causal relationship modeling and joint modeling of vision and language as the immediate areas for future efforts to focus upon. Along with the other complementary datasets, our new challenging dataset will pave the way for these developments in the field. We will release our dataset, codes, and models to help future efforts in this domain.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Noise2Image: Noise-Enabled Static Scene Recovery for Event Cameras</b></summary>
  <p><b>编号</b>：[3]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01298">https://arxiv.org/abs/2404.01298</a></p>
  <p><b>作者</b>：Ruiming Cao,  Dekel Galor,  Amit Kohli,  Jacob L Yates,  Laura Waller</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：imaging dynamic scenes, noise events, events' and generally, generally cannot measure, imaging dynamic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Event cameras capture changes of intensity over time as a stream of 'events' and generally cannot measure intensity itself; hence, they are only used for imaging dynamic scenes. However, fluctuations due to random photon arrival inevitably trigger noise events, even for static scenes. While previous efforts have been focused on filtering out these undesirable noise events to improve signal quality, we find that, in the photon-noise regime, these noise events are correlated with the static scene intensity. We analyze the noise event generation and model its relationship to illuminance. Based on this understanding, we propose a method, called Noise2Image, to leverage the illuminance-dependent noise characteristics to recover the static parts of a scene, which are otherwise invisible to event cameras. We experimentally collect a dataset of noise events on static scenes to train and validate Noise2Image. Our results show that Noise2Image can robustly recover intensity images solely from noise events, providing a novel approach for capturing static scenes in event cameras, without additional hardware.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Streaming Dense Video Captioning</b></summary>
  <p><b>编号</b>：[4]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01297">https://arxiv.org/abs/2404.01297</a></p>
  <p><b>作者</b>：Xingyi Zhou,  Anurag Arnab,  Shyamal Buch,  Shen Yan,  Austin Myers,  Xuehan Xiong,  Arsha Nagrani,  Cordelia Schmid</p>
  <p><b>备注</b>：CVPR 2024. Code is available at this https URL</p>
  <p><b>关键词</b>：detailed textual descriptions, predicting captions localized, captions localized temporally, dense video captioning, predict rich</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>An ideal model for dense video captioning -- predicting captions localized temporally in a video -- should be able to handle long input videos, predict rich, detailed textual descriptions, and be able to produce outputs before processing the entire video. Current state-of-the-art models, however, process a fixed number of downsampled frames, and make a single full prediction after seeing the whole video. We propose a streaming dense video captioning model that consists of two novel components: First, we propose a new memory module, based on clustering incoming tokens, which can handle arbitrarily long videos as the memory is of a fixed size. Second, we develop a streaming decoding algorithm that enables our model to make predictions before the entire video has been processed. Our model achieves this streaming ability, and significantly improves the state-of-the-art on three dense video captioning benchmarks: ActivityNet, YouCook2 and ViTT. Our code is released at this https URL.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：MagicMirror: Fast and High-Quality Avatar Generation with a Constrained  Search Space</b></summary>
  <p><b>编号</b>：[5]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01296">https://arxiv.org/abs/2404.01296</a></p>
  <p><b>作者</b>：Armand Comas-Massagué,  Di Qiu,  Menglei Chai,  Marcel Bühler,  Amit Raj,  Ruiqi Gao,  Qiangeng Xu,  Mark Matthews,  Paulo Gotardo,  Octavia Camps,  Sergio Orts-Escolano,  Thabo Beeler</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：enhance user engagement, human avatar generation, Neural Radiance Fields, engagement and customization, enhance user</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce a novel framework for 3D human avatar generation and personalization, leveraging text prompts to enhance user engagement and customization. Central to our approach are key innovations aimed at overcoming the challenges in photo-realistic avatar synthesis. Firstly, we utilize a conditional Neural Radiance Fields (NeRF) model, trained on a large-scale unannotated multi-view dataset, to create a versatile initial solution space that accelerates and diversifies avatar generation. Secondly, we develop a geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models, to ensure superior view invariance and enable direct optimization of avatar geometry. These foundational ideas are complemented by our optimization pipeline built on Variational Score Distillation (VSD), which mitigates texture loss and over-saturation issues. As supported by our extensive experiments, these strategies collectively enable the creation of custom avatars with unparalleled visual quality and better adherence to input text prompts. You can find more results and videos in our website: this https URL</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：CosmicMan: A Text-to-Image Foundation Model for Humans</b></summary>
  <p><b>编号</b>：[7]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01294">https://arxiv.org/abs/2404.01294</a></p>
  <p><b>作者</b>：Shikai Li,  Jianglin Fu,  Kaiyuan Liu,  Wentao Wang,  Kwan-Yee Lin,  Wayne Wu</p>
  <p><b>备注</b>：Accepted by CVPR 2024. The supplementary material is included. Project Page: this https URL</p>
  <p><b>关键词</b>：generating high-fidelity human, foundation model specialized, high-fidelity human images, human images, generating high-fidelity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present CosmicMan, a text-to-image foundation model specialized for generating high-fidelity human images. Unlike current general-purpose foundation models that are stuck in the dilemma of inferior quality and text-image misalignment for humans, CosmicMan enables generating photo-realistic human images with meticulous appearance, reasonable structure, and precise text-image alignment with detailed dense descriptions. At the heart of CosmicMan's success are the new reflections and perspectives on data and models: (1) We found that data quality and a scalable data production flow are essential for the final results from trained models. Hence, we propose a new data production paradigm, Annotate Anyone, which serves as a perpetual data flywheel to produce high-quality data with accurate yet cost-effective annotations over time. Based on this, we constructed a large-scale dataset, CosmicMan-HQ 1.0, with 6 Million high-quality real-world human images in a mean resolution of 1488x1255, and attached with precise text annotations deriving from 115 Million attributes in diverse granularities. (2) We argue that a text-to-image foundation model specialized for humans must be pragmatic -- easy to integrate into down-streaming tasks while effective in producing high-quality human images. Hence, we propose to model the relationship between dense text descriptions and image pixels in a decomposed manner, and present Decomposed-Attention-Refocusing (Daring) training framework. It seamlessly decomposes the cross-attention features in existing text-to-image diffusion model, and enforces attention refocusing without adding extra modules. Through Daring, we show that explicitly discretizing continuous text space into several basic groups that align with human body structure is the key to tackling the misalignment problem in a breeze.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Measuring Style Similarity in Diffusion Models</b></summary>
  <p><b>编号</b>：[8]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01292">https://arxiv.org/abs/2404.01292</a></p>
  <p><b>作者</b>：Gowthami Somepalli,  Anubhav Gupta,  Kamal Gupta,  Shramay Palta,  Micah Goldblum,  Jonas Geiping,  Abhinav Shrivastava,  Tom Goldstein</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：graphic designers, style, training data, Generative models, image</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generative models are now widely used by graphic designers and artists. Prior works have shown that these models remember and often replicate content from their training data during generation. Hence as their proliferation increases, it has become important to perform a database search to determine whether the properties of the image are attributable to specific training data, every time before a generated image is used for professional purposes. Existing tools for this purpose focus on retrieving images of similar semantic content. Meanwhile, many artists are concerned with style replication in text-to-image models. We present a framework for understanding and extracting style descriptors from images. Our framework comprises a new dataset curated using the insight that style is a subjective property of an image that captures complex yet meaningful interactions of factors including but not limited to colors, textures, shapes, etc. We also propose a method to extract style descriptors that can be used to attribute style of a generated image to the images used in the training dataset of a text-to-image model. We showcase promising results in various style retrieval tasks. We also quantitatively and qualitatively analyze style attribution and matching in the Stable Diffusion model. Code and artifacts are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Evaluating Text-to-Visual Generation with Image-to-Text Generation</b></summary>
  <p><b>编号</b>：[9]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01291">https://arxiv.org/abs/2404.01291</a></p>
  <p><b>作者</b>：Zhiqiu Lin,  Deepak Pathak,  Baiqi Li,  Jiayao Li,  Xide Xia,  Graham Neubig,  Pengchuan Zhang,  Deva Ramanan</p>
  <p><b>备注</b>：We open-source our data, model, and code at: this https URL ; Project page: this https URL</p>
  <p><b>关键词</b>：comprehensive evaluation remains, evaluation remains challenging, comprehensive evaluation, significant progress, progress in generative</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite significant progress in generative AI, comprehensive evaluation remains challenging because of the lack of effective metrics and standardized benchmarks. For instance, the widely-used CLIPScore measures the alignment between a (generated) image and text prompt, but it fails to produce reliable scores for complex prompts involving compositions of objects, attributes, and relations. One reason is that text encoders of CLIP can notoriously act as a "bag of words", conflating prompts such as "the horse is eating the grass" with "the grass is eating the horse". To address this, we introduce the VQAScore, which uses a visual-question-answering (VQA) model to produce an alignment score by computing the probability of a "Yes" answer to a simple "Does this figure show '{text}'?" question. Though simpler than prior art, VQAScore computed with off-the-shelf models produces state-of-the-art results across many (8) image-text alignment benchmarks. We also compute VQAScore with an in-house model that follows best practices in the literature. For example, we use a bidirectional image-question encoder that allows image embeddings to depend on the question being asked (and vice versa). Our in-house model, CLIP-FlanT5, outperforms even the strongest baselines that make use of the proprietary GPT-4V. Interestingly, although we train with only images, VQAScore can also align text with video and 3D models. VQAScore allows researchers to benchmark text-to-visual generation using complex texts that capture the compositional structure of real-world prompts. We introduce GenAI-Bench, a more challenging benchmark with 1,600 compositional text prompts that require parsing scenes, objects, attributes, relationships, and high-order reasoning like comparison and logic. GenAI-Bench also offers over 15,000 human ratings for leading image and video generation models such as Stable Diffusion, DALL-E 3, and Gen2.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Large Motion Model for Unified Multi-Modal Motion Generation</b></summary>
  <p><b>编号</b>：[11]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01284">https://arxiv.org/abs/2404.01284</a></p>
  <p><b>作者</b>：Mingyuan Zhang,  Daisheng Jin,  Chenyang Gu,  Fangzhou Hong,  Zhongang Cai,  Jingfang Huang,  Chongzhi Zhang,  Xinying Guo,  Lei Yang,  Ying He,  Ziwei Liu</p>
  <p><b>备注</b>：Homepage: this https URL</p>
  <p><b>关键词</b>：Human motion generation, motion, motion generation, video production, cornerstone technique</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Human motion generation, a cornerstone technique in animation and video production, has widespread applications in various tasks like text-to-motion and music-to-dance. Previous works focus on developing specialist models tailored for each task without scalability. In this work, we present Large Motion Model (LMM), a motion-centric, multi-modal framework that unifies mainstream motion generation tasks into a generalist model. A unified motion model is appealing since it can leverage a wide range of motion data to achieve broad generalization beyond a single task. However, it is also challenging due to the heterogeneous nature of substantially different motion data and tasks. LMM tackles these challenges from three principled aspects: 1) Data: We consolidate datasets with different modalities, formats and tasks into a comprehensive yet unified motion generation dataset, MotionVerse, comprising 10 tasks, 16 datasets, a total of 320k sequences, and 100 million frames. 2) Architecture: We design an articulated attention mechanism ArtAttention that incorporates body part-aware modeling into Diffusion Transformer backbone. 3) Pre-Training: We propose a novel pre-training strategy for LMM, which employs variable frame rates and masking forms, to better exploit knowledge from diverse training data. Extensive experiments demonstrate that our generalist LMM achieves competitive performance across various standard motion generation tasks over state-of-the-art specialist models. Notably, LMM exhibits strong generalization capabilities and emerging properties across many unseen tasks. Additionally, our ablation studies reveal valuable insights about training and scaling up large motion models for future research.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：LoSA: Long-Short-range Adapter for Scaling End-to-End Temporal Action  Localization</b></summary>
  <p><b>编号</b>：[13]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01282">https://arxiv.org/abs/2404.01282</a></p>
  <p><b>作者</b>：Akshita Gupta,  Gaurav Mittal,  Ahmed Magooda,  Ye Yu,  Graham W. Taylor,  Mei Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Temporal Action Localization, classifying action snippets, Action Localization, classifying action, action snippets</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Temporal Action Localization (TAL) involves localizing and classifying action snippets in an untrimmed video. The emergence of large video foundation models has led RGB-only video backbones to outperform previous methods needing both RGB and optical flow modalities. Leveraging these large models is often limited to training only the TAL head due to the prohibitively large GPU memory required to adapt the video backbone for TAL. To overcome this limitation, we introduce LoSA, the first memory-and-parameter-efficient backbone adapter designed specifically for TAL to handle untrimmed videos. LoSA specializes for TAL by introducing Long-Short-range Adapters that adapt the intermediate layers of the video backbone over different temporal ranges. These adapters run parallel to the video backbone to significantly reduce memory footprint. LoSA also includes Long-Short-range Fusion that strategically combines the output of these adapters from the video backbone layers to enhance the video features provided to the TAL head. Experiments show that LoSA significantly outperforms all existing methods on standard TAL benchmarks, THUMOS-14 and ActivityNet-v1.3, by scaling end-to-end backbone adaptation to billion-parameter-plus models like VideoMAEv2~(ViT-g) and leveraging them beyond head-only transfer learning.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：BiPer: Binary Neural Networks using a Periodic Function</b></summary>
  <p><b>编号</b>：[14]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01278">https://arxiv.org/abs/2404.01278</a></p>
  <p><b>作者</b>：Edwin Vargas,  Claudia Correa,  Carlos Hinojosa,  Henry Arguello</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：reduced precision representations, Quantized neural networks, employ reduced precision, Binary Neural Networks, networks employ reduced</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Quantized neural networks employ reduced precision representations for both weights and activations. This quantization process significantly reduces the memory requirements and computational complexity of the network. Binary Neural Networks (BNNs) are the extreme quantization case, representing values with just one bit. Since the sign function is typically used to map real values to binary values, smooth approximations are introduced to mimic the gradients during error backpropagation. Thus, the mismatch between the forward and backward models corrupts the direction of the gradient, causing training inconsistency problems and performance degradation. In contrast to current BNN approaches, we propose to employ a binary periodic (BiPer) function during binarization. Specifically, we use a square wave for the forward pass to obtain the binary values and employ the trigonometric sine function with the same period of the square wave as a differentiable surrogate during the backward pass. We demonstrate that this approach can control the quantization error by using the frequency of the periodic function and improves network performance. Extensive experiments validate the effectiveness of BiPer in benchmark datasets and network architectures, with improvements of up to 1% and 0.69% with respect to state-of-the-art methods in the classification task over CIFAR-10 and ImageNet, respectively. Our code is publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Language Guided Domain Generalized Medical Image Segmentation</b></summary>
  <p><b>编号</b>：[17]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01272">https://arxiv.org/abs/2404.01272</a></p>
  <p><b>作者</b>：Shahina Kunhimon,  Muzammal Naseer,  Salman Khan,  Fahad Shahbaz Khan</p>
  <p><b>备注</b>：Accepted at ISBI2024</p>
  <p><b>关键词</b>：acquisition cost constraints, Single source domain, source domain generalization, real-world clinical settings, Single source</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Single source domain generalization (SDG) holds promise for more reliable and consistent image segmentation across real-world clinical settings particularly in the medical domain, where data privacy and acquisition cost constraints often limit the availability of diverse datasets. Depending solely on visual features hampers the model's capacity to adapt effectively to various domains, primarily because of the presence of spurious correlations and domain-specific characteristics embedded within the image features. Incorporating text features alongside visual features is a potential solution to enhance the model's understanding of the data, as it goes beyond pixel-level information to provide valuable context. Textual cues describing the anatomical structures, their appearances, and variations across various imaging modalities can guide the model in domain adaptation, ultimately contributing to more robust and consistent segmentation. In this paper, we propose an approach that explicitly leverages textual information by incorporating a contrastive learning mechanism guided by the text encoder features to learn a more robust feature representation. We assess the effectiveness of our text-guided contrastive feature alignment technique in various scenarios, including cross-modality, cross-sequence, and cross-site settings for different segmentation tasks. Our approach achieves favorable performance against existing methods in literature. Our code and model weights are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Bridging Remote Sensors with Multisensor Geospatial Foundation Models</b></summary>
  <p><b>编号</b>：[23]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01260">https://arxiv.org/abs/2404.01260</a></p>
  <p><b>作者</b>：Boran Han,  Shuai Zhang,  Xingjian Shi,  Markus Reichstein</p>
  <p><b>备注</b>：Accepted to CVPR</p>
  <p><b>关键词</b>：encompassing both optical, microwave technologies, offers a wealth, optical and microwave, distinct observational capabilities</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the realm of geospatial analysis, the diversity of remote sensors, encompassing both optical and microwave technologies, offers a wealth of distinct observational capabilities. Recognizing this, we present msGFM, a multisensor geospatial foundation model that effectively unifies data from four key sensor modalities. This integration spans an expansive dataset of two million multisensor images. msGFM is uniquely adept at handling both paired and unpaired sensor data. For data originating from identical geolocations, our model employs an innovative cross-sensor pretraining approach in masked image modeling, enabling the synthesis of joint representations from diverse sensors. msGFM, incorporating four remote sensors, upholds strong performance, forming a comprehensive model adaptable to various sensor types. msGFM has demonstrated enhanced proficiency in a range of both single-sensor and multisensor downstream tasks. These include scene classification, segmentation, cloud removal, and pan-sharpening. A key discovery of our research is that representations derived from natural images are not always compatible with the distinct characteristics of geospatial remote sensors, underscoring the limitations of existing representations in this field. Our work can serve as a guide for developing multisensor geospatial pretraining models, paving the way for more advanced geospatial capabilities.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Direct Preference Optimization of Video Large Multimodal Models from  Language Model Reward</b></summary>
  <p><b>编号</b>：[24]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01258">https://arxiv.org/abs/2404.01258</a></p>
  <p><b>作者</b>：Ruohong Zhang,  Liangke Gui,  Zhiqing Sun,  Yihao Feng,  Keyang Xu,  Yuanhan Zhang,  Di Fu,  Chunyuan Li,  Alexander Hauptmann,  Yonatan Bisk,  Yiming Yang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：direct preference optimization, Preference modeling techniques, shown effective, effective in enhancing, enhancing the generalization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Preference modeling techniques, such as direct preference optimization (DPO), has shown effective in enhancing the generalization abilities of large language model (LLM). However, in tasks involving video instruction-following, providing informative feedback, especially for detecting hallucinations in generated responses, remains a significant challenge. Previous studies have explored using large large multimodal models (LMMs) as reward models to guide preference modeling, but their ability to accurately assess the factuality of generated responses compared to corresponding videos has not been conclusively established. This paper introduces a novel framework that utilizes detailed video captions as a proxy of video content, enabling language models to incorporate this information as supporting evidence for scoring video Question Answering (QA) predictions. Our approach demonstrates robust alignment with OpenAI GPT-4V model's reward mechanism, which directly takes video frames as input. Furthermore, we show that applying this tailored reward through DPO significantly improves the performance of video LMMs on video QA tasks.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：FireANTs: Adaptive Riemannian Optimization for Multi-Scale Diffeomorphic  Registration</b></summary>
  <p><b>编号</b>：[28]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01249">https://arxiv.org/abs/2404.01249</a></p>
  <p><b>作者</b>：Rohit Jena,  Pratik Chaudhari,  James C. Gee</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Diffeomorphic Image Registration, atlas building, Image Registration, critical part, downstream tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diffeomorphic Image Registration is a critical part of the analysis in various imaging modalities and downstream tasks like image translation, segmentation, and atlas building. Registration algorithms based on optimization have stood the test of time in terms of accuracy, reliability, and robustness across a wide spectrum of modalities and acquisition settings. However, these algorithms converge slowly, are prohibitively expensive to run, and their usage requires a steep learning curve, limiting their scalability to larger clinical and scientific studies. In this paper, we develop multi-scale Adaptive Riemannian Optimization algorithms for diffeomorphic image registration. We demonstrate compelling improvements on image registration across a spectrum of modalities and anatomies by measuring structural and landmark overlap of the registered image volumes. Our proposed framework leads to a consistent improvement in performance, and from 300x up to 2000x speedup over existing algorithms. Our modular library design makes it easy to use and allows customization via user-defined cost functions.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Scalable Scene Modeling from Perspective Imaging: Physics-based  Appearance and Geometry Inference</b></summary>
  <p><b>编号</b>：[29]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01248">https://arxiv.org/abs/2404.01248</a></p>
  <p><b>作者</b>：Shuang Song</p>
  <p><b>备注</b>：Ph.D. Dissertation, Geospatial Data Analytics Lab, The Ohio State University, 2024. arXiv admin note: text overlap with arXiv:2108.08378</p>
  <p><b>关键词</b>：movie industry etc., modeling techniques serve, scene modeling techniques, deep learning methods, terrain mapping</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>3D scene modeling techniques serve as the bedrocks in the geospatial engineering and computer science, which drives many applications ranging from automated driving, terrain mapping, navigation, virtual, augmented, mixed, and extended reality (for gaming and movie industry etc.). This dissertation presents a fraction of contributions that advances 3D scene modeling to its state of the art, in the aspects of both appearance and geometry modeling. In contrast to the prevailing deep learning methods, as a core contribution, this thesis aims to develop algorithms that follow first principles, where sophisticated physic-based models are introduced alongside with simpler learning and inference tasks. The outcomes of these algorithms yield processes that can consume much larger volume of data for highly accurate reconstructing 3D scenes at a scale without losing methodological generality, which are not possible by contemporary complex-model based deep learning methods. Specifically, the dissertation introduces three novel methodologies that address the challenges of inferring appearance and geometry through physics-based modeling.
Overall, the research encapsulated in this dissertation marks a series of methodological triumphs in the processing of complex datasets. By navigating the confluence of deep learning, computational geometry, and photogrammetry, this work lays down a robust framework for future exploration and practical application in the rapidly evolving field of 3D scene reconstruction. The outcomes of these studies are evidenced through rigorous experiments and comparisons with existing state-of-the-art methods, demonstrating the efficacy and scalability of the proposed approaches.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：An image speaks a thousand words, but can everyone listen? On  translating images for cultural relevance</b></summary>
  <p><b>编号</b>：[30]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01247">https://arxiv.org/abs/2404.01247</a></p>
  <p><b>作者</b>：Simran Khanuja,  Sathyanarayanan Ramamoorthy,  Yueqi Song,  Graham Neubig</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：translators increasingly focus, human translators increasingly, multimedia content, rise of multimedia, translators increasingly</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Given the rise of multimedia content, human translators increasingly focus on culturally adapting not only words but also other modalities such as images to convey the same meaning. While several applications stand to benefit from this, machine translation systems remain confined to dealing with language in speech and text. In this work, we take a first step towards translating images to make them culturally relevant. First, we build three pipelines comprising state-of-the-art generative models to do the task. Next, we build a two-part evaluation dataset: i) concept: comprising 600 images that are cross-culturally coherent, focusing on a single concept per image, and ii) application: comprising 100 images curated from real-world applications. We conduct a multi-faceted human evaluation of translated images to assess for cultural relevance and meaning preservation. We find that as of today, image-editing models fail at this task, but can be improved by leveraging LLMs and retrievers in the loop. Best pipelines can only translate 5% of images for some countries in the easier concept dataset and no translation is successful for some countries in the application dataset, highlighting the challenging nature of the task. Our code and data is released here: this https URL.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：A Unified and Interpretable Emotion Representation and Expression  Generation</b></summary>
  <p><b>编号</b>：[31]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01243">https://arxiv.org/abs/2404.01243</a></p>
  <p><b>作者</b>：Reni Paskaleva,  Mykyta Holubakha,  Andela Ilic,  Saman Motamed,  Luc Van Gool,  Danda Paudel</p>
  <p><b>备注</b>：10 pages, 9 figures, 3 tables Accepted at CVPR 2024. Project page: this https URL</p>
  <p><b>关键词</b>：emotions, Canonical, unified emotion model, emotion model, easy to understand</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Canonical emotions, such as happy, sad, and fearful, are easy to understand and annotate. However, emotions are often compound, e.g. happily surprised, and can be mapped to the action units (AUs) used for expressing emotions, and trivially to the canonical ones. Intuitively, emotions are continuous as represented by the arousal-valence (AV) model. An interpretable unification of these four modalities - namely, Canonical, Compound, AUs, and AV - is highly desirable, for a better representation and understanding of emotions. However, such unification remains to be unknown in the current literature. In this work, we propose an interpretable and unified emotion model, referred as C2A2. We also develop a method that leverages labels of the non-unified models to annotate the novel unified one. Finally, we modify the text-conditional diffusion models to understand continuous numbers, which are then used to generate continuous expressions using our unified emotion model. Through quantitative and qualitative experiments, we show that our generated images are rich and capture subtle expressions. Our work allows a fine-grained generation of expressions in conjunction with other textual inputs and offers a new label space for emotions at the same time.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：StructLDM: Structured Latent Diffusion for 3D Human Generation</b></summary>
  <p><b>编号</b>：[33]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01241">https://arxiv.org/abs/2404.01241</a></p>
  <p><b>作者</b>：Tao Hu,  Fangzhou Hong,  Ziwei Liu</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：achieved remarkable progress, latent space, latent, structured latent space, progress by learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent 3D human generative models have achieved remarkable progress by learning 3D-aware GANs from 2D images. However, existing 3D human generative methods model humans in a compact 1D latent space, ignoring the articulated structure and semantics of human body topology. In this paper, we explore more expressive and higher-dimensional latent space for 3D human modeling and propose StructLDM, a diffusion-based unconditional 3D human generative model, which is learned from 2D images. StructLDM solves the challenges imposed due to the high-dimensional growth of latent space with three key designs: 1) A semantic structured latent space defined on the dense surface manifold of a statistical human body template. 2) A structured 3D-aware auto-decoder that factorizes the global latent space into several semantic body parts parameterized by a set of conditional structured local NeRFs anchored to the body template, which embeds the properties learned from the 2D training data and can be decoded to render view-consistent humans under different poses and clothing styles. 3) A structured latent diffusion model for generative human appearance sampling. Extensive experiments validate StructLDM's state-of-the-art generation performance and illustrate the expressiveness of the structured latent space over the well-adopted 1D latent space. Notably, StructLDM enables different levels of controllable 3D human generation and editing, including pose/view/shape control, and high-level tasks including compositional generations, part-aware clothing editing, 3D virtual try-on, etc. Our project page is at: this https URL.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：AURORA: Navigating UI Tarpits via Automated Neural Screen Understanding</b></summary>
  <p><b>编号</b>：[34]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01240">https://arxiv.org/abs/2404.01240</a></p>
  <p><b>作者</b>：Safwat Ali Khan,  Wenyu Wang,  Yiran Ren,  Bin Zhu,  Jiangfan Shi,  Alyssa McGowan,  Wing Lam,  Kevin Moran</p>
  <p><b>备注</b>：Published at 17th IEEE International Conference on Software Testing, Verification and Validation (ICST) 2024, 12 pages</p>
  <p><b>关键词</b>：Automated Input Generation, Input Generation tools, software platform, software engineering, Input Generation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Nearly a decade of research in software engineering has focused on automating mobile app testing to help engineers in overcoming the unique challenges associated with the software platform. Much of this work has come in the form of Automated Input Generation tools (AIG tools) that dynamically explore app screens. However, such tools have repeatedly been demonstrated to achieve lower-than-expected code coverage - particularly on sophisticated proprietary apps. Prior work has illustrated that a primary cause of these coverage deficiencies is related to so-called tarpits, or complex screens that are difficult to navigate.
In this paper, we take a critical step toward enabling AIG tools to effectively navigate tarpits during app exploration through a new form of automated semantic screen understanding. We introduce AURORA, a technique that learns from the visual and textual patterns that exist in mobile app UIs to automatically detect common screen designs and navigate them accordingly. The key idea of AURORA is that there are a finite number of mobile app screen designs, albeit with subtle variations, such that the general patterns of different categories of UI designs can be learned. As such, AURORA employs a multi-modal, neural screen classifier that is able to recognize the most common types of UI screen designs. After recognizing a given screen, it then applies a set of flexible and generalizable heuristics to properly navigate the screen. We evaluated AURORA both on a set of 12 apps with known tarpits from prior work, and on a new set of five of the most popular apps from the Google Play store. Our results indicate that AURORA is able to effectively navigate tarpit screens, outperforming prior approaches that avoid tarpits by 19.6% in terms of method coverage. The improvements can be attributed to AURORA's UI design classification and heuristic navigation techniques.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Open-Vocabulary Federated Learning with Multimodal Prototyping</b></summary>
  <p><b>编号</b>：[37]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01232">https://arxiv.org/abs/2404.01232</a></p>
  <p><b>作者</b>：Huimin Zeng,  Zhenrui Yue,  Dong Wang</p>
  <p><b>备注</b>：Accepted at NAACL 204</p>
  <p><b>关键词</b>：training label space, test label space, label space, Existing federated learning, training label</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing federated learning (FL) studies usually assume the training label space and test label space are identical. However, in real-world applications, this assumption is too ideal to be true. A new user could come up with queries that involve data from unseen classes, and such open-vocabulary queries would directly defect such FL systems. Therefore, in this work, we explicitly focus on the under-explored open-vocabulary challenge in FL. That is, for a new user, the global server shall understand her/his query that involves arbitrary unknown classes. To address this problem, we leverage the pre-trained vision-language models (VLMs). In particular, we present a novel adaptation framework tailored for VLMs in the context of FL, named as Federated Multimodal Prototyping (Fed-MP). Fed-MP adaptively aggregates the local model weights based on light-weight client residuals, and makes predictions based on a novel multimodal prototyping mechanism. Fed-MP exploits the knowledge learned from the seen classes, and robustifies the adapted VLM to unseen categories. Our empirical evaluation on various datasets validates the effectiveness of Fed-MP.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：SurMo: Surface-based 4D Motion Modeling for Dynamic Human Rendering</b></summary>
  <p><b>编号</b>：[43]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01225">https://arxiv.org/abs/2404.01225</a></p>
  <p><b>作者</b>：Tao Hu,  Fangzhou Hong,  Ziwei Liu</p>
  <p><b>备注</b>：Accepted to CVPR 2024. Project Page: this https URL</p>
  <p><b>关键词</b>：achieved remarkable progress, motion, video sequences, sequences has achieved, achieved remarkable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Dynamic human rendering from video sequences has achieved remarkable progress by formulating the rendering as a mapping from static poses to human images. However, existing methods focus on the human appearance reconstruction of every single frame while the temporal motion relations are not fully explored. In this paper, we propose a new 4D motion modeling paradigm, SurMo, that jointly models the temporal dynamics and human appearances in a unified framework with three key designs: 1) Surface-based motion encoding that models 4D human motions with an efficient compact surface-based triplane. It encodes both spatial and temporal motion relations on the dense surface manifold of a statistical body template, which inherits body topology priors for generalizable novel view synthesis with sparse training observations. 2) Physical motion decoding that is designed to encourage physical motion learning by decoding the motion triplane features at timestep t to predict both spatial derivatives and temporal derivatives at the next timestep t+1 in the training stage. 3) 4D appearance decoding that renders the motion triplanes into images by an efficient volumetric surface-conditioned renderer that focuses on the rendering of body surfaces with motion learning conditioning. Extensive experiments validate the state-of-the-art performance of our new paradigm and illustrate the expressiveness of surface-based motion triplanes for rendering high-fidelity view-consistent humans with fast motions and even motion-dependent shadows. Our project page is at: this https URL</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Feature Splatting: Language-Driven Physics-Based Scene Synthesis and  Editing</b></summary>
  <p><b>编号</b>：[45]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01223">https://arxiv.org/abs/2404.01223</a></p>
  <p><b>作者</b>：Ri-Zhao Qiu,  Ge Yang,  Weijia Zeng,  Xiaolong Wang</p>
  <p><b>备注</b>：Project website: this https URL</p>
  <p><b>关键词</b>：produced excellent results, primitives have produced, produced excellent, excellent results, results in modeling</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Scene representations using 3D Gaussian primitives have produced excellent results in modeling the appearance of static and dynamic 3D scenes. Many graphics applications, however, demand the ability to manipulate both the appearance and the physical properties of objects. We introduce Feature Splatting, an approach that unifies physics-based dynamic scene synthesis with rich semantics from vision language foundation models that are grounded by natural language. Our first contribution is a way to distill high-quality, object-centric vision-language features into 3D Gaussians, that enables semi-automatic scene decomposition using text queries. Our second contribution is a way to synthesize physics-based dynamics from an otherwise static scene using a particle-based simulator, in which material properties are assigned automatically via text queries. We ablate key techniques used in this pipeline, to illustrate the challenge and opportunities in using feature-carrying 3D Gaussians as a unified format for appearance, geometry, material properties and semantics grounded on natural language. Project website: this https URL</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Entity-Centric Reinforcement Learning for Object Manipulation from  Pixels</b></summary>
  <p><b>编号</b>：[46]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01220">https://arxiv.org/abs/2404.01220</a></p>
  <p><b>作者</b>：Dan Haramati,  Tal Daniel,  Aviv Tamar</p>
  <p><b>备注</b>：ICLR 2024 Spotlight. Videos and code are available on the project website: this https URL</p>
  <p><b>关键词</b>：human intelligence, hallmark of human, Reinforcement Learning, objects, Manipulating objects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Manipulating objects is a hallmark of human intelligence, and an important task in domains such as robotics. In principle, Reinforcement Learning (RL) offers a general approach to learn object manipulation. In practice, however, domains with more than a few objects are difficult for RL agents due to the curse of dimensionality, especially when learning from raw image observations. In this work we propose a structured approach for visual RL that is suitable for representing multiple objects and their interaction, and use it to learn goal-conditioned manipulation of several objects. Key to our method is the ability to handle goals with dependencies between the objects (e.g., moving objects in a certain order). We further relate our architecture to the generalization capability of the trained agent, based on a theoretical result for compositional generalization, and demonstrate agents that learn with 3 objects but generalize to similar tasks with over 10 objects. Videos and code are available on the project website: this https URL</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Vision-language models for decoding provider attention during neonatal  resuscitation</b></summary>
  <p><b>编号</b>：[52]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01207">https://arxiv.org/abs/2404.01207</a></p>
  <p><b>作者</b>：Felipe Parodi,  Jordan Matelsky,  Alejandra Regla-Vargas,  Elizabeth Foglia,  Charis Lim,  Danielle Weinberg,  Konrad Kording,  Heidi Herrick,  Michael Platt</p>
  <p><b>备注</b>：9 pages, 4 figures</p>
  <p><b>关键词</b>：process multiple streams, information simultaneously, demand an exceptional, exceptional level, level of attentiveness</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neonatal resuscitations demand an exceptional level of attentiveness from providers, who must process multiple streams of information simultaneously. Gaze strongly influences decision making; thus, understanding where a provider is looking during neonatal resuscitations could inform provider training, enhance real-time decision support, and improve the design of delivery rooms and neonatal intensive care units (NICUs). Current approaches to quantifying neonatal providers' gaze rely on manual coding or simulations, which limit scalability and utility. Here, we introduce an automated, real-time, deep learning approach capable of decoding provider gaze into semantic classes directly from first-person point-of-view videos recorded during live resuscitations. Combining state-of-the-art, real-time segmentation with vision-language models (CLIP), our low-shot pipeline attains 91\% classification accuracy in identifying gaze targets without training. Upon fine-tuning, the performance of our gaze-guided vision transformer exceeds 98\% accuracy in gaze classification, approaching human-level precision. This system, capable of real-time inference, enables objective quantification of provider attention dynamics during live neonatal resuscitation. Our approach offers a scalable solution that seamlessly integrates with existing infrastructure for data-scarce gaze analysis, thereby offering new opportunities for understanding and refining clinical decision making.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Video Interpolation with Diffusion Models</b></summary>
  <p><b>编号</b>：[56]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01203">https://arxiv.org/abs/2404.01203</a></p>
  <p><b>作者</b>：Siddhant Jain,  Daniel Watson,  Eric Tabellion,  Aleksander Hołyński,  Ben Poole,  Janne Kontkanen</p>
  <p><b>备注</b>：CVPR 2024, Project page at this https URL</p>
  <p><b>关键词</b>：creates short videos, creates short, present VIDIM, VIDIM, video interpolation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present VIDIM, a generative model for video interpolation, which creates short videos given a start and end frame. In order to achieve high fidelity and generate motions unseen in the input data, VIDIM uses cascaded diffusion models to first generate the target video at low resolution, and then generate the high-resolution video conditioned on the low-resolution generated video. We compare VIDIM to previous state-of-the-art methods on video interpolation, and demonstrate how such works fail in most settings where the underlying motion is complex, nonlinear, or ambiguous while VIDIM can easily handle such cases. We additionally demonstrate how classifier-free guidance on the start and end frame and conditioning the super-resolution model on the original high-resolution frames without additional parameters unlocks high-fidelity results. VIDIM is fast to sample from as it jointly denoises all the frames to be generated, requires less than a billion parameters per diffusion model to produce compelling results, and still enjoys scalability and improved quality at larger parameter counts.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Getting it Right: Improving Spatial Consistency in Text-to-Image Models</b></summary>
  <p><b>编号</b>：[58]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01197">https://arxiv.org/abs/2404.01197</a></p>
  <p><b>作者</b>：Agneet Chatterjee,  Gabriela Ben Melech Stan,  Estelle Aflalo,  Sayak Paul,  Dhruba Ghosh,  Tejas Gokhale,  Ludwig Schmidt,  Hannaneh Hajishirzi,  Vasudev Lal,  Chitta Baral,  Yezhou Yang</p>
  <p><b>备注</b>：project webpage : this https URL</p>
  <p><b>关键词</b>：consistently generate images, text prompt, key shortcomings, inability to consistently, consistently generate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>One of the key shortcomings in current text-to-image (T2I) models is their inability to consistently generate images which faithfully follow the spatial relationships specified in the text prompt. In this paper, we offer a comprehensive investigation of this limitation, while also developing datasets and methods that achieve state-of-the-art performance. First, we find that current vision-language datasets do not represent spatial relationships well enough; to alleviate this bottleneck, we create SPRIGHT, the first spatially-focused, large scale dataset, by re-captioning 6 million images from 4 widely used vision datasets. Through a 3-fold evaluation and analysis pipeline, we find that SPRIGHT largely improves upon existing datasets in capturing spatial relationships. To demonstrate its efficacy, we leverage only ~0.25% of SPRIGHT and achieve a 22% improvement in generating spatially accurate images while also improving the FID and CMMD scores. Secondly, we find that training on images containing a large number of objects results in substantial improvements in spatial consistency. Notably, we attain state-of-the-art on T2I-CompBench with a spatial score of 0.2133, by fine-tuning on <500 images. finally, through a set of controlled experiments and ablations, we document multiple findings that believe will enhance the understanding factors affect spatial consistency in text-to-image models. publicly release our dataset model to foster further research this area.< p>
  </500></p></details>
</details>
<details>
  <summary>28. <b>标题：Adaptive Query Prompting for Multi-Domain Landmark Detection</b></summary>
  <p><b>编号</b>：[60]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01194">https://arxiv.org/abs/2404.01194</a></p>
  <p><b>作者</b>：Qiusen Wei,  Guoheng Huang,  Xiaochen Yuan,  Xuhang Chen,  Guo Zhong,  Jianwen Huang,  Jiajie Huang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：medical imaging modalities, Adaptive Query Prompting, modalities and procedures, imaging modalities, landmark detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Medical landmark detection is crucial in various medical imaging modalities and procedures. Although deep learning-based methods have achieve promising performance, they are mostly designed for specific anatomical regions or tasks. In this work, we propose a universal model for multi-domain landmark detection by leveraging transformer architecture and developing a prompting component, named as Adaptive Query Prompting (AQP). Instead of embedding additional modules in the backbone network, we design a separate module to generate prompts that can be effectively extended to any other transformer network. In our proposed AQP, prompts are learnable parameters maintained in a memory space called prompt pool. The central idea is to keep the backbone frozen and then optimize prompts to instruct the model inference process. Furthermore, we employ a lightweight decoder to decode landmarks from the extracted features, namely Light-MLD. Thanks to the lightweight nature of the decoder and AQP, we can handle multiple datasets by sharing the backbone encoder and then only perform partial parameter tuning without incurring much additional cost. It has the potential to be extended to more landmark detection tasks. We conduct experiments on three widely used X-ray datasets for different medical landmark detection tasks. Our proposed Light-MLD coupled with AQP achieves SOTA performance on many metrics even without the use of elaborate structural designs or complex frameworks.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：MonoBox: Tightness-free Box-supervised Polyp 001 001 Segmentation using  Monotonicity Constraint</b></summary>
  <p><b>编号</b>：[62]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01188">https://arxiv.org/abs/2404.01188</a></p>
  <p><b>作者</b>：Qiang Hu,  Zhenyu Yi,  Ying Zhou,  Ting Li,  Fan Huang,  Mei Liu,  Zhiwei Wang,  Qiang Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：segmentation method constrained, innovative box-supervised segmentation, box-supervised segmentation method, user-unfriendly box-tightness assumption, box-supervised segmentation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose MonoBox, an innovative box-supervised segmentation method constrained by monotonicity to liberate its training from the user-unfriendly box-tightness assumption. In contrast to conventional box-supervised segmentation, where the box edges must precisely touch the target boundaries, MonoBox leverages imprecisely-annotated boxes to achieve robust pixel-wise segmentation. The 'linchpin' is that, within the noisy zones around box edges, MonoBox discards the traditional misguiding multiple-instance learning loss, and instead optimizes a carefully-designed objective, termed monotonicity constraint. Along directions transitioning from the foreground to background, this new constraint steers responses to adhere to a trend of monotonically decreasing values. Consequently, the originally unreliable learning within the noisy zones is transformed into a correct and effective monotonicity optimization. Moreover, an adaptive label correction is introduced, enabling MonoBox to enhance the tightness of box annotations using predicted masks from the previous epoch and dynamically shrink the noisy zones as training progresses. We verify MonoBox in the box-supervised segmentation task of polyps, where satisfying box-tightness is challenging due to the vague boundaries between the polyp and normal tissues. Experiments on both public synthetic and in-house real noisy datasets demonstrate that MonoBox exceeds other anti-noise state-of-the-arts by improving Dice by at least 5.5% and 3.3%, respectively. Codes are at this https URL.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：BEM: Balanced and Entropy-based Mix for Long-Tailed Semi-Supervised  Learning</b></summary>
  <p><b>编号</b>：[65]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01179">https://arxiv.org/abs/2404.01179</a></p>
  <p><b>作者</b>：Hongwei Zheng,  Linyuan Zhou,  Han Li,  Jinming Su,  Xiaoming Wei,  Xiaoming Xu</p>
  <p><b>备注</b>：This paper is accepted to CVPR 2024. The supplementary material is included</p>
  <p><b>关键词</b>：long-tailed semi-supervised learning, long-tailed semi-supervised, SSL, semi-supervised learning, play a crucial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Data mixing methods play a crucial role in semi-supervised learning (SSL), but their application is unexplored in long-tailed semi-supervised learning (LTSSL). The primary reason is that the in-batch mixing manner fails to address class imbalance. Furthermore, existing LTSSL methods mainly focus on re-balancing data quantity but ignore class-wise uncertainty, which is also vital for class balance. For instance, some classes with sufficient samples might still exhibit high uncertainty due to indistinguishable features. To this end, this paper introduces the Balanced and Entropy-based Mix (BEM), a pioneering mixing approach to re-balance the class distribution of both data quantity and uncertainty. Specifically, we first propose a class balanced mix bank to store data of each class for mixing. This bank samples data based on the estimated quantity distribution, thus re-balancing data quantity. Then, we present an entropy-based learning approach to re-balance class-wise uncertainty, including entropy-based sampling strategy, entropy-based selection module, and entropy-based class balanced loss. Our BEM first leverages data mixing for improving LTSSL, and it can also serve as a complement to the existing re-balancing methods. Experimental results show that BEM significantly enhances various LTSSL frameworks and achieves state-of-the-art performances across multiple benchmarks.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：SpikeMba: Multi-Modal Spiking Saliency Mamba for Temporal Video  Grounding</b></summary>
  <p><b>编号</b>：[68]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01174">https://arxiv.org/abs/2404.01174</a></p>
  <p><b>作者</b>：Wenrui Li,  Xiaopeng Hong,  Xiaopeng Fan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Temporal video grounding, video content understanding, critical task, video grounding, Spiking Neural Networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Temporal video grounding (TVG) is a critical task in video content understanding. Despite significant advancements, existing methods often limit in capturing the fine-grained relationships between multimodal inputs and the high computational costs with processing long video sequences. To address these limitations, we introduce a novel SpikeMba: multi-modal spiking saliency mamba for temporal video grounding. In our work, we integrate the Spiking Neural Networks (SNNs) and state space models (SSMs) to capture the fine-grained relationships of multimodal features effectively. Specifically, we introduce the relevant slots to enhance the model's memory capabilities, enabling a deeper contextual understanding of video sequences. The contextual moment reasoner leverages these slots to maintain a balance between contextual information preservation and semantic relevance exploration. Simultaneously, the spiking saliency detector capitalizes on the unique properties of SNNs to accurately locate salient proposals. Our experiments demonstrate the effectiveness of SpikeMba, which consistently outperforms state-of-the-art methods across mainstream benchmarks.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting</b></summary>
  <p><b>编号</b>：[70]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01168">https://arxiv.org/abs/2404.01168</a></p>
  <p><b>作者</b>：Jiarui Meng,  Haijie Li,  Yanmin Wu,  Qiankun Gao,  Shuzhou Yang,  Jian Zhang,  Siwei Ma</p>
  <p><b>备注</b>：22 pages, 7 figures</p>
  <p><b>关键词</b>：Gaussian Splatting, Neural Radiance Fields, predecessor Neural Radiance, marked a significant, significant breakthrough</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>3D Gaussian Splatting (3DGS) has marked a significant breakthrough in the realm of 3D scene reconstruction and novel view synthesis. However, 3DGS, much like its predecessor Neural Radiance Fields (NeRF), struggles to accurately model physical reflections, particularly in mirrors that are ubiquitous in real-world scenes. This oversight mistakenly perceives reflections as separate entities that physically exist, resulting in inaccurate reconstructions and inconsistent reflective properties across varied viewpoints. To address this pivotal challenge, we introduce Mirror-3DGS, an innovative rendering framework devised to master the intricacies of mirror geometries and reflections, paving the way for the generation of realistically depicted mirror reflections. By ingeniously incorporating mirror attributes into the 3DGS and leveraging the principle of plane mirror imaging, Mirror-3DGS crafts a mirrored viewpoint to observe from behind the mirror, enriching the realism of scene renderings. Extensive assessments, spanning both synthetic and real-world scenes, showcase our method's ability to render novel views with enhanced fidelity in real-time, surpassing the state-of-the-art Mirror-NeRF specifically within the challenging mirror regions. Our code will be made publicly available for reproducible research.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Diagnosis of Skin Cancer Using VGG16 and VGG19 Based Transfer Learning  Models</b></summary>
  <p><b>编号</b>：[75]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01160">https://arxiv.org/abs/2404.01160</a></p>
  <p><b>作者</b>：Amir Faghihi,  Mohammadreza Fathollahi,  Roozbeh Rajabi</p>
  <p><b>备注</b>：15 pages, journal</p>
  <p><b>关键词</b>：demands special attention, Merkel cell carcinoma, cell carcinoma, basal cell carcinoma, squamous cell carcinoma</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Today, skin cancer is considered as one of the most dangerous and common cancers in the world which demands special attention. Skin cancer may be developed in different types; including melanoma, actinic keratosis, basal cell carcinoma, squamous cell carcinoma, and Merkel cell carcinoma. Among them, melanoma is more unpredictable. Melanoma cancer can be diagnosed at early stages increasing the possibility of disease treatment. Automatic classification of skin lesions is a challenging task due to diverse forms and grades of the disease, demanding the requirement of novel methods implementation. Deep convolution neural networks (CNN) have shown an excellent potential for data and image classification. In this article, we inspect skin lesion classification problem using CNN techniques. Remarkably, we present that prominent classification accuracy of lesion detection can be obtained by proper designing and applying of transfer learning framework on pre-trained neural networks, without any requirement for data enlargement procedures i.e. merging VGG16 and VGG19 architectures pre-trained by a generic dataset with modified AlexNet network, and then, fine-tuned by a subject-specific dataset containing dermatology images. The convolution neural network was trained using 2541 images and, in particular, dropout was used to prevent the network from overfitting. Finally, the validity of the model was checked by applying the K-fold cross validation method. The proposed model increased classification accuracy by 3% (from 94.2% to 98.18%) in comparison with other methods.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：SyncMask: Synchronized Attentional Masking for Fashion-centric  Vision-Language Pretraining</b></summary>
  <p><b>编号</b>：[79]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01156">https://arxiv.org/abs/2404.01156</a></p>
  <p><b>作者</b>：Chull Hwan Song,  Taebaek Hwang,  Jooyoung Yoon,  Shunghyun Choi,  Yeong Hyeon Gu</p>
  <p><b>备注</b>：CVPR2024 Accepted</p>
  <p><b>关键词</b>：made significant strides, large-scale paired datasets, made significant, significant strides, strides in cross-modal</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision-language models (VLMs) have made significant strides in cross-modal understanding through large-scale paired datasets. However, in fashion domain, datasets often exhibit a disparity between the information conveyed in image and text. This issue stems from datasets containing multiple images of a single fashion item all paired with one text, leading to cases where some textual details are not visible in individual images. This mismatch, particularly when non-co-occurring elements are masked, undermines the training of conventional VLM objectives like Masked Language Modeling and Masked Image Modeling, thereby hindering the model's ability to accurately align fine-grained visual and textual features. Addressing this problem, we propose Synchronized attentional Masking (SyncMask), which generate masks that pinpoint the image patches and word tokens where the information co-occur in both image and text. This synchronization is accomplished by harnessing cross-attentional features obtained from a momentum model, ensuring a precise alignment between the two modalities. Additionally, we enhance grouped batch sampling with semi-hard negatives, effectively mitigating false negative issues in Image-Text Matching and Image-Text Contrastive learning objectives within fashion datasets. Our experiments demonstrate the effectiveness of the proposed approach, outperforming existing methods in three downstream tasks.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Uncovering the Text Embedding in Text-to-Image Diffusion Models</b></summary>
  <p><b>编号</b>：[81]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01154">https://arxiv.org/abs/2404.01154</a></p>
  <p><b>作者</b>：Hu Yu,  Hao Luo,  Fan Wang,  Feng Zhao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：minor textual modifications, induce substantial deviations, generated image exhibits, image exhibits opacity, generated image</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The correspondence between input text and the generated image exhibits opacity, wherein minor textual modifications can induce substantial deviations in the generated image. While, text embedding, as the pivotal intermediary between text and images, remains relatively underexplored. In this paper, we address this research gap by delving into the text embedding space, unleashing its capacity for controllable image editing and explicable semantic direction attributes within a learning-free framework. Specifically, we identify two critical insights regarding the importance of per-word embedding and their contextual correlations within text embedding, providing instructive principles for learning-free image editing. Additionally, we find that text embedding inherently possesses diverse semantic potentials, and further reveal this property through the lens of singular value decomposition (SVD). These uncovered properties offer practical utility for image editing and semantic discovery. More importantly, we expect the in-depth analyses and findings of the text embedding can enhance the understanding of text-to-image diffusion models.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Detect2Interact: Localizing Object Key Field in Visual Question  Answering (VQA) with LLMs</b></summary>
  <p><b>编号</b>：[82]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01151">https://arxiv.org/abs/2404.01151</a></p>
  <p><b>作者</b>：Jialou Wang,  Manli Zhu,  Yulei Li,  Honglei Li,  Longzhi Yang,  Wai Lok Woo</p>
  <p><b>备注</b>：Accepted to IEEE Intelligent Systems</p>
  <p><b>关键词</b>：Localization plays, role in enhancing, enhancing the practicality, practicality and precision, crucial role</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Localization plays a crucial role in enhancing the practicality and precision of VQA systems. By enabling fine-grained identification and interaction with specific parts of an object, it significantly improves the system's ability to provide contextually relevant and spatially accurate responses, crucial for applications in dynamic environments like robotics and augmented reality. However, traditional systems face challenges in accurately mapping objects within images to generate nuanced and spatially aware responses. In this work, we introduce "Detect2Interact", which addresses these challenges by introducing an advanced approach for fine-grained object visual key field detection. First, we use the segment anything model (SAM) to generate detailed spatial maps of objects in images. Next, we use Vision Studio to extract semantic object descriptions. Third, we employ GPT-4's common sense knowledge, bridging the gap between an object's semantics and its spatial map. As a result, Detect2Interact achieves consistent qualitative results on object key field detection across extensive test cases and outperforms the existing VQA system with object detection by providing a more reasonable and finer visual representation.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Condition-Aware Neural Network for Controlled Image Generation</b></summary>
  <p><b>编号</b>：[87]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01143">https://arxiv.org/abs/2404.01143</a></p>
  <p><b>作者</b>：Han Cai,  Muyang Li,  Zhuoyang Zhang,  Qinsheng Zhang,  Ming-Yu Liu,  Song Han</p>
  <p><b>备注</b>：CVPR 2024</p>
  <p><b>关键词</b>：Condition-Aware Neural Network, Neural Network, present Condition-Aware Neural, image generative models, Network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present Condition-Aware Neural Network (CAN), a new method for adding control to image generative models. In parallel to prior conditional control methods, CAN controls the image generation process by dynamically manipulating the weight of the neural network. This is achieved by introducing a condition-aware weight generation module that generates conditional weight for convolution/linear layers based on the input condition. We test CAN on class-conditional image generation on ImageNet and text-to-image generation on COCO. CAN consistently delivers significant improvements for diffusion transformer models, including DiT and UViT. In particular, CAN combined with EfficientViT (CaT) achieves 2.78 FID on ImageNet 512x512, surpassing DiT-XL/2 while requiring 52x fewer MACs per sampling step.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Structured Initialization for Attention in Vision Transformers</b></summary>
  <p><b>编号</b>：[90]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01139">https://arxiv.org/abs/2404.01139</a></p>
  <p><b>作者</b>：Jianqiao Zheng,  Xueqian Li,  Simon Lucey</p>
  <p><b>备注</b>：20 pages, 5 figures, 8 tables</p>
  <p><b>关键词</b>：small-scale datasets poses, vision transformer, training of vision, datasets poses, significant challenge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The training of vision transformer (ViT) networks on small-scale datasets poses a significant challenge. By contrast, convolutional neural networks (CNNs) have an architectural inductive bias enabling them to perform well on such problems. In this paper, we argue that the architectural bias inherent to CNNs can be reinterpreted as an initialization bias within ViT. This insight is significant as it empowers ViTs to perform equally well on small-scale problems while maintaining their flexibility for large-scale applications. Our inspiration for this ``structured'' initialization stems from our empirical observation that random impulse filters can achieve comparable performance to learned filters within CNNs. Our approach achieves state-of-the-art performance for data-efficient ViT learning across numerous benchmarks including CIFAR-10, CIFAR-100, and SVHN.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：CityGaussian: Real-time High-quality Large-Scale Scene Rendering with  Gaussians</b></summary>
  <p><b>编号</b>：[93]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01133">https://arxiv.org/abs/2404.01133</a></p>
  <p><b>作者</b>：Yang Liu,  He Guan,  Chuanchen Luo,  Lue Fan,  Junran Peng,  Zhaoxiang Zhang</p>
  <p><b>备注</b>：Project Page: this https URL</p>
  <p><b>关键词</b>：Gaussian Splatting, view synthesis, significantly propelled, Splatting, training</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The advancement of real-time 3D scene reconstruction and novel view synthesis has been significantly propelled by 3D Gaussian Splatting (3DGS). However, effectively training large-scale 3DGS and rendering it in real-time across various scales remains challenging. This paper introduces CityGaussian (CityGS), which employs a novel divide-and-conquer training approach and Level-of-Detail (LoD) strategy for efficient large-scale 3DGS training and rendering. Specifically, the global scene prior and adaptive training data selection enables efficient training and seamless fusion. Based on fused Gaussian primitives, we generate different detail levels through compression, and realize fast rendering across various scales through the proposed block-wise detail levels selection and aggregation strategy. Extensive experimental results on large-scale scenes demonstrate that our approach attains state-of-theart rendering quality, enabling consistent real-time rendering of largescale scenes across vastly different scales. Our project page is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Medical Visual Prompting (MVP): A Unified Framework for Versatile and  High-Quality Medical Image Segmentation</b></summary>
  <p><b>编号</b>：[96]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01127">https://arxiv.org/abs/2404.01127</a></p>
  <p><b>作者</b>：Yulin Chen,  Guoheng Huang,  Kai Huang,  Zijin Lin,  Guo Zhong,  Shenghong Luo,  Jie Deng,  Jian Zhou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Guided Prompting, Mechanism Guided Prompting, Attention Mechanism Guided, crucial for clinical, clinical diagnosis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Accurate segmentation of lesion regions is crucial for clinical diagnosis and treatment across various diseases. While deep convolutional networks have achieved satisfactory results in medical image segmentation, they face challenges such as loss of lesion shape information due to continuous convolution and downsampling, as well as the high cost of manually labeling lesions with varying shapes and sizes. To address these issues, we propose a novel medical visual prompting (MVP) framework that leverages pre-training and prompting concepts from natural language processing (NLP). The framework utilizes three key components: Super-Pixel Guided Prompting (SPGP) for superpixelating the input image, Image Embedding Guided Prompting (IEGP) for freezing patch embedding and merging with superpixels to provide visual prompts, and Adaptive Attention Mechanism Guided Prompting (AAGP) for pinpointing prompt content and efficiently adapting all layers. By integrating SPGP, IEGP, and AAGP, the MVP enables the segmentation network to better learn shape prompting information and facilitates mutual learning across different tasks. Extensive experiments conducted on five datasets demonstrate superior performance of this method in various challenging medical image tasks, while simplifying single-task medical segmentation models. This novel framework offers improved performance with fewer parameters and holds significant potential for accurate segmentation of lesion regions in various medical tasks, making it clinically valuable.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：CLIPtone: Unsupervised Learning for Text-based Image Tone Adjustment</b></summary>
  <p><b>编号</b>：[98]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01123">https://arxiv.org/abs/2404.01123</a></p>
  <p><b>作者</b>：Hyeongmin Lee,  Kyoungkook Kang,  Jungseul Ok,  Sunghyun Cho</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：human-centric perceptual assessment, learning human-centric perceptual, predominantly adopted supervised, adopted supervised learning, supervised learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent image tone adjustment (or enhancement) approaches have predominantly adopted supervised learning for learning human-centric perceptual assessment. However, these approaches are constrained by intrinsic challenges of supervised learning. Primarily, the requirement for expertly-curated or retouched images escalates the data acquisition expenses. Moreover, their coverage of target style is confined to stylistic variants inferred from the training data. To surmount the above challenges, we propose an unsupervised learning-based approach for text-based image tone adjustment method, CLIPtone, that extends an existing image enhancement method to accommodate natural language descriptions. Specifically, we design a hyper-network to adaptively modulate the pretrained parameters of the backbone model based on text description. To assess whether the adjusted image aligns with the text description without ground truth image, we utilize CLIP, which is trained on a vast set of language-image pairs and thus encompasses knowledge of human perception. The major advantages of our approach are three fold: (i) minimal data collection expenses, (ii) support for a range of adjustments, and (iii) the ability to handle novel text descriptions unseen in training. Our approach's efficacy is demonstrated through comprehensive experiments, including a user study.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：CMT: Cross Modulation Transformer with Hybrid Loss for Pansharpening</b></summary>
  <p><b>编号</b>：[100]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01121">https://arxiv.org/abs/2404.01121</a></p>
  <p><b>作者</b>：Wen-Jie Shu,  Hong-Xia Dou,  Rui Wen,  Xiao Wu,  Liang-Jian Deng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：merging high-resolution panchromatic, enhance remote sensing, remote sensing image, quality by merging, high-resolution panchromatic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pansharpening aims to enhance remote sensing image (RSI) quality by merging high-resolution panchromatic (PAN) with multispectral (MS) images. However, prior techniques struggled to optimally fuse PAN and MS images for enhanced spatial and spectral information, due to a lack of a systematic framework capable of effectively coordinating their individual strengths. In response, we present the Cross Modulation Transformer (CMT), a pioneering method that modifies the attention mechanism. This approach utilizes a robust modulation technique from signal processing, integrating it into the attention mechanism's calculations. It dynamically tunes the weights of the carrier's value (V) matrix according to the modulator's features, thus resolving historical challenges and achieving a seamless integration of spatial and spectral attributes. Furthermore, considering that RSI exhibits large-scale features and edge details along with local textures, we crafted a hybrid loss function that combines Fourier and wavelet transforms to effectively capture these characteristics, thereby enhancing both spatial and spectral accuracy in pansharpening. Extensive experiments demonstrate our framework's superior performance over existing state-of-the-art methods. The code will be publicly available to encourage further research.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：Motion Blur Decomposition with Cross-shutter Guidance</b></summary>
  <p><b>编号</b>：[101]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01120">https://arxiv.org/abs/2404.01120</a></p>
  <p><b>作者</b>：Xiang Ji,  Haiyang Jiang,  Yinqiang Zheng</p>
  <p><b>备注</b>：Accepted to CVPR 2024</p>
  <p><b>关键词</b>：observed image artifact, frequently observed image, frequently observed, insufficient illumination, motion blur decomposition</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Motion blur is a frequently observed image artifact, especially under insufficient illumination where exposure time has to be prolonged so as to collect more photons for a bright enough image. Rather than simply removing such blurring effects, recent researches have aimed at decomposing a blurry image into multiple sharp images with spatial and temporal coherence. Since motion blur decomposition itself is highly ambiguous, priors from neighbouring frames or human annotation are usually needed for motion disambiguation. In this paper, inspired by the complementary exposure characteristics of a global shutter (GS) camera and a rolling shutter (RS) camera, we propose to utilize the ordered scanline-wise delay in a rolling shutter image to robustify motion decomposition of a single blurry image. To evaluate this novel dual imaging setting, we construct a triaxial system to collect realistic data, as well as a deep network architecture that explicitly addresses temporal and contextual information through reciprocal branches for cross-shutter motion blur decomposition. Experiment results have verified the effectiveness of our proposed algorithm, as well as the validity of our dual imaging setting.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Few shot point cloud reconstruction and denoising via learned Guassian  splats renderings and fine-tuned diffusion features</b></summary>
  <p><b>编号</b>：[104]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01112">https://arxiv.org/abs/2404.01112</a></p>
  <p><b>作者</b>：Pietro Bonazzi,  Marie-Julie Rakatosaona,  Marco Cannici,  Federico Tombari,  Davide Scaramuzza</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Existing deep learning, deep learning methods, deep learning, point clouds rely, point clouds</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing deep learning methods for the reconstruction and denoising of point clouds rely on small datasets of 3D shapes. We circumvent the problem by leveraging deep learning methods trained on billions of images. We propose a method to reconstruct point clouds from few images and to denoise point clouds from their rendering by exploiting prior knowledge distilled from image-based deep learning models. To improve reconstruction in constraint settings, we regularize the training of a differentiable renderer with hybrid surface and appearance by introducing semantic consistency supervision. In addition, we propose a pipeline to finetune Stable Diffusion to denoise renderings of noisy point clouds and we demonstrate how these learned filters can be used to remove point cloud noise coming without 3D supervision. We compare our method with DSS and PointRadiance and achieved higher quality 3D reconstruction on the Sketchfab Testset and SCUT Dataset.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：UFID: A Unified Framework for Input-level Backdoor Detection on  Diffusion Models</b></summary>
  <p><b>编号</b>：[111]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01101">https://arxiv.org/abs/2404.01101</a></p>
  <p><b>作者</b>：Zihan Guan,  Mengxuan Hu,  Sheng Li,  Anil Vullikanti</p>
  <p><b>备注</b>：20 pages,18 figures</p>
  <p><b>关键词</b>：malicious attackers inject, Diffusion Models, attackers inject backdoors, training stage, training samples</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diffusion Models are vulnerable to backdoor attacks, where malicious attackers inject backdoors by poisoning some parts of the training samples during the training stage. This poses a serious threat to the downstream users, who query the diffusion models through the API or directly download them from the internet. To mitigate the threat of backdoor attacks, there have been a plethora of investigations on backdoor detections. However, none of them designed a specialized backdoor detection method for diffusion models, rendering the area much under-explored. Moreover, these prior methods mainly focus on the traditional neural networks in the classification task, which cannot be adapted to the backdoor detections on the generative task easily. Additionally, most of the prior methods require white-box access to model weights and architectures, or the probability logits as additional information, which are not always practical. In this paper, we propose a Unified Framework for Input-level backdoor Detection (UFID) on the diffusion models, which is motivated by observations in the diffusion models and further validated with a theoretical causality analysis. Extensive experiments across different datasets on both conditional and unconditional diffusion models show that our method achieves a superb performance on detection effectiveness and run-time efficiency. The code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：HairFastGAN: Realistic and Robust Hair Transfer with a Fast  Encoder-Based Approach</b></summary>
  <p><b>编号</b>：[115]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01094">https://arxiv.org/abs/2404.01094</a></p>
  <p><b>作者</b>：Maxim Nikolaev,  Mikhail Kuznetsov,  Dmitry Vetrov,  Aibek Alanov</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：virtual hair try-on, hair try-on, addresses the complex, virtual hair, hairstyle transfer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Our paper addresses the complex task of transferring a hairstyle from a reference image to an input photo for virtual hair try-on. This task is challenging due to the need to adapt to various photo poses, the sensitivity of hairstyles, and the lack of objective metrics. The current state of the art hairstyle transfer methods use an optimization process for different parts of the approach, making them inexcusably slow. At the same time, faster encoder-based models are of very low quality because they either operate in StyleGAN's W+ space or use other low-dimensional image generators. Additionally, both approaches have a problem with hairstyle transfer when the source pose is very different from the target pose, because they either don't consider the pose at all or deal with it inefficiently. In our paper, we present the HairFast model, which uniquely solves these problems and achieves high resolution, near real-time performance, and superior reconstruction compared to optimization problem-based methods. Our solution includes a new architecture operating in the FS latent space of StyleGAN, an enhanced inpainting approach, and improved encoders for better alignment, color transfer, and a new encoder for post-processing. The effectiveness of our approach is demonstrated on realism metrics after random hairstyle transfer and reconstruction when the original hairstyle is transferred. In the most difficult scenario of transferring both shape and color of a hairstyle from different images, our method performs in less than a second on the Nvidia V100. Our code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Texture-Preserving Diffusion Models for High-Fidelity Virtual Try-On</b></summary>
  <p><b>编号</b>：[117]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01089">https://arxiv.org/abs/2404.01089</a></p>
  <p><b>作者</b>：Xu Yang,  Changxing Ding,  Zhibin Hong,  Junhao Huang,  Jin Tao,  Xiangmin Xu</p>
  <p><b>备注</b>：CVPR 2024</p>
  <p><b>关键词</b>：increasingly important task, Image-based virtual try-on, online shopping, increasingly important, image</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image-based virtual try-on is an increasingly important task for online shopping. It aims to synthesize images of a specific person wearing a specified garment. Diffusion model-based approaches have recently become popular, as they are excellent at image synthesis tasks. However, these approaches usually employ additional image encoders and rely on the cross-attention mechanism for texture transfer from the garment to the person image, which affects the try-on's efficiency and fidelity. To address these issues, we propose an Texture-Preserving Diffusion (TPD) model for virtual try-on, which enhances the fidelity of the results and introduces no additional image encoders. Accordingly, we make contributions from two aspects. First, we propose to concatenate the masked person and reference garment images along the spatial dimension and utilize the resulting image as the input for the diffusion model's denoising UNet. This enables the original self-attention layers contained in the diffusion model to achieve efficient and accurate texture transfer. Second, we propose a novel diffusion-based method that predicts a precise inpainting mask based on the person and reference garment images, further enhancing the reliability of the try-on results. In addition, we integrate mask prediction and image synthesis into a single compact model. The experimental results show that our approach can be applied to various try-on tasks, e.g., garment-to-person and person-to-person try-ons, and significantly outperforms state-of-the-art methods on popular VITON, VITON-HD databases.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：PhysReaction: Physically Plausible Real-Time Humanoid Reaction Synthesis  via Forward Dynamics Guided 4D Imitation</b></summary>
  <p><b>编号</b>：[119]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01081">https://arxiv.org/abs/2404.01081</a></p>
  <p><b>作者</b>：Yunze Liu,  Changxi Chen,  Chenjing Ding,  Li Yi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Humanoid Reaction Synthesis, creating highly interactive, Synthesis is pivotal, Reaction Synthesis, pivotal for creating</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Humanoid Reaction Synthesis is pivotal for creating highly interactive and empathetic robots that can seamlessly integrate into human environments, enhancing the way we live, work, and communicate. However, it is difficult to learn the diverse interaction patterns of multiple humans and generate physically plausible reactions. The kinematics-based approaches face challenges, including issues like floating feet, sliding, penetration, and other problems that defy physical plausibility. The existing physics-based method often relies on kinematics-based methods to generate reference states, which struggle with the challenges posed by kinematic noise during action execution. Constrained by their reliance on diffusion models, these methods are unable to achieve real-time inference. In this work, we propose a Forward Dynamics Guided 4D Imitation method to generate physically plausible human-like reactions. The learned policy is capable of generating physically plausible and human-like reactions in real-time, significantly improving the speed(x33) and quality of reactions compared with the existing method. Our experiments on the InterHuman and Chi3D datasets, along with ablation studies, demonstrate the effectiveness of our approach.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Stale Diffusion: Hyper-realistic 5D Movie Generation Using Old-school  Methods</b></summary>
  <p><b>编号</b>：[121]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01079">https://arxiv.org/abs/2404.01079</a></p>
  <p><b>作者</b>：Joao F. Henriques,  Dylan Campbell,  Tengda Han</p>
  <p><b>备注</b>：SIGBOVIK 2024</p>
  <p><b>关键词</b>：Diffusion achieved super-human, achieved super-human performance, Stable Diffusion achieved, Stable Diffusion, ossifies Stable Diffusion</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Two years ago, Stable Diffusion achieved super-human performance at generating images with super-human numbers of fingers. Following the steady decline of its technical novelty, we propose Stale Diffusion, a method that solidifies and ossifies Stable Diffusion in a maximum-entropy state. Stable Diffusion works analogously to a barn (the Stable) from which an infinite set of horses have escaped (the Diffusion). As the horses have long left the barn, our proposal may be seen as antiquated and irrelevant. Nevertheless, we vigorously defend our claim of novelty by identifying as early adopters of the Slow Science Movement, which will produce extremely important pearls of wisdom in the future. Our speed of contributions can also be seen as a quasi-static implementation of the recent call to pause AI experiments, which we wholeheartedly support. As a result of a careful archaeological expedition to 18-months-old Git commit histories, we found that naturally-accumulating errors have produced a novel entropy-maximising Stale Diffusion method, that can produce sleep-inducing hyper-realistic 5D video that is as good as one's imagination.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Prompt Learning for Oriented Power Transmission Tower Detection in  High-Resolution SAR Images</b></summary>
  <p><b>编号</b>：[124]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01074">https://arxiv.org/abs/2404.01074</a></p>
  <p><b>作者</b>：Tianyang Li,  Chao Wang,  Hong Zhang</p>
  <p><b>备注</b>：22 pages, 12figures</p>
  <p><b>关键词</b>：synthetic aperture radar, comparatively small size, hindering tower identification, frequently hindering tower, Detecting transmission towers</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Detecting transmission towers from synthetic aperture radar (SAR) images remains a challenging task due to the comparatively small size and side-looking geometry, with background clutter interference frequently hindering tower identification. A large number of interfering signals superimposes the return signal from the tower. We found that localizing or prompting positions of power transmission towers is beneficial to address this obstacle. Based on this revelation, this paper introduces prompt learning into the oriented object detector (P2Det) for multimodal information learning. P2Det contains the sparse prompt coding and cross-attention between the multimodal data. Specifically, the sparse prompt encoder (SPE) is proposed to represent point locations, converting prompts into sparse embeddings. The image embeddings are generated through the Transformer layers. Then a two-way fusion module (TWFM) is proposed to calculate the cross-attention of the two different embeddings. The interaction of image-level and prompt-level features is utilized to address the clutter interference. A shape-adaptive refinement module (SARM) is proposed to reduce the effect of aspect ratio. Extensive experiments demonstrated the effectiveness of the proposed model on high-resolution SAR images. P2Det provides a novel insight for multimodal object detection due to its competitive performance.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：T-Mamba: Frequency-Enhanced Gated Long-Range Dependency for Tooth 3D  CBCT Segmentation</b></summary>
  <p><b>编号</b>：[129]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01065">https://arxiv.org/abs/2404.01065</a></p>
  <p><b>作者</b>：Jing Hao,  Lei He,  Kuo Feng Hung</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：remains challenging due, Efficient tooth segmentation, convolutional Neural Networks, low contrast, critical for orthodontic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Efficient tooth segmentation in three-dimensional (3D) imaging, critical for orthodontic diagnosis, remains challenging due to noise, low contrast, and artifacts in CBCT images. Both convolutional Neural Networks (CNNs) and transformers have emerged as popular architectures for image segmentation. However, their efficacy in handling long-range dependencies is limited due to inherent locality or computational complexity. To address this issue, we propose T-Mamba, integrating shared positional encoding and frequency-based features into vision mamba, to address limitations in spatial position preservation and feature enhancement in frequency domain. Besides, we also design a gate selection unit to integrate two features in spatial domain and one feature in frequency domain adaptively. T-Mamba is the first work to introduce frequency-based features into vision mamba. Extensive experiments demonstrate that T-Mamba achieves new SOTA results on the public Tooth CBCT dataset and outperforms previous SOTA methods by a large margin, i.e., IoU + 3.63%, SO + 2.43%, DSC +2.30%, HD -4.39mm, and ASSD -0.37mm. The code and models are publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Roadside Monocular 3D Detection via 2D Detection Prompting</b></summary>
  <p><b>编号</b>：[130]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01064">https://arxiv.org/abs/2404.01064</a></p>
  <p><b>作者</b>：Yechi Ma,  Shuoquan Wei,  Churun Zhang,  Wei Hua,  Yanan Li,  Shu Kong</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：requires detecting objects, RGB frame, detection requires detecting, requires detecting, detecting objects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The problem of roadside monocular 3D detection requires detecting objects of interested classes in a 2D RGB frame and predicting their 3D information such as locations in bird's-eye-view (BEV). It has broad applications in traffic control, vehicle-vehicle communication, and vehicle-infrastructure cooperative perception. To approach this problem, we present a novel and simple method by prompting the 3D detector using 2D detections. Our method builds on a key insight that, compared with 3D detectors, a 2D detector is much easier to train and performs significantly better w.r.t detections on the 2D image plane. That said, one can exploit 2D detections of a well-trained 2D detector as prompts to a 3D detector, being trained in a way of inflating such 2D detections to 3D towards 3D detection. To construct better prompts using the 2D detector, we explore three techniques: (a) concatenating both 2D and 3D detectors' features, (b) attentively fusing 2D and 3D detectors' features, and (c) encoding predicted 2D boxes x, y, width, height, label and attentively fusing such with the 3D detector's features. Surprisingly, the third performs the best. Moreover, we present a yaw tuning tactic and a class-grouping strategy that merges classes based on their functionality; these techniques improve 3D detection performance further. Comprehensive ablation studies and extensive experiments demonstrate that our method resoundingly outperforms prior works, achieving the state-of-the-art on two large-scale roadside 3D detection benchmarks.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior</b></summary>
  <p><b>编号</b>：[137]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01053">https://arxiv.org/abs/2404.01053</a></p>
  <p><b>作者</b>：David Svitov,  Pietro Morerio,  Lourdes Agapito,  Alessio Del Bue</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：monocular input videos, animatable human avatar, human avatar generation, input videos, generation from monocular</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present HAHA - a novel approach for animatable human avatar generation from monocular input videos. The proposed method relies on learning the trade-off between the use of Gaussian splatting and a textured mesh for efficient and high fidelity rendering. We demonstrate its efficiency to animate and render full-body human avatars controlled via the SMPL-X parametric model. Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh where it is necessary, like hair and out-of-mesh clothing. This results in a minimal number of Gaussians being used to represent the full avatar, and reduced rendering artifacts. This allows us to handle the animation of small body parts such as fingers that are traditionally disregarded. We demonstrate the effectiveness of our approach on two open datasets: SnapshotPeople and X-Humans. Our method demonstrates on par reconstruction quality to the state-of-the-art on SnapshotPeople, while using less than a third of Gaussians. HAHA outperforms previous state-of-the-art on novel poses from X-Humans both quantitatively and qualitatively.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Action Detection via an Image Diffusion Process</b></summary>
  <p><b>编号</b>：[138]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01051">https://arxiv.org/abs/2404.01051</a></p>
  <p><b>作者</b>：Lin Geng Foo,  Tianjiao Li,  Hossein Rahmani,  Jun Liu</p>
  <p><b>备注</b>：Accepted to CVPR 2024</p>
  <p><b>关键词</b>：Action detection, Action detection aims, untrimmed videos, aims to localize, predict the classes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Action detection aims to localize the starting and ending points of action instances in untrimmed videos, and predict the classes of those instances. In this paper, we make the observation that the outputs of the action detection task can be formulated as images. Thus, from a novel perspective, we tackle action detection via a three-image generation process to generate starting point, ending point and action-class predictions as images via our proposed Action Detection Image Diffusion (ADI-Diff) framework. Furthermore, since our images differ from natural images and exhibit special properties, we further explore a Discrete Action-Detection Diffusion Process and a Row-Column Transformer design to better handle their processing. Our ADI-Diff framework achieves state-of-the-art results on two widely-used datasets.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Drag Your Noise: Interactive Point-based Editing via Diffusion Semantic  Propagation</b></summary>
  <p><b>编号</b>：[139]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01050">https://arxiv.org/abs/2404.01050</a></p>
  <p><b>作者</b>：Haofeng Liu,  Chenshu Xu,  Yifei Yang,  Lihua Zeng,  Shengfeng He</p>
  <p><b>备注</b>：Accepted by CVPR 2024</p>
  <p><b>关键词</b>：existing generative models, generative models, essential tool, tool to complement, complement the controllability</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Point-based interactive editing serves as an essential tool to complement the controllability of existing generative models. A concurrent work, DragDiffusion, updates the diffusion latent map in response to user inputs, causing global latent map alterations. This results in imprecise preservation of the original content and unsuccessful editing due to gradient vanishing. In contrast, we present DragNoise, offering robust and accelerated editing without retracing the latent map. The core rationale of DragNoise lies in utilizing the predicted noise output of each U-Net as a semantic editor. This approach is grounded in two critical observations: firstly, the bottleneck features of U-Net inherently possess semantically rich features ideal for interactive editing; secondly, high-level semantics, established early in the denoising process, show minimal variation in subsequent stages. Leveraging these insights, DragNoise edits diffusion semantics in a single denoising step and efficiently propagates these changes, ensuring stability and efficiency in diffusion editing. Comparative experiments reveal that DragNoise achieves superior control and semantic retention, reducing the optimization time by over 50% compared to DragDiffusion. Our codes are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：Higher education assessment practice in the era of generative AI tools</b></summary>
  <p><b>编号</b>：[143]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01036">https://arxiv.org/abs/2404.01036</a></p>
  <p><b>作者</b>：Bayode Ogunleye,  Kudirat Ibilola Zakariyyah,  Oluwaseun Ajao,  Olakunle Olayinka,  Hemlata Sharma</p>
  <p><b>备注</b>：11 pages, 7 tables published in the Journal of Applied Learning & Teaching</p>
  <p><b>关键词</b>：higher education, sector benefits, society at large, benefits every nation, nation economy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The higher education (HE) sector benefits every nation's economy and society at large. However, their contributions are challenged by advanced technologies like generative artificial intelligence (GenAI) tools. In this paper, we provide a comprehensive assessment of GenAI tools towards assessment and pedagogic practice and, subsequently, discuss the potential impacts. This study experimented using three assessment instruments from data science, data analytics, and construction management disciplines. Our findings are two-fold: first, the findings revealed that GenAI tools exhibit subject knowledge, problem-solving, analytical, critical thinking, and presentation skills and thus can limit learning when used unethically. Secondly, the design of the assessment of certain disciplines revealed the limitations of the GenAI tools. Based on our findings, we made recommendations on how AI tools can be utilised for teaching and learning in HE.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Survey of Bias In Text-to-Image Generation: Definition, Evaluation, and  Mitigation</b></summary>
  <p><b>编号</b>：[144]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01030">https://arxiv.org/abs/2404.01030</a></p>
  <p><b>作者</b>：Yixin Wan,  Arjun Subramonian,  Anaelia Ovalle,  Zongyu Lin,  Ashima Suvarna,  Christina Chance,  Hritik Bansal,  Rebecca Pattichis,  Kai-Wei Chang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Google Gemini, generate high-quality images, generation abilities, enables users, users to generate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The recent advancement of large and powerful models with Text-to-Image (T2I) generation abilities -- such as OpenAI's DALLE-3 and Google's Gemini -- enables users to generate high-quality images from textual prompts. However, it has become increasingly evident that even simple prompts could cause T2I models to exhibit conspicuous social bias in generated images. Such bias might lead to both allocational and representational harms in society, further marginalizing minority groups. Noting this problem, a large body of recent works has been dedicated to investigating different dimensions of bias in T2I systems. However, an extensive review of these studies is lacking, hindering a systematic understanding of current progress and research gaps. We present the first extensive survey on bias in T2I generative models. In this survey, we review prior studies on dimensions of bias: Gender, Skintone, and Geo-Culture. Specifically, we discuss how these works define, evaluate, and mitigate different aspects of bias. We found that: (1) while gender and skintone biases are widely studied, geo-cultural bias remains under-explored; (2) most works on gender and skintone bias investigated occupational association, while other aspects are less frequently studied; (3) almost all gender bias works overlook non-binary identities in their studies; (4) evaluation datasets and metrics are scattered, with no unified framework for measuring biases; and (5) current mitigation methods fail to resolve biases comprehensively. Based on current limitations, we point out future research directions that contribute to human-centric definitions, evaluations, and mitigation of biases. We hope to highlight the importance of studying biases in T2I systems, as well as encourage future efforts to holistically understand and tackle biases, building fair and trustworthy T2I technologies for everyone.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：AIGCOIQA2024: Perceptual Quality Assessment of AI Generated  Omnidirectional Images</b></summary>
  <p><b>编号</b>：[146]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01024">https://arxiv.org/abs/2404.01024</a></p>
  <p><b>作者</b>：Liu Yang,  Huiyu Duan,  Long Teng,  Yucheng Zhu,  Xiaohong Liu,  Menghan Hu,  Xiongkuo Min,  Guangtao Zhai,  Patrick Le Callet</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Intelligence Generated Content, Artificial Intelligence Generated, Artificial Intelligence, attracted widespread attention, Generated Content</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, the rapid advancement of Artificial Intelligence Generated Content (AIGC) has attracted widespread attention. Among the AIGC, AI generated omnidirectional images hold significant potential for Virtual Reality (VR) and Augmented Reality (AR) applications, hence omnidirectional AIGC techniques have also been widely studied. AI-generated omnidirectional images exhibit unique distortions compared to natural omnidirectional images, however, there is no dedicated Image Quality Assessment (IQA) criteria for assessing them. This study addresses this gap by establishing a large-scale AI generated omnidirectional image IQA database named AIGCOIQA2024 and constructing a comprehensive benchmark. We first generate 300 omnidirectional images based on 5 AIGC models utilizing 25 text prompts. A subjective IQA experiment is conducted subsequently to assess human visual preferences from three perspectives including quality, comfortability, and correspondence. Finally, we conduct a benchmark experiment to evaluate the performance of state-of-the-art IQA models on our database. The database will be released to facilitate future research.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Harnessing Large Language Models for Training-free Video Anomaly  Detection</b></summary>
  <p><b>编号</b>：[151]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01014">https://arxiv.org/abs/2404.01014</a></p>
  <p><b>作者</b>：Luca Zanella,  Willi Menapace,  Massimiliano Mancini,  Yiming Wang,  Elisa Ricci</p>
  <p><b>备注</b>：CVPR 2024. Project website at this https URL</p>
  <p><b>关键词</b>：temporally locate abnormal, locate abnormal events, aims to temporally, temporally locate, locate abnormal</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Video anomaly detection (VAD) aims to temporally locate abnormal events in a video. Existing works mostly rely on training deep models to learn the distribution of normality with either video-level supervision, one-class supervision, or in an unsupervised setting. Training-based methods are prone to be domain-specific, thus being costly for practical deployment as any domain change will involve data collection and model training. In this paper, we radically depart from previous efforts and propose LAnguage-based VAD (LAVAD), a method tackling VAD in a novel, training-free paradigm, exploiting the capabilities of pre-trained large language models (LLMs) and existing vision-language models (VLMs). We leverage VLM-based captioning models to generate textual descriptions for each frame of any test video. With the textual scene description, we then devise a prompting mechanism to unlock the capability of LLMs in terms of temporal aggregation and anomaly score estimation, turning LLMs into an effective video anomaly detector. We further leverage modality-aligned VLMs and propose effective techniques based on cross-modal similarity for cleaning noisy captions and refining the LLM-based anomaly scores. We evaluate LAVAD on two large datasets featuring real-world surveillance scenarios (UCF-Crime and XD-Violence), showing that it outperforms both unsupervised and one-class methods without requiring any training or data collection.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Teeth-SEG: An Efficient Instance Segmentation Framework for Orthodontic  Treatment based on Anthropic Prior Knowledge</b></summary>
  <p><b>编号</b>：[152]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01013">https://arxiv.org/abs/2404.01013</a></p>
  <p><b>作者</b>：Bo Zou,  Shaofeng Wang,  Hao Liu,  Gaoyue Sun,  Yajie Wang,  FeiFei Zuo,  Chengbin Quan,  Youjian Zhao</p>
  <p><b>备注</b>：This paper has been accepted by CVPR 2024</p>
  <p><b>关键词</b>：enhance dental diagnostics, treatment planning, Anthropic Prior Knowledge, oral health, great potential</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Teeth localization, segmentation, and labeling in 2D images have great potential in modern dentistry to enhance dental diagnostics, treatment planning, and population-based studies on oral health. However, general instance segmentation frameworks are incompetent due to 1) the subtle differences between some teeth' shapes (e.g., maxillary first premolar and second premolar), 2) the teeth's position and shape variation across subjects, and 3) the presence of abnormalities in the dentition (e.g., caries and edentulism). To address these problems, we propose a ViT-based framework named TeethSEG, which consists of stacked Multi-Scale Aggregation (MSA) blocks and an Anthropic Prior Knowledge (APK) layer. Specifically, to compose the two modules, we design 1) a unique permutation-based upscaler to ensure high efficiency while establishing clear segmentation boundaries with 2) multi-head self/cross-gating layers to emphasize particular semantics meanwhile maintaining the divergence between token embeddings. Besides, we collect 3) the first open-sourced intraoral image dataset IO150K, which comprises over 150k intraoral photos, and all photos are annotated by orthodontists using a human-machine hybrid algorithm. Experiments on IO150K demonstrate that our TeethSEG outperforms the state-of-the-art segmentation models on dental image segmentation.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：PosterLlama: Bridging Design Ability of Langauge Model to Contents-Aware  Layout Generation</b></summary>
  <p><b>编号</b>：[159]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00995">https://arxiv.org/abs/2404.00995</a></p>
  <p><b>作者</b>：Jaejung Seol,  Seojun Kim,  Jaejun Yoo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：graphic design fields, Visual layout plays, plays a critical, critical role, role in graphic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Visual layout plays a critical role in graphic design fields such as advertising, posters, and web UI design. The recent trend towards content-aware layout generation through generative models has shown promise, yet it often overlooks the semantic intricacies of layout design by treating it as a simple numerical optimization. To bridge this gap, we introduce PosterLlama, a network designed for generating visually and textually coherent layouts by reformatting layout elements into HTML code and leveraging the rich design knowledge embedded within language models. Furthermore, we enhance the robustness of our model with a unique depth-based poster augmentation strategy. This ensures our generated layouts remain semantically rich but also visually appealing, even with limited data. Our extensive evaluations across several benchmarks demonstrate that PosterLlama outperforms existing methods in producing authentic and content-aware layouts. It supports an unparalleled range of conditions, including but not limited to unconditional layout generation, element conditional layout generation, layout completion, among others, serving as a highly versatile user manipulation tool.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：AMOR: Ambiguous Authorship Order</b></summary>
  <p><b>编号</b>：[160]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00994">https://arxiv.org/abs/2404.00994</a></p>
  <p><b>作者</b>：Maximilian Weiherer,  Andreea Dogaru,  Shreya Kapoor,  Hannah Schieber,  Bernhard Egger</p>
  <p><b>备注</b>：SIGBOVIK '24 submission</p>
  <p><b>关键词</b>：writing scientific papers, long nights, long days, writing scientific, remarkable experience</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As we all know, writing scientific papers together with our beloved colleagues is a truly remarkable experience (partially): endless discussions about the same useless paragraph over and over again, followed by long days and long nights -- both at the same time. What a wonderful ride it is! What a beautiful life we have. But wait, there's one tiny little problem that utterly shatters the peace, turning even renowned scientists into bloodthirsty monsters: author order. The reason is that, contrary to widespread opinion, it's not the font size that matters, but the way things are ordered. Of course, this is a fairly well-known fact among scientists all across the planet (and beyond) and explains clearly why we regularly have to read about yet another escalated paper submission in local police reports.
In this paper, we take an important step backwards to tackle this issue by solving the so-called author ordering problem (AOP) once and for all. Specifically, we propose AMOR, a system that replaces silly constructs like co-first or co-middle authorship with a simple yet easy probabilistic approach based on random shuffling of the author list at viewing time. In addition to AOP, we also solve the ambiguous author ordering citation problem} (AAOCP) on the fly. Stop author violence, be human.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：SGCNeRF: Few-Shot Neural Rendering via Sparse Geometric Consistency  Guidance</b></summary>
  <p><b>编号</b>：[161]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00992">https://arxiv.org/abs/2404.00992</a></p>
  <p><b>作者</b>：Yuru Xiao,  Xianming Liu,  Deming Zhai,  Kui Jiang,  Junjun Jiang,  Xiangyang Ji</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Neural Radiance Field, Radiance Field, made significant strides, technology has made, creating novel viewpoints</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural Radiance Field (NeRF) technology has made significant strides in creating novel viewpoints. However, its effectiveness is hampered when working with sparsely available views, often leading to performance dips due to overfitting. FreeNeRF attempts to overcome this limitation by integrating implicit geometry regularization, which incrementally improves both geometry and textures. Nonetheless, an initial low positional encoding bandwidth results in the exclusion of high-frequency elements. The quest for a holistic approach that simultaneously addresses overfitting and the preservation of high-frequency details remains ongoing. This study introduces a novel feature matching based sparse geometry regularization module. This module excels in pinpointing high-frequency keypoints, thereby safeguarding the integrity of fine details. Through progressive refinement of geometry and textures across NeRF iterations, we unveil an effective few-shot neural rendering architecture, designated as SGCNeRF, for enhanced novel view synthesis. Our experiments demonstrate that SGCNeRF not only achieves superior geometry-consistent outcomes but also surpasses FreeNeRF, with improvements of 0.7 dB and 0.6 dB in PSNR on the LLFF and DTU datasets, respectively.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：360+x: A Panoptic Multi-modal Scene Understanding Dataset</b></summary>
  <p><b>编号</b>：[163]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00989">https://arxiv.org/abs/2404.00989</a></p>
  <p><b>作者</b>：Hao Chen,  Yuqi Hou,  Chenyuan Qu,  Irene Testini,  Xiaohan Hong,  Jianbo Jiao</p>
  <p><b>备注</b>：To access the public dataset, please visit this https URL</p>
  <p><b>关键词</b>：Human perception, multiple data modalities, scene, multiple viewpoints, scene understanding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Human perception of the world is shaped by a multitude of viewpoints and modalities. While many existing datasets focus on scene understanding from a certain perspective (e.g. egocentric or third-person views), our dataset offers a panoptic perspective (i.e. multiple viewpoints with multiple data modalities). Specifically, we encapsulate third-person panoramic and front views, as well as egocentric monocular/binocular views with rich modalities including video, multi-channel audio, directional binaural delay, location data and textual scene descriptions within each scene captured, presenting comprehensive observation of the world. Figure 1 offers a glimpse of all 28 scene categories of our 360+x dataset. To the best of our knowledge, this is the first database that covers multiple viewpoints with multiple data modalities to mimic how daily information is accessed in the real world. Through our benchmark analysis, we presented 5 different scene understanding tasks on the proposed 360+x dataset to evaluate the impact and benefit of each data modality and perspective in panoptic scene understanding. We hope this unique dataset could broaden the scope of comprehensive scene understanding and encourage the community to approach these problems from more diverse perspectives.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：FlexiDreamer: Single Image-to-3D Generation with FlexiCubes</b></summary>
  <p><b>编号</b>：[165]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00987">https://arxiv.org/abs/2404.00987</a></p>
  <p><b>作者</b>：Ruowen Zhao,  Zhengyi Wang,  Yikai Wang,  Zihan Zhou,  Jun Zhu</p>
  <p><b>备注</b>：project page:this https URL</p>
  <p><b>关键词</b>：made remarkable progress, speed recently, text prompts, made remarkable, remarkable progress</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>3D content generation from text prompts or single images has made remarkable progress in quality and speed recently. One of its dominant paradigms involves generating consistent multi-view images followed by a sparse-view reconstruction. However, due to the challenge of directly deforming the mesh representation to approach the target topology, most methodologies learn an implicit representation (such as NeRF) during the sparse-view reconstruction and acquire the target mesh by a post-processing extraction. Although the implicit representation can effectively model rich 3D information, its training typically entails a long convergence time. In addition, the post-extraction operation from the implicit field also leads to undesirable visual artifacts. In this paper, we propose FlexiDreamer, a novel single image-to-3d generation framework that reconstructs the target mesh in an end-to-end manner. By leveraging a flexible gradient-based extraction known as FlexiCubes, our method circumvents the defects brought by the post-processing and facilitates a direct acquisition of the target mesh. Furthermore, we incorporate a multi-resolution hash grid encoding scheme that progressively activates the encoding levels into the implicit field in FlexiCubes to help capture geometric details for per-step optimization. Notably, FlexiDreamer recovers a dense 3D structure from a single-view image in approximately 1 minute on a single NVIDIA A100 GPU, outperforming previous methodologies by a large margin.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：Make Continual Learning Stronger via C-Flat</b></summary>
  <p><b>编号</b>：[166]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00986">https://arxiv.org/abs/2404.00986</a></p>
  <p><b>作者</b>：Ang Bian,  Wei Li,  Hangjie Yuan,  Chengrong Yu,  Zixiang Zhao,  Mang Wang,  Aojun Lu,  Tao Feng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：incrementally acquiring dynamically, acquiring dynamically updating, dynamically updating knowledge, sequentially arriving tasks, Model generalization ability</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Model generalization ability upon incrementally acquiring dynamically updating knowledge from sequentially arriving tasks is crucial to tackle the sensitivity-stability dilemma in Continual Learning (CL). Weight loss landscape sharpness minimization seeking for flat minima lying in neighborhoods with uniform low loss or smooth gradient is proven to be a strong training regime improving model generalization compared with loss minimization based optimizer like SGD. Yet only a few works have discussed this training regime for CL, proving that dedicated designed zeroth-order sharpness optimizer can improve CL performance. In this work, we propose a Continual Flatness (C-Flat) method featuring a flatter loss landscape tailored for CL. C-Flat could be easily called with only one line of code and is plug-and-play to any CL methods. A general framework of C-Flat applied to all CL categories and a thorough comparison with loss minima optimizer and flat minima based CL approaches is presented in this paper, showing that our method can boost CL performance in almost all cases. Code will be publicly available upon publication.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：CAMO: Correlation-Aware Mask Optimization with Modulated Reinforcement  Learning</b></summary>
  <p><b>编号</b>：[168]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00980">https://arxiv.org/abs/2404.00980</a></p>
  <p><b>作者</b>：Xiaoxiao Liang,  Haoyu Yang,  Kang Liu,  Bei Yu,  Yuzhe Ma</p>
  <p><b>备注</b>：Accepted by DAC 2024</p>
  <p><b>关键词</b>：modern VLSI manufacturing, Optical proximity correction, VLSI manufacturing, modern VLSI, Optical proximity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Optical proximity correction (OPC) is a vital step to ensure printability in modern VLSI manufacturing. Various OPC approaches based on machine learning have been proposed to pursue performance and efficiency, which are typically data-driven and hardly involve any particular considerations of the OPC problem, leading to potential performance or efficiency bottlenecks. In this paper, we propose CAMO, a reinforcement learning-based OPC system that specifically integrates important principles of the OPC problem. CAMO explicitly involves the spatial correlation among the movements of neighboring segments and an OPC-inspired modulation for movement action selection. Experiments are conducted on both via layer patterns and metal layer patterns. The results demonstrate that CAMO outperforms state-of-the-art OPC engines from both academia and industry.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：PDF: A Probability-Driven Framework for Open World 3D Point Cloud  Semantic Segmentation</b></summary>
  <p><b>编号</b>：[169]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00979">https://arxiv.org/abs/2404.00979</a></p>
  <p><b>作者</b>：Jinfeng Xu,  Siyuan Yang,  Xianzhi Li,  Yuan Tang,  Yixue Hao,  Long Hu,  Min Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：make bad decisions, identify unknown classes, point cloud semantic, world semantic segmentation, Existing point cloud</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing point cloud semantic segmentation networks cannot identify unknown classes and update their knowledge, due to a closed-set and static perspective of the real world, which would induce the intelligent agent to make bad decisions. To address this problem, we propose a Probability-Driven Framework (PDF) for open world semantic segmentation that includes (i) a lightweight U-decoder branch to identify unknown classes by estimating the uncertainties, (ii) a flexible pseudo-labeling scheme to supply geometry features along with probability distribution features of unknown classes by generating pseudo labels, and (iii) an incremental knowledge distillation strategy to incorporate novel classes into the existing knowledge base gradually. Our framework enables the model to behave like human beings, which could recognize unknown objects and incrementally learn them with the corresponding knowledge. Experimental results on the S3DIS and ScanNetv2 datasets demonstrate that the proposed PDF outperforms other methods by a large margin in both important tasks of open world semantic segmentation.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：Improving Visual Recognition with Hyperbolical Visual Hierarchy Mapping</b></summary>
  <p><b>编号</b>：[171]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00974">https://arxiv.org/abs/2404.00974</a></p>
  <p><b>作者</b>：Hyeongjun Kwon,  Jinhyun Jang,  Jin Kim,  Kwonyoung Kim,  Kwanghoon Sohn</p>
  <p><b>备注</b>：This paper is accepted to CVPR 2024. The supplementary material is included. The code is available at \url{this https URL}</p>
  <p><b>关键词</b>：Visual Hierarchy Mapper, visual hierarchy, Deep Neural Networks, fine details, Visual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Visual scenes are naturally organized in a hierarchy, where a coarse semantic is recursively comprised of several fine details. Exploring such a visual hierarchy is crucial to recognize the complex relations of visual elements, leading to a comprehensive scene understanding. In this paper, we propose a Visual Hierarchy Mapper (Hi-Mapper), a novel approach for enhancing the structured understanding of the pre-trained Deep Neural Networks (DNNs). Hi-Mapper investigates the hierarchical organization of the visual scene by 1) pre-defining a hierarchy tree through the encapsulation of probability densities; and 2) learning the hierarchical relations in hyperbolic space with a novel hierarchical contrastive loss. The pre-defined hierarchy tree recursively interacts with the visual features of the pre-trained DNNs through hierarchy decomposition and encoding procedures, thereby effectively identifying the visual hierarchy and enhancing the recognition of an entire scene. Extensive experiments demonstrate that Hi-Mapper significantly enhances the representation capability of DNNs, leading to an improved performance on various tasks, including image classification and dense prediction tasks.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：VideoDistill: Language-aware Vision Distillation for Video Question  Answering</b></summary>
  <p><b>编号</b>：[172]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00973">https://arxiv.org/abs/2404.00973</a></p>
  <p><b>作者</b>：Bo Zou,  Chao Yang,  Yu Qiao,  Chengbin Quan,  Youjian Zhao</p>
  <p><b>备注</b>：This paper is accepted by CVPR2024</p>
  <p><b>关键词</b>：thriving large image-language, large image-language pretraining, image-language pretraining frameworks, video question answering, Significant advancements</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Significant advancements in video question answering (VideoQA) have been made thanks to thriving large image-language pretraining frameworks. Although these image-language models can efficiently represent both video and language branches, they typically employ a goal-free vision perception process and do not interact vision with language well during the answer generation, thus omitting crucial visual cues. In this paper, we are inspired by the human recognition and learning pattern and propose VideoDistill, a framework with language-aware (i.e., goal-driven) behavior in both vision perception and answer generation process. VideoDistill generates answers only from question-related visual embeddings and follows a thinking-observing-answering approach that closely resembles human behavior, distinguishing it from previous research. Specifically, we develop a language-aware gating mechanism to replace the standard cross-attention, avoiding language's direct fusion into visual representations. We incorporate this mechanism into two key components of the entire framework. The first component is a differentiable sparse sampling module, which selects frames containing the necessary dynamics and semantics relevant to the questions. The second component is a vision refinement module that merges existing spatial-temporal attention layers to ensure the extraction of multi-grained visual semantics associated with the questions. We conduct experimental evaluations on various challenging video question-answering benchmarks, and VideoDistill achieves state-of-the-art performance in both general and long-form VideoQA datasets. In Addition, we verify that VideoDistill can effectively alleviate the utilization of language shortcut solutions in the EgoTaskQA dataset.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：S2RC-GCN: A Spatial-Spectral Reliable Contrastive Graph Convolutional  Network for Complex Land Cover Classification Using Hyperspectral Images</b></summary>
  <p><b>编号</b>：[177]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00964">https://arxiv.org/abs/2404.00964</a></p>
  <p><b>作者</b>：Renxiang Guan,  Zihao Li,  Chujia Song,  Guo Yu,  Xianju Li,  Ruyi Feng</p>
  <p><b>备注</b>：Accepted to IJCNN 2024 (International Joint Conference on Neural Networks)</p>
  <p><b>关键词</b>：land cover research, Graph Convolutional Networks, mining land cover, mining land, complex land</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Spatial correlations between different ground objects are an important feature of mining land cover research. Graph Convolutional Networks (GCNs) can effectively capture such spatial feature representations and have demonstrated promising results in performing hyperspectral imagery (HSI) classification tasks of complex land. However, the existing GCN-based HSI classification methods are prone to interference from redundant information when extracting complex features. To classify complex scenes more effectively, this study proposes a novel spatial-spectral reliable contrastive graph convolutional classification framework named S2RC-GCN. Specifically, we fused the spectral and spatial features extracted by the 1D- and 2D-encoder, and the 2D-encoder includes an attention model to automatically extract important information. We then leveraged the fused high-level features to construct graphs and fed the resulting graphs into the GCNs to determine more effective graph representations. Furthermore, a novel reliable contrastive graph convolution was proposed for reliable contrastive learning to learn and fuse robust features. Finally, to test the performance of the model on complex object classification, we used imagery taken by Gaofen-5 in the Jiang Xia area to construct complex land cover datasets. The test results show that compared with other models, our model achieved the best results and effectively improved the classification performance of complex remote sensing imagery.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：Equivariant Local Reference Frames for Unsupervised Non-rigid Point  Cloud Shape Correspondence</b></summary>
  <p><b>编号</b>：[181]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00959">https://arxiv.org/abs/2404.00959</a></p>
  <p><b>作者</b>：Ling Wang,  Runfa Chen,  Yikai Wang,  Fuchun Sun,  Xinzhou Wang,  Sun Kai,  Guangyuan Fu,  Jianwei Zhang,  Wenbing Huang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：cloud shape correspondence, shape correspondence underpins, exponential complexity stemming, Local Reference Frames, Unsupervised non-rigid point</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Unsupervised non-rigid point cloud shape correspondence underpins a multitude of 3D vision tasks, yet itself is non-trivial given the exponential complexity stemming from inter-point degree-of-freedom, i.e., pose transformations. Based on the assumption of local rigidity, one solution for reducing complexity is to decompose the overall shape into independent local regions using Local Reference Frames (LRFs) that are invariant to SE(3) transformations. However, the focus solely on local structure neglects global geometric contexts, resulting in less distinctive LRFs that lack crucial semantic information necessary for effective matching. Furthermore, such complexity introduces out-of-distribution geometric contexts during inference, thus complicating generalization. To this end, we introduce 1) EquiShape, a novel structure tailored to learn pair-wise LRFs with global structural cues for both spatial and semantic consistency, and 2) LRF-Refine, an optimization strategy generally applicable to LRF-based methods, aimed at addressing the generalization challenges. Specifically, for EquiShape, we employ cross-talk within separate equivariant graph neural networks (Cross-GVP) to build long-range dependencies to compensate for the lack of semantic information in local structure modeling, deducing pair-wise independent SE(3)-equivariant LRF vectors for each point. For LRF-Refine, the optimization adjusts LRFs within specific contexts and knowledge, enhancing the geometric and semantic generalizability of point features. Our overall framework surpasses the state-of-the-art methods by a large margin on three benchmarks. Code and models will be publicly available.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：Harnessing The Power of Attention For Patch-Based Biomedical Image  Classification</b></summary>
  <p><b>编号</b>：[184]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00949">https://arxiv.org/abs/2404.00949</a></p>
  <p><b>作者</b>：Gousia Habib,  Shaima Qureshi,  Malik ishfaq</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：innovative architecture rooted, Biomedical image analysis, self-attention mechanisms, innovative architecture, architecture rooted</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Biomedical image analysis can be facilitated by an innovative architecture rooted in self-attention mechanisms. The traditional convolutional neural network (CNN), characterized by fixed-sized windows, needs help capturing intricate spatial and temporal relations at the pixel level. The immutability of CNN filter weights post-training further restricts input fluctuations. Recognizing these limitations, we propose a new paradigm of attention-based models instead of convolutions. As an alternative to traditional CNNs, these models demonstrate robust modelling capabilities and the ability to grasp comprehensive long-range contextual information efficiently. Providing a solution to critical challenges faced by attention-based vision models such as inductive bias, weight sharing, receptive field limitations, and data handling in high resolution, our work combines non-overlapping (vanilla patching) with novel overlapped Shifted Patching Techniques (S.P.T.s) to induce local context that enhances model generalization. Moreover, we examine the novel Lancoz5 interpolation technique, which adapts variable image sizes to higher resolutions. Experimental evidence validates our model's generalization effectiveness, comparing favourably with existing approaches. Attention-based methods are particularly effective with ample data, especially when advanced data augmentation methodologies are integrated to strengthen their robustness.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：Exploring the Efficacy of Group-Normalization in Deep Learning Models  for Alzheimer's Disease Classification</b></summary>
  <p><b>编号</b>：[186]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00946">https://arxiv.org/abs/2404.00946</a></p>
  <p><b>作者</b>：Gousia Habib,  Ishfaq Ahmed Malik,  Jameel Ahmad,  Imtiaz Ahmed,  Shaima Qureshi</p>
  <p><b>备注</b>：19 pages, 3 figures</p>
  <p><b>关键词</b>：Batch Normalization, Group Normalization, Normalization, Batch, Group</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Batch Normalization is an important approach to advancing deep learning since it allows multiple networks to train simultaneously. A problem arises when normalizing along the batch dimension because B.N.'s error increases significantly as batch size shrinks because batch statistics estimates are inaccurate. As a result, computer vision tasks like detection, segmentation, and video, which require tiny batches based on memory consumption, aren't suitable for using Batch Normalization for larger model training and feature transfer. Here, we explore Group Normalization as an easy alternative to using Batch Normalization A Group Normalization is a channel normalization method in which each group is divided into different channels, and the corresponding mean and variance are calculated for each group. Group Normalization computations are accurate across a wide range of batch sizes and are independent of batch size. When trained using a large ImageNet database on ResNet-50, GN achieves a very low error rate of 10.6% compared to Batch Normalization. when a smaller batch size of only 2 is used. For usual batch sizes, the performance of G.N. is comparable to that of Batch Normalization, but at the same time, it outperforms other normalization techniques. Implementing Group Normalization as a direct alternative to B.N to combat the serious challenges faced by the Batch Normalization in deep learning models with comparable or improved classification accuracy. Additionally, Group Normalization can be naturally transferred from the pre-training to the fine-tuning phase. .</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：How Can Large Language Models Enable Better Socially Assistive  Human-Robot Interaction: A Brief Survey</b></summary>
  <p><b>编号</b>：[189]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00938">https://arxiv.org/abs/2404.00938</a></p>
  <p><b>作者</b>：Zhonghao Shi,  Ellen Landrum,  Amy O' Connell,  Mina Kian,  Leticia Pinto-Alva,  Kaleen Shrestha,  Xiaoyuan Zhu,  Maja J Matarić</p>
  <p><b>备注</b>：2 pages, to be submitted to 2024 AAAI Spring Symposium</p>
  <p><b>关键词</b>：autism spectrum disorder, shown great success, providing personalized cognitive-affective, personalized cognitive-affective support, Socially assistive robots</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Socially assistive robots (SARs) have shown great success in providing personalized cognitive-affective support for user populations with special needs such as older adults, children with autism spectrum disorder (ASD), and individuals with mental health challenges. The large body of work on SAR demonstrates its potential to provide at-home support that complements clinic-based interventions delivered by mental health professionals, making these interventions more effective and accessible. However, there are still several major technical challenges that hinder SAR-mediated interactions and interventions from reaching human-level social intelligence and efficacy. With the recent advances in large language models (LLMs), there is an increased potential for novel applications within the field of SAR that can significantly expand the current capabilities of SARs. However, incorporating LLMs introduces new risks and ethical concerns that have not yet been encountered, and must be carefully be addressed to safely deploy these more advanced systems. In this work, we aim to conduct a brief survey on the use of LLMs in SAR technologies, and discuss the potentials and risks of applying LLMs to the following three major technical challenges of SAR: 1) natural language dialog; 2) multimodal understanding; 3) LLMs as robot policies.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：A Comprehensive Review of Knowledge Distillation in Computer Vision</b></summary>
  <p><b>编号</b>：[190]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00936">https://arxiv.org/abs/2404.00936</a></p>
  <p><b>作者</b>：Sheikh Musa Kaleem (1),  Tufail Rouf (1),  Gousia Habib (2),  Tausifa jan Saleem (2),  Brejesh Lall (2) ((1) National Institute of Technology Srinagar, (2) Bharti School of Telecommunication Technology and management Indian Institute of Technology New Delhi, India)</p>
  <p><b>备注</b>：37 pages ,10 figures</p>
  <p><b>关键词</b>：surpass preceding cutting-edge, preceding cutting-edge machine, cutting-edge machine learning, Deep learning, deep learning models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning techniques have been demonstrated to surpass preceding cutting-edge machine learning techniques in recent years, with computer vision being one of the most prominent examples. However, deep learning models suffer from significant drawbacks when deployed in resource-constrained environments due to their large model size and high complexity. Knowledge Distillation is one of the prominent solutions to overcome this challenge. This review paper examines the current state of research on knowledge distillation, a technique for compressing complex models into smaller and simpler ones. The paper provides an overview of the major principles and techniques associated with knowledge distillation and reviews the applications of knowledge distillation in the domain of computer vision. The review focuses on the benefits of knowledge distillation, as well as the problems that must be overcome to improve its effectiveness.</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：GOV-NeSF: Generalizable Open-Vocabulary Neural Semantic Fields</b></summary>
  <p><b>编号</b>：[192]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00931">https://arxiv.org/abs/2404.00931</a></p>
  <p><b>作者</b>：Yunsong Wang,  Hanlin Chen,  Gim Hee Lee</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：vision-language foundation models, significantly enhanced open-vocabulary, Recent advancements, advancements in vision-language, vision-language foundation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advancements in vision-language foundation models have significantly enhanced open-vocabulary 3D scene understanding. However, the generalizability of existing methods is constrained due to their framework designs and their reliance on 3D data. We address this limitation by introducing Generalizable Open-Vocabulary Neural Semantic Fields (GOV-NeSF), a novel approach offering a generalizable implicit representation of 3D scenes with open-vocabulary semantics. We aggregate the geometry-aware features using a cost volume, and propose a Multi-view Joint Fusion module to aggregate multi-view features through a cross-view attention mechanism, which effectively predicts view-specific blending weights for both colors and open-vocabulary features. Remarkably, our GOV-NeSF exhibits state-of-the-art performance in both 2D and 3D open-vocabulary semantic segmentation, eliminating the need for ground truth semantic labels or depth priors, and effectively generalize across scenes and datasets without fine-tuning.</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：Instance-Aware Group Quantization for Vision Transformers</b></summary>
  <p><b>编号</b>：[195]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00928">https://arxiv.org/abs/2404.00928</a></p>
  <p><b>作者</b>：Jaehyeon Moon,  Dohyung Kim,  Junyong Cheon,  Bumsub Ham</p>
  <p><b>备注</b>：CVPR 2024</p>
  <p><b>关键词</b>：small calibration set, efficient model compression, model compression technique, PTQ methods, samples without retraining</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Post-training quantization (PTQ) is an efficient model compression technique that quantizes a pretrained full-precision model using only a small calibration set of unlabeled samples without retraining. PTQ methods for convolutional neural networks (CNNs) provide quantization results comparable to full-precision counterparts. Directly applying them to vision transformers (ViTs), however, incurs severe performance degradation, mainly due to the differences in architectures between CNNs and ViTs. In particular, the distribution of activations for each channel vary drastically according to input instances, making PTQ methods for CNNs inappropriate for ViTs. To address this, we introduce instance-aware group quantization for ViTs (IGQ-ViT). To this end, we propose to split the channels of activation maps into multiple groups dynamically for each input instance, such that activations within each group share similar statistical properties. We also extend our scheme to quantize softmax attentions across tokens. In addition, the number of groups for each layer is adjusted to minimize the discrepancies between predictions from quantized and full-precision models, under a bit-operation (BOP) constraint. We show extensive experimental results on image classification, object detection, and instance segmentation, with various transformer architectures, demonstrating the effectiveness of our approach.</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：LLMs are Good Sign Language Translators</b></summary>
  <p><b>编号</b>：[196]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00925">https://arxiv.org/abs/2404.00925</a></p>
  <p><b>作者</b>：Jia Gong,  Lin Geng Foo,  Yixuan He,  Hossein Rahmani,  Jun Liu</p>
  <p><b>备注</b>：Accepted to CVPR 2024</p>
  <p><b>关键词</b>：Sign Language Translation, translate sign videos, sign videos, challenging task, Sign</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sign Language Translation (SLT) is a challenging task that aims to translate sign videos into spoken language. Inspired by the strong translation capabilities of large language models (LLMs) that are trained on extensive multilingual text corpora, we aim to harness off-the-shelf LLMs to handle SLT. In this paper, we regularize the sign videos to embody linguistic characteristics of spoken language, and propose a novel SignLLM framework to transform sign videos into a language-like representation for improved readability by off-the-shelf LLMs. SignLLM comprises two key modules: (1) The Vector-Quantized Visual Sign module converts sign videos into a sequence of discrete character-level sign tokens, and (2) the Codebook Reconstruction and Alignment module converts these character-level tokens into word-level sign representations using an optimal transport formulation. A sign-text alignment loss further bridges the gap between sign and text tokens, enhancing semantic compatibility. We achieve state-of-the-art gloss-free results on two widely-used SLT benchmarks.</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：BadPart: Unified Black-box Adversarial Patch Attacks against Pixel-wise  Regression Tasks</b></summary>
  <p><b>编号</b>：[197]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00924">https://arxiv.org/abs/2404.00924</a></p>
  <p><b>作者</b>：Zhiyuan Cheng,  Zhaoyi Liu,  Tengda Guo,  Shiwei Feng,  Dongfang Liu,  Mingjie Tang,  Xiangyu Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Pixel-wise regression tasks, optical flow estimation, autonomous driving, augmented reality, video composition</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pixel-wise regression tasks (e.g., monocular depth estimation (MDE) and optical flow estimation (OFE)) have been widely involved in our daily life in applications like autonomous driving, augmented reality and video composition. Although certain applications are security-critical or bear societal significance, the adversarial robustness of such models are not sufficiently studied, especially in the black-box scenario. In this work, we introduce the first unified black-box adversarial patch attack framework against pixel-wise regression tasks, aiming to identify the vulnerabilities of these models under query-based black-box attacks. We propose a novel square-based adversarial patch optimization framework and employ probabilistic square sampling and score-based gradient estimation techniques to generate the patch effectively and efficiently, overcoming the scalability problem of previous black-box patch attacks. Our attack prototype, named BadPart, is evaluated on both MDE and OFE tasks, utilizing a total of 7 models. BadPart surpasses 3 baseline methods in terms of both attack performance and efficiency. We also apply BadPart on the Google online service for portrait depth estimation, causing 43.5% relative distance error with 50K queries. State-of-the-art (SOTA) countermeasures cannot defend our attack effectively.</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,  Depth, and Inertial Measurements</b></summary>
  <p><b>编号</b>：[198]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00923">https://arxiv.org/abs/2404.00923</a></p>
  <p><b>作者</b>：Lisong C. Sun,  Neel P. Bhatt,  Jonathan C. Liu,  Zhiwen Fan,  Zhangyang Wang,  Todd E. Humphreys,  Ufuk Topcu</p>
  <p><b>备注</b>：Project Webpage: this https URL</p>
  <p><b>关键词</b>：Simultaneous localization, Gaussian-based map representations, essential for position, scene understanding, rendering</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Simultaneous localization and mapping is essential for position tracking and scene understanding. 3D Gaussian-based map representations enable photorealistic reconstruction and real-time rendering of scenes using multiple posed cameras. We show for the first time that using 3D Gaussians for map representation with unposed camera images and inertial measurements can enable accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural radiance field-based representations by enabling faster rendering, scale awareness, and improved trajectory tracking. Our framework enables keyframe-based mapping and tracking utilizing loss functions that incorporate relative pose transformations from pre-integrated inertial measurements, depth estimates, and measures of photometric rendering quality. We also release a multi-modal dataset, UT-MM, collected from a mobile robot equipped with a camera and an inertial measurement unit. Experimental evaluation on several scenes from the dataset shows that MM3DGS achieves 3x improvement in tracking and 5% improvement in photometric rendering quality compared to the current 3DGS SLAM state-of-the-art, while allowing real-time rendering of a high-resolution dense 3D map. Project Webpage: this https URL</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：Towards Memorization-Free Diffusion Models</b></summary>
  <p><b>编号</b>：[199]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00922">https://arxiv.org/abs/2404.00922</a></p>
  <p><b>作者</b>：Chen Chen,  Daochang Liu,  Chang Xu</p>
  <p><b>备注</b>：CVPR2024</p>
  <p><b>关键词</b>：widely accessible due, synthesizing high-quality images, open-source nature, widely accessible, accessible due</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pretrained diffusion models and their outputs are widely accessible due to their exceptional capacity for synthesizing high-quality images and their open-source nature. The users, however, may face litigation risks owing to the models' tendency to memorize and regurgitate training data during inference. To address this, we introduce Anti-Memorization Guidance (AMG), a novel framework employing three targeted guidance strategies for the main causes of memorization: image and caption duplication, and highly specific user prompts. Consequently, AMG ensures memorization-free outputs while maintaining high image quality and text alignment, leveraging the synergy of its guidance methods, each indispensable in its own right. AMG also features an innovative automatic detection system for potential memorization during each step of inference process, allows selective application of guidance strategies, minimally interfering with the original sampling process to preserve output utility. We applied AMG to pretrained Denoising Diffusion Probabilistic Models (DDPM) and Stable Diffusion across various generation tasks. The results demonstrate that AMG is the first approach to successfully eradicates all instances of memorization with no or marginal impacts on image quality and text-alignment, as evidenced by FID and CLIP scores.</p>
  </details>
</details>
<details>
  <summary>83. <b>标题：Towards Label-Efficient Human Matting: A Simple Baseline for Weakly  Semi-Supervised Trimap-Free Human Matting</b></summary>
  <p><b>编号</b>：[200]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00921">https://arxiv.org/abs/2404.00921</a></p>
  <p><b>作者</b>：Beomyoung Kim,  Myeong Yeon Yi,  Joonsang Yu,  Young Joon Yoo,  Sung Ju Hwang</p>
  <p><b>备注</b>：Preprint, 15 pages, 13 figures</p>
  <p><b>关键词</b>：demands delicate pixel-level, significantly laborious annotations, delicate pixel-level human, pixel-level human region, human region identification</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a new practical training method for human matting, which demands delicate pixel-level human region identification and significantly laborious annotations. To reduce the annotation cost, most existing matting approaches often rely on image synthesis to augment the dataset. However, the unnaturalness of synthesized training images brings in a new domain generalization challenge for natural images. To address this challenge, we introduce a new learning paradigm, weakly semi-supervised human matting (WSSHM), which leverages a small amount of expensive matte labels and a large amount of budget-friendly segmentation labels, to save the annotation cost and resolve the domain generalization problem. To achieve the goal of WSSHM, we propose a simple and effective training method, named Matte Label Blending (MLB), that selectively guides only the beneficial knowledge of the segmentation and matte data to the matting model. Extensive experiments with our detailed analysis demonstrate our method can substantially improve the robustness of the matting model using a few matte data and numerous segmentation data. Our training method is also easily applicable to real-time models, achieving competitive accuracy with breakneck inference speed (328 FPS on NVIDIA V100 GPU). The implementation code is available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>84. <b>标题：Rethinking Saliency-Guided Weakly-Supervised Semantic Segmentation</b></summary>
  <p><b>编号</b>：[201]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00918">https://arxiv.org/abs/2404.00918</a></p>
  <p><b>作者</b>：Beomyoung Kim,  Donghyeon Kim,  Sung Ju Hwang</p>
  <p><b>备注</b>：Preprint, 17 pages, 7 figures</p>
  <p><b>关键词</b>：weakly-supervised semantic segmentation, research directions based, saliency maps, semantic segmentation, empirical findings</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a fresh perspective on the role of saliency maps in weakly-supervised semantic segmentation (WSSS) and offers new insights and research directions based on our empirical findings. We conduct comprehensive experiments and observe that the quality of the saliency map is a critical factor in saliency-guided WSSS approaches. Nonetheless, we find that the saliency maps used in previous works are often arbitrarily chosen, despite their significant impact on WSSS. Additionally, we observe that the choice of the threshold, which has received less attention before, is non-trivial in WSSS. To facilitate more meaningful and rigorous research for saliency-guided WSSS, we introduce \texttt{WSSS-BED}, a standardized framework for conducting research under unified conditions. \texttt{WSSS-BED} provides various saliency maps and activation maps for seven WSSS methods, as well as saliency maps from unsupervised salient object detection models.</p>
  </details>
</details>
<details>
  <summary>85. <b>标题：Gyro-based Neural Single Image Deblurring</b></summary>
  <p><b>编号</b>：[202]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00916">https://arxiv.org/abs/2404.00916</a></p>
  <p><b>作者</b>：Heemin Yang,  Jaesung Rim,  Seung-Hwan Baek,  Sunghyun Cho</p>
  <p><b>备注</b>：14 pages, 11 figures</p>
  <p><b>关键词</b>：gyro, gyro data, gyro sensor, gyro deblurring block, image deblurring method</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we present GyroDeblurNet, a novel single image deblurring method that utilizes a gyro sensor to effectively resolve the ill-posedness of image deblurring. The gyro sensor provides valuable information about camera motion during exposure time that can significantly improve deblurring quality. However, effectively exploiting real-world gyro data is challenging due to significant errors from various sources including sensor noise, the disparity between the positions of a camera module and a gyro sensor, the absence of translational motion information, and moving objects whose motions cannot be captured by a gyro sensor. To handle gyro error, GyroDeblurNet is equipped with two novel neural network blocks: a gyro refinement block and a gyro deblurring block. The gyro refinement block refines the error-ridden gyro data using the blur information from the input image. On the other hand, the gyro deblurring block removes blur from the input image using the refined gyro data and further compensates for gyro error by leveraging the blur information from the input image. For training a neural network with erroneous gyro data, we propose a training strategy based on the curriculum learning. We also introduce a novel gyro data embedding scheme to represent real-world intricate camera shakes. Finally, we present a synthetic dataset and a real dataset for the training and evaluation of gyro-based single image deblurring. Our experiments demonstrate that our approach achieves state-of-the-art deblurring quality by effectively utilizing erroneous gyro data.</p>
  </details>
</details>
<details>
  <summary>86. <b>标题：Scalable 3D Registration via Truncated Entry-wise Absolute Residuals</b></summary>
  <p><b>编号</b>：[203]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00915">https://arxiv.org/abs/2404.00915</a></p>
  <p><b>作者</b>：Tianyu Huang,  Liangzu Peng,  René Vidal,  Yun-Hui Liu</p>
  <p><b>备注</b>：24 pages, 12 figures. Accepted to CVPR 2024</p>
  <p><b>关键词</b>：input set, rotation and translation, translation that align, point pairs, Entry-wise Absolute Residuals</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Given an input set of $3$D point pairs, the goal of outlier-robust $3$D registration is to compute some rotation and translation that align as many point pairs as possible. This is an important problem in computer vision, for which many highly accurate approaches have been recently proposed. Despite their impressive performance, these approaches lack scalability, often overflowing the $16$GB of memory of a standard laptop to handle roughly $30,000$ point pairs. In this paper, we propose a $3$D registration approach that can process more than ten million ($10^7$) point pairs with over $99\%$ random outliers. Moreover, our method is efficient, entails low memory costs, and maintains high accuracy at the same time. We call our method TEAR, as it involves minimizing an outlier-robust loss that computes Truncated Entry-wise Absolute Residuals. To minimize this loss, we decompose the original $6$-dimensional problem into two subproblems of dimensions $3$ and $2$, respectively, solved in succession to global optimality via a customized branch-and-bound method. While branch-and-bound is often slow and unscalable, this does not apply to TEAR as we propose novel bounding functions that are tight and computationally efficient. Experiments on various datasets are conducted to validate the scalability and efficiency of our method.</p>
  </details>
</details>
<details>
  <summary>87. <b>标题：LLaMA-Excitor: General Instruction Tuning via Indirect Feature  Interaction</b></summary>
  <p><b>编号</b>：[205]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00913">https://arxiv.org/abs/2404.00913</a></p>
  <p><b>作者</b>：Bo Zou,  Chao Yang,  Yu Qiao,  Chengbin Quan,  Youjian Zhao</p>
  <p><b>备注</b>：This paper is accepted by CVPR 2024</p>
  <p><b>关键词</b>：introduce extra modules, introduce extra, sequences to inject, inject new skills, compromise the innate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing methods to fine-tune LLMs, like Adapter, Prefix-tuning, and LoRA, which introduce extra modules or additional input sequences to inject new skills or knowledge, may compromise the innate abilities of LLMs. In this paper, we propose LLaMA-Excitor, a lightweight method that stimulates the LLMs' potential to better follow instructions by gradually paying more attention to worthwhile information. Specifically, the LLaMA-Excitor does not directly change the intermediate hidden state during the self-attention calculation of the transformer structure. We designed the Excitor block as a bypass module for the similarity score computation in LLMs' self-attention to reconstruct keys and change the importance of values by learnable prompts. LLaMA-Excitor ensures a self-adaptive allocation of additional attention to input instructions, thus effectively preserving LLMs' pre-trained knowledge when fine-tuning LLMs on low-quality instruction-following datasets. Furthermore, we unify the modeling of multi-modal tuning and language-only tuning, extending LLaMA-Excitor to a powerful visual instruction follower without the need for complex multi-modal alignment. Our proposed approach is evaluated in language-only and multi-modal tuning experimental scenarios. Notably, LLaMA-Excitor is the only method that maintains basic capabilities while achieving a significant improvement (+6%) on the MMLU benchmark. In the visual instruction tuning, we achieve a new state-of-the-art image captioning performance of 157.5 CIDEr on MSCOCO, and a comparable performance (88.39%) on ScienceQA to cutting-edge models with more parameters and extensive vision-language pertaining.</p>
  </details>
</details>
<details>
  <summary>88. <b>标题：Learning by Correction: Efficient Tuning Task for Zero-Shot Generative  Vision-Language Reasoning</b></summary>
  <p><b>编号</b>：[206]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00909">https://arxiv.org/abs/2404.00909</a></p>
  <p><b>作者</b>：Rongjie Li,  Yu Wu,  Xuming He</p>
  <p><b>备注</b>：Accepted by CVPR2024</p>
  <p><b>关键词</b>：Generative vision-language models, visual question answering, shown impressive performance, Generative vision-language, vision-language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generative vision-language models (VLMs) have shown impressive performance in zero-shot vision-language tasks like image captioning and visual question answering. However, improving their zero-shot reasoning typically requires second-stage instruction tuning, which relies heavily on human-labeled or large language model-generated annotation, incurring high labeling costs. To tackle this challenge, we introduce Image-Conditioned Caption Correction (ICCC), a novel pre-training task designed to enhance VLMs' zero-shot performance without the need for labeled task-aware data. The ICCC task compels VLMs to rectify mismatches between visual and language concepts, thereby enhancing instruction following and text generation conditioned on visual inputs. Leveraging language structure and a lightweight dependency parser, we construct data samples of ICCC task from image-text datasets with low labeling and computation costs. Experimental results on BLIP-2 and InstructBLIP demonstrate significant improvements in zero-shot image-text generation-based VL tasks through ICCC instruction tuning.</p>
  </details>
</details>
<details>
  <summary>89. <b>标题：From Pixels to Graphs: Open-Vocabulary Scene Graph Generation with  Vision-Language Models</b></summary>
  <p><b>编号</b>：[207]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00906">https://arxiv.org/abs/2404.00906</a></p>
  <p><b>作者</b>：Rongjie Li,  Songyang Zhang,  Dahua Lin,  Kai Chen,  Xuming He</p>
  <p><b>备注</b>：Accepted by CVPR 2024</p>
  <p><b>关键词</b>：intermediate graph representation, aims to parse, Scene, SGG, Scene graph</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Scene graph generation (SGG) aims to parse a visual scene into an intermediate graph representation for downstream reasoning tasks. Despite recent advancements, existing methods struggle to generate scene graphs with novel visual relation concepts. To address this challenge, we introduce a new open-vocabulary SGG framework based on sequence generation. Our framework leverages vision-language pre-trained models (VLM) by incorporating an image-to-graph generation paradigm. Specifically, we generate scene graph sequences via image-to-text generation with VLM and then construct scene graphs from these sequences. By doing so, we harness the strong capabilities of VLM for open-vocabulary SGG and seamlessly integrate explicit relational modeling for enhancing the VL tasks. Experimental results demonstrate that our design not only achieves superior performance with an open vocabulary but also enhances downstream vision-language task performance through explicit relation modeling knowledge.</p>
  </details>
</details>
<details>
  <summary>90. <b>标题：Slightly Shift New Classes to Remember Old Classes for Video  Class-Incremental Learning</b></summary>
  <p><b>编号</b>：[211]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00901">https://arxiv.org/abs/2404.00901</a></p>
  <p><b>作者</b>：Jian Jiao,  Yu Dai,  Hefei Mei,  Heqian Qiu,  Chuanyang Gong,  Shiyuan Tang,  Xinpeng Hao,  Hongliang Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：mitigate catastrophic forgetting, Recent video class-incremental, video class-incremental learning, class-incremental learning, learning usually excessively</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent video class-incremental learning usually excessively pursues the accuracy of the newly seen classes and relies on memory sets to mitigate catastrophic forgetting of the old classes. However, limited storage only allows storing a few representative videos. So we propose SNRO, which slightly shifts the features of new classes to remember old classes. Specifically, SNRO contains Examples Sparse(ES) and Early Break(EB). ES decimates at a lower sample rate to build memory sets and uses interpolation to align those sparse frames in the future. By this, SNRO stores more examples under the same memory consumption and forces the model to focus on low-semantic features which are harder to be forgotten. EB terminates the training at a small epoch, preventing the model from overstretching into the high-semantic space of the current task. Experiments on UCF101, HMDB51, and UESTC-MMEA-CL datasets show that SNRO performs better than other approaches while consuming the same memory consumption.</p>
  </details>
</details>
<details>
  <summary>91. <b>标题：Marrying NeRF with Feature Matching for One-step Pose Estimation</b></summary>
  <p><b>编号</b>：[217]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00891">https://arxiv.org/abs/2404.00891</a></p>
  <p><b>作者</b>：Ronghan Chen,  Yang Cong,  Yu Ren</p>
  <p><b>备注</b>：ICRA, 2024. Video this https URL</p>
  <p><b>关键词</b>：CAD model, image-based pose estimation, object-specific training, model nor hours, hours of object-specific</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Given the image collection of an object, we aim at building a real-time image-based pose estimation method, which requires neither its CAD model nor hours of object-specific training. Recent NeRF-based methods provide a promising solution by directly optimizing the pose from pixel loss between rendered and target images. However, during inference, they require long converging time, and suffer from local minima, making them impractical for real-time robot applications. We aim at solving this problem by marrying image matching with NeRF. With 2D matches and depth rendered by NeRF, we directly solve the pose in one step by building 2D-3D correspondences between target and initial view, thus allowing for real-time prediction. Moreover, to improve the accuracy of 2D-3D correspondences, we propose a 3D consistent point mining strategy, which effectively discards unfaithful points reconstruted by NeRF. Moreover, current NeRF-based methods naively optimizing pixel loss fail at occluded images. Thus, we further propose a 2D matches based sampling strategy to preclude the occluded area. Experimental results on representative datasets prove that our method outperforms state-of-the-art methods, and improves inference efficiency by 90x, achieving real-time prediction at 6 FPS.</p>
  </details>
</details>
<details>
  <summary>92. <b>标题：Model-Agnostic Human Preference Inversion in Diffusion Models</b></summary>
  <p><b>编号</b>：[225]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00879">https://arxiv.org/abs/2404.00879</a></p>
  <p><b>作者</b>：Jeeyung Kim,  Ze Wang,  Qiang Qiu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：challenging task due, diffusion models, remains a challenging, challenging task, task due</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Efficient text-to-image generation remains a challenging task due to the high computational costs associated with the multi-step sampling in diffusion models. Although distillation of pre-trained diffusion models has been successful in reducing sampling steps, low-step image generation often falls short in terms of quality. In this study, we propose a novel sampling design to achieve high-quality one-step image generation aligning with human preferences, particularly focusing on exploring the impact of the prior noise distribution. Our approach, Prompt Adaptive Human Preference Inversion (PAHI), optimizes the noise distributions for each prompt based on human preferences without the need for fine-tuning diffusion models. Our experiments showcase that the tailored noise distributions significantly improve image quality with only a marginal increase in computational cost. Our findings underscore the importance of noise optimization and pave the way for efficient and high-quality text-to-image synthesis.</p>
  </details>
</details>
<details>
  <summary>93. <b>标题：TryOn-Adapter: Efficient Fine-Grained Clothing Identity Adaptation for  High-Fidelity Virtual Try-On</b></summary>
  <p><b>编号</b>：[226]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00878">https://arxiv.org/abs/2404.00878</a></p>
  <p><b>作者</b>：Jiazheng Xing,  Chao Xu,  Yijie Qian,  Yang Liu,  Guang Dai,  Baigui Sun,  Yong Liu,  Jingdong Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：specific person seamlessly, Virtual try-on focuses, focuses on adjusting, clothes to fit, fit a specific</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Virtual try-on focuses on adjusting the given clothes to fit a specific person seamlessly while avoiding any distortion of the patterns and textures of the garment. However, the clothing identity uncontrollability and training inefficiency of existing diffusion-based methods, which struggle to maintain the identity even with full parameter training, are significant limitations that hinder the widespread applications. In this work, we propose an effective and efficient framework, termed TryOn-Adapter. Specifically, we first decouple clothing identity into fine-grained factors: style for color and category information, texture for high-frequency details, and structure for smooth spatial adaptive transformation. Our approach utilizes a pre-trained exemplar-based diffusion model as the fundamental network, whose parameters are frozen except for the attention layers. We then customize three lightweight modules (Style Preserving, Texture Highlighting, and Structure Adapting) incorporated with fine-tuning techniques to enable precise and efficient identity control. Meanwhile, we introduce the training-free T-RePaint strategy to further enhance clothing identity preservation while maintaining the realistic try-on effect during the inference. Our experiments demonstrate that our approach achieves state-of-the-art performance on two widely-used benchmarks. Additionally, compared with recent full-tuning diffusion-based methods, we only use about half of their tunable parameters during training. The code will be made publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>94. <b>标题：MGMap: Mask-Guided Learning for Online Vectorized HD Map Construction</b></summary>
  <p><b>编号</b>：[227]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00876">https://arxiv.org/abs/2404.00876</a></p>
  <p><b>作者</b>：Xiaolu Liu,  Song Wang,  Wentong Li,  Ruizi Yang,  Junbo Chen,  Jianke Zhu</p>
  <p><b>备注</b>：18 pages, 11 figures, accepted by CVPR 2024</p>
  <p><b>关键词</b>：online generation tendency, lightweight online generation, reliable road scene, map construction leans, road scene information</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Currently, high-definition (HD) map construction leans towards a lightweight online generation tendency, which aims to preserve timely and reliable road scene information. However, map elements contain strong shape priors. Subtle and sparse annotations make current detection-based frameworks ambiguous in locating relevant feature scopes and cause the loss of detailed structures in prediction. To alleviate these problems, we propose MGMap, a mask-guided approach that effectively highlights the informative regions and achieves precise map element localization by introducing the learned masks. Specifically, MGMap employs learned masks based on the enhanced multi-scale BEV features from two perspectives. At the instance level, we propose the Mask-activated instance (MAI) decoder, which incorporates global instance and structural information into instance queries by the activation of instance masks. At the point level, a novel position-guided mask patch refinement (PG-MPR) module is designed to refine point locations from a finer-grained perspective, enabling the extraction of point-specific patch information. Compared to the baselines, our proposed MGMap achieves a notable improvement of around 10 mAP for different input modalities. Extensive experiments also demonstrate that our approach showcases strong robustness and generalization capabilities. Our code can be found at this https URL.</p>
  </details>
</details>
<details>
  <summary>95. <b>标题：DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable  Primitive Assembly</b></summary>
  <p><b>编号</b>：[228]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00875">https://arxiv.org/abs/2404.00875</a></p>
  <p><b>作者</b>：Fenggen Yu,  Yimin Qian,  Xu Zhang,  Francisca Gil-Ureta,  Brian Jackson,  Eric Bennett,  Hao Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：RGB images capturing, sparse RGB images, differentiable rendering framework, learn structured, sparse RGB</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a differentiable rendering framework to learn structured 3D abstractions in the form of primitive assemblies from sparse RGB images capturing a 3D object. By leveraging differentiable volume rendering, our method does not require 3D supervision. Architecturally, our network follows the general pipeline of an image-conditioned neural radiance field (NeRF) exemplified by pixelNeRF for color prediction. As our core contribution, we introduce differential primitive assembly (DPA) into NeRF to output a 3D occupancy field in place of density prediction, where the predicted occupancies serve as opacity values for volume rendering. Our network, coined DPA-Net, produces a union of convexes, each as an intersection of convex quadric primitives, to approximate the target 3D object, subject to an abstraction loss and a masking loss, both defined in the image space upon volume rendering. With test-time adaptation and additional sampling and loss designs aimed at improving the accuracy and compactness of the obtained assemblies, our method demonstrates superior performance over state-of-the-art alternatives for 3D primitive abstraction from sparse views.</p>
  </details>
</details>
<details>
  <summary>96. <b>标题：DiSR-NeRF: Diffusion-Guided View-Consistent Super-Resolution NeRF</b></summary>
  <p><b>编号</b>：[229]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00874">https://arxiv.org/abs/2404.00874</a></p>
  <p><b>作者</b>：Jie Long Lee,  Chen Li,  Gim Hee Lee</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：diffusion-guided framework, framework for view-consistent, Score Distillation, Renoised Score Distillation, view-consistent super-resolution</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present DiSR-NeRF, a diffusion-guided framework for view-consistent super-resolution (SR) NeRF. Unlike prior works, we circumvent the requirement for high-resolution (HR) reference images by leveraging existing powerful 2D super-resolution models. Nonetheless, independent SR 2D images are often inconsistent across different views. We thus propose Iterative 3D Synchronization (I3DS) to mitigate the inconsistency problem via the inherent multi-view consistency property of NeRF. Specifically, our I3DS alternates between upscaling low-resolution (LR) rendered images with diffusion models, and updating the underlying 3D representation with standard NeRF training. We further introduce Renoised Score Distillation (RSD), a novel score-distillation objective for 2D image resolution. Our RSD combines features from ancestral sampling and Score Distillation Sampling (SDS) to generate sharp images that are also LR-consistent. Qualitative and quantitative results on both synthetic and real-world datasets demonstrate that our DiSR-NeRF can achieve better results on NeRF super-resolution compared with existing works. Code and video results available at the project website.</p>
  </details>
</details>
<details>
  <summary>97. <b>标题：Lipsum-FT: Robust Fine-Tuning of Zero-Shot Models Using Random Text  Guidance</b></summary>
  <p><b>编号</b>：[233]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00860">https://arxiv.org/abs/2404.00860</a></p>
  <p><b>作者</b>：Giung Nam,  Byeongho Heo,  Juho Lee</p>
  <p><b>备注</b>：ICLR 2024</p>
  <p><b>关键词</b>：image classification tasks, achieving competitive performance, Large-scale contrastive vision-language, model achieving competitive, zero-shot model achieving</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large-scale contrastive vision-language pre-trained models provide the zero-shot model achieving competitive performance across a range of image classification tasks without requiring training on downstream data. Recent works have confirmed that while additional fine-tuning of the zero-shot model on the reference data results in enhanced downstream performance, it compromises the model's robustness against distribution shifts. Our investigation begins by examining the conditions required to achieve the goals of robust fine-tuning, employing descriptions based on feature distortion theory and joint energy-based models. Subsequently, we propose a novel robust fine-tuning algorithm, Lipsum-FT, that effectively utilizes the language modeling aspect of the vision-language pre-trained models. Extensive experiments conducted on distribution shift scenarios in DomainNet and ImageNet confirm the superiority of our proposed Lipsum-FT approach over existing robust fine-tuning methods.</p>
  </details>
</details>
<details>
  <summary>98. <b>标题：Meta Episodic learning with Dynamic Task Sampling for CLIP-based Point  Cloud Classification</b></summary>
  <p><b>编号</b>：[235]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00857">https://arxiv.org/abs/2404.00857</a></p>
  <p><b>作者</b>：Shuvozit Ghose,  Yang Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：assigning semantic labels, Point cloud, Point cloud classification, CLIP-based point cloud, cloud data structure</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Point cloud classification refers to the process of assigning semantic labels or categories to individual points within a point cloud data structure. Recent works have explored the extension of pre-trained CLIP to 3D recognition. In this direction, CLIP-based point cloud models like PointCLIP, CLIP2Point have become state-of-the-art methods in the few-shot setup. Although these methods show promising performance for some classes like airplanes, desks, guitars, etc, the performance for some classes like the cup, flower pot, sink, nightstand, etc is still far from satisfactory. This is due to the fact that the adapter of CLIP-based models is trained using randomly sampled N-way K-shot data in the standard supervised learning setup. In this paper, we propose a novel meta-episodic learning framework for CLIP-based point cloud classification, addressing the challenges of limited training examples and sampling unknown classes. Additionally, we introduce dynamic task sampling within the episode based on performance memory. This sampling strategy effectively addresses the challenge of sampling unknown classes, ensuring that the model learns from a diverse range of classes and promotes the exploration of underrepresented categories. By dynamically updating the performance memory, we adaptively prioritize the sampling of classes based on their performance, enhancing the model's ability to handle challenging and real-world scenarios. Experiments show an average performance gain of 3-6\% on ModelNet40 and ScanobjectNN datasets in a few-shot setup.</p>
  </details>
</details>
<details>
  <summary>99. <b>标题：TSOM: Small Object Motion Detection Neural Network Inspired by Avian  Visual Circuit</b></summary>
  <p><b>编号</b>：[237]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00855">https://arxiv.org/abs/2404.00855</a></p>
  <p><b>作者</b>：Pignge Hu,  Xiaoteng Zhang,  Mengmeng Li,  Yingjie Zhu,  Li Shi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：small object motion, Detecting small moving, highly challenging task, machine vision systems, small moving objects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Detecting small moving objects in complex backgrounds from an overhead perspective is a highly challenging task for machine vision systems. As an inspiration from nature, the avian visual system is capable of processing motion information in various complex aerial scenes, and its Retina-OT-Rt visual circuit is highly sensitive to capturing the motion information of small objects from high altitudes. However, more needs to be done on small object motion detection algorithms based on the avian visual system. In this paper, we conducted mathematical modeling based on extensive studies of the biological mechanisms of the Retina-OT-Rt visual circuit. Based on this, we proposed a novel tectum small object motion detection neural network (TSOM). The neural network includes the retina, SGC dendritic, SGC Soma, and Rt layers, each layer corresponding to neurons in the visual pathway. The Retina layer is responsible for accurately projecting input content, the SGC dendritic layer perceives and encodes spatial-temporal information, the SGC Soma layer computes complex motion information and extracts small objects, and the Rt layer integrates and decodes motion information from multiple directions to determine the position of small objects. Extensive experiments on pigeon neurophysiological experiments and image sequence data showed that the TSOM is biologically interpretable and effective in extracting reliable small object motion features from complex high-altitude backgrounds.</p>
  </details>
</details>
<details>
  <summary>100. <b>标题：Ensemble Learning for Vietnamese Scene Text Spotting in Urban  Environments</b></summary>
  <p><b>编号</b>：[238]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00852">https://arxiv.org/abs/2404.00852</a></p>
  <p><b>作者</b>：Hieu Nguyen,  Cong-Hoang Ta,  Phuong-Thuy Le-Nguyen,  Minh-Triet Tran,  Trung-Nghia Le</p>
  <p><b>备注</b>：RIVF 2023</p>
  <p><b>关键词</b>：scene text spotting, Vietnamese scene text, efficient ensemble learning, ensemble learning framework, text spotting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a simple yet efficient ensemble learning framework for Vietnamese scene text spotting. Leveraging the power of ensemble learning, which combines multiple models to yield more accurate predictions, our approach aims to significantly enhance the performance of scene text spotting in challenging urban settings. Through experimental evaluations on the VinText dataset, our proposed method achieves a significant improvement in accuracy compared to existing methods with an impressive accuracy of 5%. These results unequivocally demonstrate the efficacy of ensemble learning in the context of Vietnamese scene text spotting in urban environments, highlighting its potential for real world applications, such as text detection and recognition in urban signage, advertisements, and various text-rich urban scenes.</p>
  </details>
</details>
<details>
  <summary>101. <b>标题：Prompt Learning via Meta-Regularization</b></summary>
  <p><b>编号</b>：[239]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00851">https://arxiv.org/abs/2404.00851</a></p>
  <p><b>作者</b>：Jinyoung Park,  Juyeon Ko,  Hyunwoo J. Kim</p>
  <p><b>备注</b>：CVPR 2024</p>
  <p><b>关键词</b>：shown impressive success, vision-language models, prompt learning, computer vision tasks, prompt learning methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pre-trained vision-language models have shown impressive success on various computer vision tasks with their zero-shot generalizability. Recently, prompt learning approaches have been explored to efficiently and effectively adapt the vision-language models to a variety of downstream tasks. However, most existing prompt learning methods suffer from task overfitting since the general knowledge of the pre-trained vision language models is forgotten while the prompts are finetuned on a small data set from a specific target task. To address this issue, we propose a Prompt Meta-Regularization (ProMetaR) to improve the generalizability of prompt learning for vision-language models. Specifically, ProMetaR meta-learns both the regularizer and the soft prompts to harness the task-specific knowledge from the downstream tasks and task-agnostic general knowledge from the vision-language models. Further, ProMetaR augments the task to generate multiple virtual tasks to alleviate the meta-overfitting. In addition, we provide the analysis to comprehend how ProMetaR improves the generalizability of prompt tuning in the perspective of the gradient alignment. Our extensive experiments demonstrate that our ProMetaR improves the generalizability of conventional prompt learning methods under base-to-base/base-to-new and domain generalization settings. The code of ProMetaR is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>102. <b>标题：Generating Content for HDR Deghosting from Frequency View</b></summary>
  <p><b>编号</b>：[241]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00849">https://arxiv.org/abs/2404.00849</a></p>
  <p><b>作者</b>：Tao Hu,  Qingsen Yan,  Yuankai Qi,  Yanning Zhang</p>
  <p><b>备注</b>：This paper is accepted by CVPR2024</p>
  <p><b>关键词</b>：High Dynamic Range, Low Dynamic Range, multiple Low Dynamic, Recovering ghost-free High, Dynamic Range</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recovering ghost-free High Dynamic Range (HDR) images from multiple Low Dynamic Range (LDR) images becomes challenging when the LDR images exhibit saturation and significant motion. Recent Diffusion Models (DMs) have been introduced in HDR imaging field, demonstrating promising performance, particularly in achieving visually perceptible results compared to previous DNN-based methods. However, DMs require extensive iterations with large models to estimate entire images, resulting in inefficiency that hinders their practical application. To address this challenge, we propose the Low-Frequency aware Diffusion (LF-Diff) model for ghost-free HDR imaging. The key idea of LF-Diff is implementing the DMs in a highly compacted latent space and integrating it into a regression-based model to enhance the details of reconstructed images. Specifically, as low-frequency information is closely related to human visual perception we propose to utilize DMs to create compact low-frequency priors for the reconstruction process. In addition, to take full advantage of the above low-frequency priors, the Dynamic HDR Reconstruction Network (DHRNet) is carried out in a regression-based manner to obtain final HDR images. Extensive experiments conducted on synthetic and real-world benchmark datasets demonstrate that our LF-Diff performs favorably against several state-of-the-art methods and is 10$\times$ faster than previous DM-based methods.</p>
  </details>
</details>
<details>
  <summary>103. <b>标题：Collaborative Learning of Anomalies with Privacy (CLAP) for Unsupervised  Video Anomaly Detection: A New Baseline</b></summary>
  <p><b>编号</b>：[243]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00847">https://arxiv.org/abs/2404.00847</a></p>
  <p><b>作者</b>：Anas Al-lahham,  Muhammad Zaigham Zaheer,  Nurbek Tastan,  Karthik Nandakumar</p>
  <p><b>备注</b>：Accepted in IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR), 2024</p>
  <p><b>关键词</b>：practical real-world applications, popularity recently due, real-world applications, gaining more popularity, popularity recently</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Unsupervised (US) video anomaly detection (VAD) in surveillance applications is gaining more popularity recently due to its practical real-world applications. As surveillance videos are privacy sensitive and the availability of large-scale video data may enable better US-VAD systems, collaborative learning can be highly rewarding in this setting. However, due to the extremely challenging nature of the US-VAD task, where learning is carried out without any annotations, privacy-preserving collaborative learning of US-VAD systems has not been studied yet. In this paper, we propose a new baseline for anomaly detection capable of localizing anomalous events in complex surveillance videos in a fully unsupervised fashion without any labels on a privacy-preserving participant-based distributed training configuration. Additionally, we propose three new evaluation protocols to benchmark anomaly detection approaches on various scenarios of collaborations and data availability. Based on these protocols, we modify existing VAD datasets to extensively evaluate our approach as well as existing US SOTA methods on two large-scale datasets including UCF-Crime and XD-Violence. All proposed evaluation protocols, dataset splits, and codes are available here: this https URL</p>
  </details>
</details>
<details>
  <summary>104. <b>标题：Transfer Learning with Point Transformers</b></summary>
  <p><b>编号</b>：[244]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00846">https://arxiv.org/abs/2404.00846</a></p>
  <p><b>作者</b>：Kartik Gupta,  Rahul Vippala,  Sahima Srivastava</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Point Cloud data, Cloud data, Point Transformers, Point Cloud, MNIST dataset</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Point Transformers are near state-of-the-art models for classification, segmentation, and detection tasks on Point Cloud data. They utilize a self attention based mechanism to model large range spatial dependencies between multiple point sets. In this project we explore two things: classification performance of these attention based networks on ModelNet10 dataset and then, we use the trained model to classify 3D MNIST dataset after finetuning. We also train the model from scratch on 3D MNIST dataset to compare the performance of finetuned and from-scratch model on the MNIST dataset. We observe that since the two datasets have a large difference in the degree of the distributions, transfer learned models do not outperform the from-scratch models in this case. Although we do expect transfer learned models to converge faster since they already know the lower level edges, corners, etc features from the ModelNet10 dataset.</p>
  </details>
</details>
<details>
  <summary>105. <b>标题：An N-Point Linear Solver for Line and Motion Estimation with Event  Cameras</b></summary>
  <p><b>编号</b>：[245]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00842">https://arxiv.org/abs/2404.00842</a></p>
  <p><b>作者</b>：Ling Gao,  Daniel Gehrig,  Hang Su,  Davide Scaramuzza,  Laurent Kneip</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：line-based motion estimation, cameras respond primarily, primarily to edges, formed by strong, strong gradients</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Event cameras respond primarily to edges--formed by strong gradients--and are thus particularly well-suited for line-based motion estimation. Recent work has shown that events generated by a single line each satisfy a polynomial constraint which describes a manifold in the space-time volume. Multiple such constraints can be solved simultaneously to recover the partial linear velocity and line parameters. In this work, we show that, with a suitable line parametrization, this system of constraints is actually linear in the unknowns, which allows us to design a novel linear solver. Unlike existing solvers, our linear solver (i) is fast and numerically stable since it does not rely on expensive root finding, (ii) can solve both minimal and overdetermined systems with more than 5 events, and (iii) admits the characterization of all degenerate cases and multiple solutions. The found line parameters are singularity-free and have a fixed scale, which eliminates the need for auxiliary constraints typically encountered in previous work. To recover the full linear camera velocity we fuse observations from multiple lines with a novel velocity averaging scheme that relies on a geometrically-motivated residual, and thus solves the problem more efficiently than previous schemes which minimize an algebraic residual. Extensive experiments in synthetic and real-world settings demonstrate that our method surpasses the previous work in numerical stability, and operates over 600 times faster.</p>
  </details>
</details>
<details>
  <summary>106. <b>标题：3MOS: Multi-sources, Multi-resolutions, and Multi-scenes dataset for  Optical-SAR image matching</b></summary>
  <p><b>编号</b>：[246]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00838">https://arxiv.org/abs/2404.00838</a></p>
  <p><b>作者</b>：Yibin Ye,  Xichao Teng,  Shuo Chen,  Yijie Bian,  Tao Tan,  Zhang Li</p>
  <p><b>备注</b>：20pages 17 figures</p>
  <p><b>关键词</b>：visual navigation, fundamental task, fusion and visual, Optical-SAR image, Optical-SAR image matching</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Optical-SAR image matching is a fundamental task for image fusion and visual navigation. However, all large-scale open SAR dataset for methods development are collected from single platform, resulting in limited satellite types and spatial resolutions. Since images captured by different sensors vary significantly in both geometric and radiometric appearance, existing methods may fail to match corresponding regions containing the same content. Besides, most of existing datasets have not been categorized based on the characteristics of different scenes. To encourage the design of more general multi-modal image matching methods, we introduce a large-scale Multi-sources,Multi-resolutions, and Multi-scenes dataset for Optical-SAR image matching(3MOS). It consists of 155K optical-SAR image pairs, including SAR data from six commercial satellites, with resolutions ranging from 1.25m to 12.5m. The data has been classified into eight scenes including urban, rural, plains, hills, mountains, water, desert, and frozen earth. Extensively experiments show that none of state-of-the-art methods achieve consistently superior performance across different sources, resolutions and scenes. In addition, the distribution of data has a substantial impact on the matching capability of deep learning models, this proposes the domain adaptation challenge in optical-SAR image matching. Our data and code will be available at:this https URL.</p>
  </details>
</details>
<details>
  <summary>107. <b>标题：Towards Robust Event-guided Low-Light Image Enhancement: A Large-Scale  Real-World Event-Image Dataset and Novel Approach</b></summary>
  <p><b>编号</b>：[248]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00834">https://arxiv.org/abs/2404.00834</a></p>
  <p><b>作者</b>：Guoqiang Liang,  Kanghao Chen,  Hangyu Li,  Yunfan Lu,  Lin Wang</p>
  <p><b>备注</b>：Accepted to CVPR 2024</p>
  <p><b>关键词</b>：high dynamic range, distinct advantages, dynamic range, camera has recently, recently received</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Event camera has recently received much attention for low-light image enhancement (LIE) thanks to their distinct advantages, such as high dynamic range. However, current research is prohibitively restricted by the lack of large-scale, real-world, and spatial-temporally aligned event-image datasets. To this end, we propose a real-world (indoor and outdoor) dataset comprising over 30K pairs of images and events under both low and normal illumination conditions. To achieve this, we utilize a robotic arm that traces a consistent non-linear trajectory to curate the dataset with spatial alignment precision under 0.03mm. We then introduce a matching alignment strategy, rendering 90% of our dataset with errors less than 0.01s. Based on the dataset, we propose a novel event-guided LIE approach, called EvLight, towards robust performance in real-world low-light scenes. Specifically, we first design the multi-scale holistic fusion branch to extract holistic structural and textural information from both events and images. To ensure robustness against variations in the regional illumination and noise, we then introduce a Signal-to-Noise-Ratio (SNR)-guided regional feature selection to selectively fuse features of images from regions with high SNR and enhance those with low SNR by extracting regional structure information from events. Extensive experiments on our dataset and the synthetic SDSD dataset demonstrate our EvLight significantly surpasses the frame-based methods. Code and datasets are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>108. <b>标题：Towards Realistic Scene Generation with LiDAR Diffusion Models</b></summary>
  <p><b>编号</b>：[255]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00815">https://arxiv.org/abs/2404.00815</a></p>
  <p><b>作者</b>：Haoxi Ran,  Vitor Guizilini,  Yue Wang</p>
  <p><b>备注</b>：CVPR 2024. Code available at this https URL</p>
  <p><b>关键词</b>：photo-realistic image synthesis, Diffusion models, LiDAR Diffusion Models, excel in photo-realistic, image synthesis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diffusion models (DMs) excel in photo-realistic image synthesis, but their adaptation to LiDAR scene generation poses a substantial hurdle. This is primarily because DMs operating in the point space struggle to preserve the curve-like patterns and 3D geometry of LiDAR scenes, which consumes much of their representation power. In this paper, we propose LiDAR Diffusion Models (LiDMs) to generate LiDAR-realistic scenes from a latent space tailored to capture the realism of LiDAR scenes by incorporating geometric priors into the learning pipeline. Our method targets three major desiderata: pattern realism, geometry realism, and object realism. Specifically, we introduce curve-wise compression to simulate real-world LiDAR patterns, point-wise coordinate supervision to learn scene geometry, and patch-wise encoding for a full 3D object context. With these three core designs, our method achieves competitive performance on unconditional LiDAR generation in 64-beam scenario and state of the art on conditional LiDAR generation, while maintaining high efficiency compared to point-based DMs (up to 107$\times$ faster). Furthermore, by compressing LiDAR scenes into a latent space, we enable the controllability of DMs with various conditions such as semantic maps, camera views, and text prompts. Our code and pretrained weights are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>109. <b>标题：GAMA-IR: Global Additive Multidimensional Averaging for Fast Image  Restoration</b></summary>
  <p><b>编号</b>：[260]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00807">https://arxiv.org/abs/2404.00807</a></p>
  <p><b>作者</b>：Youssef Mansour,  Reinhard Heckel</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：shown remarkable success, Deep learning-based methods, learning-based methods, methods have shown, shown remarkable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning-based methods have shown remarkable success for various image restoration tasks such as denoising and deblurring. The current state-of-the-art networks are relatively deep and utilize (variants of) self attention mechanisms. Those networks are significantly slower than shallow convolutional networks, which however perform worse. In this paper, we introduce an image restoration network that is both fast and yields excellent image quality. The network is designed to minimize the latency and memory consumption when executed on a standard GPU, while maintaining state-of-the-art performance. The network is a simple shallow network with an efficient block that implements global additive multidimensional averaging operations. This block can capture global information and enable a large receptive field even when used in shallow networks with minimal computational overhead. Through extensive experiments and evaluations on diverse tasks, we demonstrate that our network achieves comparable or even superior results to existing state-of-the-art image restoration networks with less latency. For instance, we exceed the state-of-the-art result on real-world SIDD denoising by 0.11dB, while being 2 to 10 times faster.</p>
  </details>
</details>
<details>
  <summary>110. <b>标题：$R^2$-Tuning: Efficient Image-to-Video Transfer Learning for Video  Temporal Grounding</b></summary>
  <p><b>编号</b>：[261]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00801">https://arxiv.org/abs/2404.00801</a></p>
  <p><b>作者</b>：Ye Liu,  Jixuan He,  Wanhua Li,  Junsik Kim,  Donglai Wei,  Hanspeter Pfister,  Chang Wen Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：natural language queries, ground relevant clips, video understanding problem, language queries, understanding problem</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Video temporal grounding (VTG) is a fine-grained video understanding problem that aims to ground relevant clips in untrimmed videos given natural language queries. Most existing VTG models are built upon frame-wise final-layer CLIP features, aided by additional temporal backbones (e.g., SlowFast) with sophisticated temporal reasoning mechanisms. In this work, we claim that CLIP itself already shows great potential for fine-grained spatial-temporal modeling, as each layer offers distinct yet useful information under different granularity levels. Motivated by this, we propose Reversed Recurrent Tuning ($R^2$-Tuning), a parameter- and memory-efficient transfer learning framework for video temporal grounding. Our method learns a lightweight $R^2$ Block containing only 1.5% of the total parameters to perform progressive spatial-temporal modeling. Starting from the last layer of CLIP, $R^2$ Block recurrently aggregates spatial features from earlier layers, then refines temporal correlation conditioning on the given query, resulting in a coarse-to-fine scheme. $R^2$-Tuning achieves state-of-the-art performance across three VTG tasks (i.e., moment retrieval, highlight detection, and video summarization) on six public benchmarks (i.e., QVHighlights, Charades-STA, Ego4D-NLQ, TACoS, YouTube Highlights, and TVSum) even without the additional backbone, demonstrating the significance and effectiveness of the proposed scheme. Our code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>111. <b>标题：Disentangling Hippocampal Shape Variations: A Study of Neurological  Disorders Using Graph Variational Autoencoder with Contrastive Learning</b></summary>
  <p><b>编号</b>：[271]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00785">https://arxiv.org/abs/2404.00785</a></p>
  <p><b>作者</b>：Jakaria Rabbi,  Johannes Kiechle,  Christian Beaulieu,  Nilanjan Ray,  Dana Cobzas</p>
  <p><b>备注</b>：Length: 23 pages and submitted to the journal: MELBA (Machine Learning for Biomedical Imaging)</p>
  <p><b>关键词</b>：diffusion tensor imaging, Supervised Contrastive Learning, Graph Variational Autoencoder, comprehensive study focused, Supervised Contrastive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a comprehensive study focused on disentangling hippocampal shape variations from diffusion tensor imaging (DTI) datasets within the context of neurological disorders. Leveraging a Graph Variational Autoencoder (VAE) enhanced with Supervised Contrastive Learning, our approach aims to improve interpretability by disentangling two distinct latent variables corresponding to age and the presence of diseases. In our ablation study, we investigate a range of VAE architectures and contrastive loss functions, showcasing the enhanced disentanglement capabilities of our approach. This evaluation uses synthetic 3D torus mesh data and real 3D hippocampal mesh datasets derived from the DTI hippocampal dataset. Our supervised disentanglement model outperforms several state-of-the-art (SOTA) methods like attribute and guided VAEs in terms of disentanglement scores. Our model distinguishes between age groups and disease status in patients with Multiple Sclerosis (MS) using the hippocampus data. Our Graph VAE with Supervised Contrastive Learning shows the volume changes of the hippocampus of MS populations at different ages, and the result is consistent with the current neuroimaging literature. This research provides valuable insights into the relationship between neurological disorder and hippocampal shape changes in different age groups of MS populations using a Graph VAE with Supervised Contrastive loss.</p>
  </details>
</details>
<details>
  <summary>112. <b>标题：Privacy-preserving Optics for Enhancing Protection in Face  De-identification</b></summary>
  <p><b>编号</b>：[274]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00777">https://arxiv.org/abs/2404.00777</a></p>
  <p><b>作者</b>：Jhon Lopez,  Carlos Hinojosa,  Henry Arguello,  Bernard Ghanem</p>
  <p><b>备注</b>：Accepted to CVPR 2024. Project Website and Code coming soon</p>
  <p><b>关键词</b>：camera usage alongside, usage alongside widespread, alongside widespread computer, widespread computer vision, computer vision technology</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The modern surge in camera usage alongside widespread computer vision technology applications poses significant privacy and security concerns. Current artificial intelligence (AI) technologies aid in recognizing relevant events and assisting in daily tasks in homes, offices, hospitals, etc. The need to access or process personal information for these purposes raises privacy concerns. While software-level solutions like face de-identification provide a good privacy/utility trade-off, they present vulnerabilities to sniffing attacks. In this paper, we propose a hardware-level face de-identification method to solve this vulnerability. Specifically, our approach first learns an optical encoder along with a regression model to obtain a face heatmap while hiding the face identity from the source image. We also propose an anonymization framework that generates a new face using the privacy-preserving image, face heatmap, and a reference face image from a public dataset as input. We validate our approach with extensive simulations and hardware experiments.</p>
  </details>
</details>
<details>
  <summary>113. <b>标题：Adapting to Length Shift: FlexiLength Network for Trajectory Prediction</b></summary>
  <p><b>编号</b>：[288]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00742">https://arxiv.org/abs/2404.00742</a></p>
  <p><b>作者</b>：Yi Xu,  Yun Fu</p>
  <p><b>备注</b>：Accepted by CVPR 2024</p>
  <p><b>关键词</b>：including autonomous driving, including autonomous, autonomous driving, scene understanding, plays an important</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Trajectory prediction plays an important role in various applications, including autonomous driving, robotics, and scene understanding. Existing approaches mainly focus on developing compact neural networks to increase prediction precision on public datasets, typically employing a standardized input duration. However, a notable issue arises when these models are evaluated with varying observation lengths, leading to a significant performance drop, a phenomenon we term the Observation Length Shift. To address this issue, we introduce a general and effective framework, the FlexiLength Network (FLN), to enhance the robustness of existing trajectory prediction techniques against varying observation periods. Specifically, FLN integrates trajectory data with diverse observation lengths, incorporates FlexiLength Calibration (FLC) to acquire temporal invariant representations, and employs FlexiLength Adaptation (FLA) to further refine these representations for more accurate future trajectory predictions. Comprehensive experiments on multiple datasets, ie, ETH/UCY, nuScenes, and Argoverse 1, demonstrate the effectiveness and flexibility of our proposed FLN framework.</p>
  </details>
</details>
<details>
  <summary>114. <b>标题：Rethinking Interactive Image Segmentation with Low Latency, High  Quality, and Diverse Prompts</b></summary>
  <p><b>编号</b>：[289]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00741">https://arxiv.org/abs/2404.00741</a></p>
  <p><b>作者</b>：Qin Liu,  Jaemin Cho,  Mohit Bansal,  Marc Niethammer</p>
  <p><b>备注</b>：CVPR 2024 this https URL</p>
  <p><b>关键词</b>：delineate specific regions, models, generalist models, delineate specific, specific regions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The goal of interactive image segmentation is to delineate specific regions within an image via visual or language prompts. Low-latency and high-quality interactive segmentation with diverse prompts remain challenging for existing specialist and generalist models. Specialist models, with their limited prompts and task-specific designs, experience high latency because the image must be recomputed every time the prompt is updated, due to the joint encoding of image and visual prompts. Generalist models, exemplified by the Segment Anything Model (SAM), have recently excelled in prompt diversity and efficiency, lifting image segmentation to the foundation model era. However, for high-quality segmentations, SAM still lags behind state-of-the-art specialist models despite SAM being trained with x100 more segmentation masks. In this work, we delve deep into the architectural differences between the two types of models. We observe that dense representation and fusion of visual prompts are the key design choices contributing to the high segmentation quality of specialist models. In light of this, we reintroduce this dense design into the generalist models, to facilitate the development of generalist models with high segmentation quality. To densely represent diverse visual prompts, we propose to use a dense map to capture five types: clicks, boxes, polygons, scribbles, and masks. Thus, we propose SegNext, a next-generation interactive segmentation approach offering low latency, high quality, and diverse prompt support. Our method outperforms current state-of-the-art methods on HQSeg-44K and DAVIS, both quantitatively and qualitatively.</p>
  </details>
</details>
<details>
  <summary>115. <b>标题：Absolute-Unified Multi-Class Anomaly Detection via Class-Agnostic  Distribution Alignment</b></summary>
  <p><b>编号</b>：[297]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00724">https://arxiv.org/abs/2404.00724</a></p>
  <p><b>作者</b>：Jia Guo,  Shuai Lu,  Weihang Zhang,  Huiqi Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：build separate models, Conventional unsupervised anomaly, methods build separate, Conventional unsupervised, build separate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Conventional unsupervised anomaly detection (UAD) methods build separate models for each object category. Recent studies have proposed to train a unified model for multiple classes, namely model-unified UAD. However, such methods still implement the unified model separately on each class during inference with respective anomaly decision thresholds, which hinders their application when the image categories are entirely unavailable. In this work, we present a simple yet powerful method to address multi-class anomaly detection without any class information, namely \textit{absolute-unified} UAD. We target the crux of prior works in this challenging setting: different objects have mismatched anomaly score distributions. We propose Class-Agnostic Distribution Alignment (CADA) to align the mismatched score distribution of each implicit class without knowing class information, which enables unified anomaly detection for all classes and samples. The essence of CADA is to predict each class's score distribution of normal samples given any image, normal or anomalous, of this class. As a general component, CADA can activate the potential of nearly all UAD methods under absolute-unified setting. Our approach is extensively evaluated under the proposed setting on two popular UAD benchmark datasets, MVTec AD and VisA, where we exceed previous state-of-the-art by a large margin.</p>
  </details>
</details>
<details>
  <summary>116. <b>标题：DRCT: Saving Image Super-resolution away from Information Bottleneck</b></summary>
  <p><b>编号</b>：[298]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00722">https://arxiv.org/abs/2404.00722</a></p>
  <p><b>作者</b>：Chih-Chung Hsu,  Chia-Ming Lee,  Yi-Shiuan Chou</p>
  <p><b>备注</b>：Submitted to NTIRE 2024</p>
  <p><b>关键词</b>：Vision Transformer-based applications, low-level vision tasks, achieved widespread success, Vision Transformer-based, Transformer-based applications</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, Vision Transformer-based applications to low-level vision tasks have achieved widespread success. Unlike CNN-based models, Transformers are more adept at capturing long-range dependencies, enabling the reconstruction of images utilizing information from non-local areas. In the domain of super-resolution, Swin-transformer-based approaches have become mainstream due to their capacity to capture global spatial information and their shifting-window attention mechanism that facilitates the interchange of information between different windows. Many researchers have enhanced image quality and network efficiency by expanding the receptive field or designing complex networks, yielding commendable results. However, we observed that spatial information tends to diminish during the forward propagation process due to increased depth, leading to a loss of spatial information and, consequently, limiting the model's potential. To address this, we propose the Dense-residual-connected Transformer (DRCT), aimed at mitigating the loss of spatial information through dense-residual connections between layers, thereby unleashing the model's potential and enhancing performance. Experiment results indicate that our approach is not only straightforward but also achieves remarkable efficiency, surpassing state-of-the-art methods and performing commendably at NTIRE2024.</p>
  </details>
</details>
<details>
  <summary>117. <b>标题：End-to-End Autonomous Driving through V2X Cooperation</b></summary>
  <p><b>编号</b>：[299]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00717">https://arxiv.org/abs/2404.00717</a></p>
  <p><b>作者</b>：Haibao Yu,  Wenxian Yang,  Jiaru Zhong,  Zhenwei Yang,  Siqi Fan,  Ping Luo,  Zaiqing Nie</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：infrastructure sensor data, Cooperatively utilizing, utilizing both ego-vehicle, ego-vehicle and infrastructure, infrastructure sensor</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cooperatively utilizing both ego-vehicle and infrastructure sensor data via V2X communication has emerged as a promising approach for advanced autonomous driving. However, current research mainly focuses on improving individual modules, rather than taking end-to-end learning to optimize final planning performance, resulting in underutilized data potential. In this paper, we introduce UniV2X, a pioneering cooperative autonomous driving framework that seamlessly integrates all key driving modules across diverse views into a unified network. We propose a sparse-dense hybrid data transmission and fusion mechanism for effective vehicle-infrastructure cooperation, offering three advantages: 1) Effective for simultaneously enhancing agent perception, online mapping, and occupancy prediction, ultimately improving planning performance. 2) Transmission-friendly for practical and limited communication conditions. 3) Reliable data fusion with interpretability of this hybrid data. We implement UniV2X, as well as reproducing several benchmark methods, on the challenging DAIR-V2X, the real-world cooperative driving dataset. Experimental results demonstrate the effectiveness of UniV2X in significantly enhancing planning performance, as well as all intermediate output performance. Code is at this https URL.</p>
  </details>
</details>
<details>
  <summary>118. <b>标题：Neural Radiance Field-based Visual Rendering: A Comprehensive Review</b></summary>
  <p><b>编号</b>：[300]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00714">https://arxiv.org/abs/2404.00714</a></p>
  <p><b>作者</b>：Mingyuan Yao,  Yukang Huo,  Yang Ran,  Qingbin Tian,  Ruifeng Wang,  Haihua Wang</p>
  <p><b>备注</b>：35 pages, 22 figures, 14 tables, 18 formulas</p>
  <p><b>关键词</b>：Neural Radiance Fields, human body reconstruction, providing strong technical, made remarkable progress, strong technical support</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, Neural Radiance Fields (NeRF) has made remarkable progress in the field of computer vision and graphics, providing strong technical support for solving key tasks including 3D scene understanding, new perspective synthesis, human body reconstruction, robotics, and so on, the attention of academics to this research result is growing. As a revolutionary neural implicit field representation, NeRF has caused a continuous research boom in the academic community. Therefore, the purpose of this review is to provide an in-depth analysis of the research literature on NeRF within the past two years, to provide a comprehensive academic perspective for budding researchers. In this paper, the core architecture of NeRF is first elaborated in detail, followed by a discussion of various improvement strategies for NeRF, and case studies of NeRF in diverse application scenarios, demonstrating its practical utility in different domains. In terms of datasets and evaluation metrics, This paper details the key resources needed for NeRF model training. Finally, this paper provides a prospective discussion on the future development trends and potential challenges of NeRF, aiming to provide research inspiration for researchers in the field and to promote the further development of related technologies.</p>
  </details>
</details>
<details>
  <summary>119. <b>标题：Unknown Prompt, the only Lacuna: Unveiling CLIP's Potential for Open  Domain Generalization</b></summary>
  <p><b>编号</b>：[302]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00710">https://arxiv.org/abs/2404.00710</a></p>
  <p><b>作者</b>：Mainak Singha,  Ankit Jha,  Shirsha Bose,  Ashwin Nair,  Moloud Abdar,  Biplab Banerjee</p>
  <p><b>备注</b>：Accepted in CVPR 2024</p>
  <p><b>关键词</b>：training labeled source, Open Domain Generalization, unlabeled target domains, category shifts, shifts between training</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We delve into Open Domain Generalization (ODG), marked by domain and category shifts between training's labeled source and testing's unlabeled target domains. Existing solutions to ODG face limitations due to constrained generalizations of traditional CNN backbones and errors in detecting target open samples in the absence of prior knowledge. Addressing these pitfalls, we introduce ODG-CLIP, harnessing the semantic prowess of the vision-language model, CLIP. Our framework brings forth three primary innovations: Firstly, distinct from prevailing paradigms, we conceptualize ODG as a multi-class classification challenge encompassing both known and novel categories. Central to our approach is modeling a unique prompt tailored for detecting unknown class samples, and to train this, we employ a readily accessible stable diffusion model, elegantly generating proxy images for the open class. Secondly, aiming for domain-tailored classification (prompt) weights while ensuring a balance of precision and simplicity, we devise a novel visual stylecentric prompt learning mechanism. Finally, we infuse images with class-discriminative knowledge derived from the prompt space to augment the fidelity of CLIP's visual embeddings. We introduce a novel objective to safeguard the continuity of this infused semantic intel across domains, especially for the shared classes. Through rigorous testing on diverse datasets, covering closed and open-set DG contexts, ODG-CLIP demonstrates clear supremacy, consistently outpacing peers with performance boosts between 8%-16%. Code will be available at this https URL.</p>
  </details>
</details>
<details>
  <summary>120. <b>标题：Training-Free Semantic Segmentation via LLM-Supervision</b></summary>
  <p><b>编号</b>：[305]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00701">https://arxiv.org/abs/2404.00701</a></p>
  <p><b>作者</b>：Wenfang Sun,  Yingjun Du,  Gaowen Liu,  Ramana Kompella,  Cees G.M. Snoek</p>
  <p><b>备注</b>：22 pages,10 figures, conference</p>
  <p><b>关键词</b>：utilizing natural language, open vocabulary models, notably advanced zero-shot, advanced zero-shot classification, Recent advancements</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advancements in open vocabulary models, like CLIP, have notably advanced zero-shot classification and segmentation by utilizing natural language for class-specific embeddings. However, most research has focused on improving model accuracy through prompt engineering, prompt learning, or fine-tuning with limited labeled data, thereby overlooking the importance of refining the class descriptors. This paper introduces a new approach to text-supervised semantic segmentation using supervision by a large language model (LLM) that does not require extra training. Our method starts from an LLM, like GPT-3, to generate a detailed set of subclasses for more accurate class representation. We then employ an advanced text-supervised semantic segmentation model to apply the generated subclasses as target labels, resulting in diverse segmentation results tailored to each subclass's unique characteristics. Additionally, we propose an assembly that merges the segmentation maps from the various subclass descriptors to ensure a more comprehensive representation of the different aspects in the test images. Through comprehensive experiments on three standard benchmarks, our method outperforms traditional text-supervised semantic segmentation methods by a marked margin.</p>
  </details>
</details>
<details>
  <summary>121. <b>标题：DMSSN: Distilled Mixed Spectral-Spatial Network for Hyperspectral  Salient Object Detection</b></summary>
  <p><b>编号</b>：[308]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00694">https://arxiv.org/abs/2404.00694</a></p>
  <p><b>作者</b>：Haolin Qin,  Tingfa Xu,  Peifu Liu,  Jingxuan Xu,  Jianan Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：approaches fall short, exhibited remarkable promise, conventional RGB-based approaches, RGB-based approaches fall, salient object detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Hyperspectral salient object detection (HSOD) has exhibited remarkable promise across various applications, particularly in intricate scenarios where conventional RGB-based approaches fall short. Despite the considerable progress in HSOD method advancements, two critical challenges require immediate attention. Firstly, existing hyperspectral data dimension reduction techniques incur a loss of spectral information, which adversely affects detection accuracy. Secondly, previous methods insufficiently harness the inherent distinctive attributes of hyperspectral images (HSIs) during the feature extraction process. To address these challenges, we propose a novel approach termed the Distilled Mixed Spectral-Spatial Network (DMSSN), comprising a Distilled Spectral Encoding process and a Mixed Spectral-Spatial Transformer (MSST) feature extraction network. The encoding process utilizes knowledge distillation to construct a lightweight autoencoder for dimension reduction, striking a balance between robust encoding capabilities and low computational costs. The MSST extracts spectral-spatial features through multiple attention head groups, collaboratively enhancing its resistance to intricate scenarios. Moreover, we have created a large-scale HSOD dataset, HSOD-BIT, to tackle the issue of data scarcity in this field and meet the fundamental data requirements of deep network training. Extensive experiments demonstrate that our proposed DMSSN achieves state-of-the-art performance on multiple datasets. We will soon make the code and dataset publicly available on this https URL.</p>
  </details>
</details>
<details>
  <summary>122. <b>标题：Learning to Rank Patches for Unbiased Image Redundancy Reduction</b></summary>
  <p><b>编号</b>：[315]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00680">https://arxiv.org/abs/2404.00680</a></p>
  <p><b>作者</b>：Yang Luo,  Zhineng Chen,  Peng Zhou,  Zuxuan Wu,  Xieping Gao,  Yu-Gang Jiang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：heavy spatial redundancy, spatially correlated, suffer from heavy, heavy spatial, pixels in neighboring</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Images suffer from heavy spatial redundancy because pixels in neighboring regions are spatially correlated. Existing approaches strive to overcome this limitation by reducing less meaningful image regions. However, current leading methods rely on supervisory signals. They may compel models to preserve content that aligns with labeled categories and discard content belonging to unlabeled categories. This categorical inductive bias makes these methods less effective in real-world scenarios. To address this issue, we propose a self-supervised framework for image redundancy reduction called Learning to Rank Patches (LTRP). We observe that image reconstruction of masked image modeling models is sensitive to the removal of visible patches when the masking ratio is high (e.g., 90\%). Building upon it, we implement LTRP via two steps: inferring the semantic density score of each patch by quantifying variation between reconstructions with and without this patch, and learning to rank the patches with the pseudo score. The entire process is self-supervised, thus getting out of the dilemma of categorical inductive bias. We design extensive experiments on different datasets and tasks. The results demonstrate that LTRP outperforms both supervised and other self-supervised methods due to the fair assessment of image content.</p>
  </details>
</details>
<details>
  <summary>123. <b>标题：Weak-to-Strong 3D Object Detection with X-Ray Distillation</b></summary>
  <p><b>编号</b>：[316]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00679">https://arxiv.org/abs/2404.00679</a></p>
  <p><b>作者</b>：Alexander Gambashidze,  Aleksandr Dadukin,  Maksim Golyadkin,  Maria Razzhivina,  Ilya Makarov</p>
  <p><b>备注</b>：Computer Vision and Pattern Recognition 2024</p>
  <p><b>关键词</b>：object detection, Object-Complete Frames, paper addresses, addresses the critical, critical challenges</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper addresses the critical challenges of sparsity and occlusion in LiDAR-based 3D object detection. Current methods often rely on supplementary modules or specific architectural designs, potentially limiting their applicability to new and evolving architectures. To our knowledge, we are the first to propose a versatile technique that seamlessly integrates into any existing framework for 3D Object Detection, marking the first instance of Weak-to-Strong generalization in 3D computer vision. We introduce a novel framework, X-Ray Distillation with Object-Complete Frames, suitable for both supervised and semi-supervised settings, that leverages the temporal aspect of point cloud sequences. This method extracts crucial information from both previous and subsequent LiDAR frames, creating Object-Complete frames that represent objects from multiple viewpoints, thus addressing occlusion and sparsity. Given the limitation of not being able to generate Object-Complete frames during online inference, we utilize Knowledge Distillation within a Teacher-Student framework. This technique encourages the strong Student model to emulate the behavior of the weaker Teacher, which processes simple and informative Object-Complete frames, effectively offering a comprehensive view of objects as if seen through X-ray vision. Our proposed methods surpass state-of-the-art in semi-supervised learning by 1-1.5 mAP and enhance the performance of five established supervised models by 1-2 mAP on standard autonomous driving datasets, even with default hyperparameters. Code for Object-Complete frames is available here: this https URL.</p>
  </details>
</details>
<details>
  <summary>124. <b>标题：OmniSDF: Scene Reconstruction using Omnidirectional Signed Distance  Functions and Adaptive Binoctrees</b></summary>
  <p><b>编号</b>：[317]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00678">https://arxiv.org/abs/2404.00678</a></p>
  <p><b>作者</b>：Hakyeong Kim,  Andreas Meuleman,  Hyeonjoong Jang,  James Tompkin,  Min H. Kim</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：omnidirectional video moving, small circular sweep, outdoor static scene, circular sweep, reconstruct indoor</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a method to reconstruct indoor and outdoor static scene geometry and appearance from an omnidirectional video moving in a small circular sweep. This setting is challenging because of the small baseline and large depth ranges, making it difficult to find ray crossings. To better constrain the optimization, we estimate geometry as a signed distance field within a spherical binoctree data structure and use a complementary efficient tree traversal strategy based on a breadth-first search for sampling. Unlike regular grids or trees, the shape of this structure well-matches the camera setting, creating a better memory-quality trade-off. From an initial depth estimate, the binoctree is adaptively subdivided throughout the optimization; previous methods use a fixed depth that leaves the scene undersampled. In comparison with three neural optimization methods and two non-neural methods, ours shows decreased geometry error on average, especially in a detailed scene, while significantly reducing the required number of voxels to represent such details.</p>
  </details>
</details>
<details>
  <summary>125. <b>标题：OmniLocalRF: Omnidirectional Local Radiance Fields from Dynamic Videos</b></summary>
  <p><b>编号</b>：[318]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00676">https://arxiv.org/abs/2404.00676</a></p>
  <p><b>作者</b>：Dongyoung Choi,  Hyeonjoong Jang,  Min H. Kim</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Local Radiance Fields, wide field, cameras are extensively, applications to provide, Radiance Fields</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Omnidirectional cameras are extensively used in various applications to provide a wide field of vision. However, they face a challenge in synthesizing novel views due to the inevitable presence of dynamic objects, including the photographer, in their wide field of view. In this paper, we introduce a new approach called Omnidirectional Local Radiance Fields (OmniLocalRF) that can render static-only scene views, removing and inpainting dynamic objects simultaneously. Our approach combines the principles of local radiance fields with the bidirectional optimization of omnidirectional rays. Our input is an omnidirectional video, and we evaluate the mutual observations of the entire angle between the previous and current frames. To reduce ghosting artifacts of dynamic objects and inpaint occlusions, we devise a multi-resolution motion mask prediction module. Unlike existing methods that primarily separate dynamic components through the temporal domain, our method uses multi-resolution neural feature planes for precise segmentation, which is more suitable for long 360-degree videos. Our experiments validate that OmniLocalRF outperforms existing methods in both qualitative and quantitative metrics, especially in scenarios with complex real-world scenes. In particular, our approach eliminates the need for manual interaction, such as drawing motion masks by hand and additional pose estimation, making it a highly effective and efficient solution.</p>
  </details>
</details>
<details>
  <summary>126. <b>标题：LLM meets Vision-Language Models for Zero-Shot One-Class Classification</b></summary>
  <p><b>编号</b>：[319]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00675">https://arxiv.org/abs/2404.00675</a></p>
  <p><b>作者</b>：Yassir Bendou,  Giulia Lioi,  Bastien Pasdeloup,  Lukas Mauch,  Ghouthi Boukli Hacene,  Fabien Cardinaux,  Vincent Gripon</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：zero-shot one-class visual, one-class visual classification, problem of zero-shot, zero-shot one-class, one-class visual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider the problem of zero-shot one-class visual classification. In this setting, only the label of the target class is available, and the goal is to discriminate between positive and negative query samples without requiring any validation example from the target task. We propose a two-step solution that first queries large language models for visually confusing objects and then relies on vision-language pre-trained models (e.g., CLIP) to perform classification. By adapting large-scale vision benchmarks, we demonstrate the ability of the proposed method to outperform adapted off-the-shelf alternatives in this setting. Namely, we propose a realistic benchmark where negative query samples are drawn from the same original dataset as positive ones, including a granularity-controlled version of iNaturalist, where negative samples are at a fixed distance in the taxonomy tree from the positive ones. Our work shows that it is possible to discriminate between a single category and other semantically related ones using only its label</p>
  </details>
</details>
<details>
  <summary>127. <b>标题：Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated  Objects</b></summary>
  <p><b>编号</b>：[320]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00674">https://arxiv.org/abs/2404.00674</a></p>
  <p><b>作者</b>：Wenxiao Cai,  Xinyue Leiınst,  Xinyu He,  Junming Leo Chen,  Yangang Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：arbitrary perspectives, challenging problem, problem with applications, NeRF, dynamic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present Knowledge NeRF to synthesize novel views for dynamic scenes.Reconstructing dynamic 3D scenes from few sparse views and rendering them from arbitrary perspectives is a challenging problem with applications in various domains. Previous dynamic NeRF methods learn the deformation of articulated objects from monocular videos. However, qualities of their reconstructed scenes are this http URL clearly reconstruct dynamic scenes, we propose a new framework by considering two frames at a time.We pretrain a NeRF model for an articulated object.When articulated objects moves, Knowledge NeRF learns to generate novel views at the new state by incorporating past knowledge in the pretrained NeRF model with minimal observations in the present state. We propose a projection module to adapt NeRF for dynamic scenes, learning the correspondence between pretrained knowledge base and current states. Experimental results demonstrate the effectiveness of our method in reconstructing dynamic 3D scenes with 5 input images in one state. Knowledge NeRF is a new pipeline and promising solution for novel view synthesis in dynamic articulated objects. The data and implementation are publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>128. <b>标题：A General and Efficient Training for Transformer via Token Expansion</b></summary>
  <p><b>编号</b>：[322]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00672">https://arxiv.org/abs/2404.00672</a></p>
  <p><b>作者</b>：Wenxuan Huang,  Yunhang Shen,  Jiao Xie,  Baochang Zhang,  Gaoqi He,  Ke Li,  Xing Sun,  Shaohui Lin</p>
  <p><b>备注</b>：Accepted to CVPR 2024. Code is available at this https URL</p>
  <p><b>关键词</b>：large training cost, extremely large training, Vision Transformers, training, requires an extremely</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The remarkable performance of Vision Transformers (ViTs) typically requires an extremely large training cost. Existing methods have attempted to accelerate the training of ViTs, yet typically disregard method universality with accuracy dropping. Meanwhile, they break the training consistency of the original transformers, including the consistency of hyper-parameters, architecture, and strategy, which prevents them from being widely applied to different Transformer networks. In this paper, we propose a novel token growth scheme Token Expansion (termed ToE) to achieve consistent training acceleration for ViTs. We introduce an "initialization-expansion-merging" pipeline to maintain the integrity of the intermediate feature distribution of original transformers, preventing the loss of crucial learnable information in the training process. ToE can not only be seamlessly integrated into the training and fine-tuning process of transformers (e.g., DeiT and LV-ViT), but also effective for efficient training frameworks (e.g., EfficientTrain), without twisting the original training hyper-parameters, architecture, and introducing additional training strategies. Extensive experiments demonstrate that ToE achieves about 1.3x faster for the training of ViTs in a lossless manner, or even with performance gains over the full-token training baselines. Code is available at this https URL .</p>
  </details>
</details>
<details>
  <summary>129. <b>标题：Statistical Analysis by Semiparametric Additive Regression and LSTM-FCN  Based Hierarchical Classification for Computer Vision Quantification of  Parkinsonian Bradykinesia</b></summary>
  <p><b>编号</b>：[323]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00670">https://arxiv.org/abs/2404.00670</a></p>
  <p><b>作者</b>：Youngseo Cho,  In Hee Kwak,  Dohyeon Kim,  Jinhee Na,  Hanjoo Sung,  Jeongjae Lee,  Young Eun Kim,  Hyeo-il Ma</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Parkinson Disease, symptom of Parkinson, characterized by involuntary, clinical diagnosis, involuntary slowing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Bradykinesia, characterized by involuntary slowing or decrement of movement, is a fundamental symptom of Parkinson's Disease (PD) and is vital for its clinical diagnosis. Despite various methodologies explored to quantify bradykinesia, computer vision-based approaches have shown promising results. However, these methods often fall short in adequately addressing key bradykinesia characteristics in repetitive limb movements: "occasional arrest" and "decrement in amplitude."
This research advances vision-based quantification of bradykinesia by introducing nuanced numerical analysis to capture decrement in amplitudes and employing a simple deep learning technique, LSTM-FCN, for precise classification of occasional arrests. Our approach structures the classification process hierarchically, tailoring it to the unique dynamics of bradykinesia in PD.
Statistical analysis of the extracted features, including those representing arrest and fatigue, has demonstrated their statistical significance in most cases. This finding underscores the importance of considering "occasional arrest" and "decrement in amplitude" in bradykinesia quantification of limb movement. Our enhanced diagnostic tool has been rigorously tested on an extensive dataset comprising 1396 motion videos from 310 PD patients, achieving an accuracy of 80.3%. The results confirm the robustness and reliability of our method.</p>
  </details>
</details>
<details>
  <summary>130. <b>标题：Weakly-Supervised Cross-Domain Segmentation of Electron Microscopy with  Sparse Point Annotation</b></summary>
  <p><b>编号</b>：[324]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00667">https://arxiv.org/abs/2404.00667</a></p>
  <p><b>作者</b>：Dafei Qiu,  Shan Xiong,  Jiajin Yi,  Jialin Peng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：electron microscopy, neuroscience researches, plays an essential, essential role, images plays</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Accurate segmentation of organelle instances from electron microscopy (EM) images plays an essential role in many neuroscience researches. However, practical scenarios usually suffer from high annotation costs, label scarcity, and large domain diversity. While unsupervised domain adaptation (UDA) that assumes no annotation effort on the target data is promising to alleviate these challenges, its performance on complicated segmentation tasks is still far from practical usage. To address these issues, we investigate a highly annotation-efficient weak supervision, which assumes only sparse center-points on a small subset of object instances in the target training images. To achieve accurate segmentation with partial point annotations, we introduce instance counting and center detection as auxiliary tasks and design a multitask learning framework to leverage correlations among the counting, detection, and segmentation, which are all tasks with partial or no supervision. Building upon the different domain-invariances of the three tasks, we enforce counting estimation with a novel soft consistency loss as a global prior for center detection, which further guides the per-pixel segmentation. To further compensate for annotation sparsity, we develop a cross-position cut-and-paste for label augmentation and an entropy-based pseudo-label selection. The experimental results highlight that, by simply using extremely weak annotation, e.g., 15\% sparse points, for model training, the proposed model is capable of significantly outperforming UDA methods and produces comparable performance as the supervised counterpart. The high robustness of our model shown in the validations and the low requirement of expert knowledge for sparse point annotation further improve the potential application value of our model.</p>
  </details>
</details>
<details>
  <summary>131. <b>标题：DeeDSR: Towards Real-World Image Super-Resolution via Degradation-Aware  Stable Diffusion</b></summary>
  <p><b>编号</b>：[327]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00661">https://arxiv.org/abs/2404.00661</a></p>
  <p><b>作者</b>：Chunyang Bi,  Xin Luo,  Sheng Shen,  Mengxi Zhang,  Huanjing Yue,  Jingyu Yang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：powerful generative capabilities, real-world super-resolution challenges, addressing real-world super-resolution, generative capabilities, play a crucial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diffusion models, known for their powerful generative capabilities, play a crucial role in addressing real-world super-resolution challenges. However, these models often focus on improving local textures while neglecting the impacts of global degradation, which can significantly reduce semantic fidelity and lead to inaccurate reconstructions and suboptimal super-resolution performance. To address this issue, we introduce a novel two-stage, degradation-aware framework that enhances the diffusion model's ability to recognize content and degradation in low-resolution images. In the first stage, we employ unsupervised contrastive learning to obtain representations of image degradations. In the second stage, we integrate a degradation-aware module into a simplified ControlNet, enabling flexible adaptation to various degradations based on the learned representations. Furthermore, we decompose the degradation-aware features into global semantics and local details branches, which are then injected into the diffusion denoising module to modulate the target generation. Our method effectively recovers semantically precise and photorealistic details, particularly under significant degradation conditions, demonstrating state-of-the-art performance across various benchmarks. Codes will be released at this https URL.</p>
  </details>
</details>
<details>
  <summary>132. <b>标题：KTPFormer: Kinematics and Trajectory Prior Knowledge-Enhanced  Transformer for 3D Human Pose Estimation</b></summary>
  <p><b>编号</b>：[328]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00658">https://arxiv.org/abs/2404.00658</a></p>
  <p><b>作者</b>：Jihua Peng,  Yanghong Zhou,  P.Y. Mok</p>
  <p><b>备注</b>：Accepted by CVPR2024</p>
  <p><b>关键词</b>：Prior Knowledge-Enhanced Transformer, Trajectory Prior Attention, simple linear mapping, Trajectory Prior Knowledge-Enhanced, Kinematics Prior Attention</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a novel Kinematics and Trajectory Prior Knowledge-Enhanced Transformer (KTPFormer), which overcomes the weakness in existing transformer-based methods for 3D human pose estimation that the derivation of Q, K, V vectors in their self-attention mechanisms are all based on simple linear mapping. We propose two prior attention modules, namely Kinematics Prior Attention (KPA) and Trajectory Prior Attention (TPA) to take advantage of the known anatomical structure of the human body and motion trajectory information, to facilitate effective learning of global dependencies and features in the multi-head self-attention. KPA models kinematic relationships in the human body by constructing a topology of kinematics, while TPA builds a trajectory topology to learn the information of joint motion trajectory across frames. Yielding Q, K, V vectors with prior knowledge, the two modules enable KTPFormer to model both spatial and temporal correlations simultaneously. Extensive experiments on three benchmarks (Human3.6M, MPI-INF-3DHP and HumanEva) show that KTPFormer achieves superior performance in comparison to state-of-the-art methods. More importantly, our KPA and TPA modules have lightweight plug-and-play designs and can be integrated into various transformer-based networks (i.e., diffusion-based) to improve the performance with only a very small increase in the computational overhead. The code is available at: this https URL.</p>
  </details>
</details>
<details>
  <summary>133. <b>标题：Dual DETRs for Multi-Label Temporal Action Detection</b></summary>
  <p><b>编号</b>：[332]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00653">https://arxiv.org/abs/2404.00653</a></p>
  <p><b>作者</b>：Yuhan Zhu,  Guozhen Zhang,  Jing Tan,  Gangshan Wu,  Limin Wang</p>
  <p><b>备注</b>：Accepted by CVPR 2024</p>
  <p><b>关键词</b>：Temporal Action Detection, untrimmed videos, category within untrimmed, TAD, Dual-level query-based TAD</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Temporal Action Detection (TAD) aims to identify the action boundaries and the corresponding category within untrimmed videos. Inspired by the success of DETR in object detection, several methods have adapted the query-based framework to the TAD task. However, these approaches primarily followed DETR to predict actions at the instance level (i.e., identify each action by its center point), leading to sub-optimal boundary localization. To address this issue, we propose a new Dual-level query-based TAD framework, namely DualDETR, to detect actions from both instance-level and boundary-level. Decoding at different levels requires semantics of different granularity, therefore we introduce a two-branch decoding structure. This structure builds distinctive decoding processes for different levels, facilitating explicit capture of temporal cues and semantics at each level. On top of the two-branch design, we present a joint query initialization strategy to align queries from both levels. Specifically, we leverage encoder proposals to match queries from each level in a one-to-one manner. Then, the matched queries are initialized using position and content prior from the matched action proposal. The aligned dual-level queries can refine the matched proposal with complementary cues during subsequent decoding. We evaluate DualDETR on three challenging multi-label TAD benchmarks. The experimental results demonstrate the superior performance of DualDETR to the existing state-of-the-art methods, achieving a substantial improvement under det-mAP and delivering impressive results under seg-mAP.</p>
  </details>
</details>
<details>
  <summary>134. <b>标题：Deep Instruction Tuning for Segment Anything Model</b></summary>
  <p><b>编号</b>：[334]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00650">https://arxiv.org/abs/2404.00650</a></p>
  <p><b>作者</b>：Xiaorui Huang,  Gen Luo,  Chaoyang Zhu,  Bo Tong,  Yiyi Zhou,  Xiaoshuai Sun,  Rongrong Ji</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Segment Anything Model, exhibits powerful, segmentation tasks recently, powerful yet versatile, versatile capabilities</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Segment Anything Model (SAM) exhibits powerful yet versatile capabilities on (un) conditional image segmentation tasks recently. Although SAM can support various segmentation prompts, we note that, compared to point- and box-guided segmentation, it performs much worse on text-instructed tasks. We argue that deep text instruction tuning is key to mitigate such shortcoming caused by the shallow fusion scheme in its default light-weight mask decoder. In this paper, two \emph{deep instruction tuning} (DIT) methods are proposed, one is end-to-end and the other is layer-wise. With these tuning methods, we can regard the image encoder of SAM as a stand-alone vision-language learner in contrast to building another deep fusion branch. Extensive experiments on three highly competitive benchmark datasets of referring image segmentation show that a simple end-to-end DIT improves SAM by a large margin, with layer-wise DIT further boosts the performance to state-of-the-art. Our code is anonymously released at: this https URL.</p>
  </details>
</details>
<details>
  <summary>135. <b>标题：SpiralMLP: A Lightweight Vision MLP Architecture</b></summary>
  <p><b>编号</b>：[335]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00648">https://arxiv.org/abs/2404.00648</a></p>
  <p><b>作者</b>：Haojie Mu,  Burhan Ul Tayyab,  Nicholas Chua</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：conventional Token Mixing, Token Mixing approach, Token Mixing, conventional Token, architecture that introduces</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present SpiralMLP, a novel architecture that introduces a Spiral FC layer as a replacement for the conventional Token Mixing approach. Differing from several existing MLP-based models that primarily emphasize axes, our Spiral FC layer is designed as a deformable convolution layer with spiral-like offsets. We further adapt Spiral FC into two variants: Self-Spiral FC and Cross-Spiral FC, which enable both local and global feature integration seamlessly, eliminating the need for additional processing steps. To thoroughly investigate the effectiveness of the spiral-like offsets and validate our design, we conduct ablation studies and explore optimal configurations. In empirical tests, SpiralMLP reaches state-of-the-art performance, similar to Transformers, CNNs, and other MLPs, benchmarking on ImageNet-1k, COCO and ADE20K. SpiralMLP still maintains linear computational complexity O(HW) and is compatible with varying input image resolutions. Our study reveals that targeting the full receptive field is not essential for achieving high performance, instead, adopting a refined approach offers better results.</p>
  </details>
</details>
<details>
  <summary>136. <b>标题：Attire-Based Anomaly Detection in Restricted Areas Using YOLOv8 for  Enhanced CCTV Security</b></summary>
  <p><b>编号</b>：[336]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00645">https://arxiv.org/abs/2404.00645</a></p>
  <p><b>作者</b>：Abdul Aziz A.B,  Aindri Bajpai</p>
  <p><b>备注</b>：9 pages, 6 figures</p>
  <p><b>关键词</b>：security enhancement approach, innovative security enhancement, enhancement approach, introduces an innovative, employing advanced image</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This research introduces an innovative security enhancement approach, employing advanced image analysis and soft computing. The focus is on an intelligent surveillance system that detects unauthorized individuals in restricted areas by analyzing attire. Traditional security measures face challenges in monitoring unauthorized access. Leveraging YOLOv8, an advanced object detection algorithm, our system identifies authorized personnel based on their attire in CCTV footage. The methodology involves training the YOLOv8 model on a comprehensive dataset of uniform patterns, ensuring precise recognition in specific regions. Soft computing techniques enhance adaptability to dynamic environments and varying lighting conditions. This research contributes to image analysis and soft computing, providing a sophisticated security solution. Emphasizing uniform-based anomaly detection, it establishes a foundation for robust security systems in restricted areas. The outcomes highlight the potential of YOLOv8-based surveillance in ensuring safety in sensitive locations.</p>
  </details>
</details>
<details>
  <summary>137. <b>标题：Learning to Generate Conditional Tri-plane for 3D-aware Expression  Controllable Portrait Animation</b></summary>
  <p><b>编号</b>：[341]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00636">https://arxiv.org/abs/2404.00636</a></p>
  <p><b>作者</b>：Taekyung Ki,  Dongchan Min,  Gyeongsu Chae</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：control the facial, expression, image, camera view, portrait animation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we present Export3D, a one-shot 3D-aware portrait animation method that is able to control the facial expression and camera view of a given portrait image. To achieve this, we introduce a tri-plane generator that directly generates a tri-plane of 3D prior by transferring the expression parameter of 3DMM into the source image. The tri-plane is then decoded into the image of different view through a differentiable volume rendering. Existing portrait animation methods heavily rely on image warping to transfer the expression in the motion space, challenging on disentanglement of appearance and expression. In contrast, we propose a contrastive pre-training framework for appearance-free expression parameter, eliminating undesirable appearance swap when transferring a cross-identity expression. Extensive experiments show that our pre-training framework can learn the appearance-free expression representation hidden in 3DMM, and our model can generate 3D-aware expression controllable portrait image without appearance swap in the cross-identity manner.</p>
  </details>
</details>
<details>
  <summary>138. <b>标题：IPT-V2: Efficient Image Processing Transformer using Hierarchical  Attentions</b></summary>
  <p><b>编号</b>：[343]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00633">https://arxiv.org/abs/2404.00633</a></p>
  <p><b>作者</b>：Zhijun Tu,  Kunpeng Du,  Hanting Chen,  Hailing Wang,  Wei Li,  Jie Hu,  Yunhe Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Recent advances, advances have demonstrated, demonstrated the powerful, transformer architecture, image restoration</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advances have demonstrated the powerful capability of transformer architecture in image restoration. However, our analysis indicates that existing transformerbased methods can not establish both exact global and local dependencies simultaneously, which are much critical to restore the details and missing content of degraded images. To this end, we present an efficient image processing transformer architecture with hierarchical attentions, called IPTV2, adopting a focal context self-attention (FCSA) and a global grid self-attention (GGSA) to obtain adequate token interactions in local and global receptive fields. Specifically, FCSA applies the shifted window mechanism into the channel self-attention, helps capture the local context and mutual interaction across channels. And GGSA constructs long-range dependencies in the cross-window grid, aggregates global information in spatial dimension. Moreover, we introduce structural re-parameterization technique to feed-forward network to further improve the model capability. Extensive experiments demonstrate that our proposed IPT-V2 achieves state-of-the-art results on various image processing tasks, covering denoising, deblurring, deraining and obtains much better trade-off for performance and computational complexity than previous methods. Besides, we extend our method to image generation as latent diffusion backbone, and significantly outperforms DiTs.</p>
  </details>
</details>
<details>
  <summary>139. <b>标题：Domain Generalizable Person Search Using Unreal Dataset</b></summary>
  <p><b>编号</b>：[347]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00626">https://arxiv.org/abs/2404.00626</a></p>
  <p><b>作者</b>：Minyoung Oh,  Duhyun Kim,  Jae-Young Sim</p>
  <p><b>备注</b>：AAAI2024 accepted</p>
  <p><b>关键词</b>：accompanies privacy issues, time and effort, privacy issues, labeling real datasets, requires a lot</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Collecting and labeling real datasets to train the person search networks not only requires a lot of time and effort, but also accompanies privacy issues. The weakly-supervised and unsupervised domain adaptation methods have been proposed to alleviate the labeling burden for target datasets, however, their generalization capability is limited. We introduce a novel person search method based on the domain generalization framework, that uses an automatically labeled unreal dataset only for training but is applicable to arbitrary unseen real datasets. To alleviate the domain gaps when transferring the knowledge from the unreal source dataset to the real target datasets, we estimate the fidelity of person instances which is then used to train the end-to-end network adaptively. Moreover, we devise a domain-invariant feature learning scheme to encourage the network to suppress the domain-related features. Experimental results demonstrate that the proposed method provides the competitive performance to existing person search methods even though it is applicable to arbitrary unseen datasets without any prior knowledge and re-training burdens.</p>
  </details>
</details>
<details>
  <summary>140. <b>标题：A Multi-Branched Radial Basis Network Approach to Predicting Complex  Chaotic Behaviours</b></summary>
  <p><b>编号</b>：[353]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00618">https://arxiv.org/abs/2404.00618</a></p>
  <p><b>作者</b>：Aarush Sinha</p>
  <p><b>备注</b>：7 pages, 4 figures</p>
  <p><b>关键词</b>：multi branched network, branched network approach, physics attractor characterized, Radial Basis Function, propose a multi</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this study, we propose a multi branched network approach to predict the dynamics of a physics attractor characterized by intricate and chaotic behavior. We introduce a unique neural network architecture comprised of Radial Basis Function (RBF) layers combined with an attention mechanism designed to effectively capture nonlinear inter-dependencies inherent in the attractor's temporal evolution. Our results demonstrate successful prediction of the attractor's trajectory across 100 predictions made using a real-world dataset of 36,700 time-series observations encompassing approximately 28 minutes of activity. To further illustrate the performance of our proposed technique, we provide comprehensive visualizations depicting the attractor's original and predicted behaviors alongside quantitative measures comparing observed versus estimated outcomes. Overall, this work showcases the potential of advanced machine learning algorithms in elucidating hidden structures in complex physical systems while offering practical applications in various domains requiring accurate short-term forecasting capabilities.</p>
  </details>
</details>
<details>
  <summary>141. <b>标题：Object-level Copy-Move Forgery Image Detection based on Inconsistency  Mining</b></summary>
  <p><b>编号</b>：[357]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00611">https://arxiv.org/abs/2404.00611</a></p>
  <p><b>作者</b>：Jingyu Wang,  Niantai Jing,  Ziyao Liu,  Jie Nie,  Yuxin Qi,  Chi-Hung Chi,  Kwok-Yan Lam</p>
  <p><b>备注</b>：4 pages, 2 figures</p>
  <p><b>关键词</b>：conceal tampering traces, copy-move tampering operations, posing significant challenges, Forgery Image Detection, Copy-Move Forgery Image</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In copy-move tampering operations, perpetrators often employ techniques, such as blurring, to conceal tampering traces, posing significant challenges to the detection of object-level targets with intact structures. Focus on these challenges, this paper proposes an Object-level Copy-Move Forgery Image Detection based on Inconsistency Mining (IMNet). To obtain complete object-level targets, we customize prototypes for both the source and tampered regions and dynamically update them. Additionally, we extract inconsistent regions between coarse similar regions obtained through self-correlation calculations and regions composed of prototypes. The detected inconsistent regions are used as supplements to coarse similar regions to refine pixel-level detection. We operate experiments on three public datasets which validate the effectiveness and the robustness of the proposed IMNet.</p>
  </details>
</details>
<details>
  <summary>142. <b>标题：Weak Distribution Detectors Lead to Stronger Generalizability of  Vision-Language Prompt Tuning</b></summary>
  <p><b>编号</b>：[360]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00603">https://arxiv.org/abs/2404.00603</a></p>
  <p><b>作者</b>：Kun Ding,  Haojian Zhang,  Qiang Yu,  Ying Wang,  Shiming Xiang,  Chunhong Pan</p>
  <p><b>备注</b>：Accepted by AAAI2024</p>
  <p><b>关键词</b>：downstream few-shot tasks, pre-trained vision-language models, vision-language models, propose a generalized, fine-tuning on downstream</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a generalized method for boosting the generalization ability of pre-trained vision-language models (VLMs) while fine-tuning on downstream few-shot tasks. The idea is realized by exploiting out-of-distribution (OOD) detection to predict whether a sample belongs to a base distribution or a novel distribution and then using the score generated by a dedicated competition based scoring function to fuse the zero-shot and few-shot classifier. The fused classifier is dynamic, which will bias towards the zero-shot classifier if a sample is more likely from the distribution pre-trained on, leading to improved base-to-novel generalization ability. Our method is performed only in test stage, which is applicable to boost existing methods without time-consuming re-training. Extensive experiments show that even weak distribution detectors can still improve VLMs' generalization ability. Specifically, with the help of OOD detectors, the harmonic mean of CoOp and ProGrad increase by 2.6 and 1.5 percentage points over 11 recognition datasets in the base-to-novel setting.</p>
  </details>
</details>
<details>
  <summary>143. <b>标题：Parameter and Data-Efficient Spectral StyleDCGAN</b></summary>
  <p><b>编号</b>：[365]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00597">https://arxiv.org/abs/2404.00597</a></p>
  <p><b>作者</b>：Aryan Garg</p>
  <p><b>备注</b>：Notable ICLR Tiny Paper 2024</p>
  <p><b>关键词</b>：data-efficient adversarial network, unconditional face generation, present a simple, data-efficient adversarial, adversarial network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a simple, highly parameter, and data-efficient adversarial network for unconditional face generation. Our method: Spectral Style-DCGAN or SSD utilizes only 6.574 million parameters and 4739 dog faces from the Animal Faces HQ (AFHQ) dataset as training samples while preserving fidelity at low resolutions up to 64x64. Code available at this https URL.</p>
  </details>
</details>
<details>
  <summary>144. <b>标题：LAESI: Leaf Area Estimation with Synthetic Imagery</b></summary>
  <p><b>编号</b>：[369]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00593">https://arxiv.org/abs/2404.00593</a></p>
  <p><b>作者</b>：Jacek Kałużny,  Yannik Schreckenberg,  Karol Cyganik,  Peter Annighöfer,  Sören Pirk,  Dominik L. Michels,  Mikolaj Cieslak,  Farhah Assaad-Gerbert,  Bedrich Benes,  Wojciech Pałubicki</p>
  <p><b>备注</b>：10 pages, 12 figures, 1 table</p>
  <p><b>关键词</b>：Synthetic Leaf Dataset, Synthetic Leaf, synthetic leaf images, surface area labels, millimeter paper</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce LAESI, a Synthetic Leaf Dataset of 100,000 synthetic leaf images on millimeter paper, each with semantic masks and surface area labels. This dataset provides a resource for leaf morphology analysis primarily aimed at beech and oak leaves. We evaluate the applicability of the dataset by training machine learning models for leaf surface area prediction and semantic segmentation, using real images for validation. Our validation shows that these models can be trained to predict leaf surface area with a relative error not greater than an average human annotator. LAESI also provides an efficient framework based on 3D procedural models and generative AI for the large-scale, controllable generation of data with potential further applications in agriculture and biology. We evaluate the inclusion of generative AI in our procedural data generation pipeline and show how data filtering based on annotation consistency results in datasets which allow training the highest performing vision models.</p>
  </details>
</details>
<details>
  <summary>145. <b>标题：Memory-based Cross-modal Semantic Alignment Network for Radiology Report  Generation</b></summary>
  <p><b>编号</b>：[373]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00588">https://arxiv.org/abs/2404.00588</a></p>
  <p><b>作者</b>：Yitian Tao,  Liyan Ma,  Jing Yu,  Han Zhang</p>
  <p><b>备注</b>：12 pages, 8 figures</p>
  <p><b>关键词</b>：reports automatically reduces, radiology reports automatically, automatically reduces, reduces the workload, workload of radiologists</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generating radiology reports automatically reduces the workload of radiologists and helps the diagnoses of specific diseases. Many existing methods take this task as modality transfer process. However, since the key information related to disease accounts for a small proportion in both image and report, it is hard for the model to learn the latent relation between the radiology image and its report, thus failing to generate fluent and accurate radiology reports. To tackle this problem, we propose a memory-based cross-modal semantic alignment model (MCSAM) following an encoder-decoder paradigm. MCSAM includes a well initialized long-term clinical memory bank to learn disease-related representations as well as prior knowledge for different modalities to retrieve and use the retrieved memory to perform feature consolidation. To ensure the semantic consistency of the retrieved cross modal prior knowledge, a cross-modal semantic alignment module (SAM) is proposed. SAM is also able to generate semantic visual feature embeddings which can be added to the decoder and benefits report generation. More importantly, to memorize the state and additional information while generating reports with the decoder, we use learnable memory tokens which can be seen as prompts. Extensive experiments demonstrate the promising performance of our proposed method which generates state-of-the-art performance on the MIMIC-CXR dataset.</p>
  </details>
</details>
<details>
  <summary>146. <b>标题：M3D: Advancing 3D Medical Image Analysis with Multi-Modal Large Language  Models</b></summary>
  <p><b>编号</b>：[377]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00578">https://arxiv.org/abs/2404.00578</a></p>
  <p><b>作者</b>：Fan Bai,  Yuxin Du,  Tiejun Huang,  Max Q.-H. Meng,  Bo Zhao</p>
  <p><b>备注</b>：MLLM, 3D medical image analysis</p>
  <p><b>关键词</b>：Medical image analysis, Medical image, image analysis, diagnosis and treatment, Medical</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Medical image analysis is essential to clinical diagnosis and treatment, which is increasingly supported by multi-modal large language models (MLLMs). However, previous research has primarily focused on 2D medical images, leaving 3D images under-explored, despite their richer spatial information. This paper aims to advance 3D medical image analysis with MLLMs. To this end, we present a large-scale 3D multi-modal medical dataset, M3D-Data, comprising 120K image-text pairs and 662K instruction-response pairs specifically tailored for various 3D medical tasks, such as image-text retrieval, report generation, visual question answering, positioning, and segmentation. Additionally, we propose M3D-LaMed, a versatile multi-modal large language model for 3D medical image analysis. Furthermore, we introduce a new 3D multi-modal medical benchmark, M3D-Bench, which facilitates automatic evaluation across eight tasks. Through comprehensive evaluation, our method proves to be a robust model for 3D medical image analysis, outperforming existing solutions. All code, data, and models are publicly available at: this https URL.</p>
  </details>
</details>
<details>
  <summary>147. <b>标题：Automated Bi-Fold Weighted Ensemble Algorithms and its Application to  Brain Tumor Detection and Classification</b></summary>
  <p><b>编号</b>：[378]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00576">https://arxiv.org/abs/2404.00576</a></p>
  <p><b>作者</b>：PoTsang B. Huang,  Muhammad Rizwan,  Mehboob Ali</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：types of cancers, uncontrolled and unstructured, unstructured growth, highest mortality rates, mortality rates</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The uncontrolled and unstructured growth of brain cells is known as brain tumor, which has one of the highest mortality rates among diseases from all types of cancers. Due to limited diagnostic and treatment capabilities, they pose significant challenges, especially in third-world countries. Early diagnosis plays a vital role in effectively managing brain tumors and reducing mortality rates. However, the availability of diagnostic methods is hindered by various limitations, including high costs and lengthy result acquisition times, impeding early detection of the disease. In this study, we present two cutting-edge bi-fold weighted voting ensemble models that aim to boost the effectiveness of weighted ensemble methods. These two proposed methods combine the classification outcomes from multiple classifiers and determine the optimal result by selecting the one with the highest probability in the first approach, and the highest weighted prediction in the second technique. These approaches significantly improve the overall performance of weighted ensemble techniques. In the first proposed method, we improve the soft voting technique (SVT) by introducing a novel unsupervised weight calculating schema (UWCS) to enhance its weight assigning capability, known as the extended soft voting technique (ESVT). Secondly, we propose a novel weighted method (NWM) by using the proposed UWCS. Both of our approaches incorporate three distinct models: a custom-built CNN, VGG-16, and InceptionResNetV2 which has been trained on publicly available datasets. The effectiveness of our proposed systems is evaluated through blind testing, where exceptional results are achieved. We then establish a comparative analysis of the performance of our proposed methods with that of SVT to show their superiority and effectiveness.</p>
  </details>
</details>
<details>
  <summary>148. <b>标题：Exploiting Inter-sample and Inter-feature Relations in Dataset  Distillation</b></summary>
  <p><b>编号</b>：[386]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00563">https://arxiv.org/abs/2404.00563</a></p>
  <p><b>作者</b>：Wenxiao Deng,  Wenbin Li,  Tianyu Ding,  Lei Wang,  Hongguang Zhang,  Kuihua Huang,  Jing Huo,  Yang Gao</p>
  <p><b>备注</b>：Accepted to CVPR 2024</p>
  <p><b>关键词</b>：enabling efficient training, deep learning, enabling efficient, promising approach, approach in deep</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Dataset distillation has emerged as a promising approach in deep learning, enabling efficient training with small synthetic datasets derived from larger real ones. Particularly, distribution matching-based distillation methods attract attention thanks to its effectiveness and low computational cost. However, these methods face two primary limitations: the dispersed feature distribution within the same class in synthetic datasets, reducing class discrimination, and an exclusive focus on mean feature consistency, lacking precision and comprehensiveness. To address these challenges, we introduce two novel constraints: a class centralization constraint and a covariance matching constraint. The class centralization constraint aims to enhance class discrimination by more closely clustering samples within classes. The covariance matching constraint seeks to achieve more accurate feature distribution matching between real and synthetic datasets through local feature covariance matrices, particularly beneficial when sample sizes are much smaller than the number of features. Experiments demonstrate notable improvements with these constraints, yielding performance boosts of up to 6.6% on CIFAR10, 2.9% on SVHN, 2.5% on CIFAR100, and 2.5% on TinyImageNet, compared to the state-of-the-art relevant methods. In addition, our method maintains robust performance in cross-architecture settings, with a maximum performance drop of 1.7% on four architectures. Code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>149. <b>标题：Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction</b></summary>
  <p><b>编号</b>：[387]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00562">https://arxiv.org/abs/2404.00562</a></p>
  <p><b>作者</b>：Junuk Cha,  Jihyeon Kim,  Jae Shin Yoon,  Seungryul Baek</p>
  <p><b>备注</b>：Accepted to CVPR 2024</p>
  <p><b>关键词</b>：text-guided work, hand-object interaction, hand-object contact generation, hand-object motion generation, hand-object</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper introduces the first text-guided work for generating the sequence of hand-object interaction in 3D. The main challenge arises from the lack of labeled data where existing ground-truth datasets are nowhere near generalizable in interaction type and object category, which inhibits the modeling of diverse 3D hand-object interaction with the correct physical implication (e.g., contacts and semantics) from text prompts. To address this challenge, we propose to decompose the interaction generation task into two subtasks: hand-object contact generation; and hand-object motion generation. For contact generation, a VAE-based network takes as input a text and an object mesh, and generates the probability of contacts between the surfaces of hands and the object during the interaction. The network learns a variety of local geometry structure of diverse objects that is independent of the objects' category, and thus, it is applicable to general objects. For motion generation, a Transformer-based diffusion model utilizes this 3D contact map as a strong prior for generating physically plausible hand-object motion as a function of text prompts by learning from the augmented labeled dataset; where we annotate text labels from many existing 3D hand and object motion data. Finally, we further introduce a hand refiner module that minimizes the distance between the object surface and hand joints to improve the temporal stability of the object-hand contacts and to suppress the penetration artifacts. In the experiments, we demonstrate that our method can generate more realistic and diverse interactions compared to other baseline methods. We also show that our method is applicable to unseen objects. We will release our model and newly labeled data as a strong foundation for future research. Codes and data are available in: this https URL.</p>
  </details>
</details>
<details>
  <summary>150. <b>标题：Comparison of Methods in Human Skin Decomposition</b></summary>
  <p><b>编号</b>：[392]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00552">https://arxiv.org/abs/2404.00552</a></p>
  <p><b>作者</b>：Hao Gong,  Michel Desvignes</p>
  <p><b>备注</b>：4 pages, 7 figures</p>
  <p><b>关键词</b>：medical fields, plays an important, important role, role in medical, skin pigment plays</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Decomposition of skin pigment plays an important role in medical fields. Human skin can be decomposed into two primitive components, hemoglobin and melanin. It is our goal to apply these results for diagnosis of skin cancer. In this paper, various methods for skin pigment decomposition are reviewed comparatively and the performance of each method is evaluated both theoretically and experimentally. In addition, isometric feature mapping (Isomap) is introduced in order to improve the dimensionality reduction performance in context of skin decomposition.</p>
  </details>
</details>
<details>
  <summary>151. <b>标题：Denoising Distillation Makes Event-Frame Transformers as Accurate Gaze  Trackers</b></summary>
  <p><b>编号</b>：[393]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00548">https://arxiv.org/abs/2404.00548</a></p>
  <p><b>作者</b>：Jiading Li,  Zhiyu Zhu,  Jinhui Hou,  Junhui Hou,  Jinjian Wu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：passive gaze estimation, gaze estimation, paper tackles, tackles the problem, problem of passive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper tackles the problem of passive gaze estimation using both event and frame data. Considering inherently different physiological structures, it's intractable to accurately estimate purely based on a given state. Thus, we reformulate the gaze estimation as the quantification of state transitions from the current state to several prior registered anchor states. Technically, we propose a two-stage learning-based gaze estimation framework to divide the whole gaze estimation process into a coarse-to-fine process of anchor state selection and final gaze location. Moreover, to improve generalization ability, we align a group of local experts with a student network, where a novel denoising distillation algorithm is introduced to utilize denoising diffusion technique to iteratively remove inherent noise of event data. Extensive experiments demonstrate the effectiveness of the proposed method, which greatly surpasses state-of-the-art methods by a large extent of 15$\%$. The code will be publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>152. <b>标题：On the Estimation of Image-matching Uncertainty in Visual Place  Recognition</b></summary>
  <p><b>编号</b>：[394]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00546">https://arxiv.org/abs/2404.00546</a></p>
  <p><b>作者</b>：Mubariz Zaffar,  Liangliang Nan,  Julian F. P. Kooij</p>
  <p><b>备注</b>：To appear in the proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2024</p>
  <p><b>关键词</b>：Visual Place Recognition, Place Recognition, Visual Place, reference images, estimated by comparing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In Visual Place Recognition (VPR) the pose of a query image is estimated by comparing the image to a map of reference images with known reference poses. As is typical for image retrieval problems, a feature extractor maps the query and reference images to a feature space, where a nearest neighbor search is then performed. However, till recently little attention has been given to quantifying the confidence that a retrieved reference image is a correct match. Highly certain but incorrect retrieval can lead to catastrophic failure of VPR-based localization pipelines. This work compares for the first time the main approaches for estimating the image-matching uncertainty, including the traditional retrieval-based uncertainty estimation, more recent data-driven aleatoric uncertainty estimation, and the compute-intensive geometric verification. We further formulate a simple baseline method, ``SUE'', which unlike the other methods considers the freely-available poses of the reference images in the map. Our experiments reveal that a simple L2-distance between the query and reference descriptors is already a better estimate of image-matching uncertainty than current data-driven approaches. SUE outperforms the other efficient uncertainty estimation methods, and its uncertainty estimates complement the computationally expensive geometric verification approach. Future works for uncertainty estimation in VPR should consider the baselines discussed in this work.</p>
  </details>
</details>
<details>
  <summary>153. <b>标题：Deep Extrinsic Manifold Representation for Vision Tasks</b></summary>
  <p><b>编号</b>：[395]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00544">https://arxiv.org/abs/2404.00544</a></p>
  <p><b>作者</b>：Tongtong Zhang,  Xian Wei,  Yuanxiang Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Extrinsic Manifold Representation, training neural networks, Deep Extrinsic Manifold, Non-Euclidean data, manifold representations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Non-Euclidean data is frequently encountered across different fields, yet there is limited literature that addresses the fundamental challenge of training neural networks with manifold representations as outputs. We introduce the trick named Deep Extrinsic Manifold Representation (DEMR) for visual tasks in this context. DEMR incorporates extrinsic manifold embedding into deep neural networks, which helps generate manifold representations. The DEMR approach does not directly optimize the complex geodesic loss. Instead, it focuses on optimizing the computation graph within the embedded Euclidean space, allowing for adaptability to various architectural requirements. We provide empirical evidence supporting the proposed concept on two types of manifolds, $SE(3)$ and its associated quotient manifolds. This evidence offers theoretical assurances regarding feasibility, asymptotic properties, and generalization capability. The experimental results show that DEMR effectively adapts to point cloud alignment, producing outputs in $ SE(3) $, as well as in illumination subspace learning with outputs on the Grassmann manifold.</p>
  </details>
</details>
<details>
  <summary>154. <b>标题：Embodied Active Defense: Leveraging Recurrent Feedback to Counter  Adversarial Patches</b></summary>
  <p><b>编号</b>：[396]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00540">https://arxiv.org/abs/2404.00540</a></p>
  <p><b>作者</b>：Lingxuan Wu,  Xiao Yang,  Yinpeng Dong,  Liuwei Xie,  Hang Su,  Jun Zhu</p>
  <p><b>备注</b>：27pages</p>
  <p><b>关键词</b>：deep neural networks, motivated numerous defense, counter adversarial patches, adversarial patches, numerous defense strategies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The vulnerability of deep neural networks to adversarial patches has motivated numerous defense strategies for boosting model robustness. However, the prevailing defenses depend on single observation or pre-established adversary information to counter adversarial patches, often failing to be confronted with unseen or adaptive adversarial attacks and easily exhibiting unsatisfying performance in dynamic 3D environments. Inspired by active human perception and recurrent feedback mechanisms, we develop Embodied Active Defense (EAD), a proactive defensive strategy that actively contextualizes environmental information to address misaligned adversarial patches in 3D real-world settings. To achieve this, EAD develops two central recurrent sub-modules, i.e., a perception module and a policy module, to implement two critical functions of active vision. These models recurrently process a series of beliefs and observations, facilitating progressive refinement of their comprehension of the target object and enabling the development of strategic actions to counter adversarial patches in 3D environments. To optimize learning efficiency, we incorporate a differentiable approximation of environmental dynamics and deploy patches that are agnostic to the adversary strategies. Extensive experiments demonstrate that EAD substantially enhances robustness against a variety of patches within just a few steps through its action policy in safety-critical tasks (e.g., face recognition and object detection), without compromising standard accuracy. Furthermore, due to the attack-agnostic characteristic, EAD facilitates excellent generalization to unseen attacks, diminishing the averaged attack success rate by 95 percent across a range of unseen adversarial attacks.</p>
  </details>
</details>
<details>
  <summary>155. <b>标题：LLMs are Good Action Recognizers</b></summary>
  <p><b>编号</b>：[399]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00532">https://arxiv.org/abs/2404.00532</a></p>
  <p><b>作者</b>：Haoxuan Qu,  Yujun Cai,  Jun Liu</p>
  <p><b>备注</b>：CVPR 2024</p>
  <p><b>关键词</b>：research attention, Skeleton-based action recognition, recognition has attracted, attracted lots, lots of research</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Skeleton-based action recognition has attracted lots of research attention. Recently, to build an accurate skeleton-based action recognizer, a variety of works have been proposed. Among them, some works use large model architectures as backbones of their recognizers to boost the skeleton data representation capability, while some other works pre-train their recognizers on external data to enrich the knowledge. In this work, we observe that large language models which have been extensively used in various natural language processing tasks generally hold both large model architectures and rich implicit knowledge. Motivated by this, we propose a novel LLM-AR framework, in which we investigate treating the Large Language Model as an Action Recognizer. In our framework, we propose a linguistic projection process to project each input action signal (i.e., each skeleton sequence) into its ``sentence format'' (i.e., an ``action sentence''). Moreover, we also incorporate our framework with several designs to further facilitate this linguistic projection process. Extensive experiments demonstrate the efficacy of our proposed framework.</p>
  </details>
</details>
<details>
  <summary>156. <b>标题：TexVocab: Texture Vocabulary-conditioned Human Avatars</b></summary>
  <p><b>编号</b>：[406]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00524">https://arxiv.org/abs/2404.00524</a></p>
  <p><b>作者</b>：Yuxiao Liu,  Zhe Li,  Yebin Liu,  Haoqian Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：video-based avatar modeling, texture maps, multi-view video-based avatar, propose TexVocab, producing texture maps</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To adequately utilize the available image evidence in multi-view video-based avatar modeling, we propose TexVocab, a novel avatar representation that constructs a texture vocabulary and associates body poses with texture maps for animation. Given multi-view RGB videos, our method initially back-projects all the available images in the training videos to the posed SMPL surface, producing texture maps in the SMPL UV domain. Then we construct pairs of human poses and texture maps to establish a texture vocabulary for encoding dynamic human appearances under various poses. Unlike the commonly used joint-wise manner, we further design a body-part-wise encoding strategy to learn the structural effects of the kinematic chain. Given a driving pose, we query the pose feature hierarchically by decomposing the pose vector into several body parts and interpolating the texture features for synthesizing fine-grained human dynamics. Overall, our method is able to create animatable human avatars with detailed and dynamic appearances from RGB videos, and the experiments show that our method outperforms state-of-the-art approaches. The project page can be found at this https URL.</p>
  </details>
</details>
<details>
  <summary>157. <b>标题：CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz  continuity constrAIned Normalization</b></summary>
  <p><b>编号</b>：[408]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00521">https://arxiv.org/abs/2404.00521</a></p>
  <p><b>作者</b>：Yao Ni,  Piotr Koniusz</p>
  <p><b>备注</b>：Accepted by CVPR2024, 26 pages full version</p>
  <p><b>关键词</b>：Generative Adversarial Networks, Adversarial Networks, performance heavily depends, Generative Adversarial, significantly advanced image</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generative Adversarial Networks (GANs) significantly advanced image generation but their performance heavily depends on abundant training data. In scenarios with limited data, GANs often struggle with discriminator overfitting and unstable training. Batch Normalization (BN), despite being known for enhancing generalization and training stability, has rarely been used in the discriminator of Data-Efficient GANs. Our work addresses this gap by identifying a critical flaw in BN: the tendency for gradient explosion during the centering and scaling steps. To tackle this issue, we present CHAIN (lipsCHitz continuity constrAIned Normalization), which replaces the conventional centering step with zero-mean regularization and integrates a Lipschitz continuity constraint in the scaling step. CHAIN further enhances GAN training by adaptively interpolating the normalized and unnormalized features, effectively avoiding discriminator overfitting. Our theoretical analyses firmly establishes CHAIN's effectiveness in reducing gradients in latent features and weights, improving stability and generalization in GAN training. Empirical evidence supports our theory. CHAIN achieves state-of-the-art results in data-limited scenarios on CIFAR-10/100, ImageNet, five low-shot and seven high-resolution few-shot image datasets.</p>
  </details>
</details>
<details>
  <summary>158. <b>标题：Transformer based Pluralistic Image Completion with Reduced Information  Loss</b></summary>
  <p><b>编号</b>：[411]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00513">https://arxiv.org/abs/2404.00513</a></p>
  <p><b>作者</b>：Qiankun Liu,  Yuqi Jiang,  Zhentao Tan,  Dongdong Chen,  Ying Fu,  Qi Chu,  Gang Hua,  Nenghai Yu</p>
  <p><b>备注</b>：Accepted by TPAMI (2024)</p>
  <p><b>关键词</b>：achieved great success, achieved great, great success, Transformer based methods, Transformer based</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transformer based methods have achieved great success in image inpainting recently. However, we find that these solutions regard each pixel as a token, thus suffering from an information loss issue from two aspects: 1) They downsample the input image into much lower resolutions for efficiency consideration. 2) They quantize $256^3$ RGB values to a small number (such as 512) of quantized color values. The indices of quantized pixels are used as tokens for the inputs and prediction targets of the transformer. To mitigate these issues, we propose a new transformer based framework called "PUT". Specifically, to avoid input downsampling while maintaining computation efficiency, we design a patch-based auto-encoder P-VQVAE. The encoder converts the masked image into non-overlapped patch tokens and the decoder recovers the masked regions from the inpainted tokens while keeping the unmasked regions unchanged. To eliminate the information loss caused by input quantization, an Un-quantized Transformer is applied. It directly takes features from the P-VQVAE encoder as input without any quantization and only regards the quantized tokens as prediction targets. Furthermore, to make the inpainting process more controllable, we introduce semantic and structural conditions as extra guidance. Extensive experiments show that our method greatly outperforms existing transformer based methods on image fidelity and achieves much higher diversity and better fidelity than state-of-the-art pluralistic inpainting methods on complex large-scale datasets (e.g., ImageNet). Codes are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>159. <b>标题：MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in  Conversations with Multimodal Language Models</b></summary>
  <p><b>编号</b>：[412]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00511">https://arxiv.org/abs/2404.00511</a></p>
  <p><b>作者</b>：Zebang Cheng,  Fuqiang Niu,  Yuxiang Lin,  Zhi-Qi Cheng,  Bowen Zhang,  Xiaojiang Peng</p>
  <p><b>备注</b>：Ranked 3rd in SemEval '24 Task 3 with F1 of 0.3435, close to 1st & 2nd by 0.0339 & 0.0025</p>
  <p><b>关键词</b>：Multimodal Emotion Recognition, multimodal emotion, analysis in conversations, paper presents, presents our winning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents our winning submission to Subtask 2 of SemEval 2024 Task 3 on multimodal emotion cause analysis in conversations. We propose a novel Multimodal Emotion Recognition and Multimodal Emotion Cause Extraction (MER-MCE) framework that integrates text, audio, and visual modalities using specialized emotion encoders. Our approach sets itself apart from top-performing teams by leveraging modality-specific features for enhanced emotion understanding and causality inference. Experimental evaluation demonstrates the advantages of our multimodal approach, with our submission achieving a competitive weighted F1 score of 0.3435, ranking third with a margin of only 0.0339 behind the 1st team and 0.0025 behind the 2nd team. Project: this https URL</p>
  </details>
</details>
<details>
  <summary>160. <b>标题：Denoising Low-dose Images Using Deep Learning of Time Series Images</b></summary>
  <p><b>编号</b>：[413]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00510">https://arxiv.org/abs/2404.00510</a></p>
  <p><b>作者</b>：Yang Shao,  Toshie Yaguchi,  Toshiaki Tanigaki</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：including scientific imaging, Digital image devices, recognition of individuals, including scientific, remote sensing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Digital image devices have been widely applied in many fields, including scientific imaging, recognition of individuals, and remote sensing. As the application of these imaging technologies to autonomous driving and measurement, image noise generated when observation cannot be performed with a sufficient dose has become a major problem. Machine learning denoise technology is expected to be the solver of this problem, but there are the following problems. Here we report, artifacts generated by machine learning denoise in ultra-low dose observation using an in-situ observation video of an electron microscope as an example. And as a method to solve this problem, we propose a method to decompose a time series image into a 2D image of the spatial axis and time to perform machine learning denoise. Our method opens new avenues accurate and stable reconstruction of continuous high-resolution images from low-dose imaging in science, industry, and life.</p>
  </details>
</details>
<details>
  <summary>161. <b>标题：DailyMAE: Towards Pretraining Masked Autoencoders in One Day</b></summary>
  <p><b>编号</b>：[414]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00509">https://arxiv.org/abs/2404.00509</a></p>
  <p><b>作者</b>：Jiantao Wu,  Shentong Mo,  Sara Atito,  Zhenhua Feng,  Josef Kittler,  Muhammad Awais</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：masked image modeling, important self-supervised learning, learning data representation, self-supervised learning, masked image</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, masked image modeling (MIM), an important self-supervised learning (SSL) method, has drawn attention for its effectiveness in learning data representation from unlabeled data. Numerous studies underscore the advantages of MIM, highlighting how models pretrained on extensive datasets can enhance the performance of downstream tasks. However, the high computational demands of pretraining pose significant challenges, particularly within academic environments, thereby impeding the SSL research progress. In this study, we propose efficient training recipes for MIM based SSL that focuses on mitigating data loading bottlenecks and employing progressive training techniques and other tricks to closely maintain pretraining performance. Our library enables the training of a MAE-Base/16 model on the ImageNet 1K dataset for 800 epochs within just 18 hours, using a single machine equipped with 8 A100 GPUs. By achieving speed gains of up to 5.8 times, this work not only demonstrates the feasibility of conducting high-efficiency SSL training but also paves the way for broader accessibility and promotes advancement in SSL research particularly for prototyping and initial testing of SSL ideas. The code is available in this https URL.</p>
  </details>
</details>
<details>
  <summary>162. <b>标题：NYC-Indoor-VPR: A Long-Term Indoor Visual Place Recognition Dataset with  Semi-Automatic Annotation</b></summary>
  <p><b>编号</b>：[418]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00504">https://arxiv.org/abs/2404.00504</a></p>
  <p><b>作者</b>：Diwei Sheng,  Anbang Yang,  John-Ross Rizzo,  Chen Feng</p>
  <p><b>备注</b>：7 pages, 7 figures, published in 2024 IEEE International Conference on Robotics and Automation (ICRA 2024)</p>
  <p><b>关键词</b>：Visual Place Recognition, Place Recognition, Visual Place, localization and navigation, indoor environments</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Visual Place Recognition (VPR) in indoor environments is beneficial to humans and robots for better localization and navigation. It is challenging due to appearance changes at various frequencies, and difficulties of obtaining ground truth metric trajectories for training and evaluation. This paper introduces the NYC-Indoor-VPR dataset, a unique and rich collection of over 36,000 images compiled from 13 distinct crowded scenes in New York City taken under varying lighting conditions with appearance changes. Each scene has multiple revisits across a year. To establish the ground truth for VPR, we propose a semiautomatic annotation approach that computes the positional information of each image. Our method specifically takes pairs of videos as input and yields matched pairs of images along with their estimated relative locations. The accuracy of this matching is refined by human annotators, who utilize our annotation software to correlate the selected keyframes. Finally, we present a benchmark evaluation of several state-of-the-art VPR algorithms using our annotated dataset, revealing its challenge and thus value for VPR research.</p>
  </details>
</details>
<details>
  <summary>163. <b>标题：94% on CIFAR-10 in 3.29 Seconds on a Single GPU</b></summary>
  <p><b>编号</b>：[421]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00498">https://arxiv.org/abs/2404.00498</a></p>
  <p><b>作者</b>：Keller Jordan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：machine learning, facilitating thousands, projects per year, widely used datasets, datasets in machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>CIFAR-10 is among the most widely used datasets in machine learning, facilitating thousands of research projects per year. To accelerate research and reduce the cost of experiments, we introduce training methods for CIFAR-10 which reach 94% accuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when run on a single NVIDIA A100 GPU. As one factor contributing to these training speeds, we propose a derandomized variant of horizontal flipping augmentation, which we show improves over the standard method in every case where flipping is beneficial over no flipping at all. Our code is released at this https URL.</p>
  </details>
</details>
<details>
  <summary>164. <b>标题：Denoising Monte Carlo Renders With Diffusion Models</b></summary>
  <p><b>编号</b>：[425]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00491">https://arxiv.org/abs/2404.00491</a></p>
  <p><b>作者</b>：Vaibhav Vavilala,  Rahul Vasanth,  David Forsyth</p>
  <p><b>备注</b>：14 pages, 12 figures</p>
  <p><b>关键词</b>：Physically-based renderings, renderings contain Monte-Carlo, variance that increases, pixel decreases, rays per pixel</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Physically-based renderings contain Monte-Carlo noise, with variance that increases as the number of rays per pixel decreases. This noise, while zero-mean for good modern renderers, can have heavy tails (most notably, for scenes containing specular or refractive objects). Learned methods for restoring low fidelity renders are highly developed, because suppressing render noise means one can save compute and use fast renders with few rays per pixel. We demonstrate that a diffusion model can denoise low fidelity renders successfully. Furthermore, our method can be conditioned on a variety of natural render information, and this conditioning helps performance. Quantitative experiments show that our method is competitive with SOTA across a range of sampling rates, but current metrics slightly favor competitor methods. Qualitative examination of the reconstructions suggests that the metrics themselves may not be reliable. The image prior applied by a diffusion method strongly favors reconstructions that are "like" real images -- so have straight shadow boundaries, curved specularities, no "fireflies" and the like -- and metrics do not account for this. We show numerous examples where methods preferred by current metrics produce qualitatively weaker reconstructions than ours.</p>
  </details>
</details>
<details>
  <summary>165. <b>标题：DiffHuman: Probabilistic Photorealistic 3D Reconstruction of Humans</b></summary>
  <p><b>编号</b>：[430]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00485">https://arxiv.org/abs/2404.00485</a></p>
  <p><b>作者</b>：Akash Sengupta,  Thiemo Alldieck,  Nikos Kolotouros,  Enric Corona,  Andrei Zanfir,  Cristian Sminchisescu</p>
  <p><b>备注</b>：CVPR 2024</p>
  <p><b>关键词</b>：single RGB image, single RGB, RGB image, probabilistic method, human reconstruction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present DiffHuman, a probabilistic method for photorealistic 3D human reconstruction from a single RGB image. Despite the ill-posed nature of this problem, most methods are deterministic and output a single solution, often resulting in a lack of geometric detail and blurriness in unseen or uncertain regions. In contrast, DiffHuman predicts a probability distribution over 3D reconstructions conditioned on an input 2D image, which allows us to sample multiple detailed 3D avatars that are consistent with the image. DiffHuman is implemented as a conditional diffusion model that denoises pixel-aligned 2D observations of an underlying 3D shape representation. During inference, we may sample 3D avatars by iteratively denoising 2D renders of the predicted 3D representation. Furthermore, we introduce a generator neural network that approximates rendering with considerably reduced runtime (55x speed up), resulting in a novel dual-branch diffusion framework. Our experiments show that DiffHuman can produce diverse and detailed reconstructions for the parts of the person that are unseen or uncertain in the input image, while remaining competitive with the state-of-the-art when reconstructing visible surfaces.</p>
  </details>
</details>
<details>
  <summary>166. <b>标题：SceneGraphLoc: Cross-Modal Coarse Visual Localization on 3D Scene Graphs</b></summary>
  <p><b>编号</b>：[438]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00469">https://arxiv.org/abs/2404.00469</a></p>
  <p><b>作者</b>：Yang Miao,  Francis Engelmann,  Olga Vysotska,  Federico Tombari,  Marc Pollefeys,  Dániel Béla Baráth</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：multi-modal reference map, reference map represented, multi-modal reference, image databases, reference map</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce a novel problem, i.e., the localization of an input image within a multi-modal reference map represented by a database of 3D scene graphs. These graphs comprise multiple modalities, including object-level point clouds, images, attributes, and relationships between objects, offering a lightweight and efficient alternative to conventional methods that rely on extensive image databases. Given the available modalities, the proposed method SceneGraphLoc learns a fixed-sized embedding for each node (i.e., representing an object instance) in the scene graph, enabling effective matching with the objects visible in the input query image. This strategy significantly outperforms other cross-modal methods, even without incorporating images into the map embeddings. When images are leveraged, SceneGraphLoc achieves performance close to that of state-of-the-art techniques depending on large image databases, while requiring three orders-of-magnitude less storage and operating orders-of-magnitude faster. The code will be made public.</p>
  </details>
</details>
<details>
  <summary>167. <b>标题：Multiway Point Cloud Mosaicking with Diffusion and Global Optimization</b></summary>
  <p><b>编号</b>：[460]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00429">https://arxiv.org/abs/2404.00429</a></p>
  <p><b>作者</b>：Shengze Jin (Department of Computer Science, ETH Zurich, Switzerland),  Iro Armeni (Department of Civil and Environmental Engineering, Stanford University),  Marc Pollefeys (Department of Computer Science, ETH Zurich, Switzerland and Microsoft Mixed Reality & AI Lab, Zurich, Switzerland),  Daniel Barath (Department of Computer Science, ETH Zurich, Switzerland)</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：moving RGB-D cameras, unified coordinate system, named Wednesday, partially overlapping point, point cloud mosaicking</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce a novel framework for multiway point cloud mosaicking (named Wednesday), designed to co-align sets of partially overlapping point clouds -- typically obtained from 3D scanners or moving RGB-D cameras -- into a unified coordinate system. At the core of our approach is ODIN, a learned pairwise registration algorithm that iteratively identifies overlaps and refines attention scores, employing a diffusion-based process for denoising pairwise correlation matrices to enhance matching accuracy. Further steps include constructing a pose graph from all point clouds, performing rotation averaging, a novel robust algorithm for re-estimating translations optimally in terms of consensus maximization and translation optimization. Finally, the point cloud rotations and positions are optimized jointly by a diffusion-based approach. Tested on four diverse, large-scale datasets, our method achieves state-of-the-art pairwise and multiway registration results by a large margin on all benchmarks. Our code and models are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>168. <b>标题：Extracting Manifold Information from Point Clouds</b></summary>
  <p><b>编号</b>：[461]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00427">https://arxiv.org/abs/2404.00427</a></p>
  <p><b>作者</b>：Patrick Guidotti</p>
  <p><b>备注</b>：27 pages, 16 figures, 5 tables</p>
  <p><b>关键词</b>：point clouds, kernel based method, kernel based, subsets, full dimensional manifolds</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A kernel based method is proposed for the construction of signature (defining) functions of subsets of $\mathbb{R}^d$. The subsets can range from full dimensional manifolds (open subsets) to point clouds (a finite number of points) and include bounded smooth manifolds of any codimension. The interpolation and analysis of point clouds are the main application. Two extreme cases in terms of regularity are considered, where the data set is interpolated by an analytic surface, at the one extreme, and by a Hölder continuous surface, at the other. The signature function can be computed as a linear combination of translated kernels, the coefficients of which are the solution of a finite dimensional linear problem. Once it is obtained, it can be used to estimate the dimension as well as the normal and the curvatures of the interpolated surface. The method is global and does not require explicit knowledge of local neighborhoods or any other structure present in the data set. It admits a variational formulation with a natural ``regularized'' counterpart, that proves to be useful in dealing with data sets corrupted by numerical error or noise. The underlying analytical structure of the approach is presented in general before it is applied to the case of point clouds.</p>
  </details>
</details>
<details>
  <summary>169. <b>标题：Do Vision-Language Models Understand Compound Nouns?</b></summary>
  <p><b>编号</b>：[465]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00419">https://arxiv.org/abs/2404.00419</a></p>
  <p><b>作者</b>：Sonal Kumar,  Sreyan Ghosh,  S Sakshi,  Utkarsh Tyagi,  Dinesh Manocha</p>
  <p><b>备注</b>：Accepted to NAACL 2024 Main Conference</p>
  <p><b>关键词</b>：Open-vocabulary vision-language models, Open-vocabulary vision-language, trained using contrastive, contrastive loss, promising new paradigm</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Open-vocabulary vision-language models (VLMs) like CLIP, trained using contrastive loss, have emerged as a promising new paradigm for text-to-image retrieval. However, do VLMs understand compound nouns (CNs) (e.g., lab coat) as well as they understand nouns (e.g., lab)? We curate Compun, a novel benchmark with 400 unique and commonly used CNs, to evaluate the effectiveness of VLMs in interpreting CNs. The Compun benchmark challenges a VLM for text-to-image retrieval where, given a text prompt with a CN, the task is to select the correct image that shows the CN among a pair of distractor images that show the constituent nouns that make up the CN. Next, we perform an in-depth analysis to highlight CLIPs' limited understanding of certain types of CNs. Finally, we present an alternative framework that moves beyond hand-written templates for text prompts widely used by CLIP-like models. We employ a Large Language Model to generate multiple diverse captions that include the CN as an object in the scene described by the caption. Our proposed method improves CN understanding of CLIP by 8.25% on Compun. Code and benchmark are available at: this https URL</p>
  </details>
</details>
<details>
  <summary>170. <b>标题：Continual Learning for Autonomous Robots: A Prototype-based Approach</b></summary>
  <p><b>编号</b>：[466]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00418">https://arxiv.org/abs/2404.00418</a></p>
  <p><b>作者</b>：Elvin Hajizada,  Balachandran Swaminathan,  Yulia Sandamirskaya</p>
  <p><b>备注</b>：Submitted to IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</p>
  <p><b>关键词</b>：Humans and animals, CLP, lives from limited, limited amounts, amounts of sensed</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Humans and animals learn throughout their lives from limited amounts of sensed data, both with and without supervision. Autonomous, intelligent robots of the future are often expected to do the same. The existing continual learning (CL) methods are usually not directly applicable to robotic settings: they typically require buffering and a balanced replay of training data. A few-shot online continual learning (FS-OCL) setting has been proposed to address more realistic scenarios where robots must learn from a non-repeated sparse data stream. To enable truly autonomous life-long learning, an additional challenge of detecting novelties and learning new items without supervision needs to be addressed. We address this challenge with our new prototype-based approach called Continually Learning Prototypes (CLP). In addition to being capable of FS-OCL learning, CLP also detects novel objects and learns them without supervision. To mitigate forgetting, CLP utilizes a novel metaplasticity mechanism that adapts the learning rate individually per prototype. CLP is rehearsal-free, hence does not require a memory buffer, and is compatible with neuromorphic hardware, characterized by ultra-low power consumption, real-time processing abilities, and on-chip learning. Indeed, we have open-sourced a simple version of CLP in the neuromorphic software framework Lava, targetting Intel's neuromorphic chip Loihi 2. We evaluate CLP on a robotic vision dataset, OpenLORIS. In a low-instance FS-OCL scenario, CLP shows state-of-the-art results. In the open world, CLP detects novelties with superior precision and recall and learns features of the detected novel classes without supervision, achieving a strong baseline of 99% base class and 65%/76% (5-shot/10-shot) novel class accuracy.</p>
  </details>
</details>
<details>
  <summary>171. <b>标题：Orchestrate Latent Expertise: Advancing Online Continual Learning with  Multi-Level Supervision and Reverse Self-Distillation</b></summary>
  <p><b>编号</b>：[467]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00417">https://arxiv.org/abs/2404.00417</a></p>
  <p><b>作者</b>：HongWei Yan,  Liyuan Wang,  Kaisheng Ma,  Yi Zhong</p>
  <p><b>备注</b>：CVPR 2024</p>
  <p><b>关键词</b>：accommodate real-world dynamics, artificial intelligence systems, sequentially arriving content, Online Continual Learning, real-world dynamics</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To accommodate real-world dynamics, artificial intelligence systems need to cope with sequentially arriving content in an online manner. Beyond regular Continual Learning (CL) attempting to address catastrophic forgetting with offline training of each task, Online Continual Learning (OCL) is a more challenging yet realistic setting that performs CL in a one-pass data stream. Current OCL methods primarily rely on memory replay of old training samples. However, a notable gap from CL to OCL stems from the additional overfitting-underfitting dilemma associated with the use of rehearsal buffers: the inadequate learning of new training samples (underfitting) and the repeated learning of a few old training samples (overfitting). To this end, we introduce a novel approach, Multi-level Online Sequential Experts (MOSE), which cultivates the model as stacked sub-experts, integrating multi-level supervision and reverse self-distillation. Supervision signals across multiple stages facilitate appropriate convergence of the new task while gathering various strengths from experts by knowledge distillation mitigates the performance decline of old tasks. MOSE demonstrates remarkable efficacy in learning new samples and preserving past knowledge through multi-level experts, thereby significantly advancing OCL performance over state-of-the-art baselines (e.g., up to 7.3% on Split CIFAR-100 and 6.1% on Split Tiny-ImageNet).</p>
  </details>
</details>
<details>
  <summary>172. <b>标题：SVGCraft: Beyond Single Object Text-to-SVG Synthesis with Comprehensive  Canvas Layout</b></summary>
  <p><b>编号</b>：[471]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00412">https://arxiv.org/abs/2404.00412</a></p>
  <p><b>作者</b>：Ayan Banerjee,  Nityanand Mathur,  Josep Lladós,  Umapada Pal,  Anjan Dutta</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：challenging vision task, Generating VectorArt, vision task, requiring diverse, unseen entities</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generating VectorArt from text prompts is a challenging vision task, requiring diverse yet realistic depictions of the seen as well as unseen entities. However, existing research has been mostly limited to the generation of single objects, rather than comprehensive scenes comprising multiple elements. In response, this work introduces SVGCraft, a novel end-to-end framework for the creation of vector graphics depicting entire scenes from textual descriptions. Utilizing a pre-trained LLM for layout generation from text prompts, this framework introduces a technique for producing masked latents in specified bounding boxes for accurate object placement. It introduces a fusion mechanism for integrating attention maps and employs a diffusion U-Net for coherent composition, speeding up the drawing process. The resulting SVG is optimized using a pre-trained encoder and LPIPS loss with opacity modulation to maximize similarity. Additionally, this work explores the potential of primitive shapes in facilitating canvas completion in constrained environments. Through both qualitative and quantitative assessments, SVGCraft is demonstrated to surpass prior works in abstraction, recognizability, and detail, as evidenced by its performance metrics (CLIP-T: 0.4563, Cosine Similarity: 0.6342, Confusion: 0.66, Aesthetic: 6.7832). The code will be available at this https URL.</p>
  </details>
</details>
<details>
  <summary>173. <b>标题：3DGSR: Implicit Surface Reconstruction with 3D Gaussian Splatting</b></summary>
  <p><b>编号</b>：[472]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00409">https://arxiv.org/abs/2404.00409</a></p>
  <p><b>作者</b>：Xiaoyang Lyu,  Yang-Tian Sun,  Yi-Hua Huang,  Xiuzhe Wu,  Ziyi Yang,  Yilun Chen,  Jiangmiao Pang,  Xiaojuan Qi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Gaussian Splatting, Gaussians, SDF, inheriting the high, Splatting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we present an implicit surface reconstruction method with 3D Gaussian Splatting (3DGS), namely 3DGSR, that allows for accurate 3D reconstruction with intricate details while inheriting the high efficiency and rendering quality of 3DGS. The key insight is incorporating an implicit signed distance field (SDF) within 3D Gaussians to enable them to be aligned and jointly optimized. First, we introduce a differentiable SDF-to-opacity transformation function that converts SDF values into corresponding Gaussians' opacities. This function connects the SDF and 3D Gaussians, allowing for unified optimization and enforcing surface constraints on the 3D Gaussians. During learning, optimizing the 3D Gaussians provides supervisory signals for SDF learning, enabling the reconstruction of intricate details. However, this only provides sparse supervisory signals to the SDF at locations occupied by Gaussians, which is insufficient for learning a continuous SDF. Then, to address this limitation, we incorporate volumetric rendering and align the rendered geometric attributes (depth, normal) with those derived from 3D Gaussians. This consistency regularization introduces supervisory signals to locations not covered by discrete 3D Gaussians, effectively eliminating redundant surfaces outside the Gaussian sampling range. Our extensive experimental results demonstrate that our 3DGSR method enables high-quality 3D surface reconstruction while preserving the efficiency and rendering quality of 3DGS. Besides, our method competes favorably with leading surface reconstruction techniques while offering a more efficient learning process and much better rendering qualities. The code will be available at this https URL.</p>
  </details>
</details>
<details>
  <summary>174. <b>标题：Constrained Layout Generation with Factor Graphs</b></summary>
  <p><b>编号</b>：[487]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00385">https://arxiv.org/abs/2404.00385</a></p>
  <p><b>作者</b>：Mohammed Haroon Dupty,  Yanfei Dong,  Sicong Leng,  Guoji Fu,  Yong Liang Goh,  Wei Lu,  Wee Sun Lee</p>
  <p><b>备注</b>：To be published at IEEE/CVF CVPR 2024</p>
  <p><b>关键词</b>：multiple domains including, domains including floorplan, object-centric layout generation, design process, including floorplan design</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper addresses the challenge of object-centric layout generation under spatial constraints, seen in multiple domains including floorplan design process. The design process typically involves specifying a set of spatial constraints that include object attributes like size and inter-object relations such as relative positioning. Existing works, which typically represent objects as single nodes, lack the granularity to accurately model complex interactions between objects. For instance, often only certain parts of an object, like a room's right wall, interact with adjacent objects. To address this gap, we introduce a factor graph based approach with four latent variable nodes for each room, and a factor node for each constraint. The factor nodes represent dependencies among the variables to which they are connected, effectively capturing constraints that are potentially of a higher order. We then develop message-passing on the bipartite graph, forming a factor graph neural network that is trained to produce a floorplan that aligns with the desired requirements. Our approach is simple and generates layouts faithful to the user requirements, demonstrated by a large improvement in IOU scores over existing methods. Additionally, our approach, being inferential and accurate, is well-suited to the practical human-in-the-loop design process where specifications evolve iteratively, offering a practical and powerful tool for AI-guided design.</p>
  </details>
</details>
<details>
  <summary>175. <b>标题：TTD: Text-Tag Self-Distillation Enhancing Image-Text Alignment in CLIP  to Alleviate Single Tag Bias</b></summary>
  <p><b>编号</b>：[488]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00384">https://arxiv.org/abs/2404.00384</a></p>
  <p><b>作者</b>：Sanghyun Jo,  Soohyun Ryu,  Sungyub Kim,  Eunho Yang,  Kyungsu Kim</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：contemporary CLIP-based models, identify a critical, contemporary CLIP-based, CLIP text embeddings, single tag bias</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We identify a critical bias in contemporary CLIP-based models, which we denote as \textit{single tag bias}. This bias manifests as a disproportionate focus on a singular tag (word) while neglecting other pertinent tags, stemming from CLIP's text embeddings that prioritize one specific tag in image-text relationships. When deconstructing text into individual tags, only one tag tends to have high relevancy with CLIP's image embedding, leading to an imbalanced tag relevancy. This results in an uneven alignment among multiple tags present in the text. To tackle this challenge, we introduce a novel two-step fine-tuning approach. First, our method leverages the similarity between tags and their nearest pixels for scoring, enabling the extraction of image-relevant tags from the text. Second, we present a self-distillation strategy aimed at aligning the combined masks from extracted tags with the text-derived mask. This approach mitigates the single tag bias, thereby significantly improving the alignment of CLIP's model without necessitating additional data or supervision. Our technique demonstrates model-agnostic improvements in multi-tag classification and segmentation tasks, surpassing competing methods that rely on external resources. Code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>176. <b>标题：DHR: Dual Features-Driven Hierarchical Rebalancing in Inter- and  Intra-Class Regions for Weakly-Supervised Semantic Segmentation</b></summary>
  <p><b>编号</b>：[490]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00380">https://arxiv.org/abs/2404.00380</a></p>
  <p><b>作者</b>：Sanghyun Jo,  Fei Pan,  In-Jae Yu,  Kyungsu Kim</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ensures high-quality segmentation, large-scale vision models, Weakly-supervised semantic segmentation, input seed masks, ensures high-quality</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Weakly-supervised semantic segmentation (WSS) ensures high-quality segmentation with limited data and excels when employed as input seed masks for large-scale vision models such as Segment Anything. However, WSS faces challenges related to minor classes since those are overlooked in images with adjacent multiple classes, a limitation originating from the overfitting of traditional expansion methods like Random Walk. We first address this by employing unsupervised and weakly-supervised feature maps instead of conventional methodologies, allowing for hierarchical mask enhancement. This method distinctly categorizes higher-level classes and subsequently separates their associated lower-level classes, ensuring all classes are correctly restored in the mask without losing minor ones. Our approach, validated through extensive experimentation, significantly improves WSS across five benchmarks (VOC: 79.8\%, COCO: 53.9\%, Context: 49.0\%, ADE: 32.9\%, Stuff: 37.4\%), reducing the gap with fully supervised methods by over 84\% on the VOC validation set. Code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>177. <b>标题：The Devil is in the Edges: Monocular Depth Estimation with Edge-aware  Consistency Fusion</b></summary>
  <p><b>编号</b>：[492]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00373">https://arxiv.org/abs/2404.00373</a></p>
  <p><b>作者</b>：Pengzhi Li,  Yikang Ding,  Haohan Wang,  Chengshuai Tang,  Zhiheng Li</p>
  <p><b>备注</b>：17 pages, 19 figures</p>
  <p><b>关键词</b>：estimating high-quality monocular, single RGB image, high-quality monocular depth, single RGB, monocular depth estimation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a novel monocular depth estimation method, named ECFNet, for estimating high-quality monocular depth with clear edges and valid overall structure from a single RGB image. We make a thorough inquiry about the key factor that affects the edge depth estimation of the MDE networks, and come to a ratiocination that the edge information itself plays a critical role in predicting depth details. Driven by this analysis, we propose to explicitly employ the image edges as input for ECFNet and fuse the initial depths from different sources to produce the final depth. Specifically, ECFNet first uses a hybrid edge detection strategy to get the edge map and edge-highlighted image from the input image, and then leverages a pre-trained MDE network to infer the initial depths of the aforementioned three images. After that, ECFNet utilizes a layered fusion module (LFM) to fuse the initial depth, which will be further updated by a depth consistency module (DCM) to form the final estimation. Extensive experimental results on public datasets and ablation studies indicate that our method achieves state-of-the-art performance. Project page: this https URL.</p>
  </details>
</details>
<details>
  <summary>178. <b>标题：Towards Variable and Coordinated Holistic Co-Speech Motion Generation</b></summary>
  <p><b>编号</b>：[496]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00368">https://arxiv.org/abs/2404.00368</a></p>
  <p><b>作者</b>：Yifei Liu,  Qiong Cao,  Yandong Wen,  Huaiguang Jiang,  Changxing Ding</p>
  <p><b>备注</b>：CVPR 2024</p>
  <p><b>关键词</b>：generating lifelike holistic, key aspects, paper addresses, addresses the problem, problem of generating</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper addresses the problem of generating lifelike holistic co-speech motions for 3D avatars, focusing on two key aspects: variability and coordination. Variability allows the avatar to exhibit a wide range of motions even with similar speech content, while coordination ensures a harmonious alignment among facial expressions, hand gestures, and body poses. We aim to achieve both with ProbTalk, a unified probabilistic framework designed to jointly model facial, hand, and body movements in speech. ProbTalk builds on the variational autoencoder (VAE) architecture and incorporates three core designs. First, we introduce product quantization (PQ) to the VAE, which enriches the representation of complex holistic motion. Second, we devise a novel non-autoregressive model that embeds 2D positional encoding into the product-quantized representation, thereby preserving essential structure information of the PQ codes. Last, we employ a secondary stage to refine the preliminary prediction, further sharpening the high-frequency details. Coupling these three designs enables ProbTalk to generate natural and diverse holistic co-speech motions, outperforming several state-of-the-art methods in qualitative and quantitative evaluations, particularly in terms of realism. Our code and model will be released for research purposes at this https URL.</p>
  </details>
</details>
<details>
  <summary>179. <b>标题：Efficient Multi-branch Segmentation Network for Situation Awareness in  Autonomous Navigation</b></summary>
  <p><b>编号</b>：[498]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00366">https://arxiv.org/abs/2404.00366</a></p>
  <p><b>作者</b>：Guan-Cheng Zhou,  Chen Chengb,  Yan-zhou Chena</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：high-precision situational awareness, situational awareness technology, unmanned surface vehicles, Real-time and high-precision, high-precision situational</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Real-time and high-precision situational awareness technology is critical for autonomous navigation of unmanned surface vehicles (USVs). In particular, robust and fast obstacle semantic segmentation methods are essential. However, distinguishing between the sea and the sky is challenging due to the differences between port and maritime environments. In this study, we built a dataset that captured perspectives from USVs and unmanned aerial vehicles in a maritime port environment and analysed the data features. Statistical analysis revealed a high correlation between the distribution of the sea and sky and row positional information. Based on this finding, a three-branch semantic segmentation network with a row position encoding module (RPEM) was proposed to improve the prediction accuracy between the sea and the sky. The proposed RPEM highlights the effect of row coordinates on feature extraction. Compared to the baseline, the three-branch network with RPEM significantly improved the ability to distinguish between the sea and the sky without significantly reducing the computational speed.</p>
  </details>
</details>
<details>
  <summary>180. <b>标题：STBA: Towards Evaluating the Robustness of DNNs for Query-Limited  Black-box Scenario</b></summary>
  <p><b>编号</b>：[500]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00362">https://arxiv.org/abs/2404.00362</a></p>
  <p><b>作者</b>：Renyang Liu,  Kwok-Yan Lam,  Wei Zhou,  Sixing Wu,  Jun Zhao,  Dongting Hu,  Mingming Gong</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：proposed to explore, Transform Black-box Attack, attack techniques, black-box attack, attack</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Many attack techniques have been proposed to explore the vulnerability of DNNs and further help to improve their robustness. Despite the significant progress made recently, existing black-box attack methods still suffer from unsatisfactory performance due to the vast number of queries needed to optimize desired perturbations. Besides, the other critical challenge is that adversarial examples built in a noise-adding manner are abnormal and struggle to successfully attack robust models, whose robustness is enhanced by adversarial training against small perturbations. There is no doubt that these two issues mentioned above will significantly increase the risk of exposure and result in a failure to dig deeply into the vulnerability of DNNs. Hence, it is necessary to evaluate DNNs' fragility sufficiently under query-limited settings in a non-additional way. In this paper, we propose the Spatial Transform Black-box Attack (STBA), a novel framework to craft formidable adversarial examples in the query-limited scenario. Specifically, STBA introduces a flow field to the high-frequency part of clean images to generate adversarial examples and adopts the following two processes to enhance their naturalness and significantly improve the query efficiency: a) we apply an estimated flow field to the high-frequency part of clean images to generate adversarial examples instead of introducing external noise to the benign image, and b) we leverage an efficient gradient estimation method based on a batch of samples to optimize such an ideal flow field under query-limited settings. Compared to existing score-based black-box baselines, extensive experiments indicated that STBA could effectively improve the imperceptibility of the adversarial examples and remarkably boost the attack success rate under query-limited settings.</p>
  </details>
</details>
<details>
  <summary>181. <b>标题：Reusable Architecture Growth for Continual Stereo Matching</b></summary>
  <p><b>编号</b>：[502]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00360">https://arxiv.org/abs/2404.00360</a></p>
  <p><b>作者</b>：Chenghao Zhang,  Gaofeng Meng,  Bin Fan,  Kun Tian,  Zhaoxiang Zhang,  Shiming Xiang,  Chunhong Pan</p>
  <p><b>备注</b>：Extended version of CVPR 2022 paper "Continual Stereo Matching of Continuous Driving Scenes with Growing Architecture" - Accepted to TPAMI in 2024</p>
  <p><b>关键词</b>：regress dense disparity, depth estimation models, estimation models benefits, recent stereo depth, stereo depth estimation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The remarkable performance of recent stereo depth estimation models benefits from the successful use of convolutional neural networks to regress dense disparity. Akin to most tasks, this needs gathering training data that covers a number of heterogeneous scenes at deployment time. However, training samples are typically acquired continuously in practical applications, making the capability to learn new scenes continually even more crucial. For this purpose, we propose to perform continual stereo matching where a model is tasked to 1) continually learn new scenes, 2) overcome forgetting previously learned scenes, and 3) continuously predict disparities at inference. We achieve this goal by introducing a Reusable Architecture Growth (RAG) framework. RAG leverages task-specific neural unit search and architecture growth to learn new scenes continually in both supervised and self-supervised manners. It can maintain high reusability during growth by reusing previous units while obtaining good performance. Additionally, we present a Scene Router module to adaptively select the scene-specific architecture path at inference. Comprehensive experiments on numerous datasets show that our framework performs impressively in various weather, road, and city circumstances and surpasses the state-of-the-art methods in more challenging cross-dataset settings. Further experiments also demonstrate the adaptability of our method to unseen scenes, which can facilitate end-to-end stereo architecture learning and practical deployment.</p>
  </details>
</details>
<details>
  <summary>182. <b>标题：Spread Your Wings: A Radial Strip Transformer for Image Deblurring</b></summary>
  <p><b>编号</b>：[503]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00358">https://arxiv.org/abs/2404.00358</a></p>
  <p><b>作者</b>：Duosheng Chen,  Shihao Zhou,  Jinshan Pan,  Jinglei Shi,  Lishen Qu,  Jufeng Yang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：window-based transformer approaches, Exploring motion information, motion, Radial Strip Transformer, Radial Strip</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Exploring motion information is important for the motion deblurring task. Recent the window-based transformer approaches have achieved decent performance in image deblurring. Note that the motion causing blurry results is usually composed of translation and rotation movements and the window-shift operation in the Cartesian coordinate system by the window-based transformer approaches only directly explores translation motion in orthogonal directions. Thus, these methods have the limitation of modeling the rotation part. To alleviate this problem, we introduce the polar coordinate-based transformer, which has the angles and distance to explore rotation motion and translation information together. In this paper, we propose a Radial Strip Transformer (RST), which is a transformer-based architecture that restores the blur images in a polar coordinate system instead of a Cartesian one. RST contains a dynamic radial embedding module (DRE) to extract the shallow feature by a radial deformable convolution. We design a polar mask layer to generate the offsets for the deformable convolution, which can reshape the convolution kernel along the radius to better capture the rotation motion information. Furthermore, we proposed a radial strip attention solver (RSAS) as deep feature extraction, where the relationship of windows is organized by azimuth and radius. This attention module contains radial strip windows to reweight image features in the polar coordinate, which preserves more useful information in rotation and translation motion together for better recovering the sharp images. Experimental results on six synthesis and real-world datasets prove that our method performs favorably against other SOTA methods for the image deblurring task.</p>
  </details>
</details>
<details>
  <summary>183. <b>标题：Rethinking Attention-Based Multiple Instance Learning for Whole-Slide  Pathological Image Classification: An Instance Attribute Viewpoint</b></summary>
  <p><b>编号</b>：[508]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00351">https://arxiv.org/abs/2404.00351</a></p>
  <p><b>作者</b>：Linghan Cai,  Shenjin Huang,  Ye Zhang,  Jinpeng Lu,  Yongbing Zhang</p>
  <p><b>备注</b>：10 pages, 8 figures</p>
  <p><b>关键词</b>：processing gigapixel-resolution images, whole-slide pathological image, gigapixel-resolution images, processing gigapixel-resolution, slide-level labels</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multiple instance learning (MIL) is a robust paradigm for whole-slide pathological image (WSI) analysis, processing gigapixel-resolution images with slide-level labels. As pioneering efforts, attention-based MIL (ABMIL) and its variants are increasingly becoming popular due to the characteristics of simultaneously handling clinical diagnosis and tumor localization. However, the attention mechanism exhibits limitations in discriminating between instances, which often misclassifies tissues and potentially impairs MIL performance. This paper proposes an Attribute-Driven MIL (AttriMIL) framework to address these issues. Concretely, we dissect the calculation process of ABMIL and present an attribute scoring mechanism that measures the contribution of each instance to bag prediction effectively, quantifying instance attributes. Based on attribute quantification, we develop a spatial attribute constraint and an attribute ranking constraint to model instance correlations within and across slides, respectively. These constraints encourage the network to capture the spatial correlation and semantic similarity of instances, improving the ability of AttriMIL to distinguish tissue types and identify challenging instances. Additionally, AttriMIL employs a histopathology adaptive backbone that maximizes the pre-trained model's feature extraction capability for collecting pathological features. Extensive experiments on three public benchmarks demonstrate that our AttriMIL outperforms existing state-of-the-art frameworks across multiple evaluation metrics. The implementation code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>184. <b>标题：SGDFormer: One-stage Transformer-based Architecture for Cross-Spectral  Stereo Image Guided Denoising</b></summary>
  <p><b>编号</b>：[510]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00349">https://arxiv.org/abs/2404.00349</a></p>
  <p><b>作者</b>：Runmin Zhang,  Zhu Yu,  Zehua Sheng,  Jiacheng Ying,  Si-Yuan Cao,  Shu-Jie Chen,  Bailin Yang,  Junwei Li,  Hui-Liang Shen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：recovering clean images, image guided denoising, rich details, shown its great, great potential</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cross-spectral image guided denoising has shown its great potential in recovering clean images with rich details, such as using the near-infrared image to guide the denoising process of the visible one. To obtain such image pairs, a feasible and economical way is to employ a stereo system, which is widely used on mobile devices. Current works attempt to generate an aligned guidance image to handle the disparity between two images. However, due to occlusion, spectral differences and noise degradation, the aligned guidance image generally exists ghosting and artifacts, leading to an unsatisfactory denoised result. To address this issue, we propose a one-stage transformer-based architecture, named SGDFormer, for cross-spectral Stereo image Guided Denoising. The architecture integrates the correspondence modeling and feature fusion of stereo images into a unified network. Our transformer block contains a noise-robust cross-attention (NRCA) module and a spatially variant feature fusion (SVFF) module. The NRCA module captures the long-range correspondence of two images in a coarse-to-fine manner to alleviate the interference of noise. The SVFF module further enhances salient structures and suppresses harmful artifacts through dynamically selecting useful information. Thanks to the above design, our SGDFormer can restore artifact-free images with fine structures, and achieves state-of-the-art performance on various datasets. Additionally, our SGDFormer can be extended to handle other unaligned cross-model guided restoration tasks such as guided depth super-resolution.</p>
  </details>
</details>
<details>
  <summary>185. <b>标题：MaGRITTe: Manipulative and Generative 3D Realization from Image, Topview  and Text</b></summary>
  <p><b>编号</b>：[512]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00345">https://arxiv.org/abs/2404.00345</a></p>
  <p><b>作者</b>：Takayuki Hara,  Tatsuya Harada</p>
  <p><b>备注</b>：Project Page: this https URL</p>
  <p><b>关键词</b>：user-specified conditions offers, conditions, offers a promising, promising avenue, avenue for alleviating</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The generation of 3D scenes from user-specified conditions offers a promising avenue for alleviating the production burden in 3D applications. Previous studies required significant effort to realize the desired scene, owing to limited control conditions. We propose a method for controlling and generating 3D scenes under multimodal conditions using partial images, layout information represented in the top view, and text prompts. Combining these conditions to generate a 3D scene involves the following significant difficulties: (1) the creation of large datasets, (2) reflection on the interaction of multimodal conditions, and (3) domain dependence of the layout conditions. We decompose the process of 3D scene generation into 2D image generation from the given conditions and 3D scene generation from 2D images. 2D image generation is achieved by fine-tuning a pretrained text-to-image model with a small artificial dataset of partial images and layouts, and 3D scene generation is achieved by layout-conditioned depth estimation and neural radiance fields (NeRF), thereby avoiding the creation of large datasets. The use of a common representation of spatial information using 360-degree images allows for the consideration of multimodal condition interactions and reduces the domain dependence of the layout control. The experimental results qualitatively and quantitatively demonstrated that the proposed method can generate 3D scenes in diverse domains, from indoor to outdoor, according to multimodal conditions.</p>
  </details>
</details>
<details>
  <summary>186. <b>标题：Learing Trimaps via Clicks for Image Matting</b></summary>
  <p><b>编号</b>：[518]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00335">https://arxiv.org/abs/2404.00335</a></p>
  <p><b>作者</b>：Chenyi Zhang,  Yihan Hu,  Henghui Ding,  Humphrey Shi,  Yao Zhao,  Yunchao Wei</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：models heavily depend, significant advancements, heavily depend, depend on manually-drawn, accurate results</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite significant advancements in image matting, existing models heavily depend on manually-drawn trimaps for accurate results in natural image scenarios. However, the process of obtaining trimaps is time-consuming, lacking user-friendliness and device compatibility. This reliance greatly limits the practical application of all trimap-based matting methods. To address this issue, we introduce Click2Trimap, an interactive model capable of predicting high-quality trimaps and alpha mattes with minimal user click inputs. Through analyzing real users' behavioral logic and characteristics of trimaps, we successfully propose a powerful iterative three-class training strategy and a dedicated simulation function, making Click2Trimap exhibit versatility across various scenarios. Quantitative and qualitative assessments on synthetic and real-world matting datasets demonstrate Click2Trimap's superior performance compared to all existing trimap-free matting methods. Especially, in the user study, Click2Trimap achieves high-quality trimap and matting predictions in just an average of 5 seconds per image, demonstrating its substantial practical value in real-world applications.</p>
  </details>
</details>
<details>
  <summary>187. <b>标题：Memory-Scalable and Simplified Functional Map Learning</b></summary>
  <p><b>编号</b>：[520]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00330">https://arxiv.org/abs/2404.00330</a></p>
  <p><b>作者</b>：Robin Magnet,  Maks Ovsjanikov</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：shape matching problems, prominent learning-based framework, non-rigid shape matching, matching problems, Deep functional maps</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep functional maps have emerged in recent years as a prominent learning-based framework for non-rigid shape matching problems. While early methods in this domain only focused on learning in the functional domain, the latest techniques have demonstrated that by promoting consistency between functional and pointwise maps leads to significant improvements in accuracy. Unfortunately, existing approaches rely heavily on the computation of large dense matrices arising from soft pointwise maps, which compromises their efficiency and scalability. To address this limitation, we introduce a novel memory-scalable and efficient functional map learning pipeline. By leveraging the specific structure of functional maps, we offer the possibility to achieve identical results without ever storing the pointwise map in memory. Furthermore, based on the same approach, we present a differentiable map refinement layer adapted from an existing axiomatic refinement algorithm. Unlike many functional map learning methods, which use this algorithm at a post-processing step, ours can be easily used at train time, enabling to enforce consistency between the refined and initial versions of the map. Our resulting approach is both simpler, more efficient and more numerically stable, by avoiding differentiation through a linear system, while achieving close to state-of-the-art results in challenging scenarios.</p>
  </details>
</details>
<details>
  <summary>188. <b>标题：CLIP-driven Outliers Synthesis for few-shot OOD detection</b></summary>
  <p><b>编号</b>：[522]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00323">https://arxiv.org/abs/2404.00323</a></p>
  <p><b>作者</b>：Hao Sun,  Rundong He,  Zhongyi Han,  Zhicong Lin,  Yongshun Gong,  Yilong Yin</p>
  <p><b>备注</b>：9 pages,5 figures</p>
  <p><b>关键词</b>：OOD, OOD detection focuses, focuses on recognizing, unseen during training, small number</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Few-shot OOD detection focuses on recognizing out-of-distribution (OOD) images that belong to classes unseen during training, with the use of only a small number of labeled in-distribution (ID) images. Up to now, a mainstream strategy is based on large-scale vision-language models, such as CLIP. However, these methods overlook a crucial issue: the lack of reliable OOD supervision information, which can lead to biased boundaries between in-distribution (ID) and OOD. To tackle this problem, we propose CLIP-driven Outliers Synthesis~(CLIP-OS). Firstly, CLIP-OS enhances patch-level features' perception by newly proposed patch uniform convolution, and adaptively obtains the proportion of ID-relevant information by employing CLIP-surgery-discrepancy, thus achieving separation between ID-relevant and ID-irrelevant. Next, CLIP-OS synthesizes reliable OOD data by mixing up ID-relevant features from different classes to provide OOD supervision information. Afterward, CLIP-OS leverages synthetic OOD samples by unknown-aware prompt learning to enhance the separability of ID and OOD. Extensive experiments across multiple benchmarks demonstrate that CLIP-OS achieves superior few-shot OOD detection capability.</p>
  </details>
</details>
<details>
  <summary>189. <b>标题：Instrument-tissue Interaction Detection Framework for Surgical Video  Understanding</b></summary>
  <p><b>编号</b>：[523]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00322">https://arxiv.org/abs/2404.00322</a></p>
  <p><b>作者</b>：Wenjun Lin,  Yan Hu,  Huazhu Fu,  Mingming Yang,  Chin-Boon Chng,  Ryo Kawasaki,  Cheekong Chui,  Jiang Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Instrument-tissue interaction detection, understand surgical activities, interaction detection task, Instrument-tissue interaction, represent instrument-tissue interaction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Instrument-tissue interaction detection task, which helps understand surgical activities, is vital for constructing computer-assisted surgery systems but with many challenges. Firstly, most models represent instrument-tissue interaction in a coarse-grained way which only focuses on classification and lacks the ability to automatically detect instruments and tissues. Secondly, existing works do not fully consider relations between intra- and inter-frame of instruments and tissues. In the paper, we propose to represent instrument-tissue interaction as <instrument class, instrument bounding box, tissue action class> quintuple and present an Instrument-Tissue Interaction Detection Network (ITIDNet) to detect the quintuple for surgery videos understanding. Specifically, we propose a Snippet Consecutive Feature (SCF) Layer to enhance features by modeling relationships of proposals in the current frame using global context information in the video snippet. We also propose a Spatial Corresponding Attention (SCA) Layer to incorporate features of proposals between adjacent frames through spatial encoding. To reason relationships between instruments and tissues, a Temporal Graph (TG) Layer is proposed with intra-frame connections to exploit relationships between instruments and tissues in the same frame and inter-frame connections to model the temporal information for the same instance. For evaluation, we build a cataract surgery video (PhacoQ) dataset and a cholecystectomy surgery video (CholecQ) dataset. Experimental results demonstrate the promising performance of our model, which outperforms other state-of-the-art models on both datasets.</instrument></p>
  </details>
</details>
<details>
  <summary>190. <b>标题：Exploring Unseen Environments with Robots using Large Language and  Vision Models through a Procedurally Generated 3D Scene Representation</b></summary>
  <p><b>编号</b>：[525]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00318">https://arxiv.org/abs/2404.00318</a></p>
  <p><b>作者</b>：Arjun P S,  Andrew Melnik,  Gora Chand Nandi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Generative Artificial Intelligence, Large Language Models, Vision Language Models, Large Vision Language, Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advancements in Generative Artificial Intelligence, particularly in the realm of Large Language Models (LLMs) and Large Vision Language Models (LVLMs), have enabled the prospect of leveraging cognitive planners within robotic systems. This work focuses on solving the object goal navigation problem by mimicking human cognition to attend, perceive and store task specific information and generate plans with the same. We introduce a comprehensive framework capable of exploring an unfamiliar environment in search of an object by leveraging the capabilities of Large Language Models(LLMs) and Large Vision Language Models (LVLMs) in understanding the underlying semantics of our world. A challenging task in using LLMs to generate high level sub-goals is to efficiently represent the environment around the robot. We propose to use a 3D scene modular representation, with semantically rich descriptions of the object, to provide the LLM with task relevant information. But providing the LLM with a mass of contextual information (rich 3D scene semantic representation), can lead to redundant and inefficient plans. We propose to use an LLM based pruner that leverages the capabilities of in-context learning to prune out irrelevant goal specific information.</p>
  </details>
</details>
<details>
  <summary>191. <b>标题：Harmonizing Light and Darkness: A Symphony of Prior-guided Data  Synthesis and Adaptive Focus for Nighttime Flare Removal</b></summary>
  <p><b>编号</b>：[526]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00313">https://arxiv.org/abs/2404.00313</a></p>
  <p><b>作者</b>：Lishen Qu,  Shihao Zhou,  Jinshan Pan,  Jinglei Shi,  Duosheng Chen,  Jufeng Yang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Intense light sources, affects downstream applications, negatively affects downstream, Intense light, downstream applications</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Intense light sources often produce flares in captured images at night, which deteriorates the visual quality and negatively affects downstream applications. In order to train an effective flare removal network, a reliable dataset is essential. The mainstream flare removal datasets are semi-synthetic to reduce human labour, but these datasets do not cover typical scenarios involving multiple scattering flares. To tackle this issue, we synthesize a prior-guided dataset named Flare7K*, which contains multi-flare images where the brightness of flares adheres to the laws of illumination. Besides, flares tend to occupy localized regions of the image but existing networks perform flare removal on the entire image and sometimes modify clean areas incorrectly. Therefore, we propose a plug-and-play Adaptive Focus Module (AFM) that can adaptively mask the clean background areas and assist models in focusing on the regions severely affected by flares. Extensive experiments demonstrate that our data synthesis method can better simulate real-world scenes and several models equipped with AFM achieve state-of-the-art performance on the real-world test dataset.</p>
  </details>
</details>
<details>
  <summary>192. <b>标题：Bayesian Exploration of Pre-trained Models for Low-shot Image  Classification</b></summary>
  <p><b>编号</b>：[527]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00312">https://arxiv.org/abs/2404.00312</a></p>
  <p><b>作者</b>：Yibo Miao,  Yu Lei,  Feng Zhou,  Zhijie Deng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large-scale vision-language models, Low-shot image classification, Low-shot image, computer vision, fundamental task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Low-shot image classification is a fundamental task in computer vision, and the emergence of large-scale vision-language models such as CLIP has greatly advanced the forefront of research in this field. However, most existing CLIP-based methods lack the flexibility to effectively incorporate other pre-trained models that encompass knowledge distinct from CLIP. To bridge the gap, this work proposes a simple and effective probabilistic model ensemble framework based on Gaussian processes, which have previously demonstrated remarkable efficacy in processing small data. We achieve the integration of prior knowledge by specifying the mean function with CLIP and the kernel function with an ensemble of deep kernels built upon various pre-trained models. By regressing the classification label directly, our framework enables analytical inference, straightforward uncertainty quantification, and principled hyper-parameter tuning. Through extensive experiments on standard benchmarks, we demonstrate that our method consistently outperforms competitive ensemble baselines regarding predictive performance. Additionally, we assess the robustness of our method and the quality of the yielded uncertainty estimates on out-of-distribution datasets. We also illustrate that our method, despite relying on label regression, still enjoys superior model calibration compared to most deterministic baselines.</p>
  </details>
</details>
<details>
  <summary>193. <b>标题：ST-LLM: Large Language Models Are Effective Temporal Learners</b></summary>
  <p><b>编号</b>：[530]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00308">https://arxiv.org/abs/2404.00308</a></p>
  <p><b>作者</b>：Ruyang Liu,  Chen Li,  Haoran Tang,  Yixiao Ge,  Ying Shan,  Ge Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, prompting research efforts, showcased impressive capabilities, facilitate human-AI interaction, Large Language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) have showcased impressive capabilities in text comprehension and generation, prompting research efforts towards video LLMs to facilitate human-AI interaction at the video level. However, how to effectively encode and understand videos in video-based dialogue systems remains to be solved. In this paper, we investigate a straightforward yet unexplored question: Can we feed all spatial-temporal tokens into the LLM, thus delegating the task of video sequence modeling to the LLMs? Surprisingly, this simple approach yields significant improvements in video understanding. Based upon this, we propose ST-LLM, an effective video-LLM baseline with Spatial-Temporal sequence modeling inside LLM. Furthermore, to address the overhead and stability issues introduced by uncompressed video tokens within LLMs, we develop a dynamic masking strategy with tailor-made training objectives. For particularly long videos, we have also designed a global-local input module to balance efficiency and effectiveness. Consequently, we harness LLM for proficient spatial-temporal modeling, while upholding efficiency and stability. Extensive experimental results attest to the effectiveness of our method. Through a more concise model and training pipeline, ST-LLM establishes a new state-of-the-art result on VideoChatGPT-Bench and MVBench. Codes have been available at this https URL.</p>
  </details>
</details>
<details>
  <summary>194. <b>标题：Monocular Identity-Conditioned Facial Reflectance Reconstruction</b></summary>
  <p><b>编号</b>：[533]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00301">https://arxiv.org/abs/2404.00301</a></p>
  <p><b>作者</b>：Xingyu Ren,  Jiankang Deng,  Yuhao Cheng,  Jia Guo,  Chao Ma,  Yichao Yan,  Wenhan Zhu,  Xiaokang Yang</p>
  <p><b>备注</b>：Accepted by CVPR 2024</p>
  <p><b>关键词</b>：made remarkable advancements, remain huge challenges, reflectance, remarkable advancements, made remarkable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent 3D face reconstruction methods have made remarkable advancements, yet there remain huge challenges in monocular high-quality facial reflectance reconstruction. Existing methods rely on a large amount of light-stage captured data to learn facial reflectance models. However, the lack of subject diversity poses challenges in achieving good generalization and widespread applicability. In this paper, we learn the reflectance prior in image space rather than UV space and present a framework named ID2Reflectance. Our framework can directly estimate the reflectance maps of a single image while using limited reflectance data for training. Our key insight is that reflectance data shares facial structures with RGB faces, which enables obtaining expressive facial prior from inexpensive RGB data thus reducing the dependency on reflectance data. We first learn a high-quality prior for facial reflectance. Specifically, we pretrain multi-domain facial feature codebooks and design a codebook fusion method to align the reflectance and RGB domains. Then, we propose an identity-conditioned swapping module that injects facial identity from the target image into the pre-trained autoencoder to modify the identity of the source reflectance image. Finally, we stitch multi-view swapped reflectance images to obtain renderable assets. Extensive experiments demonstrate that our method exhibits excellent generalization capability and achieves state-of-the-art facial reflectance reconstruction results for in-the-wild faces. Our project page is this https URL.</p>
  </details>
</details>
<details>
  <summary>195. <b>标题：HOI-M3:Capture Multiple Humans and Objects Interaction within Contextual  Environment</b></summary>
  <p><b>编号</b>：[535]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00299">https://arxiv.org/abs/2404.00299</a></p>
  <p><b>作者</b>：Juze Zhang,  Jingyan Zhang,  Zining Song,  Zhanhe Shi,  Chengfeng Zhao,  Ye Shi,  Jingyi Yu,  Lan Xu,  Jingya Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Humans naturally interact, surrounding multiple objects, multiple human-object interactions, naturally interact, multiple objects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Humans naturally interact with both others and the surrounding multiple objects, engaging in various social activities. However, recent advances in modeling human-object interactions mostly focus on perceiving isolated individuals and objects, due to fundamental data scarcity. In this paper, we introduce HOI-M3, a novel large-scale dataset for modeling the interactions of Multiple huMans and Multiple objects. Notably, it provides accurate 3D tracking for both humans and objects from dense RGB and object-mounted IMU inputs, covering 199 sequences and 181M frames of diverse humans and objects under rich activities. With the unique HOI-M3 dataset, we introduce two novel data-driven tasks with companion strong baselines: monocular capture and unstructured generation of multiple human-object interactions. Extensive experiments demonstrate that our dataset is challenging and worthy of further research about multiple human-object interactions and behavior analysis. Our HOI-M3 dataset, corresponding codes, and pre-trained models will be disseminated to the community for future research.</p>
  </details>
</details>
<details>
  <summary>196. <b>标题：LAKE-RED: Camouflaged Images Generation by Latent Background Knowledge  Retrieval-Augmented Diffusion</b></summary>
  <p><b>编号</b>：[537]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00292">https://arxiv.org/abs/2404.00292</a></p>
  <p><b>作者</b>：Pancheng Zhao,  Peng Xu,  Pengda Qin,  Deng-Ping Fan,  Zhicheng Zhang,  Guoli Jia,  Bowen Zhou,  Jufeng Yang</p>
  <p><b>备注</b>：Accepted by CVPR 2024</p>
  <p><b>关键词</b>：numerous practical applications, important vision task, practical applications, task with numerous, numerous practical</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Camouflaged vision perception is an important vision task with numerous practical applications. Due to the expensive collection and labeling costs, this community struggles with a major bottleneck that the species category of its datasets is limited to a small number of object species. However, the existing camouflaged generation methods require specifying the background manually, thus failing to extend the camouflaged sample diversity in a low-cost manner. In this paper, we propose a Latent Background Knowledge Retrieval-Augmented Diffusion (LAKE-RED) for camouflaged image generation. To our knowledge, our contributions mainly include: (1) For the first time, we propose a camouflaged generation paradigm that does not need to receive any background inputs. (2) Our LAKE-RED is the first knowledge retrieval-augmented method with interpretability for camouflaged generation, in which we propose an idea that knowledge retrieval and reasoning enhancement are separated explicitly, to alleviate the task-specific challenges. Moreover, our method is not restricted to specific foreground targets or backgrounds, offering a potential for extending camouflaged vision perception to more diverse domains. (3) Experimental results demonstrate that our method outperforms the existing approaches, generating more realistic camouflage images.</p>
  </details>
</details>
<details>
  <summary>197. <b>标题：Seeing the Unseen: A Frequency Prompt Guided Transformer for Image  Restoration</b></summary>
  <p><b>编号</b>：[538]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00288">https://arxiv.org/abs/2404.00288</a></p>
  <p><b>作者</b>：Shihao Zhou,  Jinshan Pan,  Jinglei Shi,  Duosheng Chen,  Lishen Qu,  Jufeng Yang</p>
  <p><b>备注</b>：18 pages, 10 figrues</p>
  <p><b>关键词</b>：deep image restoration, solve image restoration, image restoration, image restoration models, Prompting image restoration</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>How to explore useful features from images as prompts to guide the deep image restoration models is an effective way to solve image restoration. In contrast to mining spatial relations within images as prompt, which leads to characteristics of different frequencies being neglected and further remaining subtle or undetectable artifacts in the restored image, we develop a Frequency Prompting image restoration method, dubbed FPro, which can effectively provide prompt components from a frequency perspective to guild the restoration model address these differences. Specifically, we first decompose input features into separate frequency parts via dynamically learned filters, where we introduce a gating mechanism for suppressing the less informative elements within the kernels. To propagate useful frequency information as prompt, we then propose a dual prompt block, consisting of a low-frequency prompt modulator (LPM) and a high-frequency prompt modulator (HPM), to handle signals from different bands respectively. Each modulator contains a generation process to incorporate prompting components into the extracted frequency maps, and a modulation part that modifies the prompt feature with the guidance of the decoder features. Experimental results on commonly used benchmarks have demonstrated the favorable performance of our pipeline against SOTA methods on 5 image restoration tasks, including deraining, deraindrop, demoiréing, deblurring, and dehazing. The source code and pre-trained models will be available at this https URL.</p>
  </details>
</details>
<details>
  <summary>198. <b>标题：Long-Tailed Recognition on Binary Networks by Calibrating A Pre-trained  Model</b></summary>
  <p><b>编号</b>：[540]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00285">https://arxiv.org/abs/2404.00285</a></p>
  <p><b>作者</b>：Jihun Kim,  Dahyun Kim,  Hyungrok Jung,  Taeil Oh,  Jonghyun Choi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：real-world scenarios entails, Deploying deep models, including computational efficiency, Deploying deep, real-world scenarios</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deploying deep models in real-world scenarios entails a number of challenges, including computational efficiency and real-world (e.g., long-tailed) data distributions. We address the combined challenge of learning long-tailed distributions using highly resource-efficient binary neural networks as backbones. Specifically, we propose a calibrate-and-distill framework that uses off-the-shelf pretrained full-precision models trained on balanced datasets to use as teachers for distillation when learning binary networks on long-tailed datasets. To better generalize to various datasets, we further propose a novel adversarial balancing among the terms in the objective function and an efficient multiresolution learning scheme. We conducted the largest empirical study in the literature using 15 datasets, including newly derived long-tailed datasets from existing balanced datasets, and show that our proposed method outperforms prior art by large margins (>14.33% on average).</p>
  </details>
</details>
<details>
  <summary>199. <b>标题：Look-Around Before You Leap: High-Frequency Injected Transformer for  Image Restoration</b></summary>
  <p><b>编号</b>：[543]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00279">https://arxiv.org/abs/2404.00279</a></p>
  <p><b>作者</b>：Shihao Zhou,  Duosheng Chen,  Jinshan Pan,  Jufeng Yang</p>
  <p><b>备注</b>：19 pages, 7 figures</p>
  <p><b>关键词</b>：achieved superior performance, model long-term dependencies, achieved superior, superior performance, long-term dependencies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transformer-based approaches have achieved superior performance in image restoration, since they can model long-term dependencies well. However, the limitation in capturing local information restricts their capacity to remove degradations. While existing approaches attempt to mitigate this issue by incorporating convolutional operations, the core component in Transformer, i.e., self-attention, which serves as a low-pass filter, could unintentionally dilute or even eliminate the acquired local patterns. In this paper, we propose HIT, a simple yet effective High-frequency Injected Transformer for image restoration. Specifically, we design a window-wise injection module (WIM), which incorporates abundant high-frequency details into the feature map, to provide reliable references for restoring high-quality images. We also develop a bidirectional interaction module (BIM) to aggregate features at different scales using a mutually reinforced paradigm, resulting in spatially and contextually improved representations. In addition, we introduce a spatial enhancement unit (SEU) to preserve essential spatial relationships that may be lost due to the computations carried out across channel dimensions in the BIM. Extensive experiments on 9 tasks (real noise, real rain streak, raindrop, motion blur, moiré, shadow, snow, haze, and low-light condition) demonstrate that HIT with linear computational complexity performs favorably against the state-of-the-art methods. The source code and pre-trained models will be available at this https URL.</p>
  </details>
</details>
<details>
  <summary>200. <b>标题：HSIMamba: Hyperpsectral Imaging Efficient Feature Learning with  Bidirectional State Space for Classification</b></summary>
  <p><b>编号</b>：[545]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00272">https://arxiv.org/abs/2404.00272</a></p>
  <p><b>作者</b>：Judy X Yang,  Jun Zhou,  Jing Wang,  Hui Tian,  Alan Wee Chung Liew</p>
  <p><b>备注</b>：11 pages, 2 figures, 8 tables</p>
  <p><b>关键词</b>：Classifying hyperspectral images, complex high-dimensional data, difficult task, complex high-dimensional, HSIMamba</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Classifying hyperspectral images is a difficult task in remote sensing, due to their complex high-dimensional data. To address this challenge, we propose HSIMamba, a novel framework that uses bidirectional reversed convolutional neural network pathways to extract spectral features more efficiently. Additionally, it incorporates a specialized block for spatial analysis. Our approach combines the operational efficiency of CNNs with the dynamic feature extraction capability of attention mechanisms found in Transformers. However, it avoids the associated high computational demands. HSIMamba is designed to process data bidirectionally, significantly enhancing the extraction of spectral features and integrating them with spatial information for comprehensive analysis. This approach improves classification accuracy beyond current benchmarks and addresses computational inefficiencies encountered with advanced models like Transformers. HSIMamba were tested against three widely recognized datasets Houston 2013, Indian Pines, and Pavia University and demonstrated exceptional performance, surpassing existing state-of-the-art models in HSI classification. This method highlights the methodological innovation of HSIMamba and its practical implications, which are particularly valuable in contexts where computational resources are limited. HSIMamba redefines the standards of efficiency and accuracy in HSI classification, thereby enhancing the capabilities of remote sensing applications. Hyperspectral imaging has become a crucial tool for environmental surveillance, agriculture, and other critical areas that require detailed analysis of the Earth surface. Please see our code in HSIMamba for more details.</p>
  </details>
</details>
<details>
  <summary>201. <b>标题：IPoD: Implicit Field Learning with Point Diffusion for Generalizable 3D  Object Reconstruction from Single RGB-D Images</b></summary>
  <p><b>编号</b>：[548]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00269">https://arxiv.org/abs/2404.00269</a></p>
  <p><b>作者</b>：Yushuang Wu,  Luyue Shi,  Junhao Cai,  Weihao Yuan,  Lingteng Qiu,  Zilong Dong,  Liefeng Bo,  Shuguang Cui,  Xiaoguang Han</p>
  <p><b>备注</b>：CVPR 2024</p>
  <p><b>关键词</b>：single-view RGB-D images, RGB-D images remains, implicit field learning, Transformer-based implicit field, challenging task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generalizable 3D object reconstruction from single-view RGB-D images remains a challenging task, particularly with real-world data. Current state-of-the-art methods develop Transformer-based implicit field learning, necessitating an intensive learning paradigm that requires dense query-supervision uniformly sampled throughout the entire space. We propose a novel approach, IPoD, which harmonizes implicit field learning with point diffusion. This approach treats the query points for implicit field learning as a noisy point cloud for iterative denoising, allowing for their dynamic adaptation to the target object shape. Such adaptive query points harness diffusion learning's capability for coarse shape recovery and also enhances the implicit representation's ability to delineate finer details. Besides, an additional self-conditioning mechanism is designed to use implicit predictions as the guidance of diffusion learning, leading to a cooperative system. Experiments conducted on the CO3D-v2 dataset affirm the superiority of IPoD, achieving 7.8% improvement in F-score and 28.6% in Chamfer distance over existing methods. The generalizability of IPoD is also demonstrated on the MVImgNet dataset. Our project page is at this https URL.</p>
  </details>
</details>
<details>
  <summary>202. <b>标题：Image-to-Image Matching via Foundation Models: A New Perspective for  Open-Vocabulary Semantic Segmentation</b></summary>
  <p><b>编号</b>：[553]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00262">https://arxiv.org/abs/2404.00262</a></p>
  <p><b>作者</b>：Yuan Wang,  Rui Sun,  Naisong Luo,  Yuwen Pan,  Tianzhu Zhang</p>
  <p><b>备注</b>：Accepted to CVPR2024</p>
  <p><b>关键词</b>：Open-vocabulary semantic segmentation, Open-vocabulary semantic, semantic segmentation, aims to segment, arbitrary categories</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Open-vocabulary semantic segmentation (OVS) aims to segment images of arbitrary categories specified by class labels or captions. However, most previous best-performing methods, whether pixel grouping methods or region recognition methods, suffer from false matches between image features and category labels. We attribute this to the natural gap between the textual features and visual features. In this work, we rethink how to mitigate false matches from the perspective of image-to-image matching and propose a novel relation-aware intra-modal matching (RIM) framework for OVS based on visual foundation models. RIM achieves robust region classification by firstly constructing diverse image-modal reference features and then matching them with region features based on relation-aware ranking distribution. The proposed RIM enjoys several merits. First, the intra-modal reference features are better aligned, circumventing potential ambiguities that may arise in cross-modal matching. Second, the ranking-based matching process harnesses the structure information implicit in the inter-class relationships, making it more robust than comparing individually. Extensive experiments on three benchmarks demonstrate that RIM outperforms previous state-of-the-art methods by large margins, obtaining a lead of more than 10% in mIoU on PASCAL VOC benchmark.</p>
  </details>
</details>
<details>
  <summary>203. <b>标题：Exploiting Self-Supervised Constraints in Image Super-Resolution</b></summary>
  <p><b>编号</b>：[555]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00260">https://arxiv.org/abs/2404.00260</a></p>
  <p><b>作者</b>：Gang Wu,  Junjun Jiang,  Kui Jiang,  Xianming Liu</p>
  <p><b>备注</b>：ICME 2024</p>
  <p><b>关键词</b>：high-level visual tasks, low-level image processing, Recent advances, predominantly studied, visual tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advances in self-supervised learning, predominantly studied in high-level visual tasks, have been explored in low-level image processing. This paper introduces a novel self-supervised constraint for single image super-resolution, termed SSC-SR. SSC-SR uniquely addresses the divergence in image complexity by employing a dual asymmetric paradigm and a target model updated via exponential moving average to enhance stability. The proposed SSC-SR framework works as a plug-and-play paradigm and can be easily applied to existing SR models. Empirical evaluations reveal that our SSC-SR framework delivers substantial enhancements on a variety of benchmark datasets, achieving an average increase of 0.1 dB over EDSR and 0.06 dB over SwinIR. In addition, extensive ablation studies corroborate the effectiveness of each constituent in our SSC-SR framework. Codes are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>204. <b>标题：YOLOOC: YOLO-based Open-Class Incremental Object Detection with Novel  Class Discovery</b></summary>
  <p><b>编号</b>：[556]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00257">https://arxiv.org/abs/2404.00257</a></p>
  <p><b>作者</b>：Qian Wan,  Xiang Xiang,  Qinhao Zhou</p>
  <p><b>备注</b>：Initially submitted to ACCV 2022</p>
  <p><b>关键词</b>：open-world object detection, open-world object, attention recently, lot of attention, classes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Because of its use in practice, open-world object detection (OWOD) has gotten a lot of attention recently. The challenge is how can a model detect novel classes and then incrementally learn them without forgetting previously known classes. Previous approaches hinge on strongly-supervised or weakly-supervised novel-class data for novel-class detection, which may not apply to real applications. We construct a new benchmark that novel classes are only encountered at the inference stage. And we propose a new OWOD detector YOLOOC, based on the YOLO architecture yet for the Open-Class setup. We introduce label smoothing to prevent the detector from over-confidently mapping novel classes to known classes and to discover novel classes. Extensive experiments conducted on our more realistic setup demonstrate the effectiveness of our method for discovering novel classes in our new benchmark.</p>
  </details>
</details>
<details>
  <summary>205. <b>标题：Grid Diffusion Models for Text-to-Video Generation</b></summary>
  <p><b>编号</b>：[569]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00234">https://arxiv.org/abs/2404.00234</a></p>
  <p><b>作者</b>：Taegyeong Lee,  Soyeong Kwon,  Taehwan Kim</p>
  <p><b>备注</b>：Accepted to CVPR 2024</p>
  <p><b>关键词</b>：Recent advances, significantly improved, generation, video, methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advances in the diffusion models have significantly improved text-to-image generation. However, generating videos from text is a more challenging task than generating images from text, due to the much larger dataset and higher computational cost required. Most existing video generation methods use either a 3D U-Net architecture that considers the temporal dimension or autoregressive generation. These methods require large datasets and are limited in terms of computational costs compared to text-to-image generation. To tackle these challenges, we propose a simple but effective novel grid diffusion for text-to-video generation without temporal dimension in architecture and a large text-video paired dataset. We can generate a high-quality video using a fixed amount of GPU memory regardless of the number of frames by representing the video as a grid image. Additionally, since our method reduces the dimensions of the video to the dimensions of the image, various image-based methods can be applied to videos, such as text-guided video manipulation from image manipulation. Our proposed method outperforms the existing methods in both quantitative and qualitative evaluations, demonstrating the suitability of our model for real-world video generation.</p>
  </details>
</details>
<details>
  <summary>206. <b>标题：Attention-based Shape-Deformation Networks for Artifact-Free Geometry  Reconstruction of Lumbar Spine from MR Images</b></summary>
  <p><b>编号</b>：[571]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00231">https://arxiv.org/abs/2404.00231</a></p>
  <p><b>作者</b>：Linchen Qian,  Jiasong Chen,  Linhai Ma,  Timur Urakov,  Weiyong Gu,  Liang Liang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：low back pain, global health concern, progressive structural wear, significant global health, Lumbar disc degeneration</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Lumbar disc degeneration, a progressive structural wear and tear of lumbar intervertebral disc, is regarded as an essential role on low back pain, a significant global health concern. Automated lumbar spine geometry reconstruction from MR images will enable fast measurement of medical parameters to evaluate the lumbar status, in order to determine a suitable treatment. Existing image segmentation-based techniques often generate erroneous segments or unstructured point clouds, unsuitable for medical parameter measurement. In this work, we present TransDeformer: a novel attention-based deep learning approach that reconstructs the contours of the lumbar spine with high spatial accuracy and mesh correspondence across patients, and we also present a variant of TransDeformer for error estimation. Specially, we devise new attention modules with a new attention formula, which integrates image features and tokenized contour features to predict the displacements of the points on a shape template without the need for image segmentation. The deformed template reveals the lumbar spine geometry in the input image. We develop a multi-stage training strategy to enhance model robustness with respect to template initialization. Experiment results show that our TransDeformer generates artifact-free geometry outputs, and its variant predicts the error of a reconstructed geometry. Our code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>207. <b>标题：Latent Watermark: Inject and Detect Watermarks in Latent Diffusion Space</b></summary>
  <p><b>编号</b>：[572]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00230">https://arxiv.org/abs/2404.00230</a></p>
  <p><b>作者</b>：Zheling Meng,  Bo Peng,  Jing Dong</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：latent diffusion models, diffusion models, tool for actively, actively identifying, identifying and attributing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Watermarking is a tool for actively identifying and attributing the images generated by latent diffusion models. Existing methods face the dilemma of watermark robustness and image quality. The reason for this dilemma is that watermark detection is performed in pixel space, implying an intrinsic link between image quality and watermark robustness. In this paper, we highlight that an effective solution to the problem is to both inject and detect watermarks in latent space, and propose Latent Watermark (LW) with a progressive training strategy. Experiments show that compared to the recently proposed methods such as StegaStamp, StableSignature, RoSteALS and TreeRing, LW not only surpasses them in terms of robustness but also offers superior image quality. When we inject 64-bit messages, LW can achieve an identification performance close to 100% and an attribution performance above 97% under 9 single-attack scenarios and one all-attack scenario. Our code will be available on GitHub.</p>
  </details>
</details>
<details>
  <summary>208. <b>标题：InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning</b></summary>
  <p><b>编号</b>：[573]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00228">https://arxiv.org/abs/2404.00228</a></p>
  <p><b>作者</b>：Yan-Shuo Liang,  Wu-Jun Li</p>
  <p><b>备注</b>：Accepted by the 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2024)</p>
  <p><b>关键词</b>：Continual learning, Continual learning requires, continual learning methods, learning, Continual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Continual learning requires the model to learn multiple tasks sequentially. In continual learning, the model should possess the ability to maintain its performance on old tasks (stability) and the ability to adapt to new tasks continuously (plasticity). Recently, parameter-efficient fine-tuning (PEFT), which involves freezing a pre-trained model and injecting a small number of learnable parameters to adapt to downstream tasks, has gained increasing popularity in continual learning. Although existing continual learning methods based on PEFT have demonstrated superior performance compared to those not based on PEFT, most of them do not consider how to eliminate the interference of the new task on the old tasks, which inhibits the model from making a good trade-off between stability and plasticity. In this work, we propose a new PEFT method, called interference-free low-rank adaptation (InfLoRA), for continual learning. InfLoRA injects a small number of parameters to reparameterize the pre-trained weights and shows that fine-tuning these injected parameters is equivalent to fine-tuning the pre-trained weights within a subspace. Furthermore, InfLoRA designs this subspace to eliminate the interference of the new task on the old tasks, making a good trade-off between stability and plasticity. Experimental results show that InfLoRA outperforms existing state-of-the-art continual learning methods on multiple datasets.</p>
  </details>
</details>
<details>
  <summary>209. <b>标题：Design as Desired: Utilizing Visual Question Answering for Multimodal  Pre-training</b></summary>
  <p><b>编号</b>：[575]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00226">https://arxiv.org/abs/2404.00226</a></p>
  <p><b>作者</b>：Tongkun Su,  Jun Li,  Xi Zhang,  Haibo Jin,  Hao Chen,  Qiong Wang,  Faqin Lv,  Baoliang Zhao,  Yin Hu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Visual Question Answering, medical visual representations, representations from paired, paired medical reports, learns medical visual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multimodal pre-training demonstrates its potential in the medical domain, which learns medical visual representations from paired medical reports. However, many pre-training tasks require extra annotations from clinicians, and most of them fail to explicitly guide the model to learn the desired features of different pathologies. To the best of our knowledge, we are the first to utilize Visual Question Answering (VQA) for multimodal pre-training to guide the framework focusing on targeted pathological features. In this work, we leverage descriptions in medical reports to design multi-granular question-answer pairs associated with different diseases, which assist the framework in pre-training without requiring extra annotations from experts. We also propose a novel pre-training framework with a quasi-textual feature transformer, a module designed to transform visual features into a quasi-textual space closer to the textual domain via a contrastive learning strategy. This narrows the vision-language gap and facilitates modality alignment. Our framework is applied to four downstream tasks: report generation, classification, segmentation, and detection across five datasets. Extensive experiments demonstrate the superiority of our framework compared to other state-of-the-art methods. Our code will be released upon acceptance.</p>
  </details>
</details>
<details>
  <summary>210. <b>标题：Optimal Blackjack Strategy Recommender: A Comprehensive Study on  Computer Vision Integration for Enhanced Gameplay</b></summary>
  <p><b>编号</b>：[595]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00191">https://arxiv.org/abs/2404.00191</a></p>
  <p><b>作者</b>：Krishnanshu Gupta,  Devon Bolt,  Ben Hinchliff</p>
  <p><b>备注</b>：24 pages, 13 figures</p>
  <p><b>关键词</b>：research project investigates, Blackjack Basic Strategy, playing card detection, classifying playing cards, popular casino game</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This research project investigates the application of several computer vision techniques for playing card detection and recognition in the context of the popular casino game, blackjack. The primary objective is to develop a robust system that is capable of detecting and accurately classifying playing cards in real-time, and displaying the optimal move recommendation based on the given image of the current game. The proposed methodology involves using K-Means for image segmentation, card reprojection and feature extraction, training of the KNN classifier using a labeled dataset, and integration of the detection system into a Blackjack Basic Strategy recommendation algorithm. Further, the study aims to observe the effectiveness of this approach in detecting various card designs under different lighting conditions and occlusions. Overall, the project examines the potential benefits of incorporating computer vision techniques, with a specific focus on card detection, into commonly played games aiming to enhance player decision-making and optimize strategic outcomes. The results obtained from our experimental evaluations with models developed under considerable time constraints, highlight the potential for practical implementation in real-world casino environments and across other similarly structured games.</p>
  </details>
</details>
<details>
  <summary>211. <b>标题：On Inherent Adversarial Robustness of Active Vision Systems</b></summary>
  <p><b>编号</b>：[600]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00185">https://arxiv.org/abs/2404.00185</a></p>
  <p><b>作者</b>：Amitangshu Mukherjee,  Timur Ibrayev,  Kaushik Roy</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：carefully crafted noise, adding carefully crafted, Deep Neural Networks, Current Deep Neural, Deep Neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Current Deep Neural Networks are vulnerable to adversarial examples, which alter their predictions by adding carefully crafted noise. Since human eyes are robust to such inputs, it is possible that the vulnerability stems from the standard way of processing inputs in one shot by processing every pixel with the same importance. In contrast, neuroscience suggests that the human vision system can differentiate salient features by (1) switching between multiple fixation points (saccades) and (2) processing the surrounding with a non-uniform external resolution (foveation). In this work, we advocate that the integration of such active vision mechanisms into current deep learning systems can offer robustness benefits. Specifically, we empirically demonstrate the inherent robustness of two active vision methods - GFNet and FALcon - under a black box threat model. By learning and inferencing based on downsampled glimpses obtained from multiple distinct fixation points within an input, we show that these active methods achieve (2-3) times greater robustness compared to a standard passive convolutional network under state-of-the-art adversarial attacks. More importantly, we provide illustrative and interpretable visualization analysis that demonstrates how performing inference from distinct fixation points makes active vision methods less vulnerable to malicious inputs.</p>
  </details>
</details>
<details>
  <summary>212. <b>标题：Multi-Region Transfer Learning for Segmentation of Crop Field Boundaries  in Satellite Images with Limited Labels</b></summary>
  <p><b>编号</b>：[602]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00179">https://arxiv.org/abs/2404.00179</a></p>
  <p><b>作者</b>：Hannah Kerner,  Saketh Sundar,  Mathan Satish</p>
  <p><b>备注</b>：Accepted for 2023 AAAI Workshop on AI to Accelerate Science and Engineering</p>
  <p><b>关键词</b>：overhead remotely sensed, field boundary delineation, remotely sensed images, field boundary, boundary delineation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The goal of field boundary delineation is to predict the polygonal boundaries and interiors of individual crop fields in overhead remotely sensed images (e.g., from satellites or drones). Automatic delineation of field boundaries is a necessary task for many real-world use cases in agriculture, such as estimating cultivated area in a region or predicting end-of-season yield in a field. Field boundary delineation can be framed as an instance segmentation problem, but presents unique research challenges compared to traditional computer vision datasets used for instance segmentation. The practical applicability of previous work is also limited by the assumption that a sufficiently-large labeled dataset is available where field boundary delineation models will be applied, which is not the reality for most regions (especially under-resourced regions such as Sub-Saharan Africa). We present an approach for segmentation of crop field boundaries in satellite images in regions lacking labeled data that uses multi-region transfer learning to adapt model weights for the target region. We show that our approach outperforms existing methods and that multi-region transfer learning substantially boosts performance for multiple model architectures. Our implementation and datasets are publicly available to enable use of the approach by end-users and serve as a benchmark for future work.</p>
  </details>
</details>
<details>
  <summary>213. <b>标题：Universal Bovine Identification via Depth Data and Deep Metric Learning</b></summary>
  <p><b>编号</b>：[605]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00172">https://arxiv.org/abs/2404.00172</a></p>
  <p><b>作者</b>：Asheesh Sharma,  Lucy Randewich,  William Andrew,  Sion Hannuna,  Neill Campbell,  Siobhan Mullan,  Andrew W. Dowsey,  Melvyn Smith,  Mark Hansen,  Tilo Burghardt</p>
  <p><b>备注</b>：LaTeX, 38 pages, 14 figures, 3 tables</p>
  <p><b>关键词</b>：depth-only deep learning, deep learning system, accurately identifying individual, dorsal view, identifying individual cattle</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper proposes and evaluates, for the first time, a top-down (dorsal view), depth-only deep learning system for accurately identifying individual cattle and provides associated code, datasets, and training weights for immediate reproducibility. An increase in herd size skews the cow-to-human ratio at the farm and makes the manual monitoring of individuals more challenging. Therefore, real-time cattle identification is essential for the farms and a crucial step towards precision livestock farming. Underpinned by our previous work, this paper introduces a deep-metric learning method for cattle identification using depth data from an off-the-shelf 3D camera. The method relies on CNN and MLP backbones that learn well-generalised embedding spaces from the body shape to differentiate individuals -- requiring neither species-specific coat patterns nor close-up muzzle prints for operation. The network embeddings are clustered using a simple algorithm such as $k$-NN for highly accurate identification, thus eliminating the need to retrain the network for enrolling new individuals. We evaluate two backbone architectures, ResNet, as previously used to identify Holstein Friesians using RGB images, and PointNet, which is specialised to operate on 3D point clouds. We also present CowDepth2023, a new dataset containing 21,490 synchronised colour-depth image pairs of 99 cows, to evaluate the backbones. Both ResNet and PointNet architectures, which consume depth maps and point clouds, respectively, led to high accuracy that is on par with the coat pattern-based backbone.</p>
  </details>
</details>
<details>
  <summary>214. <b>标题：Multi-Level Neural Scene Graphs for Dynamic Urban Environments</b></summary>
  <p><b>编号</b>：[607]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00168">https://arxiv.org/abs/2404.00168</a></p>
  <p><b>作者</b>：Tobias Fischer,  Lorenzo Porzi,  Samuel Rota Bulò,  Marc Pollefeys,  Peter Kontschieder</p>
  <p><b>备注</b>：CVPR 2024. Project page is available at this https URL</p>
  <p><b>关键词</b>：varying environmental conditions, multiple vehicle captures, large-scale dynamic areas, environmental conditions, areas from multiple</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We estimate the radiance field of large-scale dynamic areas from multiple vehicle captures under varying environmental conditions. Previous works in this domain are either restricted to static environments, do not scale to more than a single short video, or struggle to separately represent dynamic object instances. To this end, we present a novel, decomposable radiance field approach for dynamic urban environments. We propose a multi-level neural scene graph representation that scales to thousands of images from dozens of sequences with hundreds of fast-moving objects. To enable efficient training and rendering of our representation, we develop a fast composite ray sampling and rendering scheme. To test our approach in urban driving scenarios, we introduce a new, novel view synthesis benchmark. We show that our approach outperforms prior art by a significant margin on both established and our proposed benchmark while being faster in training and rendering.</p>
  </details>
</details>
<details>
  <summary>215. <b>标题：Uncovering Bias in Large Vision-Language Models with Counterfactuals</b></summary>
  <p><b>编号</b>：[608]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00166">https://arxiv.org/abs/2404.00166</a></p>
  <p><b>作者</b>：Phillip Howard,  Anahita Bhiwandiwalla,  Kathleen C. Fraser,  Svetlana Kiritchenko</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Large Vision-Language Models, Large Language, increasingly impressive capabilities, possessing increasingly impressive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the advent of Large Language Models (LLMs) possessing increasingly impressive capabilities, a number of Large Vision-Language Models (LVLMs) have been proposed to augment LLMs with visual inputs. Such models condition generated text on both an input image and a text prompt, enabling a variety of use cases such as visual question answering and multimodal chat. While prior studies have examined the social biases contained in text generated by LLMs, this topic has been relatively unexplored in LVLMs. Examining social biases in LVLMs is particularly challenging due to the confounding contributions of bias induced by information contained across the text and visual modalities. To address this challenging problem, we conduct a large-scale study of text generated by different LVLMs under counterfactual changes to input images. Specifically, we present LVLMs with identical open-ended text prompts while conditioning on images from different counterfactual sets, where each set contains images which are largely identical in their depiction of a common subject (e.g., a doctor), but vary only in terms of intersectional social attributes (e.g., race and gender). We comprehensively evaluate the text produced by different LVLMs under this counterfactual generation setting and find that social attributes such as race, gender, and physical characteristics depicted in input images can significantly influence toxicity and the generation of competency-associated words.</p>
  </details>
</details>
<details>
  <summary>216. <b>标题：CT respiratory motion synthesis using joint supervised and adversarial  learning</b></summary>
  <p><b>编号</b>：[610]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00163">https://arxiv.org/abs/2404.00163</a></p>
  <p><b>作者</b>：Yi-Heng Cao,  Vincent Bourbonne,  François Lucia,  Ulrike Schick,  Julien Bert,  Vincent Jaouen,  Dimitris Visvikis</p>
  <p><b>备注</b>：to appear in Phys. Med. Biol</p>
  <p><b>关键词</b>：Four-dimensional computed tomography, Four-dimensional computed, track internal organ, computed tomography, imaging consists</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Objective: Four-dimensional computed tomography (4DCT) imaging consists in reconstructing a CT acquisition into multiple phases to track internal organ and tumor motion. It is commonly used in radiotherapy treatment planning to establish planning target volumes. However, 4DCT increases protocol complexity, may not align with patient breathing during treatment, and lead to higher radiation delivery. Approach: In this study, we propose a deep synthesis method to generate pseudo respiratory CT phases from static images for motion-aware treatment planning. The model produces patient-specific deformation vector fields (DVFs) by conditioning synthesis on external patient surface-based estimation, mimicking respiratory monitoring devices. A key methodological contribution is to encourage DVF realism through supervised DVF training while using an adversarial term jointly not only on the warped image but also on the magnitude of the DVF itself. This way, we avoid excessive smoothness typically obtained through deep unsupervised learning, and encourage correlations with the respiratory amplitude. Main results: Performance is evaluated using real 4DCT acquisitions with smaller tumor volumes than previously reported. Results demonstrate for the first time that the generated pseudo-respiratory CT phases can capture organ and tumor motion with similar accuracy to repeated 4DCT scans of the same patient. Mean inter-scans tumor center-of-mass distances and Dice similarity coefficients were $1.97$mm and $0.63$, respectively, for real 4DCT phases and $2.35$mm and $0.71$ for synthetic phases, and compares favorably to a state-of-the-art technique (RMSim).</p>
  </details>
</details>
<details>
  <summary>217. <b>标题：VSRD: Instance-Aware Volumetric Silhouette Rendering for Weakly  Supervised 3D Object Detection</b></summary>
  <p><b>编号</b>：[617]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00149">https://arxiv.org/abs/2404.00149</a></p>
  <p><b>作者</b>：Zihua Liu,  Hiroki Sakuma,  Masatoshi Okutomi</p>
  <p><b>备注</b>：CVPR 2024</p>
  <p><b>关键词</b>：scene understanding due, inherently ill-posed nature, monocular depth estimation, object detection poses, object detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Monocular 3D object detection poses a significant challenge in 3D scene understanding due to its inherently ill-posed nature in monocular depth estimation. Existing methods heavily rely on supervised learning using abundant 3D labels, typically obtained through expensive and labor-intensive annotation on LiDAR point clouds. To tackle this problem, we propose a novel weakly supervised 3D object detection framework named VSRD (Volumetric Silhouette Rendering for Detection) to train 3D object detectors without any 3D supervision but only weak 2D supervision. VSRD consists of multi-view 3D auto-labeling and subsequent training of monocular 3D object detectors using the pseudo labels generated in the auto-labeling stage. In the auto-labeling stage, we represent the surface of each instance as a signed distance field (SDF) and render its silhouette as an instance mask through our proposed instance-aware volumetric silhouette rendering. To directly optimize the 3D bounding boxes through rendering, we decompose the SDF of each instance into the SDF of a cuboid and the residual distance field (RDF) that represents the residual from the cuboid. This mechanism enables us to optimize the 3D bounding boxes in an end-to-end manner by comparing the rendered instance masks with the ground truth instance masks. The optimized 3D bounding boxes serve as effective training data for 3D object detection. We conduct extensive experiments on the KITTI-360 dataset, demonstrating that our method outperforms the existing weakly supervised 3D object detection methods. The code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>218. <b>标题：Fast OMP for Exact Recovery and Sparse Approximation</b></summary>
  <p><b>编号</b>：[618]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00146">https://arxiv.org/abs/2404.00146</a></p>
  <p><b>作者</b>：Huiyuan Yu,  Jia He,  Maggie Cheng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Orthogonal Matching Pursuit, Matching Pursuit, Orthogonal Matching, OMP, powerful method</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Orthogonal Matching Pursuit (OMP) has been a powerful method in sparse signal recovery and approximation. However OMP suffers computational issue when the signal has large number of non-zeros. This paper advances OMP in two fronts: it offers a fast algorithm for the orthogonal projection of the input signal at each iteration, and a new selection criterion for making the greedy choice, which reduces the number of iterations it takes to recover the signal. The proposed modifications to OMP directly reduce the computational complexity. Experiment results show significant improvement over the classical OMP in computation time. The paper also provided a sufficient condition for exact recovery under the new greedy choice criterion. For general signals that may not have sparse representations, the paper provides a bound for the approximation error. The approximation error is at the same order as OMP but is obtained within fewer iterations and less time.</p>
  </details>
</details>
<details>
  <summary>219. <b>标题：FISBe: A real-world benchmark dataset for instance segmentation of  long-range thin filamentous structures</b></summary>
  <p><b>编号</b>：[626]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00130">https://arxiv.org/abs/2404.00130</a></p>
  <p><b>作者</b>：Lisa Mais,  Peter Hirsch,  Claire Managan,  Ramya Kandarpa,  Josef Lorenz Rumberger,  Annika Reinke,  Lena Maier-Hein,  Gudrun Ihrke,  Dagmar Kainmueller</p>
  <p><b>备注</b>：CVPR2024, Project page: this https URL</p>
  <p><b>关键词</b>：nervous systems enables, systems enables groundbreaking, facilitating joint functional, light microscopy images, volumetric light microscopy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Instance segmentation of neurons in volumetric light microscopy images of nervous systems enables groundbreaking research in neuroscience by facilitating joint functional and morphological analyses of neural circuits at cellular resolution. Yet said multi-neuron light microscopy data exhibits extremely challenging properties for the task of instance segmentation: Individual neurons have long-ranging, thin filamentous and widely branching morphologies, multiple neurons are tightly inter-weaved, and partial volume effects, uneven illumination and noise inherent to light microscopy severely impede local disentangling as well as long-range tracing of individual neurons. These properties reflect a current key challenge in machine learning research, namely to effectively capture long-range dependencies in the data. While respective methodological research is buzzing, to date methods are typically benchmarked on synthetic datasets. To address this gap, we release the FlyLight Instance Segmentation Benchmark (FISBe) dataset, the first publicly available multi-neuron light microscopy dataset with pixel-wise annotations. In addition, we define a set of instance segmentation metrics for benchmarking that we designed to be meaningful with regard to downstream analyses. Lastly, we provide three baselines to kick off a competition that we envision to both advance the field of machine learning regarding methodology for capturing long-range data dependencies, and facilitate scientific discovery in basic neuroscience.</p>
  </details>
</details>
<details>
  <summary>220. <b>标题：AgileFormer: Spatially Agile Transformer UNet for Medical Image  Segmentation</b></summary>
  <p><b>编号</b>：[630]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00122">https://arxiv.org/abs/2404.00122</a></p>
  <p><b>作者</b>：Peijie Qiu,  Jin Yang,  Sayantan Kumar,  Soumyendu Sekhar Ghosh,  Aristeidis Sotiras</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：convolutional neural networks, deep neural networks, neural networks, medical image segmentation, convolutional neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the past decades, deep neural networks, particularly convolutional neural networks, have achieved state-of-the-art performance in a variety of medical image segmentation tasks. Recently, the introduction of the vision transformer (ViT) has significantly altered the landscape of deep segmentation models. There has been a growing focus on ViTs, driven by their excellent performance and scalability. However, we argue that the current design of the vision transformer-based UNet (ViT-UNet) segmentation models may not effectively handle the heterogeneous appearance (e.g., varying shapes and sizes) of objects of interest in medical image segmentation tasks. To tackle this challenge, we present a structured approach to introduce spatially dynamic components to the ViT-UNet. This adaptation enables the model to effectively capture features of target objects with diverse appearances. This is achieved by three main components: \textbf{(i)} deformable patch embedding; \textbf{(ii)} spatially dynamic multi-head attention; \textbf{(iii)} deformable positional encoding. These components were integrated into a novel architecture, termed AgileFormer. AgileFormer is a spatially agile ViT-UNet designed for medical image segmentation. Experiments in three segmentation tasks using publicly available datasets demonstrated the effectiveness of the proposed method. The code is available at \href{this https URL}{this https URL}.</p>
  </details>
</details>
<details>
  <summary>221. <b>标题：Deepfake Sentry: Harnessing Ensemble Intelligence for Resilient  Detection and Generalisation</b></summary>
  <p><b>编号</b>：[631]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00114">https://arxiv.org/abs/2404.00114</a></p>
  <p><b>作者</b>：Liviu-Daniel Ştefan (1),  Dan-Cristian Stanciu (1),  Mihai Dogariu (1),  Mihai Gabriel Constantin (1),  Andrei Cosmin Jitaru (1),  Bogdan Ionescu (1) ((1) University "Politehnica" of Bucharest, Romania)</p>
  <p><b>备注</b>：16 pages, 1 figure, U.P.B. Sci. Bull., Series C, Vol. 85, Iss. 4, 2023</p>
  <p><b>关键词</b>：Generative Adversarial Networks, enabled photorealistic image, advancements in Generative, photorealistic image generation, Adversarial Networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advancements in Generative Adversarial Networks (GANs) have enabled photorealistic image generation with high quality. However, the malicious use of such generated media has raised concerns regarding visual misinformation. Although deepfake detection research has demonstrated high accuracy, it is vulnerable to advances in generation techniques and adversarial iterations on detection countermeasures. To address this, we propose a proactive and sustainable deepfake training augmentation solution that introduces artificial fingerprints into models. We achieve this by employing an ensemble learning approach that incorporates a pool of autoencoders that mimic the effect of the artefacts introduced by the deepfake generator models. Experiments on three datasets reveal that our proposed ensemble autoencoder-based data augmentation learning approach offers improvements in terms of generalisation, resistance against basic data perturbations such as noise, blurring, sharpness enhancement, and affine transforms, resilience to commonly used lossy compression algorithms such as JPEG, and enhanced resistance against adversarial attacks.</p>
  </details>
</details>
<details>
  <summary>222. <b>标题：Robust Ensemble Person Re-Identification via Orthogonal Fusion with  Occlusion Handling</b></summary>
  <p><b>编号</b>：[634]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00107">https://arxiv.org/abs/2404.00107</a></p>
  <p><b>作者</b>：Syeda Nyma Ferdous,  Xin Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：variation of appearances, major challenges, diversity of poses, person reidentification, occluded regions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Occlusion remains one of the major challenges in person reidentification (ReID) as a result of the diversity of poses and the variation of appearances. Developing novel architectures to improve the robustness of occlusion-aware person Re-ID requires new insights, especially on low-resolution edge cameras. We propose a deep ensemble model that harnesses both CNN and Transformer architectures to generate robust feature representations. To achieve robust Re-ID without the need to manually label occluded regions, we propose to take an ensemble learning-based approach derived from the analogy between arbitrarily shaped occluded regions and robust feature representation. Using the orthogonality principle, our developed deep CNN model makes use of masked autoencoder (MAE) and global-local feature fusion for robust person identification. Furthermore, we present a part occlusion-aware transformer capable of learning feature space that is robust to occluded regions. Experimental results are reported on several Re-ID datasets to show the effectiveness of our developed ensemble model named orthogonal fusion with occlusion handling (OFOH). Compared to competing methods, the proposed OFOH approach has achieved competent rank-1 and mAP performance.</p>
  </details>
</details>
<details>
  <summary>223. <b>标题：PikeLPN: Mitigating Overlooked Inefficiencies of Low-Precision Neural  Networks</b></summary>
  <p><b>编号</b>：[635]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00103">https://arxiv.org/abs/2404.00103</a></p>
  <p><b>作者</b>：Marina Neseem,  Conor McCullough,  Randy Hsin,  Chas Leichner,  Shan Li,  In Suk Chong,  Andrew G. Howard,  Lukasz Lew,  Sherief Reda,  Ville-Mikko Rautio,  Daniele Moro</p>
  <p><b>备注</b>：Accepted in CVPR 2024. 10 Figures, 9 Tables</p>
  <p><b>关键词</b>：neural network optimization, Arithmetic Computation Effort, network optimization, efficacy in neural, neural network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Low-precision quantization is recognized for its efficacy in neural network optimization. Our analysis reveals that non-quantized elementwise operations which are prevalent in layers such as parameterized activation functions, batch normalization, and quantization scaling dominate the inference cost of low-precision models. These non-quantized elementwise operations are commonly overlooked in SOTA efficiency metrics such as Arithmetic Computation Effort (ACE). In this paper, we propose ACEv2 - an extended version of ACE which offers a better alignment with the inference cost of quantized models and their energy consumption on ML hardware. Moreover, we introduce PikeLPN, a model that addresses these efficiency issues by applying quantization to both elementwise operations and multiply-accumulate operations. In particular, we present a novel quantization technique for batch normalization layers named QuantNorm which allows for quantizing the batch normalization parameters without compromising the model performance. Additionally, we propose applying Double Quantization where the quantization scaling parameters are quantized. Furthermore, we recognize and resolve the issue of distribution mismatch in Separable Convolution layers by introducing Distribution-Heterogeneous Quantization which enables quantizing them to low-precision. PikeLPN achieves Pareto-optimality in efficiency-accuracy trade-off with up to 3X efficiency improvement compared to SOTA low-precision models.</p>
  </details>
</details>
<details>
  <summary>224. <b>标题：Sparse Views, Near Light: A Practical Paradigm for Uncalibrated  Point-light Photometric Stereo</b></summary>
  <p><b>编号</b>：[637]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00098">https://arxiv.org/abs/2404.00098</a></p>
  <p><b>作者</b>：Mohammed Brahimi,  Bjoern Haefner,  Zhenzhang Ye,  Bastian Goldluecke,  Daniel Cremers</p>
  <p><b>备注</b>：Accepted in CVPR 2024</p>
  <p><b>关键词</b>：significant progress, progress on camera-based, Neural approaches, sparse viewpoints, camera-based reconstruction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural approaches have shown a significant progress on camera-based reconstruction. But they require either a fairly dense sampling of the viewing sphere, or pre-training on an existing dataset, thereby limiting their generalizability. In contrast, photometric stereo (PS) approaches have shown great potential for achieving high-quality reconstruction under sparse viewpoints. Yet, they are impractical because they typically require tedious laboratory conditions, are restricted to dark rooms, and often multi-staged, making them subject to accumulated errors. To address these shortcomings, we propose an end-to-end uncalibrated multi-view PS framework for reconstructing high-resolution shapes acquired from sparse viewpoints in a real-world environment. We relax the dark room assumption, and allow a combination of static ambient lighting and dynamic near LED lighting, thereby enabling easy data capture outside the lab. Experimental validation confirms that it outperforms existing baseline approaches in the regime of sparse viewpoints by a large margin. This allows to bring high-accuracy 3D reconstruction from the dark room to the real world, while maintaining a reasonable data capture complexity.</p>
  </details>
</details>
<details>
  <summary>225. <b>标题：GDA: Generalized Diffusion for Robust Test-time Adaptation</b></summary>
  <p><b>编号</b>：[638]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00095">https://arxiv.org/abs/2404.00095</a></p>
  <p><b>作者</b>：Yun-Yun Tsai,  Fu-Chen Chen,  Albert Y. C. Chen,  Junfeng Yang,  Che-Chun Su,  Min Sun,  Cheng-Hao Kuo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：unexpected distribution shifts, Machine learning models, learning models struggle, OOD, Machine learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine learning models struggle with generalization when encountering out-of-distribution (OOD) samples with unexpected distribution shifts. For vision tasks, recent studies have shown that test-time adaptation employing diffusion models can achieve state-of-the-art accuracy improvements on OOD samples by generating new samples that align with the model's domain without the need to modify the model's weights. Unfortunately, those studies have primarily focused on pixel-level corruptions, thereby lacking the generalization to adapt to a broader range of OOD types. We introduce Generalized Diffusion Adaptation (GDA), a novel diffusion-based test-time adaptation method robust against diverse OOD types. Specifically, GDA iteratively guides the diffusion by applying a marginal entropy loss derived from the model, in conjunction with style and content preservation losses during the reverse sampling process. In other words, GDA considers the model's output behavior with the semantic information of the samples as a whole, which can reduce ambiguity in downstream tasks during the generation process. Evaluation across various popular model architectures and OOD benchmarks shows that GDA consistently outperforms prior work on diffusion-driven adaptation. Notably, it achieves the highest classification accuracy improvements, ranging from 4.4\% to 5.02\% on ImageNet-C and 2.5\% to 7.4\% on Rendition, Sketch, and Stylized benchmarks. This performance highlights GDA's generalization to a broader range of OOD benchmarks.</p>
  </details>
</details>
<details>
  <summary>226. <b>标题：DVIS-DAQ: Improving Video Segmentation via Dynamic Anchor Queries</b></summary>
  <p><b>编号</b>：[639]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00086">https://arxiv.org/abs/2404.00086</a></p>
  <p><b>作者</b>：Yikang Zhou,  Tao Zhang,  Shunping JI,  Shuicheng Yan,  Xiangtai Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：perform inter-frame association, tracking continuously appearing, methods adopt object, continuously appearing objects, segmentation methods adopt</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Modern video segmentation methods adopt object queries to perform inter-frame association and demonstrate satisfactory performance in tracking continuously appearing objects despite large-scale motion and transient occlusion.
However, they all underperform on newly emerging and disappearing objects that are common in the real world because they attempt to model object emergence and disappearance through feature transitions between background and foreground queries that have significant feature gaps. We introduce Dynamic Anchor Queries (DAQ) to shorten the transition gap between the anchor and target queries by dynamically generating anchor queries based on the features of potential candidates.
Furthermore, we introduce a query-level object Emergence and Disappearance Simulation (EDS) strategy, which unleashes DAQ's potential without any additional cost.
Finally, we combine our proposed DAQ and EDS with DVIS~\cite{zhang2023dvis} to obtain DVIS-DAQ.
Extensive experiments demonstrate that DVIS-DAQ achieves a new state-of-the-art (SOTA) performance on five mainstream video segmentation benchmarks. Code and models are available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>227. <b>标题：SLIMBRAIN: Augmented Reality Real-Time Acquisition and Processing System  For Hyperspectral Classification Mapping with Depth Information for In-Vivo  Surgical Procedures</b></summary>
  <p><b>编号</b>：[655]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00048">https://arxiv.org/abs/2404.00048</a></p>
  <p><b>作者</b>：Jaime Sancho,  Manuel Villa,  Miguel Chavarrías,  Eduardo Juarez,  Alfonso Lagares,  César Sanz</p>
  <p><b>备注</b>：14 pages, 19 figues</p>
  <p><b>关键词</b>：technological application domains, augmented reality, rapid development, fields of social, social and technological</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Over the last two decades, augmented reality (AR) has led to the rapid development of new interfaces in various fields of social and technological application domains. One such domain is medicine, and to a higher extent surgery, where these visualization techniques help to improve the effectiveness of preoperative and intraoperative procedures. Following this trend, this paper presents SLIMBRAIN, a real-time acquisition and processing AR system suitable to classify and display brain tumor tissue from hyperspectral (HS) information. This system captures and processes HS images at 14 frames per second (FPS) during the course of a tumor resection operation to detect and delimit cancer tissue at the same time the neurosurgeon operates. The result is represented in an AR visualization where the classification results are overlapped with the RGB point cloud captured by a LiDAR camera. This representation allows natural navigation of the scene at the same time it is captured and processed, improving the visualization and hence effectiveness of the HS technology to delimit tumors. The whole system has been verified in real brain tumor resection operations.</p>
  </details>
</details>
<details>
  <summary>228. <b>标题：Improve accessibility for Low Vision and Blind people using Machine  Learning and Computer Vision</b></summary>
  <p><b>编号</b>：[658]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00043">https://arxiv.org/abs/2404.00043</a></p>
  <p><b>作者</b>：Jasur Shukurov</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：mobile technology worldwide, technology worldwide, ever-growing expansion, improve accessibility, mobile technology</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the ever-growing expansion of mobile technology worldwide, there is an increasing need for accommodation for those who are disabled. This project explores how machine learning and computer vision could be utilized to improve accessibility for people with visual impairments. There have been many attempts to develop various software that would improve accessibility in the day-to-day lives of blind people. However, applications on the market have low accuracy and only provide audio feedback. This project will concentrate on building a mobile application that helps blind people to orient in space by receiving audio and haptic feedback, e.g. vibrations, about their surroundings in real-time. The mobile application will have 3 main features. The initial feature is scanning text from the camera and reading it to a user. This feature can be used on paper with text, in the environment, and on road signs. The second feature is detecting objects around the user, and providing audio feedback about those objects. It also includes providing the description of the objects and their location, and giving haptic feedback if the user is too close to an object. The last feature is currency detection which provides a total amount of currency value to the user via the camera.</p>
  </details>
</details>
<details>
  <summary>229. <b>标题：Deployment of Deep Learning Model in Real World Clinical Setting: A Case  Study in Obstetric Ultrasound</b></summary>
  <p><b>编号</b>：[664]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00032">https://arxiv.org/abs/2404.00032</a></p>
  <p><b>作者</b>：Chun Kit Wong,  Mary Ngo,  Manxi Lin,  Zahra Bashir,  Amihai Heen,  Morten Bo Søndergaard Svendsen,  Martin Grønnebæk Tolsgaard,  Anders Nymark Christensen,  Aasa Feragen</p>
  <p><b>备注</b>：10 pages</p>
  <p><b>关键词</b>：settings remains limited, medical image analysis, clinical settings remains, image analysis, remains limited</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite the rapid development of AI models in medical image analysis, their validation in real-world clinical settings remains limited. To address this, we introduce a generic framework designed for deploying image-based AI models in such settings. Using this framework, we deployed a trained model for fetal ultrasound standard plane detection, and evaluated it in real-time sessions with both novice and expert users. Feedback from these sessions revealed that while the model offers potential benefits to medical practitioners, the need for navigational guidance was identified as a key area for improvement. These findings underscore the importance of early deployment of AI models in real-world settings, leading to insights that can guide the refinement of the model and system based on actual user feedback.</p>
  </details>
</details>
<details>
  <summary>230. <b>标题：iMD4GC: Incomplete Multimodal Data Integration to Advance Precise  Treatment Response Prediction and Survival Analysis for Gastric Cancer</b></summary>
  <p><b>编号</b>：[689]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01192">https://arxiv.org/abs/2404.01192</a></p>
  <p><b>作者</b>：Fengtao Zhou,  Yingxue Xu,  Yanfen Cui,  Shenyan Zhang,  Yun Zhu,  Weiyang He,  Jiguang Wang,  Xin Wang,  Ronald Chan,  Louis Ho Shing Lau,  Chu Han,  Dafu Zhang,  Zhenhui Li,  Hao Chen</p>
  <p><b>备注</b>：27 pages, 9 figures, 3 tables (under review)</p>
  <p><b>关键词</b>：prevalent malignancy worldwide, Gastric cancer, malignancy worldwide, thousand deaths, advanced gastric cancer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Gastric cancer (GC) is a prevalent malignancy worldwide, ranking as the fifth most common cancer with over 1 million new cases and 700 thousand deaths in 2020. Locally advanced gastric cancer (LAGC) accounts for approximately two-thirds of GC diagnoses, and neoadjuvant chemotherapy (NACT) has emerged as the standard treatment for LAGC. However, the effectiveness of NACT varies significantly among patients, with a considerable subset displaying treatment resistance. Ineffective NACT not only leads to adverse effects but also misses the optimal therapeutic window, resulting in lower survival rate. However, existing multimodal learning methods assume the availability of all modalities for each patient, which does not align with the reality of clinical practice. The limited availability of modalities for each patient would cause information loss, adversely affecting predictive accuracy. In this study, we propose an incomplete multimodal data integration framework for GC (iMD4GC) to address the challenges posed by incomplete multimodal data, enabling precise response prediction and survival analysis. Specifically, iMD4GC incorporates unimodal attention layers for each modality to capture intra-modal information. Subsequently, the cross-modal interaction layers explore potential inter-modal interactions and capture complementary information across modalities, thereby enabling information compensation for missing modalities. To evaluate iMD4GC, we collected three multimodal datasets for GC study: GastricRes (698 cases) for response prediction, GastricSur (801 cases) for survival analysis, and TCGA-STAD (400 cases) for survival analysis. The scale of our datasets is significantly larger than previous studies. The iMD4GC achieved impressive performance with an 80.2% AUC on GastricRes, 71.4% C-index on GastricSur, and 66.1% C-index on TCGA-STAD, significantly surpassing other compared methods.</p>
  </details>
</details>
<details>
  <summary>231. <b>标题：Diffusion based Zero-shot Medical Image-to-Image Translation for Cross  Modality Segmentation</b></summary>
  <p><b>编号</b>：[693]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01102">https://arxiv.org/abs/2404.01102</a></p>
  <p><b>作者</b>：Zihao Wang,  Yingyu Yang,  Yuzhou Chen,  Tingting Yuan,  Maxime Sermesant,  Herve Delingette</p>
  <p><b>备注</b>：Neurips 2023 Diffusion Workshop</p>
  <p><b>关键词</b>：Cross-modality image segmentation, Cross-modality image, image segmentation, image, zero-shot cross-modality image</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cross-modality image segmentation aims to segment the target modalities using a method designed in the source modality. Deep generative models can translate the target modality images into the source modality, thus enabling cross-modality segmentation. However, a vast body of existing cross-modality image translation methods relies on supervised learning. In this work, we aim to address the challenge of zero-shot learning-based image translation tasks (extreme scenarios in the target modality is unseen in the training phase). To leverage generative learning for zero-shot cross-modality image segmentation, we propose a novel unsupervised image translation method. The framework learns to translate the unseen source image to the target modality for image segmentation by leveraging the inherent statistical consistency between different modalities for diffusion guidance. Our framework captures identical cross-modality features in the statistical domain, offering diffusion guidance without relying on direct mappings between the source and target domains. This advantage allows our method to adapt to changing source domains without the need for retraining, making it highly practical when sufficient labeled source domain data is not available. The proposed framework is validated in zero-shot cross-modality image segmentation tasks through empirical comparisons with influential generative models, including adversarial-based and diffusion-based models.</p>
  </details>
</details>
<details>
  <summary>232. <b>标题：Automated HER2 Scoring in Breast Cancer Images Using Deep Learning and  Pyramid Sampling</b></summary>
  <p><b>编号</b>：[705]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00837">https://arxiv.org/abs/2404.00837</a></p>
  <p><b>作者</b>：Sahan Yoruc Selcuk,  Xilin Yang,  Bijie Bai,  Yijie Zhang,  Yuzhu Li,  Musa Aydin,  Aras Firat Unal,  Aditya Gomatam,  Zhen Guo,  Darrow Morgan Angus,  Goren Kolodney,  Karine Atlan,  Tal Keidar Haran,  Nir Pillar,  Aydogan Ozcan</p>
  <p><b>备注</b>：21 Pages, 7 Figures</p>
  <p><b>关键词</b>：Human epidermal growth, growth factor receptor, epidermal growth factor, cancer cell growth, epidermal growth</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Human epidermal growth factor receptor 2 (HER2) is a critical protein in cancer cell growth that signifies the aggressiveness of breast cancer (BC) and helps predict its prognosis. Accurate assessment of immunohistochemically (IHC) stained tissue slides for HER2 expression levels is essential for both treatment guidance and understanding of cancer mechanisms. Nevertheless, the traditional workflow of manual examination by board-certified pathologists encounters challenges, including inter- and intra-observer inconsistency and extended turnaround times. Here, we introduce a deep learning-based approach utilizing pyramid sampling for the automated classification of HER2 status in IHC-stained BC tissue images. Our approach analyzes morphological features at various spatial scales, efficiently managing the computational load and facilitating a detailed examination of cellular and larger-scale tissue-level details. This method addresses the tissue heterogeneity of HER2 expression by providing a comprehensive view, leading to a blind testing classification accuracy of 84.70%, on a dataset of 523 core images from tissue microarrays. Our automated system, proving reliable as an adjunct pathology tool, has the potential to enhance diagnostic precision and evaluation speed, and might significantly impact cancer treatment planning.</p>
  </details>
</details>
<details>
  <summary>233. <b>标题：Intensity-based 3D motion correction for cardiac MR images</b></summary>
  <p><b>编号</b>：[707]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00767">https://arxiv.org/abs/2404.00767</a></p>
  <p><b>作者</b>：Nil Stolt-Ansó,  Vasiliki Sideri-Lampretsa,  Maik Dannecker,  Daniel Rueckert</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Cardiac magnetic resonance, image acquisition requires, acquisition requires subjects, cine images, Cardiac magnetic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cardiac magnetic resonance (CMR) image acquisition requires subjects to hold their breath while 2D cine images are acquired. This process assumes that the heart remains in the same position across all slices. However, differences in breathhold positions or patient motion introduce 3D slice misalignments. In this work, we propose an algorithm that simultaneously aligns all SA and LA slices by maximizing the pair-wise intensity agreement between their intersections. Unlike previous works, our approach is formulated as a subject-specific optimization problem and requires no prior knowledge of the underlying anatomy. We quantitatively demonstrate that the proposed method is robust against a large range of rotations and translations by synthetically misaligning 10 motion-free datasets and aligning them back using the proposed method.</p>
  </details>
</details>
<details>
  <summary>234. <b>标题：MugenNet: A Novel Combined Convolution Neural Network and Transformer  Network with its Application for Colonic Polyp Image Segmentation</b></summary>
  <p><b>编号</b>：[709]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00726">https://arxiv.org/abs/2404.00726</a></p>
  <p><b>作者</b>：Chen Peng,  Zhiqin Qian,  Kunyu Wang,  Qi Luo,  Zhuming Bi,  Wenjun Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：polyp image segmentation, disease diagnosis, important part, part in disease, image segmentation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Biomedical image segmentation is a very important part in disease diagnosis. The term "colonic polyps" refers to polypoid lesions that occur on the surface of the colonic mucosa within the intestinal lumen. In clinical practice, early detection of polyps is conducted through colonoscopy examinations and biomedical image processing. Therefore, the accurate polyp image segmentation is of great significance in colonoscopy examinations. Convolutional Neural Network (CNN) is a common automatic segmentation method, but its main disadvantage is the long training time. Transformer utilizes a self-attention mechanism, which essentially assigns different importance weights to each piece of information, thus achieving high computational efficiency during segmentation. However, a potential drawback is the risk of information loss. In the study reported in this paper, based on the well-known hybridization principle, we proposed a method to combine CNN and Transformer to retain the strengths of both, and we applied this method to build a system called MugenNet for colonic polyp image segmentation. We conducted a comprehensive experiment to compare MugenNet with other CNN models on five publicly available datasets. The ablation experiment on MugentNet was conducted as well. The experimental results show that MugenNet achieves significantly higher processing speed and accuracy compared with CNN alone. The generalized implication with our work is a method to optimally combine two complimentary methods of machine learning.</p>
  </details>
</details>
<details>
  <summary>235. <b>标题：GAN with Skip Patch Discriminator for Biological Electron Microscopy  Image Generation</b></summary>
  <p><b>编号</b>：[713]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00558">https://arxiv.org/abs/2404.00558</a></p>
  <p><b>作者</b>：Nishith Ranjon Roy,  Nailah Rawnaq,  Tulin Kaman</p>
  <p><b>备注</b>：4 pages, International Conference on Computational and Mathematical Biomedical Engineering</p>
  <p><b>关键词</b>：challenging problem due, realistic electron microscopy, Generative Adversarial Network, electron microscopy, local structures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generating realistic electron microscopy (EM) images has been a challenging problem due to their complex global and local structures. Isola et al. proposed pix2pix, a conditional Generative Adversarial Network (GAN), for the general purpose of image-to-image translation; which fails to generate realistic EM images. We propose a new architecture for the discriminator in the GAN providing access to multiple patch sizes using skip patches and generating realistic EM images.</p>
  </details>
</details>
<details>
  <summary>236. <b>标题：Pneumonia App: a mobile application for efficient pediatric pneumonia  diagnosis using explainable convolutional neural networks (CNN)</b></summary>
  <p><b>编号</b>：[715]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00549">https://arxiv.org/abs/2404.00549</a></p>
  <p><b>作者</b>：Jiaming Deng,  Zhenglin Chen,  Minjiang Chen,  Lulu Xu,  Jiaqi Yang,  Zhendong Luo,  Peiwu Qin</p>
  <p><b>备注</b>：27 Pages,7 figures</p>
  <p><b>关键词</b>：regions like China, rapid MPP detection, MPP, Mycoplasma pneumoniae pneumonia, poses significant diagnostic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Mycoplasma pneumoniae pneumonia (MPP) poses significant diagnostic challenges in pediatric healthcare, especially in regions like China where it's prevalent. We introduce PneumoniaAPP, a mobile application leveraging deep learning techniques for rapid MPP detection. Our approach capitalizes on convolutional neural networks (CNNs) trained on a comprehensive dataset comprising 3345 chest X-ray (CXR) images, which includes 833 CXR images revealing MPP and additionally augmented with samples from a public dataset. The CNN model achieved an accuracy of 88.20% and an AUROC of 0.9218 across all classes, with a specific accuracy of 97.64% for the mycoplasma class, as demonstrated on the testing dataset. Furthermore, we integrated explainability techniques into PneumoniaAPP to aid respiratory physicians in lung opacity localization. Our contribution extends beyond existing research by targeting pediatric MPP, emphasizing the age group of 0-12 years, and prioritizing deployment on mobile devices. This work signifies a significant advancement in pediatric pneumonia diagnosis, offering a reliable and accessible tool to alleviate diagnostic burdens in healthcare settings.</p>
  </details>
</details>
<details>
  <summary>237. <b>标题：Score-Based Diffusion Models for Photoacoustic Tomography Image  Reconstruction</b></summary>
  <p><b>编号</b>：[720]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00471">https://arxiv.org/abs/2404.00471</a></p>
  <p><b>作者</b>：Sreemanti Dey,  Snigdha Saha,  Berthy T. Feng,  Manxiu Cui,  Laure Delisle,  Oscar Leong,  Lihong V. Wang,  Katherine L. Bouman</p>
  <p><b>备注</b>：5 pages</p>
  <p><b>关键词</b>：ultrasound imaging depth, rapidly-evolving medical imaging, medical imaging modality, combines optical absorption, optical absorption contrast</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Photoacoustic tomography (PAT) is a rapidly-evolving medical imaging modality that combines optical absorption contrast with ultrasound imaging depth. One challenge in PAT is image reconstruction with inadequate acoustic signals due to limited sensor coverage or due to the density of the transducer array. Such cases call for solving an ill-posed inverse reconstruction problem. In this work, we use score-based diffusion models to solve the inverse problem of reconstructing an image from limited PAT measurements. The proposed approach allows us to incorporate an expressive prior learned by a diffusion model on simulated vessel structures while still being robust to varying transducer sparsity conditions.</p>
  </details>
</details>
<details>
  <summary>238. <b>标题：YNetr: Dual-Encoder architecture on Plain Scan Liver Tumors (PSLT)</b></summary>
  <p><b>编号</b>：[725]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00327">https://arxiv.org/abs/2404.00327</a></p>
  <p><b>作者</b>：Wen Sheng,  Zhong Zheng,  Jiajun Liu,  Han Lu,  Hanyuan Zhang,  Zhengyong Jiang,  Zhihong Zhang,  Daoping Zhu</p>
  <p><b>备注</b>：15 pages</p>
  <p><b>关键词</b>：health concern worldwide, Scan Liver Tumors, Plain Scan Liver, significant health concern, plain scan</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Background: Liver tumors are abnormal growths in the liver that can be either benign or malignant, with liver cancer being a significant health concern worldwide. However, there is no dataset for plain scan segmentation of liver tumors, nor any related algorithms. To fill this gap, we propose Plain Scan Liver Tumors(PSLT) and YNetr. Methods: A collection of 40 liver tumor plain scan segmentation datasets was assembled and annotated. Concurrently, we utilized Dice coefficient as the metric for assessing the segmentation outcomes produced by YNetr, having advantage of capturing different frequency information. Results: The YNetr model achieved a Dice coefficient of 62.63% on the PSLT dataset, surpassing the other publicly available model by an accuracy margin of 1.22%. Comparative evaluations were conducted against a range of models including UNet 3+, XNet, UNetr, Swin UNetr, Trans-BTS, COTr, nnUNetv2 (2D), nnUNetv2 (3D fullres), MedNext (2D) and MedNext(3D fullres). Conclusions: We not only proposed a dataset named PSLT(Plain Scan Liver Tumors), but also explored a structure called YNetr that utilizes wavelet transform to extract different frequency information, which having the SOTA in PSLT by experiments.</p>
  </details>
</details>
<details>
  <summary>239. <b>标题：Learned Scanpaths Aid Blind Panoramic Video Quality Assessment</b></summary>
  <p><b>编号</b>：[728]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00252">https://arxiv.org/abs/2404.00252</a></p>
  <p><b>作者</b>：Kanglong Fan,  Wen Wen,  Mu Li,  Yifan Peng,  Kede Ma</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：interactive viewing experience, blind PVQA, advantage of providing, providing an immersive, immersive and interactive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Panoramic videos have the advantage of providing an immersive and interactive viewing experience. Nevertheless, their spherical nature gives rise to various and uncertain user viewing behaviors, which poses significant challenges for panoramic video quality assessment (PVQA). In this work, we propose an end-to-end optimized, blind PVQA method with explicit modeling of user viewing patterns through visual scanpaths. Our method consists of two modules: a scanpath generator and a quality assessor. The scanpath generator is initially trained to predict future scanpaths by minimizing their expected code length and then jointly optimized with the quality assessor for quality prediction. Our blind PVQA method enables direct quality assessment of panoramic images by treating them as videos composed of identical frames. Experiments on three public panoramic image and video quality datasets, encompassing both synthetic and authentic distortions, validate the superiority of our blind PVQA model over existing methods.</p>
  </details>
</details>
<details>
  <summary>240. <b>标题：An Interpretable Cross-Attentive Multi-modal MRI Fusion Framework for  Schizophrenia Diagnosis</b></summary>
  <p><b>编号</b>：[734]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00144">https://arxiv.org/abs/2404.00144</a></p>
  <p><b>作者</b>：Ziyu Zhou,  Anton Orlichenko,  Gang Qu,  Zening Fu,  Vince D Calhoun,  Zhengming Ding,  Yu-Ping Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：structural magnetic resonance, magnetic resonance imaging, mental disorder, structural magnetic, magnetic resonance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Both functional and structural magnetic resonance imaging (fMRI and sMRI) are widely used for the diagnosis of mental disorder. However, combining complementary information from these two modalities is challenging due to their heterogeneity. Many existing methods fall short of capturing the interaction between these modalities, frequently defaulting to a simple combination of latent features. In this paper, we propose a novel Cross-Attentive Multi-modal Fusion framework (CAMF), which aims to capture both intra-modal and inter-modal relationships between fMRI and sMRI, enhancing multi-modal data representation. Specifically, our CAMF framework employs self-attention modules to identify interactions within each modality while cross-attention modules identify interactions between modalities. Subsequently, our approach optimizes the integration of latent features from both modalities. This approach significantly improves classification accuracy, as demonstrated by our evaluations on two extensive multi-modal brain imaging datasets, where CAMF consistently outperforms existing methods. Furthermore, the gradient-guided Score-CAM is applied to interpret critical functional networks and brain regions involved in schizophrenia. The bio-markers identified by CAMF align with established research, potentially offering new insights into the diagnosis and pathological endophenotypes of schizophrenia.</p>
  </details>
</details>
<details>
  <summary>241. <b>标题：FetalDiffusion: Pose-Controllable 3D Fetal MRI Synthesis with  Conditional Diffusion Model</b></summary>
  <p><b>编号</b>：[735]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00132">https://arxiv.org/abs/2404.00132</a></p>
  <p><b>作者</b>：Molin Zhang,  Polina Golland,  Patricia Ellen Grant,  Elfar Adalsteinsson</p>
  <p><b>备注</b>：8 pages, 3 figures, 2 tables, submitted to MICCAI 2024, code available if accepted</p>
  <p><b>关键词</b>：fetal MRI, EPI fetal MRI, fast acquisition sequences, synthetic fetal MRI, fetal pose estimation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The quality of fetal MRI is significantly affected by unpredictable and substantial fetal motion, leading to the introduction of artifacts even when fast acquisition sequences are employed. The development of 3D real-time fetal pose estimation approaches on volumetric EPI fetal MRI opens up a promising avenue for fetal motion monitoring and prediction. Challenges arise in fetal pose estimation due to limited number of real scanned fetal MR training images, hindering model generalization when the acquired fetal MRI lacks adequate pose.
In this study, we introduce FetalDiffusion, a novel approach utilizing a conditional diffusion model to generate 3D synthetic fetal MRI with controllable pose. Additionally, an auxiliary pose-level loss is adopted to enhance model performance. Our work demonstrates the success of this proposed model by producing high-quality synthetic fetal MRI images with accurate and recognizable fetal poses, comparing favorably with in-vivo real fetal MRI. Furthermore, we show that the integration of synthetic fetal MR images enhances the fetal pose estimation model's performance, particularly when the number of available real scanned data is limited resulting in 15.4% increase in PCK and 50.2% reduced in mean error. All experiments are done on a single 32GB V100 GPU. Our method holds promise for improving real-time tracking models, thereby addressing fetal motion issues more effectively.</p>
  </details>
</details>
<h1>机器学习</h1>
<details>
  <summary>1. <b>标题：NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation  Learning for Neural Radiance Fields</b></summary>
  <p><b>编号</b>：[1]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01300">https://arxiv.org/abs/2404.01300</a></p>
  <p><b>作者</b>：Muhammad Zubair Irshad,  Sergey Zakahrov,  Vitor Guizilini,  Adrien Gaidon,  Zsolt Kira,  Rares Ambrus</p>
  <p><b>备注</b>：29 pages, 13 figures. Project Page: this https URL</p>
  <p><b>关键词</b>：Neural fields excel, Neural fields, visual world, excel in computer, ability to understand</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural fields excel in computer vision and robotics due to their ability to understand the 3D visual world such as inferring semantics, geometry, and dynamics. Given the capabilities of neural fields in densely representing a 3D scene from 2D images, we ask the question: Can we scale their self-supervised pretraining, specifically using masked autoencoders, to generate effective 3D representations from posed RGB images. Owing to the astounding success of extending transformers to novel data modalities, we employ standard 3D Vision Transformers to suit the unique formulation of NeRFs. We leverage NeRF's volumetric grid as a dense input to the transformer, contrasting it with other 3D representations such as pointclouds where the information density can be uneven, and the representation is irregular. Due to the difficulty of applying masked autoencoders to an implicit representation, such as NeRF, we opt for extracting an explicit representation that canonicalizes scenes across domains by employing the camera trajectory for sampling. Our goal is made possible by masking random patches from NeRF's radiance and density grid and employing a standard 3D Swin Transformer to reconstruct the masked patches. In doing so, the model can learn the semantic and spatial structure of complete scenes. We pretrain this representation at scale on our proposed curated posed-RGB data, totaling over 1.6 million images. Once pretrained, the encoder is used for effective 3D transfer learning. Our novel self-supervised pretraining for NeRFs, NeRF-MAE, scales remarkably well and improves performance on various challenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining, NeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF scene understanding baselines on Front3D and ScanNet datasets with an absolute performance improvement of over 20% AP50 and 8% AP25 for 3D object detection.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：CausalChaos! Dataset for Comprehensive Causal Action Question Answering  Over Longer Causal Chains Grounded in Dynamic Visual Scenes</b></summary>
  <p><b>编号</b>：[2]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01299">https://arxiv.org/abs/2404.01299</a></p>
  <p><b>作者</b>：Ting En Lam,  Yuhan Chen,  Elston Tan,  Eric Peh,  Ruirui Chen,  Paritosh Parmar,  Basura Fernando</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：garnered increasing interest, video question answering, causal reasoning analysis, increasing interest, reasoning analysis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Causal video question answering (QA) has garnered increasing interest, yet existing datasets often lack depth in causal reasoning analysis. To address this gap, we capitalize on the unique properties of cartoons and construct CausalChaos!, a novel, challenging causal Why-QA dataset built upon the iconic "Tom and Jerry" cartoon series. With thoughtful questions and multi-level answers, our dataset contains much longer causal chains embedded in dynamic interactions and visuals, at the same time principles of animation allows animators to create well-defined, unambiguous causal relationships. These factors allow models to solve more challenging, yet well-defined causal relationships. We also introduce hard negative mining, including CausalConfusion version. While models perform well, there is much room for improvement, especially, on open-ended answers. We identify more advanced/explicit causal relationship modeling and joint modeling of vision and language as the immediate areas for future efforts to focus upon. Along with the other complementary datasets, our new challenging dataset will pave the way for these developments in the field. We will release our dataset, codes, and models to help future efforts in this domain.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Measuring Style Similarity in Diffusion Models</b></summary>
  <p><b>编号</b>：[8]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01292">https://arxiv.org/abs/2404.01292</a></p>
  <p><b>作者</b>：Gowthami Somepalli,  Anubhav Gupta,  Kamal Gupta,  Shramay Palta,  Micah Goldblum,  Jonas Geiping,  Abhinav Shrivastava,  Tom Goldstein</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：graphic designers, style, training data, Generative models, image</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generative models are now widely used by graphic designers and artists. Prior works have shown that these models remember and often replicate content from their training data during generation. Hence as their proliferation increases, it has become important to perform a database search to determine whether the properties of the image are attributable to specific training data, every time before a generated image is used for professional purposes. Existing tools for this purpose focus on retrieving images of similar semantic content. Meanwhile, many artists are concerned with style replication in text-to-image models. We present a framework for understanding and extracting style descriptors from images. Our framework comprises a new dataset curated using the insight that style is a subjective property of an image that captures complex yet meaningful interactions of factors including but not limited to colors, textures, shapes, etc. We also propose a method to extract style descriptors that can be used to attribute style of a generated image to the images used in the training dataset of a text-to-image model. We showcase promising results in various style retrieval tasks. We also quantitatively and qualitatively analyze style attribution and matching in the Stable Diffusion model. Code and artifacts are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Evaluating Text-to-Visual Generation with Image-to-Text Generation</b></summary>
  <p><b>编号</b>：[9]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01291">https://arxiv.org/abs/2404.01291</a></p>
  <p><b>作者</b>：Zhiqiu Lin,  Deepak Pathak,  Baiqi Li,  Jiayao Li,  Xide Xia,  Graham Neubig,  Pengchuan Zhang,  Deva Ramanan</p>
  <p><b>备注</b>：We open-source our data, model, and code at: this https URL ; Project page: this https URL</p>
  <p><b>关键词</b>：comprehensive evaluation remains, evaluation remains challenging, comprehensive evaluation, significant progress, progress in generative</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite significant progress in generative AI, comprehensive evaluation remains challenging because of the lack of effective metrics and standardized benchmarks. For instance, the widely-used CLIPScore measures the alignment between a (generated) image and text prompt, but it fails to produce reliable scores for complex prompts involving compositions of objects, attributes, and relations. One reason is that text encoders of CLIP can notoriously act as a "bag of words", conflating prompts such as "the horse is eating the grass" with "the grass is eating the horse". To address this, we introduce the VQAScore, which uses a visual-question-answering (VQA) model to produce an alignment score by computing the probability of a "Yes" answer to a simple "Does this figure show '{text}'?" question. Though simpler than prior art, VQAScore computed with off-the-shelf models produces state-of-the-art results across many (8) image-text alignment benchmarks. We also compute VQAScore with an in-house model that follows best practices in the literature. For example, we use a bidirectional image-question encoder that allows image embeddings to depend on the question being asked (and vice versa). Our in-house model, CLIP-FlanT5, outperforms even the strongest baselines that make use of the proprietary GPT-4V. Interestingly, although we train with only images, VQAScore can also align text with video and 3D models. VQAScore allows researchers to benchmark text-to-visual generation using complex texts that capture the compositional structure of real-world prompts. We introduce GenAI-Bench, a more challenging benchmark with 1,600 compositional text prompts that require parsing scenes, objects, attributes, relationships, and high-order reasoning like comparison and logic. GenAI-Bench also offers over 15,000 human ratings for leading image and video generation models such as Stable Diffusion, DALL-E 3, and Gen2.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：TWIN-GPT: Digital Twins for Clinical Trials via Large Language Model</b></summary>
  <p><b>编号</b>：[16]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01273">https://arxiv.org/abs/2404.01273</a></p>
  <p><b>作者</b>：Yue Wang,  Yingzhou Lu,  Yinlong Xu,  Zihan Ma,  Hongxia Xu,  Bang Du,  Honghao Gao,  Jian Wu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：simulate real-world scenarios, clinical trial outcome, enhance patient safety, broader scientific knowledge, virtual clinical trials</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, there has been a burgeoning interest in virtual clinical trials, which simulate real-world scenarios and hold the potential to significantly enhance patient safety, expedite development, reduce costs, and contribute to the broader scientific knowledge in healthcare. Existing research often focuses on leveraging electronic health records (EHRs) to support clinical trial outcome prediction. Yet, trained with limited clinical trial outcome data, existing approaches frequently struggle to perform accurate predictions. Some research has attempted to generate EHRs to augment model development but has fallen short in personalizing the generation for individual patient profiles. Recently, the emergence of large language models has illuminated new possibilities, as their embedded comprehensive clinical knowledge has proven beneficial in addressing medical issues. In this paper, we propose a large language model-based digital twin creation approach, called TWIN-GPT. TWIN-GPT can establish cross-dataset associations of medical information given limited data, generating unique personalized digital twins for different patients, thereby preserving individual patient characteristics. Comprehensive experiments show that using digital twins created by TWIN-GPT can boost clinical trial outcome prediction, exceeding various previous prediction approaches. Besides, we also demonstrate that TWIN-GPT can generate high-fidelity trial data that closely approximate specific patients, aiding in more accurate result predictions in data-scarce situations. Moreover, our study provides practical evidence for the application of digital twins in healthcare, highlighting its potential significance.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Decentralized Collaborative Learning Framework with External Privacy  Leakage Analysis</b></summary>
  <p><b>编号</b>：[18]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01270">https://arxiv.org/abs/2404.01270</a></p>
  <p><b>作者</b>：Tsuyoshi Idé,  Dzung T. Phan,  Rudy Raymond</p>
  <p><b>备注</b>：To appear in Proceeding of 2023 International workshop Blockchain Kaigi (BCK 23), JPS Conference Proceedings, 2024</p>
  <p><b>关键词</b>：next-generation Blockchain platforms, decentralized multi-task learning, Blockchain platforms, next-generation Blockchain, aiming to pave</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents two methodological advancements in decentralized multi-task learning under privacy constraints, aiming to pave the way for future developments in next-generation Blockchain platforms. First, we expand the existing framework for collaborative dictionary learning (CollabDict), which has previously been limited to Gaussian mixture models, by incorporating deep variational autoencoders (VAEs) into the framework, with a particular focus on anomaly detection. We demonstrate that the VAE-based anomaly score function shares the same mathematical structure as the non-deep model, and provide comprehensive qualitative comparison. Second, considering the widespread use of "pre-trained models," we provide a mathematical analysis on data privacy leakage when models trained with CollabDict are shared externally. We show that the CollabDict approach, when applied to Gaussian mixtures, adheres to a Renyi differential privacy criterion. Additionally, we propose a practical metric for monitoring internal privacy breaches during the learning process.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Mapping the Increasing Use of LLMs in Scientific Papers</b></summary>
  <p><b>编号</b>：[19]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01268">https://arxiv.org/abs/2404.01268</a></p>
  <p><b>作者</b>：Weixin Liang,  Yaohui Zhang,  Zhengxuan Wu,  Haley Lepp,  Wenlong Ji,  Xuandong Zhao,  Hancheng Cao,  Sheng Liu,  Siyu He,  Zhi Huang,  Diyi Yang,  Christopher Potts,  Christopher D Manning,  James Y. Zou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Scientific publishing lays, fostering collaboration, encouraging reproducibility, knowledge is accessible, publishing lays</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time. Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices. However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs. To address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time. Our statistical estimation operates on the corpus level and is more robust than inference on individual instances. Our findings reveal a steady increase in LLM usage, with the largest and fastest growth observed in Computer Science papers (up to 17.5%). In comparison, Mathematics papers and the Nature portfolio showed the least LLM modification (up to 6.3%). Moreover, at an aggregate level, our analysis reveals that higher levels of LLM-modification are associated with papers whose first authors post preprints more frequently, papers in more crowded research areas, and papers of shorter lengths. Our findings suggests that LLMs are being broadly used in scientific writings.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Bridging Remote Sensors with Multisensor Geospatial Foundation Models</b></summary>
  <p><b>编号</b>：[23]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01260">https://arxiv.org/abs/2404.01260</a></p>
  <p><b>作者</b>：Boran Han,  Shuai Zhang,  Xingjian Shi,  Markus Reichstein</p>
  <p><b>备注</b>：Accepted to CVPR</p>
  <p><b>关键词</b>：encompassing both optical, microwave technologies, offers a wealth, optical and microwave, distinct observational capabilities</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the realm of geospatial analysis, the diversity of remote sensors, encompassing both optical and microwave technologies, offers a wealth of distinct observational capabilities. Recognizing this, we present msGFM, a multisensor geospatial foundation model that effectively unifies data from four key sensor modalities. This integration spans an expansive dataset of two million multisensor images. msGFM is uniquely adept at handling both paired and unpaired sensor data. For data originating from identical geolocations, our model employs an innovative cross-sensor pretraining approach in masked image modeling, enabling the synthesis of joint representations from diverse sensors. msGFM, incorporating four remote sensors, upholds strong performance, forming a comprehensive model adaptable to various sensor types. msGFM has demonstrated enhanced proficiency in a range of both single-sensor and multisensor downstream tasks. These include scene classification, segmentation, cloud removal, and pan-sharpening. A key discovery of our research is that representations derived from natural images are not always compatible with the distinct characteristics of geospatial remote sensors, underscoring the limitations of existing representations in this field. Our work can serve as a guide for developing multisensor geospatial pretraining models, paving the way for more advanced geospatial capabilities.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：New logarithmic step size for stochastic gradient descent</b></summary>
  <p><b>编号</b>：[25]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01257">https://arxiv.org/abs/2404.01257</a></p>
  <p><b>作者</b>：M. Soheil Shamaee,  S. Fathi Hafshejani,  Z. Saeidian</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：stochastic gradient descent, warm restart technique, logarithmic step size, gradient descent, warm restart</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we propose a novel warm restart technique using a new logarithmic step size for the stochastic gradient descent (SGD) approach. For smooth and non-convex functions, we establish an $O(\frac{1}{\sqrt{T}})$ convergence rate for the SGD. We conduct a comprehensive implementation to demonstrate the efficiency of the newly proposed step size on the ~FashionMinst,~ CIFAR10, and CIFAR100 datasets. Moreover, we compare our results with nine other existing approaches and demonstrate that the new logarithmic step size improves test accuracy by $0.9\%$ for the CIFAR100 dataset when we utilize a convolutional neural network (CNN) model.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Privacy Backdoors: Enhancing Membership Inference through Poisoning  Pre-trained Models</b></summary>
  <p><b>编号</b>：[38]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01231">https://arxiv.org/abs/2404.01231</a></p>
  <p><b>作者</b>：Yuxin Wen,  Leo Marchyok,  Sanghyun Hong,  Jonas Geiping,  Tom Goldstein,  Nicholas Carlini</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：small bespoke dataset, produce application-specific models, commonplace to produce, produce application-specific, small bespoke</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>It is commonplace to produce application-specific models by fine-tuning large pre-trained models using a small bespoke dataset. The widespread availability of foundation model checkpoints on the web poses considerable risks, including the vulnerability to backdoor attacks. In this paper, we unveil a new vulnerability: the privacy backdoor attack. This black-box privacy attack aims to amplify the privacy leakage that arises when fine-tuning a model: when a victim fine-tunes a backdoored model, their training data will be leaked at a significantly higher rate than if they had fine-tuned a typical model. We conduct extensive experiments on various datasets and models, including both vision-language models (CLIP) and large language models, demonstrating the broad applicability and effectiveness of such an attack. Additionally, we carry out multiple ablation studies with different fine-tuning methods and inference strategies to thoroughly analyze this new threat. Our findings highlight a critical privacy concern within the machine learning community and call for a reevaluation of safety protocols in the use of open-source pre-trained models.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Collaborative Pareto Set Learning in Multiple Multi-Objective  Optimization Problems</b></summary>
  <p><b>编号</b>：[44]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01224">https://arxiv.org/abs/2404.01224</a></p>
  <p><b>作者</b>：Chikai Shang,  Rongguang Ye,  Jiaqi Jiang,  Fangqing Gu</p>
  <p><b>备注</b>：Accepted by IJCNN 2024</p>
  <p><b>关键词</b>：Multi-objective Optimization Problem, Pareto Set Learning, emerging research area, training neural networks, multi-objective optimization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pareto Set Learning (PSL) is an emerging research area in multi-objective optimization, focusing on training neural networks to learn the mapping from preference vectors to Pareto optimal solutions. However, existing PSL methods are limited to addressing a single Multi-objective Optimization Problem (MOP) at a time. When faced with multiple MOPs, this limitation not only leads to significant inefficiencies but also fails to exploit the potential synergies across varying MOPs. In this paper, we propose a Collaborative Pareto Set Learning (CoPSL) framework, which simultaneously learns the Pareto sets of multiple MOPs in a collaborative manner. CoPSL employs an architecture consisting of shared and MOP-specific layers, where shared layers aim to capture common relationships among MOPs collaboratively, and MOP-specific layers process these relationships to generate solution sets for each MOP. This collaborative approach enables CoPSL to efficiently learn the Pareto sets of multiple MOPs in a single run while leveraging the relationships among various MOPs. To further understand these relationships, we experimentally demonstrate that there exist shareable representations among MOPs. Leveraging these collaboratively shared representations can effectively improve the capability to approximate Pareto sets. Extensive experiments underscore the superior efficiency and robustness of CoPSL in approximating Pareto sets compared to state-of-the-art approaches on a variety of synthetic and real-world MOPs. Code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Feature Splatting: Language-Driven Physics-Based Scene Synthesis and  Editing</b></summary>
  <p><b>编号</b>：[45]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01223">https://arxiv.org/abs/2404.01223</a></p>
  <p><b>作者</b>：Ri-Zhao Qiu,  Ge Yang,  Weijia Zeng,  Xiaolong Wang</p>
  <p><b>备注</b>：Project website: this https URL</p>
  <p><b>关键词</b>：produced excellent results, primitives have produced, produced excellent, excellent results, results in modeling</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Scene representations using 3D Gaussian primitives have produced excellent results in modeling the appearance of static and dynamic 3D scenes. Many graphics applications, however, demand the ability to manipulate both the appearance and the physical properties of objects. We introduce Feature Splatting, an approach that unifies physics-based dynamic scene synthesis with rich semantics from vision language foundation models that are grounded by natural language. Our first contribution is a way to distill high-quality, object-centric vision-language features into 3D Gaussians, that enables semi-automatic scene decomposition using text queries. Our second contribution is a way to synthesize physics-based dynamics from an otherwise static scene using a particle-based simulator, in which material properties are assigned automatically via text queries. We ablate key techniques used in this pipeline, to illustrate the challenge and opportunities in using feature-carrying 3D Gaussians as a unified format for appearance, geometry, material properties and semantics grounded on natural language. Project website: this https URL</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Entity-Centric Reinforcement Learning for Object Manipulation from  Pixels</b></summary>
  <p><b>编号</b>：[46]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01220">https://arxiv.org/abs/2404.01220</a></p>
  <p><b>作者</b>：Dan Haramati,  Tal Daniel,  Aviv Tamar</p>
  <p><b>备注</b>：ICLR 2024 Spotlight. Videos and code are available on the project website: this https URL</p>
  <p><b>关键词</b>：human intelligence, hallmark of human, Reinforcement Learning, objects, Manipulating objects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Manipulating objects is a hallmark of human intelligence, and an important task in domains such as robotics. In principle, Reinforcement Learning (RL) offers a general approach to learn object manipulation. In practice, however, domains with more than a few objects are difficult for RL agents due to the curse of dimensionality, especially when learning from raw image observations. In this work we propose a structured approach for visual RL that is suitable for representing multiple objects and their interaction, and use it to learn goal-conditioned manipulation of several objects. Key to our method is the ability to handle goals with dependencies between the objects (e.g., moving objects in a certain order). We further relate our architecture to the generalization capability of the trained agent, based on a theoretical result for compositional generalization, and demonstrate agents that learn with 3 objects but generalize to similar tasks with over 10 objects. Videos and code are available on the project website: this https URL</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Towards System Modelling to Support Diseases Data Extraction from the  Electronic Health Records for Physicians Research Activities</b></summary>
  <p><b>编号</b>：[48]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01218">https://arxiv.org/abs/2404.01218</a></p>
  <p><b>作者</b>：Bushra F. Alsaqer,  Alaa F. Alsaqer,  Amna Asif</p>
  <p><b>备注</b>：15 pages, 18 figures and 12 tables</p>
  <p><b>关键词</b>：Electronic Health Records, Health Records, Electronic Health, data, increased dramatically</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The use of Electronic Health Records (EHRs) has increased dramatically in the past 15 years, as, it is considered an important source of managing data od patients. The EHRs are primary sources of disease diagnosis and demographic data of patients worldwide. Therefore, the data can be utilized for secondary tasks such as research. This paper aims to make such data usable for research activities such as monitoring disease statistics for a specific population. As a result, the researchers can detect the disease causes for the behavior and lifestyle of the target group. One of the limitations of EHRs systems is that the data is not available in the standard format but in various forms. Therefore, it is required to first convert the names of the diseases and demographics data into one standardized form to make it usable for research activities. There is a large amount of EHRs available, and solving the standardizing issues requires some optimized techniques. We used a first-hand EHR dataset extracted from EHR systems. Our application uploads the dataset from the EHRs and converts it to the ICD-10 coding system to solve the standardization problem. So, we first apply the steps of pre-processing, annotation, and transforming the data to convert it into the standard form. The data pre-processing is applied to normalize demographic formats. In the annotation step, a machine learning model is used to recognize the diseases from the text. Furthermore, the transforming step converts the disease name to the ICD-10 coding format. The model was evaluated manually by comparing its performance in terms of disease recognition with an available dictionary-based system (MetaMap). The accuracy of the proposed machine learning model is 81%, that outperformed MetaMap accuracy of 67%. This paper contributed to system modelling for EHR data extraction to support research activities.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Incorporating Domain Differential Equations into Graph Convolutional  Networks to Lower Generalization Discrepancy</b></summary>
  <p><b>编号</b>：[49]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01217">https://arxiv.org/abs/2404.01217</a></p>
  <p><b>作者</b>：Yue Sun,  Chao Chen,  Yuesheng Xu,  Sihong Xie,  Rick S. Blum,  Parv Venkitasubramaniam</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Graph Convolutional Network, time series prediction, Graph Convolutional, Ensuring both accuracy, ranging from urban</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Ensuring both accuracy and robustness in time series prediction is critical to many applications, ranging from urban planning to pandemic management. With sufficient training data where all spatiotemporal patterns are well-represented, existing deep-learning models can make reasonably accurate predictions. However, existing methods fail when the training data are drawn from different circumstances (e.g., traffic patterns on regular days) compared to test data (e.g., traffic patterns after a natural disaster). Such challenges are usually classified under domain generalization. In this work, we show that one way to address this challenge in the context of spatiotemporal prediction is by incorporating domain differential equations into Graph Convolutional Networks (GCNs). We theoretically derive conditions where GCNs incorporating such domain differential equations are robust to mismatched training and testing data compared to baseline domain agnostic models. To support our theory, we propose two domain-differential-equation-informed networks called Reaction-Diffusion Graph Convolutional Network (RDGCN), which incorporates differential equations for traffic speed evolution, and Susceptible-Infectious-Recovered Graph Convolutional Network (SIRGCN), which incorporates a disease propagation model. Both RDGCN and SIRGCN are based on reliable and interpretable domain differential equations that allow the models to generalize to unseen patterns. We experimentally show that RDGCN and SIRGCN are more robust with mismatched testing data than the state-of-the-art deep learning methods.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Novel Node Category Detection Under Subpopulation Shift</b></summary>
  <p><b>编号</b>：[50]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01216">https://arxiv.org/abs/2404.01216</a></p>
  <p><b>作者</b>：Hsing-Huan Chung,  Shravan Chaudhari,  Yoav Wald,  Xing Han,  Joydeep Ghosh</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：real-world graph data, relative proportions, Selective Link Prediction, distribution shifts, Link Prediction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In real-world graph data, distribution shifts can manifest in various ways, such as the emergence of new categories and changes in the relative proportions of existing categories. It is often important to detect nodes of novel categories under such distribution shifts for safety or insight discovery purposes. We introduce a new approach, Recall-Constrained Optimization with Selective Link Prediction (RECO-SLIP), to detect nodes belonging to novel categories in attributed graphs under subpopulation shifts. By integrating a recall-constrained learning framework with a sample-efficient link prediction mechanism, RECO-SLIP addresses the dual challenges of resilience against subpopulation shifts and the effective exploitation of graph structure. Our extensive empirical evaluation across multiple graph datasets demonstrates the superior performance of RECO-SLIP over existing methods.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Machine Unlearning for Traditional Models and Large Language Models: A  Short Survey</b></summary>
  <p><b>编号</b>：[53]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01206">https://arxiv.org/abs/2404.01206</a></p>
  <p><b>作者</b>：Yi Xu</p>
  <p><b>备注</b>：16 pages</p>
  <p><b>关键词</b>：data privacy regulations, personal data privacy, Large Language Models, privacy regulations, implementation of personal</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the implementation of personal data privacy regulations, the field of machine learning (ML) faces the challenge of the "right to be forgotten". Machine unlearning has emerged to address this issue, aiming to delete data and reduce its impact on models according to user requests. Despite the widespread interest in machine unlearning, comprehensive surveys on its latest advancements, especially in the field of Large Language Models (LLMs) is lacking. This survey aims to fill this gap by providing an in-depth exploration of machine unlearning, including the definition, classification and evaluation criteria, as well as challenges in different environments and their solutions. Specifically, this paper categorizes and investigates unlearning on both traditional models and LLMs, and proposes methods for evaluating the effectiveness and efficiency of unlearning, and standards for performance measurement. This paper reveals the limitations of current unlearning techniques and emphasizes the importance of a comprehensive unlearning evaluation to avoid arbitrary forgetting. This survey not only summarizes the key concepts of unlearning technology but also points out its prominent issues and feasible directions for future research, providing valuable guidance for scholars in the field.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Nearly-tight Approximation Guarantees for the Improving Multi-Armed  Bandits Problem</b></summary>
  <p><b>编号</b>：[57]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01198">https://arxiv.org/abs/2404.01198</a></p>
  <p><b>作者</b>：Avrim Blum,  Kavya Ravichandran</p>
  <p><b>备注</b>：12 pages, 0 figures</p>
  <p><b>关键词</b>：give nearly-tight upper, improving multi-armed bandits, multi-armed bandits problem, give nearly-tight, nearly-tight upper</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We give nearly-tight upper and lower bounds for the improving multi-armed bandits problem. An instance of this problem has $k$ arms, each of whose reward function is a concave and increasing function of the number of times that arm has been pulled so far. We show that for any randomized online algorithm, there exists an instance on which it must suffer at least an $\Omega(\sqrt{k})$ approximation factor relative to the optimal reward. We then provide a randomized online algorithm that guarantees an $O(\sqrt{k})$ approximation factor, if it is told the maximum reward achievable by the optimal arm in advance. We then show how to remove this assumption at the cost of an extra $O(\log k)$ approximation factor, achieving an overall $O(\sqrt{k} \log k)$ approximation relative to optimal.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Efficient Motion Planning for Manipulators with Control Barrier  Function-Induced Neural Controller</b></summary>
  <p><b>编号</b>：[63]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01184">https://arxiv.org/abs/2404.01184</a></p>
  <p><b>作者</b>：Mingxin Yu,  Chenning Yu,  M-Mahdi Naddaf-Sh,  Devesh Upadhyay,  Sicun Gao,  Chuchu Fan</p>
  <p><b>备注</b>：Accepted by IEEE International Conference on Robotics and Automation (ICRA2024)</p>
  <p><b>关键词</b>：high sampling complexity, expensive collision checking, sampling complexity, real time, manipulators in crowded</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sampling-based motion planning methods for manipulators in crowded environments often suffer from expensive collision checking and high sampling complexity, which make them difficult to use in real time. To address this issue, we propose a new generalizable control barrier function (CBF)-based steering controller to reduce the number of samples needed in a sampling-based motion planner RRT. Our method combines the strength of CBF for real-time collision-avoidance control and RRT for long-horizon motion planning, by using CBF-induced neural controller (CBF-INC) to generate control signals that steer the system towards sampled configurations by RRT. CBF-INC is learned as Neural Networks and has two variants handling different inputs, respectively: state (signed distance) input and point-cloud input from LiDAR. In the latter case, we also study two different settings: fully and partially observed environmental information. Compared to manually crafted CBF which suffers from over-approximating robot geometry, CBF-INC can balance safety and goal-reaching better without being over-conservative. Given state-based input, our neural CBF-induced neural controller-enhanced RRT (CBF-INC-RRT) can increase the success rate by 14% while reducing the number of nodes explored by 30%, compared with vanilla RRT on hard test cases. Given LiDAR input where vanilla RRT is not directly applicable, we demonstrate that our CBF-INC-RRT can improve the success rate by 10%, compared with planning with other steering controllers. Our project page with supplementary material is at this https URL.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：BEM: Balanced and Entropy-based Mix for Long-Tailed Semi-Supervised  Learning</b></summary>
  <p><b>编号</b>：[65]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01179">https://arxiv.org/abs/2404.01179</a></p>
  <p><b>作者</b>：Hongwei Zheng,  Linyuan Zhou,  Han Li,  Jinming Su,  Xiaoming Wei,  Xiaoming Xu</p>
  <p><b>备注</b>：This paper is accepted to CVPR 2024. The supplementary material is included</p>
  <p><b>关键词</b>：long-tailed semi-supervised learning, long-tailed semi-supervised, SSL, semi-supervised learning, play a crucial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Data mixing methods play a crucial role in semi-supervised learning (SSL), but their application is unexplored in long-tailed semi-supervised learning (LTSSL). The primary reason is that the in-batch mixing manner fails to address class imbalance. Furthermore, existing LTSSL methods mainly focus on re-balancing data quantity but ignore class-wise uncertainty, which is also vital for class balance. For instance, some classes with sufficient samples might still exhibit high uncertainty due to indistinguishable features. To this end, this paper introduces the Balanced and Entropy-based Mix (BEM), a pioneering mixing approach to re-balance the class distribution of both data quantity and uncertainty. Specifically, we first propose a class balanced mix bank to store data of each class for mixing. This bank samples data based on the estimated quantity distribution, thus re-balancing data quantity. Then, we present an entropy-based learning approach to re-balance class-wise uncertainty, including entropy-based sampling strategy, entropy-based selection module, and entropy-based class balanced loss. Our BEM first leverages data mixing for improving LTSSL, and it can also serve as a complement to the existing re-balancing methods. Experimental results show that BEM significantly enhances various LTSSL frameworks and achieves state-of-the-art performances across multiple benchmarks.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Do LLMs Find Human Answers To Fact-Driven Questions Perplexing? A Case  Study on Reddit</b></summary>
  <p><b>编号</b>：[85]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01147">https://arxiv.org/abs/2404.01147</a></p>
  <p><b>作者</b>：Parker Seegmiller,  Joseph Gatto,  Omar Sharif,  Madhusudan Basak,  Sarah Masud Preum</p>
  <p><b>备注</b>：4 pages, 2 figures</p>
  <p><b>关键词</b>：Large language models, correctly answering questions, Large language, online discourse, proficient in correctly</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have been shown to be proficient in correctly answering questions in the context of online discourse. However, the study of using LLMs to model human-like answers to fact-driven social media questions is still under-explored. In this work, we investigate how LLMs model the wide variety of human answers to fact-driven questions posed on several topic-specific Reddit communities, or subreddits. We collect and release a dataset of 409 fact-driven questions and 7,534 diverse, human-rated answers from 15 r/Ask{Topic} communities across 3 categories: profession, social identity, and geographic location. We find that LLMs are considerably better at modeling highly-rated human answers to such questions, as opposed to poorly-rated human answers. We present several directions for future research based on our initial findings.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Sequential-in-time training of nonlinear parametrizations for solving  time-dependent partial differential equations</b></summary>
  <p><b>编号</b>：[86]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01145">https://arxiv.org/abs/2404.01145</a></p>
  <p><b>作者</b>：Huan Zhang,  Yifan Chen,  Eric Vanden-Eijnden,  Benjamin Peherstorfer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：fit nonlinear parametrizations, approximate solution trajectories, partial differential equations, equations over time, solve a sequence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sequential-in-time methods solve a sequence of training problems to fit nonlinear parametrizations such as neural networks to approximate solution trajectories of partial differential equations over time. This work shows that sequential-in-time training methods can be understood broadly as either optimize-then-discretize (OtD) or discretize-then-optimize (DtO) schemes, which are well known concepts in numerical analysis. The unifying perspective leads to novel stability and a posteriori error analysis results that provide insights into theoretical and numerical aspects that are inherent to either OtD or DtO schemes such as the tangent space collapse phenomenon, which is a form of over-fitting. Additionally, the unified perspective facilitates establishing connections between variants of sequential-in-time training methods, which is demonstrated by identifying natural gradient descent methods on energy functionals as OtD schemes applied to the corresponding gradient flows.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：SoK: A Review of Differentially Private Linear Models For  High-Dimensional Data</b></summary>
  <p><b>编号</b>：[88]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01141">https://arxiv.org/abs/2404.01141</a></p>
  <p><b>作者</b>：Amol Khanna,  Edward Raff,  Nathan Inkawhich</p>
  <p><b>备注</b>：21 pages, 7 figures. To be published at the 2nd IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)</p>
  <p><b>关键词</b>：high dimensions, prone to overfitting, memorization in high, data science, Linear models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Linear models are ubiquitous in data science, but are particularly prone to overfitting and data memorization in high dimensions. To guarantee the privacy of training data, differential privacy can be used. Many papers have proposed optimization techniques for high-dimensional differentially private linear models, but a systematic comparison between these methods does not exist. We close this gap by providing a comprehensive review of optimization methods for private high-dimensional linear models. Empirical tests on all methods demonstrate robust and coordinate-optimized algorithms perform best, which can inform future research. Code for implementing all methods is released online.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Enhanced Precision in Rainfall Forecasting for Mumbai: Utilizing Physics  Informed ConvLSTM2D Models for Finer Spatial and Temporal Resolution</b></summary>
  <p><b>编号</b>：[99]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01122">https://arxiv.org/abs/2404.01122</a></p>
  <p><b>作者</b>：Ajay Devda,  Akshay Sunil,  Murthy R,  B Deepthi</p>
  <p><b>备注</b>：Submitted to Computer and Geosciences. arXiv admin note: substantial text overlap with arXiv:2310.09311</p>
  <p><b>关键词</b>：complex atmospheric behaviour, convective rain events, elevated humidity levels, atmospheric behaviour, elevated humidity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Forecasting rainfall in tropical areas is challenging due to complex atmospheric behaviour, elevated humidity levels, and the common presence of convective rain events. In the Indian context, the difficulty is further exacerbated because of the monsoon intra seasonal oscillations, which introduce significant variability in rainfall patterns over short periods. Earlier investigations into rainfall prediction leveraged numerical weather prediction methods, along with statistical and deep learning approaches. This study introduces deep learning spatial model aimed at enhancing rainfall prediction accuracy on a finer scale. In this study, we hypothesize that integrating physical understanding improves the precipitation prediction skill of deep learning models with high precision for finer spatial scales, such as cities. To test this hypothesis, we introduce a physics informed ConvLSTM2D model to predict precipitation 6hr and 12hr ahead for Mumbai, India. We utilize ERA5 reanalysis data select predictor variables, across various geopotential levels. The ConvLSTM2D model was trained on the target variable precipitation for 4 different grids representing different spatial grid locations of Mumbai. Thus, the use of the ConvLSTM2D model for rainfall prediction, utilizing physics informed data from specific grids with limited spatial information, reflects current advancements in meteorological research that emphasize both efficiency and localized precision.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：UFID: A Unified Framework for Input-level Backdoor Detection on  Diffusion Models</b></summary>
  <p><b>编号</b>：[111]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01101">https://arxiv.org/abs/2404.01101</a></p>
  <p><b>作者</b>：Zihan Guan,  Mengxuan Hu,  Sheng Li,  Anil Vullikanti</p>
  <p><b>备注</b>：20 pages,18 figures</p>
  <p><b>关键词</b>：malicious attackers inject, Diffusion Models, attackers inject backdoors, training stage, training samples</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diffusion Models are vulnerable to backdoor attacks, where malicious attackers inject backdoors by poisoning some parts of the training samples during the training stage. This poses a serious threat to the downstream users, who query the diffusion models through the API or directly download them from the internet. To mitigate the threat of backdoor attacks, there have been a plethora of investigations on backdoor detections. However, none of them designed a specialized backdoor detection method for diffusion models, rendering the area much under-explored. Moreover, these prior methods mainly focus on the traditional neural networks in the classification task, which cannot be adapted to the backdoor detections on the generative task easily. Additionally, most of the prior methods require white-box access to model weights and architectures, or the probability logits as additional information, which are not always practical. In this paper, we propose a Unified Framework for Input-level backdoor Detection (UFID) on the diffusion models, which is motivated by observations in the diffusion models and further validated with a theoretical causality analysis. Extensive experiments across different datasets on both conditional and unconditional diffusion models show that our method achieves a superb performance on detection effectiveness and run-time efficiency. The code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Finite Sample Frequency Domain Identification</b></summary>
  <p><b>编号</b>：[112]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01100">https://arxiv.org/abs/2404.01100</a></p>
  <p><b>作者</b>：Anastasios Tsiamis,  Mohamed Abdalmoaty,  Roy S. Smith,  John Lygeros</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：non-parametric frequency-domain system, frequency-domain system identification, study non-parametric frequency-domain, mathrm, Empirical Transfer Function</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study non-parametric frequency-domain system identification from a finite-sample perspective. We assume an open loop scenario where the excitation input is periodic and consider the Empirical Transfer Function Estimate (ETFE), where the goal is to estimate the frequency response at certain desired (evenly-spaced) frequencies, given input-output samples. We show that under sub-Gaussian colored noise (in time-domain) and stability assumptions, the ETFE estimates are concentrated around the true values. The error rate is of the order of $\mathcal{O}((d_{\mathrm{u}}+\sqrt{d_{\mathrm{u}}d_{\mathrm{y}}})\sqrt{M/N_{\mathrm{tot}}})$, where $N_{\mathrm{tot}}$ is the total number of samples, $M$ is the number of desired frequencies, and $d_{\mathrm{u}},\,d_{\mathrm{y}}$ are the dimensions of the input and output signals respectively. This rate remains valid for general irrational transfer functions and does not require a finite order state-space representation. By tuning $M$, we obtain a $N_{\mathrm{tot}}^{-1/3}$ finite-sample rate for learning the frequency response over all frequencies in the $ \mathcal{H}_{\infty}$ norm. Our result draws upon an extension of the Hanson-Wright inequality to semi-infinite matrices. We study the finite-sample behavior of ETFE in simulations.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：What's in Your "Safe" Data?: Identifying Benign Data that Breaks Safety</b></summary>
  <p><b>编号</b>：[113]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01099">https://arxiv.org/abs/2404.01099</a></p>
  <p><b>作者</b>：Luxi He,  Mengzhou Xia,  Peter Henderson</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Current Large Language, Large Language Models, Large Language, Current Large, Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Current Large Language Models (LLMs), even those tuned for safety and alignment, are susceptible to jailbreaking. Some have found that just further fine-tuning an aligned model with benign data (i.e., data without harmful content) surprisingly leads to substantial degradation in safety. We delve into the data-centric aspects of why benign fine-tuning inadvertently contributes to jailbreaking. First, we represent fine-tuning data through two lenses: representation and gradient spaces. Furthermore, we propose a bi-directional anchoring method that prioritizes data points that are close to harmful examples and distant from benign ones. By doing so, our approach effectively identifies subsets of benign data that are more likely to degrade the model's safety after fine-tuning. Training on just 100 of these seemingly benign datapoints can lead to the fine-tuned model affirmatively responding to > 70% of tested harmful requests, compared to < 20% after fine-tuning on randomly selected data. We further find that selected data are often in the form of lists and bullet points, or math questions.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Energy Model-based Accurate Shapley Value Estimation for Interpretable  Deep Learning Predictive Modelling</b></summary>
  <p><b>编号</b>：[122]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01078">https://arxiv.org/abs/2404.01078</a></p>
  <p><b>作者</b>：Cheng Lu,  Jiusun Zeng,  Yu Xia,  Jinhui Cai,  Shihua Luo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：learning based predictive, explainable artificial intelligence, based predictive models, Shapley, artificial intelligence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As a favorable tool for explainable artificial intelligence (XAI), Shapley value has been widely used to interpret deep learning based predictive models. However, accurate and efficient estimation of Shapley value is a difficult task since the computation load grows exponentially with the increase of input features. Most existing accelerated Shapley value estimation methods have to compromise on estimation accuracy with efficiency. In this article, we present EmSHAP(Energy model-based Shapley value estimation), which can effectively approximate the expectation of Shapley contribution function/deep learning model under arbitrary subset of features given the rest. In order to determine the proposal conditional distribution in the energy model, a gated recurrent unit(GRU) is introduced by mapping the input features onto a hidden space, so that the impact of input feature orderings can be eliminated. In addition, a dynamic masking scheme is proposed to improve the generalization ability. It is proved in Theorems 1, 2 and 3 that EmSHAP achieves tighter error bound than state-of-the-art methods like KernelSHAP and VAEAC, leading to higher estimation accuracy. Finally, case studies on a medical application and an industrial application show that the proposed Shapley value-based explainable framework exhibits enhanced estimation accuracy without compromise on efficiency.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Prompt Learning for Oriented Power Transmission Tower Detection in  High-Resolution SAR Images</b></summary>
  <p><b>编号</b>：[124]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01074">https://arxiv.org/abs/2404.01074</a></p>
  <p><b>作者</b>：Tianyang Li,  Chao Wang,  Hong Zhang</p>
  <p><b>备注</b>：22 pages, 12figures</p>
  <p><b>关键词</b>：synthetic aperture radar, comparatively small size, hindering tower identification, frequently hindering tower, Detecting transmission towers</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Detecting transmission towers from synthetic aperture radar (SAR) images remains a challenging task due to the comparatively small size and side-looking geometry, with background clutter interference frequently hindering tower identification. A large number of interfering signals superimposes the return signal from the tower. We found that localizing or prompting positions of power transmission towers is beneficial to address this obstacle. Based on this revelation, this paper introduces prompt learning into the oriented object detector (P2Det) for multimodal information learning. P2Det contains the sparse prompt coding and cross-attention between the multimodal data. Specifically, the sparse prompt encoder (SPE) is proposed to represent point locations, converting prompts into sparse embeddings. The image embeddings are generated through the Transformer layers. Then a two-way fusion module (TWFM) is proposed to calculate the cross-attention of the two different embeddings. The interaction of image-level and prompt-level features is utilized to address the clutter interference. A shape-adaptive refinement module (SARM) is proposed to reduce the effect of aspect ratio. Extensive experiments demonstrated the effectiveness of the proposed model on high-resolution SAR images. P2Det provides a novel insight for multimodal object detection due to its competitive performance.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：A comparison of Single- and Double-generator formalisms for  Thermodynamics-Informed Neural Networks</b></summary>
  <p><b>编号</b>：[132]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01060">https://arxiv.org/abs/2404.01060</a></p>
  <p><b>作者</b>：Pau Urdeitx,  Icíar Alfaro,  David González,  Francisco Chinesta,  Elías Cueto</p>
  <p><b>备注</b>：22 pages, 17 figures</p>
  <p><b>关键词</b>：predict physical phenomena, biases significantly increase, development of inductive, accuracy and robustness, robustness of neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The development of inductive biases has been shown to be a very effective way to increase the accuracy and robustness of neural networks, particularly when they are used to predict physical phenomena. These biases significantly increase the certainty of predictions, decrease the error made and allow considerably smaller datasets to be used.
There are a multitude of methods in the literature to develop these biases. One of the most effective ways, when dealing with physical phenomena, is to introduce physical principles of recognised validity into the network architecture.
The problem becomes more complex without knowledge of the physical principles governing the phenomena under study. A very interesting possibility then is to turn to the principles of thermodynamics, which are universally valid, regardless of the level of abstraction of the description sought for the phenomenon under study.
To ensure compliance with the principles of thermodynamics, there are formulations that have a long tradition in many branches of science. In the field of rheology, for example, two main types of formalisms are used to ensure compliance with these principles: one-generator and two-generator formalisms. In this paper we study the advantages and disadvantages of each, using classical problems with known solutions and synthetic data.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：A Novel Audio Representation for Music Genre Identification in MIR</b></summary>
  <p><b>编号</b>：[134]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01058">https://arxiv.org/abs/2404.01058</a></p>
  <p><b>作者</b>：Navin Kamuni,  Mayank Jindal,  Arpita Soni,  Sukender Reddy Mallreddy,  Sharath Chandra Macha</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Information Retrieval downstream, Music Information Retrieval, Retrieval downstream tasks, Information Retrieval, Jukebox audio representation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>For Music Information Retrieval downstream tasks, the most common audio representation is time-frequency-based, such as Mel spectrograms. In order to identify musical genres, this study explores the possibilities of a new form of audio representation one of the most usual MIR downstream tasks. Therefore, to discretely encoding music using deep vector quantization; a novel audio representation was created for the innovative generative music model i.e. Jukebox. The effectiveness of Jukebox's audio representation is compared to Mel spectrograms using a dataset that is almost equivalent to State-of-the-Art (SOTA) and an almost same transformer design. The results of this study imply that, at least when the transformers are pretrained using a very modest dataset of 20k tracks, Jukebox's audio representation is not superior to Mel spectrograms. This could be explained by the fact that Jukebox's audio representation does not sufficiently take into account the peculiarities of human hearing perception. On the other hand, Mel spectrograms are specifically created with the human auditory sense in mind.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Drag Your Noise: Interactive Point-based Editing via Diffusion Semantic  Propagation</b></summary>
  <p><b>编号</b>：[139]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01050">https://arxiv.org/abs/2404.01050</a></p>
  <p><b>作者</b>：Haofeng Liu,  Chenshu Xu,  Yifei Yang,  Lihua Zeng,  Shengfeng He</p>
  <p><b>备注</b>：Accepted by CVPR 2024</p>
  <p><b>关键词</b>：existing generative models, generative models, essential tool, tool to complement, complement the controllability</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Point-based interactive editing serves as an essential tool to complement the controllability of existing generative models. A concurrent work, DragDiffusion, updates the diffusion latent map in response to user inputs, causing global latent map alterations. This results in imprecise preservation of the original content and unsuccessful editing due to gradient vanishing. In contrast, we present DragNoise, offering robust and accelerated editing without retracing the latent map. The core rationale of DragNoise lies in utilizing the predicted noise output of each U-Net as a semantic editor. This approach is grounded in two critical observations: firstly, the bottleneck features of U-Net inherently possess semantically rich features ideal for interactive editing; secondly, high-level semantics, established early in the denoising process, show minimal variation in subsequent stages. Leveraging these insights, DragNoise edits diffusion semantics in a single denoising step and efficiently propagates these changes, ensuring stability and efficiency in diffusion editing. Comparative experiments reveal that DragNoise achieves superior control and semantic retention, reducing the optimization time by over 50% compared to DragDiffusion. Our codes are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Can LLMs get help from other LLMs without revealing private information?</b></summary>
  <p><b>编号</b>：[140]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01041">https://arxiv.org/abs/2404.01041</a></p>
  <p><b>作者</b>：Florian Hartmann,  Duc-Hieu Tran,  Peter Kairouz,  Victor Cărbune,  Blaise Aguera y Arcas</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：remote model, applying cascade systems, common type, type of machine, accurately label</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cascades are a common type of machine learning systems in which a large, remote model can be queried if a local model is not able to accurately label a user's data by itself. Serving stacks for large language models (LLMs) increasingly use cascades due to their ability to preserve task performance while dramatically reducing inference costs. However, applying cascade systems in situations where the local model has access to sensitive data constitutes a significant privacy risk for users since such data could be forwarded to the remote model. In this work, we show the feasibility of applying cascade systems in such setups by equipping the local model with privacy-preserving techniques that reduce the risk of leaking private information when querying the remote model. To quantify information leakage in such setups, we introduce two privacy measures. We then propose a system that leverages the recently introduced social learning paradigm in which LLMs collaboratively learn from each other by exchanging natural language. Using this paradigm, we demonstrate on several datasets that our methods minimize the privacy loss while at the same time improving task performance compared to a non-cascade baseline.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：A Survey on Hypergraph Neural Networks: An In-Depth and Step-By-Step  Guide</b></summary>
  <p><b>编号</b>：[141]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01039">https://arxiv.org/abs/2404.01039</a></p>
  <p><b>作者</b>：Sunwoo Kim,  Soo Yong Lee,  Yue Gao,  Alessia Antelmi,  Mirko Polato,  Kijung Shin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：real-world complex systems, machine learning communities, Higher-order interactions, ubiquitous in real-world, real-world complex</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Higher-order interactions (HOIs) are ubiquitous in real-world complex systems and applications, and thus investigation of deep learning for HOIs has become a valuable agenda for the data mining and machine learning communities. As networks of HOIs are expressed mathematically as hypergraphs, hypergraph neural networks (HNNs) have emerged as a powerful tool for representation learning on hypergraphs. Given the emerging trend, we present the first survey dedicated to HNNs, with an in-depth and step-by-step guide. Broadly, the present survey overviews HNN architectures, training strategies, and applications. First, we break existing HNNs down into four design components: (i) input features, (ii) input structures, (iii) message-passing schemes, and (iv) training strategies. Second, we examine how HNNs address and learn HOIs with each of their components. Third, we overview the recent applications of HNNs in recommendation, biological and medical science, time series analysis, and computer vision. Lastly, we conclude with a discussion on limitations and future directions.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Higher education assessment practice in the era of generative AI tools</b></summary>
  <p><b>编号</b>：[143]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01036">https://arxiv.org/abs/2404.01036</a></p>
  <p><b>作者</b>：Bayode Ogunleye,  Kudirat Ibilola Zakariyyah,  Oluwaseun Ajao,  Olakunle Olayinka,  Hemlata Sharma</p>
  <p><b>备注</b>：11 pages, 7 tables published in the Journal of Applied Learning & Teaching</p>
  <p><b>关键词</b>：higher education, sector benefits, society at large, benefits every nation, nation economy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The higher education (HE) sector benefits every nation's economy and society at large. However, their contributions are challenged by advanced technologies like generative artificial intelligence (GenAI) tools. In this paper, we provide a comprehensive assessment of GenAI tools towards assessment and pedagogic practice and, subsequently, discuss the potential impacts. This study experimented using three assessment instruments from data science, data analytics, and construction management disciplines. Our findings are two-fold: first, the findings revealed that GenAI tools exhibit subject knowledge, problem-solving, analytical, critical thinking, and presentation skills and thus can limit learning when used unethically. Secondly, the design of the assessment of certain disciplines revealed the limitations of the GenAI tools. Based on our findings, we made recommendations on how AI tools can be utilised for teaching and learning in HE.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Query Performance Prediction using Relevance Judgments Generated by  Large Language Models</b></summary>
  <p><b>编号</b>：[153]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01012">https://arxiv.org/abs/2404.01012</a></p>
  <p><b>作者</b>：Chuan Meng,  Negar Arabzadeh,  Arian Askari,  Mohammad Aliannejadi,  Maarten de Rijke</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：generated relevance judgments, relevance judgments, QPP, human relevance judgments, single scalar</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Query performance prediction (QPP) aims to estimate the retrieval quality of a search system for a query without human relevance judgments. Previous QPP methods typically return a single scalar value and do not require the predicted values to approximate a specific information retrieval (IR) evaluation measure, leading to certain drawbacks: (i) a single scalar is insufficient to accurately represent different IR evaluation measures, especially when metrics do not highly correlate, and (ii) a single scalar limits the interpretability of QPP methods because solely using a scalar is insufficient to explain QPP results. To address these issues, we propose a QPP framework using automatically generated relevance judgments (QPP-GenRE), which decomposes QPP into independent subtasks of judging the relevance of each item in a ranked list to a given query. This allows us to predict any IR evaluation measure using the generated relevance judgments as pseudo-labels; Also, this allows us to interpret predicted IR evaluation measures, and identify, track and rectify errors in generated relevance judgments to improve QPP quality. We judge relevance by leveraging a leading open-source large language model (LLM), LLaMA, to ensure scientific reproducibility. In doing so, we address two main challenges: (i) excessive computational costs of judging the entire corpus for predicting a recall-based metric, and (ii) poor performance in prompting LLaMA in a zero-/few-shot manner. We devise an approximation strategy to predict a recall-oriented IR measure and propose to fine-tune LLaMA using human-labeled relevance judgments. Experiments on the TREC 2019-2022 deep learning tracks show that QPP-GenRE achieves state-of-the-art QPP accuracy for both lexical and neural rankers in both precision- and recall-oriented metrics.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Make Continual Learning Stronger via C-Flat</b></summary>
  <p><b>编号</b>：[166]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00986">https://arxiv.org/abs/2404.00986</a></p>
  <p><b>作者</b>：Ang Bian,  Wei Li,  Hangjie Yuan,  Chengrong Yu,  Zixiang Zhao,  Mang Wang,  Aojun Lu,  Tao Feng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：incrementally acquiring dynamically, acquiring dynamically updating, dynamically updating knowledge, sequentially arriving tasks, Model generalization ability</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Model generalization ability upon incrementally acquiring dynamically updating knowledge from sequentially arriving tasks is crucial to tackle the sensitivity-stability dilemma in Continual Learning (CL). Weight loss landscape sharpness minimization seeking for flat minima lying in neighborhoods with uniform low loss or smooth gradient is proven to be a strong training regime improving model generalization compared with loss minimization based optimizer like SGD. Yet only a few works have discussed this training regime for CL, proving that dedicated designed zeroth-order sharpness optimizer can improve CL performance. In this work, we propose a Continual Flatness (C-Flat) method featuring a flatter loss landscape tailored for CL. C-Flat could be easily called with only one line of code and is plug-and-play to any CL methods. A general framework of C-Flat applied to all CL categories and a thorough comparison with loss minima optimizer and flat minima based CL approaches is presented in this paper, showing that our method can boost CL performance in almost all cases. Code will be publicly available upon publication.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Continual Learning for Smart City: A Survey</b></summary>
  <p><b>编号</b>：[167]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00983">https://arxiv.org/abs/2404.00983</a></p>
  <p><b>作者</b>：Li Yang,  Zhipeng Luo,  Shiming Zhang,  Fei Teng,  Tianrui Li</p>
  <p><b>备注</b>：Preprint. Work in Progress</p>
  <p><b>关键词</b>：powerful computational resources, computational resources facilitate, intelligent models deployed, modern cities, large data volumes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the digitization of modern cities, large data volumes and powerful computational resources facilitate the rapid update of intelligent models deployed in smart cities. Continual learning (CL) is a novel machine learning paradigm that constantly updates models to adapt to changing environments, where the learning tasks, data, and distributions can vary over time. Our survey provides a comprehensive review of continual learning methods that are widely used in smart city development. The content consists of three parts: 1) Methodology-wise. We categorize a large number of basic CL methods and advanced CL frameworks in combination with other learning paradigms including graph learning, spatial-temporal learning, multi-modal learning, and federated learning. 2) Application-wise. We present numerous CL applications covering transportation, environment, public health, safety, networks, and associated datasets related to urban computing. 3) Challenges. We discuss current problems and challenges and envision several promising research directions. We believe this survey can help relevant researchers quickly familiarize themselves with the current state of continual learning research used in smart city development and direct them to future research trends.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：Diffusion-Driven Domain Adaptation for Generating 3D Molecules</b></summary>
  <p><b>编号</b>：[178]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00962">https://arxiv.org/abs/2404.00962</a></p>
  <p><b>作者</b>：Haokai Hong,  Wanyu Lin,  Kay Chen Tan</p>
  <p><b>备注</b>：11 pages, 3 figures, and 3 tables</p>
  <p><b>关键词</b>：collect data, domain, structure variations, molecule generator, variations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Can we train a molecule generator that can generate 3D molecules from a new domain, circumventing the need to collect data? This problem can be cast as the problem of domain adaptive molecule generation. This work presents a novel and principled diffusion-based approach, called GADM, that allows shifting a generative model to desired new domains without the need to collect even a single molecule. As the domain shift is typically caused by the structure variations of molecules, e.g., scaffold variations, we leverage a designated equivariant masked autoencoder (MAE) along with various masking strategies to capture the structural-grained representations of the in-domain varieties. In particular, with an asymmetric encoder-decoder module, the MAE can generalize to unseen structure variations from the target domains. These structure variations are encoded with an equivariant encoder and treated as domain supervisors to control denoising. We show that, with these encoded structural-grained domain supervisors, GADM can generate effective molecules within the desired new domains. We conduct extensive experiments across various domain adaptation tasks over benchmarking datasets. We show that our approach can improve up to 65.6% in terms of success rate defined based on molecular validity, uniqueness, and novelty compared to alternative baselines.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Evaluating the Factuality of Large Language Models using Large-Scale  Knowledge Graphs</b></summary>
  <p><b>编号</b>：[188]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00942">https://arxiv.org/abs/2404.00942</a></p>
  <p><b>作者</b>：Xiaoze Liu,  Feijie Wu,  Tianyang Xu,  Zhuo Chen,  Yichi Zhang,  Xiaoqian Wang,  Jing Gao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：enhancing machine learning, Large Language Models, Large Language, Language Models, enhancing machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The advent of Large Language Models (LLMs) has significantly transformed the AI landscape, enhancing machine learning and AI capabilities. Factuality issue is a critical concern for LLMs, as they may generate factually incorrect responses. In this paper, we propose GraphEval to evaluate an LLM's performance using a substantially large test dataset. Specifically, the test dataset is retrieved from a large knowledge graph with more than 10 million facts without expensive human efforts. Unlike conventional methods that evaluate LLMs based on generated responses, GraphEval streamlines the evaluation process by creating a judge model to estimate the correctness of the answers given by the LLM. Our experiments demonstrate that the judge model's factuality assessment aligns closely with the correctness of the LLM's generated outputs, while also substantially reducing evaluation costs. Besides, our findings offer valuable insights into LLM performance across different metrics and highlight the potential for future improvements in ensuring the factual integrity of LLM outputs. The code is publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Instance-Aware Group Quantization for Vision Transformers</b></summary>
  <p><b>编号</b>：[195]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00928">https://arxiv.org/abs/2404.00928</a></p>
  <p><b>作者</b>：Jaehyeon Moon,  Dohyung Kim,  Junyong Cheon,  Bumsub Ham</p>
  <p><b>备注</b>：CVPR 2024</p>
  <p><b>关键词</b>：small calibration set, efficient model compression, model compression technique, PTQ methods, samples without retraining</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Post-training quantization (PTQ) is an efficient model compression technique that quantizes a pretrained full-precision model using only a small calibration set of unlabeled samples without retraining. PTQ methods for convolutional neural networks (CNNs) provide quantization results comparable to full-precision counterparts. Directly applying them to vision transformers (ViTs), however, incurs severe performance degradation, mainly due to the differences in architectures between CNNs and ViTs. In particular, the distribution of activations for each channel vary drastically according to input instances, making PTQ methods for CNNs inappropriate for ViTs. To address this, we introduce instance-aware group quantization for ViTs (IGQ-ViT). To this end, we propose to split the channels of activation maps into multiple groups dynamically for each input instance, such that activations within each group share similar statistical properties. We also extend our scheme to quantize softmax attentions across tokens. In addition, the number of groups for each layer is adjusted to minimize the discrepancies between predictions from quantized and full-precision models, under a bit-operation (BOP) constraint. We show extensive experimental results on image classification, object detection, and instance segmentation, with various transformer architectures, demonstrating the effectiveness of our approach.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Token-Efficient Leverage Learning in Large Language Models</b></summary>
  <p><b>编号</b>：[204]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00914">https://arxiv.org/abs/2404.00914</a></p>
  <p><b>作者</b>：Yuanhao Zeng,  Min Wang,  Yihang Wang,  Yingxia Shao</p>
  <p><b>备注</b>：15 pages, 16 figures</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Leverage Learning, Large Language, high-resource scenarios</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) have excelled in various tasks but perform better in high-resource scenarios, which presents challenges in low-resource scenarios. Data scarcity and the inherent difficulty of adapting LLMs to specific tasks compound the challenge. To address the twin hurdles, we introduce \textbf{Leverage Learning}. We present a streamlined implement of this methodology called Token-Efficient Leverage Learning (TELL). TELL showcases the potential of Leverage Learning, demonstrating effectiveness across various LLMs and low-resource tasks, ranging from $10^4$ to $10^6$ tokens. It reduces task data requirements by up to nearly an order of magnitude compared to conventional Supervised Fine-Tuning (SFT) while delivering competitive performance. With the same amount of task data, TELL leads in improving task performance compared to SFT. We discuss the mechanism of Leverage Learning, suggesting it aligns with quantization hypothesis and explore its promising potential through empirical testing.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：CAAP: Class-Dependent Automatic Data Augmentation Based On Adaptive  Policies For Time Series</b></summary>
  <p><b>编号</b>：[213]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00898">https://arxiv.org/abs/2404.00898</a></p>
  <p><b>作者</b>：Tien-Yu Chang,  Hao Dai,  Vincent S. Tseng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Data Augmentation, Automatic Data Augmentation, ADA, expanding the training, class-dependent bias</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Data Augmentation is a common technique used to enhance the performance of deep learning models by expanding the training dataset. Automatic Data Augmentation (ADA) methods are getting popular because of their capacity to generate policies for various datasets. However, existing ADA methods primarily focused on overall performance improvement, neglecting the problem of class-dependent bias that leads to performance reduction in specific classes. This bias poses significant challenges when deploying models in real-world applications. Furthermore, ADA for time series remains an underexplored domain, highlighting the need for advancements in this field. In particular, applying ADA techniques to vital signals like an electrocardiogram (ECG) is a compelling example due to its potential in medical domains such as heart disease diagnostics.
We propose a novel deep learning-based approach called Class-dependent Automatic Adaptive Policies (CAAP) framework to overcome the notable class-dependent bias problem while maintaining the overall improvement in time-series data augmentation. Specifically, we utilize the policy network to generate effective sample-wise policies with balanced difficulty through class and feature information extraction. Second, we design the augmentation probability regulation method to minimize class-dependent bias. Third, we introduce the information region concepts into the ADA framework to preserve essential regions in the sample. Through a series of experiments on real-world ECG datasets, we demonstrate that CAAP outperforms representative methods in achieving lower class-dependent bias combined with superior overall performance. These results highlight the reliability of CAAP as a promising ADA method for time series modeling that fits for the demands of real-world applications.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Machine Learning Robustness: A Primer</b></summary>
  <p><b>编号</b>：[214]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00897">https://arxiv.org/abs/2404.00897</a></p>
  <p><b>作者</b>：Houssem Ben Braiek,  Foutse Khomh</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2305.10862 by other authors</p>
  <p><b>关键词</b>：Artificial Intelligence, trustworthiness in Artificial, Machine Learning, integral role, role in establishing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This chapter explores the foundational concept of robustness in Machine Learning (ML) and its integral role in establishing trustworthiness in Artificial Intelligence (AI) systems. The discussion begins with a detailed definition of robustness, portraying it as the ability of ML models to maintain stable performance across varied and unexpected environmental conditions. ML robustness is dissected through several lenses: its complementarity with generalizability; its status as a requirement for trustworthy AI; its adversarial vs non-adversarial aspects; its quantitative metrics; and its indicators such as reproducibility and explainability. The chapter delves into the factors that impede robustness, such as data bias, model complexity, and the pitfalls of underspecified ML pipelines. It surveys key techniques for robustness assessment from a broad perspective, including adversarial attacks, encompassing both digital and physical realms. It covers non-adversarial data shifts and nuances of Deep Learning (DL) software testing methodologies. The discussion progresses to explore amelioration strategies for bolstering robustness, starting with data-centric approaches like debiasing and augmentation. Further examination includes a variety of model-centric methods such as transfer learning, adversarial training, and randomized smoothing. Lastly, post-training methods are discussed, including ensemble techniques, pruning, and model repairs, emerging as cost-effective strategies to make models more resilient against the unpredictable. This chapter underscores the ongoing challenges and limitations in estimating and achieving ML robustness by existing approaches. It offers insights and directions for future research on this crucial concept, as a prerequisite for trustworthy AI systems.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Modeling Output-Level Task Relatedness in Multi-Task Learning with  Feedback Mechanism</b></summary>
  <p><b>编号</b>：[220]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00885">https://arxiv.org/abs/2404.00885</a></p>
  <p><b>作者</b>：Xiangming Xi,  Feng Gao,  Jun Xu,  Fangtai Guo,  Tianlei Jin</p>
  <p><b>备注</b>：submitted to CDC2024</p>
  <p><b>关键词</b>：simultaneously learns multiple, learns multiple tasks, paradigm that simultaneously, simultaneously learns, learns multiple</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multi-task learning (MTL) is a paradigm that simultaneously learns multiple tasks by sharing information at different levels, enhancing the performance of each individual task. While previous research has primarily focused on feature-level or parameter-level task relatedness, and proposed various model architectures and learning algorithms to improve learning performance, we aim to explore output-level task relatedness. This approach introduces a posteriori information into the model, considering that different tasks may produce correlated outputs with mutual influences. We achieve this by incorporating a feedback mechanism into MTL models, where the output of one task serves as a hidden feature for another task, thereby transforming a static MTL model into a dynamic one. To ensure the training process converges, we introduce a convergence loss that measures the trend of a task's outputs during each iteration. Additionally, we propose a Gumbel gating mechanism to determine the optimal projection of feedback signals. We validate the effectiveness of our method and evaluate its performance through experiments conducted on several baseline models in spoken language understanding.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Interpretable Multi-View Clustering Based on Anchor Graph Tensor  Factorization</b></summary>
  <p><b>编号</b>：[222]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00883">https://arxiv.org/abs/2404.00883</a></p>
  <p><b>作者</b>：Jing Li,  Quanxue Gao,  Cheng Deng,  Qianqian Wang,  Ming Yang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：process large-scale data, gained significant attention, significant attention due, exceptional clustering performance, anchor graph</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The clustering method based on the anchor graph has gained significant attention due to its exceptional clustering performance and ability to process large-scale data. One common approach is to learn bipartite graphs with K-connected components, helping avoid the need for post-processing. However, this method has strict parameter requirements and may not always get K-connected components. To address this issue, an alternative approach is to directly obtain the cluster label matrix by performing non-negative matrix factorization (NMF) on the anchor graph. Nevertheless, existing multi-view clustering methods based on anchor graph factorization lack adequate cluster interpretability for the decomposed matrix and often overlook the inter-view information. We address this limitation by using non-negative tensor factorization to decompose an anchor graph tensor that combines anchor graphs from multiple views. This approach allows us to consider inter-view information comprehensively. The decomposed tensors, namely the sample indicator tensor and the anchor indicator tensor, enhance the interpretability of the factorization. Extensive experiments validate the effectiveness of this method.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Metric Learning to Accelerate Convergence of Operator Splitting Methods  for Differentiable Parametric Programming</b></summary>
  <p><b>编号</b>：[223]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00882">https://arxiv.org/abs/2404.00882</a></p>
  <p><b>作者</b>：Ethan King,  James Kotary,  Ferdinando Fioretto,  Jan Drgona</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：accelerate the solution, solution of constrained, shown a variety, constrained optimization problems, proximal metrics</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent work has shown a variety of ways in which machine learning can be used to accelerate the solution of constrained optimization problems. Increasing demand for real-time decision-making capabilities in applications such as artificial intelligence and optimal control has led to a variety of approaches, based on distinct strategies. This work proposes a novel approach to learning optimization, in which the underlying metric space of a proximal operator splitting algorithm is learned so as to maximize its convergence rate. While prior works in optimization theory have derived optimal metrics for limited classes of problems, the results do not extend to many practical problem forms including general Quadratic Programming (QP). This paper shows how differentiable optimization can enable the end-to-end learning of proximal metrics, enhancing the convergence of proximal algorithms for QP problems beyond what is possible based on known theory. Additionally, the results illustrate a strong connection between the learned proximal metrics and active constraints at the optima, leading to an interpretation in which the learning of proximal metrics can be viewed as a form of active set learning.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Rethinking the Relationship between Recurrent and Non-Recurrent Neural  Networks: A Study in Sparsity</b></summary>
  <p><b>编号</b>：[224]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00880">https://arxiv.org/abs/2404.00880</a></p>
  <p><b>作者</b>：Quincy Hershey,  Randy Paffenroth,  Harsh Pathak,  Simon Tavener</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Neural networks, Recurrent Neural Networks, broad categories, Neural, neural network models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural networks (NN) can be divided into two broad categories, recurrent and non-recurrent. Both types of neural networks are popular and extensively studied, but they are often treated as distinct families of machine learning algorithms. In this position paper, we argue that there is a closer relationship between these two types of neural networks than is normally appreciated. We show that many common neural network models, such as Recurrent Neural Networks (RNN), Multi-Layer Perceptrons (MLP), and even deep multi-layer transformers, can all be represented as iterative maps.
The close relationship between RNNs and other types of NNs should not be surprising. In particular, RNNs are known to be Turing complete, and therefore capable of representing any computable function (such as any other types of NNs), but herein we argue that the relationship runs deeper and is more practical than this. For example, RNNs are often thought to be more difficult to train than other types of NNs, with RNNs being plagued by issues such as vanishing or exploding gradients. However, as we demonstrate in this paper, MLPs, RNNs, and many other NNs lie on a continuum, and this perspective leads to several insights that illuminate both theoretical and practical aspects of NNs.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Lipsum-FT: Robust Fine-Tuning of Zero-Shot Models Using Random Text  Guidance</b></summary>
  <p><b>编号</b>：[233]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00860">https://arxiv.org/abs/2404.00860</a></p>
  <p><b>作者</b>：Giung Nam,  Byeongho Heo,  Juho Lee</p>
  <p><b>备注</b>：ICLR 2024</p>
  <p><b>关键词</b>：image classification tasks, achieving competitive performance, Large-scale contrastive vision-language, model achieving competitive, zero-shot model achieving</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large-scale contrastive vision-language pre-trained models provide the zero-shot model achieving competitive performance across a range of image classification tasks without requiring training on downstream data. Recent works have confirmed that while additional fine-tuning of the zero-shot model on the reference data results in enhanced downstream performance, it compromises the model's robustness against distribution shifts. Our investigation begins by examining the conditions required to achieve the goals of robust fine-tuning, employing descriptions based on feature distortion theory and joint energy-based models. Subsequently, we propose a novel robust fine-tuning algorithm, Lipsum-FT, that effectively utilizes the language modeling aspect of the vision-language pre-trained models. Extensive experiments conducted on distribution shift scenarios in DomainNet and ImageNet confirm the superiority of our proposed Lipsum-FT approach over existing robust fine-tuning methods.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Do language models plan ahead for future tokens?</b></summary>
  <p><b>编号</b>：[234]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00859">https://arxiv.org/abs/2404.00859</a></p>
  <p><b>作者</b>：Wilson Wu,  John X. Morris,  Lionel Levine</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：transformers prepare information, future forward passes, inference, tau, present inference task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Do transformers "think ahead" during inference at a given position? It is known transformers prepare information in the hidden states of the forward pass at $t$ that is then used in future forward passes $t+\tau$. We posit two explanations for this phenomenon: pre-caching, in which off-diagonal gradient terms present in training result in the model computing features at $t$ irrelevant to the present inference task but useful for the future, and breadcrumbs, in which features most relevant to time step $t$ are already the same as those that would most benefit inference at time $t+\tau$. We test these hypotheses by training language models without propagating gradients to past timesteps, a scheme we formalize as myopic training. In a synthetic data setting, we find clear evidence for pre-caching. In the autoregressive language modeling setting, our experiments are more suggestive of the breadcrumbs hypothesis.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Ensemble Learning for Vietnamese Scene Text Spotting in Urban  Environments</b></summary>
  <p><b>编号</b>：[238]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00852">https://arxiv.org/abs/2404.00852</a></p>
  <p><b>作者</b>：Hieu Nguyen,  Cong-Hoang Ta,  Phuong-Thuy Le-Nguyen,  Minh-Triet Tran,  Trung-Nghia Le</p>
  <p><b>备注</b>：RIVF 2023</p>
  <p><b>关键词</b>：scene text spotting, Vietnamese scene text, efficient ensemble learning, ensemble learning framework, text spotting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a simple yet efficient ensemble learning framework for Vietnamese scene text spotting. Leveraging the power of ensemble learning, which combines multiple models to yield more accurate predictions, our approach aims to significantly enhance the performance of scene text spotting in challenging urban settings. Through experimental evaluations on the VinText dataset, our proposed method achieves a significant improvement in accuracy compared to existing methods with an impressive accuracy of 5%. These results unequivocally demonstrate the efficacy of ensemble learning in the context of Vietnamese scene text spotting in urban environments, highlighting its potential for real world applications, such as text detection and recognition in urban signage, advertisements, and various text-rich urban scenes.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Predictive Performance Comparison of Decision Policies Under Confounding</b></summary>
  <p><b>编号</b>：[242]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00848">https://arxiv.org/abs/2404.00848</a></p>
  <p><b>作者</b>：Luke Guerdan,  Amanda Coston,  Kenneth Holstein,  Zhiwei Steven Wu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：existing decision-making policy, existing decision-making, decision-making tasks, predictive performance, compare predictive performance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Predictive models are often introduced to decision-making tasks under the rationale that they improve performance over an existing decision-making policy. However, it is challenging to compare predictive performance against an existing decision-making policy that is generally under-specified and dependent on unobservable factors. These sources of uncertainty are often addressed in practice by making strong assumptions about the data-generating mechanism. In this work, we propose a method to compare the predictive performance of decision policies under a variety of modern identification approaches from the causal inference and off-policy evaluation literatures (e.g., instrumental variable, marginal sensitivity model, proximal variable). Key to our method is the insight that there are regions of uncertainty that we can safely ignore in the policy comparison. We develop a practical approach for finite-sample estimation of regret intervals under no assumptions on the parametric form of the status quo policy. We verify our framework theoretically and via synthetic data experiments. We conclude with a real-world application using our framework to support a pre-deployment evaluation of a proposed modification to a healthcare enrollment policy.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Transfer Learning with Point Transformers</b></summary>
  <p><b>编号</b>：[244]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00846">https://arxiv.org/abs/2404.00846</a></p>
  <p><b>作者</b>：Kartik Gupta,  Rahul Vippala,  Sahima Srivastava</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Point Cloud data, Cloud data, Point Transformers, Point Cloud, MNIST dataset</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Point Transformers are near state-of-the-art models for classification, segmentation, and detection tasks on Point Cloud data. They utilize a self attention based mechanism to model large range spatial dependencies between multiple point sets. In this project we explore two things: classification performance of these attention based networks on ModelNet10 dataset and then, we use the trained model to classify 3D MNIST dataset after finetuning. We also train the model from scratch on 3D MNIST dataset to compare the performance of finetuned and from-scratch model on the MNIST dataset. We observe that since the two datasets have a large difference in the degree of the distributions, transfer learned models do not outperform the from-scratch models in this case. Although we do expect transfer learned models to converge faster since they already know the lower level edges, corners, etc features from the ModelNet10 dataset.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Rethinking Resource Management in Edge Learning: A Joint Pre-training  and Fine-tuning Design Paradigm</b></summary>
  <p><b>编号</b>：[247]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00836">https://arxiv.org/abs/2404.00836</a></p>
  <p><b>作者</b>：Zhonghao Lyu,  Yuchen Li,  Guangxu Zhu,  Jie Xu,  H. Vincent Poor,  Shuguang Cui</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：two-stage learning unifying, two-stage edge learning, edge learning, learning unifying pre-training, edge learning system</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In some applications, edge learning is experiencing a shift in focusing from conventional learning from scratch to new two-stage learning unifying pre-training and task-specific fine-tuning. This paper considers the problem of joint communication and computation resource management in a two-stage edge learning system. In this system, model pre-training is first conducted at an edge server via centralized learning on local pre-stored general data, and then task-specific fine-tuning is performed at edge devices based on the pre-trained model via federated edge learning. For the two-stage learning model, we first analyze the convergence behavior (in terms of the average squared gradient norm bound), which characterizes the impacts of various system parameters such as the number of learning rounds and batch sizes in the two stages on the convergence rate. Based on our analytical results, we then propose a joint communication and computation resource management design to minimize an average squared gradient norm bound, subject to constraints on the transmit power, overall system energy consumption, and training delay. The decision variables include the number of learning rounds, batch sizes, clock frequencies, and transmit power control for both pre-training and fine-tuning stages. Finally, numerical results are provided to evaluate the effectiveness of our proposed design. It is shown that the proposed joint resource management over the pre-training and fine-tuning stages well balances the system performance trade-off among the training accuracy, delay, and energy consumption. The proposed design is also shown to effectively leverage the inherent trade-off between pre-training and fine-tuning, which arises from the differences in data distribution between pre-stored general data versus real-time task-specific data, thus efficiently optimizing overall system performance.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：HeteroMILE: a Multi-Level Graph Representation Learning Framework for  Heterogeneous Graphs</b></summary>
  <p><b>编号</b>：[254]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00816">https://arxiv.org/abs/2404.00816</a></p>
  <p><b>作者</b>：Yue Zhang,  Yuntian He,  Saket Gurukar,  Srinivasan Parthasarathy</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：types of entities, ubiquitous in real-world, real-world applications, represent various relationships, graph</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Heterogeneous graphs are ubiquitous in real-world applications because they can represent various relationships between different types of entities. Therefore, learning embeddings in such graphs is a critical problem in graph machine learning. However, existing solutions for this problem fail to scale to large heterogeneous graphs due to their high computational complexity. To address this issue, we propose a Multi-Level Embedding framework of nodes on a heterogeneous graph (HeteroMILE) - a generic methodology that allows contemporary graph embedding methods to scale to large graphs. HeteroMILE repeatedly coarsens the large sized graph into a smaller size while preserving the backbone structure of the graph before embedding it, effectively reducing the computational cost by avoiding time-consuming processing operations. It then refines the coarsened embedding to the original graph using a heterogeneous graph convolution neural network. We evaluate our approach using several popular heterogeneous graph datasets. The experimental results show that HeteroMILE can substantially reduce computational time (approximately 20x speedup) and generate an embedding of better quality for link prediction and node classification.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：On Difficulties of Attention Factorization through Shared Memory</b></summary>
  <p><b>编号</b>：[262]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00798">https://arxiv.org/abs/2404.00798</a></p>
  <p><b>作者</b>：Uladzislau Yorsh,  Martin Holeňa,  Ondřej Bojar,  David Herel</p>
  <p><b>备注</b>：2 pages of main content, 8 pages in total, published as a Tiny Paper at ICLR 2024</p>
  <p><b>关键词</b>：including natural language, revolutionized deep learning, natural language processing, computer vision, numerous fields</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transformers have revolutionized deep learning in numerous fields, including natural language processing, computer vision, and audio processing. Their strength lies in their attention mechanism, which allows for the discovering of complex input relationships. However, this mechanism's quadratic time and memory complexity pose challenges for larger inputs. Researchers are now investigating models like Linear Unified Nested Attention (Luna) or Memory Augmented Transformer, which leverage external learnable memory to either reduce the attention computation complexity down to linear, or to propagate information between chunks in chunk-wise processing. Our findings challenge the conventional thinking on these models, revealing that interfacing with the memory directly through an attention operation is suboptimal, and that the performance may be considerably improved by filtering the input signal before communicating with memory.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Metarobotics for Industry and Society: Vision, Technologies, and  Opportunities</b></summary>
  <p><b>编号</b>：[263]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00797">https://arxiv.org/abs/2404.00797</a></p>
  <p><b>作者</b>：Eric Guiffo Kaigom</p>
  <p><b>备注</b>：Published on IEEE Transactions on Industrial Informatics (early access), 2023</p>
  <p><b>关键词</b>：generation wireless communication, distant robotized applications, multi-sense immersion, wireless communication, aims to combine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Metarobotics aims to combine next generation wireless communication, multi-sense immersion, and collective intelligence to provide a pervasive, itinerant, and non-invasive access and interaction with distant robotized applications. Industry and society are expected to benefit from these functionalities. For instance, robot programmers will no longer travel worldwide to plan and test robot motions, even collaboratively. Instead, they will have a personalized access to robots and their environments from anywhere, thus spending more time with family and friends. Students enrolled in robotics courses will be taught under authentic industrial conditions in real-time. This paper describes objectives of Metarobotics in society, industry, and in-between. It identifies and surveys technologies likely to enable their completion and provides an architecture to put forward the interplay of key components of Metarobotics. Potentials for self-determination, self-efficacy, and work-life-flexibility in robotics-related applications in Society 5.0, Industry 4.0, and Industry 5.0 are outlined.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Rehearsal-Free Modular and Compositional Continual Learning for Language  Models</b></summary>
  <p><b>编号</b>：[268]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00790">https://arxiv.org/abs/2404.00790</a></p>
  <p><b>作者</b>：Mingyang Wang,  Heike Adel,  Lukas Lange,  Jannik Strötgen,  Hinrich Schütze</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Continual learning aims, Continual learning, aims at incrementally, incrementally acquiring, Compositional Continual Learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Continual learning aims at incrementally acquiring new knowledge while not forgetting existing knowledge. To overcome catastrophic forgetting, methods are either rehearsal-based, i.e., store data examples from previous tasks for data replay, or isolate parameters dedicated to each task. However, rehearsal-based methods raise privacy and memory issues, and parameter-isolation continual learning does not consider interaction between tasks, thus hindering knowledge transfer. In this work, we propose MoCL, a rehearsal-free Modular and Compositional Continual Learning framework which continually adds new modules to language models and composes them with existing modules. Experiments on various benchmarks show that MoCL outperforms state of the art and effectively facilitates knowledge transfer.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Disentangling Hippocampal Shape Variations: A Study of Neurological  Disorders Using Graph Variational Autoencoder with Contrastive Learning</b></summary>
  <p><b>编号</b>：[271]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00785">https://arxiv.org/abs/2404.00785</a></p>
  <p><b>作者</b>：Jakaria Rabbi,  Johannes Kiechle,  Christian Beaulieu,  Nilanjan Ray,  Dana Cobzas</p>
  <p><b>备注</b>：Length: 23 pages and submitted to the journal: MELBA (Machine Learning for Biomedical Imaging)</p>
  <p><b>关键词</b>：diffusion tensor imaging, Supervised Contrastive Learning, Graph Variational Autoencoder, comprehensive study focused, Supervised Contrastive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a comprehensive study focused on disentangling hippocampal shape variations from diffusion tensor imaging (DTI) datasets within the context of neurological disorders. Leveraging a Graph Variational Autoencoder (VAE) enhanced with Supervised Contrastive Learning, our approach aims to improve interpretability by disentangling two distinct latent variables corresponding to age and the presence of diseases. In our ablation study, we investigate a range of VAE architectures and contrastive loss functions, showcasing the enhanced disentanglement capabilities of our approach. This evaluation uses synthetic 3D torus mesh data and real 3D hippocampal mesh datasets derived from the DTI hippocampal dataset. Our supervised disentanglement model outperforms several state-of-the-art (SOTA) methods like attribute and guided VAEs in terms of disentanglement scores. Our model distinguishes between age groups and disease status in patients with Multiple Sclerosis (MS) using the hippocampus data. Our Graph VAE with Supervised Contrastive Learning shows the volume changes of the hippocampus of MS populations at different ages, and the result is consistent with the current neuroimaging literature. This research provides valuable insights into the relationship between neurological disorder and hippocampal shape changes in different age groups of MS populations using a Graph VAE with Supervised Contrastive loss.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Addressing Loss of Plasticity and Catastrophic Forgetting in Continual  Learning</b></summary>
  <p><b>编号</b>：[273]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00781">https://arxiv.org/abs/2404.00781</a></p>
  <p><b>作者</b>：Mohamed Elsayed,  A. Rupam Mahmood</p>
  <p><b>备注</b>：Published in the Proceedings of the 12th International Conference on Learning Representations (ICLR 2024)</p>
  <p><b>关键词</b>：Perturbed Gradient Descent, Utility-based Perturbed Gradient, due to rigid, rigid and unuseful, Deep representation learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems. Finally, in extended reinforcement learning experiments with PPO, we show that while Adam exhibits a performance drop after initial learning, UPGD avoids it by addressing both continual learning issues.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：Privacy-preserving Optics for Enhancing Protection in Face  De-identification</b></summary>
  <p><b>编号</b>：[274]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00777">https://arxiv.org/abs/2404.00777</a></p>
  <p><b>作者</b>：Jhon Lopez,  Carlos Hinojosa,  Henry Arguello,  Bernard Ghanem</p>
  <p><b>备注</b>：Accepted to CVPR 2024. Project Website and Code coming soon</p>
  <p><b>关键词</b>：camera usage alongside, usage alongside widespread, alongside widespread computer, widespread computer vision, computer vision technology</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The modern surge in camera usage alongside widespread computer vision technology applications poses significant privacy and security concerns. Current artificial intelligence (AI) technologies aid in recognizing relevant events and assisting in daily tasks in homes, offices, hospitals, etc. The need to access or process personal information for these purposes raises privacy concerns. While software-level solutions like face de-identification provide a good privacy/utility trade-off, they present vulnerabilities to sniffing attacks. In this paper, we propose a hardware-level face de-identification method to solve this vulnerability. Specifically, our approach first learns an optical encoder along with a regression model to obtain a face heatmap while hiding the face identity from the source image. We also propose an anonymization framework that generates a new face using the privacy-preserving image, face heatmap, and a reference face image from a public dataset as input. We validate our approach with extensive simulations and hardware experiments.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：PyTorch Frame: A Modular Framework for Multi-Modal Tabular Learning</b></summary>
  <p><b>编号</b>：[275]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00776">https://arxiv.org/abs/2404.00776</a></p>
  <p><b>作者</b>：Weihua Hu,  Yiwen Yuan,  Zecheng Zhang,  Akihiro Nitta,  Kaidi Cao,  Vid Kocijan,  Jure Leskovec,  Matthias Fey</p>
  <p><b>备注</b>：this https URL</p>
  <p><b>关键词</b>：present PyTorch Frame, PyTorch Frame, Graph Neural Networks, multi-modal tabular data, tabular deep learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present PyTorch Frame, a PyTorch-based framework for deep learning over multi-modal tabular data. PyTorch Frame makes tabular deep learning easy by providing a PyTorch-based data structure to handle complex tabular data, introducing a model abstraction to enable modular implementation of tabular models, and allowing external foundation models to be incorporated to handle complex columns (e.g., LLMs for text columns). We demonstrate the usefulness of PyTorch Frame by implementing diverse tabular models in a modular way, successfully applying these models to complex multi-modal tabular data, and integrating our framework with PyTorch Geometric, a PyTorch library for Graph Neural Networks (GNNs), to perform end-to-end learning over relational databases.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：SOAR: Improved Indexing for Approximate Nearest Neighbor Search</b></summary>
  <p><b>编号</b>：[277]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00774">https://arxiv.org/abs/2404.00774</a></p>
  <p><b>作者</b>：Philip Sun,  David Simcha,  Dave Dopson,  Ruiqi Guo,  Sanjiv Kumar</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：paper introduces SOAR, approximate nearest neighbor, paper introduces, technique for approximate, nearest neighbor</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper introduces SOAR: Spilling with Orthogonality-Amplified Residuals, a novel data indexing technique for approximate nearest neighbor (ANN) search. SOAR extends upon previous approaches to ANN search, such as spill trees, that utilize multiple redundant representations while partitioning the data to reduce the probability of missing a nearest neighbor during search. Rather than training and computing these redundant representations independently, however, SOAR uses an orthogonality-amplified residual loss, which optimizes each representation to compensate for cases where other representations perform poorly. This drastically improves the overall index quality, resulting in state-of-the-art ANN benchmark performance while maintaining fast indexing times and low memory consumption.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery</b></summary>
  <p><b>编号</b>：[283]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00756">https://arxiv.org/abs/2404.00756</a></p>
  <p><b>作者</b>：Cristina Cornelio,  Mohammed Diab</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：implementing recovery procedures, challenging in robotics, execution and implementing, procedures is challenging, task execution</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recognizing failures during task execution and implementing recovery procedures is challenging in robotics. Traditional approaches rely on the availability of extensive data or a tight set of constraints, while more recent approaches leverage large language models (LLMs) to verify task steps and replan accordingly. However, these methods often operate offline, necessitating scene resets and incurring in high costs. This paper introduces Recover, a neuro-symbolic framework for online failure identification and recovery. By integrating ontologies, logical rules, and LLM-based planners, Recover exploits symbolic information to enhance the ability of LLMs to generate recovery plans and also to decrease the associated costs. In order to demonstrate the capabilities of our method in a simulated kitchen environment, we introduce OntoThor, an ontology describing the AI2Thor simulator setting. Empirical evaluation shows that OntoThor's logical rules accurately detect all failures in the analyzed tasks, and that Recover considerably outperforms, for both failure detection and recovery, a baseline method reliant solely on LLMs.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：Nonparametric End-to-End Probabilistic Forecasting of Distributed  Generation Outputs Considering Missing Data Imputation</b></summary>
  <p><b>编号</b>：[293]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00729">https://arxiv.org/abs/2404.00729</a></p>
  <p><b>作者</b>：Minghui Chen,  Zichao Meng,  Yanping Liu,  Longbo Luo,  Ye Guo,  Kang Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：distributed renewable, distributed renewable generation, including missing data, missing data imputation, data imputation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we introduce a nonparametric end-to-end method for probabilistic forecasting of distributed renewable generation outputs while including missing data imputation. Firstly, we employ a nonparametric probabilistic forecast model utilizing the long short-term memory (LSTM) network to model the probability distributions of distributed renewable generations' outputs. Secondly, we design an end-to-end training process that includes missing data imputation through iterative imputation and iterative loss-based training procedures. This two-step modeling approach effectively combines the strengths of the nonparametric method with the end-to-end approach. Consequently, our approach demonstrates exceptional capabilities in probabilistic forecasting for the outputs of distributed renewable generations while effectively handling missing values. Simulation results confirm the superior performance of our approach compared to existing alternatives.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：The Larger the Better? Improved LLM Code-Generation via Budget  Reallocation</b></summary>
  <p><b>编号</b>：[296]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00725">https://arxiv.org/abs/2404.00725</a></p>
  <p><b>作者</b>：Michael Hassid,  Tal Remez,  Jonas Gehring,  Roy Schwartz,  Yossi Adi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, common belief, belief that large, large language, models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>It is a common belief that large language models (LLMs) are better than smaller-sized ones. However, larger models also require significantly more time and compute during inference. This begs the question: what happens when both models operate under the same budget? (e.g., compute, run-time). To address this question, we analyze code generation LLMs of various sizes and make comparisons such as running a 70B model once vs. generating five outputs from a 13B model and selecting one. Our findings reveal that, in a standard unit-test setup, the repeated use of smaller models can yield consistent improvements, with gains of up to 15% across five tasks. On the other hand, in scenarios where unit-tests are unavailable, a ranking-based selection of candidates from the smaller model falls short of the performance of a single output from larger ones. Our results highlight the potential of using smaller models instead of larger ones, and the importance of studying approaches for ranking LLM outputs.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：Survey of Computerized Adaptive Testing: A Machine Learning Perspective</b></summary>
  <p><b>编号</b>：[301]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00712">https://arxiv.org/abs/2404.00712</a></p>
  <p><b>作者</b>：Qi Liu,  Yan Zhuang,  Haoyang Bi,  Zhenya Huang,  Weizhe Huang,  Jiatong Li,  Junhao Yu,  Zirui Liu,  Zirui Hu,  Yuting Hong,  Zachary A. Pardos,  Haiping Ma,  Mengxiao Zhu,  Shijin Wang,  Enhong Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Computerized Adaptive Testing, dynamically adjusting test, proficiency of examinees, assessing the proficiency, dynamically adjusting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Computerized Adaptive Testing (CAT) provides an efficient and tailored method for assessing the proficiency of examinees, by dynamically adjusting test questions based on their performance. Widely adopted across diverse fields like education, healthcare, sports, and sociology, CAT has revolutionized testing practices. While traditional methods rely on psychometrics and statistics, the increasing complexity of large-scale testing has spurred the integration of machine learning techniques. This paper aims to provide a machine learning-focused survey on CAT, presenting a fresh perspective on this adaptive testing method. By examining the test question selection algorithm at the heart of CAT's adaptivity, we shed light on its functionality. Furthermore, we delve into cognitive diagnosis models, question bank construction, and test control within CAT, exploring how machine learning can optimize these components. Through an analysis of current methods, strengths, limitations, and challenges, we strive to develop robust, fair, and efficient CAT systems. By bridging psychometric-driven CAT research with machine learning, this survey advocates for a more inclusive and interdisciplinary approach to the future of adaptive testing.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：Privacy Re-identification Attacks on Tabular GANs</b></summary>
  <p><b>编号</b>：[307]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00696">https://arxiv.org/abs/2404.00696</a></p>
  <p><b>作者</b>：Abdallah Alshantti,  Adil Rasheed,  Frank Westad</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：potentially leak sensitive, leak sensitive information, subject to overfitting, leak sensitive, attacks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generative models are subject to overfitting and thus may potentially leak sensitive information from the training data. In this work. we investigate the privacy risks that can potentially arise from the use of generative adversarial networks (GANs) for creating tabular synthetic datasets. For the purpose, we analyse the effects of re-identification attacks on synthetic data, i.e., attacks which aim at selecting samples that are predicted to correspond to memorised training samples based on their proximity to the nearest synthetic records. We thus consider multiple settings where different attackers might have different access levels or knowledge of the generative model and predictive, and assess which information is potentially most useful for launching more successful re-identification attacks. In doing so we also consider the situation for which re-identification attacks are formulated as reconstruction attacks, i.e., the situation where an attacker uses evolutionary multi-objective optimisation for perturbing synthetic samples closer to the training space. The results indicate that attackers can indeed pose major privacy risks by selecting synthetic samples that are likely representative of memorised training samples. In addition, we notice that privacy threats considerably increase when the attacker either has knowledge or has black-box access to the generative models. We also find that reconstruction attacks through multi-objective optimisation even increase the risk of identifying confidential samples.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：Meta Learning in Bandits within Shared Affine Subspaces</b></summary>
  <p><b>编号</b>：[310]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00688">https://arxiv.org/abs/2404.00688</a></p>
  <p><b>作者</b>：Steven Bilaj,  Sofien Dhouib,  Setareh Maghsudi</p>
  <p><b>备注</b>：Accepted in AISTATS 2024</p>
  <p><b>关键词</b>：low-dimensional affine subspace, online principal component, principal component analysis, contextual stochastic bandits, stochastic bandits tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the problem of meta-learning several contextual stochastic bandits tasks by leveraging their concentration around a low-dimensional affine subspace, which we learn via online principal component analysis to reduce the expected regret over the encountered bandits. We propose and theoretically analyze two strategies that solve the problem: One based on the principle of optimism in the face of uncertainty and the other via Thompson sampling. Our framework is generic and includes previously proposed approaches as special cases. Besides, the empirical results show that our methods significantly reduce the regret on several bandit tasks.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：Utilizing Maximum Mean Discrepancy Barycenter for Propagating the  Uncertainty of Value Functions in Reinforcement Learning</b></summary>
  <p><b>编号</b>：[311]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00686">https://arxiv.org/abs/2404.00686</a></p>
  <p><b>作者</b>：Srinjoy Roy,  Swagatam Das</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Reinforcement Learning, exploration in Reinforcement, functions boosts exploration, boosts exploration, Temporal Difference</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Accounting for the uncertainty of value functions boosts exploration in Reinforcement Learning (RL). Our work introduces Maximum Mean Discrepancy Q-Learning (MMD-QL) to improve Wasserstein Q-Learning (WQL) for uncertainty propagation during Temporal Difference (TD) updates. MMD-QL uses the MMD barycenter for this purpose, as MMD provides a tighter estimate of closeness between probability measures than the Wasserstein distance. Firstly, we establish that MMD-QL is Probably Approximately Correct in MDP (PAC-MDP) under the average loss metric. Concerning the accumulated rewards, experiments on tabular environments show that MMD-QL outperforms WQL and other algorithms. Secondly, we incorporate deep networks into MMD-QL to create MMD Q-Network (MMD-QN). Making reasonable assumptions, we analyze the convergence rates of MMD-QN using function approximation. Empirical results on challenging Atari games demonstrate that MMD-QN performs well compared to benchmark deep RL algorithms, highlighting its effectiveness in handling large state-action spaces.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：A Survey of Privacy-Preserving Model Explanations: Privacy Risks,  Attacks, and Countermeasures</b></summary>
  <p><b>编号</b>：[321]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00673">https://arxiv.org/abs/2404.00673</a></p>
  <p><b>作者</b>：Thanh Tam Nguyen,  Thanh Trung Huynh,  Zhao Ren,  Thanh Toan Nguyen,  Phi Le Nguyen,  Hongzhi Yin,  Quoc Viet Hung Nguyen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：privacy implications intensifies, continues to expand, implications intensifies, adoption of explainable, urgency to address</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As the adoption of explainable AI (XAI) continues to expand, the urgency to address its privacy implications intensifies. Despite a growing corpus of research in AI privacy and explainability, there is little attention on privacy-preserving model explanations. This article presents the first thorough survey about privacy attacks on model explanations and their countermeasures. Our contribution to this field comprises a thorough analysis of research papers with a connected taxonomy that facilitates the categorisation of privacy attacks and countermeasures based on the targeted explanations. This work also includes an initial investigation into the causes of privacy leaks. Finally, we discuss unresolved issues and prospective research directions uncovered in our analysis. This survey aims to be a valuable resource for the research community and offers clear insights for those new to this domain. To support ongoing research, we have established an online resource repository, which will be continuously updated with new and relevant findings. Interested readers are encouraged to access our repository at this https URL.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：A General and Efficient Training for Transformer via Token Expansion</b></summary>
  <p><b>编号</b>：[322]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00672">https://arxiv.org/abs/2404.00672</a></p>
  <p><b>作者</b>：Wenxuan Huang,  Yunhang Shen,  Jiao Xie,  Baochang Zhang,  Gaoqi He,  Ke Li,  Xing Sun,  Shaohui Lin</p>
  <p><b>备注</b>：Accepted to CVPR 2024. Code is available at this https URL</p>
  <p><b>关键词</b>：large training cost, extremely large training, Vision Transformers, training, requires an extremely</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The remarkable performance of Vision Transformers (ViTs) typically requires an extremely large training cost. Existing methods have attempted to accelerate the training of ViTs, yet typically disregard method universality with accuracy dropping. Meanwhile, they break the training consistency of the original transformers, including the consistency of hyper-parameters, architecture, and strategy, which prevents them from being widely applied to different Transformer networks. In this paper, we propose a novel token growth scheme Token Expansion (termed ToE) to achieve consistent training acceleration for ViTs. We introduce an "initialization-expansion-merging" pipeline to maintain the integrity of the intermediate feature distribution of original transformers, preventing the loss of crucial learnable information in the training process. ToE can not only be seamlessly integrated into the training and fine-tuning process of transformers (e.g., DeiT and LV-ViT), but also effective for efficient training frameworks (e.g., EfficientTrain), without twisting the original training hyper-parameters, architecture, and introducing additional training strategies. Extensive experiments demonstrate that ToE achieves about 1.3x faster for the training of ViTs in a lossless manner, or even with performance gains over the full-token training baselines. Code is available at this https URL .</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：Accelerated Parameter-Free Stochastic Optimization</b></summary>
  <p><b>编号</b>：[325]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00666">https://arxiv.org/abs/2404.00666</a></p>
  <p><b>作者</b>：Itai Kreisler,  Maor Ivgi,  Oliver Hinder,  Yair Carmon</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：achieves near-optimal rates, smooth stochastic convex, stochastic convex optimization, rates for smooth, smooth stochastic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a method that achieves near-optimal rates for smooth stochastic convex optimization and requires essentially no prior knowledge of problem parameters. This improves on prior work which requires knowing at least the initial distance to optimality d0. Our method, U-DoG, combines UniXGrad (Kavis et al., 2019) and DoG (Ivgi et al., 2023) with novel iterate stabilization techniques. It requires only loose bounds on d0 and the noise magnitude, provides high probability guarantees under sub-Gaussian noise, and is also near-optimal in the non-smooth case. Our experiments show consistent, strong performance on convex problems and mixed results on neural network training.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：Observations on Building RAG Systems for Technical Documents</b></summary>
  <p><b>编号</b>：[329]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00657">https://arxiv.org/abs/2404.00657</a></p>
  <p><b>作者</b>：Sumit Soman,  Sujoy Roychowdhury</p>
  <p><b>备注</b>：Published as a Tiny Paper at ICLR 2024</p>
  <p><b>关键词</b>：Retrieval augmented generation, capture domain information, technical documents creates, documents creates challenges, Retrieval augmented</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Retrieval augmented generation (RAG) for technical documents creates challenges as embeddings do not often capture domain information. We review prior art for important factors affecting RAG and perform experiments to highlight best practices and potential challenges to build RAG systems for technical documents.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：Learning Off-policy with Model-based Intrinsic Motivation For Active  Online Exploration</b></summary>
  <p><b>编号</b>：[333]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00651">https://arxiv.org/abs/2404.00651</a></p>
  <p><b>作者</b>：Yibo Wang,  Jiang Zhao</p>
  <p><b>备注</b>：Preprint</p>
  <p><b>关键词</b>：demonstrated notable progress, deep reinforcement learning, Recent advancements, spanning both model-based, model-free paradigms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advancements in deep reinforcement learning (RL) have demonstrated notable progress in sample efficiency, spanning both model-based and model-free paradigms. Despite the identification and mitigation of specific bottlenecks in prior works, the agent's exploration ability remains under-emphasized in the realm of sample-efficient RL. This paper investigates how to achieve sample-efficient exploration in continuous control tasks. We introduce an RL algorithm that incorporates a predictive model and off-policy learning elements, where an online planner enhanced by a novelty-aware terminal value function is employed for sample collection. Leveraging the forward predictive error within a latent state space, we derive an intrinsic reward without incurring parameters overhead. This reward establishes a solid connection to model uncertainty, allowing the agent to effectively overcome the asymptotic performance gap. Through extensive experiments, our method shows competitive or even superior performance compared to prior works, especially the sparse reward cases.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize  Configuration Errors via Logs</b></summary>
  <p><b>编号</b>：[338]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00640">https://arxiv.org/abs/2404.00640</a></p>
  <p><b>作者</b>：Shiwen Shan,  Yintong Huo,  Yuxin Su,  Yichen Li,  Dan Li,  Zibin Zheng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Configurable software systems, configuration errors, losses to companies, configuration, software systems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Configurable software systems are prone to configuration errors, resulting in significant losses to companies. However, diagnosing these errors is challenging due to the vast and complex configuration space. These errors pose significant challenges for both experienced maintainers and new end-users, particularly those without access to the source code of the software systems. Given that logs are easily accessible to most end-users, we conduct a preliminary study to outline the challenges and opportunities of utilizing logs in localizing configuration errors. Based on the insights gained from the preliminary study, we propose an LLM-based two-stage strategy for end-users to localize the root-cause configuration properties based on logs. We further implement a tool, LogConfigLocalizer, aligned with the design of the aforementioned strategy, hoping to assist end-users in coping with configuration errors through log analysis.
To the best of our knowledge, this is the first work to localize the root-cause configuration properties for end-users based on Large Language Models~(LLMs) and logs. We evaluate the proposed strategy on Hadoop by LogConfigLocalizer and prove its efficiency with an average accuracy as high as 99.91%. Additionally, we also demonstrate the effectiveness and necessity of different phases of the methodology by comparing it with two other variants and a baseline tool. Moreover, we validate the proposed methodology through a practical case study to demonstrate its effectiveness and feasibility.</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：RL-MUL: Multiplier Design Optimization with Deep Reinforcement Learning</b></summary>
  <p><b>编号</b>：[339]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00639">https://arxiv.org/abs/2404.00639</a></p>
  <p><b>作者</b>：Dongsheng Zuo,  Jiadong Zhu,  Yikang Ouyang,  Yuzhe Ma</p>
  <p><b>备注</b>：Extension of DAC 2023 version</p>
  <p><b>关键词</b>：fundamental operation, widely adopted, multipliers, RL-MUL, area and delay</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multiplication is a fundamental operation in many applications, and multipliers are widely adopted in various circuits. However, optimizing multipliers is challenging and non-trivial due to the huge design space. In this paper, we propose RL-MUL, a multiplier design optimization framework based on reinforcement learning. Specifically, we utilize matrix and tensor representations for the compressor tree of a multiplier, based on which the convolutional neural networks can be seamlessly incorporated as the agent network. The agent can learn to optimize the multiplier structure based on a Pareto-driven reward which is customized to accommodate the trade-off between area and delay. Additionally, the capability of RL-MUL is extended to optimize the fused multiply-accumulator (MAC) designs. Experiments are conducted on different bit widths of multipliers. The results demonstrate that the multipliers produced by RL-MUL can dominate all baseline designs in terms of area and delay. The performance gain of RL-MUL is further validated by comparing the area and delay of processing element arrays using multipliers from RL-MUL and baseline approaches.</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：HypeBoy: Generative Self-Supervised Representation Learning on  Hypergraphs</b></summary>
  <p><b>编号</b>：[340]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00638">https://arxiv.org/abs/2404.00638</a></p>
  <p><b>作者</b>：Sunwoo Kim,  Shinhwan Kang,  Fanchen Bu,  Soo Yong Lee,  Jaemin Yoo,  Kijung Shin</p>
  <p><b>备注</b>：Published as a conference paper at ICLR 2024</p>
  <p><b>关键词</b>：expressing higher-order interactions, generative SSL, SSL, generative SSL task, generative SSL strategy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple nodes with hyperedges, and better capturing the topology is essential for effective representation learning. Recent advances in generative self-supervised learning (SSL) suggest that hypergraph neural networks learned from generative self supervision have the potential to effectively encode the complex hypergraph topology. Designing a generative SSL strategy for hypergraphs, however, is not straightforward. Questions remain with regard to its generative SSL task, connection to downstream tasks, and empirical properties of learned representations. In light of the promises and challenges, we propose a novel generative SSL strategy for hypergraphs. We first formulate a generative SSL task on hypergraphs, hyperedge filling, and highlight its theoretical connection to node classification. Based on the generative SSL task, we propose a hypergraph SSL method, HypeBoy. HypeBoy learns effective general-purpose hypergraph representations, outperforming 16 baseline methods across 11 benchmark datasets.</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：Variational Autoencoders for exteroceptive perception in reinforcement  learning-based collision avoidance</b></summary>
  <p><b>编号</b>：[349]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00623">https://arxiv.org/abs/2404.00623</a></p>
  <p><b>作者</b>：Thomas Nakken Larsen,  Eirik Runde Barlaug,  Adil Rasheed</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Deep Reinforcement Learning, machine learning algorithms, Modern control systems, increasingly turning, turning to machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Modern control systems are increasingly turning to machine learning algorithms to augment their performance and adaptability. Within this context, Deep Reinforcement Learning (DRL) has emerged as a promising control framework, particularly in the domain of marine transportation. Its potential for autonomous marine applications lies in its ability to seamlessly combine path-following and collision avoidance with an arbitrary number of obstacles. However, current DRL algorithms require disproportionally large computational resources to find near-optimal policies compared to the posed control problem when the searchable parameter space becomes large. To combat this, our work delves into the application of Variational AutoEncoders (VAEs) to acquire a generalized, low-dimensional latent encoding of a high-fidelity range-finding sensor, which serves as the exteroceptive input to a DRL agent. The agent's performance, encompassing path-following and collision avoidance, is systematically tested and evaluated within a stochastic simulation environment, presenting a comprehensive exploration of our proposed approach in maritime control systems.</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：A Multi-Branched Radial Basis Network Approach to Predicting Complex  Chaotic Behaviours</b></summary>
  <p><b>编号</b>：[353]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00618">https://arxiv.org/abs/2404.00618</a></p>
  <p><b>作者</b>：Aarush Sinha</p>
  <p><b>备注</b>：7 pages, 4 figures</p>
  <p><b>关键词</b>：multi branched network, branched network approach, physics attractor characterized, Radial Basis Function, propose a multi</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this study, we propose a multi branched network approach to predict the dynamics of a physics attractor characterized by intricate and chaotic behavior. We introduce a unique neural network architecture comprised of Radial Basis Function (RBF) layers combined with an attention mechanism designed to effectively capture nonlinear inter-dependencies inherent in the attractor's temporal evolution. Our results demonstrate successful prediction of the attractor's trajectory across 100 predictions made using a real-world dataset of 36,700 time-series observations encompassing approximately 28 minutes of activity. To further illustrate the performance of our proposed technique, we provide comprehensive visualizations depicting the attractor's original and predicted behaviors alongside quantitative measures comparing observed versus estimated outcomes. Overall, this work showcases the potential of advanced machine learning algorithms in elucidating hidden structures in complex physical systems while offering practical applications in various domains requiring accurate short-term forecasting capabilities.</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：Extensive Self-Contrast Enables Feedback-Free Language Model Alignment</b></summary>
  <p><b>编号</b>：[359]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00604">https://arxiv.org/abs/2404.00604</a></p>
  <p><b>作者</b>：Xiao Liu,  Xixuan Song,  Yuxiao Dong,  Jie Tang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Reinforcement learning, recent large language, central technique, technique for recent, RLHF</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reinforcement learning from human feedback (RLHF) has been a central technique for recent large language model (LLM) alignment. However, its heavy dependence on costly human or LLM-as-Judge preference feedback could stymie its wider applications. In this work, we introduce Self-Contrast, a feedback-free large language model alignment method via exploiting extensive self-generated negatives. With only supervised fine-tuning (SFT) targets, Self-Contrast leverages the LLM itself to generate massive diverse candidates, and harnesses a pre-trained embedding model to filter multiple negatives according to text similarity. Theoretically, we illustrate that in this setting, merely scaling negative responses can still effectively approximate situations with more balanced positive and negative preference annotations. Our experiments with direct preference optimization (DPO) on three datasets show that, Self-Contrast could consistently outperform SFT and standard DPO training by large margins. And as the number of self-generated negatives increases, the performance of Self-Contrast continues to grow. Code and data are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：LAESI: Leaf Area Estimation with Synthetic Imagery</b></summary>
  <p><b>编号</b>：[369]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00593">https://arxiv.org/abs/2404.00593</a></p>
  <p><b>作者</b>：Jacek Kałużny,  Yannik Schreckenberg,  Karol Cyganik,  Peter Annighöfer,  Sören Pirk,  Dominik L. Michels,  Mikolaj Cieslak,  Farhah Assaad-Gerbert,  Bedrich Benes,  Wojciech Pałubicki</p>
  <p><b>备注</b>：10 pages, 12 figures, 1 table</p>
  <p><b>关键词</b>：Synthetic Leaf Dataset, Synthetic Leaf, synthetic leaf images, surface area labels, millimeter paper</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce LAESI, a Synthetic Leaf Dataset of 100,000 synthetic leaf images on millimeter paper, each with semantic masks and surface area labels. This dataset provides a resource for leaf morphology analysis primarily aimed at beech and oak leaves. We evaluate the applicability of the dataset by training machine learning models for leaf surface area prediction and semantic segmentation, using real images for validation. Our validation shows that these models can be trained to predict leaf surface area with a relative error not greater than an average human annotator. LAESI also provides an efficient framework based on 3D procedural models and generative AI for the large-scale, controllable generation of data with potential further applications in agriculture and biology. We evaluate the inclusion of generative AI in our procedural data generation pipeline and show how data filtering based on annotation consistency results in datasets which allow training the highest performing vision models.</p>
  </details>
</details>
<details>
  <summary>83. <b>标题：Harnessing the Power of Large Language Model for Uncertainty Aware Graph  Processing</b></summary>
  <p><b>编号</b>：[372]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00589">https://arxiv.org/abs/2404.00589</a></p>
  <p><b>作者</b>：Zhenyu Qian,  Yiming Qian,  Yuting Song,  Fei Gao,  Hai Jin,  Chen Yu,  Xia Xie</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Handling graph data, graph data, large graph data, handling large graph, graph</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Handling graph data is one of the most difficult tasks. Traditional techniques, such as those based on geometry and matrix factorization, rely on assumptions about the data relations that become inadequate when handling large and complex graph data. On the other hand, deep learning approaches demonstrate promising results in handling large graph data, but they often fall short of providing interpretable explanations. To equip the graph processing with both high accuracy and explainability, we introduce a novel approach that harnesses the power of a large language model (LLM), enhanced by an uncertainty-aware module to provide a confidence score on the generated answer. We experiment with our approach on two graph processing tasks: few-shot knowledge graph completion and graph classification. Our results demonstrate that through parameter efficient fine-tuning, the LLM surpasses state-of-the-art algorithms by a substantial margin across ten diverse benchmark datasets. Moreover, to address the challenge of explainability, we propose an uncertainty estimation based on perturbation, along with a calibration scheme to quantify the confidence scores of the generated answers. Our confidence measure achieves an AUC of 0.8 or higher on seven out of the ten datasets in predicting the correctness of the answer generated by LLM.</p>
  </details>
</details>
<details>
  <summary>84. <b>标题：Automated Bi-Fold Weighted Ensemble Algorithms and its Application to  Brain Tumor Detection and Classification</b></summary>
  <p><b>编号</b>：[378]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00576">https://arxiv.org/abs/2404.00576</a></p>
  <p><b>作者</b>：PoTsang B. Huang,  Muhammad Rizwan,  Mehboob Ali</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：types of cancers, uncontrolled and unstructured, unstructured growth, highest mortality rates, mortality rates</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The uncontrolled and unstructured growth of brain cells is known as brain tumor, which has one of the highest mortality rates among diseases from all types of cancers. Due to limited diagnostic and treatment capabilities, they pose significant challenges, especially in third-world countries. Early diagnosis plays a vital role in effectively managing brain tumors and reducing mortality rates. However, the availability of diagnostic methods is hindered by various limitations, including high costs and lengthy result acquisition times, impeding early detection of the disease. In this study, we present two cutting-edge bi-fold weighted voting ensemble models that aim to boost the effectiveness of weighted ensemble methods. These two proposed methods combine the classification outcomes from multiple classifiers and determine the optimal result by selecting the one with the highest probability in the first approach, and the highest weighted prediction in the second technique. These approaches significantly improve the overall performance of weighted ensemble techniques. In the first proposed method, we improve the soft voting technique (SVT) by introducing a novel unsupervised weight calculating schema (UWCS) to enhance its weight assigning capability, known as the extended soft voting technique (ESVT). Secondly, we propose a novel weighted method (NWM) by using the proposed UWCS. Both of our approaches incorporate three distinct models: a custom-built CNN, VGG-16, and InceptionResNetV2 which has been trained on publicly available datasets. The effectiveness of our proposed systems is evaluated through blind testing, where exceptional results are achieved. We then establish a comparative analysis of the performance of our proposed methods with that of SVT to show their superiority and effectiveness.</p>
  </details>
</details>
<details>
  <summary>85. <b>标题：ADs: Active Data-sharing for Data Quality Assurance in Advanced  Manufacturing Systems</b></summary>
  <p><b>编号</b>：[380]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00572">https://arxiv.org/abs/2404.00572</a></p>
  <p><b>作者</b>：Yue Zhao,  Yuxuan Li,  Chenang Liu,  Yinan Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：industrial applications, require a large, large amount, amount of training, data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine learning (ML) methods are widely used in industrial applications, which usually require a large amount of training data. However, data collection needs extensive time costs and investments in the manufacturing system, and data scarcity commonly exists. Therefore, data-sharing is widely enabled among multiple machines with similar functionality to augment the dataset for building ML methods. However, distribution mismatch inevitably exists in their data due to different working conditions, while the ML methods are assumed to be built and tested on the dataset following the same distribution. Thus, an Active Data-sharing (ADs) framework is proposed to ensure the quality of the shared data among multiple machines. It is designed to simultaneously select the most informative data points benefiting the downstream tasks and mitigate the distribution mismatch among all selected data points. The proposed method is validated on anomaly detection on in-situ monitoring data from three additive manufacturing processes.</p>
  </details>
</details>
<details>
  <summary>86. <b>标题：Solving the QAP by Two-Stage Graph Pointer Networks and Reinforcement  Learning</b></summary>
  <p><b>编号</b>：[397]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00539">https://arxiv.org/abs/2404.00539</a></p>
  <p><b>作者</b>：Satoko Iida,  Ryota Yasudo</p>
  <p><b>备注</b>：7 pages, 7 figures</p>
  <p><b>关键词</b>：Quadratic Assignment Problem, Quadratic Assignment, Assignment Problem, QAP, combinatorial optimization problems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Quadratic Assignment Problem (QAP) is a practical combinatorial optimization problems that has been studied for several years. Since it is NP-hard, solving large problem instances of QAP is challenging. Although heuristics can find semi-optimal solutions, the execution time significantly increases as the problem size increases. Recently, solving combinatorial optimization problems by deep learning has been attracting attention as a faster solver than heuristics. Even with deep learning, however, solving large QAP is still challenging. In this paper, we propose the deep reinforcement learning model called the two-stage graph pointer network (GPN) for solving QAP. Two-stage GPN relies on GPN, which has been proposed for Euclidean Traveling Salesman Problem (TSP). First, we extend GPN for general TSP, and then we add new algorithms to that model for solving QAP. Our experimental results show that our two-stage GPN provides semi-optimal solutions for benchmark problem instances from TSPlib and QAPLIB.</p>
  </details>
</details>
<details>
  <summary>87. <b>标题：Comparing Bad Apples to Good Oranges: Aligning Large Language Models via  Joint Preference Optimization</b></summary>
  <p><b>编号</b>：[400]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00530">https://arxiv.org/abs/2404.00530</a></p>
  <p><b>作者</b>：Hritik Bansal,  Ashima Suvarna,  Gantavya Bhatt,  Nanyun Peng,  Kai-Wei Chang,  Aditya Grover</p>
  <p><b>备注</b>：25 pages, 14 figures, 5 tables</p>
  <p><b>关键词</b>：large language models, aligning large language, comparing multiple generations, multiple generations conditioned, acquiring human preferences</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A common technique for aligning large language models (LLMs) relies on acquiring human preferences by comparing multiple generations conditioned on a fixed context. This only leverages the pairwise comparisons when the generations are placed in an identical context. However, such conditional rankings often fail to capture the complex and multidimensional aspects of human preferences. In this work, we revisit the traditional paradigm of preference acquisition and propose a new axis that is based on eliciting preferences jointly over the instruction-response pairs. While prior preference optimizations are designed for conditional ranking protocols (e.g., DPO), our proposed preference acquisition protocol introduces DOVE, a new preference optimization objective that upweights the joint probability of the chosen instruction-response pair over the rejected instruction-response pair. Interestingly, we find that the LLM trained with joint instruction-response preference data using DOVE outperforms the LLM trained with DPO by 5.2% and 3.3% win-rate for the summarization and open-ended dialogue datasets, respectively. Our findings reveal that joint preferences over instruction and response pairs can significantly enhance the alignment of LLMs by tapping into a broader spectrum of human preference elicitation. The data and code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>88. <b>标题：Super Non-singular Decompositions of Polynomials and their Application  to Robustly Learning Low-degree PTFs</b></summary>
  <p><b>编号</b>：[401]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00529">https://arxiv.org/abs/2404.00529</a></p>
  <p><b>作者</b>：Ilias Diakonikolas,  Daniel M. Kane,  Vasilis Kontonis,  Sihan Liu,  Nikos Zarifis</p>
  <p><b>备注</b>：To appear in STOC2024</p>
  <p><b>关键词</b>：strong contamination model, label noise model, study the efficient, efficient learnability, learnability of low-degree</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the efficient learnability of low-degree polynomial threshold functions (PTFs) in the presence of a constant fraction of adversarial corruptions. Our main algorithmic result is a polynomial-time PAC learning algorithm for this concept class in the strong contamination model under the Gaussian distribution with error guarantee $O_{d, c}(\text{opt}^{1-c})$, for any desired constant $c>0$, where $\text{opt}$ is the fraction of corruptions. In the strong contamination model, an omniscient adversary can arbitrarily corrupt an $\text{opt}$-fraction of the data points and their labels. This model generalizes the malicious noise model and the adversarial label noise model. Prior to our work, known polynomial-time algorithms in this corruption model (or even in the weaker adversarial label noise model) achieved error $\tilde{O}_d(\text{opt}^{1/(d+1)})$, which deteriorates significantly as a function of the degree $d$.
Our algorithm employs an iterative approach inspired by localization techniques previously used in the context of learning linear threshold functions. Specifically, we use a robust perceptron algorithm to compute a good partial classifier and then iterate on the unclassified points. In order to achieve this, we need to take a set defined by a number of polynomial inequalities and partition it into several well-behaved subsets. To this end, we develop new polynomial decomposition techniques that may be of independent interest.</p>
  </details>
</details>
<details>
  <summary>89. <b>标题：Generative weather for improved crop model simulations</b></summary>
  <p><b>编号</b>：[402]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00528">https://arxiv.org/abs/2404.00528</a></p>
  <p><b>作者</b>：Yuji Saikai</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：yield prediction, crop yield prediction, precise crop yield, farm levels, regional levels</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Accurate and precise crop yield prediction is invaluable for decision making at both farm levels and regional levels. To make yield prediction, crop models are widely used for their capability to simulate hypothetical scenarios. While accuracy and precision of yield prediction critically depend on weather inputs to simulations, surprisingly little attention has been paid to preparing weather inputs. We propose a new method to construct generative models for long-term weather forecasts and ultimately improve crop yield prediction. We demonstrate use of the method in two representative scenarios -- single-year production of wheat, barley and canola and three-year production using rotations of these crops. Results show significant improvement from the conventional method, measured in terms of mean and standard deviation of prediction errors. Our method outperformed the conventional method in every one of 18 metrics for the first scenario and in 29 out of 36 metrics for the second scenario. For individual crop modellers to start applying the method to their problems, technical details are carefully explained, and all the code, trained PyTorch models, APSIM simulation files and result data are made available.</p>
  </details>
</details>
<details>
  <summary>90. <b>标题：Creating synthetic energy meter data using conditional diffusion and  building metadata</b></summary>
  <p><b>编号</b>：[405]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00525">https://arxiv.org/abs/2404.00525</a></p>
  <p><b>作者</b>：Chun Fu,  Hussain Kazmi,  Matias Quintana,  Clayton Miller</p>
  <p><b>备注</b>：17 pages, 11 figures, submitted to journal "Energy and Buildings"</p>
  <p><b>关键词</b>：increased computational power, Advances in machine, energy-related research, machine learning, learning and increased</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Advances in machine learning and increased computational power have driven progress in energy-related research. However, limited access to private energy data from buildings hinders traditional regression models relying on historical data. While generative models offer a solution, previous studies have primarily focused on short-term generation periods (e.g., daily profiles) and a limited number of meters. Thus, the study proposes a conditional diffusion model for generating high-quality synthetic energy data using relevant metadata. Using a dataset comprising 1,828 power meters from various buildings and countries, this model is compared with traditional methods like Conditional Generative Adversarial Networks (CGAN) and Conditional Variational Auto-Encoders (CVAE). It explicitly handles long-term annual consumption profiles, harnessing metadata such as location, weather, building, and meter type to produce coherent synthetic data that closely resembles real-world energy consumption patterns. The results demonstrate the proposed diffusion model's superior performance, with a 36% reduction in Frechet Inception Distance (FID) score and a 13% decrease in Kullback-Leibler divergence (KL divergence) compared to the following best method. The proposed method successfully generates high-quality energy data through metadata, and its code will be open-sourced, establishing a foundation for a broader array of energy data generation models in the future.</p>
  </details>
</details>
<details>
  <summary>91. <b>标题：Minimum-Norm Interpolation Under Covariate Shift</b></summary>
  <p><b>编号</b>：[407]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00522">https://arxiv.org/abs/2404.00522</a></p>
  <p><b>作者</b>：Neil Mallinar,  Austin Zane,  Spencer Frei,  Bin Yu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：real-world machine learning, machine learning deployments, Transfer learning, critical part, part of real-world</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transfer learning is a critical part of real-world machine learning deployments and has been extensively studied in experimental works with overparameterized neural networks. However, even in the simplest setting of linear regression a notable gap still exists in the theoretical understanding of transfer learning. In-distribution research on high-dimensional linear regression has led to the identification of a phenomenon known as \textit{benign overfitting}, in which linear interpolators overfit to noisy training labels and yet still generalize well. This behavior occurs under specific conditions on the source covariance matrix and input data dimension. Therefore, it is natural to wonder how such high-dimensional linear models behave under transfer learning. We prove the first non-asymptotic excess risk bounds for benignly-overfit linear interpolators in the transfer learning setting. From our analysis, we propose a taxonomy of \textit{beneficial} and \textit{malignant} covariate shifts based on the degree of overparameterization. We follow our analysis with empirical studies that show these beneficial and malignant covariate shifts for linear interpolators on real image data, and for fully-connected neural networks in settings where the input data dimension is larger than the training sample size.</p>
  </details>
</details>
<details>
  <summary>92. <b>标题：CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz  continuity constrAIned Normalization</b></summary>
  <p><b>编号</b>：[408]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00521">https://arxiv.org/abs/2404.00521</a></p>
  <p><b>作者</b>：Yao Ni,  Piotr Koniusz</p>
  <p><b>备注</b>：Accepted by CVPR2024, 26 pages full version</p>
  <p><b>关键词</b>：Generative Adversarial Networks, Adversarial Networks, performance heavily depends, Generative Adversarial, significantly advanced image</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generative Adversarial Networks (GANs) significantly advanced image generation but their performance heavily depends on abundant training data. In scenarios with limited data, GANs often struggle with discriminator overfitting and unstable training. Batch Normalization (BN), despite being known for enhancing generalization and training stability, has rarely been used in the discriminator of Data-Efficient GANs. Our work addresses this gap by identifying a critical flaw in BN: the tendency for gradient explosion during the centering and scaling steps. To tackle this issue, we present CHAIN (lipsCHitz continuity constrAIned Normalization), which replaces the conventional centering step with zero-mean regularization and integrates a Lipschitz continuity constraint in the scaling step. CHAIN further enhances GAN training by adaptively interpolating the normalized and unnormalized features, effectively avoiding discriminator overfitting. Our theoretical analyses firmly establishes CHAIN's effectiveness in reducing gradients in latent features and weights, improving stability and generalization in GAN training. Empirical evidence supports our theory. CHAIN achieves state-of-the-art results in data-limited scenarios on CIFAR-10/100, ImageNet, five low-shot and seven high-resolution few-shot image datasets.</p>
  </details>
</details>
<details>
  <summary>93. <b>标题：DailyMAE: Towards Pretraining Masked Autoencoders in One Day</b></summary>
  <p><b>编号</b>：[414]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00509">https://arxiv.org/abs/2404.00509</a></p>
  <p><b>作者</b>：Jiantao Wu,  Shentong Mo,  Sara Atito,  Zhenhua Feng,  Josef Kittler,  Muhammad Awais</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：masked image modeling, important self-supervised learning, learning data representation, self-supervised learning, masked image</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, masked image modeling (MIM), an important self-supervised learning (SSL) method, has drawn attention for its effectiveness in learning data representation from unlabeled data. Numerous studies underscore the advantages of MIM, highlighting how models pretrained on extensive datasets can enhance the performance of downstream tasks. However, the high computational demands of pretraining pose significant challenges, particularly within academic environments, thereby impeding the SSL research progress. In this study, we propose efficient training recipes for MIM based SSL that focuses on mitigating data loading bottlenecks and employing progressive training techniques and other tricks to closely maintain pretraining performance. Our library enables the training of a MAE-Base/16 model on the ImageNet 1K dataset for 800 epochs within just 18 hours, using a single machine equipped with 8 A100 GPUs. By achieving speed gains of up to 5.8 times, this work not only demonstrates the feasibility of conducting high-efficiency SSL training but also paves the way for broader accessibility and promotes advancement in SSL research particularly for prototyping and initial testing of SSL ideas. The code is available in this https URL.</p>
  </details>
</details>
<details>
  <summary>94. <b>标题：Label-Agnostic Forgetting: A Supervision-Free Unlearning in Deep Models</b></summary>
  <p><b>编号</b>：[416]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00506">https://arxiv.org/abs/2404.00506</a></p>
  <p><b>作者</b>：Shaofei Shen,  Chenhao Zhang,  Yawen Zhao,  Alina Bialkowski,  Weitong Chen,  Miao Xu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Machine unlearning aims, remove information derived, unlearning, Machine unlearning, aims to remove</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine unlearning aims to remove information derived from forgotten data while preserving that of the remaining dataset in a well-trained model. With the increasing emphasis on data privacy, several approaches to machine unlearning have emerged. However, these methods typically rely on complete supervision throughout the unlearning process. Unfortunately, obtaining such supervision, whether for the forgetting or remaining data, can be impractical due to the substantial cost associated with annotating real-world datasets. This challenge prompts us to propose a supervision-free unlearning approach that operates without the need for labels during the unlearning process. Specifically, we introduce a variational approach to approximate the distribution of representations for the remaining data. Leveraging this approximation, we adapt the original model to eliminate information from the forgotten data at the representation level. To further address the issue of lacking supervision information, which hinders alignment with ground truth, we introduce a contrastive loss to facilitate the matching of representations between the remaining data and those of the original model, thus preserving predictive performance. Experimental results across various unlearning tasks demonstrate the effectiveness of our proposed method, Label-Agnostic Forgetting (LAF) without using any labels, which achieves comparable performance to state-of-the-art methods that rely on full supervision information. Furthermore, our approach excels in semi-supervised scenarios, leveraging limited supervision information to outperform fully supervised baselines. This work not only showcases the viability of supervision-free unlearning in deep models but also opens up a new possibility for future research in unlearning at the representation level.</p>
  </details>
</details>
<details>
  <summary>95. <b>标题：Transfer Learning with Reconstruction Loss</b></summary>
  <p><b>编号</b>：[417]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00505">https://arxiv.org/abs/2404.00505</a></p>
  <p><b>作者</b>：Wei Cui,  Wei Yu</p>
  <p><b>备注</b>：16 pages, 5 figures. To appear in IEEE Transactions on Machine Learning in Communications and Networking (TMLCN)</p>
  <p><b>关键词</b>：specific optimization objective, utilizing neural networks, mathematical optimization, specific optimization, neural network models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In most applications of utilizing neural networks for mathematical optimization, a dedicated model is trained for each specific optimization objective. However, in many scenarios, several distinct yet correlated objectives or tasks often need to be optimized on the same set of problem inputs. Instead of independently training a different neural network for each problem separately, it would be more efficient to exploit the correlations between these objectives and to train multiple neural network models with shared model parameters and feature representations. To achieve this, this paper first establishes the concept of common information: the shared knowledge required for solving the correlated tasks, then proposes a novel approach for model training by adding into the model an additional reconstruction stage associated with a new reconstruction loss. This loss is for reconstructing the common information starting from a selected hidden layer in the model. The proposed approach encourages the learned features to be general and transferable, and therefore can be readily used for efficient transfer learning. For numerical simulations, three applications are studied: transfer learning on classifying MNIST handwritten digits, the device-to-device wireless network power allocation, and the multiple-input-single-output network downlink beamforming and localization. Simulation results suggest that the proposed approach is highly efficient in data and model complexity, is resilient to over-fitting, and has competitive performances.</p>
  </details>
</details>
<details>
  <summary>96. <b>标题：Conditional Pseudo-Reversible Normalizing Flow for Surrogate Modeling in  Quantifying Uncertainty Propagation</b></summary>
  <p><b>编号</b>：[419]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00502">https://arxiv.org/abs/2404.00502</a></p>
  <p><b>作者</b>：Minglei Yang,  Pengjun Wang,  Ming Fan,  Dan Lu,  Yanzhao Cao,  Guannan Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：conditional pseudo-reversible normalizing, inverse uncertainty propagation, efficiently quantify forward, pseudo-reversible normalizing flow, physical model polluted</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce a conditional pseudo-reversible normalizing flow for constructing surrogate models of a physical model polluted by additive noise to efficiently quantify forward and inverse uncertainty propagation. Existing surrogate modeling approaches usually focus on approximating the deterministic component of physical model. However, this strategy necessitates knowledge of noise and resorts to auxiliary sampling methods for quantifying inverse uncertainty propagation. In this work, we develop the conditional pseudo-reversible normalizing flow model to directly learn and efficiently generate samples from the conditional probability density functions. The training process utilizes dataset consisting of input-output pairs without requiring prior knowledge about the noise and the function. Our model, once trained, can generate samples from any conditional probability density functions whose high probability regions are covered by the training set. Moreover, the pseudo-reversibility feature allows for the use of fully-connected neural network architectures, which simplifies the implementation and enables theoretical analysis. We provide a rigorous convergence analysis of the conditional pseudo-reversible normalizing flow model, showing its ability to converge to the target conditional probability density function using the Kullback-Leibler divergence. To demonstrate the effectiveness of our method, we apply it to several benchmark tests and a real-world geologic carbon storage problem.</p>
  </details>
</details>
<details>
  <summary>97. <b>标题：94% on CIFAR-10 in 3.29 Seconds on a Single GPU</b></summary>
  <p><b>编号</b>：[421]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00498">https://arxiv.org/abs/2404.00498</a></p>
  <p><b>作者</b>：Keller Jordan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：machine learning, facilitating thousands, projects per year, widely used datasets, datasets in machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>CIFAR-10 is among the most widely used datasets in machine learning, facilitating thousands of research projects per year. To accelerate research and reduce the cost of experiments, we introduce training methods for CIFAR-10 which reach 94% accuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when run on a single NVIDIA A100 GPU. As one factor contributing to these training speeds, we propose a derandomized variant of horizontal flipping augmentation, which we show improves over the standard method in every case where flipping is beneficial over no flipping at all. Our code is released at this https URL.</p>
  </details>
</details>
<details>
  <summary>98. <b>标题：Multi-hop Question Answering under Temporal Knowledge Editing</b></summary>
  <p><b>编号</b>：[424]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00492">https://arxiv.org/abs/2404.00492</a></p>
  <p><b>作者</b>：Keyuan Cheng,  Gang Lin,  Haoyang Fei,  Yuxuan zhai,  Lu Yu,  Muhammad Asif Ali,  Lijie Hu,  Di Wang</p>
  <p><b>备注</b>：23 pages</p>
  <p><b>关键词</b>：Multi-hop question answering, garnered significant attention, large language models, question answering, garnered significant</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multi-hop question answering (MQA) under knowledge editing (KE) has garnered significant attention in the era of large language models. However, existing models for MQA under KE exhibit poor performance when dealing with questions containing explicit temporal contexts. To address this limitation, we propose a novel framework, namely TEMPoral knowLEdge augmented Multi-hop Question Answering (TEMPLE-MQA). Unlike previous methods, TEMPLE-MQA first constructs a time-aware graph (TAG) to store edit knowledge in a structured manner. Then, through our proposed inference path, structural retrieval, and joint reasoning stages, TEMPLE-MQA effectively discerns temporal contexts within the question query. Experiments on benchmark datasets demonstrate that TEMPLE-MQA significantly outperforms baseline models. Additionally, we contribute a new dataset, namely TKEMQA, which serves as the inaugural benchmark tailored specifically for MQA with temporal scopes.</p>
  </details>
</details>
<details>
  <summary>99. <b>标题：PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt  Compression</b></summary>
  <p><b>编号</b>：[426]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00489">https://arxiv.org/abs/2404.00489</a></p>
  <p><b>作者</b>：Muhammad Asif Ali,  Zhengping Li,  Shu Yang,  Keyuan Cheng,  Yang Cao,  Tianhao Huang,  Lijie Hu,  Lu Yu,  Di Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：language processing tasks, natural language processing, shown exceptional abilities, Large language models, Large language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have shown exceptional abilities for multiple different natural language processing tasks. While prompting is a crucial tool for LLM inference, we observe that there is a significant cost associated with exceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead to sub-standard results in terms of readability and interpretability of the compressed prompt, with a detrimental impact on prompt utility. To address this, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an effective strategy for prompt compression over task-agnostic and task-aware prompts. PROMPT-SAW uses the prompt's textual information to build a graph, later extracts key information elements in the graph to come up with the compressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the existing GSM8k benchmark for task-agnostic prompts in order to provide a comprehensive evaluation platform. Experimental evaluation using benchmark datasets shows that prompts compressed by PROMPT-SAW are not only better in terms of readability, but they also outperform the best-performing baseline models by up to 14.3 and 13.7 respectively for task-aware and task-agnostic settings while compressing the original prompt text by 33.0 and 56.7.</p>
  </details>
</details>
<details>
  <summary>100. <b>标题：Noise-Aware Training of Layout-Aware Language Models</b></summary>
  <p><b>编号</b>：[427]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00488">https://arxiv.org/abs/2404.00488</a></p>
  <p><b>作者</b>：Ritesh Sarkhel,  Xiaoqi Ren,  Lauro Beltrao Costa,  Guolong Su,  Vincent Perot,  Yanan Xie,  Emmanouil Koukoumidis,  Arnab Nandi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：visually rich document, disseminate information, target document type, utilizes visual features, visually rich</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A visually rich document (VRD) utilizes visual features along with linguistic cues to disseminate information. Training a custom extractor that identifies named entities from a document requires a large number of instances of the target document type annotated at textual and visual modalities. This is an expensive bottleneck in enterprise scenarios, where we want to train custom extractors for thousands of different document types in a scalable way. Pre-training an extractor model on unlabeled instances of the target document type, followed by a fine-tuning step on human-labeled instances does not work in these scenarios, as it surpasses the maximum allowable training time allocated for the extractor. We address this scenario by proposing a Noise-Aware Training method or NAT in this paper. Instead of acquiring expensive human-labeled documents, NAT utilizes weakly labeled documents to train an extractor in a scalable way. To avoid degradation in the model's quality due to noisy, weakly labeled samples, NAT estimates the confidence of each training sample and incorporates it as uncertainty measure during training. We train multiple state-of-the-art extractor models using NAT. Experiments on a number of publicly available and in-house datasets show that NAT-trained models are not only robust in performance -- it outperforms a transfer-learning baseline by up to 6% in terms of macro-F1 score, but it is also more label-efficient -- it reduces the amount of human-effort required to obtain comparable performance by up to 73%.</p>
  </details>
</details>
<details>
  <summary>101. <b>标题：Cross-lingual Named Entity Corpus for Slavic Languages</b></summary>
  <p><b>编号</b>：[432]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00482">https://arxiv.org/abs/2404.00482</a></p>
  <p><b>作者</b>：Jakub Piskorski,  Michał Marcińczuk,  Roman Yangarber</p>
  <p><b>备注</b>：Published in LREC-COLING 2024 - The 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation</p>
  <p><b>关键词</b>：Slavic Natural Language, Natural Language Processing, Slavic languages, Slavic Natural, corpus manually annotated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a corpus manually annotated with named entities for six Slavic languages - Bulgarian, Czech, Polish, Slovenian, Russian, and Ukrainian. This work is the result of a series of shared tasks, conducted in 2017-2023 as a part of the Workshops on Slavic Natural Language Processing. The corpus consists of 5 017 documents on seven topics. The documents are annotated with five classes of named entities. Each entity is described by a category, a lemma, and a unique cross-lingual identifier. We provide two train-tune dataset splits - single topic out and cross topics. For each split, we set benchmarks using a transformer-based neural network architecture with the pre-trained multilingual models - XLM-RoBERTa-large for named entity mention recognition and categorization, and mT5-large for named entity lemmatization and linking.</p>
  </details>
</details>
<details>
  <summary>102. <b>标题：DE-HNN: An effective neural model for Circuit Netlist representation</b></summary>
  <p><b>编号</b>：[434]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00477">https://arxiv.org/abs/2404.00477</a></p>
  <p><b>作者</b>：Zhishang Luo,  Truong Son Hy,  Puoya Tabaghi,  Donghyeon Koh,  Michael Defferrard,  Elahe Rezaei,  Ryan Carey,  Rhett Davis,  Rajeev Jain,  Yusu Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：run-time for optimization, design, chip design, design cycle, optimization tools</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The run-time for optimization tools used in chip design has grown with the complexity of designs to the point where it can take several days to go through one design cycle which has become a bottleneck. Designers want fast tools that can quickly give feedback on a design. Using the input and output data of the tools from past designs, one can attempt to build a machine learning model that predicts the outcome of a design in significantly shorter time than running the tool. The accuracy of such models is affected by the representation of the design data, which is usually a netlist that describes the elements of the digital circuit and how they are connected. Graph representations for the netlist together with graph neural networks have been investigated for such models. However, the characteristics of netlists pose several challenges for existing graph learning frameworks, due to the large number of nodes and the importance of long-range interactions between nodes. To address these challenges, we represent the netlist as a directed hypergraph and propose a Directional Equivariant Hypergraph Neural Network (DE-HNN) for the effective learning of (directed) hypergraphs. Theoretically, we show that our DE-HNN can universally approximate any node or hyperedge based function that satisfies certain permutation equivariant and invariant properties natural for directed hypergraphs. We compare the proposed DE-HNN with several State-of-the-art (SOTA) machine learning models for (hyper)graphs and netlists, and show that the DE-HNN significantly outperforms them in predicting the outcome of optimized place-and-route tools directly from the input netlists. Our source code and the netlists data used are publicly available at this https URL</p>
  </details>
</details>
<details>
  <summary>103. <b>标题：Linguistic Calibration of Language Models</b></summary>
  <p><b>编号</b>：[435]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00474">https://arxiv.org/abs/2404.00474</a></p>
  <p><b>作者</b>：Neil Band,  Xuechen Li,  Tengyu Ma,  Tatsunori Hashimoto</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：suboptimal downstream decisions, confidently hallucinate, Language models, long-form generations, generations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate. This issue can be mitigated by having the LM verbally convey the probability that its claims are correct, but existing models cannot produce text with calibrated confidence statements. Through the lens of decision-making, we formalize linguistic calibration for long-form generations: an LM is linguistically calibrated if its generations enable its users to make calibrated probabilistic predictions. This definition enables a training framework where a supervised finetuning step bootstraps an LM to emit long-form generations with confidence statements such as "I estimate a 30% chance of..." or "I am certain that...", followed by a reinforcement learning step which rewards generations that enable a user to provide calibrated answers to related questions. We linguistically calibrate Llama 2 7B and find in automated and human evaluations of long-form generations that it is significantly more calibrated than strong finetuned factuality baselines with comparable accuracy. These findings generalize under distribution shift on question-answering and under a significant task shift to person biography generation. Our results demonstrate that long-form generations may be calibrated end-to-end by constructing an objective in the space of the predictions that users make in downstream decision-making.</p>
  </details>
</details>
<details>
  <summary>104. <b>标题：Privacy Backdoors: Stealing Data with Corrupted Pretrained Models</b></summary>
  <p><b>编号</b>：[436]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00473">https://arxiv.org/abs/2404.00473</a></p>
  <p><b>作者</b>：Shanglun Feng,  Florian Tramèr</p>
  <p><b>备注</b>：Code at this https URL</p>
  <p><b>关键词</b>：fit specific applications, Practitioners commonly download, commonly download pretrained, Practitioners commonly, specific applications</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Practitioners commonly download pretrained machine learning models from open repositories and finetune them to fit specific applications. We show that this practice introduces a new risk of privacy backdoors. By tampering with a pretrained model's weights, an attacker can fully compromise the privacy of the finetuning data. We show how to build privacy backdoors for a variety of models, including transformers, which enable an attacker to reconstruct individual finetuning samples, with a guaranteed success! We further show that backdoored models allow for tight privacy attacks on models trained with differential privacy (DP). The common optimistic practice of training DP models with loose privacy guarantees is thus insecure if the model is not trusted. Overall, our work highlights a crucial and overlooked supply chain attack on machine learning privacy.</p>
  </details>
</details>
<details>
  <summary>105. <b>标题：Classification of Short Segment Pediatric Heart Sounds Based on a  Transformer-Based Convolutional Neural Network</b></summary>
  <p><b>编号</b>：[437]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00470">https://arxiv.org/abs/2404.00470</a></p>
  <p><b>作者</b>：Md Hassanuzzaman,  Nurul Akhtar Hasan,  Mohammad Abdullah Al Mamun,  Khawza I Ahmed,  Ahsan H Khandoker,  Raqibul Mostafa</p>
  <p><b>备注</b>：16 pages,11 Figures</p>
  <p><b>关键词</b>：Congenital anomalies arising, congenital heart diseases, heart sound, heart, Congenital anomalies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Congenital anomalies arising as a result of a defect in the structure of the heart and great vessels are known as congenital heart diseases or CHDs. A PCG can provide essential details about the mechanical conduction system of the heart and point out specific patterns linked to different kinds of CHD. This study aims to investigate the minimum signal duration required for the automatic classification of heart sounds. This study also investigated the optimum signal quality assessment indicator (Root Mean Square of Successive Differences) RMSSD and (Zero Crossings Rate) ZCR value. Mel-frequency cepstral coefficients (MFCCs) based feature is used as an input to build a Transformer-Based residual one-dimensional convolutional neural network, which is then used for classifying the heart sound. The study showed that 0.4 is the ideal threshold for getting suitable signals for the RMSSD and ZCR indicators. Moreover, a minimum signal length of 5s is required for effective heart sound classification. It also shows that a shorter signal (3 s heart sound) does not have enough information to categorize heart sounds accurately, and the longer signal (15 s heart sound) may contain more noise. The best accuracy, 93.69%, is obtained for the 5s signal to distinguish the heart sound.</p>
  </details>
</details>
<details>
  <summary>106. <b>标题：Computation and Communication Efficient Lightweighting Vertical  Federated Learning</b></summary>
  <p><b>编号</b>：[439]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00466">https://arxiv.org/abs/2404.00466</a></p>
  <p><b>作者</b>：Heqiang Wang,  Jieming Bian,  Lei Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Vertical Federated Learning, Lightweight Vertical Federated, Federated Learning, field of study, Vertical Federated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The exploration of computational and communication efficiency within Federated Learning (FL) has emerged as a prominent and crucial field of study. While most existing efforts to enhance these efficiencies have focused on Horizontal FL, the distinct processes and model structures of Vertical FL preclude the direct application of Horizontal FL-based techniques. In response, we introduce the concept of Lightweight Vertical Federated Learning (LVFL), targeting both computational and communication efficiencies. This approach involves separate lightweighting strategies for the feature model, to improve computational efficiency, and for feature embedding, to enhance communication efficiency. Moreover, we establish a convergence bound for our LVFL algorithm, which accounts for both communication and computational lightweighting ratios. Our evaluation of the algorithm on a image classification dataset reveals that LVFL significantly alleviates computational and communication demands while preserving robust learning performance. This work effectively addresses the gaps in communication and computational efficiency within Vertical FL.</p>
  </details>
</details>
<details>
  <summary>107. <b>标题：Leveraging Pre-trained and Transformer-derived Embeddings from EHRs to  Characterize Heterogeneity Across Alzheimer's Disease and Related Dementias</b></summary>
  <p><b>编号</b>：[440]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00464">https://arxiv.org/abs/2404.00464</a></p>
  <p><b>作者</b>：Matthew West,  Colin Magdamo,  Lily Cheng,  Yingnan He,  Sudeshna Das</p>
  <p><b>备注</b>：14 pages, 5 figures in main text</p>
  <p><b>关键词</b>：million people globally, debilitating neurodegenerative disease, debilitating neurodegenerative, million people, people globally</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Alzheimer's disease is a progressive, debilitating neurodegenerative disease that affects 50 million people globally. Despite this substantial health burden, available treatments for the disease are limited and its fundamental causes remain poorly understood. Previous work has suggested the existence of clinically-meaningful sub-types, which it is suggested may correspond to distinct etiologies, disease courses, and ultimately appropriate treatments. Here, we use unsupervised learning techniques on electronic health records (EHRs) from a cohort of memory disorder patients to characterise heterogeneity in this disease population. Pre-trained embeddings for medical codes as well as transformer-derived Clinical BERT embeddings of free text are used to encode patient EHRs. We identify the existence of sub-populations on the basis of comorbidities and shared textual features, and discuss their clinical significance.</p>
  </details>
</details>
<details>
  <summary>108. <b>标题：Addressing Both Statistical and Causal Gender Fairness in NLP Models</b></summary>
  <p><b>编号</b>：[441]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00463">https://arxiv.org/abs/2404.00463</a></p>
  <p><b>作者</b>：Hannah Chen,  Yangfeng Ji,  David Evans</p>
  <p><b>备注</b>：NAACL 2024 (Findings)</p>
  <p><b>关键词</b>：stipulates equivalent outcomes, fairness stipulates equivalent, protected group, protected characteristics, causal fairness prescribes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Statistical fairness stipulates equivalent outcomes for every protected group, whereas causal fairness prescribes that a model makes the same prediction for an individual regardless of their protected characteristics. Counterfactual data augmentation (CDA) is effective for reducing bias in NLP models, yet models trained with CDA are often evaluated only on metrics that are closely tied to the causal fairness notion; similarly, sampling-based methods designed to promote statistical fairness are rarely evaluated for causal fairness. In this work, we evaluate both statistical and causal debiasing methods for gender bias in NLP models, and find that while such methods are effective at reducing bias as measured by the targeted metric, they do not necessarily improve results on other bias metrics. We demonstrate that combinations of statistical and causal debiasing techniques are able to reduce bias measured through both types of metrics.</p>
  </details>
</details>
<details>
  <summary>109. <b>标题：Zero-shot Safety Prediction for Autonomous Robots with Foundation World  Models</b></summary>
  <p><b>编号</b>：[442]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00462">https://arxiv.org/abs/2404.00462</a></p>
  <p><b>作者</b>：Zhenjiang Mao,  Siqi Dai,  Yuang Geng,  Ivan Ruchkin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：internal dynamic model, train a controller, world model creates, surrogate dynamics, world models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A world model creates a surrogate world to train a controller and predict safety violations by learning the internal dynamic model of systems. However, the existing world models rely solely on statistical learning of how observations change in response to actions, lacking precise quantification of how accurate the surrogate dynamics are, which poses a significant challenge in safety-critical systems. To address this challenge, we propose foundation world models that embed observations into meaningful and causally latent representations. This enables the surrogate dynamics to directly predict causal future states by leveraging a training-free large language model. In two common benchmarks, this novel model outperforms standard world models in the safety prediction task and has a performance comparable to supervised learning despite not using any data. We evaluate its performance with a more specialized and system-relevant metric by comparing estimated states instead of aggregating observation-wide error.</p>
  </details>
</details>
<details>
  <summary>110. <b>标题：Shortcuts Arising from Contrast: Effective and Covert Clean-Label  Attacks in Prompt-Based Learning</b></summary>
  <p><b>编号</b>：[443]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00461">https://arxiv.org/abs/2404.00461</a></p>
  <p><b>作者</b>：Xiaopeng Xie,  Ming Yan,  Xiwen Zhou,  Chenlong Zhao,  Suli Wang,  Yong Zhang,  Joey Tianyi Zhou</p>
  <p><b>备注</b>：10 pages, 6 figures, conference</p>
  <p><b>关键词</b>：pretrained language models, demonstrated remarkable efficacy, Prompt-based learning paradigm, learning paradigm, language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Prompt-based learning paradigm has demonstrated remarkable efficacy in enhancing the adaptability of pretrained language models (PLMs), particularly in few-shot scenarios. However, this learning paradigm has been shown to be vulnerable to backdoor attacks. The current clean-label attack, employing a specific prompt as a trigger, can achieve success without the need for external triggers and ensure correct labeling of poisoned samples, which is more stealthy compared to the poisoned-label attack, but on the other hand, it faces significant issues with false activations and poses greater challenges, necessitating a higher rate of poisoning. Using conventional negative data augmentation methods, we discovered that it is challenging to trade off between effectiveness and stealthiness in a clean-label setting. In addressing this issue, we are inspired by the notion that a backdoor acts as a shortcut and posit that this shortcut stems from the contrast between the trigger and the data utilized for poisoning. In this study, we propose a method named Contrastive Shortcut Injection (CSI), by leveraging activation values, integrates trigger design and data selection strategies to craft stronger shortcut features. With extensive experiments on full-shot and few-shot text classification tasks, we empirically validate CSI's high effectiveness and high stealthiness at low poisoning rates. Notably, we found that the two approaches play leading roles in full-shot and few-shot settings, respectively.</p>
  </details>
</details>
<details>
  <summary>111. <b>标题：QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs</b></summary>
  <p><b>编号</b>：[447]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00456">https://arxiv.org/abs/2404.00456</a></p>
  <p><b>作者</b>：Saleh Ashkboos,  Amirkeivan Mohtashami,  Maximilian L. Croci,  Bo Li,  Martin Jaggi,  Dan Alistarh,  Torsten Hoefler,  James Hensman</p>
  <p><b>备注</b>：19 pages, 6 figures</p>
  <p><b>关键词</b>：Quantization scheme based, based on Rotations, including all weights, QuaRot rotates LLMs, making quantization easier</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce QuaRot, a new Quantization scheme based on Rotations, which is able to quantize LLMs end-to-end, including all weights, activations, and KV cache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the hidden state without changing the output, making quantization easier. This computational invariance is applied to the hidden state (residual) of the LLM, as well as to the activations of the feed-forward components, aspects of the attention mechanism and to the KV cache. The result is a quantized model where all matrix multiplications are performed in 4-bits, without any channels identified for retention in higher precision. Our quantized LLaMa2-70B model has losses of at most 0.29 WikiText-2 perplexity and retains 99% of the zero-shot performance. Code is available at: this https URL.</p>
  </details>
</details>
<details>
  <summary>112. <b>标题：Communication Efficient Distributed Training with Distributed Lion</b></summary>
  <p><b>编号</b>：[456]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00438">https://arxiv.org/abs/2404.00438</a></p>
  <p><b>作者</b>：Bo Liu,  Lemeng Wu,  Lizhang Chen,  Kaizhao Liang,  Jiaxu Zhu,  Chen Liang,  Raghuraman Krishnamoorthi,  Qiang Liu</p>
  <p><b>备注</b>：22 pages</p>
  <p><b>关键词</b>：Distributed Lion, Lion, Distributed, advantages on memory, sample efficiency</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The Lion optimizer has been a promising competitor with the AdamW for training large AI models, with advantages on memory, computation, and sample efficiency. In this paper, we introduce Distributed Lion, an innovative adaptation of Lion for distributed training environments. Leveraging the sign operator in Lion, our Distributed Lion only requires communicating binary or lower-precision vectors between workers to the center server, significantly reducing the communication cost. Our theoretical analysis confirms Distributed Lion's convergence properties. Empirical results demonstrate its robustness across a range of tasks, worker counts, and batch sizes, on both vision and language problems. Notably, Distributed Lion attains comparable performance to standard Lion or AdamW optimizers applied on aggregated gradients, but with significantly reduced communication bandwidth. This feature is particularly advantageous for training large models. In addition, we also demonstrate that Distributed Lion presents a more favorable performance-bandwidth balance compared to existing efficient distributed methods such as deep gradient compression and ternary gradients.</p>
  </details>
</details>
<details>
  <summary>113. <b>标题：Automatic explanation of the classification of Spanish legal judgments  in jurisdiction-dependent law categories with tree estimators</b></summary>
  <p><b>编号</b>：[457]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00437">https://arxiv.org/abs/2404.00437</a></p>
  <p><b>作者</b>：Jaime González-González,  Francisco de Arriba-Pérez,  Silvia García-Méndez,  Andrea Busto-Castiñeira,  Francisco J. González-Castaño</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：address knowledge extraction, detect their aspects, literature to address, extraction from judgments, judgments and detect</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automatic legal text classification systems have been proposed in the literature to address knowledge extraction from judgments and detect their aspects. However, most of these systems are black boxes even when their models are interpretable. This may raise concerns about their trustworthiness. Accordingly, this work contributes with a system combining Natural Language Processing (NLP) with Machine Learning (ML) to classify legal texts in an explainable manner. We analyze the features involved in the decision and the threshold bifurcation values of the decision paths of tree structures and present this information to the users in natural language. This is the first work on automatic analysis of legal texts combining NLP and ML along with Explainable Artificial Intelligence techniques to automatically make the models' decisions understandable to end users. Furthermore, legal experts have validated our solution, and this knowledge has also been incorporated into the explanation process as "expert-in-the-loop" dictionaries. Experimental results on an annotated data set in law categories by jurisdiction demonstrate that our system yields competitive classification performance, with accuracy values well above 90%, and that its automatic explanations are easily understandable even to non-expert users.</p>
  </details>
</details>
<details>
  <summary>114. <b>标题：Visualizing Routes with AI-Discovered Street-View Patterns</b></summary>
  <p><b>编号</b>：[459]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00431">https://arxiv.org/abs/2404.00431</a></p>
  <p><b>作者</b>：Tsung Heng Wu,  Md Amiruzzaman,  Ye Zhao,  Deepshikha Bhati,  Jing Yang</p>
  <p><b>备注</b>：12 pages, 10 figures, and 3 tables</p>
  <p><b>关键词</b>：studying social systems, social systems, built environment, economic factors, studying social</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Street-level visual appearances play an important role in studying social systems, such as understanding the built environment, driving routes, and associated social and economic factors. It has not been integrated into a typical geographical visualization interface (e.g., map services) for planning driving routes. In this paper, we study this new visualization task with several new contributions. First, we experiment with a set of AI techniques and propose a solution of using semantic latent vectors for quantifying visual appearance features. Second, we calculate image similarities among a large set of street-view images and then discover spatial imagery patterns. Third, we integrate these discovered patterns into driving route planners with new visualization techniques. Finally, we present VivaRoutes, an interactive visualization prototype, to show how visualizations leveraged with these discovered patterns can help users effectively and interactively explore multiple routes. Furthermore, we conducted a user study to assess the usefulness and utility of VivaRoutes.</p>
  </details>
</details>
<details>
  <summary>115. <b>标题：Learning Service Selection Decision Making Behaviors During Scientific  Workflow Development</b></summary>
  <p><b>编号</b>：[464]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00420">https://arxiv.org/abs/2404.00420</a></p>
  <p><b>作者</b>：Xihao Xie,  Jia Zhang,  Rahul Ramachandran,  Tsengdar J. Lee,  Seungwon Lee</p>
  <p><b>备注</b>：14 pages, 8 figures. arXiv admin note: text overlap with arXiv:2205.11771</p>
  <p><b>关键词</b>：big challenge, scientific workflow, workflow, Internet, service</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Increasingly, more software services have been published onto the Internet, making it a big challenge to recommend services in the process of a scientific workflow composition. In this paper, a novel context-aware approach is proposed to recommending next services in a workflow development process, through learning service representation and service selection decision making behaviors from workflow provenance. Inspired by natural language sentence generation, the composition process of a scientific workflow is formalized as a step-wise procedure within the context of the goal of workflow, and the problem of next service recommendation is mapped to next word prediction. Historical service dependencies are first extracted from scientific workflow provenance to build a knowledge graph. Service sequences are then generated based on diverse composition path generation strategies. Afterwards, the generated corpus of composition paths are leveraged to study previous decision making strategies. Such a trained goal-oriented next service prediction model will be used to recommend top K candidate services during workflow composition process. Extensive experiments on a real-word repository have demonstrated the effectiveness of this approach.</p>
  </details>
</details>
<details>
  <summary>116. <b>标题：Continual Learning for Autonomous Robots: A Prototype-based Approach</b></summary>
  <p><b>编号</b>：[466]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00418">https://arxiv.org/abs/2404.00418</a></p>
  <p><b>作者</b>：Elvin Hajizada,  Balachandran Swaminathan,  Yulia Sandamirskaya</p>
  <p><b>备注</b>：Submitted to IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</p>
  <p><b>关键词</b>：Humans and animals, CLP, lives from limited, limited amounts, amounts of sensed</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Humans and animals learn throughout their lives from limited amounts of sensed data, both with and without supervision. Autonomous, intelligent robots of the future are often expected to do the same. The existing continual learning (CL) methods are usually not directly applicable to robotic settings: they typically require buffering and a balanced replay of training data. A few-shot online continual learning (FS-OCL) setting has been proposed to address more realistic scenarios where robots must learn from a non-repeated sparse data stream. To enable truly autonomous life-long learning, an additional challenge of detecting novelties and learning new items without supervision needs to be addressed. We address this challenge with our new prototype-based approach called Continually Learning Prototypes (CLP). In addition to being capable of FS-OCL learning, CLP also detects novel objects and learns them without supervision. To mitigate forgetting, CLP utilizes a novel metaplasticity mechanism that adapts the learning rate individually per prototype. CLP is rehearsal-free, hence does not require a memory buffer, and is compatible with neuromorphic hardware, characterized by ultra-low power consumption, real-time processing abilities, and on-chip learning. Indeed, we have open-sourced a simple version of CLP in the neuromorphic software framework Lava, targetting Intel's neuromorphic chip Loihi 2. We evaluate CLP on a robotic vision dataset, OpenLORIS. In a low-instance FS-OCL scenario, CLP shows state-of-the-art results. In the open world, CLP detects novelties with superior precision and recall and learns features of the detected novel classes without supervision, achieving a strong baseline of 99% base class and 65%/76% (5-shot/10-shot) novel class accuracy.</p>
  </details>
</details>
<details>
  <summary>117. <b>标题：Orchestrate Latent Expertise: Advancing Online Continual Learning with  Multi-Level Supervision and Reverse Self-Distillation</b></summary>
  <p><b>编号</b>：[467]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00417">https://arxiv.org/abs/2404.00417</a></p>
  <p><b>作者</b>：HongWei Yan,  Liyuan Wang,  Kaisheng Ma,  Yi Zhong</p>
  <p><b>备注</b>：CVPR 2024</p>
  <p><b>关键词</b>：accommodate real-world dynamics, artificial intelligence systems, sequentially arriving content, Online Continual Learning, real-world dynamics</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To accommodate real-world dynamics, artificial intelligence systems need to cope with sequentially arriving content in an online manner. Beyond regular Continual Learning (CL) attempting to address catastrophic forgetting with offline training of each task, Online Continual Learning (OCL) is a more challenging yet realistic setting that performs CL in a one-pass data stream. Current OCL methods primarily rely on memory replay of old training samples. However, a notable gap from CL to OCL stems from the additional overfitting-underfitting dilemma associated with the use of rehearsal buffers: the inadequate learning of new training samples (underfitting) and the repeated learning of a few old training samples (overfitting). To this end, we introduce a novel approach, Multi-level Online Sequential Experts (MOSE), which cultivates the model as stacked sub-experts, integrating multi-level supervision and reverse self-distillation. Supervision signals across multiple stages facilitate appropriate convergence of the new task while gathering various strengths from experts by knowledge distillation mitigates the performance decline of old tasks. MOSE demonstrates remarkable efficacy in learning new samples and preserving past knowledge through multi-level experts, thereby significantly advancing OCL performance over state-of-the-art baselines (e.g., up to 7.3% on Split CIFAR-100 and 6.1% on Split Tiny-ImageNet).</p>
  </details>
</details>
<details>
  <summary>118. <b>标题：SVGCraft: Beyond Single Object Text-to-SVG Synthesis with Comprehensive  Canvas Layout</b></summary>
  <p><b>编号</b>：[471]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00412">https://arxiv.org/abs/2404.00412</a></p>
  <p><b>作者</b>：Ayan Banerjee,  Nityanand Mathur,  Josep Lladós,  Umapada Pal,  Anjan Dutta</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：challenging vision task, Generating VectorArt, vision task, requiring diverse, unseen entities</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generating VectorArt from text prompts is a challenging vision task, requiring diverse yet realistic depictions of the seen as well as unseen entities. However, existing research has been mostly limited to the generation of single objects, rather than comprehensive scenes comprising multiple elements. In response, this work introduces SVGCraft, a novel end-to-end framework for the creation of vector graphics depicting entire scenes from textual descriptions. Utilizing a pre-trained LLM for layout generation from text prompts, this framework introduces a technique for producing masked latents in specified bounding boxes for accurate object placement. It introduces a fusion mechanism for integrating attention maps and employs a diffusion U-Net for coherent composition, speeding up the drawing process. The resulting SVG is optimized using a pre-trained encoder and LPIPS loss with opacity modulation to maximize similarity. Additionally, this work explores the potential of primitive shapes in facilitating canvas completion in constrained environments. Through both qualitative and quantitative assessments, SVGCraft is demonstrated to surpass prior works in abstraction, recognizability, and detail, as evidenced by its performance metrics (CLIP-T: 0.4563, Cosine Similarity: 0.6342, Confusion: 0.66, Aesthetic: 6.7832). The code will be available at this https URL.</p>
  </details>
</details>
<details>
  <summary>119. <b>标题：Deep Learning with Parametric Lenses</b></summary>
  <p><b>编号</b>：[473]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00408">https://arxiv.org/abs/2404.00408</a></p>
  <p><b>作者</b>：Geoffrey S. H. Cruttwell,  Bruno Gavranovic,  Neil Ghani,  Paul Wilson,  Fabio Zanasi</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2403.13001</p>
  <p><b>关键词</b>：reverse derivative categories, machine learning algorithms, terms of lenses, propose a categorical, categorical semantics</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a categorical semantics for machine learning algorithms in terms of lenses, parametric maps, and reverse derivative categories. This foundation provides a powerful explanatory and unifying framework: it encompasses a variety of gradient descent algorithms such as ADAM, AdaGrad, and Nesterov momentum, as well as a variety of loss functions such as MSE and Softmax cross-entropy, and different architectures, shedding new light on their similarities and differences. Furthermore, our approach to learning has examples generalising beyond the familiar continuous domains (modelled in categories of smooth maps) and can be realised in the discrete setting of Boolean and polynomial circuits. We demonstrate the practical significance of our framework with an implementation in Python.</p>
  </details>
</details>
<details>
  <summary>120. <b>标题：Aurora-M: The First Open Source Multilingual Language Model Red-teamed  according to the U.S. Executive Order</b></summary>
  <p><b>编号</b>：[479]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00399">https://arxiv.org/abs/2404.00399</a></p>
  <p><b>作者</b>：Taishi Nakamura,  Mayank Mishra,  Simone Tedeschi,  Yekun Chai,  Jason T Stillerman,  Felix Friedrich,  Prateek Yadav,  Tanmay Laud,  Vu Minh Chien,  Terry Yue Zhuo,  Diganta Misra,  Ben Bogin,  Xuan-Son Vu,  Marzena Karpinska,  Arnav Varma Dantuluri,  Wojciech Kusa,  Tommaso Furlanello,  Rio Yokota,  Niklas Muennighoff,  Suhas Pai,  Tosin Adewumi,  Veronika Laippala,  Xiaozhe Yao,  Adalberto Junior,  Alpay Ariyak,  Aleksandr Drozd,  Jordan Clive,  Kshitij Gupta,  Liangyu Chen,  Qi Sun,  Ken Tsui,  Noah Persaud,  Nour Fahmy,  Tianlong Chen,  Mohit Bansal,  Nicolo Monti,  Tai Dang,  Ziyang Luo,  Tien-Tung Bui,  Roberto Navigli,  Virendra Mehta,  Matthew Blumberg,  Victor May,  Huu Nguyen,  Sampo Pyysalo</p>
  <p><b>备注</b>：Preprint</p>
  <p><b>关键词</b>：high computational cost, training limits accessibility, limits accessibility, high computational, computational cost</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pretrained language models underpin several AI applications, but their high computational cost for training limits accessibility. Initiatives such as BLOOM and StarCoder aim to democratize access to pretrained models for collaborative community development. However, such existing models face challenges: limited multilingual capabilities, continual pretraining causing catastrophic forgetting, whereas pretraining from scratch is computationally expensive, and compliance with AI safety and development laws. This paper presents Aurora-M, a 15B parameter multilingual open-source model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually pretrained from StarCoderPlus on 435 billion additional tokens, Aurora-M surpasses 2 trillion tokens in total training token count. It is the first open-source multilingual model fine-tuned on human-reviewed safety instructions, thus aligning its development not only with conventional red-teaming considerations, but also with the specific concerns articulated in the Biden-Harris Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. Aurora-M is rigorously evaluated across various tasks and languages, demonstrating robustness against catastrophic forgetting and outperforming alternatives in multilingual settings, particularly in safety evaluations. To promote responsible open-source LLM development, Aurora-M and its variants are released at this https URL .</p>
  </details>
</details>
<details>
  <summary>121. <b>标题：Constrained Layout Generation with Factor Graphs</b></summary>
  <p><b>编号</b>：[487]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00385">https://arxiv.org/abs/2404.00385</a></p>
  <p><b>作者</b>：Mohammed Haroon Dupty,  Yanfei Dong,  Sicong Leng,  Guoji Fu,  Yong Liang Goh,  Wei Lu,  Wee Sun Lee</p>
  <p><b>备注</b>：To be published at IEEE/CVF CVPR 2024</p>
  <p><b>关键词</b>：multiple domains including, domains including floorplan, object-centric layout generation, design process, including floorplan design</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper addresses the challenge of object-centric layout generation under spatial constraints, seen in multiple domains including floorplan design process. The design process typically involves specifying a set of spatial constraints that include object attributes like size and inter-object relations such as relative positioning. Existing works, which typically represent objects as single nodes, lack the granularity to accurately model complex interactions between objects. For instance, often only certain parts of an object, like a room's right wall, interact with adjacent objects. To address this gap, we introduce a factor graph based approach with four latent variable nodes for each room, and a factor node for each constraint. The factor nodes represent dependencies among the variables to which they are connected, effectively capturing constraints that are potentially of a higher order. We then develop message-passing on the bipartite graph, forming a factor graph neural network that is trained to produce a floorplan that aligns with the desired requirements. Our approach is simple and generates layouts faithful to the user requirements, demonstrated by a large improvement in IOU scores over existing methods. Additionally, our approach, being inferential and accurate, is well-suited to the practical human-in-the-loop design process where specifications evolve iteratively, offering a practical and powerful tool for AI-guided design.</p>
  </details>
</details>
<details>
  <summary>122. <b>标题：From Learning to Analytics: Improving Model Efficacy with Goal-Directed  Client Selection</b></summary>
  <p><b>编号</b>：[493]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00371">https://arxiv.org/abs/2404.00371</a></p>
  <p><b>作者</b>：Jingwen Tong,  Zhenzhen Chen,  Liqun Fu,  Jun Zhang,  Zhu Han</p>
  <p><b>备注</b>：This work was partly presented at IEEE ICC 2022</p>
  <p><b>关键词</b>：preserving data privacy, global model, model analytics framework, well-trained global model, appealing paradigm</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Federated learning (FL) is an appealing paradigm for learning a global model among distributed clients while preserving data privacy. Driven by the demand for high-quality user experiences, evaluating the well-trained global model after the FL process is crucial. In this paper, we propose a closed-loop model analytics framework that allows for effective evaluation of the trained global model using clients' local data. To address the challenges posed by system and data heterogeneities in the FL process, we study a goal-directed client selection problem based on the model analytics framework by selecting a subset of clients for the model training. This problem is formulated as a stochastic multi-armed bandit (SMAB) problem. We first put forth a quick initial upper confidence bound (Quick-Init UCB) algorithm to solve this SMAB problem under the federated analytics (FA) framework. Then, we further propose a belief propagation-based UCB (BP-UCB) algorithm under the democratized analytics (DA) framework. Moreover, we derive two regret upper bounds for the proposed algorithms, which increase logarithmically over the time horizon. The numerical results demonstrate that the proposed algorithms achieve nearly optimal performance, with a gap of less than 1.44% and 3.12% under the FA and DA frameworks, respectively.</p>
  </details>
</details>
<details>
  <summary>123. <b>标题：Revisiting Random Weight Perturbation for Efficiently Improving  Generalization</b></summary>
  <p><b>编号</b>：[504]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00357">https://arxiv.org/abs/2404.00357</a></p>
  <p><b>作者</b>：Tao Li,  Qinghua Tao,  Weihao Yan,  Zehao Lei,  Yingwen Wu,  Kun Fang,  Mingzhen He,  Xiaolin Huang</p>
  <p><b>备注</b>：Accepted to TMLR 2024</p>
  <p><b>关键词</b>：deep neural networks, modern deep neural, neural networks, machine learning, ability of modern</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Improving the generalization ability of modern deep neural networks (DNNs) is a fundamental challenge in machine learning. Two branches of methods have been proposed to seek flat minima and improve generalization: one led by sharpness-aware minimization (SAM) minimizes the worst-case neighborhood loss through adversarial weight perturbation (AWP), and the other minimizes the expected Bayes objective with random weight perturbation (RWP). While RWP offers advantages in computation and is closely linked to AWP on a mathematical basis, its empirical performance has consistently lagged behind that of AWP. In this paper, we revisit the use of RWP for improving generalization and propose improvements from two perspectives: i) the trade-off between generalization and convergence and ii) the random perturbation generation. Through extensive experimental evaluations, we demonstrate that our enhanced RWP methods achieve greater efficiency in enhancing generalization, particularly in large-scale problems, while also offering comparable or even superior performance to SAM. The code is released at this https URL.</p>
  </details>
</details>
<details>
  <summary>124. <b>标题：CLIP-driven Outliers Synthesis for few-shot OOD detection</b></summary>
  <p><b>编号</b>：[522]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00323">https://arxiv.org/abs/2404.00323</a></p>
  <p><b>作者</b>：Hao Sun,  Rundong He,  Zhongyi Han,  Zhicong Lin,  Yongshun Gong,  Yilong Yin</p>
  <p><b>备注</b>：9 pages,5 figures</p>
  <p><b>关键词</b>：OOD, OOD detection focuses, focuses on recognizing, unseen during training, small number</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Few-shot OOD detection focuses on recognizing out-of-distribution (OOD) images that belong to classes unseen during training, with the use of only a small number of labeled in-distribution (ID) images. Up to now, a mainstream strategy is based on large-scale vision-language models, such as CLIP. However, these methods overlook a crucial issue: the lack of reliable OOD supervision information, which can lead to biased boundaries between in-distribution (ID) and OOD. To tackle this problem, we propose CLIP-driven Outliers Synthesis~(CLIP-OS). Firstly, CLIP-OS enhances patch-level features' perception by newly proposed patch uniform convolution, and adaptively obtains the proportion of ID-relevant information by employing CLIP-surgery-discrepancy, thus achieving separation between ID-relevant and ID-irrelevant. Next, CLIP-OS synthesizes reliable OOD data by mixing up ID-relevant features from different classes to provide OOD supervision information. Afterward, CLIP-OS leverages synthetic OOD samples by unknown-aware prompt learning to enhance the separability of ID and OOD. Extensive experiments across multiple benchmarks demonstrate that CLIP-OS achieves superior few-shot OOD detection capability.</p>
  </details>
</details>
<details>
  <summary>125. <b>标题：TRABSA: Interpretable Sentiment Analysis of Tweets using Attention-based  BiLSTM and Twitter-RoBERTa</b></summary>
  <p><b>编号</b>：[536]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00297">https://arxiv.org/abs/2404.00297</a></p>
  <p><b>作者</b>：Md Abrar Jahin,  Md Sakib Hossain Shovon,  M. F. Mridha</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：understanding public opinion, consumer behavior, crucial for understanding, understanding public, public opinion</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sentiment analysis is crucial for understanding public opinion and consumer behavior. Existing models face challenges with linguistic diversity, generalizability, and explainability. We propose TRABSA, a hybrid framework integrating transformer-based architectures, attention mechanisms, and BiLSTM networks to address this. Leveraging RoBERTa-trained on 124M tweets, we bridge gaps in sentiment analysis benchmarks, ensuring state-of-the-art accuracy. Augmenting datasets with tweets from 32 countries and US states, we compare six word-embedding techniques and three lexicon-based labeling techniques, selecting the best for optimal sentiment analysis. TRABSA outperforms traditional ML and deep learning models with 94% accuracy and significant precision, recall, and F1-score gains. Evaluation across diverse datasets demonstrates consistent superiority and generalizability. SHAP and LIME analyses enhance interpretability, improving confidence in predictions. Our study facilitates pandemic resource management, aiding resource planning, policy formation, and vaccination tactics.</p>
  </details>
</details>
<details>
  <summary>126. <b>标题：Survey on Large Language Model-Enhanced Reinforcement Learning: Concept,  Taxonomy, and Methods</b></summary>
  <p><b>编号</b>：[542]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00282">https://arxiv.org/abs/2404.00282</a></p>
  <p><b>作者</b>：Yuji Cao,  Huan Zhao,  Yuheng Cheng,  Ting Shu,  Guolong Liu,  Gaoqi Liang,  Junhua Zhao,  Yun Li</p>
  <p><b>备注</b>：16 pages (including bibliography), 6 figures</p>
  <p><b>关键词</b>：augment reinforcement learning, large language models, high-level general capabilities, extensive pre-trained knowledge, reinforcement learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With extensive pre-trained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects such as multi-task learning, sample efficiency, and task planning. In this survey, we provide a comprehensive review of the existing literature in $\textit{LLM-enhanced RL}$ and summarize its characteristics compared to conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs' functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. Additionally, for each role, we summarize the methodologies, analyze the specific RL challenges that are mitigated, and provide insights into future directions. Lastly, potential applications, prospective opportunities and challenges of the $\textit{LLM-enhanced RL}$ are discussed.</p>
  </details>
</details>
<details>
  <summary>127. <b>标题：TG-NAS: Leveraging Zero-Cost Proxies with Transformer and Graph  Convolution Networks for Efficient Neural Architecture Search</b></summary>
  <p><b>编号</b>：[546]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00271">https://arxiv.org/abs/2404.00271</a></p>
  <p><b>作者</b>：Ye Qiao,  Haocheng Xu,  Sitao Huang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：CNN, discovering new convolutional, search, architecture, NAS</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural architecture search (NAS) is an effective method for discovering new convolutional neural network (CNN) architectures. However, existing approaches often require time-consuming training or intensive sampling and evaluations. Zero-shot NAS aims to create training-free proxies for architecture performance prediction. However, existing proxies have suboptimal performance, and are often outperformed by simple metrics such as model parameter counts or the number of floating-point operations. Besides, existing model-based proxies cannot be generalized to new search spaces with unseen new types of operators without golden accuracy truth. A universally optimal proxy remains elusive. We introduce TG-NAS, a novel model-based universal proxy that leverages a transformer-based operator embedding generator and a graph convolution network (GCN) to predict architecture performance. This approach guides neural architecture search across any given search space without the need of retraining. Distinct from other model-based predictor subroutines, TG-NAS itself acts as a zero-cost (ZC) proxy, guiding architecture search with advantages in terms of data independence, cost-effectiveness, and consistency across diverse search spaces. Our experiments showcase its advantages over existing proxies across various NAS benchmarks, suggesting its potential as a foundational element for efficient architecture search. TG-NAS achieves up to 300X improvements in search efficiency compared to previous SOTA ZC proxy methods. Notably, it discovers competitive models with 93.75% CIFAR-10 accuracy on the NAS-Bench-201 space and 74.5% ImageNet top-1 accuracy on the DARTS space.</p>
  </details>
</details>
<details>
  <summary>128. <b>标题：DiLM: Distilling Dataset into Language Model for Text-level Dataset  Distillation</b></summary>
  <p><b>编号</b>：[552]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00264">https://arxiv.org/abs/2404.00264</a></p>
  <p><b>作者</b>：Aru Maekawa,  Satoshi Kosugi,  Kotaro Funakoshi,  Manabu Okumura</p>
  <p><b>备注</b>：Accepted by Findings of NAACL 2024</p>
  <p><b>关键词</b>：neural networks trained, networks trained, Dataset distillation aims, aims to compress, creating a small</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Dataset distillation aims to compress a training dataset by creating a small number of informative synthetic samples such that neural networks trained on them perform as well as those trained on the original training dataset. Current text dataset distillation methods create each synthetic sample as a sequence of word embeddings instead of a text to apply gradient-based optimization; however, such embedding-level distilled datasets cannot be used for training other models whose word embedding weights are different from the model used for distillation. To address this issue, we propose a novel text dataset distillation approach, called Distilling dataset into Language Model (DiLM), which trains a language model to generate informative synthetic training samples as text data, instead of directly optimizing synthetic samples. We evaluated DiLM on various text classification datasets and showed that distilled synthetic datasets from DiLM outperform those from current coreset selection methods. DiLM achieved remarkable generalization performance in training different types of models and in-context learning of large language models. Our code will be available at this https URL.</p>
  </details>
</details>
<details>
  <summary>129. <b>标题：YOLOOC: YOLO-based Open-Class Incremental Object Detection with Novel  Class Discovery</b></summary>
  <p><b>编号</b>：[556]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00257">https://arxiv.org/abs/2404.00257</a></p>
  <p><b>作者</b>：Qian Wan,  Xiang Xiang,  Qinhao Zhou</p>
  <p><b>备注</b>：Initially submitted to ACCV 2022</p>
  <p><b>关键词</b>：open-world object detection, open-world object, attention recently, lot of attention, classes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Because of its use in practice, open-world object detection (OWOD) has gotten a lot of attention recently. The challenge is how can a model detect novel classes and then incrementally learn them without forgetting previously known classes. Previous approaches hinge on strongly-supervised or weakly-supervised novel-class data for novel-class detection, which may not apply to real applications. We construct a new benchmark that novel classes are only encountered at the inference stage. And we propose a new OWOD detector YOLOOC, based on the YOLO architecture yet for the Open-Class setup. We introduce label smoothing to prevent the detector from over-confidently mapping novel classes to known classes and to discover novel classes. Extensive experiments conducted on our more realistic setup demonstrate the effectiveness of our method for discovering novel classes in our new benchmark.</p>
  </details>
</details>
<details>
  <summary>130. <b>标题：Clustering for Protein Representation Learning</b></summary>
  <p><b>编号</b>：[558]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00254">https://arxiv.org/abs/2404.00254</a></p>
  <p><b>作者</b>：Ruijie Quan,  Wenguan Wang,  Fan Ma,  Hehe Fan,  Yi Yang</p>
  <p><b>备注</b>：Accepted to CVPR2024</p>
  <p><b>关键词</b>：amino acid sequences, amino acid, aims to capture, acid sequences, Protein representation learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Protein representation learning is a challenging task that aims to capture the structure and function of proteins from their amino acid sequences. Previous methods largely ignored the fact that not all amino acids are equally important for protein folding and activity. In this article, we propose a neural clustering framework that can automatically discover the critical components of a protein by considering both its primary and tertiary structure information. Our framework treats a protein as a graph, where each node represents an amino acid and each edge represents a spatial or sequential connection between amino acids. We then apply an iterative clustering strategy to group the nodes into clusters based on their 1D and 3D positions and assign scores to each cluster. We select the highest-scoring clusters and use their medoid nodes for the next iteration of clustering, until we obtain a hierarchical and informative representation of the protein. We evaluate on four protein-related tasks: protein fold classification, enzyme reaction classification, gene ontology term prediction, and enzyme commission number prediction. Experimental results demonstrate that our method achieves state-of-the-art performance.</p>
  </details>
</details>
<details>
  <summary>131. <b>标题：Facilitating Reinforcement Learning for Process Control Using Transfer  Learning: Perspectives</b></summary>
  <p><b>编号</b>：[561]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00247">https://arxiv.org/abs/2404.00247</a></p>
  <p><b>作者</b>：Runze Lin,  Junghui Chen,  Lei Xie,  Hongye Su,  Biao Huang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：deep reinforcement learning, paper provides insights, insights into deep, deep reinforcement, transfer learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper provides insights into deep reinforcement learning (DRL) for process control from the perspective of transfer learning. We analyze the challenges of applying DRL in the field of process industries and the necessity of introducing transfer learning. Furthermore, recommendations and prospects are provided for future research directions on how transfer learning can be integrated with DRL to empower process control.</p>
  </details>
</details>
<details>
  <summary>132. <b>标题：Information Security and Privacy in the Digital World: Some Selected  Topics</b></summary>
  <p><b>编号</b>：[568]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00235">https://arxiv.org/abs/2404.00235</a></p>
  <p><b>作者</b>：Jaydip Sen,  Joceli Mayer,  Subhasis Dasgupta,  Subrata Nandi,  Srinivasan Krishnaswamy,  Pinaki Mitra,  Mahendra Pratap Singh,  Naga Prasanthi Kundeti,  Chandra Sekhara Rao MVP,  Sudha Sree Chekuri,  Seshu Babu Pallapothu,  Preethi Nanjundan,  Jossy P. George,  Abdelhadi El Allahi,  Ilham Morino,  Salma AIT Oussous,  Siham Beloualid,  Ahmed Tamtaoui,  Abderrahim Bajit</p>
  <p><b>备注</b>：Published by IntechOpen, London Uk in Nov 2023, the book contains 8 chapters spanning over 131 pages. arXiv admin note: text overlap with arXiv:2307.02055, arXiv:2304.00258</p>
  <p><b>关键词</b>：generative artificial intelligence, Internet of Things, era of generative, generative artificial, artificial intelligence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the era of generative artificial intelligence and the Internet of Things, while there is explosive growth in the volume of data and the associated need for processing, analysis, and storage, several new challenges are faced in identifying spurious and fake information and protecting the privacy of sensitive data. This has led to an increasing demand for more robust and resilient schemes for authentication, integrity protection, encryption, non-repudiation, and privacy-preservation of data. The chapters in this book present some of the state-of-the-art research works in the field of cryptography and security in computing and communications.</p>
  </details>
</details>
<details>
  <summary>133. <b>标题：Efficient Automatic Tuning for Data-driven Model Predictive Control via  Meta-Learning</b></summary>
  <p><b>编号</b>：[570]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00232">https://arxiv.org/abs/2404.00232</a></p>
  <p><b>作者</b>：Baoyu Li,  William Edwards,  Kris Hauser</p>
  <p><b>备注</b>：ICRA 2023 Workshop on Effective Representations, Abstractions, and Priors for Robot Learning (RAP4Robots)</p>
  <p><b>关键词</b>：data-driven model predictive, Python package, optimizes data-driven model, model predictive control, pure Bayesian Optimization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>AutoMPC is a Python package that automates and optimizes data-driven model predictive control. However, it can be computationally expensive and unstable when exploring large search spaces using pure Bayesian Optimization (BO). To address these issues, this paper proposes to employ a meta-learning approach called Portfolio that improves AutoMPC's efficiency and stability by warmstarting BO. Portfolio optimizes initial designs for BO using a diverse set of configurations from previous tasks and stabilizes the tuning process by fixing initial configurations instead of selecting them randomly. Experimental results demonstrate that Portfolio outperforms the pure BO in finding desirable solutions for AutoMPC within limited computational resources on 11 nonlinear control simulation benchmarks and 1 physical underwater soft robot dataset.</p>
  </details>
</details>
<details>
  <summary>134. <b>标题：Attention-based Shape-Deformation Networks for Artifact-Free Geometry  Reconstruction of Lumbar Spine from MR Images</b></summary>
  <p><b>编号</b>：[571]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00231">https://arxiv.org/abs/2404.00231</a></p>
  <p><b>作者</b>：Linchen Qian,  Jiasong Chen,  Linhai Ma,  Timur Urakov,  Weiyong Gu,  Liang Liang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：low back pain, global health concern, progressive structural wear, significant global health, Lumbar disc degeneration</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Lumbar disc degeneration, a progressive structural wear and tear of lumbar intervertebral disc, is regarded as an essential role on low back pain, a significant global health concern. Automated lumbar spine geometry reconstruction from MR images will enable fast measurement of medical parameters to evaluate the lumbar status, in order to determine a suitable treatment. Existing image segmentation-based techniques often generate erroneous segments or unstructured point clouds, unsuitable for medical parameter measurement. In this work, we present TransDeformer: a novel attention-based deep learning approach that reconstructs the contours of the lumbar spine with high spatial accuracy and mesh correspondence across patients, and we also present a variant of TransDeformer for error estimation. Specially, we devise new attention modules with a new attention formula, which integrates image features and tokenized contour features to predict the displacements of the points on a shape template without the need for image segmentation. The deformed template reveals the lumbar spine geometry in the input image. We develop a multi-stage training strategy to enhance model robustness with respect to template initialization. Experiment results show that our TransDeformer generates artifact-free geometry outputs, and its variant predicts the error of a reconstructed geometry. Our code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>135. <b>标题：InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning</b></summary>
  <p><b>编号</b>：[573]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00228">https://arxiv.org/abs/2404.00228</a></p>
  <p><b>作者</b>：Yan-Shuo Liang,  Wu-Jun Li</p>
  <p><b>备注</b>：Accepted by the 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2024)</p>
  <p><b>关键词</b>：Continual learning, Continual learning requires, continual learning methods, learning, Continual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Continual learning requires the model to learn multiple tasks sequentially. In continual learning, the model should possess the ability to maintain its performance on old tasks (stability) and the ability to adapt to new tasks continuously (plasticity). Recently, parameter-efficient fine-tuning (PEFT), which involves freezing a pre-trained model and injecting a small number of learnable parameters to adapt to downstream tasks, has gained increasing popularity in continual learning. Although existing continual learning methods based on PEFT have demonstrated superior performance compared to those not based on PEFT, most of them do not consider how to eliminate the interference of the new task on the old tasks, which inhibits the model from making a good trade-off between stability and plasticity. In this work, we propose a new PEFT method, called interference-free low-rank adaptation (InfLoRA), for continual learning. InfLoRA injects a small number of parameters to reparameterize the pre-trained weights and shows that fine-tuning these injected parameters is equivalent to fine-tuning the pre-trained weights within a subspace. Furthermore, InfLoRA designs this subspace to eliminate the interference of the new task on the old tasks, making a good trade-off between stability and plasticity. Experimental results show that InfLoRA outperforms existing state-of-the-art continual learning methods on multiple datasets.</p>
  </details>
</details>
<details>
  <summary>136. <b>标题：Heterogeneous Contrastive Learning for Foundation Models and Beyond</b></summary>
  <p><b>编号</b>：[576]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00225">https://arxiv.org/abs/2404.00225</a></p>
  <p><b>作者</b>：Lecheng Zheng,  Baoyu Jing,  Zihao Li,  Hanghang Tong,  Jingrui He</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Artificial Intelligence, large-scale heterogeneous data, contrastive learning, contrastive self-supervised learning, utilize contrastive self-supervised</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the era of big data and Artificial Intelligence, an emerging paradigm is to utilize contrastive self-supervised learning to model large-scale heterogeneous data. Many existing foundation models benefit from the generalization capability of contrastive self-supervised learning by learning compact and high-quality representations without relying on any label information. Amidst the explosive advancements in foundation models across multiple domains, including natural language processing and computer vision, a thorough survey on heterogeneous contrastive learning for the foundation model is urgently needed. In response, this survey critically evaluates the current landscape of heterogeneous contrastive learning for foundation models, highlighting the open challenges and future trends of contrastive learning. In particular, we first present how the recent advanced contrastive learning-based methods deal with view heterogeneity and how contrastive learning is applied to train and fine-tune the multi-view foundation models. Then, we move to contrastive learning methods for task heterogeneity, including pretraining tasks and downstream tasks, and show how different tasks are combined with contrastive learning loss for different purposes. Finally, we conclude this survey by discussing the open challenges and shedding light on the future directions of contrastive learning.</p>
  </details>
</details>
<details>
  <summary>137. <b>标题：Multi-Conditional Ranking with Large Language Models</b></summary>
  <p><b>编号</b>：[582]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00211">https://arxiv.org/abs/2404.00211</a></p>
  <p><b>作者</b>：Pouya Pezeshkpour,  Estevam Hruschka</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Utilizing large language, large language models, Utilizing large, large language, recommendation and retrieval</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Utilizing large language models (LLMs) to rank a set of items has become a common approach in recommendation and retrieval systems. Typically, these systems focus on ordering a substantial number of documents in a monotonic order based on a given query. However, real-world scenarios often present a different challenge: ranking a comparatively smaller set of items, but according to a variety of diverse and occasionally conflicting conditions. In this paper, we define and explore the task of multi-conditional ranking by introducing MCRank, a benchmark tailored for assessing multi-conditional ranking across various item types and conditions. Our analysis of LLMs using MCRank indicates a significant decrease in performance as the number and complexity of items and conditions grow. To overcome this limitation, we propose a novel decomposed reasoning method, consisting of EXtracting and Sorting the conditions, and then Iterativly Ranking the items (EXSIR). Our extensive experiments show that this decomposed reasoning method enhances LLMs' performance significantly, achieving up to a 12% improvement over existing LLMs. We also provide a detailed analysis of LLMs performance across various condition categories, and examine the effectiveness of decomposition step. Furthermore, we compare our method with existing approaches such as Chain-of-Thought and an encoder-type ranking model, demonstrating the superiority of our approach and complexity of MCR task. We released our dataset and code.</p>
  </details>
</details>
<details>
  <summary>138. <b>标题：Causal Inference for Human-Language Model Collaboration</b></summary>
  <p><b>编号</b>：[586]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00207">https://arxiv.org/abs/2404.00207</a></p>
  <p><b>作者</b>：Bohan Zhang,  Yixin Wang,  Paramveer S. Dhillon</p>
  <p><b>备注</b>：9 pages (Accepted for publication at NAACL 2024 (Main Conference))</p>
  <p><b>关键词</b>：typically involve LMs, involve LMs proposing, interactions typically involve, proposing text segments, LMs proposing text</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we examine the collaborative dynamics between humans and language models (LMs), where the interactions typically involve LMs proposing text segments and humans editing or responding to these proposals. Productive engagement with LMs in such scenarios necessitates that humans discern effective text-based interaction strategies, such as editing and response styles, from historical human-LM interactions. This objective is inherently causal, driven by the counterfactual `what-if' question: how would the outcome of collaboration change if humans employed a different text editing/refinement strategy? A key challenge in answering this causal inference question is formulating an appropriate causal estimand: the conventional average treatment effect (ATE) estimand is inapplicable to text-based treatments due to their high dimensionality. To address this concern, we introduce a new causal estimand -- Incremental Stylistic Effect (ISE) -- which characterizes the average impact of infinitesimally shifting a text towards a specific style, such as increasing formality. We establish the conditions for the non-parametric identification of ISE. Building on this, we develop CausalCollab, an algorithm designed to estimate the ISE of various interaction strategies in dynamic human-LM collaborations. Our empirical investigations across three distinct human-LM collaboration scenarios reveal that CausalCollab effectively reduces confounding and significantly improves counterfactual estimation over a set of competitive baselines.</p>
  </details>
</details>
<details>
  <summary>139. <b>标题：A PPO-based DRL Auto-Tuning Nonlinear PID Drone Controller for Robust  Autonomous Flights</b></summary>
  <p><b>编号</b>：[589]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00204">https://arxiv.org/abs/2404.00204</a></p>
  <p><b>作者</b>：Junyang Zhang,  Cristian Emanuel Ocampo Rivera,  Kyle Tyni,  Steven Nguyen</p>
  <p><b>备注</b>：9 pages, 12 figures</p>
  <p><b>关键词</b>：Proportional Integral Derivative, linear Proportional Integral, traditional linear Proportional, nonlinear Deep Reinforcement, Deep Reinforcement Learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This project aims to revolutionize drone flight control by implementing a nonlinear Deep Reinforcement Learning (DRL) agent as a replacement for traditional linear Proportional Integral Derivative (PID) controllers. The primary objective is to seamlessly transition drones between manual and autonomous modes, enhancing responsiveness and stability. We utilize the Proximal Policy Optimization (PPO) reinforcement learning strategy within the Gazebo simulator to train the DRL agent. Adding a $20,000 indoor Vicon tracking system offers <1mm 3 positioning accuracy, which significantly improves autonomous flight precision. to navigate the drone in shortest collision-free trajectory, we also build a dimensional a* path planner and implement it into real successfully.< p>
  </1mm></p></details>
</details>
<details>
  <summary>140. <b>标题：Multiple-policy Evaluation via Density Estimation</b></summary>
  <p><b>编号</b>：[593]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00195">https://arxiv.org/abs/2404.00195</a></p>
  <p><b>作者</b>：Yilei Chen,  Aldo Pacchiano,  Ioannis Ch. Paschalidis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：expected total rewards, multiple-policy evaluation problem, evaluate their performance, total rewards, multiple-policy evaluation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we focus on the multiple-policy evaluation problem where we are given a set of $K$ target policies and the goal is to evaluate their performance (the expected total rewards) to an accuracy $\epsilon$ with probability at least $1-\delta$. We propose an algorithm named $\mathrm{CAESAR}$ to address this problem. Our approach is based on computing an approximate optimal offline sampling distribution and using the data sampled from it to perform the simultaneous estimation of the policy values. $\mathrm{CAESAR}$ consists of two phases. In the first one we produce coarse estimates of the vistation distributions of the target policies at a low order sample complexity rate that scales with $\tilde{O}(\frac{1}{\epsilon})$. In the second phase, we approximate the optimal offline sampling distribution and compute the importance weighting ratios for all target policies by minimizing a step-wise quadratic loss function inspired by the objective in DualDICE. Up to low order and logarithm terms $\mathrm{CAESAR}$ achieves a sample complexity $\tilde{O}\left(\frac{H^4}{\epsilon^2}\sum_{h=1}^H\max_{k\in[K]}\sum_{s,a}\frac{(d_h^{\pi^k}(s,a))^2}{\mu^*_h(s,a)}\right)$, where $d^{\pi}$ is the visitation distribution of policy $\pi$ and $\mu^*$ is the optimal sampling distribution.</p>
  </details>
</details>
<details>
  <summary>141. <b>标题：Multi-Region Transfer Learning for Segmentation of Crop Field Boundaries  in Satellite Images with Limited Labels</b></summary>
  <p><b>编号</b>：[602]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00179">https://arxiv.org/abs/2404.00179</a></p>
  <p><b>作者</b>：Hannah Kerner,  Saketh Sundar,  Mathan Satish</p>
  <p><b>备注</b>：Accepted for 2023 AAAI Workshop on AI to Accelerate Science and Engineering</p>
  <p><b>关键词</b>：overhead remotely sensed, field boundary delineation, remotely sensed images, field boundary, boundary delineation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The goal of field boundary delineation is to predict the polygonal boundaries and interiors of individual crop fields in overhead remotely sensed images (e.g., from satellites or drones). Automatic delineation of field boundaries is a necessary task for many real-world use cases in agriculture, such as estimating cultivated area in a region or predicting end-of-season yield in a field. Field boundary delineation can be framed as an instance segmentation problem, but presents unique research challenges compared to traditional computer vision datasets used for instance segmentation. The practical applicability of previous work is also limited by the assumption that a sufficiently-large labeled dataset is available where field boundary delineation models will be applied, which is not the reality for most regions (especially under-resourced regions such as Sub-Saharan Africa). We present an approach for segmentation of crop field boundaries in satellite images in regions lacking labeled data that uses multi-region transfer learning to adapt model weights for the target region. We show that our approach outperforms existing methods and that multi-region transfer learning substantially boosts performance for multiple model architectures. Our implementation and datasets are publicly available to enable use of the approach by end-users and serve as a benchmark for future work.</p>
  </details>
</details>
<details>
  <summary>142. <b>标题：Comparing Hyper-optimized Machine Learning Models for Predicting  Efficiency Degradation in Organic Solar Cells</b></summary>
  <p><b>编号</b>：[604]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00173">https://arxiv.org/abs/2404.00173</a></p>
  <p><b>作者</b>：David Valientea,  Fernando Rodríguez-Mas,  Juan V. Alegre-Requena,  David Dalmau,  Juan C. Ferrer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：multilayer structure ITO, power conversion efficiency, organic solar cells, temporal degradation suffered, polymeric organic solar</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This work presents a set of optimal machine learning (ML) models to represent the temporal degradation suffered by the power conversion efficiency (PCE) of polymeric organic solar cells (OSCs) with a multilayer structure ITO/PEDOT:PSS/P3HT:PCBM/Al. To that aim, we generated a database with 996 entries, which includes up to 7 variables regarding both the manufacturing process and environmental conditions for more than 180 days. Then, we relied on a software framework that brings together a conglomeration of automated ML protocols that execute sequentially against our database by simply command-line interface. This easily permits hyper-optimizing and randomizing seeds of the ML models through exhaustive benchmarking so that optimal models are obtained. The accuracy achieved reaches values of the coefficient determination (R2) widely exceeding 0.90, whereas the root mean squared error (RMSE), sum of squared error (SSE), and mean absolute error (MAE)>1% of the target value, the PCE. Additionally, we contribute with validated models able to screen the behavior of OSCs never seen in the database. In that case, R2~0.96-0.97 and RMSE~1%, thus confirming the reliability of the proposal to predict. For comparative purposes, classical Bayesian regression fitting based on non-linear mean squares (LMS) are also presented, which only perform sufficiently for univariate cases of single OSCs. Hence they fail to outperform the breadth of the capabilities shown by the ML models. Finally, thanks to the standardized results offered by the ML framework, we study the dependencies between the variables of the dataset and their implications for the optimal performance and stability of the OSCs. Reproducibility is ensured by a standardized report altogether with the dataset, which are publicly available at Github.</p>
  </details>
</details>
<details>
  <summary>143. <b>标题：Universal Bovine Identification via Depth Data and Deep Metric Learning</b></summary>
  <p><b>编号</b>：[605]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00172">https://arxiv.org/abs/2404.00172</a></p>
  <p><b>作者</b>：Asheesh Sharma,  Lucy Randewich,  William Andrew,  Sion Hannuna,  Neill Campbell,  Siobhan Mullan,  Andrew W. Dowsey,  Melvyn Smith,  Mark Hansen,  Tilo Burghardt</p>
  <p><b>备注</b>：LaTeX, 38 pages, 14 figures, 3 tables</p>
  <p><b>关键词</b>：depth-only deep learning, deep learning system, accurately identifying individual, dorsal view, identifying individual cattle</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper proposes and evaluates, for the first time, a top-down (dorsal view), depth-only deep learning system for accurately identifying individual cattle and provides associated code, datasets, and training weights for immediate reproducibility. An increase in herd size skews the cow-to-human ratio at the farm and makes the manual monitoring of individuals more challenging. Therefore, real-time cattle identification is essential for the farms and a crucial step towards precision livestock farming. Underpinned by our previous work, this paper introduces a deep-metric learning method for cattle identification using depth data from an off-the-shelf 3D camera. The method relies on CNN and MLP backbones that learn well-generalised embedding spaces from the body shape to differentiate individuals -- requiring neither species-specific coat patterns nor close-up muzzle prints for operation. The network embeddings are clustered using a simple algorithm such as $k$-NN for highly accurate identification, thus eliminating the need to retrain the network for enrolling new individuals. We evaluate two backbone architectures, ResNet, as previously used to identify Holstein Friesians using RGB images, and PointNet, which is specialised to operate on 3D point clouds. We also present CowDepth2023, a new dataset containing 21,490 synchronised colour-depth image pairs of 99 cows, to evaluate the backbones. Both ResNet and PointNet architectures, which consume depth maps and point clouds, respectively, led to high accuracy that is on par with the coat pattern-based backbone.</p>
  </details>
</details>
<details>
  <summary>144. <b>标题：Individual Text Corpora Predict Openness, Interests, Knowledge and Level  of Education</b></summary>
  <p><b>编号</b>：[609]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00165">https://arxiv.org/abs/2404.00165</a></p>
  <p><b>作者</b>：Markus J. Hofmann,  Markus T. Jansen,  Christoph Wigbels,  Benny Briesemeister,  Arthur M. Jacobs</p>
  <p><b>备注</b>：Proceedings of the 8th workshop on Cognitive Aspects of the Lexicon (CogALex-VIII), LREC/Coling 2024</p>
  <p><b>关键词</b>：google search history, individual google search, individual google, personality dimension, individual text corpora</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Here we examine whether the personality dimension of openness to experience can be predicted from the individual google search history. By web scraping, individual text corpora (ICs) were generated from 214 participants with a mean number of 5 million word tokens. We trained word2vec models and used the similarities of each IC to label words, which were derived from a lexical approach of personality. These IC-label-word similarities were utilized as predictive features in neural models. For training and validation, we relied on 179 participants and held out a test sample of 35 participants. A grid search with varying number of predictive features, hidden units and boost factor was performed. As model selection criterion, we used R2 in the validation samples penalized by the absolute R2 difference between training and validation. The selected neural model explained 35% of the openness variance in the test sample, while an ensemble model with the same architecture often provided slightly more stable predictions for intellectual interests, knowledge in humanities and level of education. Finally, a learning curve analysis suggested that around 500 training participants are required for generalizable predictions. We discuss ICs as a complement or replacement of survey-based psychodiagnostics.</p>
  </details>
</details>
<details>
  <summary>145. <b>标题：Modeling Large-Scale Walking and Cycling Networks: A Machine Learning  Approach Using Mobile Phone and Crowdsourced Data</b></summary>
  <p><b>编号</b>：[611]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00162">https://arxiv.org/abs/2404.00162</a></p>
  <p><b>作者</b>：Meead Saberi,  Tanapon Lilasathapornkit</p>
  <p><b>备注</b>：22 pages, 8 figures, 13 tables</p>
  <p><b>关键词</b>：bring substantial health, substantial health, economic advantages, bring substantial, mobile phone data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Walking and cycling are known to bring substantial health, environmental, and economic advantages. However, the development of evidence-based active transportation planning and policies has been impeded by significant data limitations, such as biases in crowdsourced data and representativeness issues of mobile phone data. In this study, we develop and apply a machine learning based modeling approach for estimating daily walking and cycling volumes across a large-scale regional network in New South Wales, Australia that includes 188,999 walking links and 114,885 cycling links. The modeling methodology leverages crowdsourced and mobile phone data as well as a range of other datasets on population, land use, topography, climate, etc. The study discusses the unique challenges and limitations related to all three aspects of model training, testing, and inference given the large geographical extent of the modeled networks and relative scarcity of observed walking and cycling count data. The study also proposes a new technique to identify model estimate outliers and to mitigate their impact. Overall, the study provides a valuable resource for transportation modelers, policymakers and urban planners seeking to enhance active transportation infrastructure planning and policies with advanced emerging data-driven modeling methodologies.</p>
  </details>
</details>
<details>
  <summary>146. <b>标题：Does Faithfulness Conflict with Plausibility? An Empirical Study in  Explainable AI across NLP Tasks</b></summary>
  <p><b>编号</b>：[621]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00140">https://arxiv.org/abs/2404.00140</a></p>
  <p><b>作者</b>：Xiaolei Lu,  Jianghong Ma</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：model inference process, explanations accurately reflect, inference process, aimed at interpreting, interpreting decision-making</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Explainability algorithms aimed at interpreting decision-making AI systems usually consider balancing two critical dimensions: 1) \textit{faithfulness}, where explanations accurately reflect the model's inference process. 2) \textit{plausibility}, where explanations are consistent with domain experts. However, the question arises: do faithfulness and plausibility inherently conflict? In this study, through a comprehensive quantitative comparison between the explanations from the selected explainability methods and expert-level interpretations across three NLP tasks: sentiment analysis, intent detection, and topic labeling, we demonstrate that traditional perturbation-based methods Shapley value and LIME could attain greater faithfulness and plausibility. Our findings suggest that rather than optimizing for one dimension at the expense of the other, we could seek to optimize explainability algorithms with dual objectives to achieve high levels of accuracy and user accessibility in their explanations.</p>
  </details>
</details>
<details>
  <summary>147. <b>标题：Budget-aware Query Tuning: An AutoML Perspective</b></summary>
  <p><b>编号</b>：[623]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00137">https://arxiv.org/abs/2404.00137</a></p>
  <p><b>作者</b>：Wentao Wu,  Chi Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：good execution plans, cost units, cost, query, Modern database systems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Modern database systems rely on cost-based query optimizers to come up with good execution plans for input queries. Such query optimizers rely on cost models to estimate the costs of candidate query execution plans. A cost model represents a function from a set of cost units to query execution cost, where each cost unit specifies the unit cost of executing a certain type of query processing operation (such as table scan or join). These cost units are traditionally viewed as constants, whose values only depend on the platform configuration where the database system runs on top of but are invariant for queries processed by the database system. In this paper, we challenge this classic view by thinking of these cost units as variables instead. We show that, by varying the cost-unit values one can obtain query plans that significantly outperform the default query plans returned by the query optimizer when viewing the cost units as constants. We term this cost-unit tuning process "query tuning" (QT) and show that it is similar to the well-known hyper-parameter optimization (HPO) problem in AutoML. As a result, any state-of-the-art HPO technologies can be applied to QT. We study the QT problem in the context of anytime tuning, which is desirable in practice by constraining the total time spent on QT within a given budget -- we call this problem budget-aware query tuning. We further extend our study from tuning a single query to tuning a workload with multiple queries, and we call this generalized problem budget-aware workload tuning (WT), which aims for minimizing the execution time of the entire workload. WT is more challenging as one needs to further prioritize individual query tuning within the given time budget. We propose solutions to both QT and WT and experimental evaluation using both benchmark and real workloads demonstrates the efficacy of our proposed solutions.</p>
  </details>
</details>
<details>
  <summary>148. <b>标题：FISBe: A real-world benchmark dataset for instance segmentation of  long-range thin filamentous structures</b></summary>
  <p><b>编号</b>：[626]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00130">https://arxiv.org/abs/2404.00130</a></p>
  <p><b>作者</b>：Lisa Mais,  Peter Hirsch,  Claire Managan,  Ramya Kandarpa,  Josef Lorenz Rumberger,  Annika Reinke,  Lena Maier-Hein,  Gudrun Ihrke,  Dagmar Kainmueller</p>
  <p><b>备注</b>：CVPR2024, Project page: this https URL</p>
  <p><b>关键词</b>：nervous systems enables, systems enables groundbreaking, facilitating joint functional, light microscopy images, volumetric light microscopy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Instance segmentation of neurons in volumetric light microscopy images of nervous systems enables groundbreaking research in neuroscience by facilitating joint functional and morphological analyses of neural circuits at cellular resolution. Yet said multi-neuron light microscopy data exhibits extremely challenging properties for the task of instance segmentation: Individual neurons have long-ranging, thin filamentous and widely branching morphologies, multiple neurons are tightly inter-weaved, and partial volume effects, uneven illumination and noise inherent to light microscopy severely impede local disentangling as well as long-range tracing of individual neurons. These properties reflect a current key challenge in machine learning research, namely to effectively capture long-range dependencies in the data. While respective methodological research is buzzing, to date methods are typically benchmarked on synthetic datasets. To address this gap, we release the FlyLight Instance Segmentation Benchmark (FISBe) dataset, the first publicly available multi-neuron light microscopy dataset with pixel-wise annotations. In addition, we define a set of instance segmentation metrics for benchmarking that we designed to be meaningful with regard to downstream analyses. Lastly, we provide three baselines to kick off a competition that we envision to both advance the field of machine learning regarding methodology for capturing long-range data dependencies, and facilitate scientific discovery in basic neuroscience.</p>
  </details>
</details>
<details>
  <summary>149. <b>标题：PikeLPN: Mitigating Overlooked Inefficiencies of Low-Precision Neural  Networks</b></summary>
  <p><b>编号</b>：[635]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00103">https://arxiv.org/abs/2404.00103</a></p>
  <p><b>作者</b>：Marina Neseem,  Conor McCullough,  Randy Hsin,  Chas Leichner,  Shan Li,  In Suk Chong,  Andrew G. Howard,  Lukasz Lew,  Sherief Reda,  Ville-Mikko Rautio,  Daniele Moro</p>
  <p><b>备注</b>：Accepted in CVPR 2024. 10 Figures, 9 Tables</p>
  <p><b>关键词</b>：neural network optimization, Arithmetic Computation Effort, network optimization, efficacy in neural, neural network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Low-precision quantization is recognized for its efficacy in neural network optimization. Our analysis reveals that non-quantized elementwise operations which are prevalent in layers such as parameterized activation functions, batch normalization, and quantization scaling dominate the inference cost of low-precision models. These non-quantized elementwise operations are commonly overlooked in SOTA efficiency metrics such as Arithmetic Computation Effort (ACE). In this paper, we propose ACEv2 - an extended version of ACE which offers a better alignment with the inference cost of quantized models and their energy consumption on ML hardware. Moreover, we introduce PikeLPN, a model that addresses these efficiency issues by applying quantization to both elementwise operations and multiply-accumulate operations. In particular, we present a novel quantization technique for batch normalization layers named QuantNorm which allows for quantizing the batch normalization parameters without compromising the model performance. Additionally, we propose applying Double Quantization where the quantization scaling parameters are quantized. Furthermore, we recognize and resolve the issue of distribution mismatch in Separable Convolution layers by introducing Distribution-Heterogeneous Quantization which enables quantizing them to low-precision. PikeLPN achieves Pareto-optimality in efficiency-accuracy trade-off with up to 3X efficiency improvement compared to SOTA low-precision models.</p>
  </details>
</details>
<details>
  <summary>150. <b>标题：Bayesian Nonparametrics: An Alternative to Deep Learning</b></summary>
  <p><b>编号</b>：[640]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00085">https://arxiv.org/abs/2404.00085</a></p>
  <p><b>作者</b>：Bahman Moraffah</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：statistical model selection, Bayesian nonparametric models, Bayesian nonparametric, nonparametric models offer, Bayesian nonparametric methodologies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Bayesian nonparametric models offer a flexible and powerful framework for statistical model selection, enabling the adaptation of model complexity to the intricacies of diverse datasets. This survey intends to delve into the significance of Bayesian nonparametrics, particularly in addressing complex challenges across various domains such as statistics, computer science, and electrical engineering. By elucidating the basic properties and theoretical foundations of these nonparametric models, this survey aims to provide a comprehensive understanding of Bayesian nonparametrics and their relevance in addressing complex problems, particularly in the domain of multi-object tracking. Through this exploration, we uncover the versatility and efficacy of Bayesian nonparametric methodologies, paving the way for innovative solutions to intricate challenges across diverse disciplines.</p>
  </details>
</details>
<details>
  <summary>151. <b>标题：A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping  Attacks</b></summary>
  <p><b>编号</b>：[641]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00076">https://arxiv.org/abs/2404.00076</a></p>
  <p><b>作者</b>：Orson Mengara</p>
  <p><b>备注</b>：Accept by "IEEE Access" let's take a look at our global approach to the DNN(s) model(s) deployment chain in production: Danger NLP-Speech(Trigger universal approach)</p>
  <p><b>关键词</b>：Audio-based machine learning, machine learning systems, learning systems frequently, Audio-based machine, machine learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Audio-based machine learning systems frequently use public or third-party data, which might be inaccurate. This exposes deep neural network (DNN) models trained on such data to potential data poisoning attacks. In this type of assault, attackers can train the DNN model using poisoned data, potentially degrading its performance. Another type of data poisoning attack that is extremely relevant to our investigation is label flipping, in which the attacker manipulates the labels for a subset of data. It has been demonstrated that these assaults may drastically reduce system performance, even for attackers with minimal abilities. In this study, we propose a backdoor attack named 'DirtyFlipping', which uses dirty label techniques, "label-on-label", to input triggers (clapping) in the selected data patterns associated with the target class, thereby enabling a stealthy backdoor.</p>
  </details>
</details>
<details>
  <summary>152. <b>标题：BEACON: Bayesian Experimental design Acceleration with Conditional  Normalizing flows $-$ a case study in optimal monitor well placement for  CO$_2$ sequestration</b></summary>
  <p><b>编号</b>：[642]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00075">https://arxiv.org/abs/2404.00075</a></p>
  <p><b>作者</b>：Rafael Orozco,  Abhinav Gahlot,  Felix J. Herrmann</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：mitigating climate change, crucial engineering solution, climate change, engineering solution, solution for mitigating</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>CO$_2$ sequestration is a crucial engineering solution for mitigating climate change. However, the uncertain nature of reservoir properties, necessitates rigorous monitoring of CO$_2$ plumes to prevent risks such as leakage, induced seismicity, or breaching licensed boundaries. To address this, project managers use borehole wells for direct CO$_2$ and pressure monitoring at specific locations. Given the high costs associated with drilling, it is crucial to strategically place a limited number of wells to ensure maximally effective monitoring within budgetary constraints. Our approach for selecting well locations integrates fluid-flow solvers for forecasting plume trajectories with generative neural networks for plume inference uncertainty. Our methodology is extensible to three-dimensional domains and is developed within a Bayesian framework for optimal experimental design, ensuring scalability and mathematical optimality. We use a realistic case study to verify these claims by demonstrating our method's application in a large scale domain and optimal performance as compared to baseline well placement.</p>
  </details>
</details>
<details>
  <summary>153. <b>标题：A finite operator learning technique for mapping the elastic properties  of microstructures to their mechanical deformations</b></summary>
  <p><b>编号</b>：[643]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00074">https://arxiv.org/abs/2404.00074</a></p>
  <p><b>作者</b>：Shahed Rezaei,  Shirko Faroughi,  Mahdi Asgharzadeh,  Ali Harandi,  Gottfried Laschet,  Stefanie Reese,  Markus Apel</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：develop faster solvers, mechanical equilibrium, develop faster, governing physical equations, parametrically learns</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To develop faster solvers for governing physical equations in solid mechanics, we introduce a method that parametrically learns the solution to mechanical equilibrium. The introduced method outperforms traditional ones in terms of computational cost while acceptably maintaining accuracy. Moreover, it generalizes and enhances the standard physics-informed neural networks to learn a parametric solution with rather sharp discontinuities. We focus on micromechanics as an example, where the knowledge of the micro-mechanical solution, i.e., deformation and stress fields for a given heterogeneous microstructure, is crucial. The parameter under investigation is the Young modulus distribution within the heterogeneous solid system. Our method, inspired by operator learning and the finite element method, demonstrates the ability to train without relying on data from other numerical solvers. Instead, we leverage ideas from the finite element approach to efficiently set up loss functions algebraically, particularly based on the discretized weak form of the governing equations. Notably, our investigations reveal that physics-based training yields higher accuracy compared to purely data-driven approaches for unseen microstructures. In essence, this method achieves independence from data and enhances accuracy for predictions beyond the training range. The aforementioned observations apply here to heterogeneous elastic microstructures. Comparisons are also made with other well-known operator learning algorithms, such as DeepOnet, to further emphasize the advantages of the newly proposed architecture.</p>
  </details>
</details>
<details>
  <summary>154. <b>标题：A Two-Phase Recall-and-Select Framework for Fast Model Selection</b></summary>
  <p><b>编号</b>：[644]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00069">https://arxiv.org/abs/2404.00069</a></p>
  <p><b>作者</b>：Jianwei Cui,  Wenhang Shi,  Honglin Tao,  Wei Lu,  Xiaoyong Du</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：machine learning applications, neural network models, public model repositories, machine learning, machine learning assignment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As the ubiquity of deep learning in various machine learning applications has amplified, a proliferation of neural network models has been trained and shared on public model repositories. In the context of a targeted machine learning assignment, utilizing an apt source model as a starting point typically outperforms the strategy of training from scratch, particularly with limited training data. Despite the investigation and development of numerous model selection strategies in prior work, the process remains time-consuming, especially given the ever-increasing scale of model repositories. In this paper, we propose a two-phase (coarse-recall and fine-selection) model selection framework, aiming to enhance the efficiency of selecting a robust model by leveraging the models' training performances on benchmark datasets. Specifically, the coarse-recall phase clusters models showcasing similar training performances on benchmark datasets in an offline manner. A light-weight proxy score is subsequently computed between this model cluster and the target dataset, which serves to recall a significantly smaller subset of potential candidate models in a swift manner. In the following fine-selection phase, the final model is chosen by fine-tuning the recalled models on the target dataset with successive halving. To accelerate the process, the final fine-tuning performance of each potential model is predicted by mining the model's convergence trend on the benchmark datasets, which aids in filtering lower performance models more earlier during fine-tuning. Through extensive experimentation on tasks covering natural language processing and computer vision, it has been demonstrated that the proposed methodology facilitates the selection of a high-performing model at a rate about 3x times faster than conventional baseline methods. Our code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>155. <b>标题：A Data-Driven Predictive Analysis on Cyber Security Threats with Key  Risk Factors</b></summary>
  <p><b>编号</b>：[645]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00068">https://arxiv.org/abs/2404.00068</a></p>
  <p><b>作者</b>：Fatama Tuz Johora (1),  Md Shahedul Islam Khan (2),  Esrath Kanon (1),  Mohammad Abu Tareq Rony (3),  Md Zubair (4),  (5) Iqbal H. Sarker ((1) Department of Computer Science and Engineering, University of Chittagong, Chattogram, Bangladesh, (2) Department of School of Electronics and Information, Northwestern Polytechnical University, Xi'an, Shaanxi, China (3) Department of Statistics, Noakhali Science and Technology University, Noakhali, Bangladesh (4) Department of Computer Science and Engineering, Chittagong University of Engineering & Technology, Chattogram, Bangladesh (5) Centre for Securing Digital Futures, Edith Cowan University, Perth, WA, Australia)</p>
  <p><b>备注</b>：The paper contains 15 pages, 7 tables and 6 figures</p>
  <p><b>关键词</b>：Cyber risk refers, Cyber risk, monetary losses, defacing reputation, situation usually occurs</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cyber risk refers to the risk of defacing reputation, monetary losses, or disruption of an organization or individuals, and this situation usually occurs by the unconscious use of cyber systems. The cyber risk is unhurriedly increasing day by day and it is right now a global threat. Developing countries like Bangladesh face major cyber risk challenges. The growing cyber threat worldwide focuses on the need for effective modeling to predict and manage the associated risk. This paper exhibits a Machine Learning(ML) based model for predicting individuals who may be victims of cyber attacks by analyzing socioeconomic factors. We collected the dataset from victims and non-victims of cyberattacks based on socio-demographic features. The study involved the development of a questionnaire to gather data, which was then used to measure the significance of features. Through data augmentation, the dataset was expanded to encompass 3286 entries, setting the stage for our investigation and modeling. Among several ML models with 19, 20, 21, and 26 features, we proposed a novel Pertinent Features Random Forest (RF) model, which achieved maximum accuracy with 20 features (95.95\%) and also demonstrated the association among the selected features using the Apriori algorithm with Confidence (above 80\%) according to the victim. We generated 10 important association rules and presented the framework that is rigorously evaluated on real-world datasets, demonstrating its potential to predict cyberattacks and associated risk factors effectively. Looking ahead, future efforts will be directed toward refining the predictive model's precision and delving into additional risk factors, to fortify the proposed framework's efficacy in navigating the complex terrain of cybersecurity threats.</p>
  </details>
</details>
<details>
  <summary>156. <b>标题：Fingerprinting web servers through Transformer-encoded HTTP response  headers</b></summary>
  <p><b>编号</b>：[651]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00056">https://arxiv.org/abs/2404.00056</a></p>
  <p><b>作者</b>：Patrick Darwinkel</p>
  <p><b>备注</b>：Based on a bachelor's thesis. Submission to arXiv approved by supervisor</p>
  <p><b>关键词</b>：natural language processing, vulnerable web server, deep learning, big data, explored leveraging</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We explored leveraging state-of-the-art deep learning, big data, and natural language processing to enhance the detection of vulnerable web server versions. Focusing on improving accuracy and specificity over rule-based systems, we conducted experiments by sending various ambiguous and non-standard HTTP requests to 4.77 million domains and capturing HTTP response status lines. We represented these status lines through training a BPE tokenizer and RoBERTa encoder for unsupervised masked language modeling. We then dimensionality reduced and concatenated encoded response lines to represent each domain's web server. A Random Forest and multilayer perceptron (MLP) classified these web servers, and achieved 0.94 and 0.96 macro F1-score, respectively, on detecting the five most popular origin web servers. The MLP achieved a weighted F1-score of 0.55 on classifying 347 major type and minor version pairs. Analysis indicates that our test cases are meaningful discriminants of web server types. Our approach demonstrates promise as a powerful and flexible alternative to rule-based systems.</p>
  </details>
</details>
<details>
  <summary>157. <b>标题：Choreographing the Digital Canvas: A Machine Learning Approach to  Artistic Performance</b></summary>
  <p><b>编号</b>：[652]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00054">https://arxiv.org/abs/2404.00054</a></p>
  <p><b>作者</b>：Siyuan Peng,  Kate Ladenheim,  Snehesh Shrestha,  Cornelia Fermüller</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：artistic performances based, paper introduces, introduces the concept, performances based, Attribute-Conditioned Variational Autoencoder</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper introduces the concept of a design tool for artistic performances based on attribute descriptions. To do so, we used a specific performance of falling actions. The platform integrates a novel machine-learning (ML) model with an interactive interface to generate and visualize artistic movements. Our approach's core is a cyclic Attribute-Conditioned Variational Autoencoder (AC-VAE) model developed to address the challenge of capturing and generating realistic 3D human body motions from motion capture (MoCap) data. We created a unique dataset focused on the dynamics of falling movements, characterized by a new ontology that divides motion into three distinct phases: Impact, Glitch, and Fall. The ML model's innovation lies in its ability to learn these phases separately. It is achieved by applying comprehensive data augmentation techniques and an initial pose loss function to generate natural and plausible motion. Our web-based interface provides an intuitive platform for artists to engage with this technology, offering fine-grained control over motion attributes and interactive visualization tools, including a 360-degree view and a dynamic timeline for playback manipulation. Our research paves the way for a future where technology amplifies the creative potential of human expression, making sophisticated motion generation accessible to a wider artistic community.</p>
  </details>
</details>
<details>
  <summary>158. <b>标题：Deja vu: Contrastive Historical Modeling with Prefix-tuning for Temporal  Knowledge Graph Reasoning</b></summary>
  <p><b>编号</b>：[653]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00051">https://arxiv.org/abs/2404.00051</a></p>
  <p><b>作者</b>：Miao Peng,  Ben Liu,  Wenjie Xu,  Zihao Jiang,  Jiahui Zhu,  Min Peng</p>
  <p><b>备注</b>：Accepted to NAACL 2024 Findings</p>
  <p><b>关键词</b>：Knowledge Graph Reasoning, gaining increasing attention, inferring missing facts, Temporal Knowledge Graph, Graph Reasoning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Temporal Knowledge Graph Reasoning (TKGR) is the task of inferring missing facts for incomplete TKGs in complex scenarios (e.g., transductive and inductive settings), which has been gaining increasing attention. Recently, to mitigate dependence on structured connections in TKGs, text-based methods have been developed to utilize rich linguistic information from entity descriptions. However, suffering from the enormous parameters and inflexibility of pre-trained language models, existing text-based methods struggle to balance the textual knowledge and temporal information with computationally expensive purpose-built training strategies. To tap the potential of text-based models for TKGR in various complex scenarios, we propose ChapTER, a Contrastive historical modeling framework with prefix-tuning for TEmporal Reasoning. ChapTER feeds history-contextualized text into the pseudo-Siamese encoders to strike a textual-temporal balance via contrastive estimation between queries and candidates. By introducing virtual time prefix tokens, it applies a prefix-based tuning method to facilitate the frozen PLM capable for TKGR tasks under different settings. We evaluate ChapTER on four transductive and three few-shot inductive TKGR benchmarks, and experimental results demonstrate that ChapTER achieves superior performance compared to competitive baselines with only 0.17% tuned parameters. We conduct thorough analysis to verify the effectiveness, flexibility and efficiency of ChapTER.</p>
  </details>
</details>
<details>
  <summary>159. <b>标题：SLIMBRAIN: Augmented Reality Real-Time Acquisition and Processing System  For Hyperspectral Classification Mapping with Depth Information for In-Vivo  Surgical Procedures</b></summary>
  <p><b>编号</b>：[655]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00048">https://arxiv.org/abs/2404.00048</a></p>
  <p><b>作者</b>：Jaime Sancho,  Manuel Villa,  Miguel Chavarrías,  Eduardo Juarez,  Alfonso Lagares,  César Sanz</p>
  <p><b>备注</b>：14 pages, 19 figues</p>
  <p><b>关键词</b>：technological application domains, augmented reality, rapid development, fields of social, social and technological</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Over the last two decades, augmented reality (AR) has led to the rapid development of new interfaces in various fields of social and technological application domains. One such domain is medicine, and to a higher extent surgery, where these visualization techniques help to improve the effectiveness of preoperative and intraoperative procedures. Following this trend, this paper presents SLIMBRAIN, a real-time acquisition and processing AR system suitable to classify and display brain tumor tissue from hyperspectral (HS) information. This system captures and processes HS images at 14 frames per second (FPS) during the course of a tumor resection operation to detect and delimit cancer tissue at the same time the neurosurgeon operates. The result is represented in an AR visualization where the classification results are overlapped with the RGB point cloud captured by a LiDAR camera. This representation allows natural navigation of the scene at the same time it is captured and processed, improving the visualization and hence effectiveness of the HS technology to delimit tumors. The whole system has been verified in real brain tumor resection operations.</p>
  </details>
</details>
<details>
  <summary>160. <b>标题：Policy Optimization finds Nash Equilibrium in Regularized General-Sum LQ  Games</b></summary>
  <p><b>编号</b>：[657]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00045">https://arxiv.org/abs/2404.00045</a></p>
  <p><b>作者</b>：Muhammad Aneeq uz Zaman,  Shubham Aggarwal,  Melih Bastopcu,  Tamer Başar</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：linear Gaussian policies, Nash Equilibria, introducing relative entropy, relative entropy regularization, Gaussian policies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we investigate the impact of introducing relative entropy regularization on the Nash Equilibria (NE) of General-Sum $N$-agent games, revealing the fact that the NE of such games conform to linear Gaussian policies. Moreover, it delineates sufficient conditions, contingent upon the adequacy of entropy regularization, for the uniqueness of the NE within the game. As Policy Optimization serves as a foundational approach for Reinforcement Learning (RL) techniques aimed at finding the NE, in this work we prove the linear convergence of a policy optimization algorithm which (subject to the adequacy of entropy regularization) is capable of provably attaining the NE. Furthermore, in scenarios where the entropy regularization proves insufficient, we present a $\delta$-augmentation technique, which facilitates the achievement of an $\epsilon$-NE within the game.</p>
  </details>
</details>
<details>
  <summary>161. <b>标题：Improve accessibility for Low Vision and Blind people using Machine  Learning and Computer Vision</b></summary>
  <p><b>编号</b>：[658]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00043">https://arxiv.org/abs/2404.00043</a></p>
  <p><b>作者</b>：Jasur Shukurov</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：mobile technology worldwide, technology worldwide, ever-growing expansion, improve accessibility, mobile technology</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the ever-growing expansion of mobile technology worldwide, there is an increasing need for accommodation for those who are disabled. This project explores how machine learning and computer vision could be utilized to improve accessibility for people with visual impairments. There have been many attempts to develop various software that would improve accessibility in the day-to-day lives of blind people. However, applications on the market have low accuracy and only provide audio feedback. This project will concentrate on building a mobile application that helps blind people to orient in space by receiving audio and haptic feedback, e.g. vibrations, about their surroundings in real-time. The mobile application will have 3 main features. The initial feature is scanning text from the camera and reading it to a user. This feature can be used on paper with text, in the environment, and on road signs. The second feature is detecting objects around the user, and providing audio feedback about those objects. It also includes providing the description of the objects and their location, and giving haptic feedback if the user is too close to an object. The last feature is currency detection which provides a total amount of currency value to the user via the camera.</p>
  </details>
</details>
<details>
  <summary>162. <b>标题：MicroHD: An Accuracy-Driven Optimization of Hyperdimensional Computing  Algorithms for TinyML systems</b></summary>
  <p><b>编号</b>：[661]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00039">https://arxiv.org/abs/2404.00039</a></p>
  <p><b>作者</b>：Flavio Ponzina,  Tajana Rosing</p>
  <p><b>备注</b>：Accepted as a full paper by the tinyML Research Symposium 2024</p>
  <p><b>关键词</b>：effectively target TinyML, target TinyML applications, HDC, effectively target, target TinyML</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Hyperdimensional computing (HDC) is emerging as a promising AI approach that can effectively target TinyML applications thanks to its lightweight computing and memory requirements. Previous works on HDC showed that limiting the standard 10k dimensions of the hyperdimensional space to much lower values is possible, reducing even more HDC resource requirements. Similarly, other studies demonstrated that binary values can be used as elements of the generated hypervectors, leading to significant efficiency gains at the cost of some degree of accuracy degradation. Nevertheless, current optimization attempts do not concurrently co-optimize HDC hyper-parameters, and accuracy degradation is not directly controlled, resulting in sub-optimal HDC models providing several applications with unacceptable output qualities. In this work, we propose MicroHD, a novel accuracy-driven HDC optimization approach that iteratively tunes HDC hyper-parameters, reducing memory and computing requirements while ensuring user-defined accuracy levels. The proposed method can be applied to HDC implementations using different encoding functions, demonstrates good scalability for larger HDC workloads, and achieves compression and efficiency gains up to 200x when compared to baseline implementations for accuracy degradations lower than 1%.</p>
  </details>
</details>
<details>
  <summary>163. <b>标题：Towards gaze-independent c-VEP BCI: A pilot study</b></summary>
  <p><b>编号</b>：[665]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00031">https://arxiv.org/abs/2404.00031</a></p>
  <p><b>作者</b>：S. Narayanan,  S. Ahmadi,  P. Desain,  J. Thielen</p>
  <p><b>备注</b>：6 pages, 3 figures, 9th Graz Brain-Computer Interface Conference 2024</p>
  <p><b>关键词</b>：brain-computer interface, fixate on targets, limitation of brain-computer, eye movements, stimuli</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A limitation of brain-computer interface (BCI) spellers is that they require the user to be able to move the eyes to fixate on targets. This poses an issue for users who cannot voluntarily control their eye movements, for instance, people living with late-stage amyotrophic lateral sclerosis (ALS). This pilot study makes the first step towards a gaze-independent speller based on the code-modulated visual evoked potential (c-VEP). Participants were presented with two bi-laterally located stimuli, one of which was flashing, and were tasked to attend to one of these stimuli either by directly looking at the stimuli (overt condition) or by using spatial attention, eliminating the need for eye movement (covert condition). The attended stimuli were decoded from electroencephalography (EEG) and classification accuracies of 88% and 100% were obtained for the covert and overt conditions, respectively. These fundamental insights show the promising feasibility of utilizing the c-VEP protocol for gaze-independent BCIs that use covert spatial attention when both stimuli flash simultaneously.</p>
  </details>
</details>
<details>
  <summary>164. <b>标题：Visualization of Unstructured Sports Data -- An Example of Cricket Short  Text Commentary</b></summary>
  <p><b>编号</b>：[666]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00030">https://arxiv.org/abs/2404.00030</a></p>
  <p><b>作者</b>：Swarup Ranjan Behera,  Vijaya V Saradhi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Sports visualization focuses, Sports visualization, Sports visualization methods, rules, data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sports visualization focuses on the use of structured data, such as box-score data and tracking data. Unstructured data sources pertaining to sports are available in various places such as blogs, social media posts, and online news articles. Sports visualization methods either not fully exploited the information present in these sources or the proposed visualizations through the use of these sources did not augment to the body of sports visualization methods. We propose the use of unstructured data, namely cricket short text commentary for visualization. The short text commentary data is used for constructing individual player's strength rules and weakness rules. A computationally feasible definition for player's strength rule and weakness rule is proposed. A visualization method for the constructed rules is presented. In addition, players having similar strength rules or weakness rules is computed and visualized. We demonstrate the usefulness of short text commentary in visualization by analyzing the strengths and weaknesses of cricket players using more than one million text commentaries. We validate the constructed rules through two validation methods. The collected data, source code, and obtained results on more than 500 players are made publicly available.</p>
  </details>
</details>
<details>
  <summary>165. <b>标题：LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership  and Reasoning</b></summary>
  <p><b>编号</b>：[668]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00027">https://arxiv.org/abs/2404.00027</a></p>
  <p><b>作者</b>：Azmine Toushik Wasi,  Rafia Islam,  Raima Islam</p>
  <p><b>备注</b>：4 Pages, 2 Figures; The Third Workshop on Intelligent and Interactive Writing Assistants, a hybrid event co-located with The ACM CHI Conference on Human Factors in Computing Systems (CHI 2024) Openreview: this https URL</p>
  <p><b>关键词</b>：investment of thoughts, leading to attachment, Large Language Models, confines our investment, credit Large Language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems.</p>
  </details>
</details>
<details>
  <summary>166. <b>标题：Ink and Individuality: Crafting a Personalised Narrative in the Age of  LLMs</b></summary>
  <p><b>编号</b>：[669]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00026">https://arxiv.org/abs/2404.00026</a></p>
  <p><b>作者</b>：Azmine Toushik Wasi,  Raima Islam,  Rafia Islam</p>
  <p><b>备注</b>：4 Pages, 2 Figures; The Third Workshop on Intelligent and Interactive Writing Assistants, a hybrid event co-located with The ACM CHI Conference on Human Factors in Computing Systems (CHI 2024) Openreview: this https URL</p>
  <p><b>关键词</b>：effectively engage readers, conveying authenticity, comprise the distinctive, distinctive characteristics, characteristics that make</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.</p>
  </details>
</details>
<details>
  <summary>167. <b>标题：Advancing Explainable Autonomous Vehicle Systems: A Comprehensive Review  and Research Roadmap</b></summary>
  <p><b>编号</b>：[674]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00019">https://arxiv.org/abs/2404.00019</a></p>
  <p><b>作者</b>：Sule Tekkesinoglu,  Azra Habibovic,  Lars Kunze</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：existing explainability methods, contexts requiring explanations, suitable interaction strategies, uncertainty surrounding, explainability methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Given the uncertainty surrounding how existing explainability methods for autonomous vehicles (AVs) meet the diverse needs of stakeholders, a thorough investigation is imperative to determine the contexts requiring explanations and suitable interaction strategies. A comprehensive review becomes crucial to assess the alignment of current approaches with the varied interests and expectations within the AV ecosystem. This study presents a review to discuss the complexities associated with explanation generation and presentation to facilitate the development of more effective and inclusive explainable AV systems. Our investigation led to categorising existing literature into three primary topics: explanatory tasks, explanatory information, and explanatory information communication. Drawing upon our insights, we have proposed a comprehensive roadmap for future research centred on (i) knowing the interlocutor, (ii) generating timely explanations, (ii) communicating human-friendly explanations, and (iv) continuous learning. Our roadmap is underpinned by principles of responsible research and innovation, emphasising the significance of diverse explanation requirements. To effectively tackle the challenges associated with implementing explainable AV systems, we have delineated various research directions, including the development of privacy-preserving data integration, ethical frameworks, real-time analytics, human-centric interaction design, and enhanced cross-disciplinary collaborations. By exploring these research directions, the study aims to guide the development and deployment of explainable AVs, informed by a holistic understanding of user needs, technological advancements, regulatory compliance, and ethical considerations, thereby ensuring safer and more trustworthy autonomous driving experiences.</p>
  </details>
</details>
<details>
  <summary>168. <b>标题：SOMson -- Sonification of Multidimensional Data in Kohonen Maps</b></summary>
  <p><b>编号</b>：[677]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00016">https://arxiv.org/abs/2404.00016</a></p>
  <p><b>作者</b>：Simon Linke,  Tim Ziemer</p>
  <p><b>备注</b>：8 pages, 5 figures, linked YouTube videos and interactive demos</p>
  <p><b>关键词</b>：Kohonen Maps, Self-organizing maps, Maps, low-dimensional map, aka</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Kohonen Maps, aka. Self-organizing maps (SOMs) are neural networks that visualize a high-dimensional feature space on a low-dimensional map. While SOMs are an excellent tool for data examination and exploration, they inherently cause a loss of detail. Visualizations of the underlying data do not integrate well and, therefore, fail to provide an overall picture. Consequently, we suggest SOMson, an interactive sonification of the underlying data, as a data augmentation technique. The sonification increases the amount of information provided simultaneously by the SOM. Instead of a user study, we present an interactive online example, so readers can explore SOMson themselves. Its strengths, weaknesses, and prospects are discussed.</p>
  </details>
</details>
<details>
  <summary>169. <b>标题：Missing Data Imputation With Granular Semantics and AI-driven Pipeline  for Bankruptcy Prediction</b></summary>
  <p><b>编号</b>：[678]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00013">https://arxiv.org/abs/2404.00013</a></p>
  <p><b>作者</b>：Debarati Chakraborty,  Ravi Ranjan</p>
  <p><b>备注</b>：15 pages</p>
  <p><b>关键词</b>：work focuses, focuses on designing, missing, prediction, missing data imputation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This work focuses on designing a pipeline for the prediction of bankruptcy. The presence of missing values, high dimensional data, and highly class-imbalance databases are the major challenges in the said task. A new method for missing data imputation with granular semantics has been introduced here. The merits of granular computing have been explored here to define this method. The missing values have been predicted using the feature semantics and reliable observations in a low-dimensional space, in the granular space. The granules are formed around every missing entry, considering a few of the highly correlated features and most reliable closest observations to preserve the relevance and reliability, the context, of the database against the missing entries. An intergranular prediction is then carried out for the imputation within those contextual granules. That is, the contextual granules enable a small relevant fraction of the huge database to be used for imputation and overcome the need to access the entire database repetitively for each missing value. This method is then implemented and tested for the prediction of bankruptcy with the Polish Bankruptcy dataset. It provides an efficient solution for big and high-dimensional datasets even with large imputation rates. Then an AI-driven pipeline for bankruptcy prediction has been designed using the proposed granular semantic-based data filling method followed by the solutions to the issues like high dimensional dataset and high class-imbalance in the dataset. The rest of the pipeline consists of feature selection with the random forest for reducing dimensionality, data balancing with SMOTE, and prediction with six different popular classifiers including deep NN. All methods defined here have been experimentally verified with suitable comparative studies and proven to be effective on all the data sets captured over the five years.</p>
  </details>
</details>
<details>
  <summary>170. <b>标题：A Statistical Framework of Watermarks for Large Language Models: Pivot,  Detection Efficiency and Optimal Rules</b></summary>
  <p><b>编号</b>：[686]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01245">https://arxiv.org/abs/2404.01245</a></p>
  <p><b>作者</b>：Xiang Li,  Feng Ruan,  Huiyuan Wang,  Qi Long,  Weijie J. Su</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, unnoticeable statistical signals, introduced in November, detection rules, language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of incorrectly classifying LLM-generated text as human-written). Our framework further reduces the problem of determining the optimal detection rule to solving a minimax optimization program. We apply this framework to two representative watermarks -- one of which has been internally implemented at OpenAI -- and obtain several findings that can be instrumental in guiding the practice of implementing watermarks. In particular, we derive optimal detection rules for these watermarks under our framework. These theoretically derived detection rules are demonstrated to be competitive and sometimes enjoy a higher power than existing detection approaches through numerical experiments.</p>
  </details>
</details>
<details>
  <summary>171. <b>标题：Optimal Ridge Regularization for Out-of-Distribution Prediction</b></summary>
  <p><b>编号</b>：[687]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01233">https://arxiv.org/abs/2404.01233</a></p>
  <p><b>作者</b>：Pratik Patil,  Jin-Hong Du,  Ryan J. Tibshirani</p>
  <p><b>备注</b>：59 pages, 14 figures</p>
  <p><b>关键词</b>：optimal ridge regularization, distribution deviates arbitrarily, optimal ridge risk, optimal ridge, test distribution deviates</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the behavior of optimal ridge regularization and optimal ridge risk for out-of-distribution prediction, where the test distribution deviates arbitrarily from the train distribution. We establish general conditions that determine the sign of the optimal regularization level under covariate and regression shifts. These conditions capture the alignment between the covariance and signal structures in the train and test data and reveal stark differences compared to the in-distribution setting. For example, a negative regularization level can be optimal under covariate shift or regression shift, even when the training features are isotropic or the design is underparameterized. Furthermore, we prove that the optimally-tuned risk is monotonic in the data aspect ratio, even in the out-of-distribution setting and when optimizing over negative regularization levels. In general, our results do not make any modeling assumptions for the train or the test distributions, except for moment bounds, and allow for arbitrary shifts and the widest possible range of (negative) regularization levels.</p>
  </details>
</details>
<details>
  <summary>172. <b>标题：Large-Scale Non-convex Stochastic Constrained Distributionally Robust  Optimization</b></summary>
  <p><b>编号</b>：[688]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01200">https://arxiv.org/abs/2404.01200</a></p>
  <p><b>作者</b>：Qi Zhang,  Yi Zhou,  Ashley Prater-Bennette,  Lixin Shen,  Shaofeng Zou</p>
  <p><b>备注</b>：We have corrected Theorem 1 in Sec 4 for AAAI 2024 version, where the order of $n_z$ changes from ${\epsilon}^{-k_*} )$ to ${\epsilon}^{-2k_*-2}$</p>
  <p><b>关键词</b>：Distributionally robust optimization, training robust models, data distribution shifts, constrained DRO, Distributionally robust</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Distributionally robust optimization (DRO) is a powerful framework for training robust models against data distribution shifts. This paper focuses on constrained DRO, which has an explicit characterization of the robustness level. Existing studies on constrained DRO mostly focus on convex loss function, and exclude the practical and challenging case with non-convex loss function, e.g., neural network. This paper develops a stochastic algorithm and its performance analysis for non-convex constrained DRO. The computational complexity of our stochastic algorithm at each iteration is independent of the overall dataset size, and thus is suitable for large-scale applications. We focus on the general Cressie-Read family divergence defined uncertainty set which includes $\chi^2$-divergences as a special case. We prove that our algorithm finds an $\epsilon$-stationary point with a computational complexity of $\mathcal O(\epsilon^{-3k_*-5})$, where $k_*$ is the parameter of the Cressie-Read divergence. The numerical results indicate that our method outperforms existing methods.} Our method also applies to the smoothed conditional value at risk (CVaR) DRO.</p>
  </details>
</details>
<details>
  <summary>173. <b>标题：TransFusion: Covariate-Shift Robust Transfer Learning for  High-Dimensional Regression</b></summary>
  <p><b>编号</b>：[692]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01153">https://arxiv.org/abs/2404.01153</a></p>
  <p><b>作者</b>：Zelin He,  Ying Sun,  Jingyuan Liu,  Runze Li</p>
  <p><b>备注</b>：Accepted by the 27th International Conference on Artificial Intelligence and Statistics (AISTATS 2024)</p>
  <p><b>关键词</b>：marginal covariate distributions, sets transfer learning, traditional supervised learning, covariate shifts, main challenge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The main challenge that sets transfer learning apart from traditional supervised learning is the distribution shift, reflected as the shift between the source and target models and that between the marginal covariate distributions. In this work, we tackle model shifts in the presence of covariate shifts in the high-dimensional regression setting. Specifically, we propose a two-step method with a novel fused-regularizer that effectively leverages samples from source tasks to improve the learning performance on a target task with limited samples. Nonasymptotic bound is provided for the estimation error of the target model, showing the robustness of the proposed method to covariate shifts. We further establish conditions under which the estimator is minimax-optimal. Additionally, we extend the method to a distributed setting, allowing for a pretraining-finetuning strategy, requiring just one round of communication while retaining the estimation rate of the centralized version. Numerical tests validate our theory, highlighting the method's robustness to covariate shifts.</p>
  </details>
</details>
<details>
  <summary>174. <b>标题：Diffusion based Zero-shot Medical Image-to-Image Translation for Cross  Modality Segmentation</b></summary>
  <p><b>编号</b>：[693]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01102">https://arxiv.org/abs/2404.01102</a></p>
  <p><b>作者</b>：Zihao Wang,  Yingyu Yang,  Yuzhou Chen,  Tingting Yuan,  Maxime Sermesant,  Herve Delingette</p>
  <p><b>备注</b>：Neurips 2023 Diffusion Workshop</p>
  <p><b>关键词</b>：Cross-modality image segmentation, Cross-modality image, image segmentation, image, zero-shot cross-modality image</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cross-modality image segmentation aims to segment the target modalities using a method designed in the source modality. Deep generative models can translate the target modality images into the source modality, thus enabling cross-modality segmentation. However, a vast body of existing cross-modality image translation methods relies on supervised learning. In this work, we aim to address the challenge of zero-shot learning-based image translation tasks (extreme scenarios in the target modality is unseen in the training phase). To leverage generative learning for zero-shot cross-modality image segmentation, we propose a novel unsupervised image translation method. The framework learns to translate the unseen source image to the target modality for image segmentation by leveraging the inherent statistical consistency between different modalities for diffusion guidance. Our framework captures identical cross-modality features in the statistical domain, offering diffusion guidance without relying on direct mappings between the source and target domains. This advantage allows our method to adapt to changing source domains without the need for retraining, making it highly practical when sufficient labeled source domain data is not available. The proposed framework is validated in zero-shot cross-modality image segmentation tasks through empirical comparisons with influential generative models, including adversarial-based and diffusion-based models.</p>
  </details>
</details>
<details>
  <summary>175. <b>标题：A Novel Sector-Based Algorithm for an Optimized Star-Galaxy  Classification</b></summary>
  <p><b>编号</b>：[695]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01049">https://arxiv.org/abs/2404.01049</a></p>
  <p><b>作者</b>：Anumanchi Agastya Sai Ram Likhit,  Divyansh Tripathi,  Akshay Agarwal</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：latest Sloan Digital, Sloan Digital Sky, Digital Sky Survey, Sky Survey data, Sloan Digital</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper introduces a novel sector-based methodology for star-galaxy classification, leveraging the latest Sloan Digital Sky Survey data (SDSS-DR18). By strategically segmenting the sky into sectors aligned with SDSS observational patterns and employing a dedicated convolutional neural network (CNN), we achieve state-of-the-art performance for star galaxy classification. Our preliminary results demonstrate a promising pathway for efficient and precise astronomical analysis, especially in real-time observational settings.</p>
  </details>
</details>
<details>
  <summary>176. <b>标题：Automated HER2 Scoring in Breast Cancer Images Using Deep Learning and  Pyramid Sampling</b></summary>
  <p><b>编号</b>：[705]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00837">https://arxiv.org/abs/2404.00837</a></p>
  <p><b>作者</b>：Sahan Yoruc Selcuk,  Xilin Yang,  Bijie Bai,  Yijie Zhang,  Yuzhu Li,  Musa Aydin,  Aras Firat Unal,  Aditya Gomatam,  Zhen Guo,  Darrow Morgan Angus,  Goren Kolodney,  Karine Atlan,  Tal Keidar Haran,  Nir Pillar,  Aydogan Ozcan</p>
  <p><b>备注</b>：21 Pages, 7 Figures</p>
  <p><b>关键词</b>：Human epidermal growth, growth factor receptor, epidermal growth factor, cancer cell growth, epidermal growth</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Human epidermal growth factor receptor 2 (HER2) is a critical protein in cancer cell growth that signifies the aggressiveness of breast cancer (BC) and helps predict its prognosis. Accurate assessment of immunohistochemically (IHC) stained tissue slides for HER2 expression levels is essential for both treatment guidance and understanding of cancer mechanisms. Nevertheless, the traditional workflow of manual examination by board-certified pathologists encounters challenges, including inter- and intra-observer inconsistency and extended turnaround times. Here, we introduce a deep learning-based approach utilizing pyramid sampling for the automated classification of HER2 status in IHC-stained BC tissue images. Our approach analyzes morphological features at various spatial scales, efficiently managing the computational load and facilitating a detailed examination of cellular and larger-scale tissue-level details. This method addresses the tissue heterogeneity of HER2 expression by providing a comprehensive view, leading to a blind testing classification accuracy of 84.70%, on a dataset of 523 core images from tissue microarrays. Our automated system, proving reliable as an adjunct pathology tool, has the potential to enhance diagnostic precision and evaluation speed, and might significantly impact cancer treatment planning.</p>
  </details>
</details>
<details>
  <summary>177. <b>标题：C-XGBoost: A tree boosting model for causal effect estimation</b></summary>
  <p><b>编号</b>：[708]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00751">https://arxiv.org/abs/2404.00751</a></p>
  <p><b>作者</b>：Niki Kiriakidou,  Ioannis E. Livieris,  Christos Diou</p>
  <p><b>备注</b>：This paper has been accepted for presentation at IFIP International Conference on Artificial Intelligence Applications and Innovations</p>
  <p><b>关键词</b>：Average Treatment Effect, Conditional Average Treatment, effect estimation aims, Causal effect estimation, Conditional Average</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Causal effect estimation aims at estimating the Average Treatment Effect as well as the Conditional Average Treatment Effect of a treatment to an outcome from the available data. This knowledge is important in many safety-critical domains, where it often needs to be extracted from observational data. In this work, we propose a new causal inference model, named C-XGBoost, for the prediction of potential outcomes. The motivation of our approach is to exploit the superiority of tree-based models for handling tabular data together with the notable property of causal inference neural network-based models to learn representations that are useful for estimating the outcome for both the treatment and non-treatment cases. The proposed model also inherits the considerable advantages of XGBoost model such as efficiently handling features with missing values requiring minimum preprocessing effort, as well as it is equipped with regularization techniques to avoid overfitting/bias. Furthermore, we propose a new loss function for efficiently training the proposed causal inference model. The experimental analysis, which is based on the performance profiles of Dolan and Mor{é} as well as on post-hoc and non-parametric statistical tests, provide strong evidence about the effectiveness of the proposed approach.</p>
  </details>
</details>
<details>
  <summary>178. <b>标题：MugenNet: A Novel Combined Convolution Neural Network and Transformer  Network with its Application for Colonic Polyp Image Segmentation</b></summary>
  <p><b>编号</b>：[709]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00726">https://arxiv.org/abs/2404.00726</a></p>
  <p><b>作者</b>：Chen Peng,  Zhiqin Qian,  Kunyu Wang,  Qi Luo,  Zhuming Bi,  Wenjun Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：polyp image segmentation, disease diagnosis, important part, part in disease, image segmentation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Biomedical image segmentation is a very important part in disease diagnosis. The term "colonic polyps" refers to polypoid lesions that occur on the surface of the colonic mucosa within the intestinal lumen. In clinical practice, early detection of polyps is conducted through colonoscopy examinations and biomedical image processing. Therefore, the accurate polyp image segmentation is of great significance in colonoscopy examinations. Convolutional Neural Network (CNN) is a common automatic segmentation method, but its main disadvantage is the long training time. Transformer utilizes a self-attention mechanism, which essentially assigns different importance weights to each piece of information, thus achieving high computational efficiency during segmentation. However, a potential drawback is the risk of information loss. In the study reported in this paper, based on the well-known hybridization principle, we proposed a method to combine CNN and Transformer to retain the strengths of both, and we applied this method to build a system called MugenNet for colonic polyp image segmentation. We conducted a comprehensive experiment to compare MugenNet with other CNN models on five publicly available datasets. The ablation experiment on MugentNet was conducted as well. The experimental results show that MugenNet achieves significantly higher processing speed and accuracy compared with CNN alone. The generalized implication with our work is a method to optimally combine two complimentary methods of machine learning.</p>
  </details>
</details>
<details>
  <summary>179. <b>标题：Convergence of Continuous Normalizing Flows for Learning Probability  Distributions</b></summary>
  <p><b>编号</b>：[714]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00551">https://arxiv.org/abs/2404.00551</a></p>
  <p><b>作者</b>：Yuan Gao,  Jian Huang,  Yuling Jiao,  Shurong Zheng</p>
  <p><b>备注</b>：60 pages, 3 tables, and 3 figures</p>
  <p><b>关键词</b>：ordinary differential equations, Continuous normalizing flows, learning probability distributions, Continuous normalizing, differential equations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Continuous normalizing flows (CNFs) are a generative method for learning probability distributions, which is based on ordinary differential equations. This method has shown remarkable empirical success across various applications, including large-scale image synthesis, protein structure prediction, and molecule generation. In this work, we study the theoretical properties of CNFs with linear interpolation in learning probability distributions from a finite random sample, using a flow matching objective function. We establish non-asymptotic error bounds for the distribution estimator based on CNFs, in terms of the Wasserstein-2 distance. The key assumption in our analysis is that the target distribution satisfies one of the following three conditions: it either has a bounded support, is strongly log-concave, or is a finite or infinite mixture of Gaussian distributions. We present a convergence analysis framework that encompasses the error due to velocity estimation, the discretization error, and the early stopping error. A key step in our analysis involves establishing the regularity properties of the velocity field and its estimator for CNFs constructed with linear interpolation. This necessitates the development of uniform error bounds with Lipschitz regularity control of deep ReLU networks that approximate the Lipschitz function class, which could be of independent interest. Our nonparametric convergence analysis offers theoretical guarantees for using CNFs to learn probability distributions from a finite random sample.</p>
  </details>
</details>
<details>
  <summary>180. <b>标题：Unified, Verifiable Neural Simulators for Electromagnetic Wave Inverse  Problems</b></summary>
  <p><b>编号</b>：[716]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00545">https://arxiv.org/abs/2404.00545</a></p>
  <p><b>作者</b>：Charles Dove,  Jatearoon Boondicharern,  Laura Waller</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：faster electromagnetic wave, electromagnetic wave simulations, neural networks offer, faster electromagnetic, networks offer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Simulators based on neural networks offer a path to orders-of-magnitude faster electromagnetic wave simulations. Existing models, however, only address narrowly tailored classes of problems and only scale to systems of a few dozen degrees of freedom (DoFs). Here, we demonstrate a single, unified model capable of addressing scattering simulations with thousands of DoFs, of any wavelength, any illumination wavefront, and freeform materials, within broad configurable bounds. Based on an attentional multi-conditioning strategy, our method also allows non-recurrent supervision on and prediction of intermediate physical states, which provides improved generalization with no additional data-generation cost. Using this O(1)-time intermediate prediction capability, we propose and prove a rigorous, efficiently computable upper bound on prediction error, allowing accuracy guarantees at inference time for all predictions. After training solely on randomized systems, we demonstrate the unified model across a suite of challenging multi-disciplinary inverse problems, finding strong efficacy and speed improvements up to 96% for problems in optical tomography, beam shaping through volumetric random media, and freeform photonic inverse design, with no problem-specific training. Our findings demonstrate a path to universal, verifiably accurate neural surrogates for existing scattering simulators, and our conditioning and training methods are directly applicable to any PDE admitting a time-domain iterative solver.</p>
  </details>
</details>
<details>
  <summary>181. <b>标题：Convolutional Bayesian Filtering</b></summary>
  <p><b>编号</b>：[719]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00481">https://arxiv.org/abs/2404.00481</a></p>
  <p><b>作者</b>：Wenhan Cao,  Shiqi Liu,  Chang Liu,  Zeyu He,  Stephen S.-T. Yau,  Shengbo Eben Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Bayesian filtering serves, probability, dynamic systems, conditional probability, estimation in dynamic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Bayesian filtering serves as the mainstream framework of state estimation in dynamic systems. Its standard version utilizes total probability rule and Bayes' law alternatively, where how to define and compute conditional probability is critical to state distribution inference. Previously, the conditional probability is assumed to be exactly known, which represents a measure of the occurrence probability of one event, given the second event. In this paper, we find that by adding an additional event that stipulates an inequality condition, we can transform the conditional probability into a special integration that is analogous to convolution. Based on this transformation, we show that both transition probability and output probability can be generalized to convolutional forms, resulting in a more general filtering framework that we call convolutional Bayesian filtering. This new framework encompasses standard Bayesian filtering as a special case when the distance metric of the inequality condition is selected as Dirac delta function. It also allows for a more nuanced consideration of model mismatch by choosing different types of inequality conditions. For instance, when the distance metric is defined in a distributional sense, the transition probability and output probability can be approximated by simply rescaling them into fractional powers. Under this framework, a robust version of Kalman filter can be constructed by only altering the noise covariance matrix, while maintaining the conjugate nature of Gaussian distributions. Finally, we exemplify the effectiveness of our approach by reshaping classic filtering algorithms into convolutional versions, including Kalman filter, extended Kalman filter, unscented Kalman filter and particle filter.</p>
  </details>
</details>
<details>
  <summary>182. <b>标题：Score-Based Diffusion Models for Photoacoustic Tomography Image  Reconstruction</b></summary>
  <p><b>编号</b>：[720]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00471">https://arxiv.org/abs/2404.00471</a></p>
  <p><b>作者</b>：Sreemanti Dey,  Snigdha Saha,  Berthy T. Feng,  Manxiu Cui,  Laure Delisle,  Oscar Leong,  Lihong V. Wang,  Katherine L. Bouman</p>
  <p><b>备注</b>：5 pages</p>
  <p><b>关键词</b>：ultrasound imaging depth, rapidly-evolving medical imaging, medical imaging modality, combines optical absorption, optical absorption contrast</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Photoacoustic tomography (PAT) is a rapidly-evolving medical imaging modality that combines optical absorption contrast with ultrasound imaging depth. One challenge in PAT is image reconstruction with inadequate acoustic signals due to limited sensor coverage or due to the density of the transducer array. Such cases call for solving an ill-posed inverse reconstruction problem. In this work, we use score-based diffusion models to solve the inverse problem of reconstructing an image from limited PAT measurements. The proposed approach allows us to incorporate an expressive prior learned by a diffusion model on simulated vessel structures while still being robust to varying transducer sparsity conditions.</p>
  </details>
</details>
<details>
  <summary>183. <b>标题：Language Models are Spacecraft Operators</b></summary>
  <p><b>编号</b>：[722]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00413">https://arxiv.org/abs/2404.00413</a></p>
  <p><b>作者</b>：Victor Rodriguez-Fernandez,  Alejandro Carrasco,  Jason Cheng,  Eli Scharf,  Peng Mun Siew,  Richard Linares</p>
  <p><b>备注</b>：Source code available on Github at: this https URL</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, user text prompts, Recent trends</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent trends are emerging in the use of Large Language Models (LLMs) as autonomous agents that take actions based on the content of the user text prompts. We intend to apply these concepts to the field of Guidance, Navigation, and Control in space, enabling LLMs to have a significant role in the decision-making process for autonomous satellite operations. As a first step towards this goal, we have developed a pure LLM-based solution for the Kerbal Space Program Differential Games (KSPDG) challenge, a public software design competition where participants create autonomous agents for maneuvering satellites involved in non-cooperative space operations, running on the KSP game engine. Our approach leverages prompt engineering, few-shot prompting, and fine-tuning techniques to create an effective LLM-based agent that ranked 2nd in the competition. To the best of our knowledge, this work pioneers the integration of LLM agents into space research. Code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>184. <b>标题：Aardvark Weather: end-to-end data-driven weather forecasting</b></summary>
  <p><b>编号</b>：[723]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00411">https://arxiv.org/abs/2404.00411</a></p>
  <p><b>作者</b>：Anna Vaughan,  Stratis Markou,  Will Tebbutt,  James Requeima,  Wessel P. Bruinsma,  Tom R. Andersson,  Michael Herzog,  Nicholas D. Lane,  J. Scott Hosking,  Richard E. Turner</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：weather prediction, revolutionising medium-range weather, weather prediction pipeline, medium-range weather prediction, weather</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine learning is revolutionising medium-range weather prediction. However it has only been applied to specific and individual components of the weather prediction pipeline. Consequently these data-driven approaches are unable to be deployed without input from conventional operational numerical weather prediction (NWP) systems, which is computationally costly and does not support end-to-end optimisation. In this work, we take a radically different approach and replace the entire NWP pipeline with a machine learning model. We present Aardvark Weather, the first end-to-end data-driven forecasting system which takes raw observations as input and provides both global and local forecasts. These global forecasts are produced for 24 variables at multiple pressure levels at one-degree spatial resolution and 24 hour temporal resolution, and are skillful with respect to hourly climatology at five to seven day lead times. Local forecasts are produced for temperature, mean sea level pressure, and wind speed at a geographically diverse set of weather stations, and are skillful with respect to an IFS-HRES interpolation baseline at multiple lead-times. Aardvark, by virtue of its simplicity and scalability, opens the door to a new paradigm for performing accurate and efficient data-driven medium-range weather forecasting.</p>
  </details>
</details>
<details>
  <summary>185. <b>标题：Learning truly monotone operators with applications to nonlinear inverse  problems</b></summary>
  <p><b>编号</b>：[724]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00390">https://arxiv.org/abs/2404.00390</a></p>
  <p><b>作者</b>：Younes Belkouchi,  Jean-Christophe Pesquet,  Audrey Repetti,  Hugues Talbot</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：defined penalization loss, newly defined penalization, learning monotone neural, penalization loss, article introduces</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This article introduces a novel approach to learning monotone neural networks through a newly defined penalization loss. The proposed method is particularly effective in solving classes of variational problems, specifically monotone inclusion problems, commonly encountered in image processing tasks. The Forward-Backward-Forward (FBF) algorithm is employed to address these problems, offering a solution even when the Lipschitz constant of the neural network is unknown. Notably, the FBF algorithm provides convergence guarantees under the condition that the learned operator is monotone. Building on plug-and-play methodologies, our objective is to apply these newly learned operators to solving non-linear inverse problems. To achieve this, we initially formulate the problem as a variational inclusion problem. Subsequently, we train a monotone neural network to approximate an operator that may not inherently be monotone. Leveraging the FBF algorithm, we then show simulation examples where the non-linear inverse problem is successfully solved.</p>
  </details>
</details>
<details>
  <summary>186. <b>标题：YNetr: Dual-Encoder architecture on Plain Scan Liver Tumors (PSLT)</b></summary>
  <p><b>编号</b>：[725]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00327">https://arxiv.org/abs/2404.00327</a></p>
  <p><b>作者</b>：Wen Sheng,  Zhong Zheng,  Jiajun Liu,  Han Lu,  Hanyuan Zhang,  Zhengyong Jiang,  Zhihong Zhang,  Daoping Zhu</p>
  <p><b>备注</b>：15 pages</p>
  <p><b>关键词</b>：health concern worldwide, Scan Liver Tumors, Plain Scan Liver, significant health concern, plain scan</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Background: Liver tumors are abnormal growths in the liver that can be either benign or malignant, with liver cancer being a significant health concern worldwide. However, there is no dataset for plain scan segmentation of liver tumors, nor any related algorithms. To fill this gap, we propose Plain Scan Liver Tumors(PSLT) and YNetr. Methods: A collection of 40 liver tumor plain scan segmentation datasets was assembled and annotated. Concurrently, we utilized Dice coefficient as the metric for assessing the segmentation outcomes produced by YNetr, having advantage of capturing different frequency information. Results: The YNetr model achieved a Dice coefficient of 62.63% on the PSLT dataset, surpassing the other publicly available model by an accuracy margin of 1.22%. Comparative evaluations were conducted against a range of models including UNet 3+, XNet, UNetr, Swin UNetr, Trans-BTS, COTr, nnUNetv2 (2D), nnUNetv2 (3D fullres), MedNext (2D) and MedNext(3D fullres). Conclusions: We not only proposed a dataset named PSLT(Plain Scan Liver Tumors), but also explored a structure called YNetr that utilizes wavelet transform to extract different frequency information, which having the SOTA in PSLT by experiments.</p>
  </details>
</details>
<details>
  <summary>187. <b>标题：Partially-Observable Sequential Change-Point Detection for  Autocorrelated Data via Upper Confidence Region</b></summary>
  <p><b>编号</b>：[729]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00220">https://arxiv.org/abs/2404.00220</a></p>
  <p><b>作者</b>：Haijie Xu,  Xiaochen Xian,  Chen Zhang,  Kaibo Liu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：change point detection, Sequential change point, change point, point detection, point</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sequential change point detection for multivariate autocorrelated data is a very common problem in practice. However, when the sensing resources are limited, only a subset of variables from the multivariate system can be observed at each sensing time point. This raises the problem of partially observable multi-sensor sequential change point detection. For it, we propose a detection scheme called adaptive upper confidence region with state space model (AUCRSS). It models multivariate time series via a state space model (SSM), and uses an adaptive sampling policy for efficient change point detection and localization. A partially-observable Kalman filter algorithm is developed for online inference of SSM, and accordingly, a change point detection scheme based on a generalized likelihood ratio test is developed. How its detection power relates to the adaptive sampling strategy is analyzed. Meanwhile, by treating the detection power as a reward, its connection with the online combinatorial multi-armed bandit (CMAB) problem is formulated and an adaptive upper confidence region algorithm is proposed for adaptive sampling policy design. Theoretical analysis of the asymptotic average detection delay is performed, and thorough numerical studies with synthetic data and real-world data are conducted to demonstrate the effectiveness of our method.</p>
  </details>
</details>
<details>
  <summary>188. <b>标题：Functional-Edged Network Modeling</b></summary>
  <p><b>编号</b>：[730]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00218">https://arxiv.org/abs/2404.00218</a></p>
  <p><b>作者</b>：Haijie Xu,  Chen Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Contrasts with existing, existing works, represent the relationships, functional adjacency tensor, functional</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Contrasts with existing works which all consider nodes as functions and use edges to represent the relationships between different functions. We target at network modeling whose edges are functional data and transform the adjacency matrix into a functional adjacency tensor, introducing an additional dimension dedicated to function representation. Tucker functional decomposition is used for the functional adjacency tensor, and to further consider the community between nodes, we regularize the basis matrices to be symmetrical. Furthermore, to deal with irregular observations of the functional edges, we conduct model inference to solve a tensor completion problem. It is optimized by a Riemann conjugate gradient descent method. Besides these, we also derive several theorems to show the desirable properties of the functional edged network model. Finally, we evaluate the efficacy of our proposed model using simulation data and real metro system data from Hong Kong and Singapore.</p>
  </details>
</details>
<details>
  <summary>189. <b>标题：Beyond Suspension: A Two-phase Methodology for Concluding Sports Leagues</b></summary>
  <p><b>编号</b>：[731]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00178">https://arxiv.org/abs/2404.00178</a></p>
  <p><b>作者</b>：Ali Hassanzadeh,  Mojtaba Hosseini,  John G. Turner</p>
  <p><b>备注</b>：32 pages, 9 figures</p>
  <p><b>关键词</b>：Professional sports leagues, Professional sports, suspended due, sports leagues, season</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Problem definition: Professional sports leagues may be suspended due to various reasons such as the recent COVID-19 pandemic. A critical question the league must address when re-opening is how to appropriately select a subset of the remaining games to conclude the season in a shortened time frame. Academic/practical relevance: Despite the rich literature on scheduling an entire season starting from a blank slate, concluding an existing season is quite different. Our approach attempts to achieve team rankings similar to that which would have resulted had the season been played out in full. Methodology: We propose a data-driven model which exploits predictive and prescriptive analytics to produce a schedule for the remainder of the season comprised of a subset of originally-scheduled games. Our model introduces novel rankings-based objectives within a stochastic optimization model, whose parameters are first estimated using a predictive model. We introduce a deterministic equivalent reformulation along with a tailored Frank-Wolfe algorithm to efficiently solve our problem, as well as a robust counterpart based on min-max regret. Results: We present simulation-based numerical experiments from previous National Basketball Association (NBA) seasons 2004--2019, and show that our models are computationally efficient, outperform a greedy benchmark that approximates a non-rankings-based scheduling policy, and produce interpretable results. Managerial implications: Our data-driven decision-making framework may be used to produce a shortened season with 25-50\% fewer games while still producing an end-of-season ranking similar to that of the full season, had it been played.</p>
  </details>
</details>
<details>
  <summary>190. <b>标题：Fully Zeroth-Order Bilevel Programming via Gaussian Smoothing</b></summary>
  <p><b>编号</b>：[732]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00158">https://arxiv.org/abs/2404.00158</a></p>
  <p><b>作者</b>：Alireza Aghasi,  Saeed Ghadimi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：unbiased gradient estimates, lower objective, study and analyze, unbiased gradient, exploiting Stein identity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we study and analyze zeroth-order stochastic approximation algorithms for solving bilvel problems, when neither the upper/lower objective values, nor their unbiased gradient estimates are available. In particular, exploiting Stein's identity, we first use Gaussian smoothing to estimate first- and second-order partial derivatives of functions with two independent block of variables. We then used these estimates in the framework of a stochastic approximation algorithm for solving bilevel optimization problems and establish its non-asymptotic convergence analysis. To the best of our knowledge, this is the first time that sample complexity bounds are established for a fully stochastic zeroth-order bilevel optimization algorithm.</p>
  </details>
</details>
<details>
  <summary>191. <b>标题：Verifying the Selected Completely at Random Assumption in  Positive-Unlabeled Learning</b></summary>
  <p><b>编号</b>：[733]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00145">https://arxiv.org/abs/2404.00145</a></p>
  <p><b>作者</b>：Paweł Teisseyre,  Konrad Furmańczyk,  Jan Mielniczuk</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：unlabeled instances, negative class, positive class, goal of positive-unlabeled, unlabeled observations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The goal of positive-unlabeled (PU) learning is to train a binary classifier on the basis of training data containing positive and unlabeled instances, where unlabeled observations can belong either to the positive class or to the negative class. Modeling PU data requires certain assumptions on the labeling mechanism that describes which positive observations are assigned a label. The simplest assumption, considered in early works, is SCAR (Selected Completely at Random Assumption), according to which the propensity score function, defined as the probability of assigning a label to a positive observation, is constant. On the other hand, a much more realistic assumption is SAR (Selected at Random), which states that the propensity function solely depends on the observed feature vector. SCAR-based algorithms are much simpler and computationally much faster compared to SAR-based algorithms, which usually require challenging estimation of the propensity score. In this work, we propose a relatively simple and computationally fast test that can be used to determine whether the observed data meet the SCAR assumption. Our test is based on generating artificial labels conforming to the SCAR case, which in turn allows to mimic the distribution of the test statistic under the null hypothesis of SCAR. We justify our method theoretically. In experiments, we demonstrate that the test successfully detects various deviations from SCAR scenario and at the same time it is possible to effectively control the type I error. The proposed test can be recommended as a pre-processing step to decide which final PU algorithm to choose in cases when nature of labeling mechanism is not known.</p>
  </details>
</details>
<details>
  <summary>192. <b>标题：Data-Driven Room Acoustic Modeling Via Differentiable Feedback Delay  Networks With Learnable Delay Lines</b></summary>
  <p><b>编号</b>：[739]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00082">https://arxiv.org/abs/2404.00082</a></p>
  <p><b>作者</b>：Alessandro Ilic Mezza,  Riccardo Giampiccolo,  Enzo De Sena,  Alberto Bernardini</p>
  <p><b>备注</b>：The article has been submitted to EURASIP Journal on Audio, Speech, and Music Processing on Jan 02, 2024 and is currently under review</p>
  <p><b>关键词</b>：artificial reverberation algorithms, reverberation algorithms aimed, past few decades, extensive research, physical environments</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Over the past few decades, extensive research has been devoted to the design of artificial reverberation algorithms aimed at emulating the room acoustics of physical environments. Despite significant advancements, automatic parameter tuning of delay-network models remains an open challenge. We introduce a novel method for finding the parameters of a Feedback Delay Network (FDN) such that its output renders the perceptual qualities of a measured room impulse response. The proposed approach involves the implementation of a differentiable FDN with trainable delay lines, which, for the first time, allows us to simultaneously learn each and every delay-network parameter via backpropagation. The iterative optimization process seeks to minimize a time-domain loss function incorporating differentiable terms accounting for energy decay and echo density. Through experimental validation, we show that the proposed method yields time-invariant frequency-independent FDNs capable of closely matching the desired acoustical characteristics, and outperforms existing methods based on genetic algorithms and analytical filter design.</p>
  </details>
</details>
<details>
  <summary>193. <b>标题：Molecular Generative Adversarial Network with Multi-Property  Optimization</b></summary>
  <p><b>编号</b>：[740]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00081">https://arxiv.org/abs/2404.00081</a></p>
  <p><b>作者</b>：Huidong Tang,  Chen Li,  Sayaka Kamei,  Yoshihiro Yamanishi,  Yasuhiko Morimoto</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：generative adversarial networks, Deep generative models, Deep generative, Monte Carlo tree, generative adversarial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep generative models, such as generative adversarial networks (GANs), have been employed for $de~novo$ molecular generation in drug discovery. Most prior studies have utilized reinforcement learning (RL) algorithms, particularly Monte Carlo tree search (MCTS), to handle the discrete nature of molecular representations in GANs. However, due to the inherent instability in training GANs and RL models, along with the high computational cost associated with MCTS sampling, MCTS RL-based GANs struggle to scale to large chemical databases. To tackle these challenges, this study introduces a novel GAN based on actor-critic RL with instant and global rewards, called InstGAN, to generate molecules at the token-level with multi-property optimization. Furthermore, maximized information entropy is leveraged to alleviate the mode collapse. The experimental results demonstrate that InstGAN outperforms other baselines, achieves comparable performance to state-of-the-art models, and efficiently generates molecules with multi-property optimization. The source code will be released upon acceptance of the paper.</p>
  </details>
</details>
<details>
  <summary>194. <b>标题：Temporal Graph Networks for Graph Anomaly Detection in Financial  Networks</b></summary>
  <p><b>编号</b>：[741]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00060">https://arxiv.org/abs/2404.00060</a></p>
  <p><b>作者</b>：Yejin Kim,  Youngbin Lee,  Minyoung Choe,  Sungju Oh,  Yongjae Lee</p>
  <p><b>备注</b>：Presented at the AAAI 2024 Workshop on AI in Finance for Social Impact (this https URL)</p>
  <p><b>关键词</b>：Temporal Graph Networks, utilization of Temporal, digitized financial transactions, Temporal Graph, Graph Neural Network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper explores the utilization of Temporal Graph Networks (TGN) for financial anomaly detection, a pressing need in the era of fintech and digitized financial transactions. We present a comprehensive framework that leverages TGN, capable of capturing dynamic changes in edges within financial networks, for fraud detection. Our study compares TGN's performance against static Graph Neural Network (GNN) baselines, as well as cutting-edge hypergraph neural network baselines using DGraph dataset for a realistic financial context. Our results demonstrate that TGN significantly outperforms other models in terms of AUC metrics. This superior performance underlines TGN's potential as an effective tool for detecting financial fraud, showcasing its ability to adapt to the dynamic and complex nature of modern financial systems. We also experimented with various graph embedding modules within the TGN framework and compared the effectiveness of each module. In conclusion, we demonstrated that, even with variations within TGN, it is possible to achieve good performance in the anomaly detection task.</p>
  </details>
</details>
<details>
  <summary>195. <b>标题：Grappa -- A Machine Learned Molecular Mechanics Force Field</b></summary>
  <p><b>编号</b>：[742]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00050">https://arxiv.org/abs/2404.00050</a></p>
  <p><b>作者</b>：Leif Seute,  Eric Hartmann,  Jan Stühmer,  Frauke Gräter</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：long timescales requires, timescales requires force, force fields, requires force fields, accurate and efficient</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Simulating large molecular systems over long timescales requires force fields that are both accurate and efficient. In recent years, E(3) equivariant neural networks have lifted the tension between computational efficiency and accuracy of force fields, but they are still several orders of magnitude more expensive than classical molecular mechanics (MM) force fields.
Here, we propose a novel machine learning architecture to predict MM parameters from the molecular graph, employing a graph attentional neural network and a transformer with symmetry-preserving positional encoding. The resulting force field, Grappa, outperforms established and other machine-learned MM force fields in terms of accuracy at the same computational efficiency and can be used in existing Molecular Dynamics (MD) engines like GROMACS and OpenMM. It predicts energies and forces of small molecules, peptides, RNA and - showcasing its extensibility to uncharted regions of chemical space - radicals at state-of-the-art MM accuracy. We demonstrate Grappa's transferability to macromolecules in MD simulations, during which large protein are kept stable and small proteins can fold. Our force field sets the stage for biomolecular simulations close to chemical accuracy, but with the same computational cost as established protein force fields.</p>
  </details>
</details>
<details>
  <summary>196. <b>标题：UAlign: Pushing the Limit of Template-free Retrosynthesis Prediction  with Unsupervised SMILES Alignment</b></summary>
  <p><b>编号</b>：[743]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00044">https://arxiv.org/abs/2404.00044</a></p>
  <p><b>作者</b>：Kaipeng Zeng,  Xin Zhao,  Yu Zhang,  Fan Nie,  Xiaokang Yang,  Yaohui Jin,  Yanyan Xu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：organic chemical industry, Retrosynthesis planning poses, poses a formidable, formidable challenge, retrosynthesis prediction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Retrosynthesis planning poses a formidable challenge in the organic chemical industry, particularly in pharmaceuticals. Single-step retrosynthesis prediction, a crucial step in the planning process, has witnessed a surge in interest in recent years due to advancements in AI for science. Various deep learning-based methods have been proposed for this task in recent years, incorporating diverse levels of additional chemical knowledge dependency. This paper introduces UAlign, a template-free graph-to-sequence pipeline for retrosynthesis prediction. By combining graph neural networks and Transformers, our method can more effectively leverage the inherent graph structure of molecules. Based on the fact that the majority of molecule structures remain unchanged during a chemical reaction, we propose a simple yet effective SMILES alignment technique to facilitate the reuse of unchanged structures for reactant generation. Extensive experiments show that our method substantially outperforms state-of-the-art template-free and semi-template-based approaches. Importantly, Our template-free method achieves effectiveness comparable to, or even surpasses, established powerful template-based methods. Scientific contribution: We present a novel graph-to-sequence template-free retrosynthesis prediction pipeline that overcomes the limitations of Transformer-based methods in molecular representation learning and insufficient utilization of chemical information. We propose an unsupervised learning mechanism for establishing product-atom correspondence with reactant SMILES tokens, achieving even better results than supervised SMILES alignment methods. Extensive experiments demonstrate that UAlign significantly outperforms state-of-the-art template-free methods and rivals or surpasses template-based approaches, with up to 5\% (top-5) and 5.4\% (top-10) increased accuracy over the strongest baseline.</p>
  </details>
</details>
<details>
  <summary>197. <b>标题：Stochastic Optimization with Constraints: A Non-asymptotic  Instance-Dependent Analysis</b></summary>
  <p><b>编号</b>：[744]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00042">https://arxiv.org/abs/2404.00042</a></p>
  <p><b>作者</b>：Koulik Khamaru</p>
  <p><b>备注</b>：18 pages</p>
  <p><b>关键词</b>：stochastic convex optimization, VRPG algorithm, VRPG, VRPG algorithm achieves, stochastic convex</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider the problem of stochastic convex optimization under convex constraints. We analyze the behavior of a natural variance reduced proximal gradient (VRPG) algorithm for this problem. Our main result is a non-asymptotic guarantee for VRPG algorithm. Contrary to minimax worst case guarantees, our result is instance-dependent in nature. This means that our guarantee captures the complexity of the loss function, the variability of the noise, and the geometry of the constraint set. We show that the non-asymptotic performance of the VRPG algorithm is governed by the scaled distance (scaled by $\sqrt{N}$) between the solutions of the given problem and that of a certain small perturbation of the given problem -- both solved under the given convex constraints; here, $N$ denotes the number of samples. Leveraging a well-established connection between local minimax lower bounds and solutions to perturbed problems, we show that as $N \rightarrow \infty$, the VRPG algorithm achieves the renowned local minimax lower bound by Hàjek and Le Cam up to universal constants and a logarithmic factor of the sample size.</p>
  </details>
</details>
<details>
  <summary>198. <b>标题：Investigating Similarities Across Decentralized Financial (DeFi)  Services</b></summary>
  <p><b>编号</b>：[745]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00034">https://arxiv.org/abs/2404.00034</a></p>
  <p><b>作者</b>：Junliang Luo,  Stefan Kitzler,  Pietro Saggese</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Decentralized Finance, graph representation learning, representation learning, algorithms to investigate, offered by Decentralized</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We explore the adoption of graph representation learning (GRL) algorithms to investigate similarities across services offered by Decentralized Finance (DeFi) protocols. Following existing literature, we use Ethereum transaction data to identify the DeFi building blocks. These are sets of protocol-specific smart contracts that are utilized in combination within single transactions and encapsulate the logic to conduct specific financial services such as swapping or lending cryptoassets. We propose a method to categorize these blocks into clusters based on their smart contract attributes and the graph structure of their smart contract calls. We employ GRL to create embedding vectors from building blocks and agglomerative models for clustering them. To evaluate whether they are effectively grouped in clusters of similar functionalities, we associate them with eight financial functionality categories and use this information as the target label. We find that in the best-case scenario purity reaches .888. We use additional information to associate the building blocks with protocol-specific target labels, obtaining comparable purity (.864) but higher V-Measure (.571); we discuss plausible explanations for this difference. In summary, this method helps categorize existing financial products offered by DeFi protocols, and can effectively automatize the detection of similar DeFi services, especially within protocols.</p>
  </details>
</details>
<details>
  <summary>199. <b>标题：Empowering Credit Scoring Systems with Quantum-Enhanced Machine Learning</b></summary>
  <p><b>编号</b>：[746]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00015">https://arxiv.org/abs/2404.00015</a></p>
  <p><b>作者</b>：Javier Mancilla,  André Sequeira,  Iraitz Montalbán,  Tomas Tagliani,  Frnacisco Llaneza,  Claudio Beiza</p>
  <p><b>备注</b>：Preprint</p>
  <p><b>关键词</b>：quantum machine learning, provide early-stage usefulness, Kernels are projected, machine learning, early-stage usefulness</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Quantum Kernels are projected to provide early-stage usefulness for quantum machine learning. However, highly sophisticated classical models are hard to surpass without losing interpretability, particularly when vast datasets can be exploited. Nonetheless, classical models struggle once data is scarce and skewed. Quantum feature spaces are projected to find better links between data features and the target class to be predicted even in such challenging scenarios and most importantly, enhanced generalization capabilities. In this work, we propose a novel approach called Systemic Quantum Score (SQS) and provide preliminary results indicating potential advantage over purely classical models in a production grade use case for the Finance sector. SQS shows in our specific study an increased capacity to extract patterns out of fewer data points as well as improved performance over data-hungry algorithms such as XGBoost, providing advantage in a competitive market as it is the FinTech and Neobank regime.</p>
  </details>
</details>
<h1>人工智能</h1>
<details>
  <summary>1. <b>标题：NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation  Learning for Neural Radiance Fields</b></summary>
  <p><b>编号</b>：[1]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01300">https://arxiv.org/abs/2404.01300</a></p>
  <p><b>作者</b>：Muhammad Zubair Irshad,  Sergey Zakahrov,  Vitor Guizilini,  Adrien Gaidon,  Zsolt Kira,  Rares Ambrus</p>
  <p><b>备注</b>：29 pages, 13 figures. Project Page: this https URL</p>
  <p><b>关键词</b>：Neural fields excel, Neural fields, visual world, excel in computer, ability to understand</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural fields excel in computer vision and robotics due to their ability to understand the 3D visual world such as inferring semantics, geometry, and dynamics. Given the capabilities of neural fields in densely representing a 3D scene from 2D images, we ask the question: Can we scale their self-supervised pretraining, specifically using masked autoencoders, to generate effective 3D representations from posed RGB images. Owing to the astounding success of extending transformers to novel data modalities, we employ standard 3D Vision Transformers to suit the unique formulation of NeRFs. We leverage NeRF's volumetric grid as a dense input to the transformer, contrasting it with other 3D representations such as pointclouds where the information density can be uneven, and the representation is irregular. Due to the difficulty of applying masked autoencoders to an implicit representation, such as NeRF, we opt for extracting an explicit representation that canonicalizes scenes across domains by employing the camera trajectory for sampling. Our goal is made possible by masking random patches from NeRF's radiance and density grid and employing a standard 3D Swin Transformer to reconstruct the masked patches. In doing so, the model can learn the semantic and spatial structure of complete scenes. We pretrain this representation at scale on our proposed curated posed-RGB data, totaling over 1.6 million images. Once pretrained, the encoder is used for effective 3D transfer learning. Our novel self-supervised pretraining for NeRFs, NeRF-MAE, scales remarkably well and improves performance on various challenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining, NeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF scene understanding baselines on Front3D and ScanNet datasets with an absolute performance improvement of over 20% AP50 and 8% AP25 for 3D object detection.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：CausalChaos! Dataset for Comprehensive Causal Action Question Answering  Over Longer Causal Chains Grounded in Dynamic Visual Scenes</b></summary>
  <p><b>编号</b>：[2]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01299">https://arxiv.org/abs/2404.01299</a></p>
  <p><b>作者</b>：Ting En Lam,  Yuhan Chen,  Elston Tan,  Eric Peh,  Ruirui Chen,  Paritosh Parmar,  Basura Fernando</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：garnered increasing interest, video question answering, causal reasoning analysis, increasing interest, reasoning analysis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Causal video question answering (QA) has garnered increasing interest, yet existing datasets often lack depth in causal reasoning analysis. To address this gap, we capitalize on the unique properties of cartoons and construct CausalChaos!, a novel, challenging causal Why-QA dataset built upon the iconic "Tom and Jerry" cartoon series. With thoughtful questions and multi-level answers, our dataset contains much longer causal chains embedded in dynamic interactions and visuals, at the same time principles of animation allows animators to create well-defined, unambiguous causal relationships. These factors allow models to solve more challenging, yet well-defined causal relationships. We also introduce hard negative mining, including CausalConfusion version. While models perform well, there is much room for improvement, especially, on open-ended answers. We identify more advanced/explicit causal relationship modeling and joint modeling of vision and language as the immediate areas for future efforts to focus upon. Along with the other complementary datasets, our new challenging dataset will pave the way for these developments in the field. We will release our dataset, codes, and models to help future efforts in this domain.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Towards Safety and Helpfulness Balanced Responses via Controllable Large  Language Models</b></summary>
  <p><b>编号</b>：[6]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01295">https://arxiv.org/abs/2404.01295</a></p>
  <p><b>作者</b>：Yi-Lin Tuan,  Xilun Chen,  Eric Michael Smith,  Louis Martin,  Soumya Batra,  Asli Celikyilmaz,  William Yang Wang,  Daniel M. Bikel</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：easily accessible nowadays, impact user experience, significantly impact user, large language models, accessible nowadays</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As large language models (LLMs) become easily accessible nowadays, the trade-off between safety and helpfulness can significantly impact user experience. A model that prioritizes safety will cause users to feel less engaged and assisted while prioritizing helpfulness will potentially cause harm. Possible harms include teaching people how to build a bomb, exposing youth to inappropriate content, and hurting users' mental health. In this work, we propose to balance safety and helpfulness in diverse use cases by controlling both attributes in LLM. We explore training-free and fine-tuning methods that do not require extra human annotations and analyze the challenges of controlling safety and helpfulness in LLMs. Our experiments demonstrate that our method can rewind a learned model and unlock its controllability.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Evaluating Text-to-Visual Generation with Image-to-Text Generation</b></summary>
  <p><b>编号</b>：[9]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01291">https://arxiv.org/abs/2404.01291</a></p>
  <p><b>作者</b>：Zhiqiu Lin,  Deepak Pathak,  Baiqi Li,  Jiayao Li,  Xide Xia,  Graham Neubig,  Pengchuan Zhang,  Deva Ramanan</p>
  <p><b>备注</b>：We open-source our data, model, and code at: this https URL ; Project page: this https URL</p>
  <p><b>关键词</b>：comprehensive evaluation remains, evaluation remains challenging, comprehensive evaluation, significant progress, progress in generative</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite significant progress in generative AI, comprehensive evaluation remains challenging because of the lack of effective metrics and standardized benchmarks. For instance, the widely-used CLIPScore measures the alignment between a (generated) image and text prompt, but it fails to produce reliable scores for complex prompts involving compositions of objects, attributes, and relations. One reason is that text encoders of CLIP can notoriously act as a "bag of words", conflating prompts such as "the horse is eating the grass" with "the grass is eating the horse". To address this, we introduce the VQAScore, which uses a visual-question-answering (VQA) model to produce an alignment score by computing the probability of a "Yes" answer to a simple "Does this figure show '{text}'?" question. Though simpler than prior art, VQAScore computed with off-the-shelf models produces state-of-the-art results across many (8) image-text alignment benchmarks. We also compute VQAScore with an in-house model that follows best practices in the literature. For example, we use a bidirectional image-question encoder that allows image embeddings to depend on the question being asked (and vice versa). Our in-house model, CLIP-FlanT5, outperforms even the strongest baselines that make use of the proprietary GPT-4V. Interestingly, although we train with only images, VQAScore can also align text with video and 3D models. VQAScore allows researchers to benchmark text-to-visual generation using complex texts that capture the compositional structure of real-world prompts. We introduce GenAI-Bench, a more challenging benchmark with 1,600 compositional text prompts that require parsing scenes, objects, attributes, relationships, and high-order reasoning like comparison and logic. GenAI-Bench also offers over 15,000 human ratings for leading image and video generation models such as Stable Diffusion, DALL-E 3, and Gen2.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Mapping the Increasing Use of LLMs in Scientific Papers</b></summary>
  <p><b>编号</b>：[19]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01268">https://arxiv.org/abs/2404.01268</a></p>
  <p><b>作者</b>：Weixin Liang,  Yaohui Zhang,  Zhengxuan Wu,  Haley Lepp,  Wenlong Ji,  Xuandong Zhao,  Hancheng Cao,  Sheng Liu,  Siyu He,  Zhi Huang,  Diyi Yang,  Christopher Potts,  Christopher D Manning,  James Y. Zou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Scientific publishing lays, fostering collaboration, encouraging reproducibility, knowledge is accessible, publishing lays</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time. Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices. However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs. To address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time. Our statistical estimation operates on the corpus level and is more robust than inference on individual instances. Our findings reveal a steady increase in LLM usage, with the largest and fastest growth observed in Computer Science papers (up to 17.5%). In comparison, Mathematics papers and the Nature portfolio showed the least LLM modification (up to 6.3%). Moreover, at an aggregate level, our analysis reveals that higher levels of LLM-modification are associated with papers whose first authors post preprints more frequently, papers in more crowded research areas, and papers of shorter lengths. Our findings suggests that LLMs are being broadly used in scientific writings.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic  Representations</b></summary>
  <p><b>编号</b>：[20]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01266">https://arxiv.org/abs/2404.01266</a></p>
  <p><b>作者</b>：Deqing Fu,  Ghazal Khalighinejad,  Ollie Liu,  Bhuwan Dhingra,  Dani Yogatama,  Robin Jia,  Willie Neiswanger</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：exhibit impressive capabilities, models exhibit impressive, exhibit impressive, impressive capabilities, Current foundation models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Current foundation models exhibit impressive capabilities when prompted either with text only or with both image and text inputs. But do their capabilities change depending on the input modality? In this work, we propose $\textbf{IsoBench}$, a benchmark dataset containing problems from four major areas: math, science, algorithms, and games. Each example is presented with multiple $\textbf{isomorphic representations}$ of inputs, such as visual, textual, and mathematical presentations. IsoBench provides fine-grained feedback to diagnose performance gaps caused by the form of the representation. Across various foundation models, we observe that on the same problem, models have a consistent preference towards textual representations. Most prominently, when evaluated on all IsoBench problems, Claude-3 Opus performs 28.7 points worse when provided with images instead of text; similarly, GPT-4 Turbo is 18.7 points worse and Gemini Pro is 14.9 points worse. Finally, we present two prompting techniques, $\textit{IsoCombination}$ and $\textit{IsoScratchPad}$, which improve model performance by considering combinations of, and translations between, different input representations.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：FABLES: Evaluating faithfulness and content selection in book-length  summarization</b></summary>
  <p><b>编号</b>：[22]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01261">https://arxiv.org/abs/2404.01261</a></p>
  <p><b>作者</b>：Yekyung Kim,  Yapei Chang,  Marzena Karpinska,  Aparna Garimella,  Varun Manjunatha,  Kyle Lo,  Tanya Goyal,  Mohit Iyyer</p>
  <p><b>备注</b>：preprint - 39 pages</p>
  <p><b>关键词</b>：large language models, summarize book-length documents, long-context large language, technically summarize book-length, language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>While long-context large language models (LLMs) can technically summarize book-length documents (>100K tokens), the length and complexity of the documents have so far prohibited evaluations of input-dependent aspects like faithfulness. In this paper, we conduct the first large-scale human evaluation of faithfulness and content selection on LLM-generated summaries of fictional books. Our study mitigates the issue of data contamination by focusing on summaries of books published in 2023 or 2024, and we hire annotators who have fully read each book prior to the annotation task to minimize cost and cognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims made in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which allows us to rank LLM summarizers based on faithfulness: Claude-3-Opus significantly outperforms all closed-source LLMs, while the open-source Mixtral is on par with GPT-3.5-Turbo. An analysis of the annotations reveals that most unfaithful claims relate to events and character states, and they generally require indirect reasoning over the narrative to invalidate. While LLM-based auto-raters have proven reliable for factuality and coherence in other settings, we implement several LLM raters of faithfulness and find that none correlates strongly with human annotations, especially with regard to detecting unfaithful claims. Our experiments suggest that detecting unfaithful claims is an important future direction not only for summarization evaluation but also as a testbed for long-context understanding. Finally, we move beyond faithfulness by exploring content selection errors in book-length summarization: we develop a typology of omission errors related to crucial narrative elements and also identify a systematic over-emphasis on events occurring towards the end of the book.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Bridging Remote Sensors with Multisensor Geospatial Foundation Models</b></summary>
  <p><b>编号</b>：[23]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01260">https://arxiv.org/abs/2404.01260</a></p>
  <p><b>作者</b>：Boran Han,  Shuai Zhang,  Xingjian Shi,  Markus Reichstein</p>
  <p><b>备注</b>：Accepted to CVPR</p>
  <p><b>关键词</b>：encompassing both optical, microwave technologies, offers a wealth, optical and microwave, distinct observational capabilities</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the realm of geospatial analysis, the diversity of remote sensors, encompassing both optical and microwave technologies, offers a wealth of distinct observational capabilities. Recognizing this, we present msGFM, a multisensor geospatial foundation model that effectively unifies data from four key sensor modalities. This integration spans an expansive dataset of two million multisensor images. msGFM is uniquely adept at handling both paired and unpaired sensor data. For data originating from identical geolocations, our model employs an innovative cross-sensor pretraining approach in masked image modeling, enabling the synthesis of joint representations from diverse sensors. msGFM, incorporating four remote sensors, upholds strong performance, forming a comprehensive model adaptable to various sensor types. msGFM has demonstrated enhanced proficiency in a range of both single-sensor and multisensor downstream tasks. These include scene classification, segmentation, cloud removal, and pan-sharpening. A key discovery of our research is that representations derived from natural images are not always compatible with the distinct characteristics of geospatial remote sensors, underscoring the limitations of existing representations in this field. Our work can serve as a guide for developing multisensor geospatial pretraining models, paving the way for more advanced geospatial capabilities.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Direct Preference Optimization of Video Large Multimodal Models from  Language Model Reward</b></summary>
  <p><b>编号</b>：[24]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01258">https://arxiv.org/abs/2404.01258</a></p>
  <p><b>作者</b>：Ruohong Zhang,  Liangke Gui,  Zhiqing Sun,  Yihao Feng,  Keyang Xu,  Yuanhan Zhang,  Di Fu,  Chunyuan Li,  Alexander Hauptmann,  Yonatan Bisk,  Yiming Yang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：direct preference optimization, Preference modeling techniques, shown effective, effective in enhancing, enhancing the generalization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Preference modeling techniques, such as direct preference optimization (DPO), has shown effective in enhancing the generalization abilities of large language model (LLM). However, in tasks involving video instruction-following, providing informative feedback, especially for detecting hallucinations in generated responses, remains a significant challenge. Previous studies have explored using large large multimodal models (LMMs) as reward models to guide preference modeling, but their ability to accurately assess the factuality of generated responses compared to corresponding videos has not been conclusively established. This paper introduces a novel framework that utilizes detailed video captions as a proxy of video content, enabling language models to incorporate this information as supporting evidence for scoring video Question Answering (QA) predictions. Our approach demonstrates robust alignment with OpenAI GPT-4V model's reward mechanism, which directly takes video frames as input. Furthermore, we show that applying this tailored reward through DPO significantly improves the performance of video LMMs on video QA tasks.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Feature Splatting: Language-Driven Physics-Based Scene Synthesis and  Editing</b></summary>
  <p><b>编号</b>：[45]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01223">https://arxiv.org/abs/2404.01223</a></p>
  <p><b>作者</b>：Ri-Zhao Qiu,  Ge Yang,  Weijia Zeng,  Xiaolong Wang</p>
  <p><b>备注</b>：Project website: this https URL</p>
  <p><b>关键词</b>：produced excellent results, primitives have produced, produced excellent, excellent results, results in modeling</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Scene representations using 3D Gaussian primitives have produced excellent results in modeling the appearance of static and dynamic 3D scenes. Many graphics applications, however, demand the ability to manipulate both the appearance and the physical properties of objects. We introduce Feature Splatting, an approach that unifies physics-based dynamic scene synthesis with rich semantics from vision language foundation models that are grounded by natural language. Our first contribution is a way to distill high-quality, object-centric vision-language features into 3D Gaussians, that enables semi-automatic scene decomposition using text queries. Our second contribution is a way to synthesize physics-based dynamics from an otherwise static scene using a particle-based simulator, in which material properties are assigned automatically via text queries. We ablate key techniques used in this pipeline, to illustrate the challenge and opportunities in using feature-carrying 3D Gaussians as a unified format for appearance, geometry, material properties and semantics grounded on natural language. Project website: this https URL</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Incorporating Domain Differential Equations into Graph Convolutional  Networks to Lower Generalization Discrepancy</b></summary>
  <p><b>编号</b>：[49]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01217">https://arxiv.org/abs/2404.01217</a></p>
  <p><b>作者</b>：Yue Sun,  Chao Chen,  Yuesheng Xu,  Sihong Xie,  Rick S. Blum,  Parv Venkitasubramaniam</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Graph Convolutional Network, time series prediction, Graph Convolutional, Ensuring both accuracy, ranging from urban</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Ensuring both accuracy and robustness in time series prediction is critical to many applications, ranging from urban planning to pandemic management. With sufficient training data where all spatiotemporal patterns are well-represented, existing deep-learning models can make reasonably accurate predictions. However, existing methods fail when the training data are drawn from different circumstances (e.g., traffic patterns on regular days) compared to test data (e.g., traffic patterns after a natural disaster). Such challenges are usually classified under domain generalization. In this work, we show that one way to address this challenge in the context of spatiotemporal prediction is by incorporating domain differential equations into Graph Convolutional Networks (GCNs). We theoretically derive conditions where GCNs incorporating such domain differential equations are robust to mismatched training and testing data compared to baseline domain agnostic models. To support our theory, we propose two domain-differential-equation-informed networks called Reaction-Diffusion Graph Convolutional Network (RDGCN), which incorporates differential equations for traffic speed evolution, and Susceptible-Infectious-Recovered Graph Convolutional Network (SIRGCN), which incorporates a disease propagation model. Both RDGCN and SIRGCN are based on reliable and interpretable domain differential equations that allow the models to generalize to unseen patterns. We experimentally show that RDGCN and SIRGCN are more robust with mismatched testing data than the state-of-the-art deep learning methods.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Capturing Shock Waves by Relaxation Neural Networks</b></summary>
  <p><b>编号</b>：[74]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01163">https://arxiv.org/abs/2404.01163</a></p>
  <p><b>作者</b>：Nan Zhou,  Zheng Ma</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：neural network framework, neural networks, relaxation neural networks, physics-informed neural networks, PINN framework</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we put forward a neural network framework to solve the nonlinear hyperbolic systems. This framework, named relaxation neural networks(RelaxNN), is a simple and scalable extension of physics-informed neural networks(PINN). It is shown later that a typical PINN framework struggles to handle shock waves that arise in hyperbolic systems' solutions. This ultimately results in the failure of optimization that is based on gradient descent in the training process. Relaxation systems provide a smooth asymptotic to the discontinuity solution, under the expectation that macroscopic problems can be solved from a microscopic perspective. Based on relaxation systems, the RelaxNN framework alleviates the conflict of losses in the training process of the PINN framework. In addition to the remarkable results demonstrated in numerical simulations, most of the acceleration techniques and improvement strategies aimed at the standard PINN framework can also be applied to the RelaxNN framework.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：SyncMask: Synchronized Attentional Masking for Fashion-centric  Vision-Language Pretraining</b></summary>
  <p><b>编号</b>：[79]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01156">https://arxiv.org/abs/2404.01156</a></p>
  <p><b>作者</b>：Chull Hwan Song,  Taebaek Hwang,  Jooyoung Yoon,  Shunghyun Choi,  Yeong Hyeon Gu</p>
  <p><b>备注</b>：CVPR2024 Accepted</p>
  <p><b>关键词</b>：made significant strides, large-scale paired datasets, made significant, significant strides, strides in cross-modal</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Vision-language models (VLMs) have made significant strides in cross-modal understanding through large-scale paired datasets. However, in fashion domain, datasets often exhibit a disparity between the information conveyed in image and text. This issue stems from datasets containing multiple images of a single fashion item all paired with one text, leading to cases where some textual details are not visible in individual images. This mismatch, particularly when non-co-occurring elements are masked, undermines the training of conventional VLM objectives like Masked Language Modeling and Masked Image Modeling, thereby hindering the model's ability to accurately align fine-grained visual and textual features. Addressing this problem, we propose Synchronized attentional Masking (SyncMask), which generate masks that pinpoint the image patches and word tokens where the information co-occur in both image and text. This synchronization is accomplished by harnessing cross-attentional features obtained from a momentum model, ensuring a precise alignment between the two modalities. Additionally, we enhance grouped batch sampling with semi-hard negatives, effectively mitigating false negative issues in Image-Text Matching and Image-Text Contrastive learning objectives within fashion datasets. Our experiments demonstrate the effectiveness of the proposed approach, outperforming existing methods in three downstream tasks.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Uncovering the Text Embedding in Text-to-Image Diffusion Models</b></summary>
  <p><b>编号</b>：[81]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01154">https://arxiv.org/abs/2404.01154</a></p>
  <p><b>作者</b>：Hu Yu,  Hao Luo,  Fan Wang,  Feng Zhao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：minor textual modifications, induce substantial deviations, generated image exhibits, image exhibits opacity, generated image</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The correspondence between input text and the generated image exhibits opacity, wherein minor textual modifications can induce substantial deviations in the generated image. While, text embedding, as the pivotal intermediary between text and images, remains relatively underexplored. In this paper, we address this research gap by delving into the text embedding space, unleashing its capacity for controllable image editing and explicable semantic direction attributes within a learning-free framework. Specifically, we identify two critical insights regarding the importance of per-word embedding and their contextual correlations within text embedding, providing instructive principles for learning-free image editing. Additionally, we find that text embedding inherently possesses diverse semantic potentials, and further reveal this property through the lens of singular value decomposition (SVD). These uncovered properties offer practical utility for image editing and semantic discovery. More importantly, we expect the in-depth analyses and findings of the text embedding can enhance the understanding of text-to-image diffusion models.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Condition-Aware Neural Network for Controlled Image Generation</b></summary>
  <p><b>编号</b>：[87]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01143">https://arxiv.org/abs/2404.01143</a></p>
  <p><b>作者</b>：Han Cai,  Muyang Li,  Zhuoyang Zhang,  Qinsheng Zhang,  Ming-Yu Liu,  Song Han</p>
  <p><b>备注</b>：CVPR 2024</p>
  <p><b>关键词</b>：Condition-Aware Neural Network, Neural Network, present Condition-Aware Neural, image generative models, Network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present Condition-Aware Neural Network (CAN), a new method for adding control to image generative models. In parallel to prior conditional control methods, CAN controls the image generation process by dynamically manipulating the weight of the neural network. This is achieved by introducing a condition-aware weight generation module that generates conditional weight for convolution/linear layers based on the input condition. We test CAN on class-conditional image generation on ImageNet and text-to-image generation on COCO. CAN consistently delivers significant improvements for diffusion transformer models, including DiT and UViT. In particular, CAN combined with EfficientViT (CaT) achieves 2.78 FID on ImageNet 512x512, surpassing DiT-XL/2 while requiring 52x fewer MACs per sampling step.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Enhancing Reasoning Capacity of SLM using Cognitive Enhancement</b></summary>
  <p><b>编号</b>：[92]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01135">https://arxiv.org/abs/2404.01135</a></p>
  <p><b>作者</b>：Jonathan Pan,  Swee Liang Wong,  Xin Wei Chia,  Yidi Yuan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：processes including cyber, including cyber investigation, automate cyber security, cyber security activities, Large Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) have been applied to automate cyber security activities and processes including cyber investigation and digital forensics. However, the use of such models for cyber investigation and digital forensics should address accountability and security considerations. Accountability ensures models have the means to provide explainable reasonings and outcomes. This information can be extracted through explicit prompt requests. For security considerations, it is crucial to address privacy and confidentiality of the involved data during data processing as well. One approach to deal with this consideration is to have the data processed locally using a local instance of the model. Due to limitations of locally available resources, namely memory and GPU capacities, a Smaller Large Language Model (SLM) will typically be used. These SLMs have significantly fewer parameters compared to the LLMs. However, such size reductions have notable performance reduction, especially when tasked to provide reasoning explanations. In this paper, we aim to mitigate performance reduction through the integration of cognitive strategies that humans use for problem-solving. We term this as cognitive enhancement through prompts. Our experiments showed significant improvement gains of the SLMs' performances when such enhancements were applied. We believe that our exploration study paves the way for further investigation into the use of cognitive enhancement to optimize SLM for cyber security applications.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：GOV-REK: Governed Reward Engineering Kernels for Designing Robust  Multi-Agent Reinforcement Learning Systems</b></summary>
  <p><b>编号</b>：[94]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01131">https://arxiv.org/abs/2404.01131</a></p>
  <p><b>作者</b>：Ashish Rana,  Michael Oesterle,  Jannik Brinkmann</p>
  <p><b>备注</b>：Extended Abstract accepted in the 23rd International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS 2024)</p>
  <p><b>关键词</b>：formulation generally involves, generally involves investing, involves investing massive, engineering effort specific, investing massive reward</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>For multi-agent reinforcement learning systems (MARLS), the problem formulation generally involves investing massive reward engineering effort specific to a given problem. However, this effort often cannot be translated to other problems; worse, it gets wasted when system dynamics change drastically. This problem is further exacerbated in sparse reward scenarios, where a meaningful heuristic can assist in the policy convergence task. We propose GOVerned Reward Engineering Kernels (GOV-REK), which dynamically assign reward distributions to agents in MARLS during its learning stage. We also introduce governance kernels, which exploit the underlying structure in either state or joint action space for assigning meaningful agent reward distributions. During the agent learning stage, it iteratively explores different reward distribution configurations with a Hyperband-like algorithm to learn ideal agent reward models in a problem-agnostic manner. Our experiments demonstrate that our meaningful reward priors robustly jumpstart the learning process for effectively learning different MARL problems.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Medical Visual Prompting (MVP): A Unified Framework for Versatile and  High-Quality Medical Image Segmentation</b></summary>
  <p><b>编号</b>：[96]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01127">https://arxiv.org/abs/2404.01127</a></p>
  <p><b>作者</b>：Yulin Chen,  Guoheng Huang,  Kai Huang,  Zijin Lin,  Guo Zhong,  Shenghong Luo,  Jie Deng,  Jian Zhou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Guided Prompting, Mechanism Guided Prompting, Attention Mechanism Guided, crucial for clinical, clinical diagnosis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Accurate segmentation of lesion regions is crucial for clinical diagnosis and treatment across various diseases. While deep convolutional networks have achieved satisfactory results in medical image segmentation, they face challenges such as loss of lesion shape information due to continuous convolution and downsampling, as well as the high cost of manually labeling lesions with varying shapes and sizes. To address these issues, we propose a novel medical visual prompting (MVP) framework that leverages pre-training and prompting concepts from natural language processing (NLP). The framework utilizes three key components: Super-Pixel Guided Prompting (SPGP) for superpixelating the input image, Image Embedding Guided Prompting (IEGP) for freezing patch embedding and merging with superpixels to provide visual prompts, and Adaptive Attention Mechanism Guided Prompting (AAGP) for pinpointing prompt content and efficiently adapting all layers. By integrating SPGP, IEGP, and AAGP, the MVP enables the segmentation network to better learn shape prompting information and facilitates mutual learning across different tasks. Extensive experiments conducted on five datasets demonstrate superior performance of this method in various challenging medical image tasks, while simplifying single-task medical segmentation models. This novel framework offers improved performance with fewer parameters and holds significant potential for accurate segmentation of lesion regions in various medical tasks, making it clinically valuable.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：An incremental hybrid adaptive network-based IDS in Software Defined  Networks to detect stealth attacks</b></summary>
  <p><b>编号</b>：[107]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01109">https://arxiv.org/abs/2404.01109</a></p>
  <p><b>作者</b>：Abdullah H Alqahtani</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：intrusion detection systems, network intrusion detection, Advanced Persistent Threats, intrusion detection, detection systems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Network attacks have became increasingly more sophisticated and stealthy due to the advances in technologies and the growing sophistication of attackers. Advanced Persistent Threats (APTs) are a type of attack that implement a wide range of strategies to evade detection and be under the defence radar. Software Defined Network (SDN) is a network paradigm that implements dynamic configuration by separating the control plane from the network plane. This approach improves security aspects by facilitating the employment of network intrusion detection systems. Implementing Machine Learning (ML) techniques in Intrusion Detection Systems (IDSs) is widely used to detect such attacks but has a challenge when the data distribution changes. Concept drift is a term that describes the change in the relationship between the input data and the target value (label or class). The model is expected to degrade as certain forms of change occur. In this paper, the primary form of change will be in user behaviour (particularly changes in attacker behaviour). It is essential for a model to adapt itself to deviations in data distribution. SDN can help in monitoring changes in data distribution. This paper discusses changes in stealth attacker behaviour. The work described here investigates various concept drift detection algorithms. An incremental hybrid adaptive Network Intrusion Detection System (NIDS) is proposed to tackle the issue of concept drift in SDN. It can detect known and unknown attacks. The model is evaluated over different datasets showing promising results.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：What's in Your "Safe" Data?: Identifying Benign Data that Breaks Safety</b></summary>
  <p><b>编号</b>：[113]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01099">https://arxiv.org/abs/2404.01099</a></p>
  <p><b>作者</b>：Luxi He,  Mengzhou Xia,  Peter Henderson</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Current Large Language, Large Language Models, Large Language, Current Large, Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Current Large Language Models (LLMs), even those tuned for safety and alignment, are susceptible to jailbreaking. Some have found that just further fine-tuning an aligned model with benign data (i.e., data without harmful content) surprisingly leads to substantial degradation in safety. We delve into the data-centric aspects of why benign fine-tuning inadvertently contributes to jailbreaking. First, we represent fine-tuning data through two lenses: representation and gradient spaces. Furthermore, we propose a bi-directional anchoring method that prioritizes data points that are close to harmful examples and distant from benign ones. By doing so, our approach effectively identifies subsets of benign data that are more likely to degrade the model's safety after fine-tuning. Training on just 100 of these seemingly benign datapoints can lead to the fine-tuned model affirmatively responding to > 70% of tested harmful requests, compared to < 20% after fine-tuning on randomly selected data. We further find that selected data are often in the form of lists and bullet points, or math questions.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Texture-Preserving Diffusion Models for High-Fidelity Virtual Try-On</b></summary>
  <p><b>编号</b>：[117]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01089">https://arxiv.org/abs/2404.01089</a></p>
  <p><b>作者</b>：Xu Yang,  Changxing Ding,  Zhibin Hong,  Junhao Huang,  Jin Tao,  Xiangmin Xu</p>
  <p><b>备注</b>：CVPR 2024</p>
  <p><b>关键词</b>：increasingly important task, Image-based virtual try-on, online shopping, increasingly important, image</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image-based virtual try-on is an increasingly important task for online shopping. It aims to synthesize images of a specific person wearing a specified garment. Diffusion model-based approaches have recently become popular, as they are excellent at image synthesis tasks. However, these approaches usually employ additional image encoders and rely on the cross-attention mechanism for texture transfer from the garment to the person image, which affects the try-on's efficiency and fidelity. To address these issues, we propose an Texture-Preserving Diffusion (TPD) model for virtual try-on, which enhances the fidelity of the results and introduces no additional image encoders. Accordingly, we make contributions from two aspects. First, we propose to concatenate the masked person and reference garment images along the spatial dimension and utilize the resulting image as the input for the diffusion model's denoising UNet. This enables the original self-attention layers contained in the diffusion model to achieve efficient and accurate texture transfer. Second, we propose a novel diffusion-based method that predicts a precise inpainting mask based on the person and reference garment images, further enhancing the reliability of the try-on results. In addition, we integrate mask prediction and image synthesis into a single compact model. The experimental results show that our approach can be applied to various try-on tasks, e.g., garment-to-person and person-to-person try-ons, and significantly outperforms state-of-the-art methods on popular VITON, VITON-HD databases.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：AILS-NTUA at SemEval-2024 Task 9: Cracking Brain Teasers: Transformer  Models for Lateral Thinking Puzzles</b></summary>
  <p><b>编号</b>：[118]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01084">https://arxiv.org/abs/2404.01084</a></p>
  <p><b>作者</b>：Ioannis Panagiotopoulos,  Giorgos Filandrianos,  Maria Lymperaiou,  Giorgos Stamou</p>
  <p><b>备注</b>：SemEval-2024</p>
  <p><b>关键词</b>：Defying Common Sense, Task Defying Common, Common Sense, Task Defying, Defying Common</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we outline our submission for the SemEval-2024 Task 9 competition: 'BRAINTEASER: A Novel Task Defying Common Sense'. We engage in both sub-tasks: Sub-task A-Sentence Puzzle and Sub-task B-Word Puzzle. We evaluate a plethora of pre-trained transformer-based language models of different sizes through fine-tuning. Subsequently, we undertake an analysis of their scores and responses to aid future researchers in understanding and utilizing these models effectively. Our top-performing approaches secured competitive positions on the competition leaderboard across both sub-tasks. In the evaluation phase, our best submission attained an average accuracy score of 81.7% in the Sentence Puzzle, and 85.4% in the Word Puzzle, significantly outperforming the best neural baseline (ChatGPT) by more than 20% and 30% respectively.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Advancing AI with Integrity: Ethical Challenges and Solutions in Neural  Machine Translation</b></summary>
  <p><b>编号</b>：[126]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01070">https://arxiv.org/abs/2404.01070</a></p>
  <p><b>作者</b>：Richard Kimera,  Yun-Seon Kim,  Heeyoul Choi</p>
  <p><b>备注</b>：11 pages</p>
  <p><b>关键词</b>：Neural Machine Translation, Artificial Intelligence, Intelligence in Neural, Neural Machine, challenges of Artificial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper addresses the ethical challenges of Artificial Intelligence in Neural Machine Translation (NMT) systems, emphasizing the imperative for developers to ensure fairness and cultural sensitivity. We investigate the ethical competence of AI models in NMT, examining the Ethical considerations at each stage of NMT development, including data handling, privacy, data ownership, and consent. We identify and address ethical issues through empirical studies. These include employing Transformer models for Luganda-English translations and enhancing efficiency with sentence mini-batching. And complementary studies that refine data labeling techniques and fine-tune BERT and Longformer models for analyzing Luganda and English social media content. Our second approach is a literature review from databases such as Google Scholar and platforms like GitHub. Additionally, the paper probes the distribution of responsibility between AI systems and humans, underscoring the essential role of human oversight in upholding NMT ethical standards. Incorporating a biblical perspective, we discuss the societal impact of NMT and the broader ethical responsibilities of developers, positing them as stewards accountable for the societal repercussions of their creations.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language  Model Alignment</b></summary>
  <p><b>编号</b>：[136]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01054">https://arxiv.org/abs/2404.01054</a></p>
  <p><b>作者</b>：Yuu Jinnai,  Tetsuro Morimura,  Kaito Ariu,  Kenshi Abe</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：aligning Large Language, Large Language Models, aligning Large, Large Language, time of decoding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) to human preferences at the time of decoding. BoN sampling is susceptible to a problem known as reward hacking. Because the reward model is an imperfect proxy for the true objective, over-optimizing its value can compromise its performance on the true objective. A common solution to prevent reward hacking in preference learning techniques is to optimize a reward using proximity regularization (e.g., KL regularization), which ensures that the language model remains close to the reference model. In this research, we propose Regularized Best-of-N (RBoN), a variant of BoN that aims to mitigate reward hacking by incorporating a proximity term in response selection, similar to preference learning techniques. We evaluate two variants of RBoN on the AlpacaFarm dataset and find that they outperform BoN, especially when the proxy reward model has a low correlation with the true objective.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Can LLMs get help from other LLMs without revealing private information?</b></summary>
  <p><b>编号</b>：[140]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01041">https://arxiv.org/abs/2404.01041</a></p>
  <p><b>作者</b>：Florian Hartmann,  Duc-Hieu Tran,  Peter Kairouz,  Victor Cărbune,  Blaise Aguera y Arcas</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：remote model, applying cascade systems, common type, type of machine, accurately label</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cascades are a common type of machine learning systems in which a large, remote model can be queried if a local model is not able to accurately label a user's data by itself. Serving stacks for large language models (LLMs) increasingly use cascades due to their ability to preserve task performance while dramatically reducing inference costs. However, applying cascade systems in situations where the local model has access to sensitive data constitutes a significant privacy risk for users since such data could be forwarded to the remote model. In this work, we show the feasibility of applying cascade systems in such setups by equipping the local model with privacy-preserving techniques that reduce the risk of leaking private information when querying the remote model. To quantify information leakage in such setups, we introduce two privacy measures. We then propose a system that leverages the recently introduced social learning paradigm in which LLMs collaboratively learn from each other by exchanging natural language. Using this paradigm, we demonstrate on several datasets that our methods minimize the privacy loss while at the same time improving task performance compared to a non-cascade baseline.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Higher education assessment practice in the era of generative AI tools</b></summary>
  <p><b>编号</b>：[143]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01036">https://arxiv.org/abs/2404.01036</a></p>
  <p><b>作者</b>：Bayode Ogunleye,  Kudirat Ibilola Zakariyyah,  Oluwaseun Ajao,  Olakunle Olayinka,  Hemlata Sharma</p>
  <p><b>备注</b>：11 pages, 7 tables published in the Journal of Applied Learning & Teaching</p>
  <p><b>关键词</b>：higher education, sector benefits, society at large, benefits every nation, nation economy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The higher education (HE) sector benefits every nation's economy and society at large. However, their contributions are challenged by advanced technologies like generative artificial intelligence (GenAI) tools. In this paper, we provide a comprehensive assessment of GenAI tools towards assessment and pedagogic practice and, subsequently, discuss the potential impacts. This study experimented using three assessment instruments from data science, data analytics, and construction management disciplines. Our findings are two-fold: first, the findings revealed that GenAI tools exhibit subject knowledge, problem-solving, analytical, critical thinking, and presentation skills and thus can limit learning when used unethically. Secondly, the design of the assessment of certain disciplines revealed the limitations of the GenAI tools. Based on our findings, we made recommendations on how AI tools can be utilised for teaching and learning in HE.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Survey of Bias In Text-to-Image Generation: Definition, Evaluation, and  Mitigation</b></summary>
  <p><b>编号</b>：[144]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01030">https://arxiv.org/abs/2404.01030</a></p>
  <p><b>作者</b>：Yixin Wan,  Arjun Subramonian,  Anaelia Ovalle,  Zongyu Lin,  Ashima Suvarna,  Christina Chance,  Hritik Bansal,  Rebecca Pattichis,  Kai-Wei Chang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Google Gemini, generate high-quality images, generation abilities, enables users, users to generate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The recent advancement of large and powerful models with Text-to-Image (T2I) generation abilities -- such as OpenAI's DALLE-3 and Google's Gemini -- enables users to generate high-quality images from textual prompts. However, it has become increasingly evident that even simple prompts could cause T2I models to exhibit conspicuous social bias in generated images. Such bias might lead to both allocational and representational harms in society, further marginalizing minority groups. Noting this problem, a large body of recent works has been dedicated to investigating different dimensions of bias in T2I systems. However, an extensive review of these studies is lacking, hindering a systematic understanding of current progress and research gaps. We present the first extensive survey on bias in T2I generative models. In this survey, we review prior studies on dimensions of bias: Gender, Skintone, and Geo-Culture. Specifically, we discuss how these works define, evaluate, and mitigate different aspects of bias. We found that: (1) while gender and skintone biases are widely studied, geo-cultural bias remains under-explored; (2) most works on gender and skintone bias investigated occupational association, while other aspects are less frequently studied; (3) almost all gender bias works overlook non-binary identities in their studies; (4) evaluation datasets and metrics are scattered, with no unified framework for measuring biases; and (5) current mitigation methods fail to resolve biases comprehensively. Based on current limitations, we point out future research directions that contribute to human-centric definitions, evaluations, and mitigation of biases. We hope to highlight the importance of studying biases in T2I systems, as well as encourage future efforts to holistically understand and tackle biases, building fair and trustworthy T2I technologies for everyone.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Source-Aware Training Enables Knowledge Attribution in Language Models</b></summary>
  <p><b>编号</b>：[149]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01019">https://arxiv.org/abs/2404.01019</a></p>
  <p><b>作者</b>：Muhammad Khalifa,  David Wadden,  Emma Strubell,  Honglak Lee,  Lu Wang,  Iz Beltagy,  Hao Peng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large language models, Large language, learn a vast, intrinsic source citation, vast amount</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) learn a vast amount of knowledge during pretraining, but they are often oblivious to the source(s) of such knowledge. We investigate the problem of intrinsic source citation, where LLMs are required to cite the pretraining source supporting a generated response. Intrinsic source citation can enhance LLM transparency, interpretability, and verifiability. To give LLMs such ability, we explore source-aware training -- a post pretraining recipe that involves (i) training the LLM to associate unique source document identifiers with the knowledge in each document, followed by (ii) an instruction-tuning to teach the LLM to cite a supporting pretraining source when prompted. Source-aware training can easily be applied to pretrained LLMs off the shelf, and diverges minimally from existing pretraining/fine-tuning frameworks. Through experiments on carefully curated data, we demonstrate that our training recipe can enable faithful attribution to the pretraining data without a substantial impact on the model's quality compared to standard pretraining. Our results also highlight the importance of data augmentation in achieving attribution.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Teeth-SEG: An Efficient Instance Segmentation Framework for Orthodontic  Treatment based on Anthropic Prior Knowledge</b></summary>
  <p><b>编号</b>：[152]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01013">https://arxiv.org/abs/2404.01013</a></p>
  <p><b>作者</b>：Bo Zou,  Shaofeng Wang,  Hao Liu,  Gaoyue Sun,  Yajie Wang,  FeiFei Zuo,  Chengbin Quan,  Youjian Zhao</p>
  <p><b>备注</b>：This paper has been accepted by CVPR 2024</p>
  <p><b>关键词</b>：enhance dental diagnostics, treatment planning, Anthropic Prior Knowledge, oral health, great potential</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Teeth localization, segmentation, and labeling in 2D images have great potential in modern dentistry to enhance dental diagnostics, treatment planning, and population-based studies on oral health. However, general instance segmentation frameworks are incompetent due to 1) the subtle differences between some teeth' shapes (e.g., maxillary first premolar and second premolar), 2) the teeth's position and shape variation across subjects, and 3) the presence of abnormalities in the dentition (e.g., caries and edentulism). To address these problems, we propose a ViT-based framework named TeethSEG, which consists of stacked Multi-Scale Aggregation (MSA) blocks and an Anthropic Prior Knowledge (APK) layer. Specifically, to compose the two modules, we design 1) a unique permutation-based upscaler to ensure high efficiency while establishing clear segmentation boundaries with 2) multi-head self/cross-gating layers to emphasize particular semantics meanwhile maintaining the divergence between token embeddings. Besides, we collect 3) the first open-sourced intraoral image dataset IO150K, which comprises over 150k intraoral photos, and all photos are annotated by orthodontists using a human-machine hybrid algorithm. Experiments on IO150K demonstrate that our TeethSEG outperforms the state-of-the-art segmentation models on dental image segmentation.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Query Performance Prediction using Relevance Judgments Generated by  Large Language Models</b></summary>
  <p><b>编号</b>：[153]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.01012">https://arxiv.org/abs/2404.01012</a></p>
  <p><b>作者</b>：Chuan Meng,  Negar Arabzadeh,  Arian Askari,  Mohammad Aliannejadi,  Maarten de Rijke</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：generated relevance judgments, relevance judgments, QPP, human relevance judgments, single scalar</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Query performance prediction (QPP) aims to estimate the retrieval quality of a search system for a query without human relevance judgments. Previous QPP methods typically return a single scalar value and do not require the predicted values to approximate a specific information retrieval (IR) evaluation measure, leading to certain drawbacks: (i) a single scalar is insufficient to accurately represent different IR evaluation measures, especially when metrics do not highly correlate, and (ii) a single scalar limits the interpretability of QPP methods because solely using a scalar is insufficient to explain QPP results. To address these issues, we propose a QPP framework using automatically generated relevance judgments (QPP-GenRE), which decomposes QPP into independent subtasks of judging the relevance of each item in a ranked list to a given query. This allows us to predict any IR evaluation measure using the generated relevance judgments as pseudo-labels; Also, this allows us to interpret predicted IR evaluation measures, and identify, track and rectify errors in generated relevance judgments to improve QPP quality. We judge relevance by leveraging a leading open-source large language model (LLM), LLaMA, to ensure scientific reproducibility. In doing so, we address two main challenges: (i) excessive computational costs of judging the entire corpus for predicting a recall-based metric, and (ii) poor performance in prompting LLaMA in a zero-/few-shot manner. We devise an approximation strategy to predict a recall-oriented IR measure and propose to fine-tune LLaMA using human-labeled relevance judgments. Experiments on the TREC 2019-2022 deep learning tracks show that QPP-GenRE achieves state-of-the-art QPP accuracy for both lexical and neural rankers in both precision- and recall-oriented metrics.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report  Generation</b></summary>
  <p><b>编号</b>：[158]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00998">https://arxiv.org/abs/2404.00998</a></p>
  <p><b>作者</b>：Zilong Wang,  Xufang Luo,  Xinyang Jiang,  Dongsheng Li,  Lili Qiu</p>
  <p><b>备注</b>：11 pages, 6 figures</p>
  <p><b>关键词</b>：task clinical requirements, Evaluating generated radiology, existing metrics fail, generated radiology reports, Evaluating generated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Evaluating generated radiology reports is crucial for the development of radiology AI, but existing metrics fail to reflect the task's clinical requirements. This study proposes a novel evaluation framework using large language models (LLMs) to compare radiology reports for assessment. We compare the performance of various LLMs and demonstrate that, when using GPT-4, our proposed metric achieves evaluation consistency close to that of radiologists. Furthermore, to reduce costs and improve accessibility, making this method practical, we construct a dataset using LLM evaluation results and perform knowledge distillation to train a smaller model. The distilled model achieves evaluation capabilities comparable to GPT-4. Our framework and distilled model offer an accessible and efficient evaluation method for radiology report generation, facilitating the development of more clinically relevant models. The model will be further open-sourced and accessible.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：360+x: A Panoptic Multi-modal Scene Understanding Dataset</b></summary>
  <p><b>编号</b>：[163]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00989">https://arxiv.org/abs/2404.00989</a></p>
  <p><b>作者</b>：Hao Chen,  Yuqi Hou,  Chenyuan Qu,  Irene Testini,  Xiaohan Hong,  Jianbo Jiao</p>
  <p><b>备注</b>：To access the public dataset, please visit this https URL</p>
  <p><b>关键词</b>：Human perception, multiple data modalities, scene, multiple viewpoints, scene understanding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Human perception of the world is shaped by a multitude of viewpoints and modalities. While many existing datasets focus on scene understanding from a certain perspective (e.g. egocentric or third-person views), our dataset offers a panoptic perspective (i.e. multiple viewpoints with multiple data modalities). Specifically, we encapsulate third-person panoramic and front views, as well as egocentric monocular/binocular views with rich modalities including video, multi-channel audio, directional binaural delay, location data and textual scene descriptions within each scene captured, presenting comprehensive observation of the world. Figure 1 offers a glimpse of all 28 scene categories of our 360+x dataset. To the best of our knowledge, this is the first database that covers multiple viewpoints with multiple data modalities to mimic how daily information is accessed in the real world. Through our benchmark analysis, we presented 5 different scene understanding tasks on the proposed 360+x dataset to evaluate the impact and benefit of each data modality and perspective in panoptic scene understanding. We hope this unique dataset could broaden the scope of comprehensive scene understanding and encourage the community to approach these problems from more diverse perspectives.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Continual Learning for Smart City: A Survey</b></summary>
  <p><b>编号</b>：[167]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00983">https://arxiv.org/abs/2404.00983</a></p>
  <p><b>作者</b>：Li Yang,  Zhipeng Luo,  Shiming Zhang,  Fei Teng,  Tianrui Li</p>
  <p><b>备注</b>：Preprint. Work in Progress</p>
  <p><b>关键词</b>：powerful computational resources, computational resources facilitate, intelligent models deployed, modern cities, large data volumes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the digitization of modern cities, large data volumes and powerful computational resources facilitate the rapid update of intelligent models deployed in smart cities. Continual learning (CL) is a novel machine learning paradigm that constantly updates models to adapt to changing environments, where the learning tasks, data, and distributions can vary over time. Our survey provides a comprehensive review of continual learning methods that are widely used in smart city development. The content consists of three parts: 1) Methodology-wise. We categorize a large number of basic CL methods and advanced CL frameworks in combination with other learning paradigms including graph learning, spatial-temporal learning, multi-modal learning, and federated learning. 2) Application-wise. We present numerous CL applications covering transportation, environment, public health, safety, networks, and associated datasets related to urban computing. 3) Challenges. We discuss current problems and challenges and envision several promising research directions. We believe this survey can help relevant researchers quickly familiarize themselves with the current state of continual learning research used in smart city development and direct them to future research trends.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Exploring and Evaluating Hallucinations in LLM-Powered Code Generation</b></summary>
  <p><b>编号</b>：[174]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00971">https://arxiv.org/abs/2404.00971</a></p>
  <p><b>作者</b>：Fang Liu,  Yang Liu,  Lin Shi,  Houkun Huang,  Ruifeng Wang,  Zhen Yang,  Li Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, software engineering tasks, rise of Large, Language Models, Large Language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rise of Large Language Models (LLMs) has significantly advanced many applications on software engineering tasks, particularly in code generation. Despite the promising performance, LLMs are prone to generate hallucinations, which means LLMs might produce outputs that deviate from users' intent, exhibit internal inconsistencies, or misalign with the factual knowledge, making the deployment of LLMs potentially risky in a wide range of applications. Existing work mainly focuses on investing the hallucination in the domain of natural language generation (NLG), leaving a gap in understanding the types and extent of hallucinations in the context of code generation. To bridge the gap, we conducted a thematic analysis of the LLM-generated code to summarize and categorize the hallucinations present in it. Our study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinations depending on the conflicting objectives and varying degrees of deviation observed in code generation. Furthermore, we systematically analyzed the distribution of hallucinations, exploring variations among different LLMs and their correlation with code correctness. Based on the results, we proposed HalluCode, a benchmark for evaluating the performance of code LLMs in recognizing hallucinations. Hallucination recognition and mitigation experiments with HalluCode and HumanEval show existing LLMs face great challenges in recognizing hallucinations, particularly in identifying their types, and are hardly able to mitigate hallucinations. We believe our findings will shed light on future research about hallucination evaluation, detection, and mitigation, ultimately paving the way for building more effective and reliable code LLMs in the future.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Evalverse: Unified and Accessible Library for Large Language Model  Evaluation</b></summary>
  <p><b>编号</b>：[187]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00943">https://arxiv.org/abs/2404.00943</a></p>
  <p><b>作者</b>：Jihoo Kim,  Wonho Song,  Dahyun Kim,  Yunsu Kim,  Yungi Kim,  Chanjun Park</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, paper introduces Evalverse, unifying disparate evaluation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper introduces Evalverse, a novel library that streamlines the evaluation of Large Language Models (LLMs) by unifying disparate evaluation tools into a single, user-friendly framework. Evalverse enables individuals with limited knowledge of artificial intelligence to easily request LLM evaluations and receive detailed reports, facilitated by an integration with communication platforms like Slack. Thus, Evalverse serves as a powerful tool for the comprehensive assessment of LLMs, offering both researchers and practitioners a centralized and easily accessible evaluation framework. Finally, we also provide a demo video for Evalverse, showcasing its capabilities and implementation in a two-minute format.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Evaluating the Factuality of Large Language Models using Large-Scale  Knowledge Graphs</b></summary>
  <p><b>编号</b>：[188]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00942">https://arxiv.org/abs/2404.00942</a></p>
  <p><b>作者</b>：Xiaoze Liu,  Feijie Wu,  Tianyang Xu,  Zhuo Chen,  Yichi Zhang,  Xiaoqian Wang,  Jing Gao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：enhancing machine learning, Large Language Models, Large Language, Language Models, enhancing machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The advent of Large Language Models (LLMs) has significantly transformed the AI landscape, enhancing machine learning and AI capabilities. Factuality issue is a critical concern for LLMs, as they may generate factually incorrect responses. In this paper, we propose GraphEval to evaluate an LLM's performance using a substantially large test dataset. Specifically, the test dataset is retrieved from a large knowledge graph with more than 10 million facts without expensive human efforts. Unlike conventional methods that evaluate LLMs based on generated responses, GraphEval streamlines the evaluation process by creating a judge model to estimate the correctness of the answers given by the LLM. Our experiments demonstrate that the judge model's factuality assessment aligns closely with the correctness of the LLM's generated outputs, while also substantially reducing evaluation costs. Besides, our findings offer valuable insights into LLM performance across different metrics and highlight the potential for future improvements in ensuring the factual integrity of LLM outputs. The code is publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：A Survey on Multilingual Large Language Models: Corpora, Alignment, and  Bias</b></summary>
  <p><b>编号</b>：[194]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00929">https://arxiv.org/abs/2404.00929</a></p>
  <p><b>作者</b>：Yuemei Xu,  Ling Hu,  Jiayi Zhao,  Zihan Qiu,  Yuqi Ye,  Hanwen Gu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, Multilingual Large Language, achieve knowledge transfer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Based on the foundation of Large Language Models (LLMs), Multilingual Large Language Models (MLLMs) have been developed to address the challenges of multilingual natural language processing tasks, hoping to achieve knowledge transfer from high-resource to low-resource languages. However, significant limitations and challenges still exist, such as language imbalance, multilingual alignment, and inherent bias. In this paper, we aim to provide a comprehensive analysis of MLLMs, delving deeply into discussions surrounding these critical issues. First of all, we start by presenting an overview of MLLMs, covering their evolution, key techniques, and multilingual capacities. Secondly, we explore widely utilized multilingual corpora for MLLMs' training and multilingual datasets oriented for downstream tasks that are crucial for enhancing the cross-lingual capability of MLLMs. Thirdly, we survey the existing studies on multilingual representations and investigate whether the current MLLMs can learn a universal language representation. Fourthly, we discuss bias on MLLMs including its category and evaluation metrics, and summarize the existing debiasing techniques. Finally, we discuss existing challenges and point out promising research directions. By demonstrating these aspects, this paper aims to facilitate a deeper understanding of MLLMs and their potentiality in various domains.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,  Depth, and Inertial Measurements</b></summary>
  <p><b>编号</b>：[198]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00923">https://arxiv.org/abs/2404.00923</a></p>
  <p><b>作者</b>：Lisong C. Sun,  Neel P. Bhatt,  Jonathan C. Liu,  Zhiwen Fan,  Zhangyang Wang,  Todd E. Humphreys,  Ufuk Topcu</p>
  <p><b>备注</b>：Project Webpage: this https URL</p>
  <p><b>关键词</b>：Simultaneous localization, Gaussian-based map representations, essential for position, scene understanding, rendering</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Simultaneous localization and mapping is essential for position tracking and scene understanding. 3D Gaussian-based map representations enable photorealistic reconstruction and real-time rendering of scenes using multiple posed cameras. We show for the first time that using 3D Gaussians for map representation with unposed camera images and inertial measurements can enable accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural radiance field-based representations by enabling faster rendering, scale awareness, and improved trajectory tracking. Our framework enables keyframe-based mapping and tracking utilizing loss functions that incorporate relative pose transformations from pre-integrated inertial measurements, depth estimates, and measures of photometric rendering quality. We also release a multi-modal dataset, UT-MM, collected from a mobile robot equipped with a camera and an inertial measurement unit. Experimental evaluation on several scenes from the dataset shows that MM3DGS achieves 3x improvement in tracking and 5% improvement in photometric rendering quality compared to the current 3DGS SLAM state-of-the-art, while allowing real-time rendering of a high-resolution dense 3D map. Project Webpage: this https URL</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：Token-Efficient Leverage Learning in Large Language Models</b></summary>
  <p><b>编号</b>：[204]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00914">https://arxiv.org/abs/2404.00914</a></p>
  <p><b>作者</b>：Yuanhao Zeng,  Min Wang,  Yihang Wang,  Yingxia Shao</p>
  <p><b>备注</b>：15 pages, 16 figures</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Leverage Learning, Large Language, high-resource scenarios</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) have excelled in various tasks but perform better in high-resource scenarios, which presents challenges in low-resource scenarios. Data scarcity and the inherent difficulty of adapting LLMs to specific tasks compound the challenge. To address the twin hurdles, we introduce \textbf{Leverage Learning}. We present a streamlined implement of this methodology called Token-Efficient Leverage Learning (TELL). TELL showcases the potential of Leverage Learning, demonstrating effectiveness across various LLMs and low-resource tasks, ranging from $10^4$ to $10^6$ tokens. It reduces task data requirements by up to nearly an order of magnitude compared to conventional Supervised Fine-Tuning (SFT) while delivering competitive performance. With the same amount of task data, TELL leads in improving task performance compared to SFT. We discuss the mechanism of Leverage Learning, suggesting it aligns with quantization hypothesis and explore its promising potential through empirical testing.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：LLaMA-Excitor: General Instruction Tuning via Indirect Feature  Interaction</b></summary>
  <p><b>编号</b>：[205]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00913">https://arxiv.org/abs/2404.00913</a></p>
  <p><b>作者</b>：Bo Zou,  Chao Yang,  Yu Qiao,  Chengbin Quan,  Youjian Zhao</p>
  <p><b>备注</b>：This paper is accepted by CVPR 2024</p>
  <p><b>关键词</b>：introduce extra modules, introduce extra, sequences to inject, inject new skills, compromise the innate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing methods to fine-tune LLMs, like Adapter, Prefix-tuning, and LoRA, which introduce extra modules or additional input sequences to inject new skills or knowledge, may compromise the innate abilities of LLMs. In this paper, we propose LLaMA-Excitor, a lightweight method that stimulates the LLMs' potential to better follow instructions by gradually paying more attention to worthwhile information. Specifically, the LLaMA-Excitor does not directly change the intermediate hidden state during the self-attention calculation of the transformer structure. We designed the Excitor block as a bypass module for the similarity score computation in LLMs' self-attention to reconstruct keys and change the importance of values by learnable prompts. LLaMA-Excitor ensures a self-adaptive allocation of additional attention to input instructions, thus effectively preserving LLMs' pre-trained knowledge when fine-tuning LLMs on low-quality instruction-following datasets. Furthermore, we unify the modeling of multi-modal tuning and language-only tuning, extending LLaMA-Excitor to a powerful visual instruction follower without the need for complex multi-modal alignment. Our proposed approach is evaluated in language-only and multi-modal tuning experimental scenarios. Notably, LLaMA-Excitor is the only method that maintains basic capabilities while achieving a significant improvement (+6%) on the MMLU benchmark. In the visual instruction tuning, we achieve a new state-of-the-art image captioning performance of 157.5 CIDEr on MSCOCO, and a comparable performance (88.39%) on ScienceQA to cutting-edge models with more parameters and extensive vision-language pertaining.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Maximizing User Experience with LLMOps-Driven Personalized  Recommendation Systems</b></summary>
  <p><b>编号</b>：[209]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00903">https://arxiv.org/abs/2404.00903</a></p>
  <p><b>作者</b>：Chenxi Shi,  Penghao Liang,  Yichao Wu,  Tong Zhan,  Zhengyu Jin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：managing LLM-driven applications, LLM-driven applications, recommendation systems marks, marks a significant, significant advancement</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The integration of LLMOps into personalized recommendation systems marks a significant advancement in managing LLM-driven applications. This innovation presents both opportunities and challenges for enterprises, requiring specialized teams to navigate the complexity of engineering technology while prioritizing data security and model interpretability. By leveraging LLMOps, enterprises can enhance the efficiency and reliability of large-scale machine learning models, driving personalized recommendations aligned with user preferences. Despite ethical considerations, LLMOps is poised for widespread adoption, promising more efficient and secure machine learning services that elevate user experience and shape the future of personalized recommendation systems.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Machine Learning Robustness: A Primer</b></summary>
  <p><b>编号</b>：[214]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00897">https://arxiv.org/abs/2404.00897</a></p>
  <p><b>作者</b>：Houssem Ben Braiek,  Foutse Khomh</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2305.10862 by other authors</p>
  <p><b>关键词</b>：Artificial Intelligence, trustworthiness in Artificial, Machine Learning, integral role, role in establishing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This chapter explores the foundational concept of robustness in Machine Learning (ML) and its integral role in establishing trustworthiness in Artificial Intelligence (AI) systems. The discussion begins with a detailed definition of robustness, portraying it as the ability of ML models to maintain stable performance across varied and unexpected environmental conditions. ML robustness is dissected through several lenses: its complementarity with generalizability; its status as a requirement for trustworthy AI; its adversarial vs non-adversarial aspects; its quantitative metrics; and its indicators such as reproducibility and explainability. The chapter delves into the factors that impede robustness, such as data bias, model complexity, and the pitfalls of underspecified ML pipelines. It surveys key techniques for robustness assessment from a broad perspective, including adversarial attacks, encompassing both digital and physical realms. It covers non-adversarial data shifts and nuances of Deep Learning (DL) software testing methodologies. The discussion progresses to explore amelioration strategies for bolstering robustness, starting with data-centric approaches like debiasing and augmentation. Further examination includes a variety of model-centric methods such as transfer learning, adversarial training, and randomized smoothing. Lastly, post-training methods are discussed, including ensemble techniques, pruning, and model repairs, emerging as cost-effective strategies to make models more resilient against the unpredictable. This chapter underscores the ongoing challenges and limitations in estimating and achieving ML robustness by existing approaches. It offers insights and directions for future research on this crucial concept, as a prerequisite for trustworthy AI systems.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：MTLight: Efficient Multi-Task Reinforcement Learning for Traffic Signal  Control</b></summary>
  <p><b>编号</b>：[219]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00886">https://arxiv.org/abs/2404.00886</a></p>
  <p><b>作者</b>：Liwen Zhu,  Peixi Peng,  Zongqing Lu,  Yonghong Tian</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：alleviating traffic congestion, modern cities, great impact, impact on alleviating, congestion in modern</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Traffic signal control has a great impact on alleviating traffic congestion in modern cities. Deep reinforcement learning (RL) has been widely used for this task in recent years, demonstrating promising performance but also facing many challenges such as limited performances and sample inefficiency. To handle these challenges, MTLight is proposed to enhance the agent observation with a latent state, which is learned from numerous traffic indicators. Meanwhile, multiple auxiliary and supervisory tasks are constructed to learn the latent state, and two types of embedding latent features, the task-specific feature and task-shared feature, are used to make the latent state more abundant. Extensive experiments conducted on CityFlow demonstrate that MTLight has leading convergence speed and asymptotic performance. We further simulate under peak-hour pattern in all scenarios with increasing control difficulty and the results indicate that MTLight is highly adaptable.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large  Language Models</b></summary>
  <p><b>编号</b>：[221]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00884">https://arxiv.org/abs/2404.00884</a></p>
  <p><b>作者</b>：Wei He,  Shichun Liu,  Jun Zhao,  Yiwen Ding,  Yi Lu,  Zhiheng Xi,  Tao Gui,  Qi Zhang,  Xuanjing Huang</p>
  <p><b>备注</b>：Accepted to NAACL 2024 Findings</p>
  <p><b>关键词</b>：Large language models, shown promising abilities, Large language, language models, in-context learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have shown promising abilities of in-context learning (ICL), adapting swiftly to new tasks with only few-shot demonstrations. However, current few-shot methods heavily depend on high-quality, query-specific demos, which are often lacking. When faced with out-of-demonstration (OOD) queries, methods that rely on hand-crafted demos or external retrievers might fail. To bridge the gap between limited demos and OOD queries, we propose Self-Demos, a novel prompting method that elicits the inherent generalizability in LLMs by query-aware demo generation. The generated demos strategically interpolate between existing demos and the given query, transforming the query from OOD to ID. To evaluate the effectiveness of our approach, we manually constructed OOD-Toolset, a dataset in the tool-using scenario with over 300 real-world APIs and 1000 instances, each consisting of three tool-use cases as demos and an OOD query. Thorough experiments on our dataset and two public math benchmarks have shown that our method can outperform state-of-the-art baselines in the OOD setting. Moreover, we conduct a range of analyses to validate Self-Demos's generalization and provide more insights.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Bailong: Bilingual Transfer Learning based on QLoRA and Zip-tie  Embedding</b></summary>
  <p><b>编号</b>：[232]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00862">https://arxiv.org/abs/2404.00862</a></p>
  <p><b>作者</b>：Lung-Chuan Chen,  Zong-Ru Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：NLP applications, Traditional Chinese, demonstrated exceptional performance, demonstrated exceptional, Traditional Chinese data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have demonstrated exceptional performance in various NLP applications. However, the majority of existing open-source LLMs are pre-trained primarily on English data and little part of other languages. This deficiency in multilingual training data results in suboptimal performance when applied to languages with fewer available resources. Furthermore, enhancing the performance of LLMs on low-resource languages by full-parameter fine-tuning with additional data requires substantial computational resources, posing computational barriers for research organizations and individual researchers. Consequently, several techniques such as parameter-efficient tuning and advanced embedding initialization have been proposed to address these challenges. In this work, we combine them to facilitate cross-lingual transfer on English-dominated open-source LLM. To effectively enhance the model's proficiency in Traditional Chinese, we conduct secondary pre-training on Llama 2 7B with Traditional Chinese data by leveraging QLoRA and our proposed zip-tie embedding initialization. The resulting model called Bailong, which stands for Bilingual trAnsfer learnIng based on qLOra and zip-tie embeddiNG. We present Bailong-instruct 7B, a fine-tuned version of Bailong 7B optimized for multi-turn dialogue scenarios. Recognizing the inadequacy of benchmark datasets in Traditional Chinese, we further introduce Bailong-bench to assess the alignment of models with human preferences and the capability to follow instructions in both Traditional Chinese and English tasks. In our evaluation, Bailong-instruct 7B exhibits competitive performance on Bailong-bench and other benchmark datasets when compared to other open-source models of similar or even larger parameter sizes. Bailong-instruct 7B and Bailong-bench are publicly available with the aim of empowering the community to build upon our efforts.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Removing Speaker Information from Speech Representation using  Variable-Length Soft Pooling</b></summary>
  <p><b>编号</b>：[236]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00856">https://arxiv.org/abs/2404.00856</a></p>
  <p><b>作者</b>：Injune Hwang,  Kyogu Lee</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：efforts to encode, encode the linguistic, self-supervised framework, speaker information, speech synthesis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, there have been efforts to encode the linguistic information of speech using a self-supervised framework for speech synthesis. However, predicting representations from surrounding representations can inadvertently entangle speaker information in the speech representation. This paper aims to remove speaker information by exploiting the structured nature of speech, composed of discrete units like phonemes with clear boundaries. A neural network predicts these boundaries, enabling variable-length pooling for event-based representation extraction instead of fixed-rate methods. The boundary predictor outputs a probability for the boundary between 0 and 1, making pooling soft. The model is trained to minimize the difference with the pooled representation of the data augmented by time-stretch and pitch-shift. To confirm that the learned representation includes contents information but is independent of speaker information, the model was evaluated with libri-light's phonetic ABX task and SUPERB's speaker identification task.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：TSOM: Small Object Motion Detection Neural Network Inspired by Avian  Visual Circuit</b></summary>
  <p><b>编号</b>：[237]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00855">https://arxiv.org/abs/2404.00855</a></p>
  <p><b>作者</b>：Pignge Hu,  Xiaoteng Zhang,  Mengmeng Li,  Yingjie Zhu,  Li Shi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：small object motion, Detecting small moving, highly challenging task, machine vision systems, small moving objects</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Detecting small moving objects in complex backgrounds from an overhead perspective is a highly challenging task for machine vision systems. As an inspiration from nature, the avian visual system is capable of processing motion information in various complex aerial scenes, and its Retina-OT-Rt visual circuit is highly sensitive to capturing the motion information of small objects from high altitudes. However, more needs to be done on small object motion detection algorithms based on the avian visual system. In this paper, we conducted mathematical modeling based on extensive studies of the biological mechanisms of the Retina-OT-Rt visual circuit. Based on this, we proposed a novel tectum small object motion detection neural network (TSOM). The neural network includes the retina, SGC dendritic, SGC Soma, and Rt layers, each layer corresponding to neurons in the visual pathway. The Retina layer is responsible for accurately projecting input content, the SGC dendritic layer perceives and encodes spatial-temporal information, the SGC Soma layer computes complex motion information and extracts small objects, and the Rt layer integrates and decodes motion information from multiple directions to determine the position of small objects. Extensive experiments on pigeon neurophysiological experiments and image sequence data showed that the TSOM is biologically interpretable and effective in extracting reliable small object motion features from complex high-altitude backgrounds.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：HeteroMILE: a Multi-Level Graph Representation Learning Framework for  Heterogeneous Graphs</b></summary>
  <p><b>编号</b>：[254]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00816">https://arxiv.org/abs/2404.00816</a></p>
  <p><b>作者</b>：Yue Zhang,  Yuntian He,  Saket Gurukar,  Srinivasan Parthasarathy</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：types of entities, ubiquitous in real-world, real-world applications, represent various relationships, graph</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Heterogeneous graphs are ubiquitous in real-world applications because they can represent various relationships between different types of entities. Therefore, learning embeddings in such graphs is a critical problem in graph machine learning. However, existing solutions for this problem fail to scale to large heterogeneous graphs due to their high computational complexity. To address this issue, we propose a Multi-Level Embedding framework of nodes on a heterogeneous graph (HeteroMILE) - a generic methodology that allows contemporary graph embedding methods to scale to large graphs. HeteroMILE repeatedly coarsens the large sized graph into a smaller size while preserving the backbone structure of the graph before embedding it, effectively reducing the computational cost by avoiding time-consuming processing operations. It then refines the coarsened embedding to the original graph using a heterogeneous graph convolution neural network. We evaluate our approach using several popular heterogeneous graph datasets. The experimental results show that HeteroMILE can substantially reduce computational time (approximately 20x speedup) and generate an embedding of better quality for link prediction and node classification.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Towards Realistic Scene Generation with LiDAR Diffusion Models</b></summary>
  <p><b>编号</b>：[255]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00815">https://arxiv.org/abs/2404.00815</a></p>
  <p><b>作者</b>：Haoxi Ran,  Vitor Guizilini,  Yue Wang</p>
  <p><b>备注</b>：CVPR 2024. Code available at this https URL</p>
  <p><b>关键词</b>：photo-realistic image synthesis, Diffusion models, LiDAR Diffusion Models, excel in photo-realistic, image synthesis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diffusion models (DMs) excel in photo-realistic image synthesis, but their adaptation to LiDAR scene generation poses a substantial hurdle. This is primarily because DMs operating in the point space struggle to preserve the curve-like patterns and 3D geometry of LiDAR scenes, which consumes much of their representation power. In this paper, we propose LiDAR Diffusion Models (LiDMs) to generate LiDAR-realistic scenes from a latent space tailored to capture the realism of LiDAR scenes by incorporating geometric priors into the learning pipeline. Our method targets three major desiderata: pattern realism, geometry realism, and object realism. Specifically, we introduce curve-wise compression to simulate real-world LiDAR patterns, point-wise coordinate supervision to learn scene geometry, and patch-wise encoding for a full 3D object context. With these three core designs, our method achieves competitive performance on unconditional LiDAR generation in 64-beam scenario and state of the art on conditional LiDAR generation, while maintaining high efficiency compared to point-based DMs (up to 107$\times$ faster). Furthermore, by compressing LiDAR scenes into a latent space, we enable the controllability of DMs with various conditions such as semantic maps, camera views, and text prompts. Our code and pretrained weights are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Addressing Loss of Plasticity and Catastrophic Forgetting in Continual  Learning</b></summary>
  <p><b>编号</b>：[273]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00781">https://arxiv.org/abs/2404.00781</a></p>
  <p><b>作者</b>：Mohamed Elsayed,  A. Rupam Mahmood</p>
  <p><b>备注</b>：Published in the Proceedings of the 12th International Conference on Learning Representations (ICLR 2024)</p>
  <p><b>关键词</b>：Perturbed Gradient Descent, Utility-based Perturbed Gradient, due to rigid, rigid and unuseful, Deep representation learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems. Finally, in extended reinforcement learning experiments with PPO, we show that while Adam exhibits a performance drop after initial learning, UPGD avoids it by addressing both continual learning issues.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Privacy-preserving Optics for Enhancing Protection in Face  De-identification</b></summary>
  <p><b>编号</b>：[274]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00777">https://arxiv.org/abs/2404.00777</a></p>
  <p><b>作者</b>：Jhon Lopez,  Carlos Hinojosa,  Henry Arguello,  Bernard Ghanem</p>
  <p><b>备注</b>：Accepted to CVPR 2024. Project Website and Code coming soon</p>
  <p><b>关键词</b>：camera usage alongside, usage alongside widespread, alongside widespread computer, widespread computer vision, computer vision technology</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The modern surge in camera usage alongside widespread computer vision technology applications poses significant privacy and security concerns. Current artificial intelligence (AI) technologies aid in recognizing relevant events and assisting in daily tasks in homes, offices, hospitals, etc. The need to access or process personal information for these purposes raises privacy concerns. While software-level solutions like face de-identification provide a good privacy/utility trade-off, they present vulnerabilities to sniffing attacks. In this paper, we propose a hardware-level face de-identification method to solve this vulnerability. Specifically, our approach first learns an optical encoder along with a regression model to obtain a face heatmap while hiding the face identity from the source image. We also propose an anonymization framework that generates a new face using the privacy-preserving image, face heatmap, and a reference face image from a public dataset as input. We validate our approach with extensive simulations and hardware experiments.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery</b></summary>
  <p><b>编号</b>：[283]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00756">https://arxiv.org/abs/2404.00756</a></p>
  <p><b>作者</b>：Cristina Cornelio,  Mohammed Diab</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：implementing recovery procedures, challenging in robotics, execution and implementing, procedures is challenging, task execution</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recognizing failures during task execution and implementing recovery procedures is challenging in robotics. Traditional approaches rely on the availability of extensive data or a tight set of constraints, while more recent approaches leverage large language models (LLMs) to verify task steps and replan accordingly. However, these methods often operate offline, necessitating scene resets and incurring in high costs. This paper introduces Recover, a neuro-symbolic framework for online failure identification and recovery. By integrating ontologies, logical rules, and LLM-based planners, Recover exploits symbolic information to enhance the ability of LLMs to generate recovery plans and also to decrease the associated costs. In order to demonstrate the capabilities of our method in a simulated kitchen environment, we introduce OntoThor, an ontology describing the AI2Thor simulator setting. Empirical evaluation shows that OntoThor's logical rules accurately detect all failures in the analyzed tasks, and that Recover considerably outperforms, for both failure detection and recovery, a baseline method reliant solely on LLMs.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：On the True Distribution Approximation of Minimum Bayes-Risk Decoding</b></summary>
  <p><b>编号</b>：[284]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00752">https://arxiv.org/abs/2404.00752</a></p>
  <p><b>作者</b>：Atsumoto Ohashi,  Ukyo Honda,  Tetsuro Morimura,  Yuu Jinnai</p>
  <p><b>备注</b>：NAACL 2024 (main conference)</p>
  <p><b>关键词</b>：recently gained renewed, gained renewed attention, Minimum Bayes-risk, MBR decoding, recently gained</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Minimum Bayes-risk (MBR) decoding has recently gained renewed attention in text generation. MBR decoding considers texts sampled from a model as pseudo-references and selects the text with the highest similarity to the others. Therefore, sampling is one of the key elements of MBR decoding, and previous studies reported that the performance varies by sampling methods. From a theoretical standpoint, this performance variation is likely tied to how closely the samples approximate the true distribution of references. However, this approximation has not been the subject of in-depth study. In this study, we propose using anomaly detection to measure the degree of approximation. We first closely examine the performance variation and then show that previous hypotheses about samples do not correlate well with the variation, but our introduced anomaly scores do. The results are the first to empirically support the link between the performance and the core assumption of MBR decoding.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Benchmark Transparency: Measuring the Impact of Data on Evaluation</b></summary>
  <p><b>编号</b>：[286]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00748">https://arxiv.org/abs/2404.00748</a></p>
  <p><b>作者</b>：Venelin Kovatchev,  Matthew Lease</p>
  <p><b>备注</b>：Accepted at NAACL 2024</p>
  <p><b>关键词</b>：NLP models, data distribution, paper we present, present an exploratory, exploratory research</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper we present an exploratory research on quantifying the impact that data distribution has on the performance and evaluation of NLP models. We propose an automated framework that measures the data point distribution across 6 different dimensions: ambiguity, difficulty, discriminability, length, noise, and perplexity.
We use disproportional stratified sampling to measure how much the data distribution affects absolute (Acc/F1) and relative (Rank) model performance. We experiment on 2 different datasets (SQUAD and MNLI) and test a total of 135 different models (125 on SQUAD and 10 on MNLI). We demonstrate that without explicit control of the data distribution, standard evaluation frameworks are inconsistent and unreliable. We find that the impact of the data is statistically significant and is often larger than the impact of changing the metric.
In a second set of experiments, we demonstrate that the impact of data on evaluation is not just observable, but also predictable. We propose to use benchmark transparency as a method for comparing datasets and quantifying the similarity between them. We find that the ``dataset similarity vector'' can be used to predict how well a model generalizes out of distribution.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Mining Weighted Sequential Patterns in Incremental Uncertain Databases</b></summary>
  <p><b>编号</b>：[287]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00746">https://arxiv.org/abs/2404.00746</a></p>
  <p><b>作者</b>：Kashob Kumar Roy,  Md Hasibul Haque Moon,  Md Mahmudur Rahman,  Chowdhury Farhan Ahmed,  Carson Kai-Sang Leung</p>
  <p><b>备注</b>：Accepted to Information Science journal</p>
  <p><b>关键词</b>：science and technology, exponential rate, rapid development, development of science, data is increasing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Due to the rapid development of science and technology, the importance of imprecise, noisy, and uncertain data is increasing at an exponential rate. Thus, mining patterns in uncertain databases have drawn the attention of researchers. Moreover, frequent sequences of items from these databases need to be discovered for meaningful knowledge with great impact. In many real cases, weights of items and patterns are introduced to find interesting sequences as a measure of importance. Hence, a constraint of weight needs to be handled while mining sequential patterns. Besides, due to the dynamic nature of databases, mining important information has become more challenging. Instead of mining patterns from scratch after each increment, incremental mining algorithms utilize previously mined information to update the result immediately. Several algorithms exist to mine frequent patterns and weighted sequences from incremental databases. However, these algorithms are confined to mine the precise ones. Therefore, we have developed an algorithm to mine frequent sequences in an uncertain database in this work. Furthermore, we have proposed two new techniques for mining when the database is incremental. Extensive experiments have been conducted for performance evaluation. The analysis showed the efficiency of our proposed framework.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：The Larger the Better? Improved LLM Code-Generation via Budget  Reallocation</b></summary>
  <p><b>编号</b>：[296]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00725">https://arxiv.org/abs/2404.00725</a></p>
  <p><b>作者</b>：Michael Hassid,  Tal Remez,  Jonas Gehring,  Roy Schwartz,  Yossi Adi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, common belief, belief that large, large language, models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>It is a common belief that large language models (LLMs) are better than smaller-sized ones. However, larger models also require significantly more time and compute during inference. This begs the question: what happens when both models operate under the same budget? (e.g., compute, run-time). To address this question, we analyze code generation LLMs of various sizes and make comparisons such as running a 70B model once vs. generating five outputs from a 13B model and selecting one. Our findings reveal that, in a standard unit-test setup, the repeated use of smaller models can yield consistent improvements, with gains of up to 15% across five tasks. On the other hand, in scenarios where unit-tests are unavailable, a ranking-based selection of candidates from the smaller model falls short of the performance of a single output from larger ones. Our results highlight the potential of using smaller models instead of larger ones, and the importance of studying approaches for ranking LLM outputs.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：DRCT: Saving Image Super-resolution away from Information Bottleneck</b></summary>
  <p><b>编号</b>：[298]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00722">https://arxiv.org/abs/2404.00722</a></p>
  <p><b>作者</b>：Chih-Chung Hsu,  Chia-Ming Lee,  Yi-Shiuan Chou</p>
  <p><b>备注</b>：Submitted to NTIRE 2024</p>
  <p><b>关键词</b>：Vision Transformer-based applications, low-level vision tasks, achieved widespread success, Vision Transformer-based, Transformer-based applications</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, Vision Transformer-based applications to low-level vision tasks have achieved widespread success. Unlike CNN-based models, Transformers are more adept at capturing long-range dependencies, enabling the reconstruction of images utilizing information from non-local areas. In the domain of super-resolution, Swin-transformer-based approaches have become mainstream due to their capacity to capture global spatial information and their shifting-window attention mechanism that facilitates the interchange of information between different windows. Many researchers have enhanced image quality and network efficiency by expanding the receptive field or designing complex networks, yielding commendable results. However, we observed that spatial information tends to diminish during the forward propagation process due to increased depth, leading to a loss of spatial information and, consequently, limiting the model's potential. To address this, we propose the Dense-residual-connected Transformer (DRCT), aimed at mitigating the loss of spatial information through dense-residual connections between layers, thereby unleashing the model's potential and enhancing performance. Experiment results indicate that our approach is not only straightforward but also achieves remarkable efficiency, surpassing state-of-the-art methods and performing commendably at NTIRE2024.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Survey of Computerized Adaptive Testing: A Machine Learning Perspective</b></summary>
  <p><b>编号</b>：[301]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00712">https://arxiv.org/abs/2404.00712</a></p>
  <p><b>作者</b>：Qi Liu,  Yan Zhuang,  Haoyang Bi,  Zhenya Huang,  Weizhe Huang,  Jiatong Li,  Junhao Yu,  Zirui Liu,  Zirui Hu,  Yuting Hong,  Zachary A. Pardos,  Haiping Ma,  Mengxiao Zhu,  Shijin Wang,  Enhong Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Computerized Adaptive Testing, dynamically adjusting test, proficiency of examinees, assessing the proficiency, dynamically adjusting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Computerized Adaptive Testing (CAT) provides an efficient and tailored method for assessing the proficiency of examinees, by dynamically adjusting test questions based on their performance. Widely adopted across diverse fields like education, healthcare, sports, and sociology, CAT has revolutionized testing practices. While traditional methods rely on psychometrics and statistics, the increasing complexity of large-scale testing has spurred the integration of machine learning techniques. This paper aims to provide a machine learning-focused survey on CAT, presenting a fresh perspective on this adaptive testing method. By examining the test question selection algorithm at the heart of CAT's adaptivity, we shed light on its functionality. Furthermore, we delve into cognitive diagnosis models, question bank construction, and test control within CAT, exploring how machine learning can optimize these components. Through an analysis of current methods, strengths, limitations, and challenges, we strive to develop robust, fair, and efficient CAT systems. By bridging psychometric-driven CAT research with machine learning, this survey advocates for a more inclusive and interdisciplinary approach to the future of adaptive testing.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Generative Retrieval as Multi-Vector Dense Retrieval</b></summary>
  <p><b>编号</b>：[312]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00684">https://arxiv.org/abs/2404.00684</a></p>
  <p><b>作者</b>：Shiguang Wu,  Wenda Wei,  Mengqi Zhang,  Zhumin Chen,  Jun Ma,  Zhaochun Ren,  Maarten de Rijke,  Pengjie Ren</p>
  <p><b>备注</b>：12 pages, 5 figures, 8 tables, accepted at SIGIR 2024</p>
  <p><b>关键词</b>：Generative retrieval, retrieval, dense retrieval, Generative retrieval generates, Generative</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generative retrieval generates identifiers of relevant documents in an end-to-end manner using a sequence-to-sequence architecture for a given query. The relation between generative retrieval and other retrieval methods, especially those based on matching within dense retrieval models, is not yet fully comprehended. Prior work has demonstrated that generative retrieval with atomic identifiers is equivalent to single-vector dense retrieval. Accordingly, generative retrieval exhibits behavior analogous to hierarchical search within a tree index in dense retrieval when using hierarchical semantic identifiers. However, prior work focuses solely on the retrieval stage without considering the deep interactions within the decoder of generative retrieval.
In this paper, we fill this gap by demonstrating that generative retrieval and multi-vector dense retrieval share the same framework for measuring the relevance to a query of a document. Specifically, we examine the attention layer and prediction head of generative retrieval, revealing that generative retrieval can be understood as a special case of multi-vector dense retrieval. Both methods compute relevance as a sum of products of query and document vectors and an alignment matrix. We then explore how generative retrieval applies this framework, employing distinct strategies for computing document token vectors and the alignment matrix. We have conducted experiments to verify our conclusions and show that both paradigms exhibit commonalities of term matching in their alignment matrix.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：LLM meets Vision-Language Models for Zero-Shot One-Class Classification</b></summary>
  <p><b>编号</b>：[319]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00675">https://arxiv.org/abs/2404.00675</a></p>
  <p><b>作者</b>：Yassir Bendou,  Giulia Lioi,  Bastien Pasdeloup,  Lukas Mauch,  Ghouthi Boukli Hacene,  Fabien Cardinaux,  Vincent Gripon</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：zero-shot one-class visual, one-class visual classification, problem of zero-shot, zero-shot one-class, one-class visual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider the problem of zero-shot one-class visual classification. In this setting, only the label of the target class is available, and the goal is to discriminate between positive and negative query samples without requiring any validation example from the target task. We propose a two-step solution that first queries large language models for visually confusing objects and then relies on vision-language pre-trained models (e.g., CLIP) to perform classification. By adapting large-scale vision benchmarks, we demonstrate the ability of the proposed method to outperform adapted off-the-shelf alternatives in this setting. Namely, we propose a realistic benchmark where negative query samples are drawn from the same original dataset as positive ones, including a granularity-controlled version of iNaturalist, where negative samples are at a fixed distance in the taxonomy tree from the positive ones. Our work shows that it is possible to discriminate between a single category and other semantically related ones using only its label</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：A Survey of Privacy-Preserving Model Explanations: Privacy Risks,  Attacks, and Countermeasures</b></summary>
  <p><b>编号</b>：[321]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00673">https://arxiv.org/abs/2404.00673</a></p>
  <p><b>作者</b>：Thanh Tam Nguyen,  Thanh Trung Huynh,  Zhao Ren,  Thanh Toan Nguyen,  Phi Le Nguyen,  Hongzhi Yin,  Quoc Viet Hung Nguyen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：privacy implications intensifies, continues to expand, implications intensifies, adoption of explainable, urgency to address</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As the adoption of explainable AI (XAI) continues to expand, the urgency to address its privacy implications intensifies. Despite a growing corpus of research in AI privacy and explainability, there is little attention on privacy-preserving model explanations. This article presents the first thorough survey about privacy attacks on model explanations and their countermeasures. Our contribution to this field comprises a thorough analysis of research papers with a connected taxonomy that facilitates the categorisation of privacy attacks and countermeasures based on the targeted explanations. This work also includes an initial investigation into the causes of privacy leaks. Finally, we discuss unresolved issues and prospective research directions uncovered in our analysis. This survey aims to be a valuable resource for the research community and offers clear insights for those new to this domain. To support ongoing research, we have established an online resource repository, which will be continuously updated with new and relevant findings. Interested readers are encouraged to access our repository at this https URL.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：A General and Efficient Training for Transformer via Token Expansion</b></summary>
  <p><b>编号</b>：[322]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00672">https://arxiv.org/abs/2404.00672</a></p>
  <p><b>作者</b>：Wenxuan Huang,  Yunhang Shen,  Jiao Xie,  Baochang Zhang,  Gaoqi He,  Ke Li,  Xing Sun,  Shaohui Lin</p>
  <p><b>备注</b>：Accepted to CVPR 2024. Code is available at this https URL</p>
  <p><b>关键词</b>：large training cost, extremely large training, Vision Transformers, training, requires an extremely</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The remarkable performance of Vision Transformers (ViTs) typically requires an extremely large training cost. Existing methods have attempted to accelerate the training of ViTs, yet typically disregard method universality with accuracy dropping. Meanwhile, they break the training consistency of the original transformers, including the consistency of hyper-parameters, architecture, and strategy, which prevents them from being widely applied to different Transformer networks. In this paper, we propose a novel token growth scheme Token Expansion (termed ToE) to achieve consistent training acceleration for ViTs. We introduce an "initialization-expansion-merging" pipeline to maintain the integrity of the intermediate feature distribution of original transformers, preventing the loss of crucial learnable information in the training process. ToE can not only be seamlessly integrated into the training and fine-tuning process of transformers (e.g., DeiT and LV-ViT), but also effective for efficient training frameworks (e.g., EfficientTrain), without twisting the original training hyper-parameters, architecture, and introducing additional training strategies. Extensive experiments demonstrate that ToE achieves about 1.3x faster for the training of ViTs in a lossless manner, or even with performance gains over the full-token training baselines. Code is available at this https URL .</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：Observations on Building RAG Systems for Technical Documents</b></summary>
  <p><b>编号</b>：[329]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00657">https://arxiv.org/abs/2404.00657</a></p>
  <p><b>作者</b>：Sumit Soman,  Sujoy Roychowdhury</p>
  <p><b>备注</b>：Published as a Tiny Paper at ICLR 2024</p>
  <p><b>关键词</b>：Retrieval augmented generation, capture domain information, technical documents creates, documents creates challenges, Retrieval augmented</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Retrieval augmented generation (RAG) for technical documents creates challenges as embeddings do not often capture domain information. We review prior art for important factors affecting RAG and perform experiments to highlight best practices and potential challenges to build RAG systems for technical documents.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：WavLLM: Towards Robust and Adaptive Speech Large Language Model</b></summary>
  <p><b>编号</b>：[330]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00656">https://arxiv.org/abs/2404.00656</a></p>
  <p><b>作者</b>：Shujie Hu,  Long Zhou,  Shujie Liu,  Sanyuan Chen,  Hongkun Hao,  Jing Pan,  Xunying Liu,  Jinyu Li,  Sunit Sivasankaran,  Linquan Liu,  Furu Wei</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：natural language processing, progressively broadening, perception and generation, recent advancements, revolutionized the field</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. We validate the proposed model on universal speech benchmarks including tasks such as ASR, ST, SV, ER, and also apply it to specialized datasets like Gaokao English listening comprehension set for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks on the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach. Furthermore, our model successfully completes Gaokao tasks without specialized training. The codes, models, audio, and Gaokao evaluation set can be accessed at \url{this http URL}.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：Learning Off-policy with Model-based Intrinsic Motivation For Active  Online Exploration</b></summary>
  <p><b>编号</b>：[333]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00651">https://arxiv.org/abs/2404.00651</a></p>
  <p><b>作者</b>：Yibo Wang,  Jiang Zhao</p>
  <p><b>备注</b>：Preprint</p>
  <p><b>关键词</b>：demonstrated notable progress, deep reinforcement learning, Recent advancements, spanning both model-based, model-free paradigms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advancements in deep reinforcement learning (RL) have demonstrated notable progress in sample efficiency, spanning both model-based and model-free paradigms. Despite the identification and mitigation of specific bottlenecks in prior works, the agent's exploration ability remains under-emphasized in the realm of sample-efficient RL. This paper investigates how to achieve sample-efficient exploration in continuous control tasks. We introduce an RL algorithm that incorporates a predictive model and off-policy learning elements, where an online planner enhanced by a novelty-aware terminal value function is employed for sample collection. Leveraging the forward predictive error within a latent state space, we derive an intrinsic reward without incurring parameters overhead. This reward establishes a solid connection to model uncertainty, allowing the agent to effectively overcome the asymptotic performance gap. Through extensive experiments, our method shows competitive or even superior performance compared to prior works, especially the sparse reward cases.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：Learning to Generate Conditional Tri-plane for 3D-aware Expression  Controllable Portrait Animation</b></summary>
  <p><b>编号</b>：[341]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00636">https://arxiv.org/abs/2404.00636</a></p>
  <p><b>作者</b>：Taekyung Ki,  Dongchan Min,  Gyeongsu Chae</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：control the facial, expression, image, camera view, portrait animation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we present Export3D, a one-shot 3D-aware portrait animation method that is able to control the facial expression and camera view of a given portrait image. To achieve this, we introduce a tri-plane generator that directly generates a tri-plane of 3D prior by transferring the expression parameter of 3DMM into the source image. The tri-plane is then decoded into the image of different view through a differentiable volume rendering. Existing portrait animation methods heavily rely on image warping to transfer the expression in the motion space, challenging on disentanglement of appearance and expression. In contrast, we propose a contrastive pre-training framework for appearance-free expression parameter, eliminating undesirable appearance swap when transferring a cross-identity expression. Extensive experiments show that our pre-training framework can learn the appearance-free expression representation hidden in 3DMM, and our model can generate 3D-aware expression controllable portrait image without appearance swap in the cross-identity manner.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：Learning to Plan for Language Modeling from Unlabeled Data</b></summary>
  <p><b>编号</b>：[354]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00614">https://arxiv.org/abs/2404.00614</a></p>
  <p><b>作者</b>：Nathan Cornille,  Marie-Francine Moens,  Florian Mai</p>
  <p><b>备注</b>：under review</p>
  <p><b>关键词</b>：unlabeled corpus, labeled data, training to predict, learn to perform, perform many tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>By training to predict the next token in an unlabeled corpus, large language models learn to perform many tasks without any labeled data. However, their next-token-prediction objective arguably limits their performance in scenarios that require planning, such as writing a coherent article. In this paper, we train a module for planning the future writing process via a self-supervised learning objective. By conditioning on generated latent plans, our model extends the successful language model formula to more abstract planning in an unsupervised way. Empirically, we demonstrate that our method improves language modeling performance in general, particularly with respect to the text structure. Because our framework uses a planner module that is unsupervised and external to the language model, new planner modules can be trained at large scale and easily be shared with the community.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：Extensive Self-Contrast Enables Feedback-Free Language Model Alignment</b></summary>
  <p><b>编号</b>：[359]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00604">https://arxiv.org/abs/2404.00604</a></p>
  <p><b>作者</b>：Xiao Liu,  Xixuan Song,  Yuxiao Dong,  Jie Tang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Reinforcement learning, recent large language, central technique, technique for recent, RLHF</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reinforcement learning from human feedback (RLHF) has been a central technique for recent large language model (LLM) alignment. However, its heavy dependence on costly human or LLM-as-Judge preference feedback could stymie its wider applications. In this work, we introduce Self-Contrast, a feedback-free large language model alignment method via exploiting extensive self-generated negatives. With only supervised fine-tuning (SFT) targets, Self-Contrast leverages the LLM itself to generate massive diverse candidates, and harnesses a pre-trained embedding model to filter multiple negatives according to text similarity. Theoretically, we illustrate that in this setting, merely scaling negative responses can still effectively approximate situations with more balanced positive and negative preference annotations. Our experiments with direct preference optimization (DPO) on three datasets show that, Self-Contrast could consistently outperform SFT and standard DPO training by large margins. And as the number of self-generated negatives increases, the performance of Self-Contrast continues to grow. Code and data are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：AI Act and Large Language Models (LLMs): When critical issues and  privacy impact require human and ethical oversight</b></summary>
  <p><b>编号</b>：[362]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00600">https://arxiv.org/abs/2404.00600</a></p>
  <p><b>作者</b>：Nicola Fabiano</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, personal data protection, artificial intelligence systems, Language Models, Large Language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The imposing evolution of artificial intelligence systems and, specifically, of Large Language Models (LLM) makes it necessary to carry out assessments of their level of risk and the impact they may have in the area of privacy, personal data protection and at an ethical level, especially on the weakest and most vulnerable. This contribution addresses human oversight, ethical oversight, and privacy impact assessment.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：EvoCodeBench: An Evolving Code Generation Benchmark Aligned with  Real-World Code Repositories</b></summary>
  <p><b>编号</b>：[363]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00599">https://arxiv.org/abs/2404.00599</a></p>
  <p><b>作者</b>：Jia Li,  Ge Li,  Xuanming Zhang,  Yihong Dong,  Zhi Jin</p>
  <p><b>备注</b>：Data: this https URL</p>
  <p><b>关键词</b>：Large Language Models, evaluate Large Language, Language Models, Large Language, evaluate Large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>How to evaluate Large Language Models (LLMs) in code generation is an open question. Existing benchmarks demonstrate poor alignment with real-world code repositories and are insufficient to evaluate the coding abilities of LLMs. This paper proposes a new benchmark - EvoCodeBench to address the preceding problems, which has three primary advances. (1) EvoCodeBench aligns with real-world repositories in multiple dimensions, e.g., code distributions and dependency distributions. (2) EvoCodeBench offers comprehensive annotations (e.g., requirements, reference code, and reference dependencies), and robust evaluation metrics (e.g., Pass@k and Recall@k). (3) EvoCodeBench is an evolving benchmark to avoid data leakage. We build an automatic pipeline to update EvoCodeBench from the latest repositories. We release the first version - EvoCodeBench-2403, containing 275 samples from 25 real-world repositories. Based on EvoCodeBench, we propose repository-level code generation and evaluate 10 popular LLMs (e.g., gpt-4, gpt-3.5, DeepSeek Coder, StarCoder 2, CodeLLaMa, Gemma, and Qwen 1.5). Our experiments reveal the coding abilities of these LLMs in real-world repositories. For example, the highest Pass@1 of gpt-4 only is 20.73% in our experiments. We also analyze failed cases and summarize the shortcomings of existing LLMs in EvoCodeBench. We release EvoCodeBench, all prompts, and LLMs' completions for further community analysis.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：LAESI: Leaf Area Estimation with Synthetic Imagery</b></summary>
  <p><b>编号</b>：[369]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00593">https://arxiv.org/abs/2404.00593</a></p>
  <p><b>作者</b>：Jacek Kałużny,  Yannik Schreckenberg,  Karol Cyganik,  Peter Annighöfer,  Sören Pirk,  Dominik L. Michels,  Mikolaj Cieslak,  Farhah Assaad-Gerbert,  Bedrich Benes,  Wojciech Pałubicki</p>
  <p><b>备注</b>：10 pages, 12 figures, 1 table</p>
  <p><b>关键词</b>：Synthetic Leaf Dataset, Synthetic Leaf, synthetic leaf images, surface area labels, millimeter paper</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce LAESI, a Synthetic Leaf Dataset of 100,000 synthetic leaf images on millimeter paper, each with semantic masks and surface area labels. This dataset provides a resource for leaf morphology analysis primarily aimed at beech and oak leaves. We evaluate the applicability of the dataset by training machine learning models for leaf surface area prediction and semantic segmentation, using real images for validation. Our validation shows that these models can be trained to predict leaf surface area with a relative error not greater than an average human annotator. LAESI also provides an efficient framework based on 3D procedural models and generative AI for the large-scale, controllable generation of data with potential further applications in agriculture and biology. We evaluate the inclusion of generative AI in our procedural data generation pipeline and show how data filtering based on annotation consistency results in datasets which allow training the highest performing vision models.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：Memory-based Cross-modal Semantic Alignment Network for Radiology Report  Generation</b></summary>
  <p><b>编号</b>：[373]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00588">https://arxiv.org/abs/2404.00588</a></p>
  <p><b>作者</b>：Yitian Tao,  Liyan Ma,  Jing Yu,  Han Zhang</p>
  <p><b>备注</b>：12 pages, 8 figures</p>
  <p><b>关键词</b>：reports automatically reduces, radiology reports automatically, automatically reduces, reduces the workload, workload of radiologists</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generating radiology reports automatically reduces the workload of radiologists and helps the diagnoses of specific diseases. Many existing methods take this task as modality transfer process. However, since the key information related to disease accounts for a small proportion in both image and report, it is hard for the model to learn the latent relation between the radiology image and its report, thus failing to generate fluent and accurate radiology reports. To tackle this problem, we propose a memory-based cross-modal semantic alignment model (MCSAM) following an encoder-decoder paradigm. MCSAM includes a well initialized long-term clinical memory bank to learn disease-related representations as well as prior knowledge for different modalities to retrieve and use the retrieved memory to perform feature consolidation. To ensure the semantic consistency of the retrieved cross modal prior knowledge, a cross-modal semantic alignment module (SAM) is proposed. SAM is also able to generate semantic visual feature embeddings which can be added to the decoder and benefits report generation. More importantly, to memorize the state and additional information while generating reports with the decoder, we use learnable memory tokens which can be seen as prompts. Extensive experiments demonstrate the promising performance of our proposed method which generates state-of-the-art performance on the MIMIC-CXR dataset.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：RLGNet: Repeating-Local-Global History Network for Temporal Knowledge  Graph Reasoning</b></summary>
  <p><b>编号</b>：[374]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00586">https://arxiv.org/abs/2404.00586</a></p>
  <p><b>作者</b>：Ao Lv,  Yongzhong Huang,  Guige Ouyang,  Yue Chen,  Haoran Xie</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Temporal Knowledge Graph, Knowledge Graph, Temporal Knowledge, historical information, predict the future</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Temporal Knowledge Graph (TKG) reasoning is based on historical information to predict the future. Therefore, parsing and mining historical information is key to predicting the future. Most existing methods fail to concurrently address and comprehend historical information from both global and local perspectives. Neglecting the global view might result in overlooking macroscopic trends and patterns, while ignoring the local view can lead to missing critical detailed information. Additionally, some methods do not focus on learning from high-frequency repeating events, which means they may not fully grasp frequently occurring historical events. To this end, we propose the \textbf{R}epetitive-\textbf{L}ocal-\textbf{G}lobal History \textbf{Net}work(RLGNet). We utilize a global history encoder to capture the overarching nature of historical information. Subsequently, the local history encoder provides information related to the query timestamp. Finally, we employ the repeating history encoder to identify and learn from frequently occurring historical events. In the evaluation on six benchmark datasets, our approach generally outperforms existing TKG reasoning models in multi-step and single-step reasoning tasks.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：A Review of Modern Recommender Systems Using Generative Models  (Gen-RecSys)</b></summary>
  <p><b>编号</b>：[376]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00579">https://arxiv.org/abs/2404.00579</a></p>
  <p><b>作者</b>：Yashar Deldjoo,  Zhankui He,  Julian McAuley,  Anton Korikov,  Scott Sanner,  Arnau Ramisa,  René Vidal,  Maheswaran Sathiamoorthy,  Atoosa Kasirzadeh,  Silvia Milano</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Traditional recommender systems, primary data source, user-item rating histories, Traditional recommender, recommender systems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Traditional recommender systems (RS) have used user-item rating histories as their primary data source, with collaborative filtering being one of the principal methods. However, generative models have recently developed abilities to model and sample from complex data distributions, including not only user-item interaction histories but also text, images, and videos - unlocking this rich data for novel recommendation tasks. Through this comprehensive and multi-disciplinary survey, we aim to connect the key advancements in RS using Generative Models (Gen-RecSys), encompassing: a foundational overview of interaction-driven generative models; the application of large language models (LLM) for generative recommendation, retrieval, and conversational recommendation; and the integration of multimodal models for processing and generating image and video content in RS. Our holistic perspective allows us to highlight necessary paradigms for evaluating the impact and harm of Gen-RecSys and identify open challenges. A more up-to-date version of the papers is maintained at: this https URL.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：Automated Bi-Fold Weighted Ensemble Algorithms and its Application to  Brain Tumor Detection and Classification</b></summary>
  <p><b>编号</b>：[378]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00576">https://arxiv.org/abs/2404.00576</a></p>
  <p><b>作者</b>：PoTsang B. Huang,  Muhammad Rizwan,  Mehboob Ali</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：types of cancers, uncontrolled and unstructured, unstructured growth, highest mortality rates, mortality rates</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The uncontrolled and unstructured growth of brain cells is known as brain tumor, which has one of the highest mortality rates among diseases from all types of cancers. Due to limited diagnostic and treatment capabilities, they pose significant challenges, especially in third-world countries. Early diagnosis plays a vital role in effectively managing brain tumors and reducing mortality rates. However, the availability of diagnostic methods is hindered by various limitations, including high costs and lengthy result acquisition times, impeding early detection of the disease. In this study, we present two cutting-edge bi-fold weighted voting ensemble models that aim to boost the effectiveness of weighted ensemble methods. These two proposed methods combine the classification outcomes from multiple classifiers and determine the optimal result by selecting the one with the highest probability in the first approach, and the highest weighted prediction in the second technique. These approaches significantly improve the overall performance of weighted ensemble techniques. In the first proposed method, we improve the soft voting technique (SVT) by introducing a novel unsupervised weight calculating schema (UWCS) to enhance its weight assigning capability, known as the extended soft voting technique (ESVT). Secondly, we propose a novel weighted method (NWM) by using the proposed UWCS. Both of our approaches incorporate three distinct models: a custom-built CNN, VGG-16, and InceptionResNetV2 which has been trained on publicly available datasets. The effectiveness of our proposed systems is evaluated through blind testing, where exceptional results are achieved. We then establish a comparative analysis of the performance of our proposed methods with that of SVT to show their superiority and effectiveness.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：A Theory for Length Generalization in Learning to Reason</b></summary>
  <p><b>编号</b>：[388]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00560">https://arxiv.org/abs/2404.00560</a></p>
  <p><b>作者</b>：Changnan Xiao,  Bing Liu</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2311.16173</p>
  <p><b>关键词</b>：Length generalization, learning to reason, problems, reasoning problems, generalization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Length generalization (LG) is a challenging problem in learning to reason. It refers to the phenomenon that when trained on reasoning problems of smaller lengths or sizes, the resulting model struggles with problems of larger sizes or lengths. Although LG has been studied by many researchers, the challenge remains. This paper proposes a theoretical study of LG for problems whose reasoning processes can be modeled as DAGs (directed acyclic graphs). The paper first identifies and proves the conditions under which LG can be achieved in learning to reason. It then designs problem representations based on the theory to learn to solve challenging reasoning problems like parity, addition, and multiplication, using a Transformer to achieve perfect LG.</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：Deep Extrinsic Manifold Representation for Vision Tasks</b></summary>
  <p><b>编号</b>：[395]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00544">https://arxiv.org/abs/2404.00544</a></p>
  <p><b>作者</b>：Tongtong Zhang,  Xian Wei,  Yuanxiang Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Extrinsic Manifold Representation, training neural networks, Deep Extrinsic Manifold, Non-Euclidean data, manifold representations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Non-Euclidean data is frequently encountered across different fields, yet there is limited literature that addresses the fundamental challenge of training neural networks with manifold representations as outputs. We introduce the trick named Deep Extrinsic Manifold Representation (DEMR) for visual tasks in this context. DEMR incorporates extrinsic manifold embedding into deep neural networks, which helps generate manifold representations. The DEMR approach does not directly optimize the complex geodesic loss. Instead, it focuses on optimizing the computation graph within the embedded Euclidean space, allowing for adaptability to various architectural requirements. We provide empirical evidence supporting the proposed concept on two types of manifolds, $SE(3)$ and its associated quotient manifolds. This evidence offers theoretical assurances regarding feasibility, asymptotic properties, and generalization capability. The experimental results show that DEMR effectively adapts to point cloud alignment, producing outputs in $ SE(3) $, as well as in illumination subspace learning with outputs on the Grassmann manifold.</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：Embodied Active Defense: Leveraging Recurrent Feedback to Counter  Adversarial Patches</b></summary>
  <p><b>编号</b>：[396]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00540">https://arxiv.org/abs/2404.00540</a></p>
  <p><b>作者</b>：Lingxuan Wu,  Xiao Yang,  Yinpeng Dong,  Liuwei Xie,  Hang Su,  Jun Zhu</p>
  <p><b>备注</b>：27pages</p>
  <p><b>关键词</b>：deep neural networks, motivated numerous defense, counter adversarial patches, adversarial patches, numerous defense strategies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The vulnerability of deep neural networks to adversarial patches has motivated numerous defense strategies for boosting model robustness. However, the prevailing defenses depend on single observation or pre-established adversary information to counter adversarial patches, often failing to be confronted with unseen or adaptive adversarial attacks and easily exhibiting unsatisfying performance in dynamic 3D environments. Inspired by active human perception and recurrent feedback mechanisms, we develop Embodied Active Defense (EAD), a proactive defensive strategy that actively contextualizes environmental information to address misaligned adversarial patches in 3D real-world settings. To achieve this, EAD develops two central recurrent sub-modules, i.e., a perception module and a policy module, to implement two critical functions of active vision. These models recurrently process a series of beliefs and observations, facilitating progressive refinement of their comprehension of the target object and enabling the development of strategic actions to counter adversarial patches in 3D environments. To optimize learning efficiency, we incorporate a differentiable approximation of environmental dynamics and deploy patches that are agnostic to the adversary strategies. Extensive experiments demonstrate that EAD substantially enhances robustness against a variety of patches within just a few steps through its action policy in safety-critical tasks (e.g., face recognition and object detection), without compromising standard accuracy. Furthermore, due to the attack-agnostic characteristic, EAD facilitates excellent generalization to unseen attacks, diminishing the averaged attack success rate by 95 percent across a range of unseen adversarial attacks.</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：Comparing Bad Apples to Good Oranges: Aligning Large Language Models via  Joint Preference Optimization</b></summary>
  <p><b>编号</b>：[400]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00530">https://arxiv.org/abs/2404.00530</a></p>
  <p><b>作者</b>：Hritik Bansal,  Ashima Suvarna,  Gantavya Bhatt,  Nanyun Peng,  Kai-Wei Chang,  Aditya Grover</p>
  <p><b>备注</b>：25 pages, 14 figures, 5 tables</p>
  <p><b>关键词</b>：large language models, aligning large language, comparing multiple generations, multiple generations conditioned, acquiring human preferences</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A common technique for aligning large language models (LLMs) relies on acquiring human preferences by comparing multiple generations conditioned on a fixed context. This only leverages the pairwise comparisons when the generations are placed in an identical context. However, such conditional rankings often fail to capture the complex and multidimensional aspects of human preferences. In this work, we revisit the traditional paradigm of preference acquisition and propose a new axis that is based on eliciting preferences jointly over the instruction-response pairs. While prior preference optimizations are designed for conditional ranking protocols (e.g., DPO), our proposed preference acquisition protocol introduces DOVE, a new preference optimization objective that upweights the joint probability of the chosen instruction-response pair over the rejected instruction-response pair. Interestingly, we find that the LLM trained with joint instruction-response preference data using DOVE outperforms the LLM trained with DPO by 5.2% and 3.3% win-rate for the summarization and open-ended dialogue datasets, respectively. Our findings reveal that joint preferences over instruction and response pairs can significantly enhance the alignment of LLMs by tapping into a broader spectrum of human preference elicitation. The data and code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：The Emotional Impact of Game Duration: A Framework for Understanding  Player Emotions in Extended Gameplay Sessions</b></summary>
  <p><b>编号</b>：[404]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00526">https://arxiv.org/abs/2404.00526</a></p>
  <p><b>作者</b>：Anoop Kumar,  Suresh Dodda,  Navin Kamuni,  Venkata Sai Mahesh Vuppalapati</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：played a crucial, crucial role, role in entertainment, lockdown period, period when people</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Video games have played a crucial role in entertainment since their development in the 1970s, becoming even more prominent during the lockdown period when people were looking for ways to entertain them. However, at that time, players were unaware of the significant impact that playtime could have on their feelings. This has made it challenging for designers and developers to create new games since they have to control the emotional impact that these games will take on players. Thus, the purpose of this study is to look at how a player's emotions are affected by the duration of the game. In order to achieve this goal, a framework for emotion detection is created. According to the experiment's results, the volunteers' general ability to express emotions increased from 20 to 60 minutes. In comparison to shorter gameplay sessions, the experiment found that extended gameplay sessions did significantly affect the player's emotions. According to the results, it was recommended that in order to lessen the potential emotional impact that playing computer and video games may have in the future, game producers should think about creating shorter, entertaining games.</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：Transfer Learning with Reconstruction Loss</b></summary>
  <p><b>编号</b>：[417]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00505">https://arxiv.org/abs/2404.00505</a></p>
  <p><b>作者</b>：Wei Cui,  Wei Yu</p>
  <p><b>备注</b>：16 pages, 5 figures. To appear in IEEE Transactions on Machine Learning in Communications and Networking (TMLCN)</p>
  <p><b>关键词</b>：specific optimization objective, utilizing neural networks, mathematical optimization, specific optimization, neural network models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In most applications of utilizing neural networks for mathematical optimization, a dedicated model is trained for each specific optimization objective. However, in many scenarios, several distinct yet correlated objectives or tasks often need to be optimized on the same set of problem inputs. Instead of independently training a different neural network for each problem separately, it would be more efficient to exploit the correlations between these objectives and to train multiple neural network models with shared model parameters and feature representations. To achieve this, this paper first establishes the concept of common information: the shared knowledge required for solving the correlated tasks, then proposes a novel approach for model training by adding into the model an additional reconstruction stage associated with a new reconstruction loss. This loss is for reconstructing the common information starting from a selected hidden layer in the model. The proposed approach encourages the learned features to be general and transferable, and therefore can be readily used for efficient transfer learning. For numerical simulations, three applications are studied: transfer learning on classifying MNIST handwritten digits, the device-to-device wireless network power allocation, and the multiple-input-single-output network downlink beamforming and localization. Simulation results suggest that the proposed approach is highly efficient in data and model complexity, is resilient to over-fitting, and has competitive performances.</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：Configurable Safety Tuning of Language Models with Synthetic Preference  Data</b></summary>
  <p><b>编号</b>：[422]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00495">https://arxiv.org/abs/2404.00495</a></p>
  <p><b>作者</b>：Victor Gallego</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Direct Preference Optimization, restrict user control, hard-coding predefined behaviors, Preference Optimization, language model fine-tuning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>State-of-the-art language model fine-tuning techniques, such as Direct Preference Optimization (DPO), restrict user control by hard-coding predefined behaviors into the model. To address this, we propose a novel method, Configurable Safety Tuning (CST), that augments DPO using synthetic preference data to facilitate flexible safety configuration of LLMs at inference time. CST overcomes the constraints of vanilla DPO by introducing a system prompt specifying safety configurations, enabling LLM deployers to disable/enable safety preferences based on their need, just changing the system prompt. Our experimental evaluations indicate that CST successfully manages different safety configurations and retains the original functionality of LLMs, showing it is a robust method for configurable deployment. Data and models available at this https URL</p>
  </details>
</details>
<details>
  <summary>83. <b>标题：Multi-hop Question Answering under Temporal Knowledge Editing</b></summary>
  <p><b>编号</b>：[424]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00492">https://arxiv.org/abs/2404.00492</a></p>
  <p><b>作者</b>：Keyuan Cheng,  Gang Lin,  Haoyang Fei,  Yuxuan zhai,  Lu Yu,  Muhammad Asif Ali,  Lijie Hu,  Di Wang</p>
  <p><b>备注</b>：23 pages</p>
  <p><b>关键词</b>：Multi-hop question answering, garnered significant attention, large language models, question answering, garnered significant</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multi-hop question answering (MQA) under knowledge editing (KE) has garnered significant attention in the era of large language models. However, existing models for MQA under KE exhibit poor performance when dealing with questions containing explicit temporal contexts. To address this limitation, we propose a novel framework, namely TEMPoral knowLEdge augmented Multi-hop Question Answering (TEMPLE-MQA). Unlike previous methods, TEMPLE-MQA first constructs a time-aware graph (TAG) to store edit knowledge in a structured manner. Then, through our proposed inference path, structural retrieval, and joint reasoning stages, TEMPLE-MQA effectively discerns temporal contexts within the question query. Experiments on benchmark datasets demonstrate that TEMPLE-MQA significantly outperforms baseline models. Additionally, we contribute a new dataset, namely TKEMQA, which serves as the inaugural benchmark tailored specifically for MQA with temporal scopes.</p>
  </details>
</details>
<details>
  <summary>84. <b>标题：PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt  Compression</b></summary>
  <p><b>编号</b>：[426]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00489">https://arxiv.org/abs/2404.00489</a></p>
  <p><b>作者</b>：Muhammad Asif Ali,  Zhengping Li,  Shu Yang,  Keyuan Cheng,  Yang Cao,  Tianhao Huang,  Lijie Hu,  Lu Yu,  Di Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：language processing tasks, natural language processing, shown exceptional abilities, Large language models, Large language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) have shown exceptional abilities for multiple different natural language processing tasks. While prompting is a crucial tool for LLM inference, we observe that there is a significant cost associated with exceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead to sub-standard results in terms of readability and interpretability of the compressed prompt, with a detrimental impact on prompt utility. To address this, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an effective strategy for prompt compression over task-agnostic and task-aware prompts. PROMPT-SAW uses the prompt's textual information to build a graph, later extracts key information elements in the graph to come up with the compressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the existing GSM8k benchmark for task-agnostic prompts in order to provide a comprehensive evaluation platform. Experimental evaluation using benchmark datasets shows that prompts compressed by PROMPT-SAW are not only better in terms of readability, but they also outperform the best-performing baseline models by up to 14.3 and 13.7 respectively for task-aware and task-agnostic settings while compressing the original prompt text by 33.0 and 56.7.</p>
  </details>
</details>
<details>
  <summary>85. <b>标题：Noise-Aware Training of Layout-Aware Language Models</b></summary>
  <p><b>编号</b>：[427]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00488">https://arxiv.org/abs/2404.00488</a></p>
  <p><b>作者</b>：Ritesh Sarkhel,  Xiaoqi Ren,  Lauro Beltrao Costa,  Guolong Su,  Vincent Perot,  Yanan Xie,  Emmanouil Koukoumidis,  Arnab Nandi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：visually rich document, disseminate information, target document type, utilizes visual features, visually rich</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A visually rich document (VRD) utilizes visual features along with linguistic cues to disseminate information. Training a custom extractor that identifies named entities from a document requires a large number of instances of the target document type annotated at textual and visual modalities. This is an expensive bottleneck in enterprise scenarios, where we want to train custom extractors for thousands of different document types in a scalable way. Pre-training an extractor model on unlabeled instances of the target document type, followed by a fine-tuning step on human-labeled instances does not work in these scenarios, as it surpasses the maximum allowable training time allocated for the extractor. We address this scenario by proposing a Noise-Aware Training method or NAT in this paper. Instead of acquiring expensive human-labeled documents, NAT utilizes weakly labeled documents to train an extractor in a scalable way. To avoid degradation in the model's quality due to noisy, weakly labeled samples, NAT estimates the confidence of each training sample and incorporates it as uncertainty measure during training. We train multiple state-of-the-art extractor models using NAT. Experiments on a number of publicly available and in-house datasets show that NAT-trained models are not only robust in performance -- it outperforms a transfer-learning baseline by up to 6% in terms of macro-F1 score, but it is also more label-efficient -- it reduces the amount of human-effort required to obtain comparable performance by up to 73%.</p>
  </details>
</details>
<details>
  <summary>86. <b>标题：Contextual AI Journaling: Integrating LLM and Time Series Behavioral  Sensing Technology to Promote Self-Reflection and Well-being using the  MindScape App</b></summary>
  <p><b>编号</b>：[428]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00487">https://arxiv.org/abs/2404.00487</a></p>
  <p><b>作者</b>：Subigya Nepal,  Arvind Pillai,  William Campbell,  Talie Massachi,  Eunsol Soul Choi,  Orson Xu,  Joanna Kuc,  Jeremy Huckins,  Jason Holden,  Colin Depp,  Nicholas Jacobson,  Mary Czerwinski,  Eric Granholm,  Andrew T. Campbell</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, integrating time series, series behavioral patterns</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>MindScape aims to study the benefits of integrating time series behavioral patterns (e.g., conversational engagement, sleep, location) with Large Language Models (LLMs) to create a new form of contextual AI journaling, promoting self-reflection and well-being. We argue that integrating behavioral sensing in LLMs will likely lead to a new frontier in AI. In this Late-Breaking Work paper, we discuss the MindScape contextual journal App design that uses LLMs and behavioral sensing to generate contextual and personalized journaling prompts crafted to encourage self-reflection and emotional development. We also discuss the MindScape study of college students based on a preliminary user study and our upcoming study to assess the effectiveness of contextual AI journaling in promoting better well-being on college campuses. MindScape represents a new application class that embeds behavioral intelligence in AI.</p>
  </details>
</details>
<details>
  <summary>87. <b>标题：Dialectical Alignment: Resolving the Tension of 3H and Security Threats  of LLMs</b></summary>
  <p><b>编号</b>：[429]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00486">https://arxiv.org/abs/2404.00486</a></p>
  <p><b>作者</b>：Shu Yang,  Jiayuan Su,  Han Jiang,  Mengdi Li,  Keyuan Cheng,  Muhammad Asif Ali,  Lijie Hu,  Di Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, ensuring they embody, rise of large, large language, embody the principles</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the rise of large language models (LLMs), ensuring they embody the principles of being helpful, honest, and harmless (3H), known as Human Alignment, becomes crucial. While existing alignment methods like RLHF, DPO, etc., effectively fine-tune LLMs to match preferences in the preference dataset, they often lead LLMs to highly receptive human input and external evidence, even when this information is poisoned. This leads to a tendency for LLMs to be Adaptive Chameleons when external evidence conflicts with their parametric memory. This exacerbates the risk of LLM being attacked by external poisoned data, which poses a significant security risk to LLM system applications such as Retrieval-augmented generation (RAG). To address the challenge, we propose a novel framework: Dialectical Alignment (DA), which (1) utilizes AI feedback to identify optimal strategies for LLMs to navigate inter-context conflicts and context-memory conflicts with different external evidence in context window (i.e., different ratios of poisoned factual contexts); (2) constructs the SFT dataset as well as the preference dataset based on the AI feedback and strategies above; (3) uses the above datasets for LLM alignment to defense poisoned context attack while preserving the effectiveness of in-context knowledge editing. Our experiments show that the dialectical alignment model improves poisoned data attack defense by 20 and does not require any additional prompt engineering or prior declaration of ``you may be attacked`` to the LLMs' context window.</p>
  </details>
</details>
<details>
  <summary>88. <b>标题：Cross-lingual Named Entity Corpus for Slavic Languages</b></summary>
  <p><b>编号</b>：[432]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00482">https://arxiv.org/abs/2404.00482</a></p>
  <p><b>作者</b>：Jakub Piskorski,  Michał Marcińczuk,  Roman Yangarber</p>
  <p><b>备注</b>：Published in LREC-COLING 2024 - The 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation</p>
  <p><b>关键词</b>：Slavic Natural Language, Natural Language Processing, Slavic languages, Slavic Natural, corpus manually annotated</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a corpus manually annotated with named entities for six Slavic languages - Bulgarian, Czech, Polish, Slovenian, Russian, and Ukrainian. This work is the result of a series of shared tasks, conducted in 2017-2023 as a part of the Workshops on Slavic Natural Language Processing. The corpus consists of 5 017 documents on seven topics. The documents are annotated with five classes of named entities. Each entity is described by a category, a lemma, and a unique cross-lingual identifier. We provide two train-tune dataset splits - single topic out and cross topics. For each split, we set benchmarks using a transformer-based neural network architecture with the pre-trained multilingual models - XLM-RoBERTa-large for named entity mention recognition and categorization, and mT5-large for named entity lemmatization and linking.</p>
  </details>
</details>
<details>
  <summary>89. <b>标题：Linguistic Calibration of Language Models</b></summary>
  <p><b>编号</b>：[435]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00474">https://arxiv.org/abs/2404.00474</a></p>
  <p><b>作者</b>：Neil Band,  Xuechen Li,  Tengyu Ma,  Tatsunori Hashimoto</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：suboptimal downstream decisions, confidently hallucinate, Language models, long-form generations, generations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate. This issue can be mitigated by having the LM verbally convey the probability that its claims are correct, but existing models cannot produce text with calibrated confidence statements. Through the lens of decision-making, we formalize linguistic calibration for long-form generations: an LM is linguistically calibrated if its generations enable its users to make calibrated probabilistic predictions. This definition enables a training framework where a supervised finetuning step bootstraps an LM to emit long-form generations with confidence statements such as "I estimate a 30% chance of..." or "I am certain that...", followed by a reinforcement learning step which rewards generations that enable a user to provide calibrated answers to related questions. We linguistically calibrate Llama 2 7B and find in automated and human evaluations of long-form generations that it is significantly more calibrated than strong finetuned factuality baselines with comparable accuracy. These findings generalize under distribution shift on question-answering and under a significant task shift to person biography generation. Our results demonstrate that long-form generations may be calibrated end-to-end by constructing an objective in the space of the predictions that users make in downstream decision-making.</p>
  </details>
</details>
<details>
  <summary>90. <b>标题：Shortcuts Arising from Contrast: Effective and Covert Clean-Label  Attacks in Prompt-Based Learning</b></summary>
  <p><b>编号</b>：[443]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00461">https://arxiv.org/abs/2404.00461</a></p>
  <p><b>作者</b>：Xiaopeng Xie,  Ming Yan,  Xiwen Zhou,  Chenlong Zhao,  Suli Wang,  Yong Zhang,  Joey Tianyi Zhou</p>
  <p><b>备注</b>：10 pages, 6 figures, conference</p>
  <p><b>关键词</b>：pretrained language models, demonstrated remarkable efficacy, Prompt-based learning paradigm, learning paradigm, language models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Prompt-based learning paradigm has demonstrated remarkable efficacy in enhancing the adaptability of pretrained language models (PLMs), particularly in few-shot scenarios. However, this learning paradigm has been shown to be vulnerable to backdoor attacks. The current clean-label attack, employing a specific prompt as a trigger, can achieve success without the need for external triggers and ensure correct labeling of poisoned samples, which is more stealthy compared to the poisoned-label attack, but on the other hand, it faces significant issues with false activations and poses greater challenges, necessitating a higher rate of poisoning. Using conventional negative data augmentation methods, we discovered that it is challenging to trade off between effectiveness and stealthiness in a clean-label setting. In addressing this issue, we are inspired by the notion that a backdoor acts as a shortcut and posit that this shortcut stems from the contrast between the trigger and the data utilized for poisoning. In this study, we propose a method named Contrastive Shortcut Injection (CSI), by leveraging activation values, integrates trigger design and data selection strategies to craft stronger shortcut features. With extensive experiments on full-shot and few-shot text classification tasks, we empirically validate CSI's high effectiveness and high stealthiness at low poisoning rates. Notably, we found that the two approaches play leading roles in full-shot and few-shot settings, respectively.</p>
  </details>
</details>
<details>
  <summary>91. <b>标题：Interactive Multi-Robot Flocking with Gesture Responsiveness and Musical  Accompaniment</b></summary>
  <p><b>编号</b>：[453]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00442">https://arxiv.org/abs/2404.00442</a></p>
  <p><b>作者</b>：Catie Cuan,  Kyle Jeffrey,  Kim Kleiven,  Adrian Li,  Emre Fisher,  Matt Harrison,  Benjie Holson,  Allison Okamura,  Matt Bennice</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：search and rescue, researchers have pursued, cooperative manipulation, manipulation to search, robotics researchers</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>For decades, robotics researchers have pursued various tasks for multi-robot systems, from cooperative manipulation to search and rescue. These tasks are multi-robot extensions of classical robotic tasks and often optimized on dimensions such as speed or efficiency. As robots transition from commercial and research settings into everyday environments, social task aims such as engagement or entertainment become increasingly relevant. This work presents a compelling multi-robot task, in which the main aim is to enthrall and interest. In this task, the goal is for a human to be drawn to move alongside and participate in a dynamic, expressive robot flock. Towards this aim, the research team created algorithms for robot movements and engaging interaction modes such as gestures and sound. The contributions are as follows: (1) a novel group navigation algorithm involving human and robot agents, (2) a gesture responsive algorithm for real-time, human-robot flocking interaction, (3) a weight mode characterization system for modifying flocking behavior, and (4) a method of encoding a choreographer's preferences inside a dynamic, adaptive, learned system. An experiment was performed to understand individual human behavior while interacting with the flock under three conditions: weight modes selected by a human choreographer, a learned model, or subset list. Results from the experiment showed that the perception of the experience was not influenced by the weight mode selection. This work elucidates how differing task aims such as engagement manifest in multi-robot system design and execution, and broadens the domain of multi-robot tasks.</p>
  </details>
</details>
<details>
  <summary>92. <b>标题：Communication Efficient Distributed Training with Distributed Lion</b></summary>
  <p><b>编号</b>：[456]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00438">https://arxiv.org/abs/2404.00438</a></p>
  <p><b>作者</b>：Bo Liu,  Lemeng Wu,  Lizhang Chen,  Kaizhao Liang,  Jiaxu Zhu,  Chen Liang,  Raghuraman Krishnamoorthi,  Qiang Liu</p>
  <p><b>备注</b>：22 pages</p>
  <p><b>关键词</b>：Distributed Lion, Lion, Distributed, advantages on memory, sample efficiency</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The Lion optimizer has been a promising competitor with the AdamW for training large AI models, with advantages on memory, computation, and sample efficiency. In this paper, we introduce Distributed Lion, an innovative adaptation of Lion for distributed training environments. Leveraging the sign operator in Lion, our Distributed Lion only requires communicating binary or lower-precision vectors between workers to the center server, significantly reducing the communication cost. Our theoretical analysis confirms Distributed Lion's convergence properties. Empirical results demonstrate its robustness across a range of tasks, worker counts, and batch sizes, on both vision and language problems. Notably, Distributed Lion attains comparable performance to standard Lion or AdamW optimizers applied on aggregated gradients, but with significantly reduced communication bandwidth. This feature is particularly advantageous for training large models. In addition, we also demonstrate that Distributed Lion presents a more favorable performance-bandwidth balance compared to existing efficient distributed methods such as deep gradient compression and ternary gradients.</p>
  </details>
</details>
<details>
  <summary>93. <b>标题：Automatic explanation of the classification of Spanish legal judgments  in jurisdiction-dependent law categories with tree estimators</b></summary>
  <p><b>编号</b>：[457]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00437">https://arxiv.org/abs/2404.00437</a></p>
  <p><b>作者</b>：Jaime González-González,  Francisco de Arriba-Pérez,  Silvia García-Méndez,  Andrea Busto-Castiñeira,  Francisco J. González-Castaño</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：address knowledge extraction, detect their aspects, literature to address, extraction from judgments, judgments and detect</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automatic legal text classification systems have been proposed in the literature to address knowledge extraction from judgments and detect their aspects. However, most of these systems are black boxes even when their models are interpretable. This may raise concerns about their trustworthiness. Accordingly, this work contributes with a system combining Natural Language Processing (NLP) with Machine Learning (ML) to classify legal texts in an explainable manner. We analyze the features involved in the decision and the threshold bifurcation values of the decision paths of tree structures and present this information to the users in natural language. This is the first work on automatic analysis of legal texts combining NLP and ML along with Explainable Artificial Intelligence techniques to automatically make the models' decisions understandable to end users. Furthermore, legal experts have validated our solution, and this knowledge has also been incorporated into the explanation process as "expert-in-the-loop" dictionaries. Experimental results on an annotated data set in law categories by jurisdiction demonstrate that our system yields competitive classification performance, with accuracy values well above 90%, and that its automatic explanations are easily understandable even to non-expert users.</p>
  </details>
</details>
<details>
  <summary>94. <b>标题：Orchestrate Latent Expertise: Advancing Online Continual Learning with  Multi-Level Supervision and Reverse Self-Distillation</b></summary>
  <p><b>编号</b>：[467]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00417">https://arxiv.org/abs/2404.00417</a></p>
  <p><b>作者</b>：HongWei Yan,  Liyuan Wang,  Kaisheng Ma,  Yi Zhong</p>
  <p><b>备注</b>：CVPR 2024</p>
  <p><b>关键词</b>：accommodate real-world dynamics, artificial intelligence systems, sequentially arriving content, Online Continual Learning, real-world dynamics</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To accommodate real-world dynamics, artificial intelligence systems need to cope with sequentially arriving content in an online manner. Beyond regular Continual Learning (CL) attempting to address catastrophic forgetting with offline training of each task, Online Continual Learning (OCL) is a more challenging yet realistic setting that performs CL in a one-pass data stream. Current OCL methods primarily rely on memory replay of old training samples. However, a notable gap from CL to OCL stems from the additional overfitting-underfitting dilemma associated with the use of rehearsal buffers: the inadequate learning of new training samples (underfitting) and the repeated learning of a few old training samples (overfitting). To this end, we introduce a novel approach, Multi-level Online Sequential Experts (MOSE), which cultivates the model as stacked sub-experts, integrating multi-level supervision and reverse self-distillation. Supervision signals across multiple stages facilitate appropriate convergence of the new task while gathering various strengths from experts by knowledge distillation mitigates the performance decline of old tasks. MOSE demonstrates remarkable efficacy in learning new samples and preserving past knowledge through multi-level experts, thereby significantly advancing OCL performance over state-of-the-art baselines (e.g., up to 7.3% on Split CIFAR-100 and 6.1% on Split Tiny-ImageNet).</p>
  </details>
</details>
<details>
  <summary>95. <b>标题：TACO -- Twitter Arguments from COnversations</b></summary>
  <p><b>编号</b>：[474]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00406">https://arxiv.org/abs/2404.00406</a></p>
  <p><b>作者</b>：Marc Feger,  Stefan Dietze</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：user-generated content, global hub, hub for engaging, research corpus, recognized the significance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Twitter has emerged as a global hub for engaging in online conversations and as a research corpus for various disciplines that have recognized the significance of its user-generated content. Argument mining is an important analytical task for processing and understanding online discourse. Specifically, it aims to identify the structural elements of arguments, denoted as information and inference. These elements, however, are not static and may require context within the conversation they are in, yet there is a lack of data and annotation frameworks addressing this dynamic aspect on Twitter. We contribute TACO, the first dataset of Twitter Arguments utilizing 1,814 tweets covering 200 entire conversations spanning six heterogeneous topics annotated with an agreement of 0.718 Krippendorff's alpha among six experts. Second, we provide our annotation framework, incorporating definitions from the Cambridge Dictionary, to define and identify argument components on Twitter. Our transformer-based classifier achieves an 85.06\% macro F1 baseline score in detecting arguments. Moreover, our data reveals that Twitter users tend to engage in discussions involving informed inferences and information. TACO serves multiple purposes, such as training tweet classifiers to manage tweets based on inference and information elements, while also providing valuable insights into the conversational reply patterns of tweets.</p>
  </details>
</details>
<details>
  <summary>96. <b>标题：Aurora-M: The First Open Source Multilingual Language Model Red-teamed  according to the U.S. Executive Order</b></summary>
  <p><b>编号</b>：[479]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00399">https://arxiv.org/abs/2404.00399</a></p>
  <p><b>作者</b>：Taishi Nakamura,  Mayank Mishra,  Simone Tedeschi,  Yekun Chai,  Jason T Stillerman,  Felix Friedrich,  Prateek Yadav,  Tanmay Laud,  Vu Minh Chien,  Terry Yue Zhuo,  Diganta Misra,  Ben Bogin,  Xuan-Son Vu,  Marzena Karpinska,  Arnav Varma Dantuluri,  Wojciech Kusa,  Tommaso Furlanello,  Rio Yokota,  Niklas Muennighoff,  Suhas Pai,  Tosin Adewumi,  Veronika Laippala,  Xiaozhe Yao,  Adalberto Junior,  Alpay Ariyak,  Aleksandr Drozd,  Jordan Clive,  Kshitij Gupta,  Liangyu Chen,  Qi Sun,  Ken Tsui,  Noah Persaud,  Nour Fahmy,  Tianlong Chen,  Mohit Bansal,  Nicolo Monti,  Tai Dang,  Ziyang Luo,  Tien-Tung Bui,  Roberto Navigli,  Virendra Mehta,  Matthew Blumberg,  Victor May,  Huu Nguyen,  Sampo Pyysalo</p>
  <p><b>备注</b>：Preprint</p>
  <p><b>关键词</b>：high computational cost, training limits accessibility, limits accessibility, high computational, computational cost</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pretrained language models underpin several AI applications, but their high computational cost for training limits accessibility. Initiatives such as BLOOM and StarCoder aim to democratize access to pretrained models for collaborative community development. However, such existing models face challenges: limited multilingual capabilities, continual pretraining causing catastrophic forgetting, whereas pretraining from scratch is computationally expensive, and compliance with AI safety and development laws. This paper presents Aurora-M, a 15B parameter multilingual open-source model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually pretrained from StarCoderPlus on 435 billion additional tokens, Aurora-M surpasses 2 trillion tokens in total training token count. It is the first open-source multilingual model fine-tuned on human-reviewed safety instructions, thus aligning its development not only with conventional red-teaming considerations, but also with the specific concerns articulated in the Biden-Harris Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. Aurora-M is rigorously evaluated across various tasks and languages, demonstrating robustness against catastrophic forgetting and outperforming alternatives in multilingual settings, particularly in safety evaluations. To promote responsible open-source LLM development, Aurora-M and its variants are released at this https URL .</p>
  </details>
</details>
<details>
  <summary>97. <b>标题：Constrained Layout Generation with Factor Graphs</b></summary>
  <p><b>编号</b>：[487]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00385">https://arxiv.org/abs/2404.00385</a></p>
  <p><b>作者</b>：Mohammed Haroon Dupty,  Yanfei Dong,  Sicong Leng,  Guoji Fu,  Yong Liang Goh,  Wei Lu,  Wee Sun Lee</p>
  <p><b>备注</b>：To be published at IEEE/CVF CVPR 2024</p>
  <p><b>关键词</b>：multiple domains including, domains including floorplan, object-centric layout generation, design process, including floorplan design</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper addresses the challenge of object-centric layout generation under spatial constraints, seen in multiple domains including floorplan design process. The design process typically involves specifying a set of spatial constraints that include object attributes like size and inter-object relations such as relative positioning. Existing works, which typically represent objects as single nodes, lack the granularity to accurately model complex interactions between objects. For instance, often only certain parts of an object, like a room's right wall, interact with adjacent objects. To address this gap, we introduce a factor graph based approach with four latent variable nodes for each room, and a factor node for each constraint. The factor nodes represent dependencies among the variables to which they are connected, effectively capturing constraints that are potentially of a higher order. We then develop message-passing on the bipartite graph, forming a factor graph neural network that is trained to produce a floorplan that aligns with the desired requirements. Our approach is simple and generates layouts faithful to the user requirements, demonstrated by a large improvement in IOU scores over existing methods. Additionally, our approach, being inferential and accurate, is well-suited to the practical human-in-the-loop design process where specifications evolve iteratively, offering a practical and powerful tool for AI-guided design.</p>
  </details>
</details>
<details>
  <summary>98. <b>标题：SpikingJET: Enhancing Fault Injection for Fully and Convolutional  Spiking Neural Networks</b></summary>
  <p><b>编号</b>：[489]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00383">https://arxiv.org/abs/2404.00383</a></p>
  <p><b>作者</b>：Anil Bayram Gogebakan,  Enrico Magliano,  Alessio Carpegna,  Annachiara Ruospo,  Alessandro Savino,  Stefano Di Carlo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：artificial neural networks, Spiking Neural Networks, neural networks, convolutional Spiking Neural, autonomous vehicles</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As artificial neural networks become increasingly integrated into safety-critical systems such as autonomous vehicles, devices for medical diagnosis, and industrial automation, ensuring their reliability in the face of random hardware faults becomes paramount. This paper introduces SpikingJET, a novel fault injector designed specifically for fully connected and convolutional Spiking Neural Networks (SNNs). Our work underscores the critical need to evaluate the resilience of SNNs to hardware faults, considering their growing prominence in real-world applications. SpikingJET provides a comprehensive platform for assessing the resilience of SNNs by inducing errors and injecting faults into critical components such as synaptic weights, neuron model parameters, internal states, and activation functions. This paper demonstrates the effectiveness of Spiking-JET through extensive software-level experiments on various SNN architectures, revealing insights into their vulnerability and resilience to hardware faults. Moreover, highlighting the importance of fault resilience in SNNs contributes to the ongoing effort to enhance the reliability and safety of Neural Network (NN)-powered systems in diverse domains.</p>
  </details>
</details>
<details>
  <summary>99. <b>标题：Worker Robot Cooperation and Integration into the Manufacturing Workcell  via the Holonic Control Architecture</b></summary>
  <p><b>编号</b>：[495]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00369">https://arxiv.org/abs/2404.00369</a></p>
  <p><b>作者</b>：Ahmed R. Sadik,  Bodo Urban,  Omar Adel</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：intelligent manufacturing techniques, industrial trend, manufacturing control system, manufacturing, aims to sum</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Worker-Robot Cooperation is a new industrial trend, which aims to sum the advantages of both the human and the industrial robot to afford a new intelligent manufacturing techniques. The cooperative manufacturing between the worker and the robot contains other elements such as the product parts and the manufacturing tools. All these production elements must cooperate in one manufacturing workcell to fulfill the production requirements. The manufacturing control system is the mean to connect all these cooperative elements together in one body. This manufacturing control system is distributed and autonomous due to the nature of the cooperative workcell. Accordingly, this article proposes the holonic control architecture as the manufacturing concept of the cooperative workcell. Furthermore, the article focuses on the feasibility of this manufacturing concept, by applying it over a case study that involves the cooperation between a dual-arm robot and a worker. During this case study, the worker uses a variety of hand gestures to cooperate with the robot to achieve the highest production flexibility</p>
  </details>
</details>
<details>
  <summary>100. <b>标题：Accurate Cutting-point Estimation for Robotic Lychee Harvesting through  Geometry-aware Learning</b></summary>
  <p><b>编号</b>：[499]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00364">https://arxiv.org/abs/2404.00364</a></p>
  <p><b>作者</b>：Gengming Zhang,  Hao Cao,  Kewei Hu,  Yaoqiang Pan,  Yuqin Deng,  Hongjun Wang,  Hanwen Kang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：lychee picking points, picking points, lychee picking, Accurately identifying lychee-picking, identifying lychee-picking points</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Accurately identifying lychee-picking points in unstructured orchard environments and obtaining their coordinate locations is critical to the success of lychee-picking robots. However, traditional two-dimensional (2D) image-based object detection methods often struggle due to the complex geometric structures of branches, leaves and fruits, leading to incorrect determination of lychee picking points. In this study, we propose a Fcaf3d-lychee network model specifically designed for the accurate localisation of lychee picking points. Point cloud data of lychee picking points in natural environments are acquired using Microsoft's Azure Kinect DK time-of-flight (TOF) camera through multi-view stitching. We augment the Fully Convolutional Anchor-Free 3D Object Detection (Fcaf3d) model with a squeeze-and-excitation(SE) module, which exploits human visual attention mechanisms for improved feature extraction of lychee picking points. The trained network model is evaluated on a test set of lychee-picking locations and achieves an impressive F1 score of 88.57%, significantly outperforming existing models. Subsequent three-dimensional (3D) position detection of picking points in real lychee orchard environments yields high accuracy, even under varying degrees of occlusion. Localisation errors of lychee picking points are within 1.5 cm in all directions, demonstrating the robustness and generality of the model.</p>
  </details>
</details>
<details>
  <summary>101. <b>标题：Can LLMs Master Math? Investigating Large Language Models on Math Stack  Exchange</b></summary>
  <p><b>编号</b>：[513]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00344">https://arxiv.org/abs/2404.00344</a></p>
  <p><b>作者</b>：Ankit Satpute,  Noah Giessing,  Andre Greiner-Petter,  Moritz Schubotz,  Olaf Teschke,  Akiko Aizawa,  Bela Gipp</p>
  <p><b>备注</b>：Accepted for publication at the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) July 14--18, 2024, Washington D.C.,USA</p>
  <p><b>关键词</b>：Large Language Models, natural language tasks, Language Models, Large Language, language tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) have demonstrated exceptional capabilities in various natural language tasks, often achieving performances that surpass those of humans. Despite these advancements, the domain of mathematics presents a distinctive challenge, primarily due to its specialized structure and the precision it demands. In this study, we adopted a two-step approach for investigating the proficiency of LLMs in answering mathematical questions. First, we employ the most effective LLMs, as identified by their performance on math question-answer benchmarks, to generate answers to 78 questions from the Math Stack Exchange (MSE). Second, a case analysis is conducted on the LLM that showed the highest performance, focusing on the quality and accuracy of its answers through manual evaluation. We found that GPT-4 performs best (nDCG of 0.48 and P@10 of 0.37) amongst existing LLMs fine-tuned for answering mathematics questions and outperforms the current best approach on ArqMATH3 Task1, considering P@10. Our Case analysis indicates that while the GPT-4 can generate relevant responses in certain instances, it does not consistently answer all questions accurately. This paper explores the current limitations of LLMs in navigating complex mathematical problem-solving. Through case analysis, we shed light on the gaps in LLM capabilities within mathematics, thereby setting the stage for future research and advancements in AI-driven mathematical reasoning. We make our code and findings publicly available for research: \url{this https URL}</p>
  </details>
</details>
<details>
  <summary>102. <b>标题：Ontology in Holonic Cooperative Manufacturing: A Solution to Share and  Exchange the Knowledge</b></summary>
  <p><b>编号</b>：[515]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00341">https://arxiv.org/abs/2404.00341</a></p>
  <p><b>作者</b>：Ahmed R.Sadik,  Bodo Urban</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：collaborative robot, trend in industry, cooperative manufacturing knowledge, Cooperative manufacturing, manufacturing knowledge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cooperative manufacturing is a new trend in industry, which depends on the existence of a collaborative robot. A collaborative robot is usually a light-weight robot which is capable of operating safely with a human co-worker in a shared work environment. During this cooperation, a vast amount of information is exchanged between the collaborative robot and the worker. This information constructs the cooperative manufacturing knowledge, which describes the production components and environment. In this research, we propose a holonic control solution, which uses the ontology concept to represent the cooperative manufacturing knowledge. The holonic control solution is implemented as an autonomous multi-agent system that exchanges the manufacturing knowledge based on an ontology model. Ultimately, the research illustrates and implements the proposed solution over a cooperative assembly scenario, which involves two workers and one collaborative robot, whom cooperate together to assemble a customized product.</p>
  </details>
</details>
<details>
  <summary>103. <b>标题：Memory-Scalable and Simplified Functional Map Learning</b></summary>
  <p><b>编号</b>：[520]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00330">https://arxiv.org/abs/2404.00330</a></p>
  <p><b>作者</b>：Robin Magnet,  Maks Ovsjanikov</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：shape matching problems, prominent learning-based framework, non-rigid shape matching, matching problems, Deep functional maps</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep functional maps have emerged in recent years as a prominent learning-based framework for non-rigid shape matching problems. While early methods in this domain only focused on learning in the functional domain, the latest techniques have demonstrated that by promoting consistency between functional and pointwise maps leads to significant improvements in accuracy. Unfortunately, existing approaches rely heavily on the computation of large dense matrices arising from soft pointwise maps, which compromises their efficiency and scalability. To address this limitation, we introduce a novel memory-scalable and efficient functional map learning pipeline. By leveraging the specific structure of functional maps, we offer the possibility to achieve identical results without ever storing the pointwise map in memory. Furthermore, based on the same approach, we present a differentiable map refinement layer adapted from an existing axiomatic refinement algorithm. Unlike many functional map learning methods, which use this algorithm at a post-processing step, ours can be easily used at train time, enabling to enforce consistency between the refined and initial versions of the map. Our resulting approach is both simpler, more efficient and more numerically stable, by avoiding differentiation through a linear system, while achieving close to state-of-the-art results in challenging scenarios.</p>
  </details>
</details>
<details>
  <summary>104. <b>标题：Advancing Multimodal Data Fusion in Pain Recognition: A Strategy  Leveraging Statistical Correlation and Human-Centered Perspectives</b></summary>
  <p><b>编号</b>：[524]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00320">https://arxiv.org/abs/2404.00320</a></p>
  <p><b>作者</b>：Xingrui Gu,  Zhixuan Wang,  Irisa Jin,  Zekun Wu</p>
  <p><b>备注</b>：Under reviewed by ACII 2024</p>
  <p><b>关键词</b>：integrating heterogeneous data, harmonizes statistical correlations, specific behavior recognition, research tackles, tackles the challenge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This research tackles the challenge of integrating heterogeneous data for specific behavior recognition within the domain of Pain Recognition, presenting a novel methodology that harmonizes statistical correlations with a human-centered approach. By leveraging a diverse range of deep learning architectures, we highlight the adaptability and efficacy of our approach in improving model performance across various complex scenarios. The novelty of our methodology is the strategic incorporation of statistical relevance weights and the segmentation of modalities from a human-centric perspective, enhancing model precision and providing a explainable analysis of multimodal data. This study surpasses traditional modality fusion techniques by underscoring the role of data diversity and customized modality segmentation in enhancing pain behavior analysis. Introducing a framework that matches each modality with an suited classifier, based on the statistical significance, signals a move towards customized and accurate multimodal fusion strategies. Our contributions extend beyond the field of Pain Recognition by delivering new insights into modality fusion and human-centered computing applications, contributing towards explainable AI and bolstering patient-centric healthcare interventions. Thus, we bridge a significant void in the effective and interpretable fusion of multimodal data, establishing a novel standard for forthcoming inquiries in pain behavior recognition and allied fields.</p>
  </details>
</details>
<details>
  <summary>105. <b>标题：Bayesian Exploration of Pre-trained Models for Low-shot Image  Classification</b></summary>
  <p><b>编号</b>：[527]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00312">https://arxiv.org/abs/2404.00312</a></p>
  <p><b>作者</b>：Yibo Miao,  Yu Lei,  Feng Zhou,  Zhijie Deng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large-scale vision-language models, Low-shot image classification, Low-shot image, computer vision, fundamental task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Low-shot image classification is a fundamental task in computer vision, and the emergence of large-scale vision-language models such as CLIP has greatly advanced the forefront of research in this field. However, most existing CLIP-based methods lack the flexibility to effectively incorporate other pre-trained models that encompass knowledge distinct from CLIP. To bridge the gap, this work proposes a simple and effective probabilistic model ensemble framework based on Gaussian processes, which have previously demonstrated remarkable efficacy in processing small data. We achieve the integration of prior knowledge by specifying the mean function with CLIP and the kernel function with an ensemble of deep kernels built upon various pre-trained models. By regressing the classification label directly, our framework enables analytical inference, straightforward uncertainty quantification, and principled hyper-parameter tuning. Through extensive experiments on standard benchmarks, we demonstrate that our method consistently outperforms competitive ensemble baselines regarding predictive performance. Additionally, we assess the robustness of our method and the quality of the yielded uncertainty estimates on out-of-distribution datasets. We also illustrate that our method, despite relying on label regression, still enjoys superior model calibration compared to most deterministic baselines.</p>
  </details>
</details>
<details>
  <summary>106. <b>标题：Leveraging Intelligent Recommender system as a first step resilience  measure -- A data-driven supply chain disruption response framework</b></summary>
  <p><b>编号</b>：[531]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00306">https://arxiv.org/abs/2404.00306</a></p>
  <p><b>作者</b>：Yang Hu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：increase supply chain, supply chain resilience, global pandemic, supply chain, digital technologies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Interests in the value of digital technologies for its potential uses to increase supply chain resilience (SCRes) are increasing in light to the industry 4.0 and the global pandemic. Utilization of Recommender systems (RS) as a supply chain (SC) resilience measure is neglected although RS is a capable tool to enhance SC resilience from a reactive aspect. To address this problem, this research proposed a novel data-driven supply chain disruption response framework based on the intelligent recommender system techniques and validated the conceptual model through a practical use case. Results show that our framework can be implemented as an effective SC disruption mitigation measure in the very first response phrase and help SC participants get better reaction performance after the SC disruption.</p>
  </details>
</details>
<details>
  <summary>107. <b>标题：Long-Tailed Recognition on Binary Networks by Calibrating A Pre-trained  Model</b></summary>
  <p><b>编号</b>：[540]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00285">https://arxiv.org/abs/2404.00285</a></p>
  <p><b>作者</b>：Jihun Kim,  Dahyun Kim,  Hyungrok Jung,  Taeil Oh,  Jonghyun Choi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：real-world scenarios entails, Deploying deep models, including computational efficiency, Deploying deep, real-world scenarios</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deploying deep models in real-world scenarios entails a number of challenges, including computational efficiency and real-world (e.g., long-tailed) data distributions. We address the combined challenge of learning long-tailed distributions using highly resource-efficient binary neural networks as backbones. Specifically, we propose a calibrate-and-distill framework that uses off-the-shelf pretrained full-precision models trained on balanced datasets to use as teachers for distillation when learning binary networks on long-tailed datasets. To better generalize to various datasets, we further propose a novel adversarial balancing among the terms in the objective function and an efficient multiresolution learning scheme. We conducted the largest empirical study in the literature using 15 datasets, including newly derived long-tailed datasets from existing balanced datasets, and show that our proposed method outperforms prior art by large margins (>14.33% on average).</p>
  </details>
</details>
<details>
  <summary>108. <b>标题：Survey on Large Language Model-Enhanced Reinforcement Learning: Concept,  Taxonomy, and Methods</b></summary>
  <p><b>编号</b>：[542]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00282">https://arxiv.org/abs/2404.00282</a></p>
  <p><b>作者</b>：Yuji Cao,  Huan Zhao,  Yuheng Cheng,  Ting Shu,  Guolong Liu,  Gaoqi Liang,  Junhua Zhao,  Yun Li</p>
  <p><b>备注</b>：16 pages (including bibliography), 6 figures</p>
  <p><b>关键词</b>：augment reinforcement learning, large language models, high-level general capabilities, extensive pre-trained knowledge, reinforcement learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With extensive pre-trained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects such as multi-task learning, sample efficiency, and task planning. In this survey, we provide a comprehensive review of the existing literature in $\textit{LLM-enhanced RL}$ and summarize its characteristics compared to conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs' functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. Additionally, for each role, we summarize the methodologies, analyze the specific RL challenges that are mitigated, and provide insights into future directions. Lastly, potential applications, prospective opportunities and challenges of the $\textit{LLM-enhanced RL}$ are discussed.</p>
  </details>
</details>
<details>
  <summary>109. <b>标题：Instruction-Driven Game Engines on Large Language Models</b></summary>
  <p><b>编号</b>：[544]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00276">https://arxiv.org/abs/2404.00276</a></p>
  <p><b>作者</b>：Hongqiu Wu,  Yan Wang,  Xingyuan Liu,  Hai Zhao,  Min Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：generate game-play processes, autonomously generate game-play, follow free-form game, large language model, democratize game development</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The Instruction-Driven Game Engine (IDGE) project aims to democratize game development by enabling a large language model (LLM) to follow free-form game rules and autonomously generate game-play processes. The IDGE allows users to create games by issuing simple natural language instructions, which significantly lowers the barrier for game development. We approach the learning process for IDGEs as a Next State Prediction task, wherein the model autoregressively predicts in-game states given player actions. It is a challenging task because the computation of in-game states must be precise; otherwise, slight errors could disrupt the game-play. To address this, we train the IDGE in a curriculum manner that progressively increases the model's exposure to complex scenarios.
Our initial progress lies in developing an IDGE for Poker, a universally cherished card game. The engine we've designed not only supports a wide range of poker variants but also allows for high customization of rules through natural language inputs. Furthermore, it also favors rapid prototyping of new games from minimal samples, proposing an innovative paradigm in game development that relies on minimal prompt and data engineering. This work lays the groundwork for future advancements in instruction-driven game creation, potentially transforming how games are designed and played.</p>
  </details>
</details>
<details>
  <summary>110. <b>标题：TG-NAS: Leveraging Zero-Cost Proxies with Transformer and Graph  Convolution Networks for Efficient Neural Architecture Search</b></summary>
  <p><b>编号</b>：[546]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00271">https://arxiv.org/abs/2404.00271</a></p>
  <p><b>作者</b>：Ye Qiao,  Haocheng Xu,  Sitao Huang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：CNN, discovering new convolutional, search, architecture, NAS</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural architecture search (NAS) is an effective method for discovering new convolutional neural network (CNN) architectures. However, existing approaches often require time-consuming training or intensive sampling and evaluations. Zero-shot NAS aims to create training-free proxies for architecture performance prediction. However, existing proxies have suboptimal performance, and are often outperformed by simple metrics such as model parameter counts or the number of floating-point operations. Besides, existing model-based proxies cannot be generalized to new search spaces with unseen new types of operators without golden accuracy truth. A universally optimal proxy remains elusive. We introduce TG-NAS, a novel model-based universal proxy that leverages a transformer-based operator embedding generator and a graph convolution network (GCN) to predict architecture performance. This approach guides neural architecture search across any given search space without the need of retraining. Distinct from other model-based predictor subroutines, TG-NAS itself acts as a zero-cost (ZC) proxy, guiding architecture search with advantages in terms of data independence, cost-effectiveness, and consistency across diverse search spaces. Our experiments showcase its advantages over existing proxies across various NAS benchmarks, suggesting its potential as a foundational element for efficient architecture search. TG-NAS achieves up to 300X improvements in search efficiency compared to previous SOTA ZC proxy methods. Notably, it discovers competitive models with 93.75% CIFAR-10 accuracy on the NAS-Bench-201 space and 74.5% ImageNet top-1 accuracy on the DARTS space.</p>
  </details>
</details>
<details>
  <summary>111. <b>标题：A Simple Yet Effective Approach for Diversified Session-Based  Recommendation</b></summary>
  <p><b>编号</b>：[554]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00261">https://arxiv.org/abs/2404.00261</a></p>
  <p><b>作者</b>：Qing Yin,  Hui Fang,  Zhu Sun,  Yew-Soon Ong</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Session-based recommender systems, dynamic user preferences, Session-based recommender, recommender systems, extremely popular</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Session-based recommender systems (SBRSs) have become extremely popular in view of the core capability of capturing short-term and dynamic user preferences. However, most SBRSs primarily maximize recommendation accuracy but ignore user minor preferences, thus leading to filter bubbles in the long run. Only a handful of works, being devoted to improving diversity, depend on unique model designs and calibrated loss functions, which cannot be easily adapted to existing accuracy-oriented SBRSs. It is thus worthwhile to come up with a simple yet effective design that can be used as a plugin to facilitate existing SBRSs on generating a more diversified list in the meantime preserving the recommendation accuracy. In this case, we propose an end-to-end framework applied for every existing representative (accuracy-oriented) SBRS, called diversified category-aware attentive SBRS (DCA-SBRS), to boost the performance on recommendation diversity. It consists of two novel designs: a model-agnostic diversity-oriented loss function, and a non-invasive category-aware attention mechanism. Extensive experiments on three datasets showcase that our framework helps existing SBRSs achieve extraordinary performance in terms of recommendation diversity and comprehensive performance, without significantly deteriorating recommendation accuracy compared to state-of-the-art accuracy-oriented SBRSs.</p>
  </details>
</details>
<details>
  <summary>112. <b>标题：YOLOOC: YOLO-based Open-Class Incremental Object Detection with Novel  Class Discovery</b></summary>
  <p><b>编号</b>：[556]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00257">https://arxiv.org/abs/2404.00257</a></p>
  <p><b>作者</b>：Qian Wan,  Xiang Xiang,  Qinhao Zhou</p>
  <p><b>备注</b>：Initially submitted to ACCV 2022</p>
  <p><b>关键词</b>：open-world object detection, open-world object, attention recently, lot of attention, classes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Because of its use in practice, open-world object detection (OWOD) has gotten a lot of attention recently. The challenge is how can a model detect novel classes and then incrementally learn them without forgetting previously known classes. Previous approaches hinge on strongly-supervised or weakly-supervised novel-class data for novel-class detection, which may not apply to real applications. We construct a new benchmark that novel classes are only encountered at the inference stage. And we propose a new OWOD detector YOLOOC, based on the YOLO architecture yet for the Open-Class setup. We introduce label smoothing to prevent the detector from over-confidently mapping novel classes to known classes and to discover novel classes. Extensive experiments conducted on our more realistic setup demonstrate the effectiveness of our method for discovering novel classes in our new benchmark.</p>
  </details>
</details>
<details>
  <summary>113. <b>标题：Facilitating Reinforcement Learning for Process Control Using Transfer  Learning: Perspectives</b></summary>
  <p><b>编号</b>：[561]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00247">https://arxiv.org/abs/2404.00247</a></p>
  <p><b>作者</b>：Runze Lin,  Junghui Chen,  Lei Xie,  Hongye Su,  Biao Huang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：deep reinforcement learning, paper provides insights, insights into deep, deep reinforcement, transfer learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper provides insights into deep reinforcement learning (DRL) for process control from the perspective of transfer learning. We analyze the challenges of applying DRL in the field of process industries and the necessity of introducing transfer learning. Furthermore, recommendations and prospects are provided for future research directions on how transfer learning can be integrated with DRL to empower process control.</p>
  </details>
</details>
<details>
  <summary>114. <b>标题：Your Co-Workers Matter: Evaluating Collaborative Capabilities of  Language Models in Blocks World</b></summary>
  <p><b>编号</b>：[562]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00246">https://arxiv.org/abs/2404.00246</a></p>
  <p><b>作者</b>：Guande Wu,  Chen Zhao,  Claudio Silva,  He He</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：automating digital tasks, great potential, potential for automating, automating digital, digital tasks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Language agents that interact with the world on their own have great potential for automating digital tasks. While large language model (LLM) agents have made progress in understanding and executing tasks such as textual games and webpage control, many real-world tasks also require collaboration with humans or other LLMs in equal roles, which involves intent understanding, task coordination, and communication. To test LLM's ability to collaborate, we design a blocks-world environment, where two agents, each having unique goals and skills, build a target structure together. To complete the goals, they can act in the world and communicate in natural language. Under this environment, we design increasingly challenging settings to evaluate different collaboration perspectives, from independent to more complex, dependent tasks. We further adopt chain-of-thought prompts that include intermediate reasoning steps to model the partner's state and identify and correct execution errors. Both human-machine and machine-machine experiments show that LLM agents have strong grounding capacities, and our approach significantly improves the evaluation metric.</p>
  </details>
</details>
<details>
  <summary>115. <b>标题：DeFT: Flash Tree-attention with IO-Awareness for Efficient  Tree-search-based LLM Inference</b></summary>
  <p><b>编号</b>：[565]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00242">https://arxiv.org/abs/2404.00242</a></p>
  <p><b>作者</b>：Jinwei Yao,  Kaiqi Chen,  Kexun Zhang,  Jiaxuan You,  Binhang Yuan,  Zeke Wang,  Tao Lin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, transformer-based Large Language, Language Models, Large Language, transformer-based Large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Decoding using tree search can greatly enhance the inference quality for transformer-based Large Language Models (LLMs). Depending on the guidance signal, it searches for the best path from root to leaf in the tree by forming LLM outputs to improve controllability, reasoning ability, alignment, et cetera. However, current tree decoding strategies and their inference systems do not suit each other well due to redundancy in computation, memory footprints, and memory access, resulting in inefficient inference. To address this issue, we propose DeFT, an IO-aware tree attention algorithm that maintains memory-efficient attention calculation with low memory footprints in two stages: (1) QKV Preparation: we propose a KV-Guided Tree Split strategy to group QKV wisely for high utilization of GPUs and reduction of memory reads/writes for the KV cache between GPU global memory and on-chip shared memory as much as possible; (2) Attention Calculation: we calculate partial attention of each QKV groups in a fused kernel then apply a Tree-topology-aware Global Reduction strategy to get final attention. Thanks to a reduction in KV cache IO by 3.6-4.5$\times$, along with an additional reduction in IO for $\mathbf{Q} \mathbf{K}^\top$ and Softmax equivalent to 25% of the total KV cache IO, DeFT can achieve a speedup of 1.7-2.4$\times$ in end-to-end latency across two practical reasoning tasks over the SOTA attention algorithms.</p>
  </details>
</details>
<details>
  <summary>116. <b>标题：Attention-based Shape-Deformation Networks for Artifact-Free Geometry  Reconstruction of Lumbar Spine from MR Images</b></summary>
  <p><b>编号</b>：[571]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00231">https://arxiv.org/abs/2404.00231</a></p>
  <p><b>作者</b>：Linchen Qian,  Jiasong Chen,  Linhai Ma,  Timur Urakov,  Weiyong Gu,  Liang Liang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：low back pain, global health concern, progressive structural wear, significant global health, Lumbar disc degeneration</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Lumbar disc degeneration, a progressive structural wear and tear of lumbar intervertebral disc, is regarded as an essential role on low back pain, a significant global health concern. Automated lumbar spine geometry reconstruction from MR images will enable fast measurement of medical parameters to evaluate the lumbar status, in order to determine a suitable treatment. Existing image segmentation-based techniques often generate erroneous segments or unstructured point clouds, unsuitable for medical parameter measurement. In this work, we present TransDeformer: a novel attention-based deep learning approach that reconstructs the contours of the lumbar spine with high spatial accuracy and mesh correspondence across patients, and we also present a variant of TransDeformer for error estimation. Specially, we devise new attention modules with a new attention formula, which integrates image features and tokenized contour features to predict the displacements of the points on a shape template without the need for image segmentation. The deformed template reveals the lumbar spine geometry in the input image. We develop a multi-stage training strategy to enhance model robustness with respect to template initialization. Experiment results show that our TransDeformer generates artifact-free geometry outputs, and its variant predicts the error of a reconstructed geometry. Our code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>117. <b>标题：InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning</b></summary>
  <p><b>编号</b>：[573]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00228">https://arxiv.org/abs/2404.00228</a></p>
  <p><b>作者</b>：Yan-Shuo Liang,  Wu-Jun Li</p>
  <p><b>备注</b>：Accepted by the 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2024)</p>
  <p><b>关键词</b>：Continual learning, Continual learning requires, continual learning methods, learning, Continual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Continual learning requires the model to learn multiple tasks sequentially. In continual learning, the model should possess the ability to maintain its performance on old tasks (stability) and the ability to adapt to new tasks continuously (plasticity). Recently, parameter-efficient fine-tuning (PEFT), which involves freezing a pre-trained model and injecting a small number of learnable parameters to adapt to downstream tasks, has gained increasing popularity in continual learning. Although existing continual learning methods based on PEFT have demonstrated superior performance compared to those not based on PEFT, most of them do not consider how to eliminate the interference of the new task on the old tasks, which inhibits the model from making a good trade-off between stability and plasticity. In this work, we propose a new PEFT method, called interference-free low-rank adaptation (InfLoRA), for continual learning. InfLoRA injects a small number of parameters to reparameterize the pre-trained weights and shows that fine-tuning these injected parameters is equivalent to fine-tuning the pre-trained weights within a subspace. Furthermore, InfLoRA designs this subspace to eliminate the interference of the new task on the old tasks, making a good trade-off between stability and plasticity. Experimental results show that InfLoRA outperforms existing state-of-the-art continual learning methods on multiple datasets.</p>
  </details>
</details>
<details>
  <summary>118. <b>标题：Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge  Editing Benchmark</b></summary>
  <p><b>编号</b>：[579]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00216">https://arxiv.org/abs/2404.00216</a></p>
  <p><b>作者</b>：Baolong Bi,  Shenghua Liu,  Yiwei Wang,  Lingrui Mei,  Xueqi Cheng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, human-like fashion, decoding methods, rapid development, development of large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rapid development of large language models (LLMs) enables them to convey factual knowledge in a more human-like fashion. Extensive efforts have been made to reduce factual hallucinations by modifying LLMs with factuality decoding. However, they also pose risks of hindering knowledge updates, as they make models overly confident in known facts. In this work, we first revisite the current factuality decoding methods and verified their effectiveness in enhancing factual accuracy. Subsequently, we conduct further evaluation of several strong factuality decoding methods on the knowledge editing benchmark. All these decoding methods significantly diminish the performance of llama2 models compared to their original decoding, with the largest decrease being a staggering 81.3\%. This further indicates that the current existing decoding methods still cannot perfectly address the factual hallucinations, as they overlook the importance of preserving the flexibility for knowledge editing. Therefore, our work suggests that research into factual alignment should simultaneously focus on the effectiveness of knowledge editing.</p>
  </details>
</details>
<details>
  <summary>119. <b>标题：Causal Inference for Human-Language Model Collaboration</b></summary>
  <p><b>编号</b>：[586]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00207">https://arxiv.org/abs/2404.00207</a></p>
  <p><b>作者</b>：Bohan Zhang,  Yixin Wang,  Paramveer S. Dhillon</p>
  <p><b>备注</b>：9 pages (Accepted for publication at NAACL 2024 (Main Conference))</p>
  <p><b>关键词</b>：typically involve LMs, involve LMs proposing, interactions typically involve, proposing text segments, LMs proposing text</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we examine the collaborative dynamics between humans and language models (LMs), where the interactions typically involve LMs proposing text segments and humans editing or responding to these proposals. Productive engagement with LMs in such scenarios necessitates that humans discern effective text-based interaction strategies, such as editing and response styles, from historical human-LM interactions. This objective is inherently causal, driven by the counterfactual `what-if' question: how would the outcome of collaboration change if humans employed a different text editing/refinement strategy? A key challenge in answering this causal inference question is formulating an appropriate causal estimand: the conventional average treatment effect (ATE) estimand is inapplicable to text-based treatments due to their high dimensionality. To address this concern, we introduce a new causal estimand -- Incremental Stylistic Effect (ISE) -- which characterizes the average impact of infinitesimally shifting a text towards a specific style, such as increasing formality. We establish the conditions for the non-parametric identification of ISE. Building on this, we develop CausalCollab, an algorithm designed to estimate the ISE of various interaction strategies in dynamic human-LM collaborations. Our empirical investigations across three distinct human-LM collaboration scenarios reveal that CausalCollab effectively reduces confounding and significantly improves counterfactual estimation over a set of competitive baselines.</p>
  </details>
</details>
<details>
  <summary>120. <b>标题：Multiple-policy Evaluation via Density Estimation</b></summary>
  <p><b>编号</b>：[593]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00195">https://arxiv.org/abs/2404.00195</a></p>
  <p><b>作者</b>：Yilei Chen,  Aldo Pacchiano,  Ioannis Ch. Paschalidis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：expected total rewards, multiple-policy evaluation problem, evaluate their performance, total rewards, multiple-policy evaluation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this work, we focus on the multiple-policy evaluation problem where we are given a set of $K$ target policies and the goal is to evaluate their performance (the expected total rewards) to an accuracy $\epsilon$ with probability at least $1-\delta$. We propose an algorithm named $\mathrm{CAESAR}$ to address this problem. Our approach is based on computing an approximate optimal offline sampling distribution and using the data sampled from it to perform the simultaneous estimation of the policy values. $\mathrm{CAESAR}$ consists of two phases. In the first one we produce coarse estimates of the vistation distributions of the target policies at a low order sample complexity rate that scales with $\tilde{O}(\frac{1}{\epsilon})$. In the second phase, we approximate the optimal offline sampling distribution and compute the importance weighting ratios for all target policies by minimizing a step-wise quadratic loss function inspired by the objective in DualDICE. Up to low order and logarithm terms $\mathrm{CAESAR}$ achieves a sample complexity $\tilde{O}\left(\frac{H^4}{\epsilon^2}\sum_{h=1}^H\max_{k\in[K]}\sum_{s,a}\frac{(d_h^{\pi^k}(s,a))^2}{\mu^*_h(s,a)}\right)$, where $d^{\pi}$ is the visitation distribution of policy $\pi$ and $\mu^*$ is the optimal sampling distribution.</p>
  </details>
</details>
<details>
  <summary>121. <b>标题：DataAgent: Evaluating Large Language Models' Ability to Answer  Zero-Shot, Natural Language Queries</b></summary>
  <p><b>编号</b>：[598]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00188">https://arxiv.org/abs/2404.00188</a></p>
  <p><b>作者</b>：Manit Mishra,  Abderrahman Braham,  Charles Marsom,  Bryan Chung,  Gavin Griffin,  Dakshesh Sidnerlikar,  Chatanya Sarin,  Arjun Rajaram</p>
  <p><b>备注</b>：5 pages, Submitted to International Conference on AI in Cybersecurity</p>
  <p><b>关键词</b>：extracting meaningful information, Conventional processes, time-consuming and laborious, processes for analyzing, extracting meaningful</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Conventional processes for analyzing datasets and extracting meaningful information are often time-consuming and laborious. Previous work has identified manual, repetitive coding and data collection as major obstacles that hinder data scientists from undertaking more nuanced labor and high-level projects. To combat this, we evaluated OpenAI's GPT-3.5 as a "Language Data Scientist" (LDS) that can extrapolate key findings, including correlations and basic information, from a given dataset. The model was tested on a diverse set of benchmark datasets to evaluate its performance across multiple standards, including data science code-generation based tasks involving libraries such as NumPy, Pandas, Scikit-Learn, and TensorFlow, and was broadly successful in correctly answering a given data science query related to the benchmark dataset. The LDS used various novel prompt engineering techniques to effectively answer a given question, including Chain-of-Thought reinforcement and SayCan prompt engineering. Our findings demonstrate great potential for leveraging Large Language Models for low-level, zero-shot data analysis.</p>
  </details>
</details>
<details>
  <summary>122. <b>标题：On Inherent Adversarial Robustness of Active Vision Systems</b></summary>
  <p><b>编号</b>：[600]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00185">https://arxiv.org/abs/2404.00185</a></p>
  <p><b>作者</b>：Amitangshu Mukherjee,  Timur Ibrayev,  Kaushik Roy</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：carefully crafted noise, adding carefully crafted, Deep Neural Networks, Current Deep Neural, Deep Neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Current Deep Neural Networks are vulnerable to adversarial examples, which alter their predictions by adding carefully crafted noise. Since human eyes are robust to such inputs, it is possible that the vulnerability stems from the standard way of processing inputs in one shot by processing every pixel with the same importance. In contrast, neuroscience suggests that the human vision system can differentiate salient features by (1) switching between multiple fixation points (saccades) and (2) processing the surrounding with a non-uniform external resolution (foveation). In this work, we advocate that the integration of such active vision mechanisms into current deep learning systems can offer robustness benefits. Specifically, we empirically demonstrate the inherent robustness of two active vision methods - GFNet and FALcon - under a black box threat model. By learning and inferencing based on downsampled glimpses obtained from multiple distinct fixation points within an input, we show that these active methods achieve (2-3) times greater robustness compared to a standard passive convolutional network under state-of-the-art adversarial attacks. More importantly, we provide illustrative and interpretable visualization analysis that demonstrates how performing inference from distinct fixation points makes active vision methods less vulnerable to malicious inputs.</p>
  </details>
</details>
<details>
  <summary>123. <b>标题：Universal Bovine Identification via Depth Data and Deep Metric Learning</b></summary>
  <p><b>编号</b>：[605]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00172">https://arxiv.org/abs/2404.00172</a></p>
  <p><b>作者</b>：Asheesh Sharma,  Lucy Randewich,  William Andrew,  Sion Hannuna,  Neill Campbell,  Siobhan Mullan,  Andrew W. Dowsey,  Melvyn Smith,  Mark Hansen,  Tilo Burghardt</p>
  <p><b>备注</b>：LaTeX, 38 pages, 14 figures, 3 tables</p>
  <p><b>关键词</b>：depth-only deep learning, deep learning system, accurately identifying individual, dorsal view, identifying individual cattle</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper proposes and evaluates, for the first time, a top-down (dorsal view), depth-only deep learning system for accurately identifying individual cattle and provides associated code, datasets, and training weights for immediate reproducibility. An increase in herd size skews the cow-to-human ratio at the farm and makes the manual monitoring of individuals more challenging. Therefore, real-time cattle identification is essential for the farms and a crucial step towards precision livestock farming. Underpinned by our previous work, this paper introduces a deep-metric learning method for cattle identification using depth data from an off-the-shelf 3D camera. The method relies on CNN and MLP backbones that learn well-generalised embedding spaces from the body shape to differentiate individuals -- requiring neither species-specific coat patterns nor close-up muzzle prints for operation. The network embeddings are clustered using a simple algorithm such as $k$-NN for highly accurate identification, thus eliminating the need to retrain the network for enrolling new individuals. We evaluate two backbone architectures, ResNet, as previously used to identify Holstein Friesians using RGB images, and PointNet, which is specialised to operate on 3D point clouds. We also present CowDepth2023, a new dataset containing 21,490 synchronised colour-depth image pairs of 99 cows, to evaluate the backbones. Both ResNet and PointNet architectures, which consume depth maps and point clouds, respectively, led to high accuracy that is on par with the coat pattern-based backbone.</p>
  </details>
</details>
<details>
  <summary>124. <b>标题：Uncovering Bias in Large Vision-Language Models with Counterfactuals</b></summary>
  <p><b>编号</b>：[608]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00166">https://arxiv.org/abs/2404.00166</a></p>
  <p><b>作者</b>：Phillip Howard,  Anahita Bhiwandiwalla,  Kathleen C. Fraser,  Svetlana Kiritchenko</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Large Vision-Language Models, Large Language, increasingly impressive capabilities, possessing increasingly impressive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the advent of Large Language Models (LLMs) possessing increasingly impressive capabilities, a number of Large Vision-Language Models (LVLMs) have been proposed to augment LLMs with visual inputs. Such models condition generated text on both an input image and a text prompt, enabling a variety of use cases such as visual question answering and multimodal chat. While prior studies have examined the social biases contained in text generated by LLMs, this topic has been relatively unexplored in LVLMs. Examining social biases in LVLMs is particularly challenging due to the confounding contributions of bias induced by information contained across the text and visual modalities. To address this challenging problem, we conduct a large-scale study of text generated by different LVLMs under counterfactual changes to input images. Specifically, we present LVLMs with identical open-ended text prompts while conditioning on images from different counterfactual sets, where each set contains images which are largely identical in their depiction of a common subject (e.g., a doctor), but vary only in terms of intersectional social attributes (e.g., race and gender). We comprehensively evaluate the text produced by different LVLMs under this counterfactual generation setting and find that social attributes such as race, gender, and physical characteristics depicted in input images can significantly influence toxicity and the generation of competency-associated words.</p>
  </details>
</details>
<details>
  <summary>125. <b>标题：Accelerating Search-Based Planning for Multi-Robot Manipulation by  Leveraging Online-Generated Experiences</b></summary>
  <p><b>编号</b>：[619]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00143">https://arxiv.org/abs/2404.00143</a></p>
  <p><b>作者</b>：Yorai Shaoul,  Itamar Mishani,  Maxim Likhachev,  Jiaoyang Li</p>
  <p><b>备注</b>：The first two authors contributed equally. Accepted to ICAPS 2024</p>
  <p><b>关键词</b>：exciting frontier, multiple arms, planning concurrent motions, MAPF, algorithms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>An exciting frontier in robotic manipulation is the use of multiple arms at once. However, planning concurrent motions is a challenging task using current methods. The high-dimensional composite state space renders many well-known motion planning algorithms intractable. Recently, Multi-Agent Path-Finding (MAPF) algorithms have shown promise in discrete 2D domains, providing rigorous guarantees. However, widely used conflict-based methods in MAPF assume an efficient single-agent motion planner. This poses challenges in adapting them to manipulation cases where this assumption does not hold, due to the high dimensionality of configuration spaces and the computational bottlenecks associated with collision checking. To this end, we propose an approach for accelerating conflict-based search algorithms by leveraging their repetitive and incremental nature -- making them tractable for use in complex scenarios involving multi-arm coordination in obstacle-laden environments. We show that our method preserves completeness and bounded sub-optimality guarantees, and demonstrate its practical efficacy through a set of experiments with up to 10 robotic arms.</p>
  </details>
</details>
<details>
  <summary>126. <b>标题：Does Faithfulness Conflict with Plausibility? An Empirical Study in  Explainable AI across NLP Tasks</b></summary>
  <p><b>编号</b>：[621]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00140">https://arxiv.org/abs/2404.00140</a></p>
  <p><b>作者</b>：Xiaolei Lu,  Jianghong Ma</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：model inference process, explanations accurately reflect, inference process, aimed at interpreting, interpreting decision-making</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Explainability algorithms aimed at interpreting decision-making AI systems usually consider balancing two critical dimensions: 1) \textit{faithfulness}, where explanations accurately reflect the model's inference process. 2) \textit{plausibility}, where explanations are consistent with domain experts. However, the question arises: do faithfulness and plausibility inherently conflict? In this study, through a comprehensive quantitative comparison between the explanations from the selected explainability methods and expert-level interpretations across three NLP tasks: sentiment analysis, intent detection, and topic labeling, we demonstrate that traditional perturbation-based methods Shapley value and LIME could attain greater faithfulness and plausibility. Our findings suggest that rather than optimizing for one dimension at the expense of the other, we could seek to optimize explainability algorithms with dual objectives to achieve high levels of accuracy and user accessibility in their explanations.</p>
  </details>
</details>
<details>
  <summary>127. <b>标题：Security Risks Concerns of Generative AI in the IoT</b></summary>
  <p><b>编号</b>：[622]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00139">https://arxiv.org/abs/2404.00139</a></p>
  <p><b>作者</b>：Honghui Xu,  Yingshu Li,  Olusesi Balogun,  Shaoen Wu,  Yue Wang,  Zhipeng Cai</p>
  <p><b>备注</b>：6 pages, 2 figures</p>
  <p><b>关键词</b>：generative Artificial Intelligence, Internet of Things, Artificial Intelligence, generative Artificial, intersects increasingly</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In an era where the Internet of Things (IoT) intersects increasingly with generative Artificial Intelligence (AI), this article scrutinizes the emergent security risks inherent in this integration. We explore how generative AI drives innovation in IoT and we analyze the potential for data breaches when using generative AI and the misuse of generative AI technologies in IoT ecosystems. These risks not only threaten the privacy and efficiency of IoT systems but also pose broader implications for trust and safety in AI-driven environments. The discussion in this article extends to strategic approaches for mitigating these risks, including the development of robust security protocols, the multi-layered security approaches, and the adoption of AI technological solutions. Through a comprehensive analysis, this article aims to shed light on the critical balance between embracing AI advancements and ensuring stringent security in IoT, providing insights into the future direction of these intertwined technologies.</p>
  </details>
</details>
<details>
  <summary>128. <b>标题：Budget-aware Query Tuning: An AutoML Perspective</b></summary>
  <p><b>编号</b>：[623]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00137">https://arxiv.org/abs/2404.00137</a></p>
  <p><b>作者</b>：Wentao Wu,  Chi Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：good execution plans, cost units, cost, query, Modern database systems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Modern database systems rely on cost-based query optimizers to come up with good execution plans for input queries. Such query optimizers rely on cost models to estimate the costs of candidate query execution plans. A cost model represents a function from a set of cost units to query execution cost, where each cost unit specifies the unit cost of executing a certain type of query processing operation (such as table scan or join). These cost units are traditionally viewed as constants, whose values only depend on the platform configuration where the database system runs on top of but are invariant for queries processed by the database system. In this paper, we challenge this classic view by thinking of these cost units as variables instead. We show that, by varying the cost-unit values one can obtain query plans that significantly outperform the default query plans returned by the query optimizer when viewing the cost units as constants. We term this cost-unit tuning process "query tuning" (QT) and show that it is similar to the well-known hyper-parameter optimization (HPO) problem in AutoML. As a result, any state-of-the-art HPO technologies can be applied to QT. We study the QT problem in the context of anytime tuning, which is desirable in practice by constraining the total time spent on QT within a given budget -- we call this problem budget-aware query tuning. We further extend our study from tuning a single query to tuning a workload with multiple queries, and we call this generalized problem budget-aware workload tuning (WT), which aims for minimizing the execution time of the entire workload. WT is more challenging as one needs to further prioritize individual query tuning within the given time budget. We propose solutions to both QT and WT and experimental evaluation using both benchmark and real workloads demonstrates the efficacy of our proposed solutions.</p>
  </details>
</details>
<details>
  <summary>129. <b>标题：Robust Ensemble Person Re-Identification via Orthogonal Fusion with  Occlusion Handling</b></summary>
  <p><b>编号</b>：[634]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00107">https://arxiv.org/abs/2404.00107</a></p>
  <p><b>作者</b>：Syeda Nyma Ferdous,  Xin Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：variation of appearances, major challenges, diversity of poses, person reidentification, occluded regions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Occlusion remains one of the major challenges in person reidentification (ReID) as a result of the diversity of poses and the variation of appearances. Developing novel architectures to improve the robustness of occlusion-aware person Re-ID requires new insights, especially on low-resolution edge cameras. We propose a deep ensemble model that harnesses both CNN and Transformer architectures to generate robust feature representations. To achieve robust Re-ID without the need to manually label occluded regions, we propose to take an ensemble learning-based approach derived from the analogy between arbitrarily shaped occluded regions and robust feature representation. Using the orthogonality principle, our developed deep CNN model makes use of masked autoencoder (MAE) and global-local feature fusion for robust person identification. Furthermore, we present a part occlusion-aware transformer capable of learning feature space that is robust to occluded regions. Experimental results are reported on several Re-ID datasets to show the effectiveness of our developed ensemble model named orthogonal fusion with occlusion handling (OFOH). Compared to competing methods, the proposed OFOH approach has achieved competent rank-1 and mAP performance.</p>
  </details>
</details>
<details>
  <summary>130. <b>标题：Efficient and Sharp Off-Policy Evaluation in Robust Markov Decision  Processes</b></summary>
  <p><b>编号</b>：[636]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00099">https://arxiv.org/abs/2404.00099</a></p>
  <p><b>作者</b>：Andrew Bennett,  Nathan Kallus,  Miruna Oprescu,  Wen Sun,  Kaiwen Wang</p>
  <p><b>备注</b>：40 pages, 1 figure</p>
  <p><b>关键词</b>：Markov decision process, original MDP, Markov decision, transition observations, MDP</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study evaluating a policy under best- and worst-case perturbations to a Markov decision process (MDP), given transition observations from the original MDP, whether under the same or different policy. This is an important problem when there is the possibility of a shift between historical and future environments, due to e.g. unmeasured confounding, distributional shift, or an adversarial environment. We propose a perturbation model that can modify transition kernel densities up to a given multiplicative factor or its reciprocal, which extends the classic marginal sensitivity model (MSM) for single time step decision making to infinite-horizon RL. We characterize the sharp bounds on policy value under this model, that is, the tightest possible bounds given by the transition observations from the original MDP, and we study the estimation of these bounds from such transition observations. We develop an estimator with several appealing guarantees: it is semiparametrically efficient, and remains so even when certain necessary nuisance functions such as worst-case Q-functions are estimated at slow nonparametric rates. It is also asymptotically normal, enabling easy statistical inference using Wald confidence intervals. In addition, when certain nuisances are estimated inconsistently we still estimate a valid, albeit possibly not sharp bounds on the policy value. We validate these properties in numeric simulations. The combination of accounting for environment shifts from train to test (robustness), being insensitive to nuisance-function estimation (orthogonality), and accounting for having only finite samples to learn from (inference) together leads to credible and reliable policy evaluation.</p>
  </details>
</details>
<details>
  <summary>131. <b>标题：A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping  Attacks</b></summary>
  <p><b>编号</b>：[641]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00076">https://arxiv.org/abs/2404.00076</a></p>
  <p><b>作者</b>：Orson Mengara</p>
  <p><b>备注</b>：Accept by "IEEE Access" let's take a look at our global approach to the DNN(s) model(s) deployment chain in production: Danger NLP-Speech(Trigger universal approach)</p>
  <p><b>关键词</b>：Audio-based machine learning, machine learning systems, learning systems frequently, Audio-based machine, machine learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Audio-based machine learning systems frequently use public or third-party data, which might be inaccurate. This exposes deep neural network (DNN) models trained on such data to potential data poisoning attacks. In this type of assault, attackers can train the DNN model using poisoned data, potentially degrading its performance. Another type of data poisoning attack that is extremely relevant to our investigation is label flipping, in which the attacker manipulates the labels for a subset of data. It has been demonstrated that these assaults may drastically reduce system performance, even for attackers with minimal abilities. In this study, we propose a backdoor attack named 'DirtyFlipping', which uses dirty label techniques, "label-on-label", to input triggers (clapping) in the selected data patterns associated with the target class, thereby enabling a stealthy backdoor.</p>
  </details>
</details>
<details>
  <summary>132. <b>标题：PerOS: Personalized Self-Adapting Operating Systems in the Cloud</b></summary>
  <p><b>编号</b>：[650]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00057">https://arxiv.org/abs/2404.00057</a></p>
  <p><b>作者</b>：Hongyu Hè</p>
  <p><b>备注</b>：29 pages, 3 figures</p>
  <p><b>关键词</b>：managing hardware resources, ensuring secure environments, diverse applications, foundational to computer, resources and ensuring</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Operating systems (OSes) are foundational to computer systems, managing hardware resources and ensuring secure environments for diverse applications. However, despite their enduring importance, the fundamental design objectives of OSes have seen minimal evolution over decades. Traditionally prioritizing aspects like speed, memory efficiency, security, and scalability, these objectives often overlook the crucial aspect of intelligence as well as personalized user experience. The lack of intelligence becomes increasingly critical amid technological revolutions, such as the remarkable advancements in machine learning (ML).
Today's personal devices, evolving into intimate companions for users, pose unique challenges for traditional OSes like Linux and iOS, especially with the emergence of specialized hardware featuring heterogeneous components. Furthermore, the rise of large language models (LLMs) in ML has introduced transformative capabilities, reshaping user interactions and software development paradigms.
While existing literature predominantly focuses on leveraging ML methods for system optimization or accelerating ML workloads, there is a significant gap in addressing personalized user experiences at the OS level. To tackle this challenge, this work proposes PerOS, a personalized OS ingrained with LLM capabilities. PerOS aims to provide tailored user experiences while safeguarding privacy and personal data through declarative interfaces, self-adaptive kernels, and secure data management in a scalable cloud-centric architecture; therein lies the main research question of this work: How can we develop intelligent, secure, and scalable OSes that deliver personalized experiences to thousands of users?</p>
  </details>
</details>
<details>
  <summary>133. <b>标题：Deja vu: Contrastive Historical Modeling with Prefix-tuning for Temporal  Knowledge Graph Reasoning</b></summary>
  <p><b>编号</b>：[653]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00051">https://arxiv.org/abs/2404.00051</a></p>
  <p><b>作者</b>：Miao Peng,  Ben Liu,  Wenjie Xu,  Zihao Jiang,  Jiahui Zhu,  Min Peng</p>
  <p><b>备注</b>：Accepted to NAACL 2024 Findings</p>
  <p><b>关键词</b>：Knowledge Graph Reasoning, gaining increasing attention, inferring missing facts, Temporal Knowledge Graph, Graph Reasoning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Temporal Knowledge Graph Reasoning (TKGR) is the task of inferring missing facts for incomplete TKGs in complex scenarios (e.g., transductive and inductive settings), which has been gaining increasing attention. Recently, to mitigate dependence on structured connections in TKGs, text-based methods have been developed to utilize rich linguistic information from entity descriptions. However, suffering from the enormous parameters and inflexibility of pre-trained language models, existing text-based methods struggle to balance the textual knowledge and temporal information with computationally expensive purpose-built training strategies. To tap the potential of text-based models for TKGR in various complex scenarios, we propose ChapTER, a Contrastive historical modeling framework with prefix-tuning for TEmporal Reasoning. ChapTER feeds history-contextualized text into the pseudo-Siamese encoders to strike a textual-temporal balance via contrastive estimation between queries and candidates. By introducing virtual time prefix tokens, it applies a prefix-based tuning method to facilitate the frozen PLM capable for TKGR tasks under different settings. We evaluate ChapTER on four transductive and three few-shot inductive TKGR benchmarks, and experimental results demonstrate that ChapTER achieves superior performance compared to competitive baselines with only 0.17% tuned parameters. We conduct thorough analysis to verify the effectiveness, flexibility and efficiency of ChapTER.</p>
  </details>
</details>
<details>
  <summary>134. <b>标题：Policy Optimization finds Nash Equilibrium in Regularized General-Sum LQ  Games</b></summary>
  <p><b>编号</b>：[657]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00045">https://arxiv.org/abs/2404.00045</a></p>
  <p><b>作者</b>：Muhammad Aneeq uz Zaman,  Shubham Aggarwal,  Melih Bastopcu,  Tamer Başar</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：linear Gaussian policies, Nash Equilibria, introducing relative entropy, relative entropy regularization, Gaussian policies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we investigate the impact of introducing relative entropy regularization on the Nash Equilibria (NE) of General-Sum $N$-agent games, revealing the fact that the NE of such games conform to linear Gaussian policies. Moreover, it delineates sufficient conditions, contingent upon the adequacy of entropy regularization, for the uniqueness of the NE within the game. As Policy Optimization serves as a foundational approach for Reinforcement Learning (RL) techniques aimed at finding the NE, in this work we prove the linear convergence of a policy optimization algorithm which (subject to the adequacy of entropy regularization) is capable of provably attaining the NE. Furthermore, in scenarios where the entropy regularization proves insufficient, we present a $\delta$-augmentation technique, which facilitates the achievement of an $\epsilon$-NE within the game.</p>
  </details>
</details>
<details>
  <summary>135. <b>标题：MicroHD: An Accuracy-Driven Optimization of Hyperdimensional Computing  Algorithms for TinyML systems</b></summary>
  <p><b>编号</b>：[661]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00039">https://arxiv.org/abs/2404.00039</a></p>
  <p><b>作者</b>：Flavio Ponzina,  Tajana Rosing</p>
  <p><b>备注</b>：Accepted as a full paper by the tinyML Research Symposium 2024</p>
  <p><b>关键词</b>：effectively target TinyML, target TinyML applications, HDC, effectively target, target TinyML</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Hyperdimensional computing (HDC) is emerging as a promising AI approach that can effectively target TinyML applications thanks to its lightweight computing and memory requirements. Previous works on HDC showed that limiting the standard 10k dimensions of the hyperdimensional space to much lower values is possible, reducing even more HDC resource requirements. Similarly, other studies demonstrated that binary values can be used as elements of the generated hypervectors, leading to significant efficiency gains at the cost of some degree of accuracy degradation. Nevertheless, current optimization attempts do not concurrently co-optimize HDC hyper-parameters, and accuracy degradation is not directly controlled, resulting in sub-optimal HDC models providing several applications with unacceptable output qualities. In this work, we propose MicroHD, a novel accuracy-driven HDC optimization approach that iteratively tunes HDC hyper-parameters, reducing memory and computing requirements while ensuring user-defined accuracy levels. The proposed method can be applied to HDC implementations using different encoding functions, demonstrates good scalability for larger HDC workloads, and achieves compression and efficiency gains up to 200x when compared to baseline implementations for accuracy degradations lower than 1%.</p>
  </details>
</details>
<details>
  <summary>136. <b>标题：Complementarity in Human-AI Collaboration: Concept, Sources, and  Evidence</b></summary>
  <p><b>编号</b>：[667]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00029">https://arxiv.org/abs/2404.00029</a></p>
  <p><b>作者</b>：Patrick Hemmer,  Max Schemmer,  Niklas Kühl,  Michael Vössing,  Gerhard Satzger</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Artificial intelligence, application areas, CTP, improve human decision-making, complementarity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Artificial intelligence (AI) can improve human decision-making in various application areas. Ideally, collaboration between humans and AI should lead to complementary team performance (CTP) -- a level of performance that neither of them can attain individually. So far, however, CTP has rarely been observed, suggesting an insufficient understanding of the complementary constituents in human-AI collaboration that can contribute to CTP in decision-making. This work establishes a holistic theoretical foundation for understanding and developing human-AI complementarity. We conceptualize complementarity by introducing and formalizing the notion of complementarity potential and its realization. Moreover, we identify and outline sources that explain CTP. We illustrate our conceptualization by applying it in two empirical studies exploring two different sources of complementarity potential. In the first study, we focus on information asymmetry as a source and, in a real estate appraisal use case, demonstrate that humans can leverage unique contextual information to achieve CTP. In the second study, we focus on capability asymmetry as an alternative source, demonstrating how heterogeneous capabilities can help achieve CTP. Our work provides researchers with a theoretical foundation of complementarity in human-AI decision-making and demonstrates that leveraging sources of complementarity potential constitutes a viable pathway toward effective human-AI collaboration.</p>
  </details>
</details>
<details>
  <summary>137. <b>标题：LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership  and Reasoning</b></summary>
  <p><b>编号</b>：[668]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00027">https://arxiv.org/abs/2404.00027</a></p>
  <p><b>作者</b>：Azmine Toushik Wasi,  Rafia Islam,  Raima Islam</p>
  <p><b>备注</b>：4 Pages, 2 Figures; The Third Workshop on Intelligent and Interactive Writing Assistants, a hybrid event co-located with The ACM CHI Conference on Human Factors in Computing Systems (CHI 2024) Openreview: this https URL</p>
  <p><b>关键词</b>：investment of thoughts, leading to attachment, Large Language Models, confines our investment, credit Large Language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems.</p>
  </details>
</details>
<details>
  <summary>138. <b>标题：Ink and Individuality: Crafting a Personalised Narrative in the Age of  LLMs</b></summary>
  <p><b>编号</b>：[669]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00026">https://arxiv.org/abs/2404.00026</a></p>
  <p><b>作者</b>：Azmine Toushik Wasi,  Raima Islam,  Rafia Islam</p>
  <p><b>备注</b>：4 Pages, 2 Figures; The Third Workshop on Intelligent and Interactive Writing Assistants, a hybrid event co-located with The ACM CHI Conference on Human Factors in Computing Systems (CHI 2024) Openreview: this https URL</p>
  <p><b>关键词</b>：effectively engage readers, conveying authenticity, comprise the distinctive, distinctive characteristics, characteristics that make</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.</p>
  </details>
</details>
<details>
  <summary>139. <b>标题：Analysing and Organising Human Communications for AI Fairness-Related  Decisions: Use Cases from the Public Sector</b></summary>
  <p><b>编号</b>：[672]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00022">https://arxiv.org/abs/2404.00022</a></p>
  <p><b>作者</b>：Mirthe Dankloff,  Vanja Skoric,  Giovanni Sileno,  Sennay Ghebreab,  Jacco Van Ossenbruggen,  Emma Beauxis-Aussalet</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：allocating social benefits, involve multiple public, predicting fraud, allocating social, social benefits</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>AI algorithms used in the public sector, e.g., for allocating social benefits or predicting fraud, often involve multiple public and private stakeholders at various phases of the algorithm's life-cycle. Communication issues between these diverse stakeholders can lead to misinterpretation and misuse of algorithms. We investigate the communication processes for AI fairness-related decisions by conducting interviews with practitioners working on algorithmic systems in the public sector. By applying qualitative coding analysis, we identify key elements of communication processes that underlie fairness-related human decisions. We analyze the division of roles, tasks, skills, and challenges perceived by stakeholders. We formalize the underlying communication issues within a conceptual framework that i. represents the communication patterns ii. outlines missing elements, such as actors who miss skills for their tasks. The framework is used for describing and analyzing key organizational issues for fairness-related decisions. Three general patterns emerge from the analysis: 1. Policy-makers, civil servants, and domain experts are less involved compared to developers throughout a system's life-cycle. This leads to developers taking on extra roles such as advisor, while they potentially miss the required skills and guidance from domain experts. 2. End-users and policy-makers often lack the technical skills to interpret a system's limitations, and rely on developer roles for making decisions concerning fairness issues. 3. Citizens are structurally absent throughout a system's life-cycle, which may lead to decisions that do not include relevant considerations from impacted stakeholders.</p>
  </details>
</details>
<details>
  <summary>140. <b>标题：Advancing Explainable Autonomous Vehicle Systems: A Comprehensive Review  and Research Roadmap</b></summary>
  <p><b>编号</b>：[674]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00019">https://arxiv.org/abs/2404.00019</a></p>
  <p><b>作者</b>：Sule Tekkesinoglu,  Azra Habibovic,  Lars Kunze</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：existing explainability methods, contexts requiring explanations, suitable interaction strategies, uncertainty surrounding, explainability methods</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Given the uncertainty surrounding how existing explainability methods for autonomous vehicles (AVs) meet the diverse needs of stakeholders, a thorough investigation is imperative to determine the contexts requiring explanations and suitable interaction strategies. A comprehensive review becomes crucial to assess the alignment of current approaches with the varied interests and expectations within the AV ecosystem. This study presents a review to discuss the complexities associated with explanation generation and presentation to facilitate the development of more effective and inclusive explainable AV systems. Our investigation led to categorising existing literature into three primary topics: explanatory tasks, explanatory information, and explanatory information communication. Drawing upon our insights, we have proposed a comprehensive roadmap for future research centred on (i) knowing the interlocutor, (ii) generating timely explanations, (ii) communicating human-friendly explanations, and (iv) continuous learning. Our roadmap is underpinned by principles of responsible research and innovation, emphasising the significance of diverse explanation requirements. To effectively tackle the challenges associated with implementing explainable AV systems, we have delineated various research directions, including the development of privacy-preserving data integration, ethical frameworks, real-time analytics, human-centric interaction design, and enhanced cross-disciplinary collaborations. By exploring these research directions, the study aims to guide the development and deployment of explainable AVs, informed by a holistic understanding of user needs, technological advancements, regulatory compliance, and ethical considerations, thereby ensuring safer and more trustworthy autonomous driving experiences.</p>
  </details>
</details>
<details>
  <summary>141. <b>标题：Can AI Outperform Human Experts in Creating Social Media Creatives?</b></summary>
  <p><b>编号</b>：[675]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00018">https://arxiv.org/abs/2404.00018</a></p>
  <p><b>作者</b>：Eunkyung Park,  Raymond K. Wong,  Junbum Kwon</p>
  <p><b>备注</b>：17 pages, 5 figures</p>
  <p><b>关键词</b>：Artificial Intelligence, Intelligence has outperformed, chess and baduk, Large Language Models, outperformed human experts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Artificial Intelligence has outperformed human experts in functional tasks such as chess and baduk. How about creative tasks? This paper evaluates AI's capability in the creative domain compared to human experts, which little research has been conducted so far. We propose a novel Prompt-for-Prompt to generate social media creatives via prompt augmentation by Large Language Models. We take the most popular Instagram posts (with the biggest number of like clicks) in top brands' Instagram accounts to create social media creatives. We give GPT 4 several prompt instructions with text descriptions to generate the most effective prompts for cutting-edge text-to-image generators: Midjourney, DALL E 3, and Stable Diffusion. LLM-augmented prompts can boost AI's abilities by adding objectives, engagement strategy, lighting and brand consistency for social media image creation. We conduct an extensive human evaluation experiment, and find that AI excels human experts, and Midjourney is better than the other text-to-image generators. Surprisingly, unlike conventional wisdom in the social media industry, prompt instruction including eye-catching shows much poorer performance than those including natural. Regarding the type of creatives, AI improves creatives with animals or products but less with real people. Also, AI improves creatives with short text descriptions more than with long text descriptions, because there is more room for AI to augment prompts with shorter descriptions.</p>
  </details>
</details>
<details>
  <summary>142. <b>标题：Psittacines of Innovation? Assessing the True Novelty of AI Creations</b></summary>
  <p><b>编号</b>：[676]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00017">https://arxiv.org/abs/2404.00017</a></p>
  <p><b>作者</b>：Anirban Mukherjee</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Artificial Intelligence, regurgitating patterns learned, examine whether Artificial, learned during training, regurgitating patterns</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We examine whether Artificial Intelligence (AI) systems generate truly novel ideas rather than merely regurgitating patterns learned during training. Utilizing a novel experimental design, we task an AI with generating project titles for hypothetical crowdfunding campaigns. We compare within AI-generated project titles, measuring repetition and complexity. We compare between the AI-generated titles and actual observed field data using an extension of maximum mean discrepancy--a metric derived from the application of kernel mean embeddings of statistical distributions to high-dimensional machine learning (large language) embedding vectors--yielding a structured analysis of AI output novelty. Results suggest that (1) the AI generates unique content even under increasing task complexity, and at the limits of its computational capabilities, (2) the generated content has face validity, being consistent with both inputs to other generative AI and in qualitative comparison to field data, and (3) exhibits divergence from field data, mitigating concerns relating to intellectual property rights. We discuss implications for copyright and trademark law.</p>
  </details>
</details>
<details>
  <summary>143. <b>标题：Missing Data Imputation With Granular Semantics and AI-driven Pipeline  for Bankruptcy Prediction</b></summary>
  <p><b>编号</b>：[678]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00013">https://arxiv.org/abs/2404.00013</a></p>
  <p><b>作者</b>：Debarati Chakraborty,  Ravi Ranjan</p>
  <p><b>备注</b>：15 pages</p>
  <p><b>关键词</b>：work focuses, focuses on designing, missing, prediction, missing data imputation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This work focuses on designing a pipeline for the prediction of bankruptcy. The presence of missing values, high dimensional data, and highly class-imbalance databases are the major challenges in the said task. A new method for missing data imputation with granular semantics has been introduced here. The merits of granular computing have been explored here to define this method. The missing values have been predicted using the feature semantics and reliable observations in a low-dimensional space, in the granular space. The granules are formed around every missing entry, considering a few of the highly correlated features and most reliable closest observations to preserve the relevance and reliability, the context, of the database against the missing entries. An intergranular prediction is then carried out for the imputation within those contextual granules. That is, the contextual granules enable a small relevant fraction of the huge database to be used for imputation and overcome the need to access the entire database repetitively for each missing value. This method is then implemented and tested for the prediction of bankruptcy with the Polish Bankruptcy dataset. It provides an efficient solution for big and high-dimensional datasets even with large imputation rates. Then an AI-driven pipeline for bankruptcy prediction has been designed using the proposed granular semantic-based data filling method followed by the solutions to the issues like high dimensional dataset and high class-imbalance in the dataset. The rest of the pipeline consists of feature selection with the random forest for reducing dimensionality, data balancing with SMOTE, and prediction with six different popular classifiers including deep NN. All methods defined here have been experimentally verified with suitable comparative studies and proven to be effective on all the data sets captured over the five years.</p>
  </details>
</details>
<details>
  <summary>144. <b>标题：Nonlinear Impulse Pattern Formulation dynamical social and political  prediction algorithm for city planning and public participation</b></summary>
  <p><b>编号</b>：[700]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00977">https://arxiv.org/abs/2404.00977</a></p>
  <p><b>作者</b>：Rolf Bader,  Simon Linke,  Stefanie Gernert</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Impulse Pattern Formulation, Pattern Formulation, Impulse Pattern, predicting relevant parameters, artistic freedom</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>A nonlinear-dynamical algorithm for city planning is proposed as an Impulse Pattern Formulation (IPF) for predicting relevant parameters like health, artistic freedom, or financial developments of different social or political stakeholders over the cause of a planning process. The IPF has already shown high predictive precision at low computational cost in musical instrument simulations, brain dynamics, and human-human interactions. The social and political IPF consists of three basic equations of system state developments, self-adaptation of stakeholders, two adaptive interactions, and external impact terms suitable for respective planning situations. Typical scenarios of stakeholder interactions and developments are modeled by adjusting a set of system parameters. These include stakeholder reaction to external input, enhanced system stability through self-adaptation, stakeholder convergence due to mediative interaction adaptation, as well as complex dynamics in terms of direct stakeholder impacts. A workflow for implementing the algorithm in real city planning scenarios is outlined. This workflow includes machine learning of a suitable set of parameters suggesting best-practice planning to aim at the desired development of the planning process and its output.</p>
  </details>
</details>
<details>
  <summary>145. <b>标题：Algorithmic Collusion by Large Language Models</b></summary>
  <p><b>编号</b>：[706]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00806">https://arxiv.org/abs/2404.00806</a></p>
  <p><b>作者</b>：Sara Fish,  Yannai A. Gonczarowski,  Ran I. Shorrer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, pricing raises concerns, algorithmic pricing raises, algorithmic pricing, raises concerns</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rise of algorithmic pricing raises concerns of algorithmic collusion. We conduct experiments with algorithmic pricing agents based on Large Language Models (LLMs), and specifically GPT-4. We find that (1) LLM-based agents are adept at pricing tasks, (2) LLM-based pricing agents autonomously collude in oligopoly settings to the detriment of consumers, and (3) variation in seemingly innocuous phrases in LLM instructions ("prompts") may increase collusion. These results extend to auction settings. Our findings underscore the need for antitrust regulation regarding algorithmic pricing, and uncover regulatory challenges unique to LLM-based pricing agents.</p>
  </details>
</details>
<details>
  <summary>146. <b>标题：Scaling Properties of Speech Language Models</b></summary>
  <p><b>编号</b>：[710]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00685">https://arxiv.org/abs/2404.00685</a></p>
  <p><b>作者</b>：Santiago Cuervo,  Ricard Marxer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Language Models, Large Language Models, aim to learn, raw audio, textual resources</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost semantic understanding and the effects of coarser speech tokenization.</p>
  </details>
</details>
<details>
  <summary>147. <b>标题：From attention to profit: quantitative trading strategy based on  transformer</b></summary>
  <p><b>编号</b>：[721]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00424">https://arxiv.org/abs/2404.00424</a></p>
  <p><b>作者</b>：Zhaofeng Zhang,  Banghao Chen,  Shengxin Zhu,  Nicolas Langrené</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：financial market presents, dynamic financial market, navigating the complicated, persistent challenge, complicated and dynamic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In traditional quantitative trading practice, navigating the complicated and dynamic financial market presents a persistent challenge. Former machine learning approaches have struggled to fully capture various market variables, often ignore long-term information and fail to catch up with essential signals that may lead the profit. This paper introduces an enhanced transformer architecture and designs a novel factor based on the model. By transfer learning from sentiment analysis, the proposed model not only exploits its original inherent advantages in capturing long-range dependencies and modelling complex data relationships but is also able to solve tasks with numerical inputs and accurately forecast future returns over a period. This work collects more than 5,000,000 rolling data of 4,601 stocks in the Chinese capital market from 2010 to 2019. The results of this study demonstrated the model's superior performance in predicting stock trends compared with other 100 factor-based quantitative strategies with lower turnover rates and a more robust half-life period. Notably, the model's innovative use transformer to establish factors, in conjunction with market sentiment information, has been shown to enhance the accuracy of trading signals significantly, thereby offering promising implications for the future of quantitative trading strategies.</p>
  </details>
</details>
<details>
  <summary>148. <b>标题：Language Models are Spacecraft Operators</b></summary>
  <p><b>编号</b>：[722]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00413">https://arxiv.org/abs/2404.00413</a></p>
  <p><b>作者</b>：Victor Rodriguez-Fernandez,  Alejandro Carrasco,  Jason Cheng,  Eli Scharf,  Peng Mun Siew,  Richard Linares</p>
  <p><b>备注</b>：Source code available on Github at: this https URL</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, user text prompts, Recent trends</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent trends are emerging in the use of Large Language Models (LLMs) as autonomous agents that take actions based on the content of the user text prompts. We intend to apply these concepts to the field of Guidance, Navigation, and Control in space, enabling LLMs to have a significant role in the decision-making process for autonomous satellite operations. As a first step towards this goal, we have developed a pure LLM-based solution for the Kerbal Space Program Differential Games (KSPDG) challenge, a public software design competition where participants create autonomous agents for maneuvering satellites involved in non-cooperative space operations, running on the KSP game engine. Our approach leverages prompt engineering, few-shot prompting, and fine-tuning techniques to create an effective LLM-based agent that ranked 2nd in the competition. To the best of our knowledge, this work pioneers the integration of LLM agents into space research. Code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>149. <b>标题：Beyond Suspension: A Two-phase Methodology for Concluding Sports Leagues</b></summary>
  <p><b>编号</b>：[731]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00178">https://arxiv.org/abs/2404.00178</a></p>
  <p><b>作者</b>：Ali Hassanzadeh,  Mojtaba Hosseini,  John G. Turner</p>
  <p><b>备注</b>：32 pages, 9 figures</p>
  <p><b>关键词</b>：Professional sports leagues, Professional sports, suspended due, sports leagues, season</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Problem definition: Professional sports leagues may be suspended due to various reasons such as the recent COVID-19 pandemic. A critical question the league must address when re-opening is how to appropriately select a subset of the remaining games to conclude the season in a shortened time frame. Academic/practical relevance: Despite the rich literature on scheduling an entire season starting from a blank slate, concluding an existing season is quite different. Our approach attempts to achieve team rankings similar to that which would have resulted had the season been played out in full. Methodology: We propose a data-driven model which exploits predictive and prescriptive analytics to produce a schedule for the remainder of the season comprised of a subset of originally-scheduled games. Our model introduces novel rankings-based objectives within a stochastic optimization model, whose parameters are first estimated using a predictive model. We introduce a deterministic equivalent reformulation along with a tailored Frank-Wolfe algorithm to efficiently solve our problem, as well as a robust counterpart based on min-max regret. Results: We present simulation-based numerical experiments from previous National Basketball Association (NBA) seasons 2004--2019, and show that our models are computationally efficient, outperform a greedy benchmark that approximates a non-rankings-based scheduling policy, and produce interpretable results. Managerial implications: Our data-driven decision-making framework may be used to produce a shortened season with 25-50\% fewer games while still producing an end-of-season ranking similar to that of the full season, had it been played.</p>
  </details>
</details>
<details>
  <summary>150. <b>标题：Molecular Generative Adversarial Network with Multi-Property  Optimization</b></summary>
  <p><b>编号</b>：[740]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00081">https://arxiv.org/abs/2404.00081</a></p>
  <p><b>作者</b>：Huidong Tang,  Chen Li,  Sayaka Kamei,  Yoshihiro Yamanishi,  Yasuhiko Morimoto</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：generative adversarial networks, Deep generative models, Deep generative, Monte Carlo tree, generative adversarial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep generative models, such as generative adversarial networks (GANs), have been employed for $de~novo$ molecular generation in drug discovery. Most prior studies have utilized reinforcement learning (RL) algorithms, particularly Monte Carlo tree search (MCTS), to handle the discrete nature of molecular representations in GANs. However, due to the inherent instability in training GANs and RL models, along with the high computational cost associated with MCTS sampling, MCTS RL-based GANs struggle to scale to large chemical databases. To tackle these challenges, this study introduces a novel GAN based on actor-critic RL with instant and global rewards, called InstGAN, to generate molecules at the token-level with multi-property optimization. Furthermore, maximized information entropy is leveraged to alleviate the mode collapse. The experimental results demonstrate that InstGAN outperforms other baselines, achieves comparable performance to state-of-the-art models, and efficiently generates molecules with multi-property optimization. The source code will be released upon acceptance of the paper.</p>
  </details>
</details>
<details>
  <summary>151. <b>标题：Temporal Graph Networks for Graph Anomaly Detection in Financial  Networks</b></summary>
  <p><b>编号</b>：[741]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00060">https://arxiv.org/abs/2404.00060</a></p>
  <p><b>作者</b>：Yejin Kim,  Youngbin Lee,  Minyoung Choe,  Sungju Oh,  Yongjae Lee</p>
  <p><b>备注</b>：Presented at the AAAI 2024 Workshop on AI in Finance for Social Impact (this https URL)</p>
  <p><b>关键词</b>：Temporal Graph Networks, utilization of Temporal, digitized financial transactions, Temporal Graph, Graph Neural Network</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper explores the utilization of Temporal Graph Networks (TGN) for financial anomaly detection, a pressing need in the era of fintech and digitized financial transactions. We present a comprehensive framework that leverages TGN, capable of capturing dynamic changes in edges within financial networks, for fraud detection. Our study compares TGN's performance against static Graph Neural Network (GNN) baselines, as well as cutting-edge hypergraph neural network baselines using DGraph dataset for a realistic financial context. Our results demonstrate that TGN significantly outperforms other models in terms of AUC metrics. This superior performance underlines TGN's potential as an effective tool for detecting financial fraud, showcasing its ability to adapt to the dynamic and complex nature of modern financial systems. We also experimented with various graph embedding modules within the TGN framework and compared the effectiveness of each module. In conclusion, we demonstrated that, even with variations within TGN, it is possible to achieve good performance in the anomaly detection task.</p>
  </details>
</details>
<details>
  <summary>152. <b>标题：UAlign: Pushing the Limit of Template-free Retrosynthesis Prediction  with Unsupervised SMILES Alignment</b></summary>
  <p><b>编号</b>：[743]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00044">https://arxiv.org/abs/2404.00044</a></p>
  <p><b>作者</b>：Kaipeng Zeng,  Xin Zhao,  Yu Zhang,  Fan Nie,  Xiaokang Yang,  Yaohui Jin,  Yanyan Xu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：organic chemical industry, Retrosynthesis planning poses, poses a formidable, formidable challenge, retrosynthesis prediction</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Retrosynthesis planning poses a formidable challenge in the organic chemical industry, particularly in pharmaceuticals. Single-step retrosynthesis prediction, a crucial step in the planning process, has witnessed a surge in interest in recent years due to advancements in AI for science. Various deep learning-based methods have been proposed for this task in recent years, incorporating diverse levels of additional chemical knowledge dependency. This paper introduces UAlign, a template-free graph-to-sequence pipeline for retrosynthesis prediction. By combining graph neural networks and Transformers, our method can more effectively leverage the inherent graph structure of molecules. Based on the fact that the majority of molecule structures remain unchanged during a chemical reaction, we propose a simple yet effective SMILES alignment technique to facilitate the reuse of unchanged structures for reactant generation. Extensive experiments show that our method substantially outperforms state-of-the-art template-free and semi-template-based approaches. Importantly, Our template-free method achieves effectiveness comparable to, or even surpasses, established powerful template-based methods. Scientific contribution: We present a novel graph-to-sequence template-free retrosynthesis prediction pipeline that overcomes the limitations of Transformer-based methods in molecular representation learning and insufficient utilization of chemical information. We propose an unsupervised learning mechanism for establishing product-atom correspondence with reactant SMILES tokens, achieving even better results than supervised SMILES alignment methods. Extensive experiments demonstrate that UAlign significantly outperforms state-of-the-art template-free methods and rivals or surpasses template-based approaches, with up to 5\% (top-5) and 5.4\% (top-10) increased accuracy over the strongest baseline.</p>
  </details>
</details>
<details>
  <summary>153. <b>标题：Stochastic Optimization with Constraints: A Non-asymptotic  Instance-Dependent Analysis</b></summary>
  <p><b>编号</b>：[744]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00042">https://arxiv.org/abs/2404.00042</a></p>
  <p><b>作者</b>：Koulik Khamaru</p>
  <p><b>备注</b>：18 pages</p>
  <p><b>关键词</b>：stochastic convex optimization, VRPG algorithm, VRPG, VRPG algorithm achieves, stochastic convex</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider the problem of stochastic convex optimization under convex constraints. We analyze the behavior of a natural variance reduced proximal gradient (VRPG) algorithm for this problem. Our main result is a non-asymptotic guarantee for VRPG algorithm. Contrary to minimax worst case guarantees, our result is instance-dependent in nature. This means that our guarantee captures the complexity of the loss function, the variability of the noise, and the geometry of the constraint set. We show that the non-asymptotic performance of the VRPG algorithm is governed by the scaled distance (scaled by $\sqrt{N}$) between the solutions of the given problem and that of a certain small perturbation of the given problem -- both solved under the given convex constraints; here, $N$ denotes the number of samples. Leveraging a well-established connection between local minimax lower bounds and solutions to perturbed problems, we show that as $N \rightarrow \infty$, the VRPG algorithm achieves the renowned local minimax lower bound by Hàjek and Le Cam up to universal constants and a logarithmic factor of the sample size.</p>
  </details>
</details>
<details>
  <summary>154. <b>标题：Deep Geometry Handling and Fragment-wise Molecular 3D Graph Generation</b></summary>
  <p><b>编号</b>：[747]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00014">https://arxiv.org/abs/2404.00014</a></p>
  <p><b>作者</b>：Odin Zhang,  Yufei Huang,  Shichen Cheng,  Mengyao Yu,  Xujun Zhang,  Haitao Lin,  Yundian Zeng,  Mingyang Wang,  Zhenxing Wu,  Huifeng Zhao,  Zaixi Zhang,  Chenqing Hua,  Yu Kang,  Sunliang Cui,  Peichen Pan,  Chang-Yu Hsieh,  Tingjun Hou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：incrementally adding atoms, generation approaches follow, partially built molecular, built molecular fragment, incrementally adding</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Most earlier 3D structure-based molecular generation approaches follow an atom-wise paradigm, incrementally adding atoms to a partially built molecular fragment within protein pockets. These methods, while effective in designing tightly bound ligands, often overlook other essential properties such as synthesizability. The fragment-wise generation paradigm offers a promising solution. However, a common challenge across both atom-wise and fragment-wise methods lies in their limited ability to co-design plausible chemical and geometrical structures, resulting in distorted conformations. In response to this challenge, we introduce the Deep Geometry Handling protocol, a more abstract design that extends the design focus beyond the model architecture. Through a comprehensive review of existing geometry-related models and their protocols, we propose a novel hybrid strategy, culminating in the development of FragGen - a geometry-reliable, fragment-wise molecular generation method. FragGen marks a significant leap forward in the quality of generated geometry and the synthesis accessibility of molecules. The efficacy of FragGen is further validated by its successful application in designing type II kinase inhibitors at the nanomolar level.</p>
  </details>
</details>
<details>
  <summary>155. <b>标题：Stress index strategy enhanced with financial news sentiment analysis  for the equity markets</b></summary>
  <p><b>编号</b>：[748]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.00012">https://arxiv.org/abs/2404.00012</a></p>
  <p><b>作者</b>：Baptiste Lefort,  Eric Benhamou,  Jean-Jacques Ohana,  David Saltiel,  Beatrice Guez,  Thomas Jacquot</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：interpreting Bloomberg daily, Bloomberg daily market, daily market summaries, risk-on risk-off strategy, interpreting Bloomberg</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper introduces a new risk-on risk-off strategy for the stock market, which combines a financial stress indicator with a sentiment analysis done by ChatGPT reading and interpreting Bloomberg daily market summaries. Forecasts of market stress derived from volatility and credit spreads are enhanced when combined with the financial news sentiment derived from GPT-4. As a result, the strategy shows improved performance, evidenced by higher Sharpe ratio and reduced maximum drawdowns. The improved performance is consistent across the NASDAQ, the S&P 500 and the six major equity markets, indicating that the method generalises across equities markets.</p>
  </details>
</details>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">徐耀彬</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://louishsu.xyz/2024/04/03/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">http://louishsu.xyz/2024/04/03/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://louishsu.xyz" target="_blank">LOUIS' BLOG</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2024/02/03/Stable%20Diffusion%20%E6%8F%90%E7%A4%BA%E8%AF%8D%E6%8C%87%E5%8D%97%E4%B9%A6.html"><img class="next-cover" src="https://huggingface.co/blog/assets/98_stable_diffusion/stable_diffusion_12_1.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">🎨 Stable Diffusion 提示词指南书</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">徐耀彬</div><div class="author-info__description">💭这个人很懒，什么都没有留下</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/isLouisHsu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/isLouisHsu" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:is.louishsu@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">记录和分享一些学习和开源内容，若有问题可通过邮箱is.louishsu@foxmail.com联系，欢迎交流！！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">统计</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">自然语言处理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">计算机视觉</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">机器学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">人工智能</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/04/03/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2024-04-03)"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv每日速递(2024-04-03)"/></a><div class="content"><a class="title" href="/2024/04/03/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2024-04-03)">Arxiv每日速递(2024-04-03)</a><time datetime="2024-04-03T00:36:54.645Z" title="发表于 2024-04-03 08:36:54">2024-04-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/02/03/Stable%20Diffusion%20%E6%8F%90%E7%A4%BA%E8%AF%8D%E6%8C%87%E5%8D%97%E4%B9%A6.html" title="🎨 Stable Diffusion 提示词指南书"><img src="https://huggingface.co/blog/assets/98_stable_diffusion/stable_diffusion_12_1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="🎨 Stable Diffusion 提示词指南书"/></a><div class="content"><a class="title" href="/2024/02/03/Stable%20Diffusion%20%E6%8F%90%E7%A4%BA%E8%AF%8D%E6%8C%87%E5%8D%97%E4%B9%A6.html" title="🎨 Stable Diffusion 提示词指南书">🎨 Stable Diffusion 提示词指南书</a><time datetime="2024-02-03T06:57:45.000Z" title="发表于 2024-02-03 14:57:45">2024-02-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/22/Transformer%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E4%B8%8E%E9%95%BF%E5%BA%A6%E5%A4%96%E6%8E%A8.html" title="Transformer语言模型的位置编码与长度外推"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/city.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer语言模型的位置编码与长度外推"/></a><div class="content"><a class="title" href="/2023/10/22/Transformer%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E4%B8%8E%E9%95%BF%E5%BA%A6%E5%A4%96%E6%8E%A8.html" title="Transformer语言模型的位置编码与长度外推">Transformer语言模型的位置编码与长度外推</a><time datetime="2023-10-22T14:55:45.000Z" title="发表于 2023-10-22 22:55:45">2023-10-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/09/22/vLLM%EF%BC%9A%E5%88%A9%E7%94%A8%E5%88%86%E9%A1%B5%E7%BC%93%E5%AD%98%E5%92%8C%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E6%8F%90%E9%AB%98%E5%A4%A7%E6%A8%A1%E5%9E%8B2~4x%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6.html" title="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww5.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度"/></a><div class="content"><a class="title" href="/2023/09/22/vLLM%EF%BC%9A%E5%88%A9%E7%94%A8%E5%88%86%E9%A1%B5%E7%BC%93%E5%AD%98%E5%92%8C%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E6%8F%90%E9%AB%98%E5%A4%A7%E6%A8%A1%E5%9E%8B2~4x%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6.html" title="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度">vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度</a><time datetime="2023-09-22T14:55:45.000Z" title="发表于 2023-09-22 22:55:45">2023-09-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/09/06/Prompt%EF%BC%9A%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%89%A7%E8%A1%8C%E6%8C%87%E5%8D%97.html" title="Prompt：大语言模型的执行指南"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww6.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Prompt：大语言模型的执行指南"/></a><div class="content"><a class="title" href="/2023/09/06/Prompt%EF%BC%9A%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%89%A7%E8%A1%8C%E6%8C%87%E5%8D%97.html" title="Prompt：大语言模型的执行指南">Prompt：大语言模型的执行指南</a><time datetime="2023-09-06T14:45:45.000Z" title="发表于 2023-09-06 22:45:45">2023-09-06</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By 徐耀彬</div><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender"title="主题采用butterfly"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr"title="本站使用JsDelivr为静态资源提供CDN加速"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub"title="本站项目由Gtihub托管"alt="img"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"alt="img"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></br></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'default',
      })
      false && mermaid.init()
    })
  }
}</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    let initData = {
      el: '#twikoo-wrap',
      envId: 'blog-',
      region: ''
    }

    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    twikoo.init(initData)
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'blog-',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/机器学习/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐱 机器学习 (3)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/自然语言处理/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 自然语言处理 (8)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/竞赛相关/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 竞赛相关 (3)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/阅读笔记/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 阅读笔记 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="http://louishsu.xyz/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt=""><img width="48" height="48" src="https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161037709574435991610377095138.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2021-05-19</span><a class="blog-slider__title" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt="">全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/cail2021.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2021-10-22</span><a class="blog-slider__title" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt="">中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt=""><img width="48" height="48" src="https://cdn.kesci.com/upload/image/r7j60un866.png?imageView2/2/w/2500/h/2500" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-11-17</span><a class="blog-slider__title" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt="">2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww5.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-09-22</span><a class="blog-slider__title" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt="">vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/10/22/Transformer语言模型的位置编码与长度外推.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/city.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-10-22</span><a class="blog-slider__title" href="2023/10/22/Transformer语言模型的位置编码与长度外推.html" alt="">Transformer语言模型的位置编码与长度外推</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/10/22/Transformer语言模型的位置编码与长度外推.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/02/03/Stable Diffusion 提示词指南书.html" alt=""><img width="48" height="48" src="https://huggingface.co/blog/assets/98_stable_diffusion/stable_diffusion_12_1.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-02-03</span><a class="blog-slider__title" href="2024/02/03/Stable Diffusion 提示词指南书.html" alt="">🎨 Stable Diffusion 提示词指南书</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2024/02/03/Stable Diffusion 提示词指南书.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww6.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-09-06</span><a class="blog-slider__title" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt="">Prompt：大语言模型的执行指南</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/11/26/升级深度学习开发环境全攻略.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/default.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-11-26</span><a class="blog-slider__title" href="2022/11/26/升级深度学习开发环境全攻略.html" alt="">升级深度学习开发环境全攻略</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2022/11/26/升级深度学习开发环境全攻略.html" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>